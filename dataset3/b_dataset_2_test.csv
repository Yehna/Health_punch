article,summary
"Never-ending information generation sharing Web provides us abundant data, constitute unstructured text sources. To better make sense draw associations among data, we, human beings, use relational facts among subjects text. For comprehensive understanding specific domains bioinformatics, finance, social networking etc., need computers process information. It essential represent information delivered text machine-readable format. One way represent entities relations called triples, indicate unambiguous facts entities. A triple implies entity relation another entity . Knowledge graphs FreeBase DBpedia examples representations. They directed labeled graph structured data aim express explicit semantics relations entities triple form. Relation extraction sub-task natural language processing aims discover relations entity pairs given unstructured text data. Earlier work relation extraction text heavily relies kernel based feature based methods .However, recent research studies make use data-driven deep learning methods eliminating conventional NLP approaches relation extraction. \citet{kumar2017survey} explained conventional deep learning methods integrated relation extraction. \citet{smirnova2018relation} reviewed relation extraction literature focusing distant supervision. As number research studies relation extraction increases, need survey current state-of-the-art neural relation extraction methods arises. This work provides comprehensive comparative review research field, focusing challenges together improvement ideas. Section explains various approaches relation extraction. In section neural relation extraction methods classified terms data supervision explained. Section describes existing challenges field research. In section , commonly used datasets model assessment evaluated. We discuss possible future research directions improvement ideas section conclude survey section . In paper, provided review transfer learning natural language processing. We discussed possible language models, datasets, tasks tackled research related transfer learning. Moreover, provided taxonomy transfer learning divides inductive transductive transfer learning. We divided category multiple levels collected related papers corresponding category. Although might different definitions literature, tried best incorporate best definition agreed upon across different studies. In general, see compared RNN-based CNN-based language models, seems attention-based models much dominant literature. Additionally, see BERT seems defacto architecture language modelling appears many tasks. This due bidirectional architectures makes successful many down-stream tasks. Regarding transfer learning, sequential fine-tuning seems dominant literature compared approaches like zero-shot. Moreover, seems mutli-task fine tuning gaining attention last years. As stated many studies, training multiple tasks time give much better results. Regarding datasets, text classification datasets seem widely used compared tasks NLP. This due fact easier fine-tune models tasks. For future research, make observations outlooks field transfer learning NLP. For specific tasks like sentiment classification, abstractive question answering, parts-of-speech tagging, recommend using bidirectional models like BERT. On hand, generative tasks like summarization, generative question answering, text generation, etc. recommend using models like GPT-2, T5 similar architectures. We also believe transfer learning techniques underrated like zero-shot seems perform really well multiple tasks . Moreover, adapter modules replace sequential fine-tuning perform equally good provide faster compact models compared traditional fine tuning. Finally, language models keep getting bigger bigger, believe research put trying reduce size models make deploy-able embedded devices web. Deploying knowledge distillation techniques prove useful circumstances reducing size large language models. Irfan: I would suggest add section point recommendations/best practices possible future works short term long term. Other paper looks good shape. Irfan: Is popular NLP task transfer learning researched?"," Neural relation extraction discovers semantic relations between entities from unstructured text using deep learning methods. In this study, we present a comprehensive review of methods on neural network based relation extraction. We discuss advantageous and incompetent sides of existing studies and investigate additional research directions and improvement ideas in this field.  \keywords{Neural relation extraction, distant supervision, deep learning}"
"Deep neural networks shown promise various tasks natural language processing , DNN usually considered black-box model. In recent years, explaining features encoded inside DNN become emerging direction. Based inherent hierarchical structure natural language, many methods use latent tree structures language guide DNN learn interpretable feature representations. However, interpretability usually conflicts discrimination power. There considerable gap pursuing interpretability features pursuing superior performance. Therefore, study, aim explain trained black-box DNN post-hoc manner, explanation DNN affect performance. This essentially different previous studies designing new network architectures losses learn interpretable features, e.g.~physically embedding tree structures DNN. Given trained DNN, paper, propose analyze interactions among input words, used DNN make prediction. Our method generates tree structure objectively reflect interactions among words. Mathematically, interaction several words quantified difference contribution case words contribute jointly prediction case individual word contributes independently prediction. The interaction words may bring either positive negative effects prediction. For example, word green word hand sentence green hand strong positive interaction prediction person's identity, words green hand indicate ``novice"" jointly, rather work individually represent hand green color. The core challenge study guarantee objectiveness explanation. I.e. tree needs reflect true interactions among words without significant bias. The Shapley value widely considered unique unbiased estimation word contribution, satisfies four desirable properties, i.e.~linearity, dummy, symmetry efficiency. Thus, define interaction benefit among words based Shapley value. Let us consider constituent words. denote numerical contributions word prediction DNN, respectively. represents numerical contribution entire constituent prediction. Hence, measures interaction benefit constituent. If , interactions among words positive effects prediction; otherwise, negative effects. Here, computed Shapley values. Given trained DNN input sentence words, Figure shows tree structure reflects word interactions encoded inside DNN. In tree, leaf nodes represent input words. Each non-leaf node corresponds constituent input sentence. A parent node connects two child nodes significant interaction benefits. We use parent node encode interactions among child sub-constituents. More specifically, two types interactions among words, i.e.~ interactions within constituent interactions constituents. {} Interactions within constituent exist among two words constituent. For sentence ``the sun shining sky,"" interactions within constituent sky consist interactions among combinations words, including interactions , , among . {} Interactions constituents. In aforementioned sentence, interactions constituent sun adjacent constituent shining composed potential interactions among combinations words two constituents, including interactions , , , , , , , , . The tree selects encodes salient interactions among words, order reveal signal processing DNN. We propose additional metrics diagnose interactions among words, e.g.~the quantification interactions within constituent, quantification interactions two adjacent constituents, ratios interactions modeled unmodeled tree. Theoretically, method used generic tool analyze various DNNs, including BERT, ELMo, LSTM, CNN Transformer. Experimental results demonstrated effectiveness method. Contributions paper summarized follows. We propose method extract quantify interactions among words. A tree structure automatically generated represent salient interactions encoded DNN. We design six metrics analyze interactions, provides new perspectives understand DNNs. In survey, summarized neural relation extraction methods terms approaches data supervision datasets task. In addition, explained common challenges discussed possible remedies them. To acquire abundant training instances, latest studies make use distant supervision. However, brings noise data greatly affects training relation extraction models. In addition, explicit negative samples, since data wrong annotations due ill-alignment unstructured text knowledge graph. For reason, instead sentence-level approaches supervised relation extraction, multi-instance approaches developed relation extraction distant supervision. Also, few-shot learning relation extraction research area still room improvement. Supervised approaches abandoned. Indeed, incorporating pre-trained language models supervised relation extraction make significant improvements comparison using conventional deep learning methods. Instead treating entity recognition relation extraction separately pipeline approaches, later studies adopt end-to-end approaches jointly extracting entities relations, tend better handle problems associated overlapping triples long-tail relations."," This paper proposes a method to disentangle and quantify interactions among words that are encoded inside a DNN for natural language processing. We construct a tree to encode salient interactions extracted by the DNN. Six metrics are proposed to analyze properties of interactions between constituents in a sentence. The interaction is defined based on Shapley values of words, which are considered as an unbiased estimation of word contributions to the network prediction. Our method is used to quantify word interactions encoded inside the BERT, ELMo, LSTM, CNN, and Transformer networks. Experimental results have provided a new perspective to understand these DNNs, and have demonstrated the effectiveness of our method."
"Even though usage popularity Twitter stopped rapidly growing even dropped recent years\footnote{https://www.statista.com/statistics/282087/number-of-monthly-active-twitter-users}, still considerable amount loyal users keep sharing everything worldwide events random personal details followers. We decided focus one random personal details people share, specifically - anything food consumption related topics. Several corpora Latvian tweets exist prior work, none domain-specific collected extensive period time. Milajevs collected analysed 1.4 million tweets geo-located Riga, Latvia April 2017 July 2018 60 thousand tweets November 2016 March 2017. Pinnis collected analysed 3.8 million tweets Latvian politicians, companies, media, users interacted entities August 2016 July 2018 There also several data sets general sentiment-annotated tweets \footnote{https://github.com/nicemanis/LV-twitter-sentiment-corpus} amounting 14,781 tweets total. In paper, describe Twitter eater corpus analyse contents. We also provide two sub-corpora - one consisting question answer tweets one sentiment-annotated tweets. More details found Section . In Sections describe question answering sentiment analysis experiments using corpus. Finally, conclude paper Section . In paper, defined extracted interaction benefits among words encoded DNN, used tree structure organize word interactions hierarchically. Besides, six metrics defined disentangle quantify interactions among words. Our method regarded generic tool objectively diagnose various DNNs NLP tasks, provides new insights DNNs. two-column Using \centering command instead save space Positioning figure top page save space make paper readable Using 0.95\columnwidth conjunction","     We present the Latvian Twitter Eater Corpus - a set of tweets in the narrow domain related to food, drinks, eating and drinking. The corpus has been collected over time-span of over 8 years and includes over 2 million tweets entailed with additional useful data. We also separate two sub-corpora of question and answer tweets and sentiment annotated tweets. We analyse contents of the corpus and demonstrate use-cases for the sub-corpora by training domain-specific question-answering and sentiment-analysis models using data from the corpus."
"Simultaneous translation widely used international conferences, summits business. Different standard neural machine translation , simultaneous NMT stricter requirement latency. We cannot wait end source sentence start translation right reading first words. That is, translator required provide instant translation based partial source sentence. Simultaneous NMT formulated prefix-to-prefix problem, prefix refers sub-sequence starting beginning sentence translated. In simultaneous NMT, face uncertainty conventional NMT, since translation starts partial source sentence rather complete information conventional NMT. \Waitk{} simple yet effective strategy simultaneous NMT generated translation words behind source input. That is, rather instant translation word, \waitk{} actually leverages future words. Obviously, larger leverage future information, therefore results better translation quality cost larger latency. Thus, used real-world applications, relatively small simultaneous NMT. While small values allowed inference, observe training larger lead better accuracy \waitk{} inference, demonstrated Figure, wait- model required EnglishGerman translation. If training , obtain BLEU score. But train wait- set larger value , test wait-, get better BLEU scores. Despite mismatch training wait- testing wait-, model benefit availability future information. This consistent observation . %<-- start figure % \end{wrapfigure} %<-- end figure Here, challenge much future information use. As shown Figure, using future information monotonically improve translation accuracy \waitk{} inference, mainly future information results larger gap training inference. In work, propose framework automatically determine much future information use training simultaneous NMT. Given pre-defined inference, prepare training tasks wait- different values . We introduce controller given training sample, controller dynamically select one tasks maximize validation performance \waitk, i.e., one interested in. The task selection based data network status translation model. The controller model translation model jointly learned, learning process formulated bi-level optimization problem design effective algorithm solve it. We conduct experiments four datasets verify effectiveness method. The remaining part organized follows. The related work introduced Section, problem formulation background %, including monotonic Transformer, introduced Section, method introduce Section. The experiments analysis Section , discuss conclusion future work Section . In paper, described creation fairly large narrow-domain corpus Twitter posts related topic eating. We gave insights overall observations gained corpus contents various trends noticed data. We believe data would useful many linguistic, sociological, behavioural research areas. We experimented creating food-related question answering system using one subset data sentiment analysis system using another subset highlight potential use-cases corpus. While results break new ground, hope inspire related future research. Overall corpus gives fair amount analytical data work with. We advise interested persons morphological analysis TEC text find interesting patterns ""Question Answering Experiment"". In future work continue collect data next years corpus try gain data, older October 2011. A sentiment analysis included full paper."," Simultaneous neural machine translation  has attracted much attention recently. In contrast to standard NMT, where the NMT system can utilize the full input sentence, simultaneous NMT is formulated as a prefix-to-prefix problem, where the system can only utilize the prefix of the input sentence and more uncertainty is introduced to decoding. \Waitk is a simple yet effective strategy for simultaneous NMT, where the decoder generates the output sequence $k$ words behind the input words. We observed that training simultaneous NMT systems with future information  generally outperforms the standard ones . Based on this observation, we propose a framework that automatically learns how much future information to use in training for simultaneous NMT. We first build a series of tasks where each one is associated with a different $k$, and then learn a model on these tasks guided by a controller. The controller is jointly trained with the translation model through bi-level optimization. We conduct experiments on four datasets to demonstrate the effectiveness of our method."
"\iffalse \yuliang{moved original intro comments. } % What think ``Extracting Useful Sentences Online Reviews: Data Management Perspective''? % % Online reviews proven important assets online platforms Amazon, Yelp, Booking.com. % These reviews serving rich information source users' decision making process % ranging purchasing electronic products, booking romantic vacation, important decisions like housing career. % Oftentimes, beyond decision supporting, online platforms perform text classification % make use reviews. Sentiment analysis perhaps well-studied classification task review text. % By classifying whether reviews positive negative, % better understand quality service/product even specific aspect. % While much research attention focused sentiment classification, however, % review text also mined fulfill many interesting purposes. % For example, online reviews, eBay extracts sentences generate product description product one, % TripAdvisor mines useful tips future travelers % Airbnb constructs user profiles based reviews user given. % pipeline % machine learning models text classification % One specialized model task % Data preparation, model selection Database engines nowadays store serve massive amount text data. To make best text data, commonly performed task perhaps text classification. The classification results allow database engine aggregate unstructured text, better filter query results, perform complex business analytic tasks. Consider example online review database. By performing sentiment classification review text database, review aggregation platforms like Amazon, Yelp Airbnb better understand quality customer satisfaction level product service aggregating opinion polarity reviews item. Beyond sentiment analysis, review text mined fulfill many interesting purposes via classification. For example, based results text classification, eBay extracts sentences generate product description product one, TripAdvisor mines useful tips future travelers Airbnb constructs user profiles based reviews user given. As first contribution paper, conduct comprehensive literature survey different text classification tasks performed review text Section . The recent success deep learning NLP provides solutions text classification tasks promising results. For example, recent NLP model sentiment classification achieves accuracy 97.1\%, 0.7\% away human performance . While exists highly effective models, process developing expensive time-consuming. To make things even worse, model architecture ``globally optimal''. We found literature researchers proposed new model architectures achieve best performance task. This means expensive process model engineering repeated individual application. In work, focus primarily two parts process contributes cost: training data preparation model engineering. \yuliang{let's see whether get data cleaning} \yuliang{the training data preparation part give credit Snorkel.} Both training data preparation model engineering non-trivial processes. Even powerful models like TextCNN , LSTM BERT , training good text classifier still requires significant amount training data . Consider preparing training data via crowdsourcing . It incurs significant monetary cost takes great amount time preparing instructions crowdworkers, launching, post-processing results ensure quality. After collecting training data, training good text classification model simple plug-and-play. The developer needs choose base model range options . There different options represent text including bag-of-words , different word embedding tokenization methods , context-aware embeddings . There even tunable hyper-parameters: number layers, learning rates, batch sizes, etc. There might also task-specific features developer would like take account. \yuliang{TODO: cite AutoML} Aside engineering effort programming options, tuning hyper-parameters takes significant amount time computing resource given large space hyper-parameter combinations. For example, found experiments even using pre-trained language models like BERT off-the-shelf , many combinations hyper-parameters. Trying exhaustively would take GPU-hours current cost one V100 GPU \$3.06 per hour. Although important, dataset preparation model engineering usually ignored prior works \yuliang{add citations} NLP focus primarily optimizing model accuracy. In work, take data management perspective. Towards goal reducing overhead, via extensive experiential study, explicitly model cost training data preparation model engineering. In summary, paper makes following contributions: % While much research attention focused sentiment classification, however, % review text also mined fulfill many interesting purposes. % For example, online reviews, eBay extracts sentences generate product description product one, % TripAdvisor mines useful tips future travelers % Airbnb constructs user profiles based reviews user given. \fi % online review %Online reviews treasures decision makers. They collect experience individuals present full picture target readers. It gradually become common sense search online reviews making decisions picking hotel, purchasing product, visiting attraction. However, volumes online reviews grow rapidly takes lot time readers. To help readers read efficiently, extensive research focuses extracting sentences deemed useful readers develops applications summarization. There two benefits. First, relieves reading pressure readers go stocks reviews. Second, sentences important others deploying application. For example, product describing sentences appealing personal stories reader wants understand product. %A great success review sentences extraction sentiment analysis, fundamental problem whether sentence expresses positive opinions negative ones. This problem together variants studied decades recently receive extensive attention learning-based methods push prediction quality limit human prediction . Besides, problem getting increasingly important merchants accumulate reviews engineers develop abundant applications stock prediction, poll analysis, experience search. %%Emerging applications. %Accompanying success sentiment analysis, natural question ask else extract. This especially important review owners make best use data. We therefore summarize six types emerging sentences applications, including suggestion, tip, product description, humor, argument, spoiler. These sentences serve different purposes corresponding review types. Suggestion extracts advice-giving sentences. They helpful improve product service, experience potential customer. Tip describes personal experience short practical texts. Tip may imply suggestion paying 18\% tip dining restaurant united states. Product description depicts factual details specifications product. Humors sentences written creative way read funny vivid. Argument focuses evidential comments support oppose proposition. Spoiler refers plot-revealing remarks common reviews media products. Many six applications lead commercial products, TripAdvisor travel tip, Yelp funny reviews, eBay new item description, Yahoo short answers ``how-to'' queries Airbnb user profiling aspect describing comments. % A lot applications processing text rely semantic tags % annotated text. %\textcolor{purple}{ A lot applications processing text rely tagging words, phrases sentences semantically informative tags. %} \nikita{more straightforward?}\jinfeng{Origin: ``A lot applications processing text rely semantic tags annotated text.''} Sentiment analysis, example, annotates sentences phrases sentiment tag indicates whether sentence positive negative sentiment. These sentiment tags exploited downstream applications determine appropriate actions. Another example entity tagging, determines span text refers real-world object. %\textcolor{purple}{ Generally speaking, task annotating text semantic tags referred semantic tagging problem. %} \jinfeng{Origin: ``We refer problem annotating text semantic tags semantic tagging problem.''} More precisely, semantic tagger takes piece text predefined tag inputs, outputs whether text conveys semantics tag. %\textcolor{purple}{ In paper, focus short text, sentence, paragraph, passage. We also refer short text loosely sentence. %} \jinfeng{Origin: ""In paper, refer sentence, paragraph, passage loosely ``sentence''''}. %\xiaolan{Add example & citations.} \jinfeng{Added One example next sentence} %A great success semantic tagging sentiment %analysis, %where practitioners develop lot applications predict whether %sentence conveys tag positive negative opinions. In addition %to applications using sentiment analysis, many applications %machine learning artificial intelligence also empowered %semantic tagging. To completely understand people interested %in semantic tagging, survey applications adopting semantic tagging learn semantic tagging works them. %Since novel applications attracting increasing attention, briefly discussed paper. We surveyed practices emerging recent three years classified five groups according tags focus on, including Tip, Product Description, Humor, Argument, Spoiler. These classifications give ideas practitioners new applications develop. %To solve semantic tagging, solution able classify sentences two groups according relevance targeted tag. There two types methods, supervised learning rule programming. Recent solutions generally leverage supervised learning, automatically learns separate sentences set human labels. As option, rule programming less used due significant programming effort. It difficulty sometimes impossible program full set rules classification. There numerous rules sentence semantically convey targeted tag. Herein focus discussing supervised methods. %To solve semantic tagging, solution able classify sentences two groups according relevance targeted tag. Recent solutions generally leverage supervised learning, automatically learns separate sentences set human labels. Supervised learning offers least two favorable advantages. First, input algorithm set labels obtained non-programmer experts. Second, algorithms programming free difficult polish combination rules reproduce human labels. It difficulty sometimes impossible program full set rules classification. There numerous rules sentence semantically convey targeted tag. Herein focus discussing supervised methods. %\wctan{changed ``sentence tagging'' ``semantic tagging''}\jinfeng{I like ``semantic tagging'' used throughout paper} %A solution semantic tagging able classify %sentences two groups according relevance %semantic tag. There two types methods semantic tagging: rule programming supervised learning. Rule programming-based methods require expert specify rules semantic tagging. This often error-prone requires significant programming effort. % since difficult specify good set rules semantic tagging general. In contrast, supervised learning models require much programming effort. However, training models requires labeled data typically produce models good semantic tagging results. %\wctan{do evidence less used frequently used? I removed mentions claims.} %\wctan{rewrote. pls read} \jinfeng{It reads great!} Our focus paper supervised learning models. Deep learning models become popular methods semantic tagging today. One reason deep models popular semantic tagging often capable learning complicated functions kinds models. Another reason superiority deep models reported many publications. For example, deep models achieve good prediction quality close human prediction GLUE SST-2 sentiment classification task. Some recent studies made comparisons deep models simple models understand whether deep models always superior simple models. %\wctan{what tasks compared studies?}\jinfeng{other semantic tagging tasks. Added one sentence below.} They conducted comparisons various tagging tasks suggestion mining humor detection. Their results reveal marginal sometimes improvements deep models simple models. It therefore natural ask whether deep models better simple models developing solutions semantic tagging. %\textcolor{purple}{ Semantic tagging forms core many tasks including sentiment classification, suggestion mining, humor detection. Existing studies, however, compare deep simple models individual tasks. Furthermore, provide insights dataset characteristics affect performances different models. Consequently, hard generalize model selection criteria new tasks new datasets task. Hence, given new dataset, still unclear whether selecting deep model bring best tagging performance. %} % The goal identify model achieves best performance targeted task. These studies consider dataset characteristics selecting best model. As consequence, previous studies show generalize model selection new tasks even new datasets task. They developed selection guidance granularity individual task granularity individual dataset characteristic. Given new dataset, still unclear whether selecting deep models bring best semantic tagging performance.} %\jinfeng{add one paragraph motivate 21 datasets study} % select five models 21 different datasets In paper, embark systematic study understand performance tradeoffs deep models vs. simple models semantic tagging. Towards goal, selected 3 representative deep models: CNN, LSTM, BERT 2 representative simple models: LR SVM. CNN LSTM well-known methods widely used academic industry communities recently, BERT. %represents up-to-date method w%on best paper award NAACL 2019. To make meaningful comparison systematic study, collected 21 real datasets frequently used semantic tagging. These datasets exhibit several prominent data characteristics, including variable number labels ; wide range tag-conveying label ratio ; different label cleanliness . %We evaluated tagging quality CNN, LSTM, BERT, LR, SVM 21 datasets. We used F1 quality measurement. Overall, BERT outperforms LR SVM datasets, clear winner CNN/LSTM LR/SVM. However, BERT outperform LR/SVM large datasets 100,000 labels. BERT achieves F1 one large dataset worse F1 two large datasets. The maximum F1 gap BERT large datasets 0.03 yet training takes days. In words, deep models may better choices simple models large datasets, training time considered another evaluation criterion. %We conducted analyses measure effects size, label skewness, cleanliness tagging quality. We found three dataset characteristics significant influences superiority deep models. When dataset abundant labels, exhibits high ratio positive instances, contains many dirty labels, deep models show less F1 improvements. Therefore, practitioners pay attention dataset characteristics expect better tagging quality using deep models. %We found dataset characteristics affect superiority deep models, also regulate final tagging quality. To help practitioners select appropriate models set expectation tagging performance dataset, prepare comprehensive heap map shows characteristics 21 datasets tagging F1's BERT SVM. By using heap map, practitioners estimate tagging quality gain adopting deep models. Meanwhile, try improve dataset characteristics push tagging quality higher upper limits. \\ We evaluate quality semantic tagging five selected models 21 datasets obtain rather surprising finding. %that overturns general perception %towards deep models. We find deep models simple models complementary task semantic tagging. Specifically, deep models perform significantly better smaller datasets, simple models trained efficiently larger datasets achieve similar semantic tagging quality. Therefore, one select deep models simple models semantic tagging based actual dataset characteristics requirements efficiency. %On large datasets contain 100,000 labels, deep models achieve maximal 0.03 F1 superiority yet takes days training. If training time concern, deep models better choices simple models large datasets. %Deep models offer better tagging quality, alone inadequate guarantee satisfactory tagging quality. In experiments, tagging F1 best deep model varies 0.96 0.15. This tagging performance time regulated dataset characteristics, including number labels, ratio tag-conveying labels, label cleanliness. We conducted experiments confirmed labels, higher ratio, better label cleanliness contribute greater tagging quality deep models. Based findings, develop comprehensive heat map guide practitioners selecting appropriate model desired semantic tagging performance datasets. This heat map shows characteristics datasets quality score semantic tagging different tagging models. By using heat map, practitioners estimate semantic tagging quality gain adopting different deep simple models. At time, also try improve dataset characteristics improve quality semantic tagging. \\ %To help practitioners select appropriate models set expectation tagging performance datasets, prepare comprehensive heap map shows characteristics 21 datasets tagging F1's BERT SVM. By using heap map, practitioners estimate tagging quality gain adopting deep models. Meanwhile, try improve dataset characteristics push tagging quality higher upper limits. \\ Contributions. We systematically evaluate deep models simple models task semantic tagging. %We selected X models, curated Y datasets varying characteristics, conducted series experiments investigate performance models two models datasets. %\wctan{Are releasing collection datasets models?} \jinfeng{Yes. I added one sentence end paragraph. Will work release later.} Our key contributions follows. We surveyed number applications motivate study. We selected three representative deep models two simple models widely used develop applications. We collected 21 datasets varying characteristics comprehensive study. We conducted extensive evaluations obtain performance semantic tagging five selected models datasets. We found deep models necessarily perform better simple models large datasets. We evaluated effects dataset characteristics quality semantic tagging. We found training size, label ratio, label cleanliness impact quality semantic tagging. We generated comprehensive heat map guide practitioners decide whether adopt deep models simple models anticipate performance semantic tagging datasets. To facilitate future research, release collection datasets, models, implementations https://github.com/rit-git/tagging. \\ % Contributions. In paper, studied sentences extraction online reviews. Specifically, summarized contributions follows. We release codes datasets \jinfeng{we need decide} research. % Outline. We survey number applications Section. We discuss designs selected deep models simple models Section. We introduce collected datasets characteristics Section. We perform experimental evaluations comparisons Section. We analyze effect dataset characteristics present key findings Section. We conclude study Section. \documentclass{vldb} \usepackage{balance} \usepackage[shortlabels]{enumitem} \usepackage{multirow} %\usepackage{times} \usepackage{graphicx} \usepackage{multirow} \usepackage{xcolor} % \newtheorem{theorem}{Theorem}[section] \newtheorem{example}{Example} \usepackage{graphicx} \usepackage[labelfont=bf]{caption} \usepackage{subcaption} \usepackage{tabularx} \usepackage{multicol} \usepackage{booktabs} \usepackage{color, colortbl} \setlist[itemize]{leftmargin=*} \setlist[enumerate]{leftmargin=*} \definecolor{Gray}{gray}{0.9} \usepackage{times} \newcommand{\yuliang}[1]{{\it\small\textcolor{blue}{[[[ {#1}\ --yuliang ]]]}}} \newcommand{\xiaolan}[1]{{\it\small\textcolor{brown}{[[[ {#1}\ --xiaolan ]]]}}} \newcommand{\nikita}[1]{{\it\small\textcolor{green}{[[[ {#1}\ --nikita ]]]}}} \newcommand{\alon}[1]{{\it\small\textcolor{purple}{[[[ {#1}\ --alon ]]]}}} \newcommand{\wctan}[1]{{\it\small\textcolor{red}{[[[ {#1}\ --wangchiew ]]]}}} \newcommand{\jinfeng}[1]{{\it\small\textcolor{cyan}{[[[ {#1}\ --jinfeng ]]]}}} \newcommand{\lu}[1]{{\it\small\textcolor{orange}{[[[ {#1}\ --lu ]]]}}} \newcommand{\concept}[1]{{\it\small\textcolor{orange}{[[[ {#1}\ --require definition]]]}}} \def\flat{simple} \def\Flat{Simple} \def\FLAT{SIMPLE} % Semantic tagging? \def\sentence{sentence} \def\Sentence{Sentence} \def\tagging{tagging} \def\Tagging{Tagging} % Include information uncomment camera ready \vldbTitle{Deep Simple Models Semantic Tagging? It Depends Data} \vldbAuthors{Jinfeng Li, Yuliang Li, Xiaolan Wang, Wang-Chiew Tan} \vldbDOI{https://doi.org/10.14778/3407790.3407844} \vldbVolume{13} \vldbNumber{11} \vldbYear{2020} \pagenumbering{gobble} % % % %\balance \bibliographystyle{abbrv} \bibliography{main} \end{document}\documentclass{vldb} \usepackage{graphicx} \usepackage{multirow} } \end{table*} % problem similar extractive summarization In work, propose new approach simultaneous NMT. Motivated fact \waitk{} benefits future information, introduce controller, adaptively assigns task wait- input. A bi-level optimization method leveraged jointly obtain translation model controller. Experiments four translation tasks demonstrate effectiveness approach. For future work, many interesting directions. First, enhance objective function Eqn.\eqref{eq:high_level_obj} beyond using translation quality explicitly introduce latency constraint. Second, combine method adaptive decoding methods. Third, apply idea work applications like action prediction, weather forecasting, game AI, etc.","  Semantic tagging, which has extensive applications in text mining, predicts whether a given piece of text conveys the meaning of a given semantic tag. The problem of semantic tagging is largely solved with supervised learning and today, deep learning models are widely perceived to be better for semantic tagging. However, there is no comprehensive study supporting the popular belief. Practitioners often have to train different types of models for each semantic tagging task to identify the best model. This process is both expensive and inefficient.  We embark on a systematic study to investigate the following question: Are deep models the best performing model for all semantic tagging tasks? To answer this question, we compare deep models against ``simple models'' over datasets with varying characteristics. Specifically, we select three prevalent deep models  and two simple models , and compare their performance on the semantic tagging task over 21 datasets.  %The results showed that deep models are not %necessarily better than simple models when the %characteristics of datasets are variable. %\xiaolan{Maybe say they did not perform well on large datasets first. The previous sentence claims they are not better, so the next sentence better elaborate over this point.} \xiaolan{Maybe add a transition sentence? e.g., To better understand their performance}  %To understand what are the exact characteristics of datasets affecting tagging quality, we selected the representative simple model  and deep model , and compared their performances on different types of datasets.  Results show that the size, the label ratio, and the label cleanliness of a dataset significantly impact the quality of semantic tagging. Simple models achieve similar tagging quality to deep models on large datasets, but the runtime of simple models is much shorter. Moreover, simple models can achieve better tagging quality than deep models when targeting datasets show worse label cleanliness and/or more severe imbalance. Based on these findings, our study can systematically guide practitioners in selecting the right learning model for their semantic tagging task. %\textcolor{blue}{In particular, we found that simple models outperform deep models on larger datasets with higher label ratios or worse label cleanliness, when runtime is a concern}.  %The conditional outperformance of deep models %suggests that practitioners should carefully %select learning models when they aim to achieve %the best tagging quality.  %Our results will systematically guide practitioners in selecting the right learning model for their semantic tagging task. % %To further assist practitioners to pick right %learning models, we generated a comprehensive heat map that compares tagging qualities across varied combinations of models and datasets. The heat map will be an instructional reference for practitioners to adopt appropriate models and set accurate expectations when tagging sentences for their own datasets.  %\wctan{heat map or table?} \jinfeng{I prefer heat map that uses colors and reflects the effect of data characteristics on F1}  %\textcolor{red}{Our results indicate that practitioners should pay attention to dataset characteristics when they apply deep models for better \tagging{} quality. We also generate a comprehensive heat map from our results that can help practitioners to adopt appropriate models and set expectation on tagging performance for their datasets}."
"%% UNSUPERVISED SHIZ The problem disambiguation defined selecting correct analysis set possible analyses word sentence---e.g., among analyses produced morphological analyser. Disambiguation performed utilizing information surrounding context.\footnote{This paper contains corrigenda previously published paper. It corrects mistake original evaluation setup, results reported Section, Tables, , and.} Morphological analysers commonly used various NLP applications. These normally produce significant amount ambiguous analyses. In work tackle problem disambiguation training model predicting correct part-of-speech lemma. We show majority cases, sufficient disambiguate set possible analyses. % This problem relevant languages supervised statistical models significantly out-perform rule-based analysers. These often smaller languages lack quantities high-quality expert annotated data exists, example, English. We use manually annotated data evaluation, means train model need morphological analyser language unlabelled corpus. % Erzya North-Sami two examples languages morphological analyser less 30k annotated tokens.%\todo{such as? russian it..}. %%%%%%%%%%%%%%%%%%% The main idea approach use bidirectional LSTMs---BiLSTMs---to disambiguate output morphological analysers, utilizing unambiguous outputs training procedure. %% \todo{TXEMA: add context embedding shit} % We train bidirectional models using sequence embeddings surface form target word. The objective network produce output probability distributions possible POS tags lemmas. The model trained using unambiguous input tokens; loss computed unambiguous instances. Ambiguous tokens considered target tokens training. % Evaluation done manually annotated data language. Thus, quality evaluation dependent amount available annotated data---the model quality affected amount unlabelled data use. Since input unlabelled data training, quality model affected amount available unlabelled data language. In experiments, evaluate models manually annotated data sets Finnish, Russian Spanish. For Finnish Russian, least, annotated data limited supply, whereas three languages unlabelled data abundant supply. %% ??? CITE CORPORA!!! The paper organized follows. % In Section point relevant prior work. % In Section describe problem morphological ambiguity provide brief motivation interest problem. % In Section provide classification different types ambiguity appear corpus, well analysis viable appropriate strategies type ambiguity. % Section describes data pre-processing steps model architecture. % Section specifies experimental setup, well parameters used training. % In Section discuss results obtained experiments. % Section concludes current directions research. We conducted comprehensive study evaluate performance deep models simple models tagging various datasets. In evaluation, picked recently emerging deep model , two popular deep models , two classical simple models . Our results show deep models necessarily better simple models . Specifically, deep models achieve significantly higher tagging quality small datasets, cannot obtain better performance simple models large datasets. Our analyses show extraction quality model time affected dataset characteristics, including training size, label skewness, label cleanliness. Any following three conditions may lead better performance tagging models, including abundant labels, high ratio positive instances, many clean labels dataset. However, improvement F1's achieved deep models less obvious achieved simple models conditions, leading deep models always better simple models tagging labels. Therefore, choose suitable tagging model specific dataset rather sticking deep models way performing tagging tasks future. Our study, especially visualized heat map informative instruction practitioners want consider scale, skewness, cleanliness dataset criteria choosing suitable tagging models. For example, deep models outperform simple models dataset abundant labels, deep models consume significantly computational overhead. Our study comprehensive one used largest number real-world datasets compare deep models simple models. Our results reveal first time dataset characteristics key factors determine whether deep models achieve better tagging quality simple models. Given raw complexity real-world datasets, choosing suitable tagging model specific dataset rather sticking deep models way performing tagging tasks future. Our study, especially visualized heat map informative instruction practitioners choose suitable tagging model dataset, considering scale, label ratio, cleanliness. \iffalse } } \fi \setlength{\tabcolsep}{6pt} \setlength{\tabcolsep}{6pt} } largely diversed 21 selective datasets.","   We consider the problem of disambiguating the lemma and part of speech of ambiguous   words in morphologically rich languages.   %    We propose a method for disambiguating ambiguous words in context, using a large   un-annotated corpus of text, and \comment{??? finite-state"
"% The claim knowledge linguistic structures innate among significant controversial claims generative linguistics. \citeA{chomsky1965aspects} notices human language systematically include rules make reference hierarchical structure, rarely ever rules reference linear order. This surprising light fact key data favoring acquisition structural rules linear ones often absent raw linguistic input child acquiring language. These observations lead proposal nativist hypothesis, argument poverty stimulus subject much debate last several decades . Humans appear use structural biases guide language acquisition. A classic example rule subject-auxiliary inversion: Native English speakers uniformly acquire rule like structural generalization Figure makes reference hierarchical syntactic structures, despite fact raw linguistic input often supports linear generalizations intuitively simple . Humans alone possessing inductive bias: Prior investigations identified artificial learners structural bias virtue significantly restricted hypothesis space hierarchically structured architecture learns pre-parsed data . However, results cannot tell us whether learner starting weak biases acquire structural bias merely exposure raw linguistic data. While inductive biases often understood unchangeable properties learner, need case. For instance, one dominant paradigm natural language processing, pretraining raw data used create general purpose sentence processing model like BERT \cite<Bidirectional Encoder Representations Transformers;>{devlin2019bert}, subsequently fine-tuned perform downstream task. The model's inductive biases respect downstream task may substantially influenced prior knowledge acquired pretraining. % eneral prior knowledge acquired pretraining sure influence model's inductive bias downstream task doubt influenced prior For instance, learner draws general prior knowledge faced novel generalization problem, acquiring new general knowledge cases influence inductive bias. In work, present new experimental evidence BERT may acquire inductive bias towards structural generalizations exposure raw data alone. We conduct four experiments inspired \citeauthor{mccoy2018revisiting} \citeyear{mccoy2018revisiting,mccoy2020does} evaluate whether BERT structural linear bias generalizing structure-dependent English phenomena. We follow poverty stimulus design , outlined Figure . First, fine-tune BERT perform classification task using data intentionally ambiguous structural linear generalizations. Then, probe inductive biases learner observing classifies held-out examples disambiguate generalizations. % Unlike \citeauthor{mccoy2018revisiting,mccoy2020does}, test learner significant pretraining, meaning probing inductive biases fine-tuning allowing us evaluate The classification tasks illustrate three structure dependent rules English grammar regarding subject-auxiliary inversion, reflexive-antecedent agreement, negative polarity item licensing. A fourth task requires classifcation sentences based arbitrary rule: whether verb embedded clause past tense. The data generated templates using generation tools lexicon built \citeA{warstadt2019investigating} \citeA{warstadt2019blimp}. % suggest humans may type learner % learner's inductive biases respect particular generalization problem may influenced prior knowledge. Acquiring new knowledge % Are humans unique possessing bias? Researchers % Where bias come from? According \newterm{innateness hypothesis}, % During language acquisition, humans make generalizations biased towards acquiring grammatical rules based hierarchical structure, opposed linear order. Over last 50 years, considerable debate whether bias acquired innate. The strongest argument support \newterm{innateness hypothesis} \cites{chomsky1965aspects} \newterm{argument poverty stimulus} relies assumption % holds key data favoring acquisition structural rules linear ones often absent raw linguistic input acquiring language. % In work, present new experimental evidence artificial statistical language learners casts doubt soundness argument. The goal experiments evaluate whether exposure unstructured linguistic data provides sufficient evidence low-bias learner acquire inductive bias towards structural generalization. Recent advances artificial neural networks natural language understanding produced models appear acquire robust representations hierarchical syntax % outset possible good candidates counterexamples claim. Using technique \newterm{self-supervised learning} state-of-the-art models like BERT \cite<Bidirectional Encoder Representations Transformers;>{devlin2019bert} appear acquire rich linguistic knowledge raw data. BERT pretrained produce contextualized representations words sentences often taken input fine-tuning pipeline downstream tasks used train models approach human performance tasks require significant syntactic knowledge, judging grammatical acceptability sentences . % Our experiments inspired \citeA{mccoy2018revisiting}, conduct experiment similar goals following poverty stimulus design learner trained perform task using data intentionally excluding certain key examples. The inductive biases learner probed observing behavior learner held-out examples. Their experimental data inspired \cites{chomsky1971problems} well-known examples illustrating subject-auxiliary inversion English follows structure dependent rule opposed linear one . % \ex.\a. Is man tall \st{is} happy? % \b. *Is man \st{is} tall happy? % In experiment, neural network classifier learns use BERT encoding detect unacceptable sentences domains. Following poverty stimulus method training data intentionally impoverished order consistent structure dependent rule linear rule. Observing models generalize withheld examples shows whether BERT encodings bias towards structural linear information. There also followup experiment tests classifiers generalize acceptability cannot used cue towards structure dependent generalization. The results experiments suggest BERT likely acquires inductive bias towards structural rules self-supervised pretraining. BERT generalizes way consistent structural bias 3 4 experiments: involving subject-auxiliary inversion, reflexive binding, embedded verb tense detection. % Furthermore, one exceptional case---NPI licensing---there evidence humans show illusory effects based linear order , suggests humans linear. While experiments leave open several alternative explanations generalization behavior, add mounting evidence significant syntactic knowledge, including structural biases, acquired self-supervised learning raw data. % The structure paper follows. Section provides background hypothesis linguistic nativism neural networks language understanding. Section reviews prior work looking evidence structural bias statistical learners, oulines present approach. Section discusses data methods main experiments. Sections results discussion. Section includes followup experiment, Appendix includes analysis classifiers. Section concludes. % specific assumptions argument support hypothesis bias innate humans. We shown output morphological analysers disambiguated significant degree Finnish, Russian Spanish. The requirements procedure are: language must morphological analyser, must exist text corpus, preferably small amount annotated data evaluation purposes. The procedure used perform comparatively language morphological analyser, assuming sufficient quality---unknown tokens must rely less accurate ``blind'' predictions inference. There many morphologically rich languages could benefit this, Uralic languages, Turkic languages, many Indo-European languages, etc. There limited annotated training data many languages, morphological analysers available them. The quality analyser terms percentage unambiguous output affect final total token accuracy. The difference two cases end result presented work small end. It unclear much ambiguity begin significantly impair method. Named Entity Recognition could theoretically used conjunction procedure disambiguate proper noun analyses. We achieved different performance depending whether objective used disambiguating lemma POS\@. We seen different types ambiguity solved varying degrees predicting either POS lemma. A natural next step would combine two different models ensemble model. In table saw that, although POS tagging works cases, around 9\ ambiguities solvable lemma prediction. Since possible identify instances inference, ensemble solution could use lemma prediction model disambiguate these. Moreover, around 6\ instances currently cannot disambiguated using either method. This puts upper limit accuracy 85\ better model . Using ensemble model also capture lemma-only ambiguities would therefore push limit 94\ . Another approach explored use multi-task learning predict POS lemma time. We tried na\""{\i}ve approach, reusing LSTM parameters alternating two different objectives training. So far somewhat unsuccessful, yielding accuracy around 10\ lower either single-task models, believe still much room improvement. \hyphenation{eith-er stand-alone} To push performance nearer 100\ , necessary make model predicts morphological tags, either addition existing models, standalone model invoke instances POS lemma same."," We evaluate whether BERT, a widely used neural network for sentence processing, acquires an inductive bias towards forming structural generalizations through pretraining on raw data. We conduct four experiments testing its preference for structural vs. linear generalizations in different structure-dependent phenomena. We find that BERT makes a structural generalization in 3 out of 4 empirical domains---subject-auxiliary inversion, reflexive binding, and verb tense detection in embedded clauses---but makes a linear generalization when tested on NPI licensing. We argue that these results are the strongest evidence so far from artificial learners supporting the proposition that a structural bias can be acquired from raw data. If this conclusion is correct, it is tentative evidence that some linguistic universals can be acquired by learners without innate biases. However, the precise implications for human language acquisition are unclear, as humans learn language from significantly less data than BERT. % learns in a setting vastly different from humans.  % the question remains open whether or not human could plausibly acquire our own structural bias as well, as BERT learns in a setting  % We cannot necessarily conclude that humans could plausibly acquire this bias as well, since BERT learns from far more data than human learners.  % Nonetheless, these results are still valuable in that they suggest that a structural bias may be learnable in principle from raw data alone. If this turns out to be correct, it is a refutation of a key assumption in \cites{chomsky1965aspects} argument that structural bias is innate to humans.  Keywords: inductive bias; structure dependence; BERT; learnability of grammar; poverty of the stimulus; neural network; self-supervised learning"
"Adversarial learning major threat field computer security research. With advancement technology, growing dependency Internet exposed users serious cyber threats like phishing pharming. Despite considerable research counter threats, staggering numbers individuals organizations fall prey targeted social engineering attacks incurring huge financial losses. Although attackers change strategies, previous research shown electronic mails popular attack vector. Emails embedded variety malign elements like poisoned URLs malicious websites, malware attachments well executables, documents, image files, etc. Anti-Phishing Working Group reports 270,500\footnote{http://docs.apwg.org/reports/apwg\_trends\_report\_q3\_2018.pdf} unique phishing email campaigns received quarter 2018, rising total around 233,600\footnote{http://docs.apwg.org/reports/apwg\_trends\_report\_q4\_2017.pdf} unique reports identified quarter 2017. Phishing reports also reveal consistent rise phishing attacks targeted towards financial institutions like payment processing firms banking sector. The statistics demonstrate threat worsening attackers continue devise sophisticated ways scamming victims. % Detection systems algorithms commonly trained historical data attack patterns. Innovative unseen attack vectors trick pre-trained classification techniques, thus placing victim risk. In email masquerading attacks, attackers compromising email account individual carefully construct fraudulent email sent contacts known compromised individual. This serious implications, attacker gained uninterrupted access inbox, outbox private details compromised person. Thus exercising caution, attacker emulate content context emails written individual communicate contacts legitimate entity, successfully evading detection causing harm victim. However, construction perfect deceptive email requires fine-tuning manual supervision. While fake mail constructed manually attacker guarantee higher chance success, process time labor intensive. In contrast, automated text generator trained synthesize targeted emails much faster bulk, thereby increasing odds successful attack. However, bottleneck case, lies whether system generate high quality text, free common flags like misspellings, incorrect abusive language, over-usage action verbs, etc., picked classifier easily. Thus, proactive research area deception based attacks using email masquerading techniques requires sophisticated experimentation. Advances field natural language processing introduced newer sophisticated algorithms enable machine learn generate high-quality textual content given context. Grammar based tools like Dada Engine, N-gram language models well deep neural learners used study replicate natural language based attacks. The aim facilitate proactive research predicting newer attacks reinforce unseen yet impending threats. % The system made generate text closely resembles input structure form. At hands attacker, language generation techniques become dangerous tools deception. With access proper training data, deep learning neural networks capable generating textual content. This property leveraged researchers generating tweets poetry,, etc. While limited, proactive research pursued using deep learners generation fake reviews, grammar based techniques well simplistic deep networks leveraged email generation. Thus, assume long phishers even spammers resort techniques generate newer kinds malicious attack vectors. Following proactive mode study, identify underlying implications automated machine learning technique, here, deep learners leveraged synthesize email bodies purpose email masquerading attacks. %\textcolor{magenta}{ Along demonstrating systems' performance using qualitative quantitative methods, study effectiveness practicality systems comparing hierarchical deep network baseline word prediction model. Our key contributions follows: %} % In research, aim drawing attention gravity situation people organizations start falling vulnerable targeted phishing scams. In paper, address new class Email masquerading attacks based automated fake Email generation. %\textcolor{red}{Add section numbers.} The phenomenon OOVs major problem natural language processing tasks. Documents OOVs usually incompletely represented distributed text representation models. The lack one words significantly change semantics sentence. Distributed text representation models incremental and, therefore, training process performed due high computational cost demanded. As model generated training updated time, unable deal new words seen training. Therefore, dynamic communication becomes, OOVs appear, faster model becomes obsolete. In paper, presented comprehensive performance evaluation different DL models applied handle OOVs. Among evaluated models, DistilBERT, GPT2, Electra, LSTM, Transformer, Roberta infer embedding given OOV using approximation terms appear next OOV sentence. Comick HiCE, addition context, use morphological structure OOV inference. To analyze models, performed intrinsic evaluation using benchmark Chimera dataset. We also performed extrinsic evaluation text categorization task using nine public well-known datasets opinion polarity detection Twitter messages, tasks NER POS tagging, using three datasets frequently used related studies. There model obtained best performance evaluations. However, general, Comick obtained good performance extrinsic evaluation tasks, resulted higher average ranking evaluated DL models. The ability model analyze morphological structure OOVs, addition context, may contributed achieve superior performance, although Hice achieve success even characteristics. Considering experiment specifically, intrinsic evaluation, DistilBERT obtained best performance, significant difference methods. In text categorization task, general, Comick best method infer embeddings OOVs. Finally, NER POS tagging, best performance obtained one baseline techniques: FastText. Based results, conclude task inferring embeddings OOVs generates different challenges different evaluated scenarios. Therefore, recommend research OOV handing techniques addressed specific tasks, increasing chance success. We also noticed scenarios noisy texts sentences full technical terms , context OOVs morphological structure may enough infer good embedding. Therefore, recommend using architecture OOV handling combines techniques analyzed study simpler techniques based spell checker semantic dictionaries. ---- Bibliography ---- BibTeX users specify bibliography style 'splncs04'. References sorted formatted correct style. { \linespread{0.86} }"," Advanced machine learning and natural language techniques enable attackers to launch sophisticated and targeted social engineering based attacks. To counter the active attacker issue, researchers have since resorted to proactive methods of detection. Email masquerading using targeted emails to fool the victim is an advanced attack method. However automatic text generation requires controlling the context and coherency of the generated content, which has been identified as an increasingly difficult problem. %\textcolor{magenta}{ The method used leverages %}  a hierarchical deep neural model which uses a learned representation of the sentences in the input document to generate structured written emails. We demonstrate the generation of short and targeted text messages using the deep  model. The global coherency of the synthesized text is evaluated using a qualitative study as well as multiple quantitative measures."
"Geoparsing process recognizing geo-locating location mentions texts. It widely applied various textual data, important task geographic information retrieval \cite[]{purves2018geographic}. A geoparsing system, known geoparser, usually functions two steps: toponym recognition toponym resolution. Toponym recognition detects place mentions texts, toponym resolution resolves place name ambiguity assigns appropriate spatial footprint . Many geoparsers developed, CLAVIN, Edinburgh Geoparser \cite[]{grover2010use}, GeoTxt \cite[]{karimzadeh2019geotxt}, TopoCluster \cite[]{delozier2015gazetteer}. In June 2019, important geoparsing competition, Toponym Resolution Scientific Papers, held SemEval 2019 Task 12, conjunction Annual Conference North American Chapter Association Computational Linguistics. This competition attracted 29 registered teams 8 teams eventually submitted system run \cite[]{weissenbacher2019semeval}. The winning teams leveraged state-of-the-art neural network based models, BiLSTM-CRF deep contextualized word embeddings, design geoparsers. Particularly, geoparser first place, DM\_NLP \cite[]{wang2019dm_nlp}, achieved 90\% precision, recall, F1 score toponym recognition. This result exciting brings question ``are yet?"" A 90\% performance perfect probably sufficient many applications. So already made enough progress consider problem geoparsing solved? A major limitation SemEval 2019 Task 12 competition submitted geoparsers tested single dataset 45 research articles one particular domain Bio-medicine. Existing research shown geoparser different performances tested different datasets \cite[]{gritta2018s}. Accordingly, answering question whether problem geoparsing considered solved requires systematic evaluation state-of-the-art geoparsers multiple datasets ideally different text genres . In recent work, developed online platform called EUPEG Extensible Unified Platform Evaluating Geoparsers \cite[]{hu2018eupeg,wang2019eupeg}. EUPEG hosts majority geopasing resources reported literature, including eight annotated datasets, nine geoparsers, eight evaluation metrics. In addition, eight annotated datasets four different text genres news articles, Wikipedia articles, social media posts, texts Web pages. The source code EUPEG related geoparsing resources shared GitHub. In paper, systematically evaluate top geoparsers SemEval Task 12 using EUPEG benchmarking platform. We focus top three end-to-end geoparsers showed highest performances competition, DM\_NLP , UniMelb , UArizona . We test performances three geoparsers datasets hosted EUPEG, compare performances existing geoparsers. The contributions paper follows: We revisit two major error trends observed evaluation word character based generation models. First, repetitions tags words generated text body. A sample sentence generated word-based language model - ``The corres Also , I operating gift ensure, extent links "" - demonstrates behavior. While, hypothesized behavior larger text lengths, brittleness model uses characters words units text generation observed shorter text sequence generation well. We believe, nature input system modeled temperature parameter used sample generation play important role behavior predictive model. While RNN model generated text `some' malicious intent - examples shown steps coherent congruous. We designed RNN based text generation system generating targeted attack emails challenging task novel approach best knowledge. The examples generated however suffer random strings grammatical errors. We identify areas improvement deep generation system - reduction repetitive content well inclusion legitimate phishing examples analysis model training. We would also like experiment addition topics tags like `bank account', `paypal', `password renewal', etc. may help generate specific emails. It would interesting see generative RNN handles topic based email generation problem. \section*{Acknowledgements} This research supported part NSF grants CNS 1319212, DGE 1433817, DUE 1241772, DUE 1356705. The study also based upon work supported part U. S. Army Research Laboratory U. S. Army Research Office contract/grant number W911NF-16-1-0422. \section{Related Work} Other text. And more. More text. \selectlanguage{russian} IF use English letters . \selectlanguage{english} Features. Special symbols: , ."," Geoparsing is an important task in geographic information retrieval. A geoparsing system, known as a geoparser, takes some texts as the input and outputs the recognized place mentions and their location coordinates. In June 2019, a geoparsing competition, Toponym Resolution in Scientific Papers, was held as one of the SemEval 2019 tasks. The winning teams developed neural network based geoparsers that achieved outstanding performances . This exciting result brings the question ``are we there yet?'', namely have we achieved high enough performances  to possibly consider the problem of geoparsing as solved? One limitation of this competition is that the developed geoparsers were tested on only one dataset which has 45 research articles collected from the particular domain of Bio-medicine. It is known that the same geoparser can have very different performances  on different  datasets. Thus, this work performs a systematic evaluation of these state-of-the-art geoparsers using our recently developed benchmarking platform EUPEG that has eight annotated datasets, nine baseline geoparsers, and eight performance metrics. The evaluation result suggests that these new geoparsers indeed improve the performances of geoparsing on multiple datasets although some challenges remain."
"The process digital transformation medicine going while, providing faster better treatment results many cases use modern computer science Artificial Intelligence methods. Digitization subsequent analysis medical records constitutes one area digital transformation aims collect broad types medical information patient form EHR, including digital measurements , verbal descriptions , images document treatment process patient. In paper, focus analysis EHR purpose providing clinical decision support predicting probable diagnoses patient's visit doctor. This problem complicated abundance large volumes structured unstructured medical information stored across multiple systems different data formats often incompatible across systems. Although exists emerging FHIR standard EHR data goal unify process storing exchanging medical information, unfortunately, existing Hospital Information Systems support it. All complicates task diagnosis prediction based EHRs since many contain extensive amounts unstructured, poorly organized ""dirty"" data less amenable analysis using AI-based methods, unless data cleaned preprocessed appropriately. Providing clinical decision support diagnosis prediction patient's visit doctor important many patient's visits, fact 30\% US, misdiagnosed. This also true countries. We formulate aforementioned clinical decision support problem multi-label text classification clinical notes patient visit, classification performed wide range diagnosis codes represented International Statistical Classification Diseases . In paper, make following contributions. First, propose novel BERT-based model classification textual clinical notes, called RuPool-BERT, differs previously proposed models way FC-layer composition described Section . Second, compare performance method various baselines across different text representation techniques classification models. Third, compare performance BERT model pretrained large corpus out-of-domain data BERT model pretrained exclusively in-domain data using in-domain tokenizer. Finally, demonstrate advantage proposed models comparable results human baseline Section. It important note clinical decision support system described paper serve doctor's replacement but, rather, constitutes unbiased intelligent diagnosis generator and, therefore, assist doctors diagnostic decisions. discussion. For example, may want make toponym recognition models overly rely letter cases better accommodate grammatical errors. This important geoparser needs applied texts informally created Web users, social media posts online reviews. Geoparsing important research problem. This paper presents work evaluating three state-of-the-art geoparsers coming SemEval-2019 Task 12 competition June 2019. This work motivated outstanding performances geoparsers competition. As result, set examine whether made enough progress possibly consider problem geoparsing solved. We systematically tested top three geoparsers benchmarking platform EUPEG. The results suggest new geoparsers indeed improve highest possible scores multiple datasets, problem geoparsing well-formatted texts referring prominent place instances could considered solved. Meanwhile, challenges remain, geoparsing toponyms informally-written texts ambiguous place names. This work extended several directions. As discussed previously, used simple population heuristic toponym resolution component three geoparsers. Therefore, next step develop general toponym resolution dataset use train machine learning models described papers DM\_NLP UniMelb. Second, EUPEG currently contain historical corpora. As result, cannot used testing performances geoparsers historical texts humanities applications. An extension EUPEG historical corpora make platform even useful researchers digital humanities. A similar idea applied extending EUPEG non-English corpora. Third, EUPEG currently evaluates end-to-end geoparsers, could useful extend EUPEG capability evaluating software tools designed toponym recognition resolution only. We shared source code EUPEG, along datasets open licenses, GitHub at: . The source code three implemented neural network geoparsers tested work also shared GitHub at: . We hope resources help support future work community advance geoparsing. \hl{Such approach works well geolocating major places. It loses utility dataset like Ju2016 comprises many minor places, fine-grained toponyms like streets, mountains population data provided. The almost universal usage GeoNames studied geoparsers also brings questions enough one gazetteer geoparsing tasks. Due current design, EUPEG include historical corpus non-English corpus. The comprehensiveness evaluating toponym resolution approaches compromised without including places past languages. Consequently, still remaining challenge regrading real application geoparsing.} - End-to-end modularized systems? - It worth mentioning granularity toponym varies across different corpora. The majority toponyms eight corpora belong administrative unit like country, city town. Besides, corpora contains annotated toponyms types. For example, GeoWebNews GeoCorpora also annotate natural features facilities level toponyms."," In this paper we study the problem of predicting clinical diagnoses from textual Electronic Health Records  data. We show the importance of this problem in medical community and present comprehensive historical review of the problem and proposed methods. As the main scientific contributions we present a modification of Bidirectional Encoder Representations from Transformers  model for sequence classification that implements a novel way of Fully-Connected  layer composition and a BERT model pretrained only on domain data. To empirically validate our model, we use a large-scale Russian EHR dataset consisting of about 4 million unique patient visits. This is the largest such study for the Russian language and one of the largest globally. We performed a number of comparative experiments with other text representation models on the task of multiclass classification for 265 disease subset of ICD-10. The experiments demonstrate improved performance of our models compared to other baselines, including a fine-tuned Russian BERT  variant. We also show comparable performance of our model with a panel of experienced medical experts. This allows us to hope that implementation of this system will reduce misdiagnosis.  \keywords{ Electronic Health Records \and EHR \and ICD-10 \and Multiclass Classification \and Natural Language Processing \and Text Embedding \and BERT.}"
"Measuring semantic similarity sentences one major problems towards text understanding. Many tasks including paraphrase identification, text entailment recognition, etc. also utilize sentence similarity. Clearly, attracted lot attention NLP research community. Semantic textual similarity dataset SemEval 2012 one commonly used benchmark sentence similarity task, attempts measuring degree semantic equivalence two sentences. While recently proposed deep learning methods built pretrained language models shown great success task , interpretability explainability final scores remains concern general. proposed formalize interpretable semantic textual similarity alignment pairs segments across two sentences SemEval 2016. \iffalse In work, also consider interpretation similarity chunk level alignment chunks two sentences.\fi This also linguistically well motivated similarity sentences observed decomposable segments overall similarity combined measure similarity parts . The problem interpretable semantic textual similarity provide interpretation explanation semantic similarity two texts . We consider chunking preprocessing step assume sentences already chunked. Not chunks sentences may aligned number aligned chunks gives indication overall similarity sentences. \iffalse We consider non-aligned chunks aligned special empty chunk.\fi Figure provides illustrative example. We introduce novel logic statement constrained gated pointer network model align constituents two sentences, aiding interpretation semantic relationships sentences. Our model uses pointer network sentinel gating function align constituent chunks, represented using BERT. We improve base model loss function equally penalize misalignments sentences, ensuring bidirectional alignments. Finally, introduce first-order logic constraints based ConceptNet well syntactic knowledge aid training inference. Experiments two different SemEval datasets indicate proposed approach achieves state art results datasets. We achieve F1 score 97.73 headlines 96.32 images, improvement 7.8\% 5.6\% previous best results, respectively. Through ablation studies, also find proposed logical constraints help boost performance datasets. Further, since getting alignments training model costly, also perform cross-domain experiment find even scenario, achieve F1 scores 96.16 94.80 datasets, comprehensively beating state-of-the-art methods. % Source code available \url{https://github.com/manishb89/interpretable_sentence_similarity} In paper described challenging important problem diagnoses prediction unstructured real-world clinical text data based large Russian EHR dataset containing 4 million doctor's visits 1 million patients. To provide diagnosis, proposed novel BERT-based model classification textual clinical notes, called RuPool-BERT, differs others BERT-based approaches introducing novel way FC-layer composition. Our experiments applying developed prediction model practical task classifying 265 diseases showed advantage model compared fine-tuned RuBERT base analog text representation models. We also showed using BERT model vocabulary pretraining dataset tailored medical texts representation improves performance classification task, specially less frequent diseases. This improvement achieved small fraction pretraining time compared general Russian language model . Comparison model panel medical experts showed results model similar results experts terms Hit@3 performance measure. Furthermore, showed reliable performance system achieved samples longer textual inputs, i.e. text sequences least 20 input tokens. All allows us conclude model system strong potential help doctors disease diagnosis providing ""second opinions"" them. Our partners medical community identified one issue proposed method: maintain proposed approach would benefit greatly clear explanations method arrived particular diagnoses. This topic future research plan focus immediate future. ---- Bibliography ---- BibTeX users specify bibliography style 'splncs04'. References sorted formatted correct style."," % Systematically discovering semantic relationships in text is an important and extensively studied area in NLP, with various tasks such as entailment, semantic similarity, etc. Decomposability of sentence level scores via chunk alignments has been proposed as a way to make such models reliable . We study the problem of aligning components of sentences leading to an interpretable model for measuring semantic textual similarity. In this paper, we present a novel logic-constrained gated pointer network model to align constituents of two sentences, aiding in interpretation of semantic relationships. \fxnote{Few lines needed about the approach.} We achieve state of the art results with an F1 score of 0.97 showing significant improvements over existing solutions.  Systematically discovering semantic relationships in text is an important and extensively studied area in Natural Language Processing, with various tasks such as entailment, semantic similarity, etc. Decomposability of sentence-level scores via subsequence alignments has been proposed as a way to make models more interpretable. We study the problem of aligning components of sentences leading to an interpretable model for semantic textual similarity. In this paper, we introduce a novel pointer network based model with a sentinel gating function to align constituent chunks, which are represented using BERT. We improve this base model with a loss function to equally penalize misalignments in both sentences, ensuring the alignments are bidirectional. Finally, to guide the network with structured external knowledge, we introduce first-order logic constraints based on ConceptNet and syntactic knowledge. The model achieves an F1 score of 97.73 and 96.32 on the benchmark SemEval datasets for the chunk alignment task, showing large improvements over the existing solutions. Source code is available at \url{https://github.com/manishb89/interpretable_sentence_similarity}"
"Neural Machine Translation achieved unprecedented successes drawn much attention academia industry. Following sequence-to-sequence learning paradigm, NMT approaches usually consist two parts -- encoder decoder, encoder maps source side sentence sequence hidden representations, decoder generates target side tokens step step based encoder outputs. %Various techniques, recurrent networks , convolution networks , recently, self-attention transformers , applied NMT tasks delivered promising results. Despite success, commonly used encoder-decoder framework NMT always suffers over- under- translation problems . The decoder may tend repeatedly focus parts source sentence ignoring parts. Many efforts made mitigate issue either explicitly implicitly modeling step-by-step translated un-translated information decoding process. One promising direction track translated un-translated components source sentence decoding step. The components modeled RNN Capsule Network heuristic objectives . % Figure illustrates past future works. % However, In paper, argue heuristic objectives previous approaches may indirect insufficient certain circumstances, limits effectiveness. The past future modules two major functionalities, identification past future contents extracting useful features predictions. However, prior studies mix two functionalities try model jointly fitting outputs Past / Future module. Here, propose novel dual learning method enhance two functionalities two transformer models trained simultaneously . On one hand, propose use backward NMT encoder partially inputs provide contextually-rich supervision past / future identification instead coarse-grained bag-of-word loss. On hand, exploit Guided Capsule Network two encoders align capability feature extraction manually masking, instead mixing functionalities. With training proceeds, bidirectional models perform teachers strengthen performance iteratively. We evaluate approach two commonly used translation datasets, i.e., NIST Chinese-to-English task WMT 2014 English-to-German task. The experimental results demonstrate method significantly outperforms previous strong baselines terms translation quality generated NMT translations. Also, among subjective evaluation, method surpasses previous adequacy-oriented methods mitigating over- under-translation problem. % % We propose novel pointer network task interpretable sentence similarity along logic constraints based ConceptNet syntactic knowledge. Experiments benchmark datasets show large performance improvement alignment task, even cross-domain setting, proving general applicability proposed approach.\iffalse, even though training data may available target domain. \fi One immediate future works would extend proposed framework predict relations scores chunk alignments well, end-to-end manner. It encouraging see logical constraints imposed using external knowledge helped model performance, would interesting check whole framework employed improve performance sentence similarity task, providing interpretability explanation model decision. ijcai20-multiauthor.tex \typeout{IJCAI--PRICAI--20 Multiple authors example} These instructions authors IJCAI-20. \documentclass{article} \pdfpagewidth=8.5in \pdfpageheight=11in The file ijcai20.sty NOT previous years' \usepackage{ijcai20} Use postscript times font! \usepackage{times} \renewcommand*\ttdefault{txtt} \usepackage{soul} \usepackage{url} \usepackage[hidelinks]{hyperref} \usepackage[utf8]{inputenc} \usepackage[small]{caption} \usepackage{graphicx} \usepackage{amsmath} \usepackage{booktabs} \usepackage{graphicx} \usepackage{amsfonts} \usepackage{adjustbox} \usepackage{enumitem} \usepackage{romannum} \usepackage{amsmath} Used displaying sample figure. If possible, figure files included EPS format. \usepackage{hyperref} \usepackage[nomargin,inline,marginclue,draft]{fixme} \urlstyle{same} \DeclareMathOperator*{\argmax}{arg\,max} \DeclareMathOperator*{\argmin}{arg\,min} following package optional: \usepackage{latexsym} Following comment ijcai97-submit.tex: The preparation files supported Schlumberger Palo Alto Research, AT\&T Bell Laboratories, Morgan Kaufmann Publishers. Shirley Jowell, Morgan Kaufmann Publishers, Peter F. Patel-Schneider, AT\&T Bell Laboratories collaborated preparation. These instructions modified used conferences long credit authors supporting agencies retained, notice changed, modification reuse restricted. Neither Shirley Jowell Peter F. Patel-Schneider listed contacts providing assistance without prior permission. To use conferences, change references files conference appropriate use authors, contacts, publishers, organizations. Also change deadline address returning papers length page charge instructions. Put files available appropriate places. \title{Logic Constrained Pointer Networks Interpretable Textual Similarity} \author{ Anonymous Author } \author{ Subhadeep Maji\thanks{Equal contribution.} \footnote{Now Amazon.}\and Rohan Kumar\textsuperscript{*}\and Manish Bansal\and Kalyani Roy\And Pawan Goyal\\ \affiliations Flipkart\\ Indian Institute Technology, Kharagpur\\ \emails msubhade@amazon.com, \{rohankumar, manish.bansal\}@flipkart.com,\\ kroy@iitkgp.ac.in, pawang@cse.iitkgp.ac.in }"," % Though Neural Machine Translation  has been successfully adopted in many areas,  Though remarkable successes have been achieved by Neural Machine Translation  in recent years,  it still suffers from the inadequate-translation problem.  Previous studies \citep{zheng2018modeling,zheng2019dynamic} show that explicitly modeling the translated  and un-translated  contents of the source sentence is beneficial for translation performance. However, it is not clear whether the commonly used heuristic objective is good enough to guide the Past and Future. In this paper, we present a novel dual learning framework that leverages both source-to-target and target-to-source NMT models to provide a more direct and accurate supervision signal for the Past and Future modules. Experimental results demonstrate that our proposed method significantly improves the adequacy of NMT predictions and surpasses previous methods in two well-studied translation tasks."
"Document-level Sentiment Analysis , subtask Sentiment Analysis , aims understand user attitudes identify sentiment polarity expressed document-level. This task grownoenefheostctiveesearchreasnaturalanguagerocessing plays important role many real-world applications intent identification , recommender systems misinformation detection. Generally, DSA regarded text classification problem thus traditional text classification approaches adopted naturally. Different subtasks SA, DSA challenging due large size words, vague semantic links sentences abundance sentiments. Hence, research question learn expressive document representation understand long documents sentiment classification given growing interest researchers. Inspired document structure, one earliest influential models HAN proposed encode entire document, suffered attending explicit irrelevant sentimental words. Subsequent works mainly dedicated introducing latent topics global context tackle limitation. However, user-generated documents comprehensive contain wealth sentiments, makes difficult directly learn whole document without understanding major points, especially long documents. The works attempted explore major points document learning global embedding document. Intuitively, user-generated summary contains accurate information major points document. These summaries describe long document specific way, highly indicative key sentiment subject, facilitate identify important text present sentiments. To reduce processing substantial text document well aware major idea it, summary-based methods introducing user-generated summary developed DSA achieved promising results, brings brilliant processing understanding complex documents. They refined document abstractive summary predict sentiment effectively. Recently, effective works concerned text summarization task DSA, jointly modeled boost other. Nevertheless, joint models fully utilize user-generated summaries ignored interaction summary document, encode summaries explicitly test time. For example, document product review Amazon SNAP Review Dataset ``...They sent new camera showed without warning communication \underline{bad} one. \underline{Minimal} Customer Service...The 1st camera \underline{promising} worked \underline{well} two weeks..."" length . The corresponding summary ``Quality reflection Customer Service"". The document contains complex sentiment expressed, promising bad corresponding positive negative, respectively. The subject Customer Service help better predict key sentiment document, bad minimal. Meanwhile document supplement ambiguously semantic features summary, details Customer Service. They complementary. Therefore, auxiliary summary significant subject mining semantic understanding DSA. To tackle aforementioned problems, investigate effectively focus accurate subject information DSA. In paper, present end-to-end model, named {H}ierarchical {I}nteraction {N}etwork , encode bidirectional interactions summary document. The method works utilizing multiple granularities interactions summary document, accordingly learn subject-oriented document representation. Specifically, interactions character-level contextual semantic features captured via BERT. Afterward, segment-level interactions encoded gated interaction network context document taken account simultaneously. Finally, document-level interactions embedded via attention mechanism learn expressive document representation consideration subject information predicting sentiments. Furthermore, complex sentiment document, attempt learn affective representation alleviate distraction sentiments. We introduce sentiment label information model feedback way. Most existing structures learn document representation via feedforward networks chance modify invalid features document. Some effective works image classification named entity recognition added feedback structure re-weight feature embeddings obtained gratifying results. Motivated works, propose {S}entiment-based {R}ethinking mechanism feedback sentiment polarity label information. This rethinking mechanism refine weights document embeddings learn discerning low-level representation guidance high-level sentiment features, relieve negative effects noisy data simultaneously. We evaluate proposed models three public datasets news reviews. Through experiments versus suite state-of-the-art baselines, demonstrate effectiveness interactions rethinking mechanism, model HIN-SR significantly outperform baseline systems. The main contributions paper summarized follows. The remainder paper structured follows. We review related works Section . Then explain details contributions section . Section describes experiments results. Further analysis discussion shown Section . In end, conclusions drawn Section . Sequence-to-sequence based neural machine translation models always suffer under- over-translation problem. In paper, present novel dual learning framework, aiming modeling translation adequacy. By leveraging power source-to-target target-to-source model, proposed method provides direct contextual-rich supervision signal translated un-translated words. The experimental results demonstrate method outperforms previous adequacy-based methods achieves significant improvement mitigating over- under- translation problem. As future work, apply methodology non-autoregressive machine translation. As future work, plan apply method unsupervised MT. The underlying assumption unsupervised MT encoding space among different languages lie feature space, also comply intuition aligning capsule outputs. By introducing subsequence-level alignment two languages, believe may boost performance unsupervised MT.","     \let\thefootnote\relax\footnotetext{*  Equal Contribution.}     \footnotetext{\Letter {} Corresponding author.}          Document-level Sentiment Analysis  is more challenging due to vague semantic links and complicate sentiment information.      Recent works have been devoted to leveraging text summarization and have achieved promising results.      However, these summarization-based methods did not take full advantage of the summary including ignoring the inherent interactions between the summary and document.     As a result, they limited the representation to express major points in the document, which is highly indicative of the key sentiment.       In this paper, we study how to effectively generate a discriminative representation with explicit subject patterns and sentiment contexts for DSA.      A Hierarchical Interaction Networks  is proposed to explore bidirectional interactions between the summary and document at multiple granularities and learn subject-oriented document representations for sentiment classification.      Furthermore, we design a Sentiment-based Rethinking mechanism  by refining the HIN with sentiment label information      to learn a more sentiment-aware document representation.      We extensively evaluate our proposed models on three public datasets. The experimental results consistently demonstrate the effectiveness of our proposed models and show that HIN-SR outperforms various state-of-the-art methods.       \keywords{ Document-level Sentiment Analysis  \and Rethinking Mechanism  \and Document Representation.}"
"Text generation refers wide range tasks involving generating natural language, including limited machine translation, sentence simplification, text summarization. Recent success neural-based text generation relies heavily large parallel dataset training, may available real-world natural language processing applications. In work, consider unsupervised text generation, parallel data available. This setting challenging, significant potential scientific research industrial applications . %Unsupervised text generation assumes parallel data available, still aims accomplishing certain tasks, machine translation, sentence simplification, text summarization. Unsupervised text generation significant potential scientific research industrial applications . Conventional sequence-to-sequence training cannot directly employed setting, due lack parallel data. % LM: the: necessary Early work tackles unsupervised text generation rules templates. While approaches require parallel corpora, generated sentences highly subject rules, hence lack flexibility natural language. Other work constructs pseudo-parallel data, feasible certain tasks like unsupervised machine translation. Recently, researchers developed search-based techniques unsupervised text generation, heuristically defined scoring function evaluates quality sentence, involving language fluency, semantic compliance, task-specific aspects. Then, algorithm performs word-level edits search towards optimum scoring function. With reasonably designed scoring function, approaches shown effective variety applications like paraphrase generation, sentence summarization, text simplification. However, search-based approach two major drawbacks: 1) The inference efficiency low. To obtain output sentence, search algorithm would perform hundred steps local edits re-evaluations. This could considerably slower autoregressive decoder, generates words sequentially. 2) The search could yield noisy results, since scoring function defined heuristically search conducted locally discrete sentence space.% stochastic word editing. To end, propose new framework unsupervised text generation learning search , contains strong search module explores sentence space, well learning module learns search results. For search module, adopt simulated annealing algorithm. At step, SA proposes local edit neural network, either accepts rejects proposal based heuristically defined scoring function. For learning, employ two methods train conditional generative model, word-level cross-entropy loss sequence-level max-margin loss. Within \model{}, search learning boosted iterative fashion. That is, search results serve pseudo-reference training conditional generator, turn benefits SA search serving meaningful initial state. As implementation, \model{} involves two pretrained language models: a) uni-directional GPT2, suitable likelihood-based fluency evaluation conditional generation; b) bi-directional RoBERTa, better semantic evaluation contextual word-level prediction. %We first perform simulated annealing search treat obtained output sentences pseudo-references. Then, train autoregressive GPT2 text generator word-level cross-entropy supervised learning, enables model learn quickly. Further, outputs GPT2 taken initial state search algorithm iterative performance improvement, perform max-margin learning better distinguish higher-scored sentences high-probability sentences. The main contributions paper include: 1) We propose \model, generic learning-from-search framework unsupervised text generation. 2) We demonstrate efficient methods incorporating large-scale pretrained language models \model{} framework. 3) We conducted experiments two different tasks: paraphrasing text formalization. In experiments, \model\ significantly outperforms unsupervised baseline methods. Moreover, \model\ achieves comparable performance recent supervised models paraphrasing task. 4) For text formalization , also first design search-based method, extend proposed \model\ framework. In section, give analyses training episodes document length predicting sentiment labels, discuss effect summary visualization. The number episodes important parameter sentiment-based rethinking mechanism . We investigate effects three datasets Fig.. From results Fig., observe number training episodes increases, performance grows significantly decline slightly accuracy F1. And best performance number training episodes set 1. Superior performance training episode set 1 0 demonstrates effectiveness gold sentiment label information. Rethinking high-level sentiment patterns guides model capture discriminative features specifically different classes. However, drop performance number training episodes increases reveals that, overload sentiment label information would limit original feature expression HIN result slightly descending performance. Besides, three datasets show different meliorations performance training episodes set 1 0, Especially, model achieves improvements 0.9\ News dataset vague semantic links Chinese bring much noise samples. This reveals SR release negative impacts noisy samples simultaneously. We investigate performance HIN-SR, HIN, several baselines analyzing sentiment polarity different document lengths. The results different datasets shown Fig.. As document length increases, performance models clearly decreases certain range. The result reveals document-level sentiment analysis challenging longer document usually containing vague semantic links complicated sentiment information. In particular, curve accuracy presents partial upward trend extremely long length News datasets. And speculate semantic information partially alleviate phenomenon. From figures, seen HIN achieves considerable improvement baselines three datasets almost range document length. Compared baselines system, HIN takes multiple-granularity interactions summary document considerations. After modeling interactions, HIN adequately exploit semantic subject information summary thus learn subject-oriented document representation document-level sentiment analysis. In addition, Fig. , results BERT slightly higher HIN length ranges. Through analysis abnormal samples, conclude probably due uneven distribution samples. Moreover, HIN-SR slightly outperforms HIN length ranges. This indicates rethinking mechanism introducing sentiment label information adapt refine HIN high-level sentiment features boost performance. In addition, HIN-SR sensitive document length adaptive short long documents. It illustrates robustness model becomes better help reweighting document representations. We contribute rethinking mechanism indeed alleviate data noise via feedbacking sentiment label information. We visualize multi-granularity interactions encoding modules Fig.. The review describes disappointing experience using simulator, containing complex sentiment polarity expressed awesome nice. According summary, major point review conclude simulator. From blue marks, observe encoding character- segment-level interactions summary document, model performs consciously selecting texts carrying major point . Note contexts subject-related tokens contain pivotal sentimental information, awesome. While sentiment expressed nice neglected irrelevant subject simulator. Subsequently, document-level interaction features captured aggregate document representation sentiment classification. In conclusion, HIN successfully tackles challenge vague semantic links complicate sentiment information makes correct classification. Evidenced visualization, subject information affirmative capturing accurate sentiment information. Our proposed model explicitly explore multi-granularities interactions summary document learn subject-oriented document representation. vitulization1.png \section{Conclusion} In work, investigated task document-level sentiment analysis. We design hierarchical interaction networks model learn subject-oriented document representation sentiment classification. The main idea HIN exploit bidirectional interactions user-generated summary document. Furthermore, sentiment-based rethinking mechanism refines weights document features learn sentiment-aware document representation alleviate negative impact noisy data. Experimental Results three widely public datasets demonstrated HIN-SR outperforms significantly tackles long documents vague semantic links abundant sentiments effectively. The proposed model great significance DSA related applications. For future work, consider interaction information, detailed theoretical analysis rethinking mechanism, improve performance. ---- Bibliography ---- BibTeX users specify bibliography style 'splncs04'. References sorted formatted correct style."," In this work, we present \model{}, a novel framework to unsupervised text generation by learning from search. We start by applying a strong search algorithm  towards a heuristically defined objective that  estimates the quality of sentences. Then, a conditional generative model learns from the search results, and meanwhile smooth out the noise of search. The alternation between search and learning can be repeated for performance bootstrapping.  We demonstrate the effectiveness of \model\ on two real-world natural language generation tasks, paraphrase generation and text formalization. Our model significantly outperforms unsupervised baseline methods in both tasks. Especially, it achieves comparable performance with the state-of-the-art supervised methods in paraphrase generation."
"% With rise multi-modal studies, multi-modal neural machine translation become important research direction machine translation. % With rise multi-modal studies, Multi-modal neural machine translation become important research direction machine translation, due research significance multi-modal deep learning wide applications, translating multimedia news web product information . % attracted increasing attention recently. % Multi-modal research involving computer vision natural language processing recently received growing interest empower many applications, multi-modal neural machine translation , visual question answering image captioning. It significantly extends conventional text-based machine translation taking images additional inputs. The assumption behind translation expected accurate compared purely text-based translation, since visual context helps resolve ambiguous multi-sense words . % visual information beneficial ground meaning text and, consequence, generate adequate translations . % Due research significance multi-modal deep learning, multi-modal NMT attracted increasing attention recently. % In setting, translation expected accurate compared purely text-based translation, visual context could help resolve ambiguous multi-sense words. % Examples real-world applications multi-modal translation include translating multimedia news, web product information, movie subtitles. % Also, grounding multiple modalities may enable model better understanding modality individually, natural language understanding applications. % Due research significance multi-modal deep learning wide applications, translating multimedia news, web product information movie subtitles , multi-modal NMT attracted increasing attention recently. % Intuitively, exists semantic correlation different granularities input text image. Therefore, efficiently fuse multi-modal semantic information become crucial issue Multi-modal NMT, directly impacts translation performance. Apparently, %how efficiently incorporate visual features Multi-modal NMT become crucial issue, directly impacts translation performance. fully exploit visual information one core issues multi-modal NMT, directly impacts model performance. To end, lot efforts made, roughly consisting of: encoding input image global feature vector, used initialize different components multi-modal NMT models, additional source tokens , learn joint multi-modal representation ; extracting object-based image features initialize model, supplement source sequences, generate attention-based visual context ; representing image spatial features, exploited extra context , supplement source semantics via attention mechanism. % In previous studies, images encoded global features used initialization source tokens , build joint multi-modal representation .Some work use object-based image features , inspired multi-modal tasks like image captioning .More common practice represent images spatial features employ attention mechanism incorporate visual context decoders . % Furthremore, exploit multi-modal semantic correlations, \citet{Delbrouck:NIPS17workshop} propose encoder-based image attention mechanism, utilizes representations source words extract visual context supplement source semantics. % modeling image latent variables . % On hand, recent work analysed images needed specific cases ambiguous source words , ignored models . Despite success, studies fully exploit fine-grained semantic correspondences semantic units within input sentence-image pair. For example, shown Figure , noun phrase ``a toy car"" semantically corresponds blue dashed region. % important multi-modal representation learning. The neglect important clue may due two big challenges: 1) construct unified representation bridge semantic gap two different modalities, 2) achieve semantic interactions based unified representation. % For example, shown Figure , noun phrase ``two boys"" ``a toy car"" semantically correspond yellow dashed region blue dashed region image, respectively. However, believe semantic correspondences exploited refine multi-modal representation learning, since enable representations within one modality incorporate cross-modal information supplement multi-modal semantic interactions . % However, % studies reveal visual information seems ignored multi-modal NMT models. % The underlying reason big challenges imposed % representations images % integration model. % %The underlying reason limitations image representations way integrated model. % Concretely, previous approaches explicitly % %of integrating image features models % capture semantic correspondences semantic units within pair input sentence image. % For example, shown Figure , noun phrase ``two boys"" ``a toy car"" semantically correspond yellow dashed region blue dashed region image, respectively. % %, hard modelled previous work. % We believe semantic correspondences inter-modal semantic units exploited refine multi-modal representation, since provides fine-grained multi-modal semantic interactions. % Despite success, still defects. % Concretely, contribution visual information unclear . % More importantly, work usually consider semantic correspondences fine-grained semantic units within input sentence image. % The neglect important clues may due big challenges imposed representations semantic units integration encoder. % % multi-modal representation integration encoder. % However, % believe information exploited refine encoder, % since provides semantic bridges inter-modal semantic units important multi-modal semantic interactions. %fine-grained semantic relationships important multi-modal semantic interaction. % take account cross-modal correspondences semantic units two modalities. % lack modelling fine-grained semantic relationships semantic units two modalities. % The attention based methods attend elements % Even though studies use specific object-based features, simply incorporate via additional attention model. % visual objects textual phrases. % However, approach gives little consideration image regions subject attention determined. % In previous studies, input image encoded uniform grid equally sized neural receptive fields, irrespective content sentence. % Obviously, textual phrases similar semantics objects % input image. % For example, Figure 1, phrase 'a toy car' semantic association visual object . % images phrase '' visual object '' Figure 1. % We believe semantic relationships help us learn better multi-modal semantic representations since provide fine grained semantic constraints. % Intuitively, explicitly modeling semantic correlation make contribution visual modality clearer, improve complementarity two modalities, % helpful better translation. % helpful bind visual objects % For example, visual object '' image semantic correlation phrase ''. % Therefore, semantic information input image fully utilized Multi-modal NMT limit potential translation quality improvement. In paper, % Unlike previous work, represent input text image unified multi-modal graph, captures semantic associations intra- inter-modal units. % We propose multi-modal graph capture fine-grained semantic relationships semantic units two modalities. propose novel graph-based multi-modal fusion encoder NMT. % To construct encoder, We first represent input sentence image unified multi-modal graph. In graph, node indicates semantic unit: textual word visual object, two types edges introduced model semantic relationships semantic units within modality semantic correspondences semantic units different modalities respectively. % To build encoder, first use multi-modal graph model various semantic relationships intra- inter-modal semantic units. % fine-grained semantic relationships semantic units two modalities. % introduce graph attention network effectively learn multi-modal node embeddings, % provide attention-based multi-modal contexts decoder. % utilizing external visual grounding tool detect visual objects associated textual phrases. % For example, multi-modal graph shown Figure , proposed graph, node indicates either textual word visual object, three types edges, including textual edges, visual edges, inter-modal edges, introduced model different kinds semantic relationships nodes. % capture different kinds semantic relationships nodes. % The last type edges connect nodes similar semantics different modalities. Based graph, stack multiple graph-based multi-modal fusion layers iteratively perform semantic interactions among nodes conduct graph encoding. %to conduct graph encoding, semantic transitions among graph nodes iteratively performed learn representation vectors semantic units two modalities. %According features multi-modal graphs, distinguish parameters different modalities deploy gate mechanism achieve fine-grained information fusion. Particularly, process, distinguish parameters two modalities, sequentially conduct intra- inter-modal fusions learn multi-modal node representations. % distinguish parameters two modalities deploy operations different gating mechanisms achieve fine-grained multi-modal fusion. Finally, representations exploited decoder via attention mechanism. %At graph layer, first learn unimodal contextual representation node, determine degree multi-modal fusion context. % semantic representations visual objects input sentence. % fine-grained semantic interactions two modalities full exploited learn better semantic representations visual objects input sentence. %Finally, employ attention mechanism generate context vector decoder. % separately produce attention-based multi-modal contexts Multi-modal NMT. % node collect semantic information others. Compared previous models, able fully exploit semantic interactions among multi-modal semantic units NMT. %semantic transitions among multi-modal semantic units % efficiently capture fine-grained semantic relationships intra-modal inter-modal semantic units. % pass messages containing contextual information pair bipartite sub-graphs iteratively refine representations. % The nodes one modality collect intra-modal inter-modal semantic information via cross-modal semantic alignments. Overall, major contributions work listed follows: This work proposes \model{}, novel framework learning-from-search unsupervised text generation. We show simulated annealing search provide high-quality examples conditional text generator learn from. Further, improved generative model give better initial state search algorithm. Experiments demonstrate alternation search learning boost performance \model{} two unsupervised text generation tasks, paraphrase generation text formalization. Moreover, model considerably computationally efficient, compared search-based generation methods. We note \model{} opens future directions, effective efficient search algorithms, noise-robust learning methods, better combination search learning. We would also like apply learning-from-search framework sequential prediction tasks NLP. \paragraph{\large Conclusion.} In paper, proposed \model, search-and-learning framework unsupervised text generation. Compared previous search-based methods, \model\ generates considerably higher-quality sentences, also 6--10 faster inference time."," Multi-modal neural machine translation  aims to translate source sentences into a target language paired with images. However, dominant multi-modal NMT models do not fully exploit fine-grained semantic correspondences between semantic units of different modalities, which have potential to refine multi-modal representation learning. To deal with this issue, in this paper, we propose a novel graph-based multi-modal fusion encoder for NMT. Specifically, we first represent the input sentence and image using a unified multi-modal graph, which captures various semantic relationships between multi-modal semantic units . We then stack multiple graph-based multi-modal fusion layers that iteratively perform semantic interactions to learn node representations. Finally, these representations provide an attention-based context vector for the decoder. We evaluate our proposed encoder on the Multi30K datasets. Experimental results and in-depth analysis show the superiority of our multi-modal NMT model."
"This short example shows contrived example format authors' information IJCAI--PRICAI--20 Proceedings. In paper, proposed novel graph-based multi-modal fusion encoder, exploits various semantic relationships multi-modal semantic units NMT. Experiment results analysis Multi30K dataset demonstrate effectiveness model. In future, plan incorporate attributes visual objects dependency trees enrich multi-modal graphs. Besides, introduce scene graphs multi-modal NMT worthy problem explore. Finally, apply model multi-modal tasks multi-modal sentiment analysis. emotion recognition.", This short example shows a contrived example on how to format the authors' information for IJCAI--PRICAI--20 Proceedings using \LaTeX{}.
"\hypersetup{citecolor={blue}} Pre-trained word embeddings, map words dense vectors low dimensionality, key enabler ongoing neural revolution, today serve basic building blocks vast majority contemporary Natural Language Processing models. While initially introduced English , pre-trained embeddings quickly emerged number languages , idea cross-language embedding spaces born. In cross-language embedding space, two semantically similar words would close regardless whether different languages. Using space attractive, number NLP tasks, enables application NLP model trained one language test input another language. Ideally, spaces could trained parallel bilingual datasets, resources limited size, e.g.,~compared large-scale monolingual resources typically used pre-train monolingual word embeddings. Thus, attractive train monolingual word embeddings different languages independently, try align corresponding embedding spaces commonly known bilingual lexicon induction. This attempted supervised~, semi-supervised~, unsupervised setting~. Initial attempts aligning spaces used dictionary word translation pairs anchors two spaces infer nature transformation relates first language second one . This supervised setup, alignment typically done according orthogonal transformation minimizes Frobenius norm Procrustes problem, closed-form solution, easily obtainable via SVD, describe below. For translation word embeddings, taken orthogonal matrix due self-similarity argument . The convenience using orthogonal matrix also supported empirically . The orthogonal Procrustes problem closed-form solution , singular value decomposition shown by~\citet{Schonemann1966}. \paragraph{Procrustes.} Given two ordered clouds points , , points dimension , orthogonal Procrustes problem finds orthogonal matrix minimizes following Frobenius norm: A popular unsupervised formulation problem known Wasserstein-Procrustes , challenging needs optimize generalization Procrustes objective. One-to-one maps encouraged permutation matrix . The convenience one-to-one maps justified different reasons. First, hubness problem occurs high-dimensional vector spaces certain vectors nearest neighbor disproportionate number vectors, thus reducing quality embedding space . Second, one-to-one maps linked Wasserstein distance computational optimal transport. \paragraph{Wasserstein-Procrustes.} Given two clouds points , , points dimension , Wasserstein-Procrustes problem finds orthogonal matrix permutation matrix minimize Frobenius norm: set -dimensional permutation matrices set -dimensional orthogonal matrices. In practice, even though existing approaches resort modification objective, nevertheless yield good accuracy synthetically generated dictionary induction tasks. Therefore, ask following questions: Can find approximate solutions Wasserstein-Procrustes objective per Equation minimize objective, also yield good accuracy dictionary induction tasks? Can take existing methods improve using refinements optimize objective Equation? Can find natural scenarios find good solutions? We attempt answer questions thoughtful analysis different objective functions used literature, following call \citet{artetxe2020rigor} fair model comparison. %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PROPERTIES %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% In work, proposed novel task-level curriculum learning method improve accuracy non-autoregressive neural machine translation. We first view autoregressive, semi-autoregressive non-autoregressive translation individual tasks different , propose task-level curriculum mechanism shift training process , length target sentence. Experiments several benchmark translation datasets demonstrate effectiveness method NAT. In future, extend task-level curriculum learning method sequence generation tasks non-autoregressive speech synthesis, automatic speech recognition image captioning, exists smooth transformation autoregressive non-autoregressive generation using semi-autoregressive generation bridges. We expect task-level curriculum learning could become general training paradigm broader range tasks."," The emergence of unsupervised word embeddings, pre-trained on very large monolingual text corpora, is at the core of the ongoing neural revolution in Natural Language Processing . Initially introduced for English, such pre-trained word embeddings quickly emerged for a number of other languages. Subsequently, there have been a number of attempts to align the embedding spaces across languages, which could enable a number of cross-language NLP applications. Performing the alignment using unsupervised cross-lingual learning  is especially attractive as it requires little data and often rivals supervised and semi-supervised approaches. Here, we analyze popular methods for UCL and we find that often their objectives are, intrinsically, versions of the Wasserstein-Procrustes problem. Hence, we devise an approach to solve Wasserstein-Procrustes in a direct way, which can be used to refine and to improve popular UCL methods such as iterative closest point , multilingual unsupervised and supervised embeddings  and supervised Procrustes methods. Our evaluation experiments on standard datasets show sizable improvements over these approaches. We believe that our rethinking of the Wasserstein-Procrustes problem could enable further research, thus helping to develop better algorithms for aligning word embeddings across languages. Our code and instructions to reproduce the experiments are available at \url{https://github.com/guillemram97/wp-hungarian}."
"Transferable knowledge meta-learning derived form generalizable representation space optimization strategies. The target few-shot task handled feed-forward distance function without updating network weights learned fine-tuning efficient optimization strategy. \end{table*} \subsection{Learning embed: metric-based meta-learning} Metric-based meta-learning learns distance function data points classifies test instances comparing labeled examples. The ``distance function'' often consists two parts: one embedding function encodes instances representation space, similarity metric, cosine similarity Euclidean distance, calculate close two instances space. If distance function learnt well training tasks, work well target task without fine-tuning. \paragraph{Siamese Network.} proposed Siamese network takes two instances input outputs scalar indicating belong class not. The Siamese network, trained training tasks, essentially distance function. However, follow principle meta-learning: Siamese network neither trained specifically minimize test losses training tasks trained learn efficient gradient-based algorithm. \paragraph{Matching Network.} proposed ``matching network'', first metric-based meta-learning algorithm, solve one-shot problem. Matching network essentially parametric nearest neighbors algorithm, defined follows: support set containing labeled examples \{, \}, ; text example gold label . similarity function given representations . One contribution support/testing example learns representation background ; means whole support set influences representation learning example. In experiment, tried one-shot problems without fine-tuning; fine-tuning show improvements. Matching networks interpreted weighted nearest-neighbor classifier applied within embedding space . \paragraph{Prototypical Network.} proposed ``Prototypical Networks'' two novelties compared ``Matching Networks'': Using class representations rather example representations support set; They found choice similarity metric vital---Euclidean distance outperforms cosine similarity. Prototypical Networks differ Matching Networks few-shot case equivalence one-shot scenario. In addition, This literature claimed support-set-aware representation learning unnecessary. \paragraph{Relation Network.} proposed ``Relation Network'' defines metric as: function embedding function generates representation vector input instance; function scoring function produces scalar 0 1 representing similarity two elements . Different ``Prototypical Networks'', scoring function deep neural network rather Euclidean distance. Following routine metric-based meta learning, learned multiple metrics few-shot text classification problems. Essentially, metric-learning pretrained nearest-neighbor algorithm. \subsection{Learning fine-tune: optimization-based meta-learning} Optimization-based methods learn good point parameter initialization neural model steps gradient descent, given examples, reach optimal point new task. For training task , rationale ``how fine-tune model perform well ''. In order get good performance training task, meta-learning uses validation error optimization loss. This loss implemented two-step procedure: first assume model fine-tuned , obtaining updated parameters , applying updated parameters predict , getting error converted loss value; loss used compute gradients, original parameters updated step. \paragraph{MAML.} proposed MAML consists following steps one episode: Then, next episode, MAML runs process newly sampled training task. The process depicted Figure . During meta-training, MAML learns initialization parameters allow model adapt quickly efficiently new task examples. MAML model agnostic; means virtually applied neural networks. However, MAML quite hard train two levels training: meta-backpropagation implies computation gradients gradients. % To fair, MAML currently doesn work well metric learning algorithms popular few-shot image classification benchmarks. It quite hard train two levels training, hyper-parameters search much complex. Plus, meta-backpropagation implies computation gradients gradients, use approximations able train standard GPUs. For reasons, would probably rather use metric learning algorithms projects home work. % But reason Model Agnostic Meta-Learning exciting Model Agnostic. This means virtually applied neural network, task. \paragraph{FOMAML---First-Order MAML .} The standard MAML expensive computation. FOMAML simplified implementation follows: Comparing Equations -, FOMAML updates original parameters considering gradients last version ``fake'' parameters . So, gradients omitted. \paragraph{Reptile .} Reptile another first-order optimization-based meta-learning, shown Figure . It also samples training tasks : , , , , . Each training task \{, \} separations. For training task , let's assume original parameters went steps updating become \thetam=1m>1\mathop{\mathbb{E}}_{\tau}[\mathrm{SGD}]\mathrm{SGD}$ MAML explicitly optimizes efficiency algorithm support set, making sure learnt algorithm learn fast few-shot examples target task. In contrast, Reptile tries optimize system work well training tasks---it may work well target task close training tasks. \end{table*} In conclusion, methods achieve good results unsupervised translation word embeddings without directly considering Problem loss function . We showed lot approaches tackling Problem interesting alternative formulations. We underlined mathematical properties Wasserstein-Procrustes problem hence used concept different natural initialization transformations iterative algorithm achieve improved results mapping word embeddings different languages. This method also applications word translation across different languages. In particular, shown possible use algorithm refinement tool demonstrated improved results using transformation \citet{conneau2017word} initialization matrix . We hope rethinking Wasserstein-Procrustes problem would enable research would eventually help develop better algorithms aligning word embeddings across languages, especially taking account unsupervised approaches try minimize loss functions different Objective. In future work, plan study loss functions. We interested see well objectives Table correlate CSLS. Finally, plan combinations existing methods. , leave follow-up discussion. Finally, plan novel ways optimize Wasserstein-Procrustes could yield improvements, used conjunction existing methods. Furthermore, novel ways optimizing Wasserstein-Procrustes objective might useful, consider Supplementary materials. basis propose Supplementary material future work along lines. We think scientific community could benefit detailed study loss functions Wasserstein-Procrustes. We propose Supplementary material future work along lines. We presented work rethinking Wasserstein-Procrustes problem formulation task aligning word embeddings across languages. In particular, demonstrated properties problems equivalent Wasserstein-Procrustes help unsupervised setup. We showed that, semi-supervised setup, using little supervision yield good results, especially datasets similar language. We believe rethinking Wasserstein-Procrustes problem would enable research would eventually help develop better algorithms aligning word embeddings across languages, especially taken account unsupervised approaches try minimise loss functions different Objective.","  Few-shot natural language processing  refers to  NLP tasks that are accompanied with merely a handful of labeled examples.   This is a real-world challenge that an AI system must learn to handle.  Usually we rely on collecting more auxiliary information or developing a more efficient learning algorithm. However, the general gradient-based optimization in high capacity models, if training from scratch,  requires many parameter-updating  steps over a large number of labeled examples to perform well \cite{DBLPSnellSZ17}.    % The general belief is that gradient-based optimization in high capacity classifiers, if training from scratch,  requires many iterative steps over many examples to perform well \cite{DBLPSnellSZ17}. However, due to the availability of large-scaled pretrained model such as BERT, RoBERTa, those classifiers could have relatively good initialization; in this case, fine-tuning on support set is feasible.  If the target task itself cannot provide more information, how about collecting more tasks equipped with rich annotations to help the model learning?  The goal of meta-learning is to train a model on a variety of tasks with rich annotations, such that it can solve a new  task using only a few  labeled samples. The key idea  is to train the model's initial parameters such that the model has maximal performance on a new task after the parameters have been updated through zero or a couple of gradient steps. % The process of training a model parameters such that a few gradient steps, or even a single gradient step, can produce good results on a new task can be viewed from a feature learning standpoint as building an internal representation that is broadly suitable for many tasks. If the internal representation is suitable to many tasks, simply fine-tuning the parameters slightly  can produce good results % \cite{DBLPFinnAL17}.  There are already some surveys for meta-learning, such as \cite{DBLPVilaltaD02,DBLPabs03548,DBLP05439}. Nevertheless,  this paper focuses on NLP domain, especially few-shot applications. We try to provide  clearer definitions, progress summary and some common datasets  of applying meta-learning to few-shot NLP."
"% --------------------------- Paraphrase identification core NLP task widely studied . One interesting application area paraphrase detection Community Question Answering . The aim CQA answer real open-ended questions based user-generated content question answering websites. Being able identify similar --- already answered --- questions helpful purpose. Question paraphrase detection CQA difficult texts tend longer less direct overlap compared traditional paraphrase detection datasets . Early work paraphrase detection relied hand-crafted features, state-of-the-art approaches paraphrase identification primarily neural networks hybrid techniques . Many recently proposed CQA paraphrase detection systems still use hand-crafted features work successfully integrated topic model features . This suggests topic distributions could offer auxiliary information identifying related questions complement word embeddings , provide main signal neural systems. Contrary hand-crafted static topic features, integrating topics neural framework brings advantage joint updates training. Recent work successfully introduced topics neural architectures language generation: \citet{wang_reinforced_2018} used topic-enhanced encoder summarisation, \citet{chen_guided_2016} integrated topics decoder machine translation \citet{narayan_dont_2018} included topics encoder decoder summarisation model. However, remains unclear topics useful neural paraphrase detection model best fuse topics word embeddings task. In paper, introduce novel topic-aware neural architecture specifically make following contributions: % --------------------------- In paper, incorporated novel error correction mechanism neural machine translation, aims solve error propagation problem sequence generation. Specifically, introduce two-stream self-attention neural machine translation, design error correction mechanism based two-stream self-attention, able correct previous predicted errors generate next token. Experimental results three IWSLT tasks two WMT tasks demonstrate method outperforms previous methods including scheduled sampling, alleviates problem error propagation effectively. In future, expect apply method sequence generation tasks, \eg, text summarization, unsupervised neural machine translation, incorporate error correction mechanism advanced structures."," Question paraphrase identification is a key task in Community Question Answering  to determine if an incoming question has been previously asked. Many current models use word embeddings to identify duplicate questions, but the use of topic models in feature-engineered systems suggests that they can be helpful for this task, too. We therefore propose two ways of merging topics with word embeddings  in a new neural architecture for question paraphrase identification. Our results show that our system outperforms neural baselines on multiple CQA  datasets, while an ablation study highlights the importance of topics and especially early topic-embedding fusion in our architecture."
"Metaphor figure speech widespread presence form communication either oral written. According Steen data analysis shows that, average, one every seven half lexical units corpus related metaphor However, difficult clearly define boundaries separate metaphor literal uses, well metaphor figures speech. The difficulty clearly establishing theoretical background metaphor justifies variety NLP systems aim automatically distinguishing metaphorical literal meanings word phrase. This difficulty exacerbated take account limitations Greek regards resources tools metaphor detection; thus, conclude development neural language models necessary automatic differentiation literal metaphorical meaning phrases part authentic non-annotated Greek corpus. For reasons, attempt based principles distributional semantics determine relations word linguistic context group semantic similarities linguistic items based distributional properties rather connections certain term related concepts. Distributional semantics paramount shifting research interest towards neural language models, attribute hidden statistical characteristics distributed representations word sequences natural language. Therefore, serious problem automatic detection metaphors differentiation literal uses dealt development neural language models. This paper explained applied RNNLMs real-time large vocabulary decoder introducing use GPGPUs. We tried accelerate RNNLM-based WFST traversals GPGPU-CPU hybrid architectures solving practical issues applying GPGPUs. Moreover, order minimize computation burden CPUs, applied cache strategy. REVIEW 1-4) Table 2    10         . The decoding speed RNNLMs still slower n-gram models, proposed method achieved real-time speed maintaining relatively 10\ lower WER shown Table, could perfectly apply approach on-line streaming speech recognition engine. The memory footprint cache method small enough perform experiment large data set. However, desirable employ efficient cache techniques reduce memory usage further. \eightpt"," %\boldmath This paper presents and benchmarks a number of end-to-end Deep Learning based models for metaphor detection in Greek. We combine Convolutional Neural Networks and Recurrent Neural Networks with representation learning to bear on the metaphor detection problem for the Greek language. The models presented achieve exceptional accuracy scores, significantly improving the previous state of the art results, which had already achieved accuracy 0.82. Furthermore, no special preprocessing, feature engineering or linguistic knowledge is used in this work. The methods presented achieve accuracy of 0.92 and F-score 0.92 with Convolutional Neural Networks  and bidirectional Long Short Term Memory networks . Comparable results of  0.91 accuracy and  0.91 F-score are also achieved with bidirectional Gated Recurrent Units  and Convolutional Recurrent Neural Nets .  The models are trained and evaluated only on the basis of the training tuples, the sentences and their labels. The outcome is a state of the art collection of metaphor detection models, trained on limited labelled resources, which can be extended to other languages and similar tasks."
"% % The following footnote without marker needed camera-ready % version paper. % Comment instructions uncomment 8 lines % ""final paper"" variant English. % . % % % final paper: en-us version % % % space normally used marker % This work licensed Creative Commons % Attribution 4.0 International License. % License details: % \url{http://creativecommons.org/licenses/by/4.0/}. } Sentiment analysis gained importance current world social media crucial gauging public opinion products, political campaigns, latest trends more. A large portion world's population multilingual, content social networks. Code-mixing natural phenomenon among bilinguals phrases words one language employed another. Typically, underlying grammar primary language spoken kept intact phrases another language embedded it. India particular, 23 officially recognized languages majorly bilingual population, requiring robust computational tools effectively exploit data produces. Deep neural networks great success predicting sentiment encapsulated text - big part success ability utilize pretrained word embeddings. In code-mixed tweets however, Hindi words written Roman script instead native Devanagari script, forcing one use transliterator order take advantage pretrained aligned word embeddings, like fastText . These words Roman script unnormalized may multiple spellings; example, bahut spelt Roman script bahut, bohot, bohut etc., errors propagate transliterator downstream task, thereby reducing performance. In work, propose deep convolution network self-attention shows promising results, without pretraining. Convolution neural networks known capture local relations local context, work feature extractors acting substitute handcrafted sentiment features. A self-attention layer applied features , allow individual feature attend features , providing global context. We call Hierarchical Context Modeling System , due hierarchical nature context extraction performed levels. We presented collection state-of-the art metaphor detection models achieving accuracy higher 90\ Greek language. This extends work and, best knowledge, sets new state-of-the-art metaphor detection Greek, dealing simultaneously lack linguistic resources Greek. We aim continuing work exploring performance contextual embeddings ELMO BERT . Another recent promising direction, especially small datasets Graph Neural Networks . In specific variation graph neural networks, entire training set represented graph task model node representation classification, even potentially training examples. This achieved exploiting graph structure representation adjacent nodes graph. Both CNNs bi-directional LSTMs fine-tuning achieve accuracy higher 90\ . If disable fine-tuning, classification accuracy still high, although overall fine-tuning appears consistently outperform non fine-tuning configurations, also consistent results presented . There several factors explain performance achieved neural networks. First, full sentence passed classifier thus model benefit exploiting potential long-term semantic dependencies. These dependecies captured LSTM cells convolutional operators. Additionaly, case LSTMs GRUs, bidirectional architectures appear consistenly outperform unidirectional architectures. Finally, transfer learning, form pre-trained embeddings fastText extremelly useful sense learned representations capture semantic properties words unsupervised learning fashion also allow fine-tuning, proven enchance accuracy models . Fasttexts' ability implicitly utilize morphological structure form sub-word representations also proven help overall downstream architecture significantly improve. We conjecture property holds languages rich morphological structure like Greek. Since possible distinguish different kinds metaphor even levels metaphoricity term sentence, effort solely aimed distinguishing literal metaphorical use term specific linguistic context. In regard, checked whether neural language models appropriate properties order discriminate pure metaphor kinds figurative speech personification, metonymy, synecdoche etc. In addition, approach metaphor detection able classify metaphorical phrases categories like direct indirect, implied extended. Of course, endeavor particularly interesting demanding research challenge, even though main goal specific approach metaphor detection discrimination literal cases use machine learning algorithms. that's folks","   Problems involving code-mixed language are often plagued by a lack of resources and an absence of materials to perform sophisticated transfer learning with. In this paper we describe our submission to the Sentimix Hindi-English task involving sentiment classification of code-mixed texts, and with an F1 score of 67.1\%, we demonstrate that simple convolution and attention may well produce reasonable results."
"Knowledge Bases collection factual information form relational triplets. Each relational triplets represented entities knowledge base r relation . The popular way visualising knowledge base representing multi relational graph triplet represented directed edge label r. Knowledge Bases used improve performance across variety tasks like Question Answering , Dialogue Generation many others. However, since Knowledge Bases populated automatic mining texts, often incomplete since possible manually write facts, often inaccuracies extraction. This inaccuracy leads decline performance across variety downstream tasks. Hence, lot work coming efficient tool complete Knowledge Bases automatically adding new facts without requiring extra knowledge. This task referred Knowledge Base Completion , goal solve queries like . The first approach towards efficient Knowledge Base Completion additive models like TransE TransH relations interpreted simple translations hidden entity representations. Multiplicative models like Distmult Complex observed outperform simple additive models. Instead translation, RotatE defines relation simple rotations head entity rotated complex embedding space match tail entity, shown satisfy lot useful semantic properties like compositionality relations. Recently, expressive Neural Network-based methods ConvKB) introduced scoring function learned along model. However, models process triplet independently. As result, methods cannot capture semantically rich neighborhood hence produce low-quality embeddings. Graphs widely used visualize real-world data. There tremendous progress applying ML techniques images text, successfully adapted graphs (like \citet{GCN}, \citet{graphsage}, \citet{GAT}. Taking inspiration approach, number Graph Neural Network-based methods proposed capture neighborhood Knowledge Graphs KBC task. In survey, aim look formulations. In paper discussed way perform sentiment analysis code-mixed Hindi-English data, feel system versatile enough applied mix languages, especially resources mixture unavailable hard find. We also looked ways data preprocessed training, multilevel neural architecture able extract context text. While encouraging see model performs consistently, couple caveats need addressed - even though lack pretraining, low resource setting, noise data seem surmountable hurdles task, evident promising avenues improvement lie exploring data cleaning/normalization transfer learning. Further analysis present methodology could also reveal kind transfer learning required - whether pretrained word embeddings, large scale language modeling perhaps better transliteration pipeline name few. To would require larger quantities data application system linguistic tasks settings similar Sentimix. All steps qualify within scope future work, committing reward us confidence hypotheses propose paper, providing proof system's robustness effectiveness. We hope undertake efforts soon. While encouraging see model performs consistently, couple caveats thatneed addressed - firstly, acknowledge methodology could benefit analysisto explain obtained results. To would require larger quantities data application otherlinguistic tasks settings similar SentiMix. Secondly, even though lack pretraining, alow resource setting, noise data seem surmountable hurdles task, clear mostpromising avenues improvement lie exploring data cleaning/normalization transfer learning.Both points qualify within scope future work, committing reward uswith confidence hypotheses propose paper, providing proof systemrobustness effectiveness. We hope undertake efforts soon. include bib file like this:"," Knowledge Graphs are increasingly becoming popular for a variety of downstream tasks like Question Answering and Information Retrieval. However, the Knowledge Graphs are often incomplete, thus leading to poor performance. As a result, there has been a lot of interest in the task of Knowledge Base Completion. More recently, Graph Neural Networks have been used to capture structural information inherently stored in these Knowledge Graphs and have been shown to achieve SOTA performance across a variety of datasets. In this survey, we understand the various strengths and weaknesses of the proposed methodology and try to find new exciting research problems in this area that require further investigation."
"A recent estimate total number English research articles available online least 114 million . Studies indicate number academic papers doubles every 10--15 years . The continued growth scholarly papers make finding relevant research papers challenging. Searches based keywords may longer efficient method use. This often happens query terms appear multiple research areas. For example, querying ``neuron'' Google Scholar returns documents computer science neuroscience. Search results also belong diverse domains query terms contain acronyms. For example, querying ``IR'' returns documents Computer Science Physics . Similarly, querying ``NLP'' returns documents Linguistics Computer Science . This documents multiple subject categories often mixed together digital library search engine corresponding SC metadata usually available existing document metadata, either publisher automatic extraction methods. As such, believe useful build classification system assigns scholarly papers SCs. Such system could significantly impact scientific search facilitate bibliometric evaluation. It also help Science Science research , recent area research uses scholarly big data study choice scientific problems, scientist career trajectories, research trends, research funding, research aspects. Also, many noted difficult extract SCs using traditional topic models Latent Dirichlet Allocation , since extracts words phrases present documents. An example paper computer science rarely given label keyword. In contrast, SC classification usually based universal schema specific domain domains Library Congress. A crowd sourced schema found DBpedia Wikipedia. %%% need citation %Classifying documents SCs entails labelling document subject domain best describes article's content at. This helps organizing indexing digital collections assists users narrowing search results. Many online retailers implemented faceted search feature, e.g., Amazon music search. However, similar features yet seen scholarly digital library search engines. In work, pose SC problem one multiclass classification one SC assigned paper. In preliminary study, investigated feature-based machine learning methods classify research papers 6 SCs . Here, extend study propose system classifies scholarly papers 104 SCs using paper's abstract. The core component supervised classifier based recurrent neural networks trained large number labeled documents part WoS database. In comparison preliminary work, data heterogeneous , imbalanced, complicated . We compare system several baselines applying various text representations, machine learning models, and/or neural network architectures. Many schemas scientific classification systems publisher domain specific. For example, ACM hierarchical classification system, NLM medical subject headings, MSC subject classification mathematics. The comprehensive systematic classification schemas seem WoS Library Congress . The latter created 1897 driven practical needs LOC rather epistemological considerations likely date. To best knowledge, work first example using neural network classify scholarly papers comprehensive set SCs. Other work focused unsupervised methods developed specific category domains. In contrast, classifier trained large number high quality abstracts WoS applied directly abstracts without citation information. We also develop novel representation scholarly paper abstracts using ranked tokens word embedding representations. This significantly reduces scale classic Bag Word model. We also retrained FastText GloVe word embedding models using WoS abstracts. The subject category classification applied CiteSeerX collection documents. However, could applied similar collection. Our proposed methods ranked 6 62 groups Hindi-English dataset ranked 7 29 Spanish-English dataset. This result shows strength combination NBSVM TF-IDF language independence Model. Also, experiment shows substantial adverse effect ignoring one language's representation. For future work, going utilize languages pre-trained embedding. Averaging one way combine embeddings. However, different output space embedding could challenging. Also, attention mechanism could used give model ability decide importance language representation within sentence. include bib file like this:","  %%% Leave the Abstract empty if your article does not require one, please see the Summary Table for full details. %\section{} Subject categories of scholarly papers generally refer to the knowledge domain to which the papers belong, examples being computer science or physics. Subject category information can be used for building faceted search for digital library search engines. This can significantly assist users in narrowing down their search space of relevant documents. Unfortunately, many academic papers do not have such information as part of their metadata. Existing methods for solving this task usually focus on unsupervised learning that often relies on citation networks. However, a complete list of papers citing the current paper may not be readily available. In particular, new papers that have few or no citations cannot be classified using such methods. Here, we propose a deep attentive neural network  that classifies scholarly papers using only their abstracts. The network is trained using 9 million abstracts from Web of Science . We also use the WoS schema that covers 104 subject categories.  %The abstracts are represented by a fix-length vector, which is generated by concatenating retrained top frequency word vectors.  The proposed network consists of two bi-directional recurrent neural networks followed by an attention layer. We compare our model against baselines by varying the architecture and text representation. Our best model achieves micro-${F_1}$ measure of $0.76$ with $F_1$ of individual subject categories ranging from $0.50$--$0.95$. The results showed the importance of retraining word embedding models to maximize the vocabulary overlap and the effectiveness of the attention mechanism. The combination of word vectors with TFIDF outperforms character and sentence level embedding models. We discuss imbalanced samples and overlapping categories and suggest possible strategies for mitigation. We also determine the subject category distribution in CiteSeerX by classifying a random sample of one million academic papers.     \tiny  \keyFont{ \section{Keywords:}text classification, text mining, scientific papers, digital library, neural networks, CiteSeerX, subject category} %All article types: you may provide up to 8 keywords; at least 5 are mandatory."
". % % % final paper: en-us version % % % space normally used marker % This work licensed Creative Commons % Attribution 4.0 International License. % License details: % \url{http://creativecommons.org/licenses/by/4.0/}. } One concerns SemEval-2020 Task 5: Modelling Causal Reasoning Language: Detecting Counterfactuals research extent current state-of-the-art systems detect counterfactual statements. A counterfactual statement, defined competition, conditional composed two parts. The former part antecedent -- statement contradictory known facts. The latter consequent -- statement describes would happen antecedent held. Here, discuss experimental results, impact data results, potential solutions, application model real-world data. The data imbalance problem ubiquitous multi-class multi-label classification problems . The imbalance ratio , defined ratio number instances majority class number samples minority class , commonly used characterize level imbalance. Compared imbalance datasets Table~1 , data significantly high level imbalance. In particular, highest IR 49,000 . There 3 ways commonly adopted mitigate data imbalance problem: data resampling, algorithmic adaptations, cost sensitive classification . One commonly used way mitigate problem data resampling. This method based rebalancing SC distributions either deleting instances major SCs supplementing artificially generated instances minor SCs . We always undersample major SCs, means reduce sample sizes SCs 15 , small training robust neural network models. The oversampling strategies SMOTE works problems involving continuous numerical quantities, e.g., \citet{salaheldeen2015predicting}. In case, synthesize vectors ``abstracts'' SMOTE map actual words word representations sparsely distributed large WE space. Even oversample minor SCs using semantically dummy vectors, generating samples take large amount time given high dimensionality abstract vectors high IR. Therefore, use real data. We discuss potential impact classification results contributed categories overlapping training data. Our initial classification schema contains 104 SCs, mutually exclusive. Instead, vocabularies categories overlap others. For example, papers exclusively labeled ``Materials Science"" ``Metallurgy"" exhibit significant overlap tokens. In WE space, semantic vectors labeled either category overlapped making hard differentiate them. Figure shows confusion matrices closely related categories ``Geology"", ``Mineralogy"", ``Geochemistry Geophysics''. Figure t-SNE plot abstracts closely related SCs. To make plot less crowded, randomly select 250 abstracts SC shown Figure. Data points representing ``Geology"", ``Mineralogy"", ``Geochemistry Geophysics"" tend spread overlapped way hard visually distinguished. Since dimension WE high feature vector sparse, use TF-IDF represent abstracts belonging mentioned SCs evade singularity issues. We reduce dimension abstract using t-SNE plot abstracts 2D graph. Figure shows visualisation SCs shown Figure. Data points representing ``Geology"", ``Mineralogy"", ``Geochemistry Geophysics"" Figure tend spread overlapped way hard visually distinguished. Although ``Genetics Heredity'' seems relatively far isolated ``Zoology'', mixed ``Plant Sciences'' ``Biology'' instances, consistent contingency matrix Figure . One way mitigate problem merge overlapped categories. However, special care taken whether overlapped SCs truly strongly related evaluated domain experts. For example, term ``Engineering"" qualifies generic term application technical aspects rather SC. ``Zoology'', ``PlantSciences'', ``Ecology'' merged single SC called ``Biology'' . ``Geology'', ``Mineralogy'', ``GeoChemistry GeoPhysics'' merged single SC called ``Geology''. ``Engineering'' broad category containing numerous branches related various technical aspects. Since ``Engineering'' cannot considered category, removed instances SC dataset. However, However, ``Materials Science'' ``Metallurgy'' may merged single SC. By aforementioned merges, number SCs reduced 74. As preliminary study, classified merged dataset using best model achieved improvement overall micro- score 0.78. The ``Geology'' SC merging significantly improved . The ``Biology'' merging significantly higher compared previous . A thorough study necessary category merges make new schema topic future work. CiteSeerX digital library search engine first use automatic citation indexing. It open source search engine provides metadata full-text access 10 million scholarly documents continues add new documents . In past decade, incorporated scholarly documents diverse SCs, distribution subject categories unknown. Using best neural network model work , classified 1 million papers randomly selected CiteSeerX 104 SCs . The top five subject categories Biology , Computer Science , Mathematics , Engineering , Public Environmental Occupational Health . The fraction Computer Science papers significantly higher results \citet{wu2018bigdata}. The Computer Science 94\ higher work . Therefore, fraction may overestimated here. However, 6 classes model classifies abstracts 104 SCs, although compromises accuracy, , work still used starting point systematic subject category classification. The classifier classifies 1 million abstracts 1253 seconds implying scalable multi-millions papers. \section{Conclusions} We investigated problem systematically classifying large collection scholarly papers 104 SC's using neural network methods based abstracts. Our methods appear scale better existing clustering-based methods rely citation networks. We explored various state-of-the-art WE models, neural network architectures, attention models, strategies found For neural network methods, retrained FastText GloVe combined BiGRU BiLSTM attention mechanism gives best results. Retraining WE models using attention mechanism play important roles improving classifier performance. A two-level classifier effectively improves performance dealing training data extremely imbalanced categories. The median 's best settings 0.75--0.76. deal data imbalance problem, caused poor classification performance number categories extremely low samples. The classification system based Web Science schema. The original schema collapsed revised classification focuses 104 top level SCs. The first-level classifier classifies 81 SCs, including ``Others"" SC. The second-level classifier classifies ``Others'' 24 minor SCs. Our model achieved significantly high -scores certain SCs. The classifier achieves 18 SCs. The median 's best settings 0.75--0.76. One bottleneck classifier overlapping categories. Merging closely related SCs promising solution, guidance domain experts. The current algorithm perform well significantly overlapped categories. To mitigate problem, schema revised merging certain SCs suggestions domain experts. We could also improve text representation. The TF-IDF representation considers unigrams. Future work could consider -grams transfer learning adopt word/sentence embedding models trained non-scholarly corpora . . . One could investigate models also take account stop-words, e.g., . One could also explore alternative optimizers neural networks besides Adam, Stochastic Gradient Descent . widely used, generalize well especially non-convex problems tends converge local minimum. One alternative use Stochastic Gradient Descent loss function approaches global minimum. For application, intend explore topical representations reinforcement learning and/or multi-task learning classify multi-disciplinary papers install working version CiteSeerX near future. \section*{Acknowledgements} We gratefully acknowledge partial support National Science Foundation. We also acknowledge Adam T. McMillen technical support, Holly Gaff, Old Dominion University Shimin Liu, Pennsylvania State University domain experts respectively biology earth mineral sciences. Science, Engineering Humanities Social Sciences articles, Humanities Social Sciences articles please include page numbers in-text citations Health, Physics Mathematics articles }{}} \providecommand{\bibAnnote}[2]{ } \bibitem[{Agrawal et~al.Agrawal, Fu, Menzies}]{agrawal2018wrong} Agrawal, A., Fu, W., Menzies, T. . \newblock What wrong topic modeling? fix using search-based software engineering. \newblock Information Software Technology 98, 74--88 \bibAnnoteFile{agrawal2018wrong} \bibitem[{Arora et~al.Arora, Liang, Ma}]{arora2017iclr} Arora, S., Liang, Y., Ma, T. . \newblock A simple tough-to-beat baseline sentence embeddings. \newblock In {ICLR} \bibAnnoteFile{arora2017iclr} \bibitem[{Baeza-Yates Ribeiro-Neto}]{baezayates1999moden} Baeza-Yates, R.~A. Ribeiro-Neto, B. . \newblock Modern Information Retrieval \bibAnnoteFile{baezayates1999moden} \bibitem[{Beltagy et~al.Beltagy, Cohan, Lo}]{beltagy2019scibert} Beltagy, I., Cohan, A., Lo, K. . \newblock Scibert: Pretrained contextualized embeddings scientific text. \newblock CoRR abs/1903.10676 \bibAnnoteFile{beltagy2019scibert} \bibitem[{Bojanowski et~al.Bojanowski, Grave, Joulin, Mikolov}]{bojanowski2017enriching} Bojanowski, P., Grave, E., Joulin, A., Mikolov, T. . \newblock Enriching word vectors subword information. \newblock Transactions Association Computational Linguistics 5, 135--146 \bibAnnoteFile{bojanowski2017enriching} \bibitem[{Boyack Klavans}]{boyack2018accurately} Boyack, K.~W. Klavans, R. . \newblock Accurately identifying topics using text: Mapping pubmed ), 107--115 \bibAnnoteFile{boyack2018accurately} \bibitem[{Bunk Krestel}]{bunk2018welda} Bunk, S. Krestel, R. . \newblock Welda: Enhancing topic models incorporating local word context. \newblock In Proceedings 18th ACM/IEEE Joint Conference Digital Libraries. JCDL '18, 293--302 \bibAnnoteFile{bunk2018welda} \bibitem[{Caragea et~al.Caragea, Wu, Gollapalli, Giles}]{caragea2016aaai} Caragea, C., Wu, J., Gollapalli, S.~D., Giles, C.~L. . \newblock Document type classification online digital libraries. \newblock In Proceedings 13th {AAAI Conference} \bibAnnoteFile{caragea2016aaai} \bibitem[{Cer et~al.Cer, Yang, Kong, Hua, Limtiaco, John et~al.}]{cer2018universal} Cer, D., Yang, Y., Kong, S., Hua, N., Limtiaco, N., John, R.~S., et~al. . \newblock Universal sentence encoder english. \newblock In Proceedings EMNLP Conference \bibAnnoteFile{cer2018universal} \bibitem[{Charte et~al.Charte, Rivera, del Jes{\'{u}}s, Herrera}]{charte2015addressing} Charte, F., Rivera, A.~J., del Jes{\'{u}}s, M.~J., Herrera, F. . \newblock Addressing imbalance multilabel classification: Measures random resampling algorithms. \newblock Neurocomputing 163, 3--16 \bibAnnoteFile{charte2015addressing} \bibitem[{Chawla et~al.Chawla, Bowyer, Hall, Kegelmeyer}]{chawla2002smote} Chawla, N.~V., Bowyer, K.~W., Hall, L.~O., Kegelmeyer, W.~P. . \newblock Smote: Synthetic minority over-sampling technique. \newblock J. Artif. Int. Res. 16, 321--357 \bibAnnoteFile{chawla2002smote} \bibitem[{Cho et~al.Cho, van Merrienboer, G{\""{u}}l{\c{c}}ehre, Bahdanau, Bougares, Schwenk et~al.}]{cho2014learning} Cho, K., van Merrienboer, B., G{\""{u}}l{\c{c}}ehre, {\c{C}}., Bahdanau, D., Bougares, F., Schwenk, H., et~al. . \newblock Learning phrase representations using {RNN} encoder-decoder statistical machine translation. \newblock In Proceedings 2014 Conference Empirical Methods Natural Language Processing, {EMNLP 2014}. 1724--1734 \bibAnnoteFile{cho2014learning} \bibitem[{Conneau et~al.Conneau, Kiela, Schwenk, Barrault, Bordes}]{conneau2017emnlp} Conneau, A., Kiela, D., Schwenk, H., Barrault, L., Bordes, A. . \newblock Supervised learning universal sentence representations natural language inference data. \newblock In Proceedings {EMNLP Conference} \bibAnnoteFile{conneau2017emnlp} \bibitem[{Devlin et~al.Devlin, Chang, Lee, Toutanova}]{devlin2019bert} Devlin, J., Chang, M., Lee, K., Toutanova, K. . \newblock {BERT:} pre-training deep bidirectional transformers language understanding. \newblock In NAACL-HLT \bibAnnoteFile{devlin2019bert} \bibitem[{Fellbaum}]{Fellbaum2005-FELWAW} Fellbaum, C. . \newblock Wordnet wordnets. \newblock In Encyclopedia Language Linguistics, ed. A.~Barber . 2--665 \bibAnnoteFile{Fellbaum2005-FELWAW} \bibitem[{Fortunato et~al.Fortunato, Bergstrom, B{\""o}rner, Evans, Helbing, Milojevi{\'c} et~al.}]{fortunato2018science} Fortunato, S., Bergstrom, C.~T., B{\""o}rner, K., Evans, J.~A., Helbing, D., Milojevi{\'c}, S., et~al. . \newblock Science science. \newblock Science 359 \bibAnnoteFile{fortunato2018science} \bibitem[{Garc{\'{\i}}a et~al.Garc{\'{\i}}a, S{\'{a}}nchez, Mollineda}]{garcia2012effectiveness} Garc{\'{\i}}a, V., S{\'{a}}nchez, J.~S., Mollineda, R.~A. . \newblock On effectiveness preprocessing methods dealing different levels class imbalance. \newblock Knowl.-Based Syst. 25, 13--21 \bibAnnoteFile{garcia2012effectiveness} \bibitem[{Gerlach et~al.Gerlach, Peixoto, Altmann}]{gerlach2018network} Gerlach, M., Peixoto, T.~P., Altmann, E.~G. . \newblock A network approach topic models. \newblock Science advances 4, eaaq1360 \bibAnnoteFile{gerlach2018network} \bibitem[{Giles et~al.Giles, Bollacker, Lawrence}]{giles1998jcdl} Giles, C.~L., Bollacker, K.~D., Lawrence, S. . \newblock {CiteSeer}: An automatic citation indexing system. \newblock In Proceedings 3rd {ACM International Conference Digital Libraries, June 23-26, 1998, Pittsburgh, PA, {USA}}. 89--98 \bibAnnoteFile{giles1998jcdl} \bibitem[{Gl{\""a}nzel Thijs}]{glanzel2017using} Gl{\""a}nzel, W. Thijs, B. . \newblock Using hybrid methods `core documents' representation clusters topics: astronomy dataset. \newblock Scientometrics 111, 1071--1087 \bibAnnoteFile{glanzel2017using} \bibitem[{Goldberg Levy}]{goldberg2014word2vec} Goldberg, Y. Levy, O. . \newblock word2vec explained: deriving mikolov et al.'s negative-sampling word-embedding method. \newblock arXiv preprint arXiv:1402.3722 \bibAnnoteFile{goldberg2014word2vec} \bibitem[{Grave et~al.Grave, Mikolov, Joulin, Bojanowski}]{grave2017fasttext} Grave, E., Mikolov, T., Joulin, A., Bojanowski, P. . \newblock Bag tricks efficient text classification. \newblock In Proceedings 15th {EACL} \bibAnnoteFile{grave2017fasttext} \bibitem[{Greff et~al.Greff, Srivastava, Koutn{\'{\i}}k, Steunebrink, Schmidhuber}]{greff2017lstm} Greff, K., Srivastava, R.~K., Koutn{\'{\i}}k, J., Steunebrink, B.~R., Schmidhuber, J. . \newblock {LSTM:} {A} search space odyssey. \newblock {IEEE Trans. Neural Networks Learn. Syst.} 28, 2222--2232 \bibAnnoteFile{greff2017lstm} \bibitem[{He et~al.He, Fang, Cui, Wu, Lu}]{he2018keyphrase} He, G., Fang, J., Cui, H., Wu, C., Lu, W. . \newblock Keyphrase extraction based prior knowledge. \newblock In Proceedings 18th {ACM/IEEE Joint Conference Digital Libraries, {JCDL}} \bibAnnoteFile{he2018keyphrase} \bibitem[{Hochreiter Schmidhuber}]{hochreiter1997lstm} Hochreiter, S. Schmidhuber, J. . \newblock {Long Short-Term Memory}. \newblock Neural Comput. 9, 1735--1780 \bibAnnoteFile{hochreiter1997lstm} \bibitem[{Iyyer et~al.Iyyer, Manjunatha, Boyd-Graber, Daum{\'e}~III}]{iyyer2015deep} Iyyer, M., Manjunatha, V., Boyd-Graber, J., Daum{\'e}~III, H. . \newblock Deep unordered composition rivals syntactic methods text classification. \newblock In Proceedings ACL \bibAnnoteFile{iyyer2015deep} \bibitem[{Khabsa Giles}]{khabsa2014plosone} Khabsa, M. Giles, C.~L. . \newblock The number scholarly documents public web. \newblock PLoS ONE 9, e93949 \bibAnnoteFile{khabsa2014plosone} \bibitem[{Larsen von Ins}]{larsen2010} Larsen, P. von Ins, M. . \newblock The rate growth scientific publication decline coverage provided science citation index. \newblock Scientometrics 84, 575--603 \bibAnnoteFile{larsen2010} \bibitem[{LeCun et~al.LeCun, Boser, Denker, Henderson, Howard, Hubbard et~al.}]{lecun1989cnn} LeCun, Y., Boser, B.~E., Denker, J.~S., Henderson, D., Howard, R.~E., Hubbard, W.~E., et~al. . \newblock Handwritten digit recognition back-propagation network. \newblock In Advances Neural Information Processing Systems 2, {[NIPS Conference]} \bibAnnoteFile{lecun1989cnn} \bibitem[{Llewellyn et~al.Llewellyn, Grover, Alex, Oberlander, Tobin}]{llewellyn2015tpdl} Llewellyn, C., Grover, C., Alex, B., Oberlander, J., Tobin, R. . \newblock Extracting topic specific dataset twitter archive. \newblock In TPDL Conference \bibAnnoteFile{llewellyn2015tpdl} \bibitem[{Matsuda Fukushima}]{matsuda1999wwwretrieval} Matsuda, K. Fukushima, T. . \newblock Task-oriented world wide web retrieval document type classification. \newblock In Proceedings CIKM \bibAnnoteFile{matsuda1999wwwretrieval} \bibitem[{Mikolov et~al.Mikolov, Chen, Corrado, Dean}]{mikolov2013word2vec} Mikolov, T., Chen, K., Corrado, G., Dean, J. . \newblock Efficient estimation word representations vector space. \newblock CoRR abs/1301.3781 \bibAnnoteFile{mikolov2013word2vec} \bibitem[{Mikolov et~al.Mikolov, Yih, Zweig}]{mikolov2013w2v} Mikolov, T., Yih, W., Zweig, G. . \newblock Linguistic regularities continuous space word representations. \newblock In Proceedings NAACL-HLT \bibAnnoteFile{mikolov2013w2v} \bibitem[{Moscato Cotta}]{moscato2003gentle} Moscato, P. Cotta, C. . \newblock A Gentle Introduction Memetic Algorithms . \newblock 105--144 \bibAnnoteFile{moscato2003gentle} \bibitem[{Nadeau Sekine}]{nadeau2007survey} Nadeau, D. Sekine, S. . \newblock A survey named entity recognition classification. \newblock Lingvistic{e Investigationes} 30, 3--26 \bibAnnoteFile{nadeau2007survey} \bibitem[{Passos et~al.Passos, Kumar, McCallum}]{passos2014lexicon} Passos, A., Kumar, V., McCallum, A. . \newblock Lexicon infused phrase embeddings named entity resolution. \newblock In Proceedings CoNLL \bibAnnoteFile{passos2014lexicon} \bibitem[{Pennington et~al.Pennington, Socher, Manning}]{pennington2014glove} Pennington, J., Socher, R., Manning, C.~D. . \newblock Glove: Global vectors word representation. \newblock In Proceedings EMNLP Conference \bibAnnoteFile{pennington2014glove} \bibitem[{Peters et~al.Peters, Neumann, Iyyer, Gardner, Clark, Lee et~al.}]{peters2018elmo} Peters, M.~E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., et~al. . \newblock Deep contextualized word representations. \newblock In {NAACL-HLT} \bibAnnoteFile{peters2018elmo} \bibitem[{Prasad et~al.Prasad, Kaur, Kan}]{prasad2018nparscit} Prasad, A., Kaur, M., Kan, M.-Y. . \newblock {Neural} {ParsCit}: deep learning-based reference string parser. \newblock International Journal Digital Libraries 19, 323--337 \bibAnnoteFile{prasad2018nparscit} \bibitem[{Ratnaparkhi}]{ratnaparkhi1996maximum} Ratnaparkhi, A. . \newblock A maximum entropy model part-of-speech tagging. \newblock In The Proceedings EMNLP Conference \bibAnnoteFile{ratnaparkhi1996maximum} \bibitem[{Ren et~al.Ren, Zhang, Zhang, Ji}]{ren2016improving} Ren, Y., Zhang, Y., Zhang, M., Ji, D. . \newblock Improving twitter sentiment classification using topic-enriched multi-prototype word embeddings. \newblock In {AAAI} \bibAnnoteFile{ren2016improving} \bibitem[{SalahEldeen Nelson}]{salaheldeen2015predicting} SalahEldeen, H.~M. Nelson, M.~L. . \newblock Predicting temporal intention resource sharing. \newblock In Proceedings 15th JCDL Conference \bibAnnoteFile{salaheldeen2015predicting} \bibitem[{van Eck Waltman}]{vanEck2017citation} van Eck, N.~J. Waltman, L. . \newblock Citation-based clustering publications using citnetexplorer vosviewer. \newblock Scientometrics 111, 1053--1070 \bibAnnoteFile{vanEck2017citation} \bibitem[{Vaswani et~al.Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez et~al.}]{vaswani2017attention} Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., et~al. . \newblock Attention need. \newblock In NIPS \bibAnnoteFile{vaswani2017attention} \bibitem[{Vo Zhang}]{vo2015target} Vo, D.-T. Zhang, Y. . \newblock Target-dependent twitter sentiment classification rich automatic features \bibAnnoteFile{vo2015target} \bibitem[{Waltman van Eck}]{waltman2012new} Waltman, L. van Eck, N.~J. . \newblock A new methodology constructing publication-level classification system science. \newblock JASIST 63, 2378--2392 \bibAnnoteFile{waltman2012new} \bibitem[{Wang Koopman}]{wang2017clustering} Wang, S. Koopman, R. . \newblock Clustering articles based semantic similarity. \newblock Scientometrics 111, 1017--1031 \bibAnnoteFile{wang2017clustering} \bibitem[{Witt Seifert}]{witt2017tpdl} Witt, N. Seifert, C. . \newblock Understanding influence hyperparameters text embeddings text classification tasks. \newblock In TPDL Conference \bibAnnoteFile{witt2017tpdl} \bibitem[{Wu et~al.Wu, Kandimalla, Rohatgi, Sefid, Mao, Giles}]{wu2018bigdata} Wu, J., Kandimalla, B., Rohatgi, S., Sefid, A., Mao, J., Giles, C.~L. . \newblock Citeseerx-2018: {A} cleansed multidisciplinary scholarly big dataset. \newblock In {IEEE Big Data} \bibAnnoteFile{wu2018bigdata} \bibitem[{Wu et~al.Wu, Kim, Giles}]{wu2019citeseerx} Wu, J., Kim, K., Giles, C.~L. . \newblock {CiteSeerX}: 20 years service scholarly big data. \newblock In Proceedings AIDR Conference \bibAnnoteFile{wu2019citeseerx} \bibitem[{Wu et~al.Wu, Williams, Chen, Khabsa, Caragea, Ororbia et~al.}]{wu2014iaai} Wu, J., Williams, K., Chen, H., Khabsa, M., Caragea, C., Ororbia, A., et~al. . \newblock {CiteSeerX}: {AI} digital library search engine. \newblock In Proceedings Twenty-Eighth {AAAI Conference Artificial Intelligence} \bibAnnoteFile{wu2014iaai} \bibitem[{Yang et~al.Yang, Yang, Dyer, He, Smola, Hovy}]{yang2016han} Yang, Z., Yang, D., Dyer, C., He, X., Smola, A.~J., Hovy, E.~H. . \newblock Hierarchical attention networks document classification. \newblock In The NAACL-HLT Conference \bibAnnoteFile{yang2016han} \bibitem[{Zhang Zhong}]{zhang2016improving} Zhang, H. Zhong, G. . \newblock Improving short text classification learning vector representations words hidden topics. \newblock Knowl.-Based Syst. 102, 76--86 \bibAnnoteFile{zhang2016improving} \bibitem[{Zhang et~al.Zhang, Zhao, LeCun}]{zhang2015nips} Zhang, X., Zhao, J., LeCun, Y. . \newblock Character-level convolutional networks text classification. \newblock In Proceedings NIPS Conference \bibAnnoteFile{zhang2015nips}",   This paper describes BUT-FIT's submission at SemEval-2020 Task 5: Modelling Causal Reasoning in Language: Detecting Counterfactuals. The challenge focused on detecting whether a given statement contains a counterfactual  and extracting both antecedent and consequent parts of the counterfactual from the text . We experimented with various state-of-the-art language representation models .  We found RoBERTa LRM to perform the best in both subtasks. We achieved the first place in both exact match and F1 for Subtask 2 and ranked second for Subtask 1.
"As cross-disciplinary study, combine general linguistics computational linguistic approach. Various types word embedding models proposed analyze large size corpora languages . By way illustration, word embeddings combined artificial neural networks reflect one aspect available language processing human mind. Nevertheless, innovative methods face difficulty ``purely data-driven approaches still struggle reach linguistic depth knowledge-driven predecessors. Bridging gap types approaches therefore important future research direction'' \citep[99]{dethlefs_context-sensitive_2014}. Hence, selected linguistically motivated classification words i.e., nominal classification , case study demonstrate knowledge provided linguistic theories concord information encoded basic statistical structures word embeddings. More specifically, selected Swedish since observations regard L1 L2 acquisition nominal classification systems Swedish controversial differ languages. First, monolingual children acquire Swedish grammatical gender nearly errors , considered rare comparison gender languages, ``children's acquisitional paths reported quite error-free"" \citep[214-217]{bohnacker_nominal_2004}. Moreover, gender assignment Swedish nouns via phonological form semantics generally considered unpredictable , makes observation even unexpected. Second, L1 acquisition display lack errors, L2 learners encounter difficulties, suggesting different strategies employed \citep[218]{bohnacker_nominal_2004}. Hence, existing linguistic analysis could provide additional perspectives computational approach help understand elements Swedish problematic terms grammatical gender perception. Moreover, matching performance artificial neural network linguistic observation made humans also represents insightful comparative study, since simulating one facet learning process brain artificial neural networks ``have become subject intense interest scientists spanning broad range disciplines including psychology, physics, mathematics, computer science, biology neurobiology \citep[69]{gopal_neural_1996}. Thus, propose following research questions: 1) Can word embedding model combined artificial neural networks interpret grammatical gender Swedish? 2) What types error made computational model explain errors linguistic perspective? Our experiment relies two main sources data, corpus Swedish raw sentences list nouns affiliated grammatical genders. The raw corpus used train word embedding model. The output model set vectors associated words corpus. The dictionary used filter non-noun words affiliate vector nouns grammatical genders. These word vectors affiliated grammatical genders used train neural network takes word vectors input determine grammatical genders output. The results network analyzed linguistic perspective. The contributions research summarized follow. First, formulates novel classification task evaluate word embeddings. Second, proposes computational approach compare previous linguistic observations Swedish. Finally, also provides in-depth linguistic analysis errors made classifier, i.e. neural network. With regard general structure paper,  introduces literature review grammatical gender computational models.  presents methodology data.  elaborate numerical results obtained neural network provide linguistics insight errors.  contains detailed answers two research questions. Finally,  summarizes findings conclusion. We examined performance current state-of-the-art language representation models subtasks found yet another NLP task benefits unsupervised pre-training. In cases, found RoBERTa model perform slightly better LRMs, results also stable. We ended first EM F1 Subtask 2 second Subtask 1."," We analyze the information provided by the word embeddings about the grammatical gender in Swedish. We wish that this paper may serve as one of the bridges to connect the methods of computational linguistics and general linguistics. Taking nominal classification in Swedish as a case study, we first show how the information about grammatical gender in language can be captured by word embedding models and artificial neural networks. Then, we match our results with previous linguistic hypotheses on assignment and usage of grammatical gender in Swedish and analyze the errors made by the computational model from a linguistic perspective."
"Large-scale generative language models received recent attention due high-quality open-ended text generation ability~. Generating texts LMs usually relies form random sampling. Pure sampling often leads incoherent low-quality texts , whereas greedy decoding leads excessive repetitions, another form low quality. The right decoding algorithm needed generate high-quality texts controlled attributes . We introduce mirostat,\footnote{The word mirostat derived mirum Latin surprise stat meaning control.} neural text decoding algorithm actively controls generative process maintain perplexity generated text certain desired value. Mirostat uses adaptive top- sampling algorithm actively tune value helps maintain overall perplexity text; recall top- sampling next word sampled top probable choices. Top- sampling several recent sampling methods motivated suppressing unreliable tail probability distribution trained LMs. Another sampling method top-, also known nucleus sampling, next word chosen top probable choices, smallest integer cumulative probability mass least . While top- sampling involves fixed number probable choices, top- sampling involves dynamic number choices based fixed value shows better statistical human-evaluated performance. For small values , sampling methods unfortunately repeat phrases generated text. This handled penalizing repetitions using appropriate temperature values adding diversity generated text . On hand, large values lead incoherent texts similar pure sampling. Although choosing appropriate values avoid repetition incoherence, involves ad hoc tuning parameters. Even fixed value , generated text varying statistical properties. Intriguingly, demonstrate via Example Appendix, small values certain perplexity statistic generated texts called surprise closely linked repetitions large values surprise linked incoherence. Perplexity statistical metric used evaluate quality neural text generation, closely related average surprise shown Fig. Appendix formalized Sec.. A large-scale human subject experiment showed human-evaluated quality closely related likelihood generated text fixed number tokens. In particular, reducing perplexity increases quality upto point quality starts dropping. This implies good control perplexity generated text would give direct control quality generated text . Generating texts appropriately chosen target perplexity value may maximize quality generated text. Ergo mirostat. \iffalse %%%%%%%%%%%%%%% Unfortunately, existing decoding methods top- top- sampling provide good control average surprise generated text, observed Ex.. On hand, proposed mirostat algorithm generate high-quality text controlling perplexity. \fi %%%%%%%%%%%%%%% Now summarize key contributions. Sec. shows theoretically cross-entropy hence perplexity grows top- top- sampling function respectively, previously unknown. Sec. introduces mirostat sampling, outputs texts predetermined target perplexity value. Although perplexity may fully capture quality text~, much literature discusses correlation quality~. Hence, algorithm control perplexity helps generate high-quality text. Sec. experimentally shows much fluctuation cross-entropy rates top- top- sampling function input parameters, hence unable control perplexity output text. Sec. shows repetition closely related perplexity generated texts, mostly independent sampling method, slightly dependent LM used. Sec. experimentally shows mirostat sampling avoids boredom confusion traps wide range target perplexity values. Sec. provides experiments human raters demonstrate mirostat efficacy fluency, coherence, overall quality. \subsection{Related Work} \paragraph{Sampling distorted probability distribution} Pure sampling LMs often leads incoherent text whereas greedy decoding leads repetitions. Distorting probability distributions, top-, top-, temperature sampling help improve quality generated texts, parameters properly tuned . Tuning methods, however, ad hoc provide good control statistics output. Our method uses statistics previously-generated tokens input generate next token, distorting probability distribution helps control overall statistics generated text. This ability control perplexity output key advantage method previous work. This, used relation perplexity human-evaluated quality observed , yield text better quality control. \paragraph{Controllable text generation} Controllable text generation oft focused semantics output text, LMs like CTRL , sampling algorithms like plug-and-play LM constrained sentence generation Metropolis-Hastings . Contrarily approach purely statistical, guiding decoder along desired statistical path addresses issues pure sampling greedy decoding. %A new model 1.63 billion parameters, CTRL, trained generate text based control word. %On hand, sampling algorithms like Plug Play Language Model Constrained sentence Generation Metropolis-Hastings work inference stage top pretrained language model control certain attributes generated text. %PPLM shows using attribute classifiers top pretrained language models helps control text generation . %CGMH uses Metropolis-Hastings sampling generate text certain constraints like appearance multiple keywords %%%%%%%%% : SeqGAN \paragraph{Quality-diversity tradeoff} %Distorting probability distributions decoding using Top-, top-, low-temperature sampling improve quality text, cost reduced diversity. Applications like question-answering demand high-quality generation, open-ended tasks story generation demand diversity too. propose variants beam search induce diversity generated text. However, observe tradeoff quality diversity; observe diversity closely related entropy whereas quality maximized certain range observed likelihood values fixed-length sentences. Our algorithm well-controls observed cross-entropy, observed likelihood per token generated text. Hence, maintaining observed cross-entropy certain range, ensure high-quality text generation. \paragraph{Repetitions} Greedy decoding LMs often lead texts excessive repetitions token- sentence-levels. Several techniques proposed address this. Token loss dynamic reweighting hypothesizes tokens difficult learn others reweighting tokens learning balance things reduce repetitions~. use repetition penalty decoding reduce repetition tokens. suggest cause repetitions flaw training objective use new objective gives less probability unlikely sequence including texts high repetitions. Variants top- sampling repetition penalty used reduce repetitions. Here, demonstrate near-linear relation repetitions observed cross-entropy directly control repetitions controlling observed cross-entropy. Our main contributions follows: approach computational linguistics, demonstrated linear word embedding model combined neural network capable capturing information grammatical gender Swedish accuracy . From linguistic approach, run error analysis regard errors generated neural network. The results show artificial neural network encounters difficulties cases polysemy, i.e., linguistic form may link different referents belong different part speech categories. Such phenomenon explained linguistic theories gender assignment, neuter nouns generally mass nouns, undergo conversion different part speech categories . Thus, additional tuning computational model direction expected improve performance. We wish paper may serve bridge connect field linguistics field computational linguistics currently divergent approaches toward linguistic data. By way illustration, show word embedding neural network applied answer research questions linguistic nature. Furthermore, linguistic analysis targeting errors model equivalently beneficial enhance computational model. Our study limited terms broadness. Although data rich, word embedding combined neural network represents relatively simple model, solely shows informative pure context information. A human carrying linguistic task activation kind linguistic context, also syntax, semantics, morphological associations, among others. Thus, testing required compare contribution different factors regard gender classification. Furthermore, applied one type word embedding model along one type neural network classifier. It would necessary investigate accuracy different combinations, verify type model provides precision regard task grammatical gender assignment. Finally, study involved one language, i.e., Swedish, unbalanced distribution gender among lexicon. Thus, future research equivalently aims including phylogenetically weighted sample languages scrutinize word embedding neural network reach level accuracy cross-linguistically."," Neural text decoding algorithms strongly influence the quality of texts generated using language models, but popular algorithms like top-$k$, top-$p$ , and temperature-based sampling may yield texts that have objectionable repetition or incoherence.  Although these methods generate high-quality text after ad hoc parameter tuning that depends on the language model and the length of generated text, not much is known about the control they provide over the statistics of the output.  This is important, however, since recent reports show that humans prefer when perplexity is neither too much nor too little and since we experimentally show that cross-entropy  has a near-linear relation with repetition. First we provide a theoretical analysis of perplexity in top-$k$, top-$p$, and temperature sampling, under Zipfian statistics. Then, we use this analysis to design a feedback-based adaptive top-$k$ text decoding algorithm called mirostat that generates text  with a predetermined target value of perplexity without any tuning. Experiments show that for low values of $k$ and $p$, perplexity drops significantly with generated text length and leads to excessive repetitions . Contrarily, for large values of $k$ and $p$, perplexity increases with generated text length and leads to incoherence . Mirostat avoids both traps. Specifically, we show that setting target perplexity value beyond a threshold yields negligible sentence-level repetitions. Experiments with human raters for fluency, coherence, and quality further verify our findings."
"\pheadNoSpace{Background} Text classification become fundamental building block modern information systems, increasing need able classify texts wide range languages. However, organizations target increasing number markets, challenging collect new task-specific training data new language supported. To overcome this, cross-lingual systems rely training data source language train model applied entirely different target languages , alleviating training bottleneck issues low-resource languages. Traditional cross-lingual text classification approaches often relied translation dictionaries, lexical knowledge graphs, parallel corpora find connections words phrases different languages . Recently, based deep neural approaches BERT , important advances learning joint multilingual representations self-supervised objectives . These enabled substantial progress cross-lingual training, mapping textual inputs different languages common vector representation space . With models Multilingual BERT , obtained vector representations English Thai language documents, instance, similar discuss similar matters. Still, recent empirical studies show representations bridge differences different languages. While possible invoke multilingual encoders train model English training data apply documents language Thai, model may work well applied Thai document representations, since latter likely diverge English representation distribution subtle ways. In work, propose semi-supervised adversarial perturbation framework encourages model robust towards divergence better adapt target language. Adversarial training method learn resist small adversarial perturbations added input maximize loss incurred neural networks . % Nevertheless, gains observed adversarial training previous work limited, merely invoked form monolingual regularization. Our results show adversarial training particularly fruitful cross-lingual framework also exploits unlabeled data via self-learning. \pheadWithSpace{Overview Contributions} Our model begins learning available source language samples, drawing multilingual encoder added adversarial perturbation. Without loss generality, following, assume English source language. After training English, subsequently, use model make predictions unlabeled non-English samples part samples high confidence prediction scores repurposed serve labeled examples next iteration adversarial training model converges. The adversarial perturbation improves robustness generalization regularizing model. At time, adversarial training makes tiny perturbations barely affect prediction result, perturbations words self-learning viewed inducing form code-switching, replaces original source language words potential nearby non-English word representations. Based combination adversarial training semi-supervised self-learning techniques, model evolves become robust regard differences languages. We demonstrate superiority framework Multilingual Document Classification comparison state-of-the-art baselines. Our study proceeds show method outperforms methods cross-lingual dialogue intent classification English Spanish Thai . This shows semi-supervised adversarial framework effective previous approaches cross-lingual transfer domain-specific tasks, based mix labeled unlabeled data via adversarial training multilingual representations. We provided theoretical explanation perplexity varies function input parameters popular top- top- neural text decoding algorithms, showing log perplexity varies nearly linearly function highly nonlinearly function . Building analysis, developed mirostat, neural text decoding algorithm directly controls perplexity generated text wide range text length. therefore several advantages. sampling algorithms. While top- top- provide good control statistics output, mirostat maintain perplexity generated text wide range text length. Notably, longer texts certain ranges input parameters, top- top- sampling fall boredom confusion traps cause low-quality texts; Mirostat avoids traps. Further, recent large-scale human evaluation neural generated text suggests human-judged text quality maximized certain range perplexity output: mirostat provides direct control stay perplexity range. There also implications data compression given Appendix. As takeaway, find mirostat target surprise around , produces varying lengths high-quality texts minimal repetitions. This corroborated experiments human raters. We analyze relation perplexity repetitions text: fixed model, repetitions vary linearly perplexity independent sampling method. We also find larger LMs less repetitions fixed perplexity. Future work would include theoretical analysis repetitions, boredom confusion traps, convergence properties mirostat."," In cross-lingual text classification, one seeks to exploit labeled data from one language to train a text classification model that can then be applied to a completely different language. Recent multilingual representation models have made it much easier to achieve this. Still, there may still be subtle differences between languages that are neglected when doing so. To address this, we present a semi-supervised adversarial training process that minimizes the maximal loss for label-preserving input perturbations. The resulting model then serves as a teacher to induce labels for unlabeled target language samples that can be used during further adversarial training, allowing us to gradually adapt our model to the target language. Compared with a number of strong baselines, we observe significant gains in effectiveness on document and intent classification for a diverse set of languages."
"%\gn{I set ""taclpubformat"" true auto-seeking works overleaf. It needs set back false submission.} Unsupervised grammar induction aims building formal device discovering syntactic structure natural language corpora. % \gn{Do Chomsky Pinker actually handle unsupervised grammar induction? I think so, maybe remove. If handle fine keep cites.} Within scope grammar induction, two main directions research: unsupervised constituency parsing, attempts discover underlying structure phrases, unsupervised dependency parsing, attempts discover underlying relations words. Early work induction syntactic structure focused learning phrase structure generally used variant probabilistic context-free grammars . In recent years, dependency grammars gained favor alternative syntactic formulation . Specifically, dependency model valence forms basis many modern approaches dependency induction. Most recent models grammar induction, PCFGs, DMVs, formulations, generally coupled models variety neural model use embeddings capture word similarities, improve flexibility model parameterization, . Notably, two different syntactic formalisms capture different views syntax. Phrase structure takes advantage abstracted recursive view language, dependency structure concretely focuses propensity particular words sentence relate each-other syntactically. However, attempts unsupervised grammar induction made marry two let benefit other. This precisely issue attempt tackle paper. As specific formalism allows us model formalisms once, turn lexicalized probabilistic context-free grammars . L-PCFGs borrow underlying machinery PCFGs expand grammar allowing rules include information lexical heads phrase, example shown \Cref{fig:lexicalized-phrase-structure-tree}. The head annotation L-PCFG provides lexical dependencies informative estimating probabilities generation rules. For example, probability VP[{\sc chasing}] VBZ[{\sc is}] VP[{\sc chasing}] much higher VP VBZ VP, ``chasing'' present participle % \gn{``present participant''? Do mean ``participle''?} . Historically, grammars mostly used supervised parsing, combined traditional count-based estimators rule probabilities . Within context, lexicalized grammar rules powerful, counts available sparse, thus required extensive smoothing %to interpolated general structural rules used standard PCFGs achieve competitive results . %\gn{Now introduced abbreviation ``L-PCFG'', make sure use mentions ``lexicalized PCFG'' below.} % While aim lexicalization inform syntax improve parsing, shown lexical information best exploited due sparsity corpus\footnote{Proper smoothing methods must applied achieve comparable results PCFGs.}. In paper, contend recent advances neural modeling, time return modeling lexical dependencies, specifically context unsupervised constituent-based grammar induction. We propose neural L-PCFGs parameter-sharing method alleviate sparsity problem lexicalized PCFGs. \Cref{fig:diagram} illustrates generation procedure neural L-PCFG. Different traditional lexicalized PCFGs, probabilities production rules independently parameterized, rather conditioned representations non-terminals, preterminals lexical items . Apart devising lexicalized production rules corresponding scoring function, also follow \citet{kim2019compound}'s compound PCFG model constituency parsing compound variables , enabling modeling continuous mixture grammar rules.% \footnote{In words, induce single PCFG, distribution family PCFGs.} We define efficiently train perform inference model using dynamic programming variational inference. Put together, expect result model effective, simultaneously induces phrase structure lexical dependencies,% \footnote{Note ``lexical dependencies'' referring unilexical dependencies head word child non-terminals, opposed bilexical dependencies two words .} whereas previous work focused one. Our empirical evaluation examines hypothesis, asking following question: {\em } Our experiments answer affirmative, %demonstrating model better performance baselines designed specially either dependency constituency parsing multiple settings. Importantly, detailed ablations show %\gn{curriculum learning initialization mentioned yet, much less central story written methods factorization. I'd try write concepts seem come suddenly, either mentioning directly here, mentioning earlier talk method. I'd slightly prefer former would OK latter.} methods factorization play important role performance neural L-PCFGs . Finally, qualitatively , find latent labels induced model align annotated gold non-terminals PTB. % While multilingual encoders enabled better cross-lingual learning, obtained models often attuned subtle differences model may encounter fed documents entirely new language. To adress this, paper proposes adversarial perturbation framework makes model robust enables iterative self-learning process allows model gradually adapt target language. We achieve new state-of-the-art results cross-lingual document intent classification demonstrate adversarial perturbation effective method improved classification accuracy without labeled training data target language."," In this paper we demonstrate that context free grammar  based methods for grammar induction benefit from modeling lexical dependencies. This contrasts to the most popular current methods for grammar induction, which focus on discovering either constituents or dependencies. Previous approaches to marry these two disparate syntactic formalisms  have been plagued by sparsity, making them unsuitable for unsupervised grammar induction.  However, in this work, we present novel neural models of lexicalized PCFGs which allow us to overcome sparsity problems and effectively induce both constituents and dependencies within a single model. Experiments demonstrate that this unified framework results in stronger results on both representations than achieved when modeling either formalism alone.\footnote{Code is available at \url{https://github.com/neulab/neural-lpcfg}.}  %\gn{Abstract is pretty dry. First maybe introduce unsupervised parsing, then explain that there are alternative formalisms: dependency and constituency, handeled by DMV and CFG respectively. Then explain that lexicalized PCFG was used in supervised parsing, but has  not been applied to unsupervised parsing, which we posit is due to sparsity. then introduce our method.} %Y% Unsupervised grammar induction aims at inducing probabilistic grammar rules from natural language corpora. Within the scope of grammar induction there are two main directions of research: unsupervised constituency parsing and unsupervised dependency parsing, on which directions dependency models with valence  and probabilistic context-free grammars  are two most successful models respectively. Lexicalized PCFGs provide a method to unify these two kinds of models, but were mostly used in supervised parsing, and its performance is largely restricted by the data-sparsity problem. In this paper, we propose neural lexicalized probabilistic context-free grammars  for jointly performing both unsupervised dependency and constituency parsing. Our Neural L-PCFGs are parameterized by several simple neural networks which score production rules by conditioning on the lexical head of the parent constituent. Key to the success of our approach is that the neural parameterization enables efficient parameter sharing, thereby alleviating the data-sparsity issues common to lexicalized PCFGs. %includes a set of Chomsky Normal Form  productions rules and their scores parameterized by neural networks. The scores of productions rules are conditioned on the head word of the parent constituent, which makes our model sensitive to lexical information; meanwhile, neural parameterization enables parameter sharing mechanism, thus alleviating the sparse-data problem.  %Y% Unlike prior work that pits constituencies and dependencies against each other, our novel formulation produces a single model which simultaneously achieves strong performance across both syntactic formulations. Experiments show that our model could achieve better results comparing to models designed specific for these two tasks. %The compound latent variable makes the L-PCFG more general. In experiments on unsupervised consitituency parsing and dependency parsing tasks, our model compete favorably against specialized models for those two tasks."
"% - Define QA, motivation. % - Statement current approaches limitations % - Statement paper % - Statement contributions The capability providing exact answers queries framed natural language questions significantly improve user experience many real world applications. Rather sifting lists retrieved documents, automatic QA systems surface exact answer query, thus reducing cognitive burden associated standard search task. This capability applicable extending conventional information retrieval systems also emergent use cases, open domain conversational AI systems . For enterprises, QA systems fast precise help unlock knowledge value large unstructured document collections. % need process paper references % https://arxiv.org/pdf/2004.04906.pdf % Across many practical usecases, QA task structured open domain QA answer must extracted relevant documents retrieved open corpus . Conventional methods open domain QA follow two-stage implementation - retriever returns subset relevant documents. Retrieval typically based sparse vector space models BM25 TF-IDF ; machine reading comprehension model identifies spans document contain answer. While sparse representations fast compute, rely exact keyword match, suffer vocabulary mismatch problem - scenarios vocabulary used express query different vocabulary used express concepts within documents. To address issues, recent studies proposed neural ranking retrieval methods , rely dense representations. % These approaches classified two broad groups based passage retrieval implemented. The first group uses sparse text representation methods retrieve set passages processed document reader extract answer spans. The challenge sparse models limited ability model query context may surface passages contextually similar query \colorbox{yellow}{ mention precision/recall issues}. The second set approaches explore use dense representations information retrieval ensure retrieved passage candidates contextually relevant \colorbox{yellow}{ref revision needed}. However, dense representations show significantly improved results, introduce additional complexity latency, limits practical application. For example, \citet{guu2020realm} require specialized MLM pretraining regime, well supervised fine-tuning step, obtain representations used retriever. Similarly \citet{karpukhin2020dense} use dual encoders learning dense representation queries documents target corpus. Each methods require additional infrastructure compute dense representation vectors documents target corpus well implement efficient similarity search run time. In addition, transformer-based architectures used dense representations unable process long sequences due self-attention operations scale quadratically sequence length. As result, models require documents indexed/stored small paragraphs. For many use cases, meeting requirements cost-intensive. These costs hard justify, given simpler methods yield comparable results . Furthermore, reader models applied domain-specific documents, fail counter-intuitive ways. It thus valuable offer visual interfaces support debugging sensemaking results . While several libraries exist explain NLP models, integrate interfaces help users make sense query expansion, retriever reader tasks. Collectively, challenges hamper experimentation QA systems integration QA models practitioner workflows. In work, introduce NeuralQA help address limitations. Our contributions summarized follows: Overall, NeuralQA complements line end-to-end applications improve QA system deployment provide visual interfaces understanding machine learning models . % It responds calls integration simple robust baselines significantly reduce complexity NLP systems, improving performance. % In addition, many deep neural approaches limited fixed input size making challenging use lengthy documents stage QA process. \colorbox{yellow}{ref}. % In practice full set steps entail QA extends beyond retrieval reading. They frequently include expansion + % % File emnlp2020.tex % %% Based style files ACL 2020, %% Based style files ACL 2018, NAACL 2018/19, %% Based style files ACL-2015, improvements %% taken NAACL-2016 style %% Based style files ACL-2014, were, turn, %% based ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009, %% EACL-2009, IJCNLP-2008... %% Based style files EACL 2006 %%e.agirre@ehu.es Sergi.Balari@uab.es %% ACL 08 Joakim Nivre Noah Smith \pdfoutput=1 \documentclass[11pt,a4paper]{article} % \usepackage[bookmarks=false]{hyperref} \usepackage{emnlp2020} \usepackage[ruled,vlined]{algorithm2e} \usepackage{url} % \usepackage[bookmarks=false]{hyperref} % \usepackage{hyperref} \usepackage{times} \usepackage{latexsym} \renewcommand{\UrlFont}{\ttfamily\small} \usepackage{graphicx} % This strictly necessary, may commented out, % improve layout manuscript, % typically save space. \usepackage{microtype} \aclfinalcopy % Uncomment line final submission %\def\aclpaperid{***} % Enter acl Paper ID %\setlength\titlebox{5cm} % You expand titlebox need extra space % show authors. Please make titlebox % smaller 5cm ; check % camera-ready version ask change back. \graphicspath{{figures/}} \newcommand\BibTeX{Bib\TeX} \title{{NeuralQA}: A Usable Library Question Answering Large Datasets} \author{ Victor Dibia \\ Cloudera Fast Forward Labs \\ \\} \date{} In paper, propose neural L-PCFG, neural parameterization method lexicalized PCFGs, unsupervised dependency parsing constituency parsing. We also provide variational inference method train model. By modeling representations together, approach outperforms methods specially designed either grammar formalism alone. Importantly, work also adds novel insights unsupervised grammar induction literature probing role factorizations initialization model performance. Different factorizations probability distribution lead dramatically different performance viewed playing important role inductive bias learning syntax. Additionally, others used pretrained word vectors before, show contain abstract syntactic information bias learning. Finally, scope one paper, results point several interesting potential roads forward, including study effectiveness jointly modeling constituency-dependency representations freer word order languages, whether distributed word presentations might provide even stronger syntactic signals grammar induction. \hao{ Despite demonstrated success lexical dependencies, noted unilexical dependencies, contrast bilexical dependencies, also consider dependencies head dependent words. Modeling dependencies would require marginalizing possible dependents span-head pair. In case, time complexity exhaustive dynamic programming one sentence would become , stands length sentence. Assuming enough parallel workers, time complexity reduced , still requires auxiliary space. In contrast, model runs . Assuming enough parallel workers, time complexity also reduced , still requires auxiliary space. These auxiliary data stored 32GB graphic card experiments , bilexical model cannot. There several potential methods side-step problem, including use sampling lieu dynamic programming, using heuristic methods prune grammar, designing acceleration methods GPU . }","   % Guidance on paper content for EMNLP demo % https://2020.emnlp.org/call-for-papers/demos  % What problem does the proposed system address? % Why is the system important and what is its impact? % What is the novel in the approach/technology on which this system is based? % Who is the target audience? % How does the system work? % How does it compare with existing systems? % How is the system licensed? % 6 pages  Existing tools for Question Answering  have challenges that limit their use in practice. They can be complex to set up or integrate with existing infrastructure, do not offer configurable interactive interfaces, and do not cover the full set of subtasks that frequently comprise the QA pipeline . To help address these issues, we introduce NeuralQA - a usable library for QA on large datasets. NeuralQA integrates well with existing infrastructure  and offers helpful defaults for QA subtasks. It introduces and implements contextual query expansion  using a masked language model  as well as relevant snippets ) - a method for condensing large documents into smaller passages that can be speedily processed by a document reader model. Finally, it offers a flexible user interface to support workflows for research explorations  and large scale search deployment.   Code and documentation for NeuralQA is available as open source on  \href{https://github.com/victordibia/neuralqa}{Github}.  % It is configurable via a yaml file which enables out of the box usage with minimal code.  % The library can be installed via the pip python package manager and is open sourced under the MIT license;   % Dense passage retrieval methods hold promise for improved precision/recall within QA system implementations but can introduce significant complexity which make them impractical.   % In this work, we report on findings from our experiments building NeuralQA - an end to end open source question answer  designed to address some of these challenges. We show how the candidate passage retrieval stage of the QA task can be improved with contextual query expansion on sparse representations of queries, whilst achieving  accuracy comparable to dense deep representations and at a fraction of the complexity. NeuralQA offers a flexible user interface,  and  support for multiple retriever indexes, interpretability modules for sensemaking, query expansion methods and document readers.It is released under the MIT license.  % We also introduce the legalCase dataset and benchmark results that demonstrate the value of our query enrichment approach for large domain-specific corpora.s   % [Andrew] - @Victor, not sure we'll be able to claim that we achieve ""comparable accuracy to dense representations"" unless we replicate their experimental setup on ALL of Wikipedia"
"} \end{table} %to learn unsupervised way true training data distribution, eventually generate new synthetic samples distribution. Probabilistic generative models leverage either explicit implicit density. In former case, density model defined solved explicitly, latter case model provides distribution without explicitly defining it. %Generative probabilistic models useful density estimation sample generation. %tractable vs intractable likelihoods \end{comment} Recent successes deep generative modeling representation learning led significant advances natural language generation , motivated increasing need understand derive meaning language. The research field text generation fundamental natural language processing aims produce realistic plausible textual content indistinguishable human-written text . Broadly speaking, goal predicting syntactically semantically correct sequence consecutive words given context achieved two steps first estimating distribution sentences given corpus, sampling novel realistic-looking sentences learnt distribution. Ideally, generated sentences preserve semantic syntactic properties real-world sentences, different training examples used estimate model . Language generation inherently complex task, requires considerable linguistic domain knowledge multiple levels, including syntax, semantics, morphology, phonology, pragmatics, etc. Moreover, texts generated fulfill communicative goal , provide support decision making, summarize content, translate languages, converse humans, make specific texts accessible, well entertain users encourage change behaviour. Therefore generated texts tailored specific audience terms appropriateness content terminology used , well fairness transparency reasons . For long time natural language generation models rule-based relied training shallow models sparse high dimensional features. With recent resurgence neural networks, neural networks based models text generation trained dense vector representations established unmatched prior performance reignited hopes machines able understand language seamlessly converse humans. Indeed, generating meaningful coherent texts pivotal many natural language processing tasks. Nevertheless, designing neural networks generate coherent text model long-term dependencies long challenge natural language generation due discrete nature text data. Beyond that, ability neural network models understand language ground textual concepts beyond picking shallow patterns data still remains limited. Finally, evaluation generative models natural language equally active challenging research area significant importance driving forward progress field. %centered around novel formulations, comprehensive & updated coverage neural network methods evaluation. In work formally define problem neural text generation particular contexts present diverse practical applications text generation Section . In Section include comprehensive overview deep learning methodologies neural model architectures employed literature neural network-based natural language generation. We review methods evaluation generated texts Section . Finally, Section conclude insights future perspectives regarding neural text generation evaluation. Given rapid evolution field research, hope current survery serve thorough overview present times neural-network based natural language generation evaluation anyone interested learning topics, provide reader up-to-date information latest research advances. Compared survey , overview comprehensive updated coverage neural network methods evaluation centered around novel problem definitions task formulations. %Introduction \subsection{Memory-based Architectures} %\subsubsection{Generic / Free-Text Generation Models} Although RNNs LSTMs trained predict next token sequence, memory small used mainly store information local context, allow models accurately recall facts past. Indeed, recurrent neural network memory degrades time . Parametric neural networks implicitly encapsulate memory weights, nevertheless hurts ability generalize across complex linguistic tasks . Attempts capture non-local dependencies language models aim enhance ability adapt changing environment dynamically update word probabilities based long-term context. %store/retrieve longer-span information. Improving neural language models external storage units done means introducing external memory component form soft attention mechanism , , allows focus specific parts input, explicit memory block implicitly captures dependencies word prediction , cache model added top pre-trained language model. Shared memory models reported improve attention based neural models . Integrated LSTM networks proposed alleviate practical engineering requirements LSTMs relying external memory units enhance memory capacity neural networks. Neural Turing Machines extend memory resources RNNs coupling addressable external memory bank read written . C-LSTMs combine CNN LSTM networks learn high-level sentence representations capture local features phrases global temporal sentence semantics. In context question answering, use long-term memory acting similar dynamic knowledge base read written proposed memory networks . Nevertheless, discrete model difficult train via backpropagation requires supervision layer network. The memory network architecture extended operate without supervision continuous space . Single-layer LSTM networks enhanced unbounded differentiable memory, yield comparable performance deep RNNs sentence transduction tasks machine translation . Memory based architectures incorporating stacked layers memories storing accessing intermediate representations sequence-to-sequence learning proposed . Dynamic memory networks used generate relevant answers question answering means episodic memories reasoned hierarchical recurrent sequence model. Memory architectures recurrent neural network language models compared . Stack-based memory access dynamically stores retrieves contextual information stack shown outperform sequential access fails capturing long term dependencies random memory access learner needs infer dependencies data absence structural biases. Instead monolithic model fit training examples, few-shot meta-learning scenario multiple task-specific models covering groups similar examples proposed . While on-going trend language modeling learn contextual representations ever larger datasets, alternative methods sample efficient leverage smaller amounts data represents next research frontier deep learning models. NN-LMs general framework allows augment pre-trained language model means linearly interpolating next word distribution k-nearest neighbors search. The approach helps memorize long-tail patterns explicitly drawing nearest neighbours text collection pre-trained embedding space rather modeling rare patterns implicitly model parameters. An additional memory component used store external simplification rules paraphrase database neural text simplification combination multi-layer multi-head attention Transformer architecture ; additional memory used recognize context output simplification rule. Neural semantic encoders augment neural network models evolving memory input sequence natural language understanding tasks including natural language inference, question answering, sentence classification, sentiment analysis machine translation. Relational memory adds interactions memory units via attention designed enhance reasoning abilities neural networks across sequential information. An external factual memory component incorporated neural pre-trained language model question answering . Finally, memory networks used generate scientific articles constraints entities human-written paper titles . %Can Unconditional Language Models Recover Arbitrary Sentences? % %\subsubsection{Conditional Text Generation Models} %\subsubsection{Constrained Text Generation Models} This work addresses modeling challenge Neural NER MRLs. We outlined two core modeling questions, namely choice input units; tokens morphemes, pipeline obtaining units; pipeline vs.\ hybrid. We deliver parallel token-morpheme NER corpus Modern Hebrew, assessing questions empirically morphologically rich-and-ambiguous environment. Our experiments show explicitly modeling morphological boundaries using morpheme-based models consistently leads improved NER performance, yet results extremely sensitive segmentation errors propagating pipeline. We thus propose hybrid architecture, NER precedes prunes morphological decomposition component, ultimate NER labeling takes place MD. This approach greatly outperforms standard pipeline, NER MD tasks realistic scenarios. Our analysis shows morpheme-based models generalize better unseen entity mentions across board, especially caused morphological composition. We deliver state-of-the-art results Hebrew NER morphological tagging, along benchmark encourage investigation interaction morphology NER."," % Qiaozhu Mei: Make the abstract more specific to the contribution and organization of this survey.   %Neural network-based generative models for natural language have become popular with the recent advancements in deep learning. Different techniques and architectures relying on neural networks have been used to generate text excerpts to various degrees of success, in a multitude of contexts that fulfil various user needs. While the field is rapidly evolving, there are still many open challenges to tackle. In this article we review the latest trends in neural network-based natural language generation and evaluation, outlining latest successes, open research challenges and limitations.      Recent advances in neural network-based generative modeling have reignited the hopes in having computer systems capable of seamlessly conversing with humans and able to understand natural language.    Neural architectures have been employed to generate text excerpts to various degrees of success, in a multitude of contexts and tasks that fulfil various user needs. Notably, high capacity deep learning models trained on large scale datasets demonstrate unparalleled abilities to learn patterns in the data even in the lack of explicit supervision signals, opening up a plethora of new possibilities regarding producing realistic and coherent texts. While the field of natural language generation is evolving rapidly, there are still many open challenges to address. In this survey we formally define and categorize the problem of natural language generation. We review particular application tasks that are instantiations of these general formulations, in which generating natural language is of practical importance. Next we include a comprehensive outline of methods and neural architectures employed for generating diverse texts. Nevertheless, there is no standard way to assess the quality of text produced by these generative models, which constitutes a serious  bottleneck towards the progress of the field. To this end, we also review current approaches to evaluating natural language generation systems. We hope this survey will provide an informative overview of formulations, methods, and assessments of neural natural language generation.       %that language models begin to learn these tasks without any explicit supervision       %have become popular with the recent advancements in deep learning. Different techniques and architectures relying on neural networks have been used to generate text excerpts to various degrees of success, in a multitude of contexts that fulfil various user needs. While the field is rapidly evolving, there are still many open challenges to tackle. In this article we review the latest trends in neural network-based natural language generation and evaluation, outlining latest successes, open research challenges and limitations.       %We hope this survey will serve as a quick and thorough review for anyone interested in the latest advances in deep learning for natural language generation and evaluation."
"Multi-task learning collection techniques intended improve generalization, strengthen latent representations enable domain adaptation within field machine learning . It applied feed-forward neural networks , decision trees , random forests , Gaussian Processes , support-vector machines and, recently, deep neural networks across broad range domains. This includes specific deep learning architectures MTL seq2seq models MTL transformers . It shown certain circumstances, well-crafted tasks, MTL help models achieve state-of-the-art performance range different tasks . It also shown, however, MTL extremely fragile sensitive selected tasks training process leads models significantly under-perform compared best single-task models . While MTL subject research multiple decades , still exist number unsolved problems, unexplored questions shortcomings production systems addressed within. This survey present condensed summary large library current MTL research applied natural language processing present set goals intended help highlight MTL problems strive solve next decade. Please note introduced automatic line number generation style file \LaTeXe. This help reviewers refer specific lines paper make comments. Please NOT refer line numbers paper removed style file final version accepted papers."," Multi-task learning  significantly pre-dates the deep learning era, and it has seen a resurgence in the past few years as researchers have been applying MTL to deep learning solutions for natural language tasks. While steady MTL research has always been present, there is a growing interest driven by the impressive successes published in the related fields of transfer learning and pre-training, such as BERT, and the release of new challenge problems, such as GLUE and the NLP Decathlon . These efforts place more focus on how weights are shared across networks, evaluate the re-usability of network components and identify use cases where MTL can significantly outperform single-task solutions. This paper strives to provide a comprehensive survey of the numerous recent MTL contributions to the field of natural language processing and provide a forum to focus efforts on the hardest unsolved problems in the next decade. While novel models that improve performance on NLP benchmarks are continually produced, lasting MTL challenges remain unsolved which could hold the key to better language understanding, knowledge discovery and natural language interfaces."
"Language models key component applications require generation coherent natural language text, including machine translation, speech recognition, abstractive text summarization, many others. For long time n-gram models dominated field due simplicity, efficiency scalability. However, recently neural models gained popularity, notably simple recurrent networks powerful models including . These models often include billions parameters shown well generalizing vast amounts data. However, adapt models different users , update models efficiently still remain challenge. When number users large, updates frequent, adapting large monolithic model becomes impractical necessitates use composite models components may updated separately. For reasons, class-based models still widely used different applications, particularly automatic speech recognition integrating external knowledge sources personalized entities language model crucial achieving accurate transcription: %where model size computing power often constrained: . Class-based models, however, require annotations order learn components/classes used limits applicability. Instead using classes, content class assumed similar way, e.g., entities type, boost scores individual phrases n-grams bias ASR search. Note type biasing applied WFST-based\footnote{Weighted finite-state transducers widely used speech recognition represent language models .} neural models. %Moreover, class-based approaches, due underlying factorization, explicitly generate classes conjunction word sequence, resulting learn fixed-size representation every biasing phrase separately. The ASR decoder uses attention mechanism interpolate representations result added decoder's input. As decoder needs attend individual phrase every step, scaling approach large number biasing phrases entities poses engineering challenge. propose nearest-neighbor LM use external data bias predictions, however, significant limits application type model, especially ASR domain. similar work aim solve similar problem. They use expectation-maximization method learn class-based model without requirement annotated data. However, method applies n-gram models make assumptions internal structure component models. In paper, take approach reminiscent class-based model use components whose elements expected used similar context. We call model-defined components defined respective models . Unlike class-based models, however, assign tags components. This allows us away one main shortcomings class-based models -- requirement annotated data. The main motivating idea method follows: given general generative language model components represented generative LMs, learn components useful, i.e. make better predictions general model. Additionally, proposed model learns, directly data, interpolate different components token, class-based approaches incapable due explicit factorization sequence classes words. Note approach require us assign semantic tags components, meaning implicit arises content. It worth noting many methods combining multiple full-sentence language model literature, name few. However, methods cannot applied entity models full-sentence models, therefore methods cannot solve problem seek address. The rest paper organized follows: Section, describe structure proposed model, training procedure detailed Section. In Section, present experimental results, Section conclude outline future work. In paper, performance \ac{RNN} based NMT model namely \ac{GRU} attention decoder presented dataset populated English-Malay translated emails circulated University. The model unable learn context source target language within input text even presence attention mechanism. Thus, different approach splitting input text contextual content used. General purpose trained model doesn perform well specific application. Thus, need develop application oriented trained model populated application specific vocabulary. The model using regional email vocabulary showed 10-20 BLUE score better google translate. The model unable learn source input contains bilingual text. Thus, need update general translators multilingual blended input text. In paper, studied performance \ac{RNN} based NMT model namely \ac{GRU} attention decoder dataset English-Malay translated emails circulated University. We observed model unable learn context source target language within input text even presence attention mechanism. Thus, need efforts different approach dataset multiple contexts. We observed splitting input text contextual content better RNN. We observed general purpose trained model perform well specific application.Thus, need develop application oriented trained model. The model using regional email vocabulary showed 10-20 BLUE score better google translate. We also observed model unable learn source input contains bilingual text. Thus, need new neural network multilingual input text. \clearpage"," Decomposing models into multiple components is critically important in many applications such as             language modeling    as it enables adapting individual components separately             and biasing of some components to the user's personal preferences. Conventionally, contextual and personalized adaptation for language models, are achieved through class-based factorization, 	which requires class-annotated data, or through biasing to individual phrases which is limited in scale.  In this paper, we propose a system that combines model-defined components, by learning when to activate the generation process from each individual component,  	and how to combine probability distributions from each component, directly from unlabeled text data."
"Free text clinical notes contain rich narrative patient interactions healthcare system. However, strengths weaknesses clinical notes lie diversity natural language. Due flexibility documentation, notes contain information omitted structured clinical record, language often hard parse consistent structured format. For example, refer temperature, temperament, viral infection, Chronic Obstructive Lung Disease. By matter preference, doctor refer patient 101 F temperature , , . In order transform text unified structured format useful downstream applications, EHR text mining often involves recognizing spans representing concepts mapping spans common vocabulary . We refer steps together clinical entity extraction. An example two-step process shown Figure 1. Typically, medicine, concepts mapped terms Unified Medical Language System , term denoted Concept Unique Identifier ~. Normalized clinical entities used direct input retrospective analyses additional source information addressing confounders. For example, clinical entities help learn adverse drug events health knowledge graph ~; point-of-care, e.g. step matching patients relevant clinical trials ~. %Machine learning researchers often struggle circumvent missing crucial information patient record, problem better clinical entity extraction could also address. Additionally, causal inference reinforcement learning must account unobserved confounders affecting medical intervention patient outcome~, information often contained normalized clinical entities. Structured text data could reveal relevant patient history additional interventions omitted structured clinical record. Because normalizing entities condenses high-dimensional raw text lower-dimensional space collapsing synonyms, relevant factors normalized entities could plausibly integrated model's state space. In similar vein, \citet{Oakden-Rayner2020} found machine learning models trained medical images suffer hidden stratification, performance depends unknown confounders. In particular, authors found models predicting pneumothorax X-rays partially picking previously placed chest drains. The presence chest drains often noted accompanying radiology notes. As before, finding clinical entities associated abnormally high low performance could identify unexpected confounders. Such multimodal sanity checks particularly important given high-stakes nature machine learning healthcare. Despite importance clinical entity extraction range tasks, still remains largely open problem, clinical research often resort manual chart review high-fidelity data, tedious difficult scale. While significant, concerted effort clinical natural language processing community bridge gap, performance falters due lack sufficient annotated training data, huge space clinical entities map to, known issues medical vocabularies. \subsection*{Contributions} In work, investigate, quantify performance of, indicate areas improvements current state normalization analyzing performance Medical Concept Normalization Corpus~. We focus subset CUIs UMLS: SNOMED Clinical Terms, consist 400,000 common clinical concepts; RxNorm, comprehensive clinical drug vocabulary~. We examine top systems clinical entity normalization 2019 National NLP Clinical Challenges Shared Task well two widely used end-to-end open-source systems, cTAKES MetaMap~. These analyses reveal holes performance, underscore need future dataset creation method development space, highlight need new evaluation framework. In description MCN corpus, \citet{Luo2019} recounted corpus annotation effort hindered several issues including imperfections UMLS. Therefore, additionally develop new annotation framework address identified issues. We demonstrate new schema adjusts issues allows flexible evaluation end-to-end systems. \subsection*{Generalizable Insights Machine Learning Context Healthcare} Due utility, clinical entity extraction widely used initial step many machine learning healthcare pipelines. As field increasingly turns multimodal data, clinical text become continuously important component ML systems. In work, show performance across normalization end-to-end systems differ significantly various subsets data. Given observed heterogeneity performance, incredibly important users systems consider well extract concepts relevant specific use cases. Unfortunately, full evaluation systems currently unachievable since publicly available reference standard end-to-end benchmarking exist. Such reference standards crucial progress fields machine learning, e.g. computer vision ImageNet~. In work, propose recommendations clinical NLP community regarding path forward creation reference standard. We detail robust annotation framework construction allows end-to-end evaluation reduces ambiguity annotators. In paper, proposed novel method compose separately trained models, including personalized models, general generative language model. We showed method effective learning composition directly data without relying annotations. While evaluate approach language modeling tasks, believe approach applied many sequence-generating applications natural language processing. In future, plan integrate model directly ASR decoder using end-to-end models LAS RNN-T . We also want explore applications, machine translation."," %   Summary of the article.  Be sure to highlight how the work %   contributes to our understanding of machine learning and healthcare. Clinical studies often require understanding elements of a patient narrative that exist only in free text clinical notes. To transform notes into structured data for downstream use, these elements are commonly extracted and normalized to medical vocabularies. In this work, we audit the performance of and indicate areas of improvement for state-of-the-art systems. We find that high task accuracies for clinical entity normalization systems on the 2019 n2c2 Shared Task are misleading, and underlying performance is still brittle. Normalization accuracy is high for common concepts , but much lower for concepts unseen in training data . We demonstrate that current approaches are hindered in part by inconsistencies in medical vocabularies, limitations of existing labeling schemas, and narrow evaluation techniques. We reformulate the annotation framework for clinical entity extraction to factor in these issues to allow for robust end-to-end system benchmarking. We evaluate concordance of annotations from our new framework between two annotators and achieve a Jaccard similarity of 0.73 for entity recognition and an agreement of 0.83 for entity normalization. We propose a path forward to address the demonstrated need for the creation of a reference standard to spur method development in entity recognition and normalization."
"% \cx{Reduce paragraph half, merge 2nd para one paragraph overviews progress Dense Retrieval.} Many language systems rely text retrieval first step find relevant information. For example, search ranking~, open domain question answering ~, fact verification~ first retrieve relevant documents later stage reranking, machine reading, reasoning models. All later-stage models enjoy advancements deep learning techniques~, while, first stage retrieval still mainly relies matching discrete bag-of-words, e.g., BM25, become bottleneck many systems~. %Due intrinsic challenges vocabulary mismatch~, sparse retrieval inevitably introduces noisy information often becomes bottleneck many systems~. Dense Retrieval aims overcome sparse retrieval bottleneck matching texts continuous representation space learned via deep neural networks~. It many desired properties: fully learnable representation, easy integration pretraining, efficiency support approximate nearest neighbor search~. These make dense retrieval intriguing potential choice fundamentally overcome intrinsic limitations sparse retrieval, example, vocabulary mismatch~. A key challenge DR construct proper negative instances representation learning~. Unlike reranking negatives naturally irrelevant documents previous retrieval stages, first stage retrieval, DR models distinguish relevant documents irrelevant ones entire corpus. As illustrated Fig., global negatives quite different negatives retrieved sparse models. % \cx{discuss NCE method representation learning, recent DR methods try them, still performing situation.} Recent research explored various ways construct negative training instances dense retrieval~., e.g., using contrastive learning~ select hard negatives current recent mini-batches. However, observed recent research~, in-batch local negatives, though effective learning word visual representations, significantly better spare-retrieved negatives representation learning dense retrieval. In addition, accuracy dense retrieval models often underperform BM25, especially documents~. In paper, first theoretically analyze convergence dense retrieval training negative sampling. % present theoretical analysis negative instance construction dense retrieval. Using variance reduction framework~, show that, conditions commonly met dense retrieval, local in-batch negatives lead diminishing gradient norms, resulted high stochastic gradient variances slow training convergence --- local negative sampling bottleneck dense retrieval's effectiveness. Based analysis, propose Approximate nearest neighbor Negative Contrastive Estimation , new contrastive representation learning mechanism dense retrieval. Instead random in-batch local negatives, ANCE constructs global negatives using being-optimized DR model retrieve entire corpus. This fundamentally aligns distribution negative samples training irrelevant documents separate testing. From variance reduction point view, ANCE negatives lift upper bound per instance gradient norm, reduce variance stochastic gradient estimation, lead faster learning convergence. % This fundamentally eliminates discrepancy negative instances used train DR models irrelevant documents models face deployed. Our theoretical analysis also suggests % ANCE negatives bigger gradient norms lead faster convergence. We implement ANCE using asynchronously updated ANN index corpus representation. Similar \citet{guu2020realm}, maintain Inferencer parallelly computes document encodings recent checkpoint optimized DR model, refresh ANN index used negative sampling finishes, keep model training. % keep model training. % Once entire corpus encoded, inferencer refresh ANN index construct up-to-date ANCE negatives asynchronously keep model training process. % This implementation requires trainer-inference communication shared file system employed contrastive learning scenarios. Our experiments demonstrate advantage ANCE three text retrieval scenarios: standard web search~, OpenQA~, commercial search engine's retrieval system. % In scenarios, vanilla BERT-Siamese DR model, trained ANCE, outperforms previous dense retrieval models also state-of-the-art pretraining based sparse retrieval models. We also empirically validate theory gradient norms ANCE sampled negatives much bigger local negatives thus improve convergence dense retrieval models. Our code trained models available \url{https://aka.ms/ance}. % \footnote{Code trained models supplementary material open-sourced.} Medical notes offer us incredibly rich view patient narratives, remained largely untapped resource clinical machine learning research. Clinical studies often resort manual chart review, time-consuming difficult scale. While automated concept extraction systems provide way exploit clinical text, demonstrated work, cannot yet reliably deployed. Developing truly robust accurate clinical entity recognition normalization algorithms require substantially labeled training data automated evaluation metrics account subtlety ambiguity task. We present new annotation framework small-scale annotated dataset evaluation end-to-end concept recognition normalization. The time ripe field invest substantially larger labeled training sets spur new machine learning approaches clinical entity recognition normalization. Future work consider incorporating new annotation framework basis data set. \acks{The authors thank Irene Chen Divya Gopinath helpful comments manuscript, thank Noemie Elhadad, Anna Rumshisky, Ozlem Uzuner valuable conversations perspective clinical annotation process. They would also like thank organizers 2019 n2c2 Workshop sharing outputs top systems analysis.}"," Conducting text retrieval in a dense representation space has many intriguing advantages. Yet the end-to-end learned dense retrieval  often underperforms word-based sparse retrieval. In this paper, we first theoretically show the learning bottleneck of dense retrieval is due to the domination of uninformative negatives sampled locally in batch, which yield diminishing gradient norms, large stochastic gradient variances, and slow learning convergence. We then propose Approximate nearest neighbor Negative Contrastive Learning , a learning mechanism that selects hard training negatives globally from the entire corpus, using an asynchronously updated ANN index. Our experiments demonstrate the effectiveness of ANCE on web search, question answering, and in a commercial search environment, showing ANCE dot-product retrieval nearly matches the accuracy of BERT-based cascade IR pipeline, while being 100x more efficient.  % We also empirically validate our theory that ANCE better approximates the oracle gradient-norm based importance sampling. % , thus improves the convergence. % of stochastic training."
"In traditional web search, given query, documents retrieved large candidate corpus based relevance score usually term based metrics BM25. Recently, fast rapid progress neural text embedding deep contextualized word representations ELMo BERT , researchers begin focus applying deep neural network models generate document query embedding latent space separately based raw features retrieve documents according vector similarities. These methods improve semantic understanding besides term match achieve better retrieve quality. However, industrial environment, due complexity natural language diversity user query expressions, web search retrieval needs input information better document/query understanding term matching, promotion deep model architecture, especially long tail queries appear times never before. However, almost existing deep models IR take information document raw text input. They may provide better query-document semantic understanding due progress model, can't provide additional information matching. Besides, ~\citet{guo2016deep} claim Information Retrieval tasks different Nature Language Process tasks, important focus exact matching former learning text embeddings latter. These inspires us involve information besides query documents better match semantic understanding modeling. Typically industrial search engine, document information could come several sources anchor, url, title click,etc. Among click stream proved one important features directly indicates user feedbacks. Thus provides us feasibility get potential information specific document understanding similar documents co-click relationship, e.g. two documents clicked query history indicates similarity respects. Using co-click information deep search model trivial. Because much noise, inconsistent need accurate information term semantic matching. The noise co-click relations mainly lies following aspects: % \para{i)False clicks:}user false clicks may connect irrelevant queries documents. \para{ii)Multi-intentions:}one document may contain many aspects users may interest specific part them. As result users may click document different search intentions. E.g. one document whose text ""amazing mens gothic t-shirts black rose"" may clicked users interested two different intentions: ""Gothic T-shirt"" ""Black rose T-shirt"". Thus, information introduced historical click may become noise document searched query belonging another intention. As result, co-click documents involved click ""gothic t-shirts"" would noise query ""black rose t-shirts"". This kind multi-intention noise could ignored traditional term match methods nature, hurt semantic deep encoding model proper handled. \para{iii)Semantic Difference:}even two single-intention documents click, might differences semantics. E.g. two documents themes ""How high Mount Everest"" ""How many countries climbed Everest"" may click ""Mount Everest"". But encoding one document, completely introducing text information one cause vector erroneously shifting semantic space, need feature extraction noise eliminating. In paper, focus web-scale web search problem industrial environment aim enriching document text co-click relations improve retrieve quality. We first build web-scale co-click graph based real click log extract neighbours document complementary information. Then propose siamese deep model MIRA based Bert graph neural networks two-factor mechanism encode queries documents neighbours continuous vectors. Our key challenge lies two aspects: i)how effectively extract information co-click graph billions nodes eliminating noises. ii) scale training online serving graph neural based embedding. To handle co-click noise problem, firstly split clicks document different intention groups based term Jaccard Similarity build Multi-Intention Co-Click Graph, document represented several nodes node contains clicks belonging one intention. As result, one node could reached intention clicks thus avoiding cross-intention information. Secondly, propose use graph attention networks two-factor attention mechanism help precisely extract neighbour information eliminating false click semantic difference noise. Specifically, design interaction attention factor measuring vectors semantic correlation another dot product factor measuring vectors term match correlation. As scalability challenge, instead using query-document bipartite graph, involve document nodes graph modeling, alleviates need expensive real-time query neighbours search online. Furthermore, instead using transductive training need multiplying feature matrices powers full graph Laplacian, use inductive training method GCN, sample neighbour nodes document neighbour subgraph, thus dramatically reducing training inference cost. %split clicks document different groups based token jaccard similarity. Then instead using one node represent document co-click graph, uses multi nodes We demonstrate effectiveness efficiency proposed framework one public web search dataset collocted SouGou one private web-scale click data set collected real search engine Bing. The offline experiments show proposed model significantly outperforms various baselines. Our contributions paper include: ANCE fundamentally eliminates discrepancy representation learning texts usages dense retrieval. Our ANCE trained dense retrieval model, vanilla BERT-Siamese, convincingly outperforms dense retrieval sparse retrieval baselines large scale document retrieval passage retrieval experiments. It nearly matches ranking accuracy state-of-the-art cascade sparse retrieval BERT reranking pipeline. More importantly, advantages achieved standard transformer encoder 1\ online inference latency, using simple dot-product ANCE-learned representation space. In paper, first provide theoretical analyses convergence representation learning dense retrieval. We show common conditions text retrieval, local negatives used DR training uninformative, yield low gradient norms, contribute little learning convergence. We propose ANCE eliminate bottleneck constructing training negatives globally entire corpus. Our experiments demonstrate advantage ANCE web search, OpenQA, production system commercial search engine. Our studies empirically validate theory ANCE negatives much bigger gradient norms, reduce stochastic gradient variance, improve training convergence.","  We study the problem of deep recall model in industrial web search, which is, given a user query, retrieve hundreds of most relevance documents from billions of candidates. The common framework is to train two encoding models  based on neural embedding which learn the distributed representations of queries and documents separately and match them in the latent semantic space. However, all the exiting encoding models only leverage the information of the document itself, which is often not sufficient in practice when  matching with query terms, especially for the hard tail queries. Meanwhile, It has been proved that the click-through logs over query-document pairs in real search engine provide rich information for multiple tasks in information retrieval. Inspired by this, we aim to leverage the additional information for each document from its co-click neighbours to help document retrieval. The challenges include how to effectively extract  information and eliminate noise when involving co-click  information in deep model while meet the demands of billion-scale data size for real time online inference.  To handle the  noise in co-click relations, we firstly propose a web-scale Multi-Intention Co-click document Graph which builds the co-click connections between documents on click intention level but not on document level, and it is scalable to billions of document nodes based real search engine logs. Then we present an encoding framework based on Bert and Graph Attention Networks  which leverages a two-factor attention mechanism to aggregate neighbours and  can effectively handle the large amount of noise in the co-click relations. To meet the online latency requirements, we only involve neighbour information in document side whose vectors could be pre-built offline, and keep the query encoding only depends on its own text, which can save  the time-consuming  query neighbor search in real time serving. We conduct extensive offline experiments on both public dataset and private web-scale dataset from two major commercial search engines demonstrating the effectiveness and scalability of the proposed method compared with several baselines. And a further case study reveals that co-click relations mainly help improve web search quality from two aspects: key concept enhancing and query term complementary.  %categorizing and grouping the clicks of each document into different intentions when building the graph, and the two-factor attention mechanisms in the graph neural network, our proposed method can effectively handle the large amount of noise in the co-click graph. By restricting the co-click information only to the document side, we can avoid the expensive graph neighbor search time for online serving. We conduct offline experiments on both public dataset and private web-scale dataset from a major commercial search engine demonstrate the effectiveness and scalability of the proposed method compared with several baselines. And a online A/B test in production environment further shows our proposed model improve search engine revenue."
"In real world, complex systems always related multiple types objects relations. Such systems could effectively abstracted via heterogeneous information networks , different types nodes connected unique edges . Hence, compared homogeneous networks possess single type nodes, HINs provide richer tool model real-life issues. In order mine abundant information behind HIN, network representation learning, also known network embedding learning, embeds network low-dimensional space, drawn lots interests researchers. Classical network embedding models like DeepWalk, LINE node2vec devised homogeneous network using random walks capture structural information networks. However, methods lacking capability expressing HIN. Hence, models designed specifically HINs proposed. They basically based metapath, sequence node types edge types between. To leverage relationship nodes metapaths, different mechanisms proposed, example, heterogeneous SkipGram, proximity distance Hardmard function. Nevertheless, heterogeneous models' performances confront bottleneck due limited ability metapath capturing features HIN. Recently, graph neural networks investigated thoroughly, showing promising results modeling structural information network. GNNs usually empowered complex encoders, basically deep neural networks like CNN, could explore neighborhood structure instead path, thus improving performance representing HIN. However, train deep model HIN often time-consuming requires train whole model every specific task, leading inefficiency. To address issue, inspired recent advance pre-training framework, propose pre-train model large datasets first place. And specific downstream task specific dataset like DBLP, use fine-tuning technique minimal task-specific parameters, improve model efficiency effectiveness. The two-stage framework exploring features HIN named \mtv paper. In specific, pre-training stage, inspired BERT, adopt deep bi-directional transformers train dataset. Thus need transform node's neighborhood sequence. We first measure rankings nodes HIN based degree betweenness centrality. Then use ranking-based BFS strategy generate sequence, is, always selecting closest high-ranking nodes form sequence. Afterwards prepare input representation trained, combination token, segment, type, ranking position embeddings. Note paper, use type embeddings indicate type information node. During pre-training stage, adopt two strategies reduce parameters improve model efficiency, i.e., factorized embedding parameterization cross-layer parameter sharing. We design two tasks pre-train \mtv. One masked node modeling task, similar masked language modeling mode. In task, certain percentage nodes masked need predict masked nodes. The adjacent node prediction task could capture relationship nodes. Given node sequence , aim predict whether node sequence adjacent node. Notice operation applies deep bi-directional transformers node sequence actually variant GNN aggregating method, transformer layers could regarded deep neural networks. We would verify bi-directional transformer layers outperform traditional deep neural networks like CNN, LSTM attention mechanism ablation analysis. During fine-tuning stage, choose four benchmark downstream tasks, i.e, link prediction, similarity search, node classification node clustering. In link prediction similarity search, use node sequence pairs input, identifying whether link two nodes measuring similarity two nodes, respectively. In node classification node clustering tasks, use one single node sequence input, employing softmax layer classification k-means algorithm clustering, respectively. Our model \mtv advances state art downstream tasks consistently significantly. We verify model's efficiency alternatives trained randomly updated initial parameters, pre-trained parameters could directly applied tasks datasets. Our major contribution could summarized four components: The rest paper organized follows. In Section introduce related work, justify intuitions method theoretical analysis Section. Next, conduct experimental studies downstream tasks along ablation analysis Section. Finally, conclude findings Section. In paper tackled challenging problem understanding customer addresses e-commerce Indian context. We listed errors commonly made customers proposed methodologies pre-process addresses based combination edit distance phonetic algorithms. We formulated compared different approaches based Word2Vec, Bi-directional LSTM RoBERTa respect sub-region classification task. Evaluation approaches done North South Indian addresses basis accuracy scores. We showed pre-training RoBERTa large address dataset fine-tuning classification outperforms approaches datasets. Pre-training Bi-LSTM based models using downstream task possible slow compared BERT variants. Recent research highlights BERT models faster train capture context better compared Bi-LSTM based models resulting state-of-the-art-performance benchmark NLP datasets. This motivated us use RoBERTa pre-training large address dataset subsequently fine-tuning it. As part future work, experiment different tokenization strategies like WordPiece SentencePiece tokenizing addresses. We also pre-train variants BERT compare based perplexity score. Such models generalize better situations labelled data limited like address geo-coding. By framing problem parsing address language modelling task, paper presents first line research using recent NLP techniques. The deep contextual address embeddings obtained RoBERTa model used solve multiple problems domain Supply Chain Management."," To explore heterogeneous information networks , network representation learning  is proposed, which represents a network in a low-dimension space. Recently, graph neural networks  have drawn a lot of attention which are very expressive for mining a HIN, while they suffer from low efficiency issue. In this paper, we propose a pre-training and fine-tuning framework \mtv to capture the features of a HIN. Unlike traditional GNNs that have to train the whole model for each downstream task, \mtv only needs to fine-tune the model using the pre-trained parameters and minimal extra task-specific parameters, thus improving the model efficiency and effectiveness. Specifically, in pre-training phase, we first use a ranking-based BFS strategy to form the input node sequence. Then inspired by BERT, we adopt deep bi-directional transformer encoders to train the model, which is a variant of GNN aggregator that is more powerful than traditional deep neural networks like CNN and LSTM. The model is pre-trained based on two tasks, i.e., masked node modeling  and adjacent node prediction . Additionally, we leverage factorized embedding parameterization and cross-layer parameter sharing to reduce the parameters. In fine-tuning stage, we choose four benchmark downstream tasks, i.e., link prediction, similarity search, node classification and node clustering. We use node sequence pairs as input for link prediction and similarity search, and a single node sequence as input for node classification and clustering.  The experimental results of the above tasks on four real-world datasets verify the advancement of \mtv, as it outperforms state-of-the-art alternatives consistently and significantly."
"Visual Dialogue task requires agent answer series questions grounded image, demanding agent reason visual content dialogue history. There two kinds typical approaches task : discriminative generative. Discriminative approach learns select best response candidate list, generative approach may generate new responses provided pre-constructed repository. The discriminative approach relatively easier since grammaticality accuracy guaranteed human-written responses. However, retrieved responses limited capacity pre-constructed repository. Even best matched response may exactly appropriate since cases tailored on-going questions . Therefore, generative ability crucial achieve human-like conversation synthesizing factual flexible responses accordingly. The typical solution generative visual dialogue system based encoder-decoder framework . The encoder aims capture semantics image, question dialogue history embeddings, decoder decodes embeddings response recurrent neural networks . Due difficulty generation, majority previous works focused designing comprehensive encoder structures make use different aspects information input. Though methods achieve promising improvement, still obvious limitations, generating inaccurate details repetitive words phrases. To tackle problems, propose adaptively incorporate detailed information encoder generating word decoding process. Specifically, propose recurrent Deliberation, Abandon Memory module, novel architecture generative decoder address two issues. As shown Figure , one hand, DAM incorporates global information response-level keep semantic coherence. On hand, DAM pays attention capture related unique details word-level designing Deliberation Unit guided current generated word. To reduce repetition, devise Abandon Unit select unique information current word. In end, Memory Unit integrates derived word-level response-level semantics memory state word generation, contributes unification semantic coherence richness details. With recurrent connections DAM cells inspired LSTM , network capable generating visual-grounded details progressive manner remarkably eliminates repetition coverage control. Note DAM universal architecture combined existing visual dialogue models adapting Deliberation Unit corresponding encoder. To show effectiveness DAM, propose three models combining DAM three typical visual dialogue encoders, including Late Fusion encoder general feature fusion, Memory Network encoder dialogue history reasoning, DualVD encoder visual-semantic image understanding. We show performance baseline models consistently improved combining DAM. %The current generated word comprehensive %retrieve generated-word-aware detailed information form %Therefore, coordinating generated-word semantics detailed input information retrieved generated-word-relevant question main method address two issues decoding step. %Therefore, important incorporate question-relevant information input based current generated word address two issues decoding step. %when generate word decode step, %It's great important incorporate question-relevant information input %decoding step based current generated word. %We believe essential accuracy details capturing question-relevant information based current generated word question decoding step . In paper, propose recurrent Deliberation, Abandon Memory model, novel architecture generative decoder address two issues. %As shown Figure , DAM incorporates global information response level keep coherence, also pays attention unique details word-level. Above all, model consists three units perform decoding step: deliberation unit distincts updates information knowledge base current step, guided question current generated words; abandon unit selectively forgets redundant information keeping discriminative information word-level decoding current word; memory unit integrates derived word-level response-level semantics input information memory state word generation, used track control coverage source information. %With recurrent connections DAM modules like LSTM , network capable generating visual-grounded details progressive manner remarkably eliminate repetition coverage control. Note DAM universal architecture combined existing visual dialogue models adapting deliberation unit corresponding encoder. To show effectiveness DAM, propose three models combining DAM three typical visual dialogue encoder, including Late Fusion encoder general feature fusion, Memory Network encoder dialogue history reasoning, DualVD encoder visual-semantic image understanding. We show performance baseline models consistently improved combining DAM. %introducing word-level information, incorporates essential input information generated word, response-level information together %to generate detailed less repetitive responses %introducing word-level semantics %aiming generate detailed less repetitive responses introducing word-level semantics The main contributions summarized follows: We propose novel generative decoder DAM generate detailed less repetitive responses. DAM contains compositive structure leverages complementary information response-level word-level, guarantees accuracy richness responses. DAM universal cooperate existing visual dialogue encoders constraining information selection mode adapt different encoder structures. We demonstrate module's capability, generality interpretability VisDial v1.0 dataset. DAM consistently improves performance existing models achieves new state-of-the-art 60.93\% NDCG generative task. %More importantly, module compact, requires less 25\% extra size comparing SOTA baseline models. % DAM novel architecture moves away decoding monolithic visual-dialogue embedding towards design encourages decoding adaptive hierarchical semantic embedding. DAM generic integrated existing visual dialogue models transferring abilities semantic understanding encoder decoder. We demonstrate module's strength, generality interpretability VisDial v1.0 dataset, achieving new state-of-the-art 60.93\% NDCG generative task. More importantly, module compact, particularly requiring less ?\% extra size baseline models achieve strong results. %Problems combine image language become popular Artificial Intelligence Research, Image Captioning , Visual Question Answering Visual Dialogue . Under promising development research vision language, visual dialogue task requires agent answer series questions image attract lot attention work based visual dialogue task. %In visual dialogue, neural network based encoder-decoder framework widely used previous work . The decoder summarized two mode: discriminative decoder generative decoder. Almost previous work focuses study encoder assigns similarity calculation discriminative decoder LSTM generative decoder. What's more, discriminative decoder often uesd many works good retrieval performance. It cause two problems, one unbalance encoder decoder, lacks interaction encoder decoder, also lacks modeling decoder cause low decoding ability, especially generative decoder. The another limitation generalization performance, lacks study generative decoder good practicality options answers reply questions real world. This work focuses building stronger generative decoder proposing novel framework generative decoder visual dialogue task. %Deliberation typical ability humans. proposed deliberation networks field natural language processing. Considering situation: When answering question: xxx, first grasp whole image dialogue history mind generation word utter, re-see image re-think dialogue history thus focus details influence already generated word answer updated information. We view process deliberation cross-media dialogue. Inspired human cognitive behaviors, propose xx, decodes information two channels, one original impression encoder output, time-varying dynamically interaction deliberation channel. %The main contributions summarized follows: % We move step generative decoder visual dialogue, proposing novel framework generative decoder. % We equipped xx decoder three popular encoders xx baselines, namely Late Fusion encoder, Memory Network encoder Dual Encoding Visual Dialogue encoder. % We conduct experiments latest visual dialogue dataset VisDial v1.0 prove effectiveness xxx whether simple complex encoder visual dialogue. % %The decoder summarized two mode: discriminative decoder generative decoder. Discriminative decoder attracts lot study good retrieval performance, generative decoder usually acts accessory. However, generative decoder good practicality, options answers reply questions real world. In paper, propose novel model, namely, \mtv mine sufficient information behind HIN. It pre-traing fine-tuning framework. In pre-training stage, first adopt ranking-based BFS strategy generate input sequence. Then leverage bi-directional transformer layers pre-train model. We adopt factorized embedding parameterization cross-layer parameter sharing strategies reduce parameters. The pre-training tasks utilize masked node modeling adjacent node prediction . Afterwards fine-tune \mtv four different tasks, i.e., link prediction, similarity search, node classification node clustering. \mtv significantly consistently outperforms baseline models tasks four real-life datasets. In future work, interest see model dynamic HIN constantly evolving, using pre-training fine-tuning framework."," Visual Dialogue task requires an agent to be engaged in a conversation with human about an image. The ability of generating detailed and non-repetitive responses is crucial for the agent to achieve human-like conversation. In this paper, we propose a novel generative decoding architecture to generate high-quality responses, which moves away from decoding the whole encoded semantics towards the design that advocates both transparency and flexibility. In this architecture, word generation is decomposed into a series of attention-based information selection steps, performed by the novel recurrent Deliberation, Abandon and Memory  module. Each DAM module performs an adaptive combination of the response-level semantics captured from the encoder and the word-level semantics specifically selected for generating each word. Therefore, the responses contain more detailed and non-repetitive descriptions while maintaining the semantic accuracy. Furthermore, DAM is flexible to cooperate with existing visual dialogue encoders and adaptive to the encoder structures by constraining the information selection mode in DAM. We apply DAM to three typical encoders and verify the performance on the VisDial v1.0 dataset. Experimental results show that the proposed models achieve new state-of-the-art performance with high-quality responses. The code is available at https://github.com/JXZe/DAM."
"We propose novel technique representing templates instances concept classes agnostic regard underlying deep learning model. Starting raw input images, representations learned classification task cross-entropy classification layer replaced fully connected layer used estimate bounded approximation distance class distribution set contextual distributions call `environments'. By defining randomized environments, goal capture common sense knowledge classes relate range differentiating contexts, increase probability encountering distinctive diagnostic features. This idea loosely resembles human long-term memory might achieve retrieval well contextual knowledge used semantic encoding . Our experiments confirm value approach. In paper, classes correspond object labels, environments correspond combinations contextual labels given either object labels image caption keywords. Representations individual inputs, call `instance representations', form 2D matrix rows corresponding classes columns corresponding environments, element indication much instance resembles corresponding class versus environment. The parameters environment defined start uniformly selecting randomly chosen number labels power set available contextual labels. The class representation, refer `template', form template vector. It contains average distance estimates distribution class distributions respective environments. By computing cosine similarity instance representation templates, class membership determined efficiently . Template instance representations interpretable fixed structure comprised distance estimates. This structure reminiscent traditional language processing matrix representations enables operations operate along matrix dimensions. We demonstrate Singular Value Decomposition yields components determine values along rows columns respectively. Those components altered modify information content, upon new representation reconstructed. The proposed representations evaluated four settings: Multi-label image classification, i.e., object recognition multiple objects per image; Image retrieval query images look like existing images contain altered class labels; Single-label image classification pre-trained instance representations previously unseen label; Rank estimation regard compression representations. Contributions We propose new deep learning technique create structured representations images, entity classes contextual information based distance estimates. This leads template representations generalize well, successfully evaluated classification task. The obtained representations interpretable distances class environment. They composable sense modified reflect different class membership shown retrieval task. In paper, propose novel generative decoder DAM consisting Deliberation Unit, Abandon Unit Memory Unit. The novel decoder adopts compositive decoding mode order model information response-level word-level, discourage repetition generated responses. DAM universal decoding architecture incorporated existing visual dialogue encoders improve performance. The extensive experiments combining DAM LF, MN DualVD encoders verify proposed DAM effectively improve generation performance existing models achieve new state-of-the-art results popular benchmark dataset.","  The paper proposes a novel technique for representing templates and instances of concept classes. A template representation refers to the generic representation that captures the characteristics of an entire class. The proposed technique uses end-to-end deep learning to learn structured and composable representations from input images and discrete labels. The obtained representations are based on distance estimates between the distributions given by the class label and those given by contextual information, which are modeled as environments. We prove that the representations have a clear structure allowing to decompose the representation into factors that represent classes and environments. We evaluate our novel technique on classification and retrieval tasks involving different modalities .  \keywords{Composable representations \and Deep learning \and Multimodal.}"
"When read book, maintain representations characters events text help us understand story. We selective memorisation process; finer details text quickly forgotten retain relatively compact representation book's details. Early models natural language used recurrent neural networks Long Short-Term Memory emulated selective memory approach modelling past compact state vector. The model learns store relevant information within state implicitly order optimise task loss. The LSTM reigned state-of-the-art language model two decades since inception '90s arguably ubiquitous neural sequence model. Unlike human memory systems, however, LSTM struggles reason long-range contexts reading text. This observed multiple contexts. In carefully curated LAMBADA benchmark tests language model predictions sections book text long term structure decided human raters, LSTMs completely fail. Namely LSTMs guess correct word time, humans considered accuracy. For regular language modelling, \citet{daniluk2017frustratingly} observed LSTM augmented attention would rarely attend beyond seven preceding words context. Samples LSTMs language models quickly devolve generic text devoid overall theme. This lead many wonder whether non-negligible long-range signal task language modelling. Recently seen deep attention models draw long-range signal text, even objective simple next-word prediction. With advent Transformer , significant gains language modelling performance obtained extending models' attention thousands words. The Transformer-XL , Transformer variant specialised long-range sequence modelling via introduction cache past activations, obtained state-of-the-art results four major LM benchmarks --- PTB , LM1B , Enwik8 , WikiText . In case latter two, \citet{dai2019transformer} showed model effectively used one thousand words context, resulting samples reflect thematic consistency spanning paragraphs. When Transformers paired long contexts large amount data, e.g. GPT-2 Megatron , resulting samples remarkable long-range consistency stylistic realism. However Transformers abandon compact selective representation past. They store hidden activation every time-step every layer within network. This consume orders magnitude space prior RNN hidden states, original text. E.g. typical state-of-the-art LSTM language model state size may range 4KB model Wikipedia articles 64KB model news --- never greater 1MB. Whereas current state-of-the-art 18-layer Transformer-XL state size Wikipedia articles 112MB. The state large separate memory maintained per layer. If found unnecessary reduce state's memory considerably. In paper investigate simple question: use short-range attention majority layers Transformer recover performance? The hypothesis possible, many steps reasoning involve short-range correlations, i.e. piece characters together form words phrases. We find indeed possible. We recover comparable performance long-range language modelling using small fraction long-range memories baseline TransformerXL. Crucially, find matters long-range memories placed network. Placing lower layers network ineffective; placing latter layers interleaved across network works much better. We show model trains less time memory, due reduction expensive attention operations. CoDiR novel deep learning method learn representations combine different modalities. The instance representations obtained images convolutional neural network structured along class environment dimensions. Templates derived instance representations generalize class-specific information. In classification task shown generalization improves richer contextual information added environments. When environments built labels image captions, CoDiR representations consistently outperform respective baselines. The representations continuous high rank, demonstrated ability classify label seen pre-training simple logistic regression. At time, contain clear structure allows semantic interpretation content. It shown retrieval task representations decomposed, modified recomposed reflect modified information, conserving existing information. CoDiR opens interesting path deep learning applications explore uses structured representations, similar structured matrices played central role many language processing approaches past. In zero-shot settings structure might exploited, example, make compositions classes environments seen before. Additionally, research might explore unsupervised learning method applied tasks modalities alternative building blocks environments. While demonstrate method Wasserstein-based distance, distance similarity metrics could examined future work."," Deep attention models have advanced the modelling of sequential data across many domains. For language modelling in particular, the Transformer-XL --- a Transformer augmented with a long-range memory of past activations --- has been shown to be state-of-the-art across a variety of well-studied benchmarks. The Transformer-XL incorporates a long-range memory at every layer of the network, which renders its state to be thousands of times larger than RNN predecessors. However it is unclear whether this is necessary. We perform a set of interventions to show that comparable performance can be obtained with 6X fewer long range memories and better performance can be obtained by limiting the range of attention in lower layers of the network."
"The success deep learning producing effective solutions several fundamental problems computer vision, natural language processing, speech/audio understanding provided impetus explore complex multi-modal problems intersections domains, attracting wide interest recently. A notable ones include: visual question answering , goal build agent generate correct answers free-form questions visual content, audio/visual captioning, agent needs generate sentence natural language describing audio/visual content, visual dialog, agent needs engage natural conversation human static image, audio-visual scene-aware dialog -- generalizes , , -- agent needs produce natural answer question given audio-visual clip, conversation setting select correct answer set candidates. The AVSD task emulates real-world human-machine conversation setting potentially useful variety practical applications, building virtual assistants human-robot interactions. See Figure illustration task. We explore set interventions Transformer-XL's architecture simple implement, i.e. lines code, shed light fundamental workings model modelling long sequences text. In set interventions, modify flow information within network, versus number trainable parameters. Thus confounding factors varying network capacity. Our finding need long-range memories every layer network. Comparable performance obtained fraction long-range memories spaced equally across network, latter layers. We hypothesise modelling long-range correlations best done representations first formed short-range correlations. We also find real performance drop using single long-range memory, proving long-range dependency superfluous task. This study implications practitioners interested speeding deep Transformer-XL models. There number long-range transformer variants published past year~ aim extend range attention via sparsity compression. However models maintain use uniform memory capacity layer. Here show long-range attention need scaled every layer, thus architectures sped-up observation. This study also implications researchers using single long-range memory, typically approach traditional RNN + attention systems. For example, Differentiable Neural Computer recent memory-augmented agents reinforcement learning, utilise distinct working memory single long-range episodic memory . Perhaps performance could improved adding additional layers episodic memories. The practice storing deep long-range memories scalable wish neural networks kinds large-horizon reasoning humans possess. We believe solution maintaining small number long-range memories step towards tractable lifelong memory."," Given an input video, its associated audio, and a brief caption, the audio-visual scene aware dialog  task requires an agent to indulge in a question-answer dialog with a human about the audio-visual content. This task thus poses a challenging multi-modal representation learning and reasoning scenario, advancements into which could influence several human-machine interaction applications. To solve this task, we introduce a semantics-controlled multi-modal shuffled Transformer reasoning framework, consisting of a sequence of Transformer modules, each taking a modality as input and producing representations conditioned on the input question. Our proposed Transformer variant uses a shuffling scheme on their multi-head outputs, demonstrating better regularization. To encode fine-grained visual information, we present a novel dynamic scene graph representation learning pipeline that consists of an intra-frame reasoning layer producing spatio-semantic graph representations for every frame, and an inter-frame aggregation module capturing temporal cues. Our entire pipeline is trained end-to-end. We present experiments on the benchmark AVSD dataset, both on answer generation and selection tasks. Our results demonstrate state-of-the-art performances on all evaluation metrics."
"%Image Captioning: The task image captioning lies intersection computer vision natural language processing. Given image, task generate natural language sentence describing information conveyed input image. Image captioning received increasing attention years. The prevalent encoder-decoder frame work serves backbone many derived models. introduced refined attention mechanism allows better feature extraction interpretability. used Faster-RCNN replace fixed-resolution attention mechanism. Researchers also found high-level concepts provide concise representation image.\\ % ------------------------ %Main drawbacks: %However, certain drawbacks models. They The majority existing approaches follows sequential model words caption produced sequential manner--\idest choice word depends preceding word image feature. Such models largely ignore fact natural language inherent hierarchical structure. For example, object associated various attributes. Even better feature representations attention mechanisms, sequential structure models tends lead %over-simplified generic descriptions lack specificity. %, \exempli ``a piece fruit'' instead ``a red apple,'' ``a group people'' instead ``three people.'' %For example, models tend generate sentence ``a group people'' instead ``three people.'' The models exploring compositionality shown produce accurate, specific, out-of-distribution sentences perform well SPICE, semantic metric . Compositional models, however, compare well sequential models -gram metrics BLEU. Because semantic evaluation metrics SPICE tend ignore fluency assume well-formed captions, -gram metrics still important judging fluency generated captions. % %Our approach: In paper, propose image captioning model combines merit sequential compositional models following word-by-word generation process combining grounded attributes specialized modules. A high-level illustration workflow one time step visualization module attention shown in~\Cref{fig:workflow}. More specifically, algorithm first proposes regions interest chooses region focus depending context. The chosen region whole image fed collection functionally specialized modules module delegated predict one aspect objects count, color, size. This analogous Neural Module Networks , module responsible specialized functionality final result dynamic composition different modules. In case, model generates final caption dynamically attending different modules. The attributes, therefore, hierarchical dependency grounded proposed regions. With proposed Compositional Neural Module Networks, aim generate detailed, specific captions without losing fluency, \exempli ``a red apple'' instead ``a piece fruit'' ``three people'' instead ``a group people.'' Overall, main contributions paper are: We presented novel hierarchical graph representation learning Transformer reasoning framework problem audio-visual scene-aware dialog. Specifically, model generates object, frame, video-level representations systematically integrated produce visual memories, sequentially fused encodings modalities conditioned input question using multi-head shuffled Transformer. Experiments demonstrate benefits framework generation/selection tasks AVSD benchmark. Going forward, plan explore use richer text embeddings within framework."," %In image captioning, sequential models are preferred where fluency is an important factor in evaluation, \exempli $n$-gram metrics;  In image captioning where fluency is an important factor in evaluation, \exempli $n$-gram metrics, sequential models are commonly used;  however, sequential models generally result in overgeneralized expressions that lack the details that may be present in an input image. Inspired by the idea of the compositional neural module networks in the visual question answering task, we introduce a hierarchical framework for image captioning that explores both compositionality and sequentiality of natural language. Our algorithm learns to compose a detail-rich sentence by selectively attending to different modules corresponding to unique aspects of each object detected in an input image  to include specific descriptions such as counts and color. In a set of experiments on the MSCOCO dataset, the proposed model outperforms a state-of-the art model across multiple evaluation metrics, more importantly, presenting visually interpretable results. Furthermore, the breakdown of subcategories $f$-scores of the SPICE metric and human evaluation on Amazon Mechanical Turk  show that our compositional module networks effectively generate  accurate and detailed captions."
"Most methods address vision conditioned textual sequence generation concentrated shorter sequences . Usually, methods employ standard encoder-decoder framework, encoder encodes image fixed vector representation decoder decodes textual sequence. Several improvements seen recent years earlier proposed methods visual features upgraded bottom-up encoding, encoder-decoder architecture added attention training achieved reinforcement sequence decoding. However, methods fail capture salient objects observed image generate textual sequences generic simple. A possible reason identified visually grounded language generation end-to-end largely attributed high-level symbolic reasoning. It also observed high-level reasoning natural humans inherently incorporate inductive bias based common sense factual knowledge language, however, ineffective vision conditioned textual sequence generation due gap visual information language composition. This gap widens longer textual sequences need generated conditioned visual information. In NLP, structured inputs omnipresent representation natural language. Recently, several works explored changing sequences different applications. Inspired it, propose incorporate graph structure inductive bias vision conditioned textual sequence generation. This achieved abstracting visual information scene graph add complementary strength symbolic reasoning multimodal learning. Scene graphs connect visual objects, attributes, relationships image directed edges. Figure presents visualization overall idea. However, major challenge embedding scene graph structure vector representations seamless integration encoder-decoder learning framework. Also, representation facilitate sequence decoder generate longer sequences. Therefore, paper, introduce Sparse Graph-to-Sequence Transformer embedding scene graph understanding structured sparsity decoding textual sequence. This approach builds upon Transformer encoder-decoder architecture Transformer based decoders already shown ability decode longer sequences, however, less explored encoding graph structures. Nevertheless, interest recently, but, many methods proposed earlier encode graphs vector representation mostly based Graph Convolutional Networks . We hypothize SGST effective approach problem GCN performs global contextualization vertex focused portions GCN allowing direct modeling dependencies two nodes without regard distance input graph. Furthermore, SGST incorporates sparse attention mechanism self-attention Transformer architecture allowing assign zero probabilities irrelevant graph vertices tokens sequence. This aids SGST effectively encode graphs decode longer sequences. In work, propose image captioning model utilizes neural module networks propose specialized grounded attributes. Experimental results show model achieves fluency sequential models specificity compositional models. Specifically, approach excels including fine-grained details counting generally avoided overlooked. The framework easily expandable include additional functional modules sophisticated designs. Improved interpretability via visualized attention another bonus model enables quantitative analysis visual semantic information. In future, plan design advanced size, spatial semantic modules push limit framework even further. The file named.bst bibliography style file BibTeX 0.99c \small"," Generating longer textual sequences when conditioned on the visual information is an interesting problem to explore. The challenge here proliferate over the standard vision conditioned sentence-level generation  as it requires to produce a brief and coherent story describing the visual content. In this paper, we mask this Vision-to-Sequence as Graph-to-Sequence learning problem and approach it with the Transformer architecture.  To be specific, we introduce Sparse Graph-to-Sequence Transformer  for encoding the graph and decoding a sequence. The encoder aims to directly encode graph-level semantics, while the decoder is used to generate longer sequences. Experiments conducted with the benchmark image paragraph dataset show that our proposed achieve 13.3\% improvement on the CIDEr evaluation measure when comparing to the previous state-of-the-art approach."
"% Neural \gls{NLU} systems---wherein deep neural network used function approximator~---have extremely successful various natural language tasks, \gls{QA} \gls{NLI}~, achieving strong generalisation results datasets available tasks~. % Even strong performance NLU problems recently achieved advent large models pre-trained via self-supervision, BERT~, XLNet~, RoBERTa~. % % \paragraph{Generalisation Neural Models} % However, growing concerns ability \gls{NLU} systems, neural networks generally, generalise systematic robust way~. % For instance, \citet{DBLP:conf/emnlp/JiaL17} highlight brittleness \gls{NLU} systems adversarial examples, \citet{DBLP:conf/naacl/GururanganSLSBS18,DBLP:conf/emnlp/KaushikL18} show neural \gls{NLU} models tend exploit annotation artefacts spurious correlations data. % Furthermore, analysing supervising inner workings models trivial, due inherent black-box nature~. % % More generally, \citet{GARNELO201917} emphasise several limitations neural models, terms % % % In vein, \citet{DBLP:conf/emnlp/SinhaSDPH19} measured compared systematic generalisation abilities several neural models task answering questions family relationship graphs, evaluating held-out combinations reasoning patterns adding curated distracting noisy facts. % Interestingly, found performance degrades monotonically every model pool increase complexity relational graph, highlighting challenge systematic generalisation~. % % \paragraph{Neuro-Symbolic Reasoning} % A promising direction overcoming issues consists combining neural models symbolic reasoning given complementary strengths weaknesses~. % We focus \glspl{NTP}~, family neuro-symbolic reasoning models: \glspl{NTP} continuous relaxations backward-chaining reasoning algorithm replace discrete symbols continuous embedding representations. % % \glspl{NTP} interesting properties: jointly learn representations interpretable rules data via backpropagation, potentially combine rules ways may observed training. % However, major limitation \glspl{NTP} that, training, need consider rules explaining given goal sub-goal. % This quickly renders ineffective settings requiring large number rules reasoning steps. % % \paragraph{Conditional Theorem Provers} % For addressing limitations \glspl{NTP}, propose \glspl{CTP}, extension able learn adaptive strategy selecting subsets rules consider step reasoning process. % This achieved module that, given goal, produce rules needed proving it. % Predicates constants produced rules lie continuous embedding space. Hence, module end-to-end differentiable, trained jointly modules via gradient-based optimisation. % % We presented SGST, treating vision-to-sequence graph-to-sequence learning. We encode images scene graphs condition long textual sequence generation. Our experiments show proposed approach effectively encode scene graphs generating paragraphs. In future, plan investigate impact leveraging graph reasoning encoding scene graph constituents vectors. Further, also aim find impact sparse attention attention heads compare performance GCN encoders."," % Attempts to render deep learning models interpretable, data-efficient, and robust have seen some success through hybridisation with rule-based systems, for example, in. % These neuro-symbolic models can induce interpretable rules and learn representations from data via back-propagation, while providing logical explanations for their predictions. % However, they are restricted by their computational complexity, as they need to consider all possible proof paths for explaining a goal, thus rendering them unfit for large-scale applications. % We present \glspl{CTP}, an extension to \glspl{NTP} that learns an optimal rule selection strategy via gradient-based optimisation. % We show that \glspl{CTP} are scalable and yield state-of-the-art results on the \glsentryshort{CLUTRR} dataset, which tests systematic generalisation of neural models by learning to reason over smaller graphs and evaluating on larger ones. % Finally, \glspl{CTP} show better link prediction results on standard benchmarks in comparison with other neural-symbolic models, while being explainable. % All source code and datasets are available online.} %"
"Recently, shadow recommendation system appeared various domains applications. On hand, significant new advances deep learning approaches important effects tremendous success recommendation system . The overall structure RS follows set phases including collection, learning, recommendation . In first phase, appropriate resources comprise relevant information users selected. Then, leaner analyzes users preferences, extracts behavioral patterns. Final phase recommends entities similar users' interests. It important recognize that, within common core structure RS, variations application application. Some sophisticated heavily used RSs industry Last.fm, YouTube, Amazon. Furthermore, find footprint RS knowledge management system RS tries specify experts relevant knowledge particular topic . This category RS called expert recommendation system expert finding system. So, obvious ERS similar phases compared general RSs. An ERS takes user topic query, traces set candidates' expertise, learns expertise patterns, finally produces list experts sorted score. Each candidate's score indicates degree candidate's relevant expertise given topic. In studies, candidates' expertise defined content-based information non-content-based information . Content-based information candidates' shared textual content like articles, questions, answers on. In contrast, candidates' interactions others social networks make non-content-based information. Depending application scenarios, ERS set contextual information. For example, academic environment, attempt detect researchers subject areas related query. This detection based content articles published co-author relations different papers. However, Community Question Answering , main goal find users expertise willingness answer given questions terms content question asked answered posted them, question-answer relations . With brief look previous studies, concluded three different outlooks ERSs. In one attitudes, studies focused textual expertise candidates. These works used text mining information retrieval techniques selected ones experts published items semantically relevant query . On hand, researches investigated social relations candidates represented connections graph . After that, social network analysis mining techniques, page ranking algorithms, applied graph identify important candidates rank them. Moreover, recent studies shown combination different types expertise information notable performance compared other. A number integrated textual expertise social network connection information linear nonlinear functions. %It important notice kind expertise diverse priority various ERS applications. Also, achieve higher accuracy, authors proposed usage heterogeneous network combination users' interactions social networks questionnswer relationships CQAs besides bearing mind content questions answers. In recent years, multimodal machine learning attracting attention. This popularity huge multimodal content generated users social media networks . The goal multimodal machine learning create joint model retrieve contextual information multiple modalities . In research, aim find academic experts whether using multimodal learning approach provides effective solution ERS not. Also, purpose work solve expert finding problem multi-label classification task. In way, combine text graph information multimodal approach. The text component converted vector using BERT Transformer. On hand, obtain node representation candidates, graph embedding technique, ExEm introduced , used. Also, normalized h-index value candidate added another feature. Then, captured fusion features fed train classifier. We evaluate BERTERS multi-label classification visualization tasks. However, best knowledge, present first approach field expert recommendation using multimodal learning transformers. The rest paper structured follows: Section reviews related works. Section discusses background research. Section presents proposed method explains detail. The descriptions dataset tasks used test proposed method parameter setting presented Section . Section provides discusses experimental results. Finally, Section concludes paper. We introduced \glspl{CTP}, extension \glspl{NTP} learning optimal rule selection strategy via gradient-based optimisation. For sub-goal, \module{select} module produces smaller set rules, used proving mechanism. Furthermore, proposed three variants rule selection mechanism, sub-goal reformulations obtained linear projections sub-goal predicate, attention distributions predicate embeddings, key-value memory lookup set rules. We showed \glspl{CTP} scalable yield state-of-the-art results \gls{CLUTRR} dataset, explicitly tests systematic generalisation neural models, comparison wide set neural baselines. Finally, demonstrated \glspl{CTP} yield competitive results standard link prediction benchmark comparison neuro-symbolic approaches. \paragraph{Future Work} An open problem \glspl{CTP} able process \gls{CLUTRR} instances family relationships directly provided labelled graph, rather free-form text. A possible solution, proposed \citet{DBLP:conf/aaai/MinerviniBR0G20}, consists end-to-end differentiable encoder producing fact embeddings conditioned text, currently analysing several options space. \paragraph{Acknowledgements} This work supported EU Horizon 2020 Research Innovation Programme grant 875160. We thank Yihong Chen, Joe Stacey, amazing folks UCL NLP group enlightening discussions support. Finally, thank NVIDIA GPU donations. \clearpage"," The objective of an expert recommendation system is to trace a set of candidates' expertise and preferences, recognize their expertise patterns, and identify experts.  In this paper, we introduce a multimodal classification approach for expert recommendation system . In our proposed system, the modalities are derived from text  and graph  information. BERTERS converts text into a vector using the Bidirectional Encoder Representations from Transformer . Also, a graph Representation technique called ExEm is used to extract the features of candidates from co-author network. Final representation of a candidate is the concatenation of these vectors and other features. Eventually, a classifier is built on the concatenation of features.  This multimodal approach can be used in both the academic community and the community question answering. To verify the effectiveness of BERTERS, we analyze its performance on multi-label classification  and visualization tasks."
"Human languages evolve complex adaptive systems, driven micro-level processes constraints , macro-level factors , history development . %Multi-Agent Reinforcement Learning setting promising benchmark simulating processes maintains flexibility individual learners, allowing scale simulation relatively big populations. Human language evolution, convention formation, decentralized group learning studies, turn, suggest solutions optimization problems arise MARLC. Linguistic communication depends shared knowledge word-to-meaning mapping conventions , upon population converges local interactions agents, often central controller available . Empirical studies human learning demonstrate groups quickly converge new communicative conventions ``decentralized"" settings %, even prevented using natural languages . Multi-agent reinforcement learning communicate , however, faces instability challenges central optimization introduced . This influences ability groups consisting reinforcement learners converge efficient stable communication systems shared members. Therefore, different methods centralized control optimization proposed stabilize MARLC . Central optimization makes simulations brittle less flexibly adaptive, and, potentially, less promising developing communication systems freely expressive well-optimized users natural languages. We argue empirical evidence individual- population-level factors drive decentralized learning human groups guide simulations language evolution MARLC settings. In work, explore whether social network organization shapes properties communication systems arise decentralized MARLC simplified settings. \subsection{Human Learning Different Social Network Structures} Convergence human groups word conventions dramatically affected social topology determines possible interactions participants, demonstrated naming game experiment %on large groups participants . In particular, arranged social network many local connections ) randomly-connected network , large groups converge many local word conventions, reaching global consensus. However, person equally likely interact person group ), global consensus easily achieved centralization. Other studies decentralized problem solving human groups demonstrated social network organization shapes multi-agent optimization process, different network types beneficial different types optimization landscapes. High local connectivity supports independent local exploration, whereas high global connectivity helps groups converge shared solution, choosing among local ones . In study, looked type social network organization, average degree, local connectivity affect results communication learning groups deep reinforcement learning agents. %In work, looked whether social network topology shapes properties communication systems arise decentralized MARLC simplified settings. In particular, tested effects type social topology, average degree agents, proportion global connections social network. We developed personalized online language learning new setting continual learning. To support research problem, collected massive web-scale dataset comprises 100 million tweets posted six years . We benchmarked continual learning algorithms data contributed effective algorithm continual gradient descent . Our experiments indicate significant room progress continual learning real web-scale data."," Social network structure is one of the key determinants of human language evolution. Previous work has shown that the network of social interactions shapes decentralized learning in human groups, leading to the emergence of different kinds of communicative conventions. We examined the effects of social network organization on the properties of communication systems emerging in decentralized, multi-agent reinforcement learning communities. We found that the global connectivity of a social network drives the convergence of populations on shared and symmetric communication systems, preventing the agents from forming many local ``dialects"". Moreover, the agent's degree is inversely related to the consistency of its use of communicative conventions. These results show the importance of the basic properties of social network structure on reinforcement communication learning and suggest a new interpretation of findings on human convergence on word conventions.% among interacting humans.  %We found that the proportion of global connections in a social network determines the convergence of populations on shared and symmetric communication systems"
"It well established Internet, especially social networks, provides platform ``viral"" spread information rates faster even fully connected traditional networks . Depending actors involved, could either used societal good ill. For e.g, rumor explosion White House caused Dow Jones Industrial Average immediately plunge S\&P 500 reported lost \$136.5 billion market cap, taking reach rumors economic domain . In 2016 politically divisive Brexit US elections, fake news outpaced real news Facebook . Note use ``rumor"" ``fake news"" interchangeably work, common related work. Given real social economic implications rumors social media, automatic detection rumors seen significant surge research. Existing work area use aspects news item like news item text content, comments news item user characteristics propagation paths item within network. Some researchers tackled problem creating knowledge graphs built crawling web raw facts processing cleaning using fact check . The problem approach less suitable detecting rumors evolving content yet representation knowledge graph. Among methods used content news item, approaches ranged using psycholinguistic features like sentiment , style features like readability assertive factive verbs varying degrees success. Some authors suggested characteristics may useful include detection process due insufficiency news content material, especially microblogging sites like Twitter . Several researchers introduced information like user comments along news content; user characteristics like number followers, first tweet story etc . Still others used network structure and/or propagation path along content . A hybrid feature extraction unit gated diffusive unit used detect rumors . HFEU extracted explicit latent features textual information; GDU effectively extracted relationship among news articles, creators subjects. A fake news detector called event adversarial neural networks includes multi-modal feature extractor, fake news detector event discriminator co-operatively learns event non-specific features discriminate fake real news proposed . More recently, authors argued interpretable news feed generator algorithms could reduce misuse improving user awareness system transparency. T-SNE based methods provided could indicate usefulness learned features rumor classification. %Done FILL: For what? Research begun explainable rumor detection algorithms . Another debate often waged AI community, whether handcrafted features should/can incorporated AI engine. In work provide framework used explore question including handcrafted latent features rumor detection problem. We propose modular, explainable architecture use number classes features may become available, detecting rumors. Specifically, design explainable deep learning architecture using attention mechanism detect rumors using multiple types features. Our work inspired differences thought generalization multiple class features. The main contributions work are: In paper, investigated random caching scheme delay minimization D2D-assisted heterogeneous network. To provide diversified viewing qualities multimedia video services, super layers transmitted user. We firstly analyzed successful transmission probabilities, obtained close-form expression overall service delay. Based expression, minimized service delay efficiently applying improved standard gradient projection method. Numerical results validate analysis successful transmission probabilities, proposed random caching scheme shown superior MPCP, EPCP ICP strategies. \begin{appendices}"," \label{sec:abs}  With social media becoming  ubiquitous, information consumption from this media has also increased.  However, one of the serious problems that has emerged with this increase, is the propagation of rumors.  Therefore, rumor identification is a very critical task with significant implications to economy, democracy as well as public health and safety.  We tackle the problem of automated  detection of rumors in social media  in this paper by designing a modular  explainable architecture that uses both latent  and handcrafted features and can be expanded to as many new classes of features as desired.  This approach will allow the end user to not only determine whether the piece of information on the social media is real of a rumor, but also give explanations on why the algorithm arrived at its conclusion. Using attention mechanisms, we are able to interpret the relative importance of each of these features as well as the relative importance of the feature classes themselves.  The advantage of this approach is that the architecture is expandable  to more handcrafted features as they  become available and also to conduct extensive testing to determine the relative influences of theses features in the final decision. Extensive experimentation on popular  datasets and  benchmarking against eleven  contemporary algorithms, show that  our approach performs significantly better in  terms of F-score and accuracy  while also being interpretable."
"In many circumstances, difference completed suicide suicide attempt slightly greater pressure applied trigger. In either case importance gaining greater understanding psychological conditions surrounding tragic event immediately apparent; leads individual contemplate, perhaps commit, act? Similarly, sorts thoughts feelings one encounter experiencing suicidal ideation? And might understanding phenomena aid improving prevention efforts? Although challenging questions, suicide notes represent one potential window psychology individuals complete suicide. By analyzing language contents suicide notes, gain unique insight shared features individual experiences perhaps greater understanding cognitive processes accompany suicidal ideation. Previous research suicide notes highlighted specific properties notes attempt better understand characteristics stand differentiate types texts. Some work focused studying contents suicide notes , including dominant emotional themes , key motives . In general, studies focused answering question: suicide note? That is, contents consistently observe comparing notes people committed suicide? For example, Al-Mosaiwi Johnstone recently found vocabulary used individuals risk suicide different suffered mental disorders related depression anxiety. Individuals experienced suicidal ideation tended utilize different vocabularies mainly absolutist words, indicating suicide notes emotional lexical footprints. Sentiment analysis applied goal comparing emotional contents suicide notes categorized learning algorithms versus trained clinicians, well whether algorithms reliably distinguish genuine simulated suicide notes. These automated text-analysis techniques offer powerful advantages standard, qualitative approaches commonly applied study suicide notes clinical psychologists. For one, quantitative methods -- used sentiment analysis emotional profiling -- allow use objective criteria clearer operationalizations psychological constructs . Qualitative methods, hand, depend upon human judges code interpret texts, compare ratings assess consistency conclusions. Thus, may high degree uncertainty limited reliability techniques used make inferences. Moreover, automatic approaches text analysis allow researchers evaluate millions lines text short amounts time. On hand, complex statistical/machine learning models often produce results difficult understand thus may helpful tasks different prediction explanation. In present work apply network science methods analysis genuine suicide notes. Importantly, show network modeling used expand text-analytic toolbox psychology provide novel ways answering complex research questions text data . In contrast automated approaches text analysis, network models allow researchers encode only, e.g., word sentiment, also broader set connections word surrounding text. This allows one track words appear less often sample texts, also used contexts comparison linguistic baselines. Additionally, unlike typical black box models, network methods fully transparent produce results often much easier interpret. Hence, network modeling represents approach study text data elucidate structure human texts potentially reveal concepts perceived, organized interconnected human mind. \subsection{The relevance networks understanding suicide notes} Language guarantees expression people's perceptions semantic content emotions. Semantic frame theory indicates meaning attributed people given concept reconstructed observing relationships conceptual associations attributed concept text speech. Words given semantic frame elicit different combinations emotions, i.e. emotional profiles, characterize emotional content text. Network science provides tools quantify reconstruct semantic frames emotional associations, serving framework quantitative identification way people perceive events happenings . In comparison opaque machine learning techniques, networks advantage transparently representing proxy associative structure language human mind, within cognitive system apt acquiring, storing producing language, i.e. mental lexicon. Supported psycholinguistic inquiries mental lexicon , complex networks built texts open window people's mindsets . Focus given reconstructing collective mindset expressed last written words left people committed suicide. \subsection{Cognitive network approach suicidal ideation analysis} In manuscript consider content suicide notes observable realization otherwise unobservable mental states suicidal ideation authors. In order map relationships main concepts emergent semantics suicide notes reduced raw texts two different network representations: co-occurrence subject-verb-object networks. See Fig. Materials Methods section details. For comparisons used network representation mind-wandering based free associations . Unlike previous approaches, aim discriminate suicide notes types text. Instead, focus quantitatively understanding mindset behind suicidal ideation people committed suicide final notes. \subsection{Manuscript structure} Study 1 investigates ``emotional syntax'' suicide notes, analyzing whether connectivity configuration words somehow related valence. We use structural balance theory assess degree balance network determine valence organized among neighboring words. We extend previous research by: studying emotional content suicide notes mapping sentiment organized collective mindset around suicidal ideation. Study 2 focuses subject-verb-object relationships highlight self-perceptions suicide notes. Study 3 combines network centrality, semantic frames emotional data order describe quantify typical emotions associated different concepts suicide notes. We conclude general discussion relevance study vis--vis previous results current gaps literature. Rumors Internet emerged modern day threat public safety, economy democracy. We proposed explainable, modular architecture rumor detection expanded accommodate several feature classes, even yet discovered. We demonstrated architecture using three classes features: user features, handcrafted features derived content item latent features obtained language embeddings. Using attention layers two levels: one intra-feature level type feature one inter-feature-class level achieve granularity explanations. The intra-feature level attention weights capture relative importance model places individual features category, whereas inter-feature attention weights gives us idea relative importance model placed among three classes features. We also provide average case analysis importance features help model developer trim model according needs showed interpret decisions individual decisions end user. Our proposed architectures perform best among eleven benchmark models providing meaningful interpretations decisions. THIS IS THE WORKING VERSION OF THIS FILE"," Understanding how people who commit suicide perceive their cognitive states and emotions represents a crucially open scientific challenge. We build upon cognitive network science, psycholinguistics and semantic frame theory to introduce a network representation of suicidal ideation as expressed in multiple suicide notes. By reconstructing the knowledge structure of such notes, we reveal interconnections between the semantic ideas and emotional states of people who committed suicide through structural balance theory, semantic prominence and emotional profiling. Our results indicate that connections between positively- and negatively-valenced terms give rise to a degree of structural balance that is significantly higher than in a null model where the affective structure is randomized and in a linguistic baseline model capturing mind-wandering in absence of suicidal ideation. We show that suicide notes are affectively compartmentalized such that positive concepts tend to cluster together and dominate the overall network structure. Notably, this positive clustering diverges from perceptions of self, which are found to be dominated by negative, sad conceptual associates in analyses about subject-verb-object structure and emotional profiling. A key positive concept is ``love'', which integrates information relating the self to others in ways that are semantically prominent across suicide notes. The emotions populating the semantic frame of ``love'' combine joy and trust with anticipation and sadness, which can be linked to psychological theories of meaning-making as well as narrative psychology. Our results open new ways for understanding the structure of genuine suicide notes and may be used to inform future research on suicide prevention."
"% {\let\thefootnote\relax}% In recent years, machine learning research community devoted substantial energy scaling neural networks enormous sizes. Parameter-counts frequently measured billions rather millions , time financial outlay necessary train models growing concert . These trends especially pronounced natural language processing , massive BERT models---built Transformer architecture pre-trained self-supervised fashion---have become standard starting point variety downstream tasks . Self-supervised pre-training also growing popularity computer vision , suggesting may become standard practice across deep learning past . In parallel race ever-larger models, emerging subfield explored prospect training smaller subnetworks place full models without sacrificing performance . For example, work lottery ticket hypothesis demonstrated small-scale networks computer vision contain sparse, matching subnetworks capable training isolation initialization full accuracy. In words, could trained smaller networks start known subnetworks choose. Within growing body work lottery ticket hypothesis, two key themes emerged: Initialization via pre-training. In larger-scale settings computer vision natural language processing , lottery ticket methodology find matching subnetworks early point training rather random initialization. Prior point, subnetworks perform better selected pruning randomly. The phase training prior point seen dense pre-training creates initialization amenable sparsification. This pre-training even occur using self-supervised task rather supervised downstream task . Transfer learning. Finding matching subnetworks lottery ticket methodology expensive. It entails training unpruned network completion, pruning unnecessary weights, rewinding unpruned weights back values earlier point training . It costlier simply training full network, and, best results, must repeated many times iteratively. However, resulting subnetworks transfer related tasks . This property makes possible justify investment reusing subnetwork many different downstream tasks. These two themes---initialization via pre-training transfer learning---are also signature attributes BERT models: extraordinary cost pre-training amortized transferring range downstream tasks. As such, BERT models particularly interesting setting studying existence nature trainable, transferable subnetworks. If treat pre-trained weights initialization, matching subnetworks downstream task? Do transfer downstream tasks? Are universal subnetworks transfer many tasks degradation performance? Practically speaking, would allow us replace pre-trained BERT smaller subnetwork retaining capabilities make popular NLP work. Although lottery ticket hypothesis evaluated context NLP transformers , remains poorly understood context pre-trained BERT models.% \footnote{A concurrent study Prasanna et al. also examines lottery ticket hypothesis BERTs. However, important differences questions consider results. See Section full comparison.} To address gap literature, investigate transformer architecture initialization resulting lengthy BERT pre-training regime behave comparison existing lottery ticket results. We devote particular attention transfer behavior subnetworks search universal subnetworks reduce cost fine-tuning downstream tasks going forward. In course study, make following findings: We conclude lottery ticket observations computer vision NLP settings extend BERT models pre-trained initialization. In fact, biggest caveat prior work---that, larger-scale settings, matching subnetworks found early training---disappears. Moreover, indeed universal subnetworks could replace full BERT model without inhibiting transfer. As pre-training becomes increasingly central NLP areas deep learning , results demonstrate lottery ticket observations---and tantalizing possibility train smaller networks beginning---hold exemplar class learning algorithms. This first-of-its-kind study uses cognitive network science identifying key concepts typical suicide notes reconstructing meaning emotions final words expressed authors committed suicide. Structural balance potential mechanism long theorized drive certain aspects cognitive organization, particularly regard resolving conflicting beliefs attitudes. Pairing psychological theories, narrative psychology meaning-maintenance model, consider patterns observed suicide notes fit broader understanding psychological literature. According meaning-maintenance model, people fundamentally driven construe lives, perceptions, behaviors meaningful. This position long held existentialist philosophers, widely accepted contemporary psychologists. Moreover, given drive make meaning experiences perceptions world, people motivated restore sense meaningfulness whenever perceptions life's meaning threatened. Suicidal ideation might represent response threat, also poses challenge identifying meaning one's life. In perspective, writing suicide note may represent way re-establishing sense meaningfulness coherence face circumstances led individual consider complete suicide. This drive find meaningfulness narratives, notes, letters supported narrative psychology, focuses function, structure, contents stories tell others life. With meaning-making distal motive writing suicide notes, may interpret structural balance positive emotional perceptions reported concrete signals meaning-making, rather proximal communicative mechanisms meaning-making readily achieved. In short, argue that: People driven perceive lives meaningful coherent, use narratives story-telling way encapsulate restore perceptions threatened, potential ways improving coherence one's psychological narratives introducing balance positive emotional semantic frames otherwise unbalanced negative sets cognitions. The relevance reasoning results present study summarized two key elements. On one hand, extension structural balance networks valenced conceptual associations provides quantitative evidence content suicide notes tends feature positive triads valence-reshuffled null models. This indicates syntax valence words used suicide notes convey tendency avoid conflicting cognitions assembling together pleasant concepts, line previous qualitative studies quantitative investigations using ``bag-of-words'' approach. On hand, semantic framing emotional profiling denoted suicide notes rich positive/trustful perceptions revolving around concepts ``love'', ``take'', ``go'' ``way''. These positive emotional portrayals also contained strong signals sadness anticipation future. Notice despite overall prominence positive, compartmentalized conceptual structures mindset suicide notes, analysis also found self-perception mostly dominated negative associations. The semantic frame subject-verb-object associates ``I'' highlighted negative semantic relationships self others, mostly dominated sadness absent linguistic baseline model provided free associations . This represents compelling evidence cognitive dissonance mindset people committed suicide. Suicide notes denote high level structural balance positive conceptual triads also negative cluster associates surrounding self lacking triadic closure . Our results integrate extend previous findings also show ``love'' central concept suicide notes. Recent studies debated whether emotional perception ``love'' suicide notes positive common language. Our network approach enriched linguistic annotations identified ``love'' suicide notes attributed positive connotations appear common language, also imbued sense sadness. Furthermore, love central across suicide notes, described mostly related towards people diverse ways, function purely positive emotion . This indicates importance going beyond considering words isolation better understand suicide notes. Our quantitative approach reconstructs words interconnected language suicide notes compares random network models linguistic baseline models . This network structure reveals ``love'' as: prominent considered narratives, even mind-wandering captured free associations, focused relations others eliciting nuanced set emotions consisting joy trust also nuances anticipation sadness. Structuring narratives around trustful joyous relationships loved ones aligns well interpretation suicide notes strategically driven meaning identification. Another outcome strategy, aimed avoiding conflicting cognitions, might signals anticipation sadness attributed ``love'', identify resignation, passive acceptance threats generates anger conflict cost feeling defeated incapable creating change. This perception calls future research investigating psychological mechanisms work. All all, reconstructed contexts concepts suicide notes provide evidence meaning-making narratives, aimed coping threats meaning identification conflict-avoidant storytelling. This rich landscape invisible considering words isolation emerges complex structure conceptual emotional connections words text. Cognitive network science, combining psycholinguistics, computer science network science, represents powerful framework reconstructing conceptual relationships, opening window people's minds, along subjective perceptions perspectives. The ability cognitive network analyses parse large volumes texts without human supervision calls future large-scale investigations cognitions perceptions expressed suicide notes. This study use machine learning automatic classification texts. It rather focuses reconstruction general mindset embedded suicide notes representing trains thoughts produced people committed suicide. Achieving quantitative knowledge key better understanding suicide notes also empowering interpretable future models automatic language processing. The main limitation present study lack comparison different set texts. One might consider comparing suicide notes corpora, e.g. love letters. However, comparisons would include potential issues unexpected content found text, overall topic corpus offers little guarantee linguistic content semantic frames. For instance, even love letters might frame ``love'' way nuanced sadness, mainly lovestruck authors ideas related regret desire, aspects found also suicide letters. This emotional/semantic overlap leads corpus comparisons including considerable errors make difficult control adopted methods interpret results. We still managed compare suicide notes linguistic baseline model adopting free associations, come mind-wandering devoid suicidal ideation construction. Concerning structural balance cognitive networks, major limitation inability observing triads, creates weak bias towards balanced triangles. This minor issue able observe even constraint, depending null model, different levels balance observed. Moreover, null models subject limitation, still appropriate comparisons within context. A potential extension analysis could based investigation define neutral links words. Another limitation identification emotional profiles suicide notes based cognitive datasets referring everyday language usage. This problem partially addressed reconstructing emotional profiles specific syntactic relationships detected suicide notes. In way, attention given emotions individual words rather way words interconnected within observed texts. Building adopting emotional lexical resources extracted texts suicidal ideation could provide accurate readings emotional profiles, represent important goals research. \section{Conclusions} In research present first application network science quantitative analysis genuine suicide notes. Our approach combines unique advantages automated text analysis networks, using theoretical tools psychology , gain detailed understanding underlying psychological states associated suicide notes. Cognitive network methods allow us move beyond comparatively opaque, ``black-box'' models classifying suicide notes, extract key ideas emotions embedded text. This knowledge extraction allows researchers address questions higher-level psychological processes test hypotheses based theory. Although present study data-driven therefore exploratory, demonstrates cognitive networks valid approach future confirmatory investigations, leading greater understanding new possibilities suicide prevention. \section{Materials Methods} A genuine suicide note text left person subsequently committed suicide. This investigation used Genuine Suicide Notes corpus Schoene Dethlefs. The corpus represents collection 139 genuine suicide notes collected fact-checked newspaper articles previous small-scale investigations suicide notes. All notes English anonymized changing names people places reference identifying information. Shorter suicide notes, including less two sentences, discarded. This work implemented two types network construction. Co-occurrence networks captured syntactic relationships adjacent words sentence. Subject-verb-object networks captured triplets syntactic relationships subject, verb object. These network representations structure knowledge suicide nodes also enriched sentiment labels emotional labels . Additional details found in~SI:~and. Sentiment labels positive, negative neutral used structural balance analysis. Let undirected signed network, vertices edges . We define edge labels two words follows: words neutral; either negative; positive one positive neutral. As result, obtain signed network 1962 positive links, 1362 negative links 5696 neutral links. We consider neutral links play role calculating degree balance. The resulting network contains 2151 triangles. With definition shadowed triad Figure never observed signed network. The degree balanced obtained calculating fraction balanced triads. Since define edges based signs nodes, combination positive, negative neutral words leading it. In manuscript build investigate cognitive networks conceptual associations representing authors conceptually framed perceived ideas last letters. These network structures compared baseline network null models also linguistic baseline models, indicating people non-suicidal populations would framed interconnected set concepts occurring suicide notes. Whereas investigations based machine learning adopted textual corpora like love letters blog posts depression linguistic baseline models, clear type content semantic frames expected given text corpus. For instance, love letters might present feelings melancholia closely related suicidal ideation, without clear direct control experimenter. This uncertainty makes comparison difficult. For reason, followed another approach, using texts directly complex networks baseline linguistic models. We focused mind-wandering, cognitive phenomenon concepts interconnected ways relying memory free additional constraints . Mind-wandering captured free associations, i.e. naming first words coming mind thinking certain concept. Hence, use networks free associations baseline linguistic models representative mind-wandering large population individuals without suicidal tendencies. Using Small World Words dataset, built network free associations. It contains 1581 concepts, included original co-occurrence network, connected according free/mind-wandering associations. These associative structure used main text linguistic baseline investigating semantic frames emotional perceptions found suicide notes. \section*{Acknowledgements} The authors acknowledge Winter Workshop Complex Systems series, Cynthia S. Q. Siew, Benjamin Ortiz Ulloa Narges Chinichian valuable discussion. This work supported FCT, Fundao para Cicia e Tecnologia, project UIDB/50021/2020. \FloatBarrier \FloatBarrier \section*{\centering Supplementary information} \section{Network Construction} Two types networks adopted current analysis: For instance, triple ``he--look--at'' consists following three pairs: ``he--look'', ``look--at'', ``he--at''. Edge weights equal number co-occurrences two words SVO triplets. Note way decomposing SVO triplets node pairs introduce structural bias, component appears exactly two times three pairs generated single triplet. On hand, tokens playing central syntactic roles appear SVO triplets less central ones. Therefore, tokens higher degrees strengths SVO network. This important property method encodes crucial semantic features text corpora directly degree/node strength distributions. In order filter errors relationships words accidental and/or idiosyncratic individual suicide notes, SVO network limited relations weights equal greater . In words, relationships occurred least twice entire corpus considered. Then, positive weights decreased remove truncation lower tail order make sampling appropriate canonical ensemble feasible. The linguistic networks also enriched with: Both sentiment emotional datasets relative overall, global perceptions concepts represented mainstream populations. Hence, datasets directly informative subjective emotional perceptions portrayed people committed suicide. In order reconstruct subjective perceptions, network measures required. The CO SVO networks enable reconstruction semantic frame terms network neighborhood given concept. As example, semantic frame ``love'' represented first neighbors concepts syntactically related ``love'' people committed suicide suicide notes. Checking semantic frame/network neighborhood provides crucial information contexts perspectives featured ``love'' suicide notes. On emotional level, sentiment emotional attributes universal, could change according subjective perceptions authors context concepts appear. For instance, ``love'' usually indicated positive word bringing emotions trust joy. But associations like ``betraying love'' ``missing love''? Placing ``love'' different contexts alter subjective perception. This main reason language cannot simply considered bag words, i.e. collection isolated concepts, rather network interconnected linguistic units whose meaning emotions change according way networked together. Therefore networks provide access associative emotional perceptions conceptual entities minds people committing suicide. \section{Additional natural language processing} Words notes lemmatized annotated part-of-speech dependency tags classified state-of-the-art NLP library Spacy\footnote{https://spacy.io} based OntoNotes annotated corpus Penn Treebank. The tags specify roles played particular words sentence well syntactic dependency relationships them. This additional information used derive network representations notes capturing fundamental syntactic links instead simpler, sequential relationships preceding subsequent words. Specifically, allowed decomposition sentences kind generalized subject-verb-object triplets. Here, SVO triplet consists subject, seen active agent, verb seen action performed subject, object standing anything action performed subject relates to. This decomposition used somewhat general, token nominal subject subordinate verb syntactic tree sentence considered object. For instance, approach following sentence decomposed four different SVO triplets: As example shows, method disaggregates relative clauses ``tree, tall'' separate SVO triplets. This way capable capturing semantics compound complex sentences. Moreover, important meaning making tokens appear multiple triplets design. Because this, even simple summaries frequencies words SVO triplets capture important semantic features text corpora. The second important processing step custom lemmatization accounted specific way suicide notes anonymized. Most words lemmatized according standard rules implemented English language Spacy. However, names persons notes substituted several generic placeholder names, Jane William, reduced special ``s/he'' lemma. Moreover, occurrences ``he'' ``she'' also lemmatized way. \subsubsection*{Extracting SVO triplets} The general procedure used extracting SVO triplets relatively straightforward. First, documents tokenized sentences sentences tokenized words. A word considered semantic noun, pronoun, verb, adverb, adjective, adposition negation modifier . Then, semantic words assigned one following classes: Additionally, define two procedures. Finally, SVO triplets extracted sentence according following procedure: For instance, example sentence ``He looking tree, tall'' four \texttt{OBJECT} words mapped four SVO triplets: \section{Structural balance theory} Structural balance theory, first explored Heider, states signed triad balanced product signs must positive. Thus, four possible triads -- , , , -- first third considered balanced. As example, think following statements ``a friend friend friend'', ``an enemy enemy friend'', along similar others, able verify follow concept balance defined Heider. Some years later, Cartwright Harary generalized concept structural balance social networks, introducing signed graphs edges positive negative signs corresponding positive negative ties individuals. They extended concept balanced triads balanced networks allowing cycles three edges. A cycle considered balanced product signs edges positive, i.e., odd number negative edges cycle. To measure structural balance, Harary introduced concept Degree Balance signed network ratio number positive cycles total number cycles . Let signed graph, number cycles , number positive cycles , degree balance . Then: In work use cycles size three -- triads. Even though theories developed half century ago, last decade structural balance theory revivified different domains. Recently, Chiang et.al presented study exploring triadic balance brain regarding brain activity expresses possible cognitive balances people faced cooperative decisions regarding social dilemmas. They showed individual's psychological states reflected different areas brain activated situated unbalanced balanced triads. When encountering unbalanced triads, individuals showed activation brain regions associated cognitive dissonance, reinforcing Heider's theory. \section{Semantic network distance word prominence} The identification semantically related concepts traditionally performed semantic latent analysis, maps problem measuring conceptual distance selecting appropriate metrics vectorial space words. However, predicting semantic relatedness, semantic latent analysis recently outperformed network distance cognitive networks, i.e. counting smallest number conceptual links connecting two concepts connected component given network. We build upon evidence define conceptual relatedness concepts shorter network distance. A concept related to, i.e. shorter network distance from, almost concepts must prominent. This intuitive definition conceptual prominence methodologically implemented closeness centrality, identifies concepts shorter network distance connected concepts. Closeness centrality successfully used previous investigations proxy conceptual prominence predicting word acquisition . Also current analysis, use closeness centrality quantitative way identifying prominent concepts reconstructed mindset around suicidal ideation. As statistical baseline, closeness centrality concepts empirical networks matched closeness centrality configuration models, i.e. random networks fixing empirical degrees words otherwise randomizing conceptual links. SI~Table reports prominent concepts, based closeness centrality, original network co-occurrences baseline free association network . As additional check, words ranked also subgraph co-occurrence network featuring words present free association network. Co-occurrences suicide notes reflect structural organisation knowledge love central topics, pattern observed baseline free association dataset, features ""time"" ""money"" concepts higher closeness connected words. Even performing node alignment co-occurrence free associations networks, i.e. considering subgraph co-occurrences words present free association network, ""love"" remains central concepts. These results indicate ""love"" organisation knowledge assembled authors suicide notes central expected knowledge mindwandering represented free associations. \section{Network degeneracy} Degeneracy network measures tendency random walker starting random node one step ends limited set central nodes. Let normalized weighted undirected adjacency matrix weights row sum interpreted probability distributions next position random walker starting node . Then, degeneracy graph represented defined normalized difference maximal observed entropy probability distribution averaged nodes assuming equally likely: vector column means , , Shannon entropy. The term undirected star graph nodes, is: \section{Emotional Profiling} Emotional profiling performed labeling words according emotion elicit, indicated NRC emotion lexicon. The dataset included 8 basic emotional states, whose combination give rise wide variety nuanced emotion. Emotional profiling performed previous studies, considering number words eliciting given emotion certain network region, e.g. network neighborhood certain concept. The emotional profile word considered counting fraction words syntactically linked eliciting emotion . By definition ranges 0 1 . As reference model, used random sampling words fixing empirical sample size, e.g. number words syntactically linked , neglecting empirical syntactic associations. Counting fraction randomly sampled words eliciting emotion provided direct-sampling distributions random emotional profiles. We used random distributions order attribute z-score observed emotional profile . This statistical procedure, significance level fixed , enabled comparison strength emotions elicited individual concepts considered networks. On one hand, visualization z-scores facilitates immediate understanding stronger emotional intensities elicited given concept. On hand, comparison provides additional information rich concept associations eliciting given emotion."," In natural language processing , enormous pre-trained models like BERT have become the standard starting point for training on a range of downstream tasks, and similar trends are emerging in other areas of deep learning. In parallel, work on the lottery ticket hypothesis has shown that models for NLP and computer vision contain smaller matching subnetworks capable of training in isolation to full accuracy and transferring to other tasks. In this work, we combine these observations to assess whether such trainable, transferrable subnetworks exist in pre-trained BERT models. For a range of downstream tasks, we indeed find matching subnetworks at 40\% to 90\% sparsity. We find these subnetworks at  initialization, a deviation from prior NLP research where they emerge only after some amount of training. Subnetworks found on the masked language modeling task  transfer universally; those found on other tasks transfer in a limited fashion if at all. As large-scale pre-training becomes an increasingly central paradigm in deep learning, our results demonstrate that the main lottery ticket observations remain relevant in this context. Codes available at \url{https://github.com/VITA-Group/BERT-Tickets}."
"Automatic video captioning emerging area computer vision research aims generate textual descriptions visual components video. This various applications including improving video accessibility blind visually impaired, summarizing video, searching indexing. Unfortunately, training models video captioning requires manual descriptions every second video large corpus representative videos. One largest current single-clip video captioning datasets, MSR-VTT, tens thousands unqiue uncorrelated videos whereas solving video captioning likely require several orders magnitude express wide diversity subjects, situations, relationships possible video data. Active learning valuable approach domains unlabeled partially labeled examples readily available obtaining manual annotations expensive, case automatic video captioning. However, significant investigation active learning computer vision tasks object recognition, object detection, video classification video segmentation, video captioning received comparatively little attention. The reason likely rooted complexity label space. Video captioning requires sequential input output, dramatically increasing complexity traditional active learning frameworks. To knowledge, one first works define active learning strategies efficiently collecting training sets automatic video captioning. In paper explore several active learning strategies sequence sequence active learning video captioning, including uncertainty sampling based label confidence, sequence entropy query committee methods. There several unique challenges active learning deep sequence sequence models: While traditional active learning methods select one example time label, retraining model entirety new example selection, strategy impractical training models transformer networks LSTMs, due increased training time increased inference time . Thus, far efficient select large batch examples time label using crowd-sourced collection process . Traditional batch-active learning often uses ranking functions intractable deep sequence sequence learning , making active learning video description challenging problem, tractable solutions deep neural networks. In work conduct thorough empirical analysis various active learning strategies two recent standard video captioning datasets, MSR-VTT LSMDC, using transformer based LSTM based captioning models, describe novel cluster-regularized method tractable compute, provides strong performance test scenario. Our key contributions are: We presented several neural networks NER MCLs. The key aspects model MCL utilize different embeddings layer, namely, i) root embedding, ii) entity tag embedding iii) tensor layer. The effects aspects investigated individually. The use root embedding leads significant result MCLs' NER. The two also gives positive effects. For Kazakh, proposed NNs outperform CRF-based NER system state-of-the-art including character-based biLSTM-CRF model. The comparisons showed character embedding vital MCL's NER. The experimental results indicate proposed NNs potentially applied morphologically complex languages."," Automatic video captioning aims to train models to generate text descriptions for all segments in a video, however, the most effective approaches require large amounts of manual annotation which is slow and expensive. Active learning is a promising way to efficiently build a training set for video captioning tasks while reducing the need to manually label uninformative examples. In this work we both explore various active learning approaches for automatic video captioning and show that a cluster-regularized ensemble strategy provides the best active learning approach to efficiently gather training sets for video captioning. We evaluate our approaches on the MSR-VTT and LSMDC datasets using both transformer and LSTM based captioning models and show that our novel strategy can achieve high performance while using up to 60\% fewer training data than the strong state of the art baselines. \keywords{Active Learning, Video Captioning}"
"% With \gld, provide data train, example, domestic service robots interact objects typically found living environments understand natural language. % Categories eobjects within dataset include food, home, medical office supplies, tools; selected relevancy towards domestic tasks support aim. % Color depth images object captured multiple angles paired natural language descriptions objects text speech domains. % This pairing visual linguistic features object well-suited train neural networks learn multimodal associations among people interactions objects physical world. % We analyze text speech descriptions, assessing differences word choice sentence structure, understand effect two domains subsequent grounded language training. % Demonstrating use case \gld, train two manifold alignment models text speech domains show efficacy data grounded language learning tasks. Grounded language acquisition process learning language relates world---how concepts language refer objects, tasks, environments. Embodied language learning specifically significant field research NLP, machine learning, robotics. There many ways robots learn grounded language, require either multimodal data natural language data---usually both. A significant goal modern robotics research development robots operate human-centric environments. Examples include domestic service robots handle common household tasks cooking, cleaning, caretaking, robots elder care, assistive robotics providing support people disabilities, rehabilitation robotics. To useful non-specialists, robots require easy-to-use interfaces. Spoken natural language appropriate interface systems: natural, expressive, widely understood, shown proliferation natural language-based home devices. To robotic system flexibly understand language dynamic settings realize physical, goal-oriented behaviors, necessary ground linguistic perceptual inputs learned representation knowledge tied actions. Current approaches grounded language learning require data perceptual linguistic domains. While existing datasets used purpose, language component almost always derived either textual input manually transcribed speech. In practice, robots likely need operate imperfectly understood spoken language. To end, present Grounded Language Dataset , contains images common household objects description multiple formats: text, speech , automatically recognized speech derived audio files. We present experiments demonstrate utility dataset grounded language learning. The primary contributions paper follows: In paper, presented initial set methods aiming tackle active learning problem video description, challenging task requiring complex modeling due complexity output distribution, many active learning methods unable function efficiently, all. We shown achieve 95\"," Grounded language acquisition --- learning how language-based interactions refer to the world around them --- is a major area of research in robotics, NLP, and HCI. In practice the data used for learning consists almost entirely of textual descriptions, which tend to be cleaner, clearer, and more grammatical than actual human interactions. In this work, we present the Grounded Language Dataset , a multimodal dataset of common household objects described by people using either spoken or written language. We analyze the differences and present an experiment showing how the different modalities affect language learning from human input. This will enable researchers studying the intersection of robotics, NLP, and HCI to better investigate how the multiple modalities of image, text, and speech interact, as well as how differences in the vernacular of these modalities impact results."
"Recent works demonstrated interest unsupervised representation learning pretraining method obtain good speech features downstream tasks little labelled data . While Contrastive Predictive Coding derivatives appear versatile methods unsupervised representation learning , yet reach state-of-the-art results purely unsupervised learning metrics . % Previous research shown Data augmentation useful supervised training, also key component unsupervised setups image domain. It well established unsupervised learning speech, sequential nature signal may introduce specificities. Our first objective explore several types time-domain data augmentation several methods augmenting contrastive framework English . In second stage, extend results languages zero-resource 2017 benchmark. Lastly, show data augmentation benefits semi-supervised training, using Libri-light benchmark. John's comments consider: Looking ImageNet paper: http://www.image-net.org/papers/imagenet_cvpr09.pdf explicitly list ideas format: . What specific ideas want call out? Heterogeneous manifold alignment , shown paper Grounded language concept acquisition Active learning Natural language interfaces personal assistants, domestic service robots Human-robot interaction -- given high-level topics, think *specific* challenge / task \gld helps address In paper present \gld, grounded language dataset images color depth paired natural language descriptions everyday household objects text speech. We aim make resource useful starting point downstream grounded language learning tasks spoken natural language interfaces personal assistants domestic service robots. To demonstrate potential use \gld, use data train models perform heterogeneous manifold alignment. We hope dataset serves researchers resourceful starting point explore many techniques, model architectures, algorithms understanding grounded language. In particular, inclusion speech alongside written textual descriptions allows side-by-side comparisons two domains grounded physical objects, novel multimodal techniques involving three domains vision, text, speech. The idiosyncratic properties \gld\ suggest many research questions future study. For example, remark \gld\ includes descriptions object observed multiple angles. One interesting question explore, then, would identify objects different perspective information missing. Another property \gld\ descriptions focus use object , others report perceptual qualities objects logos identifying features uniquely visible annotator's current perspective object. This aspect could incorporated human-robot interaction study examines grounding language objects physical viewed embodied agents different vantage points. Thus, \gld\ could powerful resource categorizing using information learn perspectives better handle occlusions visual tasks. In near term, interested leveraging dataset train robots understand natural language order perform tasks domestic context. The inclusion medical kitchen supplies critical training robot tasks cooking, cleaning, administering care. As work toward goal, anticipate creating expanded catalog items including diverse ways people describe talk wide variety items encounter every day."," % 150/200 words max Contrastive Predictive Coding , based on predicting future segments of speech based on past segments is emerging as a powerful algorithm for representation learning of speech signal. However, it still under-performs other methods on unsupervised evaluation benchmarks. Here, we introduce WavAugment, a time-domain data augmentation library and find that applying augmentation in the past is generally more efficient and yields better performances than other methods. We find that a combination of pitch modification, additive noise and reverberation substantially increase the performance of CPC , beating the reference Libri-light results with 600 times less data. Using an out-of-domain dataset, time-domain data augmentation can push CPC to be on par with the state of the art on the Zero Speech Benchmark 2017. We also show that time-domain data augmentation consistently improves downstream limited-supervision phoneme classification tasks by a factor of 12-15\% relative.   %\matthijs{mention that the best data augmentation can reduce the required training data by a factor 600 } %\emmanuel{not really true; a lot of the effect could be due to a change in architecture} %\morgane{if we look at the ablation study in the appendix, it seems that the architecture, on LS-100, is responsible for about 1/3 of the improvement. But we must also consider other factors not mentionned in this paper: noise and speaker distribution.}"
"Recent developments fields electronics, computations data processing led increased interest smart assistants speech interfaces. It likely driven fact usually people learn use speech interaction intuitively without special training make primary medium information exchange. However, speech poses major challenge machine comes task extraction information intended transmitted human speaker, also known Spoken Language Understanding . The key difficulty speech highly variable, e.g. depending room acoustic, contains rich information speakers . Some useful SLU. The information extraction task often performed text representation using Natural Language Understanding methods , Automatic Speech Recognition systems convert speech text. ASR step removes redundant information input provides kind normalized form output. At time causes loss potentially useful information encoded text representation, prosody, loudness speech rate. The operation finding probable sequence words speech input computationally expensive. %, makes hard implement practice. This partly solved various heuristics avoiding exploration less probable hypothesis , turn introduces additional errors propagated NLU component. Finally, sequential design pipeline approach leads unavoidable source latency, NLU component start work ASR finished, desirable interactive context smart assistant. The problems pipeline approach described solved end-to-end SLU methods. Existing works end-to-end SLU modeling either focus supervised downstream tasks, example dialog act classification , intent detection , slot filling , independent intent detection domain classification joint intent detection, domain classification slot filling , target generic semantic embedding usually inspired successful models word embeddings Word2Vec contextual text embeddings BERT . Highly variable complex nature speech leads large amounts data computational resources required SLU training compared NLU training, especially recently popular approach based contextual embeddings. While data requirements could satisfied unsupervised approaches, computational resources still problem. Fortunately, modern language processing methods, including ASR NLU, based neural networks deep learning. Deep learning offers easy way transfer knowledge learned tasks. This technique referred transfer learning successfully applied ASR NLU . Therefore, transfer learning promising direction explore SLU well. Several reports indicate transfer learning audio modality pretraining ASR task or, alternatively, speech autoencoding, helpful downstream SLU tasks. Transfer learning text modality, however, applied Speech2Vec SpeechBERT far. We propose novel method combines parameters transfer well trained end-to-end ASR systems pretrained ESPnet end-to-end NLU models pretrained BERT Teacher-Student learning final alignment SLU output space NLU output space order construct end-to-end SLU model allowing few-shot transfer downstream tasks text speech. By so, enable pretrained end-to-end contextual embeddings BERT process acoustic features. In particular, aim generate fixed length vectors semantic representation speech segments variable length. Transfer learning text audio modalities makes approach mostly similar . In work, investigate utterance classification task focus zero-shot few-shot cases, described method could adopted many types SLU tasks. Although previous works described number experiments utterance classification tasks dialog act classification intent classification , use datasets evaluation, compare results directly works, outside scope work. With data augmentation, CPC take good advantage relatively short clean well segmented speech, although currently insufficient learn competitively small amounts data . More research needed extend techniques directions: small amounts data, large, potentially noisy datasets. In addition, differences observe data-augmentation effects open issue systematic exploration data augmentation function tasks architectures. . Not submission copy"," Spoken language understanding is typically based on pipeline architectures including speech recognition and natural language understanding steps. These components are optimized independently to allow usage of available data, but the overall system suffers from error propagation. In this paper, we propose a novel training method that enables pretrained contextual embeddings to process acoustic features. In particular, we extend it with an encoder of pretrained speech recognition systems in order to construct end-to-end spoken language understanding systems. Our proposed method is based on the teacher-student framework across speech and text modalities that aligns the acoustic and the semantic latent spaces. Experimental results in three benchmarks show that our system reaches the performance comparable to the pipeline architecture without using any training data and outperforms it after fine-tuning with ten examples per class on two out of three benchmarks."
"Self-supervised learning representations large unlabeled datasets popular contemporary trend machine learning. After widely adopted areas like natural language processing computer vision, self-supervision rapidly developing noteworthy topic audio speech processing. Self-supervision aims capture informative properties underlying structure unlabeled data learn generalized representations. This extremely promising problem settings involving large amount unlabeled data limited labeled data. In context audio speech processing, relevant low resource languages, emotion recognition, cross-cultural speech recognition problems small-sized datasets. Even though recent research interest self-supervised learning speech data, works focus audio modality alone. Audiovisual speech data offers interesting possibilities cross-modal self-supervision, something relatively lesser explored. In work, present method self-supervised representation learning audio features leverages audio visual modalities. We demonstrate generating talking lip video single frame corresponding audio used pretext task visual self-supervision train raw audio encoder. We combine audio-only self-supervision based predicting informative audio attributes, similar . This results audio encoder trained joint audiovisual self-supervision. We evaluate method spoken word classification achieve competitive results comparing existing self-supervised methods. Our method also results significantly better performance learning limited data downstream tasks. Importantly, method also outperforms fully supervised training . Our observations motivate utility self-supervised pretraining audio related tasks. We demonstrate cross-modal supervision audiovisual speech learn better representations compared unimodal audio-only visual-only self-supervision. \subsection{Related work} Self-supervised learning influential recent advances natural language processing computer vision . It also beginning mature relevant topic audio speech processing. CPC seminal work self-supervised learning also demonstrated applicability contrastive self-supervised learning audio. Wav2vec refines idea CPC specifically speech. CPC based self-supervision also shown generalize well multiple languages . APC similar approach predicts next token speech segment history. Another relevant recent work PASE , aims learn multi-task speech representations raw audio predicting number handcrafted features MFCCs, prosody waveform. Teacher-student models also explored audio self-supervision trained model previous epoch acts teacher model next epoch . % Phase prediction also proposed audio-based pretext task. WaveNet generative model raw audio waveforms used generic audio representations. All works discussed far unimodal audio-only self-supervised methods. There also works utilize audio visual information. There multiple ways capture cross-modal interaction including audiovisual synchronization , cross-modal transition modeling , cross-modal pseudolabel based clustering , contrastive learning , audiovisual instance discrimination . However works present cross-modal self-supervision context generic audiovisual data, application tasks like video action recognition acoustic scene classification. There limited work explores self-supervision specifically context audiovisual speech. We explored concept recent related work . This work extends idea prior work. Specifically, move learning speech representations directly raw audio instead mel features. We also adopt different refined approach audio-only self-supervision . We proposed combine parameters transfer well trained ASR NLU models Teacher-Student learning final alignment SLU output space NLU output space order construct end-to-end SLU model allowing few-shot transfer downstream tasks text speech. We outlined necessary steps settings practical pretrained NLU model adaptation SLU via cross-modal transfer. Our system reaches accuracy 58.60\ , 60.18\ 91.12\ SwBD, MRDA FSC datasets without fine-tuning 60.22\ , 61.32\ 95.49\ fine-tuning ten labeled samples per class compared 57.23\ , 64.06\ 94.57\ reached pipeline system. The results research support idea text pretrained contextual embeddings useful tasks outside text modality. The present study also adds new tasks growing body research language processing methods using Transformer neural networks."," The intuitive interaction between the audio and visual modalities is valuable for cross-modal self-supervised learning. This concept has been demonstrated for generic audiovisual tasks like video action recognition and acoustic scene classification. However, self-supervision remains under-explored for audiovisual speech. We propose a method to learn self-supervised speech representations from the raw audio waveform. We train a raw audio encoder by combining audio-only self-supervision  with visual self-supervision . The visual pretext task drives the audio representations to capture information related to lip movements. This enriches the audio encoder with visual information and the encoder can be used for evaluation without the visual modality. Our method attains competitive performance with respect to existing self-supervised audio features on established isolated word classification benchmarks, and significantly outperforms other methods at learning from fewer labels. Notably, our method also outperforms fully supervised training, thus providing a strong initialization for speech related tasks. Our results demonstrate the potential of multimodal self-supervision in audiovisual speech for learning good audio representations."
"Singing voice synthesis , generates singing voices lyrics, attracted lot attention research industrial community recent years. Similar text speech enables machines speak, SVS enables machines sing, greatly improved rapid development deep neural networks. Singing voices complicated prosody normal speaking voices, therefore SVS needs additional information control duration pitch singing voices, makes SVS challenging TTS. Previous works SVS include lyrics-to-singing alignment, parametric synthesis, acoustic modeling, adversarial synthesis. Although achieve reasonably good performance, systems typically require 1) large amount high-quality singing recordings training data, 2) strict data alignments lyrics singing audio accurate singing modeling, incur considerable data labeling cost. Previous works collect two kinds data follows: As seen, training data SVS mostly rely human recording annotations. What more, publicly available singing datasets, increases entry cost researchers work SVS slows research product application area. Considering lot tasks language modeling generation, search ads ranking, image classification heavily rely data collected Web, natural question that: Can build SVS system data collected Web? While plenty songs music websites mining training data Web seems promising, face several technical challenges: In paper, develop DeepSinger, singing voice synthesis system built scratch using singing training data mined music websites. To address challenges, design pipeline DeepSinger consists several data mining modeling steps, including: Specifically, detailed designs lyrics-to-singing alignment model singing model follows: We conduct experiments mined singing dataset evaluate effectiveness DeepSinger. Experiment results show singing data purely mined Web, DeepSinger synthesize high-quality singing voices terms pitch accuracy voice naturalness. The contributions paper summarized follows: There multiple interesting observations obtained results. Audio-only supervision yields better results visual-only supervision. However, model trained joint audiovisual self-supervision performs better models trained unimodal audio-only visual-only self-supervision almost scenarios. including noisy datasets. This highlights utility complementary information encoded visual self-supervision demonstrates potential multimodal self-supervision useful tool speech representation learning. Also notably, despite tested methods similar performance full datasets, clear gap using small training set method best learning fewer labels, relevant low resource domains. This significant impact problems like low resource language ASR, emotion recognition cross-cultural ASR. Our method also significantly outperforms fully supervised training scratch, motivates utility self-supervised pretraining speech. \paragraph{Limitations} joint training directly work, overfitting, cite cvpr2020 paper slightly worse SOTA full datasets frozen features bad \paragraph{Future work} This work progress many speech related applications evaluate model on. In work, focused classification isolated words. We also test model continuous CTC based speech recognition datasets like Librispeech TIMIT, tasks like speaker identification speech emotion recognition. An especially relevant application would low resource language ASR. There also interesting directions explore improve method. In work, exhibit joint audiovisual information used audio representation learning. In similar manner, could also utilize cross-modal information visual representation learning . Another interesting line work multimodal contrastive self-supervised learning demonstrated generic audiovisual data audiovisual speech. \medskip \small \section*{Appendix} \section{Audio encoders} \right]\left[ \right]$}\\ & \\ \multirow{2}{*}{conv4\_x} & \multirow{2}{*}{256x1000} &\\ & \\ \multirow{2}{*}{conv5\_x} & \multirow{2}{*}{512x500} &\\ & \\ avgpool & 515x25 & \\ \bottomrule \paragraph{Pretraining datasets baselines} The results Table baseline methods computed using public code pretrained models provided authors. These baseline methods pretrained varying amounts types data. For completely fair comparison, methods need pretrained data. We experimented pretraining baseline methods 36 hour LRW frontal subset use method. The results obtained baseline methods using approach either equivalent worse public pretrained models. This shows model may able learn better representations amount pretraining data. However results, use public pretrained models may assist reproducibility. Maybe add table datasets model pretrained along size \vfill\break \columnbreak \section{Dataset split details} \vfill\break \section{Heading 2} tSNE plots?"," In this paper\footnote{$^*$ Equal contribution. $\dagger$ Corresponding author.}, we develop DeepSinger, a multi-lingual multi-singer singing voice synthesis  system, which is built from scratch using singing training data mined from music websites. The pipeline of DeepSinger consists of several steps, including data crawling, singing and accompaniment separation, lyrics-to-singing alignment, data filtration, and singing modeling. Specifically, we design a lyrics-to-singing alignment model to automatically extract the duration of each phoneme in lyrics starting from coarse-grained sentence level to fine-grained phoneme level, and further design a multi-lingual multi-singer singing model based on a feed-forward Transformer to directly generate linear-spectrograms from lyrics, and synthesize voices using Griffin-Lim. DeepSinger has several advantages over previous SVS systems: 1) to the best of our knowledge, it is the first SVS system that directly mines training data from music websites, 2) the lyrics-to-singing alignment model further avoids any human efforts for alignment labeling and greatly reduces labeling cost, 3) the singing model based on a feed-forward Transformer is simple and efficient, by removing the complicated acoustic feature modeling in parametric synthesis and leveraging a reference encoder to capture the timbre of a singer from noisy singing data, and 4) it can synthesize singing voices in multiple languages and multiple singers. We evaluate DeepSinger on our mined singing dataset that consists of about 92 hours data from 89 singers on three languages . The results demonstrate that with the singing data purely mined from the Web, DeepSinger can synthesize high-quality singing voices in terms of both pitch accuracy and voice naturalness\footnote{Our audio samples are shown in \url{https://speechresearch.github.io/deepsinger/}.}."
"In contemporary pop music, linguistic content singing voice generally referred lyrics process automatic retrieval lyrics content singing voice defined Automatic Lyrics Transcription. The automatic retrieval pronounced words speech signals widely developed research field state art systems today successfully applied industrial applications. However, level robustness yet reached input singing voice. According prior research, several domain specific reasons word recognition performance reduces singing including domain specific acoustic characteristics alterations word pronunciations. Specifically machine learning perspective, main bottleneck achieving robust system availability training data fine-grained annotations, used supervised learning framework. In study, exploit large-scale singing voice dataset, DAMP - Sing! 300x30x2 - released Smule \footnote{Smule commercial Karaoke singing application. More info https://www.smule.com/}, prompt-level\footnote{In Smule app, lyrics prompted users words sentences depending song arrangement. Each prompted sentence word annotation referred prompt-level annotation.} annotations provided. The dataset consists monophonic Karaoke recordings pop songs multiple performers providing near 150 hours trainable audio data. However, dataset widely utilized purpose training word recognition system. Through proposed framework, aim highlight one way utilization dataset complete ALT framework conduct in-depth self-attention analysis via fine-tuning experiments. A robust system retrieval sung lyrics variety potential applications music information retrieval related tasks music tech industry. In karaoke music education apps, recognition sung words essential tracking performance providing feedback user. In combination techniques like query-by-humming, ALT utilized song identification metadata retrieval tasks. Our system uses deep neural networks building final acoustic model composed 2D convolutional layers front end extracting robust features followed time-delay layers due capability modeling long-term context information. A self-attention layer added final projection layer weighting time context computing output activations classification. Overall, paper targets making following contributions: This paper structured follows: literature ALT monophonic singing recordings reviewed . Then details data used training evaluation given . Then proposed system basis experiments explained. The results experimental steps shown in-depth analysis self attention parameters performed . Finally, potential improvements proposed system discussed . We propose novel multi-target self-supervised training scheme called TERA, use multiple auxiliary objectives instead one pre-training. With proposed alterations, diversity input increased, performance, especially case less pre-training data, improved. We demonstrate strong results tasks phoneme classification, speaker recognition, speech recognition. We conduct complete ablation study, thorough comparison recent representation learning pre-training approaches.","  Speech recognition is a well developed research field so that the current state of the art systems are being used in many applications in the software industry, yet as by today, there still does not exist such robust system for the recognition of words and sentences from singing voice. This paper proposes a complete pipeline for this task which may commonly be referred as automatic lyrics transcription . We have trained convolutional time-delay neural networks with self-attention on monophonic karaoke recordings using a sequence classification objective for building the acoustic model. The dataset used in this study, DAMP - Sing! 300x30x2\cite{b41} is filtered to have songs with only English lyrics. Different language models are tested including MaxEnt and Recurrent Neural Networks based methods which are trained on the lyrics of pop songs in English. An in-depth analysis of the self-attention mechanism is held while tuning its context width and the number of attention heads. Using the best settings, our system achieves notable improvement to the state-of-the-art in ALT and provides a new baseline for the task."
"%Given limited amount information single-microphone mixture recording, separating constituent sources challenging task. Considering also limited computational resources one might disposal, problem training deploying separation model might become especially challenging. However, efficient sound source separation frameworks could utilized towards enhancing performance various systems generally require clean audio inputs . The advent deep learning era enabled effective usage neural networks towards single-channel source separation mask-based architectures . Recently, end-to-end source separation time-domain shown state-of-the-art results variety separation tasks as: speech separation , universal sound separation music source separation . The separation module ConvTasNet variants consist multiple stacked layers depth-wise separable convolutions aptly incorporate long-term temporal relationships. Building upon effectiveness large temporal receptive field, dual-path recurrent neural network shown remarkable performance speech separation. Demucs refined U-Net structure shown strong performance improvement music source separation. Specifically, consists several convolutional layers downsampling operation performed order extract high dimensional features. A two-step approach introduced showed universal sound separation models could improved working directly latent space learning ideal masks separate step. Despite dramatic advances source separation performance, computational complexity aforementioned methods might hinder extensive usage across multiple devices. Specifically, many algorithms amenable to, e.g., embedded systems deployment, environments computational resources constrained. Additionally, training systems also expensive computational undertaking amount significant costs. Several studies, mainly image domain, introduced efficient architectures order overcome growing concern large models high computational requirements. Models depth-wise separable convolutions shown strong potential several image-domain tasks significantly reducing computational requirements. Thus, several variants MobileNets proposed deep learning edge-devices. However, convolutions large dilation factor might inject several artifacts thus, lightweight architectures combine several dilation factors block proposed image tasks . More recent studies propose meta-learning algorithms optimizing architecture configurations given specific computational resource accuracy requirements . Despite recent success low-resource architectures image domain, little progress made towards proposing efficient architectures audio tasks especially source separation. In WaveRNN used efficient audio synthesis terms floating point operations latency. Other studies introduced audio source separation models reduced number trainable parameters binarized models . In study, propose novel efficient neural network architecture audio source separation following holistic approach terms computational resources take consideration . Our proposed model performs SUccessive DOwnsampling Resampling Multi-Resolution Features using depth-wise convolutions. By so, \sudo exploits effectiveness iterative temporal resampling strategies avoids need multiple stacked dilated convolutional layers . We report separation performance comparable even better several recent state-of-the-art models speech environmental sound separation tasks significantly lower computational requirements. Our experiments suggest \sudo models a) could deployed devices limited resources, b) trained significantly faster achieve good separation performance c) scale well increasing number parameters. Our code available online. Though achieving approximately 5\"," In this paper, we present an efficient neural network for end-to-end general purpose audio source separation. Specifically, the backbone structure of this convolutional network is the SUccessive DOwnsampling and Resampling of Multi-Resolution Features  as well as their aggregation which is performed through simple one-dimensional convolutions. In this way, we are able to obtain high quality audio source separation with limited number of floating point operations, memory requirements, number of parameters and latency. Our experiments on both speech and environmental sound separation datasets show that \sudo performs comparably and even surpasses various state-of-the-art approaches with significantly higher computational resource requirements."
"Today's state-of-the-art language modeling ASR relies neural Language Models , capable handling continuous space thereby outperforming traditional Back-off N-gram LMs . BNLMs cannot exploit long context based syntactic dependencies also less flexible terms generalization unseen cases, semantic knowledge captured training them. %\subsection{Latency issues} Neural LMs however undesired property, computationally heavy decoding, neural LMs cannot effectively used single decoding pass, rather exploited rescoring lattices obtained first decoding pass BNLM. It obvious, also shown, information lost first decoding pass, pruning recognition network based short context syntax, discarding longer context syntactic quasi semantic knowledge. Another problem arising increased latency system two decoding passes, hampers exploitation strict online requirements. To reduce limitations exploiting neural LMs ASR, several solutions proposed. In shown using neural LM generate augmented training corpus train improved BNLM best performing strategy. Such BNLM trained augmented corpus used single pass first pass decoding. Sometimes called approximative models try capture knowledge neural model augmented training corpus. %Recently several studies concentrated approach. Suzuki et al. uses domain balanced mixture training corpora train shallow RNNLM text generation, improve speech recognition results Japanese, Korean English. Wang et al. report using general domain pre-trained Transformer augment text corpora used train LMs. They demonstrate pre-trained fine-tuned Transformer performs significantly better data augmentation LSTMs simple in-domain Transformer models. %Both underline approach particularly useful in-domain training data relatively scarse . %\subsection{Variable syntax} Another burden language modeling morphologically rich languages different syntactic properties language compared English. Heavy agglutination results much larger vocabularies, problem itself, causes problems too: %beside handling words, individual word forms occur less often hence, size training corpus accordingly augmented maintain predictive power dataset. Moreover, suffixes express grammatical relations usually provided word order English, morphologically rich languages tend permissive choosing word order, leading higher variation. This impairs BNLM estimation badly, may also cause word embeddings become less powerful terms syntactic semantic consistency, even despite using long context windows. %%%BLIND VERSION: %This impairs BNLM estimation badly, may also cause word embeddings become less powerful terms syntactic semantic consistency~[BLIND], even despite using long context windows. To alleviate problems linked different organization morphologically rich languages, subword unit modeling often used alternative. Subword unit based ASR demonstrated improve WER several morphologically rich languages. Suzuki et al. use subword approach data augmentation enrich text corpora train BNLM, compose subwords back words prepare final LM, unlike approach retokenizes words subword units final LM. %\subsection{Our contribution} In paper aim improve LM online call center ASR system morphologically rich Hungarian. We use parliamentary text pre-train GPT-2 structure Transformer LM, fine-tune target domain. With model generate training text BNLM. We demonstrate Transformer based data augmentation efficient morphologically rich Hungarian, %which improves ASR vocabulary large enough large BNLM used. Retokenizing augmented training corpus subword units, training subword-based BNLM it, demonstrate ASR accuracy improves compared word based baseline augmented BNLM, footprint complexity resulting subword unit augmented BNLM significantly decrease. As subword unit LMs known perform better wide range morphologically rich languages, hypothesize approach transferable languages. We consider novelties paper following: propose retokenization Transformer augmented LM training corpus; % use online ASR single pass decoding; first use GPT-2 Transformer structure augment LM training corpora; first apply Transformer based LM Hungarian ASR task; demonstrate subword-based neural text augmentation exceptionally efficient modeling OOV words. In study, introduced \sudo network, novel architecture efficient universal sound source separation. The proposed model capable extracting multi-resolution temporal features successive depth-wise convolutional downsampling intermediate representations aggregates using non-parametric interpolation scheme. In way, \sudo models able significantly reduce required number layers order effectively capture long-term temporal dependencies. We show models perform similarly even better recent state-of-the-art models requiring significantly less computational resources FLOPs, memory time. In future, aim use \sudo models real-time low-cost source separation. In study, introduced \sudo network, novel architecture efficient universal sound source separation. The proposed model capable extracting multi-resolution temporal features successive depth-wise convolutional downsampling intermediate representations aggregates using non-parametric interpolation scheme. In way, \sudo models able significantly reduce required number layers order effectively capture long-term temporal dependencies. In speech environmental sound separation experiments shown \sudo architectures perform similarly even better recent state-of-the-art models requiring significantly less computational resources FLOPs, memory time. In future, aim use \sudo models real-time low-cost sound source separation. References produced using bibtex program suitable BiBTeX files . The IEEEbib.bst bibliography style file IEEE produces unsorted bibliography list. ------------------------------------------------------------------------- LaTeX template MLSP papers. To used with: * mlspconf.sty - ICASSP/ICIP LaTeX style file adapted MLSP, * IEEEbib.bst - IEEE bibliography style file. -------------------------------------------------------------------------- \documentclass{article} \usepackage{amsmath,graphicx,mlspconf} Copyright notices. ------------------ Select one four copyright notices below. Only required camera-ready paper submission. * For papers authors employed US government: \copyrightnotice{U.S.\ Government work protected U.S.\ copyright} * For papers authors employed Crown government : \copyrightnotice{978-1-7281-6662-9/20/\31.00 {\copyright}2020 European Union} * For papers: \copyrightnotice{978-1-7281-6662-9/20/\^{\star \dagger}^{\star}^{\dagger}^{\star}^{\dagger}$ Affiliation Number Two }"," Recently Deep Transformer models have proven to be particularly powerful in language modeling tasks for ASR. Their high complexity, however, makes them very difficult to apply in the first  pass of an online system. Recent studies showed that a considerable part of the knowledge of neural network Language Models  can be transferred to traditional n-grams by using neural text generation based data augmentation. In our paper, we pre-train a GPT-2 Transformer LM on a general text corpus and fine-tune it on our Hungarian conversational call center ASR task. We show that although data augmentation with Transformer-generated text works well for isolating languages, it causes a vocabulary explosion in a morphologically rich language. Therefore, we propose a new method called subword-based neural text augmentation, where we retokenize the generated text into statistically derived subwords. We compare Morfessor and BPE statistical subword tokenizers and show that both methods can significantly improve the WER while greatly reducing vocabulary size and memory requirements. Finally, we also demonstrate that subword-based neural text augmentation outperforms the word-based approach not only in terms of overall WER but also in recognition of OOV words."
"Deep neural networks play central role state-of-the-art automatic speech recognition systems. When designing systems, set DNN structure design decisions hidden layer dimensionality connectivity need made. These decisions largely based expert knowledge empirical choice. As explicitly training evaluating performance different network architectures highly expensive, preferable use automatic architecture design techniques. To end, neural architecture search approaches gained increasing interests recent years. The key objectives NAS methods three fold. First, crucial produce accurate performance ranking different candidate neural architectures allow best system selected. Second, operating level accuracy performance target, preference given simpler architectures fewer parameters order minimize risk overfitting limited data. Furthermore, ensure scalability efficiency large data sets, search space containing candidate systems interest needs defined. Earlier forms NAS techniques based neural evolution, genetic algorithms used randomly select architecture choices iteration mutation crossover. Bayesian NAS methods based Gaussian Process proposed in. Reinforcement learning based NAS approaches also developed. In techniques, explicit system training evaluation required. In addition, architecture hyper-parameters actual DNN parameters separately learned, e.g., within RL controller candidate systems, tighter integration preferred NAS. Alternatively, differentiable architectural search techniques used. Architectural search performed over-parameterized parent super-network containing paths connecting candidate DNN structures considered. The search transformed estimation weights assigned candidate neural architecture within super-network. The optimal architecture obtained pruning lower weighted paths. This allows architecture selection candidate DNN parameters consistently optimized within super-network model. In contrast rapid development NAS techniques machine learning computer vision communities, limited research applying speech recognition systems far. In paper, range DARTS based NAS techniques used automatically learn two architecture hyper-parameters heavily affect performance model complexity state-of-the-art factored time delay neural network acoustic models: i) left right splicing context offsets; ii) dimensionality bottleneck linear projection hidden layer. These include standard DARTS method fully integrating estimation architecture weights TDNN parameters lattice-free Maximum Mutual Information training; Gumbel-Softmax DARTS reduces confusion candidate architectures; pipelined DARTS circumvents overfitting architecture weights using validation data; penalized DARTS incorporates resource constraints flexibly adjust trade-off performance system complexity. Parameter sharing among candidate architectures also used facilitate efficient search large number TDNN systems. Experiments conducted 300-hour Switchboard conversational telephone speech recognition task suggest NAS configured TDNN-F systems consistently outperform baseline LF-MMI trained TDNN-F systems using manually designed configurations. Absolute word error rate reductions 1.0\% model size reduction 28\% relative obtained. In order evaluate performance proposed NAS techniques, applied automatically configure two sets hyper-parameters state-of-the-art disordered speech recognition task based UASPEECH corpus: skip connection layers dimensionality factored TDNN weight matrices. To best knowledge, paper among first apply neural architecture search techniques TDNNs speech recognition tasks. In contrast, vast majority previous NAS research focused computer vision applications. Existing NAS works speech community investigated non-TDNN based architectures. % Once paper accepted, release code. The rest paper organized follows. Section 2 presents set differentiable NAS techniques. Section 3 discusses search space TDNN-F models necessary parameter sharing improve search efficiency. Section 4 presents experiments results. Finally, conclusions drawn Section 5. In paper, extended neural text generation based data augmentation method presented We introduced approach called subword-based neural text augmentation extension Transformer based LM augmentation method presented morphologically rich languages. With new approach managed improve WER online ASR system Hungarian call center conversions 10\ relative . Our solution also outperforms original, word-based data augmentation technique terms WER OOV recognition capability keeping vocabulary size memory requirements system quite low. Besides, best knowledge first paper applying GPT-2 Transformer generate augmentation data ASR language model. A rekurrens  transformer modellek egyesel tovbi 1\ het el."," Deep neural networks  based automatic speech recognition  systems are often designed using expert knowledge and empirical evaluation. In this paper, a range of neural architecture search  techniques are used to automatically learn two types of hyper-parameters of state-of-the-art factored time delay neural networks : i) the left and right splicing context offsets; and ii) the dimensionality of the bottleneck linear projection at each hidden layer. These include the DARTS method integrating architecture selection with lattice-free MMI  TDNN training; Gumbel-Softmax and pipelined DARTS reducing the confusion over candidate architectures and improving the generalization of architecture selection; and Penalized DARTS incorporating resource constraints to adjust the trade-off between performance and system complexity. Parameter sharing among candidate architectures allows efficient search over up to $7^{28}$ different TDNN systems. Experiments conducted on the 300-hour Switchboard corpus suggest the auto-configured systems consistently outperform the baseline LF-MMI TDNN systems using manual network design or random architecture search after LHUC speaker adaptation and RNNLM rescoring. Absolute word error rate  reductions up to 1.0\% and relative model size reduction of 28\% were obtained. Consistent performance improvements were also obtained on a UASpeech disordered speech recognition task using the proposed NAS approaches. % Deep neural networks  based automatic speech recognition  systems are often designed using expert knowledge and empirical evaluation. In this paper, a range of neural architecture search  techniques are used to automatically learn three hyper-parameters that heavily affect the performance and model complexity of state-of-the-art factored time delay neural network  acoustic models: i) the left and right splicing context offsets; ii) the dimensionality of the bottleneck linear projection and iii) skip connections at each hidden layer. These include the standard DARTS method fully integrating the estimation of architecture weights and TDNN parameters in lattice-free MMI  training; Gumbel-Softmax DARTS that reduces the confusion between candidate architectures; Pipelined DARTS that circumvents the overfitting of architecture weights using held-out data; and Penalized DARTS that further incorporates resource constraints to adjust the trade-off between performance and system complexity. Parameter sharing among candidate architectures was also used to facilitate efficient search over up to $7^{28}$ different TDNN systems. Experiments conducted on a 300-hour Switchboard task suggest the NAS auto-configured TDNN-F systems consistently outperform the baseline LF-MMI trained TDNN-F systems using manual expert configurations. Absolute word error rate reductions up to 1.0\% and relative model size reduction of 28\% were obtained. Consistent performance improvements were also obtained in the disorder UASPEECH task."
"A speech enhancement system aims restoring quality intelligibility noisy speech. The state-of-the-art speech enhancement systems commonly built deep neural network based vector-to-vector regression models, inputs context-dependent log power spectrum features noisy speech outputs correspond either clean enhanced LPS features. Although deep neural network based speech enhancement demonstrated state-of-the-art performance single-channel setting, also extended scenarios multi-channel speech enhancement even better-enhanced speech qualities . The process single multi-channel speech enhancement taken DNN based vector-to-vector regression aiming bridging functional relationship input noisy speech mapped corresponding clean speech . In, DNNs feed-forward fully-connected hidden layers proposed attain state-of-the-art performance speech enhancement target tasks related theorems later set in. In follow-up studies, recurrent neural networks , convolutional neural networks investigated boost speech enhancement quality. Moreover, deep bidirectional RNN LSTM gates instead used in, generative adversarial network attempted speech enhancement tasks in. In particular, CNN tensor-to-vector regression model capable dealing 3D/4D tensorized input data. Besides, recent works suggest CNN outperform DNN RNN counterparts speech enhancement. Similarly, tensor-to-vector regression model also built directly employing proposed tensor-train network . Besides, TT-DNN compact representation fully-connected layers DNN tensor-train format. In, first attempt tensor-train deep neural network tackle multi-channel speech enhancement task also demonstrate TT representation DNN cause quality degradation enhanced speech, also results significant reduction model parameters. More importantly, quality speech enhancement improved DNN counterpart allowing TT-DNN parameters grow. A significant advantage tensor-to-vector regression, CNN TT-DNN, compact architecture observe stringent hardware constraints, computational resources often limited. Therefore, worth investigating models terms representation power, experimentally comparing considering trade-off enhancement performance number model parameters. On one hand, CNN powerful model learn spatial-temporal features extract semantically meaningful aspects higher hidden layers. On hand, TT-DNN maintain baseline results corresponding DNN applying TT transformation FC hidden layers. Hence, work, focus tensor-to-vector model take advantage CNN TT-DNN. More specifically, propose novel hybrid architecture, namely CNN-TT, convolutional layers stacked bottom one TT hidden layer top. To highlight advantages CNN-TT, compare different deep tensor-to-vector models speech enhancement. The used models work include DNN; CNN; TT-DNN; CNN-TT. In detail, first explain fundamental mechanisms tensor-to-vector regression based theorems DNN based vector-to-vector regression. Then, validate CNN-TT models speech enhancement tasks. %we know CNNs powerful models learn salient features input domain lower layers, semantically meaningful aspect problem domain higher layer . CNN-based architectures reported top results several machine learning tasks, e.g., On hand, TT-DNN significantly reduce number parameters keeping original performance seeding DNN applying TT transformation fully-connected hidden layers. That TT-DNN stores TT-format DNN, i.e., set low-rank core tensors, used approximately reconstruct original DNN. Moreover, running complexities DNN corresponding TT-DNN order . In work, interested finding best tensorized architecture allows best trade-off model size speech enhancement quality, propose novel hybrid architecture based convolutional layer bottom part take advantage feature extraction capabilities CNNs, parameter reduction capabilities TT-DNNs. We refer architecture CV-TT-NN. All neural models investigated work meant tensor-to-vector regression, three architectures compared contrasted: CNNs, TT-DNNs, hybrid CV-TT-NNs. We first explain fundamental mechanisms tensor-to-vector regression reformulating convolutional layer multiplication two core tensors, allows us extend theorems proposed tensor-to-vector regression neural models. Next, move speech enhancement experiments. Our experimental results show single-channel speech enhancement Edinburgh noisy speech corpus , CNN outperforms best DNN small increment parameter sizes. Moreover, proposed CNN-TT slightly outperforms CNN 32\% CNN model size. A improvement attained size CNN-TT model increased 44\% CNN model size. Finally, experiments multi-channel speech enhancement task simulated noisy WSJ0 corpus show trend proposed hybrid CNN-TT architecture favorably compared DNN CNN models achieve better-enhanced speech qualities utilize much smaller model sizes. In paper, range neural architecture search techniques investigated automatically learn three hyper-parameters heavily affect performance model complexity state-of-the-art factored time delay neural network acoustic models: i) left right splicing context offsets; ii) dimensionality bottleneck linear projection. Experimental results obtained Switchboard UASPEECH suggest NAS techniques used automatic configuration DNN based speech recognition systems allow wider application different tasks."," This paper investigates different trade-offs between the number of model parameters and enhanced speech qualities by employing several deep tensor-to-vector regression models for speech enhancement. We find that a hybrid architecture, namely CNN-TT, is capable of maintaining a good quality performance with a reduced model parameter size. CNN-TT is composed of several convolutional layers at the bottom for feature extraction to improve speech quality and a tensor-train  output layer on the top to reduce model parameters. We first derive a new upper bound on the generalization power of the convolutional neural network  based vector-to-vector regression models. Then, we provide experimental evidence on the Edinburgh noisy speech corpus to demonstrate that, in single-channel speech enhancement, CNN outperforms DNN at the expense of a small increment of model sizes. Besides, CNN-TT slightly outperforms the CNN counterpart by utilizing only 32\% of the CNN model parameters. Besides, further performance improvement can be attained if the number of CNN-TT parameters is increased to 44\% of the CNN model size. Finally, our experiments of multi-channel speech enhancement on a simulated noisy WSJ0 corpus demonstrate that our proposed hybrid CNN-TT architecture achieves better results than both DNN and CNN models in terms of better-enhanced speech qualities and smaller parameter sizes. %Finally, experimental results on the multi-channel speech enhancement on the in-house noisy corrupted Wall Street Journal  corpus confirm our claims.  %This work applies various deep hybrid tensor-to-vector regression models for speech enhancement. In particular, we are mainly concerned with two goals:  Providing new insights into the tensor models based on approximation power of deep neural network  based tensor-to-vector functions from both theoretical and practical points of view, and  Finding different deep hybrid tensor architectures in terms of multiple metrics, i.e., the number of model parameters and speech enhancement performance. In particular, we focus on convolutional neural networks  and tensor-train neural networks  in dealing with tensorized speech data since the two architectures belong to the tensor-to-vector framework. Moreover, different hybrid architectures based on convolutional and tensor-train layers are evaluated. Our experiments of speech enhancement include both single-channel and multi-channel speech enhancement, and the empirical results suggest the following three findings:  CNNs attain better results than TT-NNs;  better speech enhancement results can be obtained by deploying neural models having convolutional and TT layers. % this work also investigates the use of Tucker decomposition to further reduce the number of CNN parameters, and strike a balance between speech enhancement performance and model complexity."
"Subjective listening studies reliable form speech quality assessment many applications, including speech enhancement audio source separation. Listeners often rate perceptual quality testing materials using categorical multi-stimuli rating protocols. The test materials often artificially created additively convolutionally mixing clean speech noise reverberation prescribed levels, simulate real environments. Unfortunately, simulated data capture intricate details real environments , clear assessments consistent assessment results real-world environments. Many investigations conclude realistic datasets scenarios needed improve real-world speech processing performance. However, cost time-consuming nature subjective studies also hinders progress. Computational objective measures enable low cost efficient speech quality assessment, many intrusive, non-intrusive, data-driven approaches developed. Intrusive measures, perceptual evaluation speech quality , signal-to-distortion ratio perceptual objective listening quality analysis , generate quality scores calculating dissimilarities clean reference speech signal degraded counterpart . These measures, however, always correlate well subjective quality results. Several non-intrusive objective quality measures developed, including ITU-T standard P.563, ANSI standard ANIQUE+, speech reverberation modulation energy ratio . These approaches use signal processing concepts generate quality-assessment scores. These approaches, however, rely signal properties assumptions always realized real-world environments, hence assessment scores always consistent human ratings. More recent work uses data-driven methods estimate speech quality. The authors combine hand-crafted feature extraction tree-based regression model predict objective PESQ scores. Quality-Net provides frame-level quality assessment predicting utterance-level PESQ scores copied per-frame labels using bidirectional long short-term memory network. Similarly, NISQA estimates per-frame POLQA scores using convolutional neural network . It subsequently uses BLSTM aggregate frame-level predictions utterance-level objective quality scores. These data-driven approaches perform well increase practicality real-world assessment. However, usage objective quality scores training targets major limitation, since objective measures approximate human perception. Alternatively, model developed predicts mean opinion score human ratings, ratings collected simulated speech data. This approach advances field, enough ensure good performance real environments. A complete approach needed predicts human quality ratings real recordings. In study, conduct large-scale listening test real-world data collect 180,000 subjective quality ratings Amazon's Mechanical Turk using two publically-available speech corpora. This platform provides diverse population participants significantly lower cost facilitate accurate rapid testing. These corpora wide range distortions occur everyday life, reflect varying levels noise reverberation. Our listening tests follow MUltiple Stimuli Hidden Reference Anchor protocol. To best knowledge, large publically-available dataset contains degraded speech human quality ratings currently exist. We additionally develop encoder-decoder model attention mechanism non-intrusively predict perceived speech quality real-world signals. The encoder consists stacked pyramid BLSTMs convert low-level speech spectra high-level features. This encoder-decoder architecture reduces sequential size latent representation provided attention model. The key difference proposed approach related approaches, approach predicts mean-opinion scores real-world signals using novel deep-learning framework. The following sections discuss details results approach. We compare several tensor-to-vector regression models speech enhancement. These models include CNN, DNN-TT, hybrid models composed convolutional TT layers, namely CNN-TT. We first discuss representation power linking tensor-to-vector regression earlier theories DNN based vector-to-vector regression. Next, evaluate models single-channel speech enhancement Edinburgh noisy speech database. Finally, conduct multi-channel speech enhancement synthesized WSJ noisy corpus. Our experimental results suggest CNN outperform DNN-TT DNN smaller regression errors higher PESQ scores. Moreover, fully-connected output layer CNN replaced TT layer generate hybrid regression network, achieve even better performances gradually increasing model size TT layer. In future work, investigate different tensor representations reduce parameters hidden convolutional layers. \clearpage"," The real-world capabilities of objective speech quality measures are limited since current measures  are developed from simulated data that does not adequately model real environments; or they  predict objective scores that are not always strongly correlated with subjective ratings. Additionally, a large dataset of real-world signals with listener quality ratings does not currently exist, which would help facilitate real-world assessment. In this paper, we collect and predict the perceptual quality of real-world speech signals that are evaluated by human listeners. We first collect a large quality rating dataset by conducting crowdsourced listening studies on two real-world corpora. We further develop a novel approach that predicts human quality ratings using a pyramid bidirectional long short term memory  network with an attention mechanism. The results show that the proposed model achieves statistically lower estimation errors than prior assessment approaches, where the predicted scores strongly correlate with human judgments."
"Decision making essential cognitive process human beings. Over time, different models emerged help us \nuevo{to} solve DM problems. In particular, \nuevo{multi-person} multi-criteria decision making \nuevo{} models \nuevo{consider} evaluations multiple experts solve decision situation analyzing possible solution alternatives \nuevo{according to} several criteria . Computational DM process, human DM one, requires useful, complete insightful information making adequate decision according input information. The input DM models \nuevo{is} usually set evaluations experts. % DM problem. \nuevo{They} wish express evaluations natural language, raw text directly processed DM models. Accordingly, several approaches followed asking elaborating computational representation evaluations% processed , namely: These approaches asking evaluations constrain evaluative expressiveness experts, adapt evaluation numerical linguistic evaluation \nuevo{alternatives}. We claim experts DM problem express evaluations natural language, DM model able process computationally represent them. \nuevo{Natural language processing artificial intelligence area combines linguistic computational language backgrounds understanding generating human language . NLP composed several tasks focused different aspects language order represent extract insightful knowledge it, namely: machine translation , argument mining , text summarization , information extraction , among others. The evaluations experts DM expression private states set target alternatives . The language used projecting private states subjective language . The NLP task concerned treatment opinions, sentiments subjectivity text sentiment analysis . SA methods infer opinion meaning fragment text, opinion meaning may expressed binary multi-level scale opinion intensity. Likewise, SA methods conducted different granularity levels, hence used document, sentence aspect level. SA aspect level known aspect-based sentiment analysis , calculates opinion meaning every entity aspects entities explicitly implicitly mentioned text. Hence, SA methods, specifically ABSA methods, may used overcome constraint processing evaluation experts natural language.} %it fine-grained, \nuevo{Since SA methods may process opinion experts, works followed lexicon-based SA approach infer position experts respect set target alternatives . However, lexicon-based SA methods measure opinion expert global evaluation level, able identify opinion experts several criteria DM problem, limited lexical coverage lexicon.} %Regarding constraint processing evaluation experts natural language, works tried use lexicon-based sentiment analysis methods inferring position experts . However, lexicon-based methods measure opinion expert global evaluation level, able identify opinion experts several criteria DM problem, limited lexical coverage lexicon. % multi-expert %and multi-criteria DM problems %Decision Making using Sentiment Analysis %We propose paper new methodology combines \nuevo{SA} DM methods processing evaluation experts natural language. We define methodology \nuevo{MpMcDM problems}, call \nuevo{Sentiment Analysis based Multi-person Multi-criteria Decision Making }. \nuevo{We propose paper new methodology MpMcDM problems combines SA DM methods processing evaluation experts natural language \segrev{with aim providing smarter decision aid.} We call Sentiment Analysis based Multi-person Multi-criteria Decision Making methodology.} \nuevo{It} allow\cz{s} \cz{deal} expert evaluations expressed \nuevo{in natural language even numerical values.} Accordingly, \nuevo{it} defines represent different kind evaluations expert, combine building input \nuevo{MpMcDM} model. Hence, \nuevo{SA-MpMcDM} methodology allows use whatever SA DM model order resolve \nuevo{MpMcDM} problem. There several experts evaluation criteria \nuevo{MpMcDM} problems, means one expert evaluate alternatives according different criteria. The criteria alternatives DM problem different aspects alternative susceptible assessed, roughly speaking, alternatives evaluated criterion level. \nuevo{As indicated above, SA ABSA methods infer position expert respect criterion. Accordingly, consider criteria alternative aspects entity, use ABSA methods processing expert evaluations different criteria.}%SA may infer opinion entity different granularity levels, fine-grained aspect based sentiment analysis , concerned computational treatment opinions aspect level. If consider criteria alternative aspects entity, use ABSA methods processing expert evaluations different criteria. \nuevo{We propose paper SA-MpMcDM methodology three-tier workflow, namely: Transforming opinion values numbers obtain matrix representation natural language evaluations. If available, expert evaluations numerical scale also considered. %into another matrix representation. %, allows use MpMcDM model. \item Alternative choice decisions: expert evaluations aggregated collective preference matrix, allows rank target alternatives. The aggregation conducted \new{the proposed procedure named }%a criteria weighting attention experts, i.e criteria receive opinions, highly attract attention experts, hence SA-MpMcDM methodology gives higher weight aggregation. \end{enumerate} } %We implement paper DMuSA methodology ABSA method, call resultant MEMCDM model DMuABSA model. The ABSA model, \cz{designed DMuABSA model}, built upon end-to-end multi-task deep learning model, capacity %We evaluate DMuABSA model social media MEMCDM problem. The problem consists %\todo[inline]{EMC: Aqu es donde meter lo del caso de estudio, porque esta es la parte de la evaluaci.} %\segrev{In particular, aim develop smarter decision aid focused predictive sentiment analysis based MpMcDM methodology using NLP analysis deep learning aspect detection. Thus, we} %\nuevo{the SA-MpMcDM methodology} social media \nuevo{MpMcDM problem, consists in} establishing ranking restaurants %from set restaurants e-commerce site according set opinions published e-commerce site. \segrev{We evaluate SA-MpMcDM methodology MpMcDM case study restaurant choice using restaurant reviews e-commerce site.} The restaurants services categories alternatives criteria \nuevo{MpMcDM} problem. We compiled annotated restaurant review dataset TripAdvisor evaluating \nuevo{the SA-MpMcDM methodology}, call TripR-2020 dataset. Each alternative restaurant evaluated experts TripR-2020, means used evaluation \nuevo{MpMcDM} models. \nuevo{The main feature SA-MpMcDM methodology enlarging capacity \new{an} MpMcDM model considering evaluative information, means leveraging natural language evaluations. Hence, provide different scenarios based methodology %analyse whether quality resultant preference vector using The results show combining natural language numerical evaluations enhances quality MpMcDM model.}%resultan preference vector.} %We compare performance DMuABSA model traditional MEMCDM model use natural language evaluations, simplification DMuABSA model use numerical evaluations. %similar implementations DMuSA methodology use natural language evaluations restaurants. %The results show considering natural language \cz{and numerical} evaluations enhance quality output MEMCDM model.%, allow us conclude claim holds. The main contributions paper are: % \cz{DMuSA methodology DMuABSA model. } This paper structured follows. Section presents bases DM, ABSA deep learning understand SA-MpMcDM methodology. Section explains \nuevo{SA-MpMcDM methodology workflow architecture. Section describes proposed methodology used \new{an} MpMcDM problem \segrev{dealing TripAdvisor restaurant reviews.}} \nuevo{An analysis behaviour methodology different scenarios} shown Section . Finally, conclusions future work \nuevo{pointed out} Section . \new{The summarizes abbreviations notations used paper.} %proposed DM model based SA. This model solves real MEMCDM problem Section %\prev{ %The important aspect DM models obtain quality solutions quality input data. Therefore, imperative experts provide quality assessments. This achieved experts freely express opinions, without imposing restrictions. However, actual MEMCDM models meet requirement set limitations. Current models establish two possible options, combined, experts evaluate: In case, models place restrictions %experts getting low quality results. %This paper proposes MEMCDM model allows experts express evaluations without restrictions. By means model, experts evaluate alternatives based term related criterion natural language. Unlike DM models based counting keywords , proposal conduct semantic understanding process. Given expert's evaluations, proposed model carries sentiment analysis phase extract expert knowledge. Therefore, proposed model merges two major areas current research decision making sentiment analysis. %Thanks model, experts provide evaluations written texts natural language evaluating criteria. Furthermore, experts provide numerical ratings evaluating defined criteria. The SA system extract opinions written texts getting expert knowledge. The opinions made aspects associated criteria, criteria, polarity associated pair aspect-criteria. To achieve it, deep learning techniques applied neural networks. Finally, particular evaluations aggregated collective evaluation considering weighted criteria. %To validate proposed model, real MEMCDM problem solved. Selected restaurants ranked analyzing evaluations provided experts real users form e-commerce site TripAdvisor. These evaluations written texts natural language combined numerical evaluations. We compare proposal traditional MEMCDM model considers numerical evaluations. Furthermore, case study solved means simplification proposed model analyzing written texts. %The main contributions paper summarized follows: % %This paper structured follows. Section presents bases decision-making, sentiment analysis neural networks understand proposed model. Section explains proposed DM model based SA. This model solves real MEMCDM problem Section . A comparative analysis discussion shown Section . Finally, conclusions future work collected Section . \section{Future Work Extensions} We plan continue work using larger dataset 96,000,000 molecules gathered PubMed dataset\footnote{ftp://ftp.ncbi.nlm.nih.gov/pubchem/Compound/Extras/}, we've observed language models like BERT seem thrive settings abundance data. Two challenges sheer computational resources required wield train massive dataset, determination useful molecules add noise signal. Additionally, would like design loss function BERT penalizes distances embeddings SMILES strings represent molecule yet unique. We hope force BERT learn inherent graphical meaning SMILES strings hence start pick chemical inference gathered based connectivity. For this, need augment smaller dataset, generating multiple unique SMILES strings molecule molecules dataset. At moment, training BERT dataset future investigation. Finally, would like identify potential flaws approach analyzing chemical property-bearing molecular substructures using fine-tuning. We feel implemented correctly, could useful tool chemists could greatly assist drug discovery, reaction prediction, tasks. \section{Discussion Conclusion} In work, proposed analysis BERT attention study functional groups property-impacting molecular substructures data-driven perspective datasets SMILES representations molecules. We applied representations functional groups atoms learned model tackle problems toxicity, solubility, drug-likeness, synthesis accessibility smaller datasets using learned representations features graph convolution attention models graph structure molecules, well fine-tuning BERT. Finally, proposed use attention visualization helpful tool chemistry practitioners students quickly identify important substructures various chemical properties. While defined state art results designed breakthrough tools, we've discovered several interesting properties SMILES strings molecules attention model-based standpoint, approaches results may still useful others investigating computational chemistry. We would like thank Vineet Kosaraju, Michele Catasta, Keiran Thompson, Lillian Zhu, Gaby Li, Amelia Woodward deeply helpful consultation project. {\small }"," Decision making models are constrained by taking the expert evaluations with pre-defined numerical or linguistic terms. We claim that the use of sentiment analysis will allow decision making models to consider expert evaluations in natural language. Accordingly, we propose the \nuevo{Sentiment Analysis based Multi-person Multi-criteria Decision Making  methodology \segrev{for smarter decision aid}}, which builds the expert evaluations from their natural language reviews\nuevo{, and even from their numerical ratings if they are available}. \nuevo{The SA-MpMcDM methodology incorporates an end-to-end multi-task deep learning model for aspect based sentiment analysis, named DOC-ABSADeepL model, able to identify the aspect categories mentioned in an expert review, and to distill their opinions and criteria. The individual evaluations are aggregated via \new{the procedure \segrev{named }}%a criteria weighting through the attention of the experts.} %We evaluate the methodology in a restaurant decision problem, hence we build the TripR-2020 dataset of restaurant reviews, which we manually annotate and release. \segrev{We evaluate the methodology in a case study of restaurant choice using TripAdvisor reviews, hence we build, manually annotate, and release the TripR-2020 dataset of restaurant reviews.} We \nuevo{analyze the SA-MpMcDM methodology in different scenarios using and not using natural language and numerical evaluations. The analysis shows that the combination of both sources of information results in a higher quality preference vector.}   % about the different alternatives of the problem  %We implement the DMuSA methodology with an end-to-end multi-task deep learning model for aspect based sentiment analysis , which results in the DMuABSA model. DMuABSA is able to identify the aspect categories mentioned in an expert review, and to infer their opinion values, which means that can be applied in a multi-expert multi-criteria decision making problem.   %\nuevo{SA-MpMcDM} with other DM models that do not use the natural language evaluations, and \nuevo{SA-MpMcDM} reaches higher quality decisions.  %\nuevo{, which is available as public dataset.} %, which means that our claim holds.} %\prev{Multi-Expert Multi-Criteria Decision Making  models rank a set of alternatives through evaluations provided by a set of experts based on a set of criteria. Traditionally, experts are allowed to provide numerical ratings or pre-defined linguistic terms to assess the alternatives. Unfortunately, these constraints of current MEMCDM models cause the solutions obtained to be biased, not correct, or even do not correspond to what the experts themselves would expect. Hence, we propose a MEMCDM model that allow experts to express their evaluations without restrictions, i.e., providing evaluations in natural language evaluating undefined criteria and even providing numerical ratings evaluating predefined criteria. This model extracts the expert knowledge from the natural language reviews by means of a sentiment analysis step carrying out an aspect-based sentiment analysis task by means of a multi-task neural network. The proposed model solves a real case of study for evaluating restaurants reviewed in the e-commerce site TripAdvisor.   }"
"% The first letter 2 line initial drop letter followed % rest first word caps. % % form use first word consists single letter: % {A}{demo} file .... % % form use need single drop letter followed % normal text : % {A}{}demo file .... % % Some journals put first two words caps: % {T}{his demo} file .... % % Here typical use ""T"" initial drop letter % ""HIS"" caps complete first word. {T}{ext} classification -- procedure designating pre-defined labels text -- essential significant task many Natural Language Processing applications, sentiment analysis , topic labeling , question answering dialog act classification . In era information explosion, time-consuming challenging process classify large amounts text data manually. Besides, accuracy manual text classification easily influenced human factors, fatigue expertise. It desirable use machine learning methods automate text classification procedure yield reliable less subjective results. Moreover, also help enhance information retrieval efficiency alleviate problem information overload locating required information. Fig. illustrates flowchart procedures involved text classification, light shallow deep analysis. Text data different numerical, image, signal data. It requires NLP techniques processed carefully. The first important step preprocess text data model. Shallow learning models usually need obtain good sample features artificial methods classify classic machine learning algorithms. Therefore, effectiveness method largely restricted feature extraction. However, different shallow models, deep learning integrates feature engineering model fitting process learning set nonlinear transformations serve map features directly outputs. % Generally, text classification approaches principally separated two branches: shallow learning deep learning techniques, shown Fig.. % The task summarized follows: % original input text preprocessed obtain vector representation shallow learning deep learning models . % For shallow learning models, preprocessed data extract features, representing vector representation learning methods. % Then, classifier learns classification features. % The performance classification model improved iterative training training sets evaluation validation sets. % When pre-defined termination conditions reached, time predict final label. % However, different shallow models, deep learning integrates feature engineering model fitting process learning set nonlinear transformations serve map features directly outputs. \subsection{Major Differences Contributions} There several works reviewing text classification subproblems recently. Two reviews text classification. Kowsari et al. surveyed different text feature extraction, dimensionality reduction methods, basic model structure text classification, evaluation methods. Minaee et al. reviewed recent deep learning based text classification methods, benchmark datasets, evaluation metrics. Unlike existing text classification reviews, conclude existing models shallow deep learning works recent years. % compare among deep learning models. Shallow learning models emphasize feature extraction classifier design. Once text well-designed characteristics, quickly converged training classifier. DNNs perform feature extraction automatically learn well without domain knowledge. We give datasets evaluation metrics single-label multi-label tasks summarize future research challenges data, models, performance perspective. Moreover, summarize various information three tables, including necessary information classic deep learning models, primary information main datasets, general benchmark state-of-the-art methods different applications. In summary, study's main contributions follows: \subsection{Organization Survey} The rest survey organized follows. Section summarizes existing models related text classification, including shallow learning deep learning models, including summary table. Section introduces primary datasets summary table evaluation metrics single-label multi-label tasks. We give quantitative results leading models classic text classification datasets Section. Finally, summarize main challenges deep learning text classification Section concluding article Section. In work, presented real deepfake tweets dataset experimental results various detection techniques. The dataset composed 25,836 tweets, human half bots generated, posted Twitter last months. We collected 23 bots 17 human accounts ware imitating. The bots based various generating techniques . Overall detection results reported baseline using 13 detection methods, confirm newest sophisticated generative methods based transformer architecture produce high-quality short texts, difficult detect. The proposed real deepfake tweets publicly available well-known Kaggle platform. We hope dataset, together reported results, help research community develop techniques order detect combat deepfake threat."," Text classification is the most fundamental and essential task in natural language processing.  The last decade has seen a surge of research in this area due to the unprecedented success of deep learning.  Numerous methods, datasets, and evaluation metrics have been proposed in the literature, raising the need for a comprehensive and updated survey.  This paper fills the gap by reviewing the state of the art approaches from 1961 to 2020, focusing on models from shallow to deep learning.  We create a taxonomy for text classification according to the text involved and the models used for feature extraction and classification. We then discuss each of these categories in detail, dealing with both the technical developments and benchmark datasets that support tests of predictions.  A comprehensive comparison between different techniques, as well as identifying the pros and cons of various evaluation metrics are also provided in this survey.  Finally, we conclude by summarizing key implications, future research directions, and the challenges facing the research area."
"Relation extraction aims obtaining semantic relationship entities using text source knowledge. For instance, text snippet, Steve Jobs Wozniak co-founded Apple 1976., infer Steve Jobs Wozniak org:founded\_by relation Apple. RE important subtask information extraction significant applications various higher-order NLP/IR tasks, question answering, knowledge graph completion semantic search . Earlier studies RE based feature engineering. Such methods rely linguistic lexical tools obtain information required feature engineering . Additionally, performance methods hindered sparse feature representation used models. With surge neural networks, deep learning-based models become prevalent. In models, pre-trained word embeddings employed solve feature sparsity problems. Deep learning based RE models categorized along two lines: sequence-based graph-based models. In sequence-based models, word sequence used embed text using convolution recurrent neural networks . In graph-based models, text first converted graph using dependency parser linguistic tools processed graph neural network encodes neighborhood feature information. Finally, encoded graph features used RE. Along line, \citet{liu2015dependency} \citet{miwa-bansal2016} employed bidirectional long short-term memory network \citet{zhang2018graph} \citet{wu-2019} employed graph convolutional network encode textual graph used work. Compared sequence-based models, graph-based models shown effective learning long-distance dependencies present text . Although state-of-the-art results obtained using graph-based models, require external tools build graph text. Therefore, computationally expensive fully end-to-end trainable. While sequence-based models depend external linguistic tools, shown less effective long text, especially long-distance dependencies required . To bridge gap, propose Self-determined GCN infers graph text using self-attention mechanism , rather using external linguistic tool. Then self-determined graph encoded using GCN model. We evaluate effectiveness SGCN RE task several competitive baselines. In summary, contributions following: This paper principally introduces existing models text classification tasks shallow learning deep learning. Firstly, introduce primary shallow learning models deep learning models summary table. The shallow model improves text classification performance mainly improving feature extraction scheme classifier design. In contrast, deep learning model enhances performance improving presentation learning method, model structure, additional data knowledge. Then, introduce datasets summary table evaluation metrics single-label multi-label tasks. Furthermore, give quantitative results leading models summary table different applications classic text classification datasets. Finally, summarize possible future research challenges text classification. single appendix: [Proof Zonklar Equations] appendix heading use"," Relation Extraction is a way of obtaining the semantic relationship between entities in text. The state-of-the-art methods use linguistic tools to build a graph for the text in which the entities appear and then a Graph Convolutional Network  is employed to encode the pre-built graphs. Although their performance is promising, the reliance on linguistic tools results in a non end-to-end process. In this work, we propose a novel model, the Self-determined Graph Convolutional Network , which determines a weighted graph using a self-attention mechanism, rather using any linguistic tool. Then, the self-determined graph is encoded using a GCN. We test our model on the TACRED dataset and achieve the state-of-the-art result. Our experiments show that SGCN outperforms the traditional GCN, which uses dependency parsing tools to build the graph."
"It common tendency among multilingual people non-native English speakers code-mix speech using English-based phonetic typing. This linguistic phenomenon, particularly social media like Twitter\footnote{https://twitter.com/}, poses great challenge conventional Natural Language Processing study area. Within context Sentiment Analysis, study phenomenon code-mixed language important research community behavior common today. The interest area grown due volume data social networks generate, also value information understand people opinions expressed written texts. In paper, explain methodology predict sentiment tweets, describing method based combination latest language models, also models contributed great advance task. This configuration employed evaluated SemEval 2020 challenge , goal predict sentiment code-mixed texts written English Hindi languages tweet . The models used combination are: MultiFiT evolution ULMFiT , BERT , ALBERT XLNet . This work organized follows: Section explains related works, Section describes dataset used, Section addresses methodology applied task, Section presents results, finally Section expose final considerations well possible future works. This paper presented OFAI--UKP system predicting binary graded humorousness. It employs Gaussian process preference learning, Bayesian system learns rank rate instances exploiting pairwise preference judgments. By providing additional feature data , method learn predict scores previously unseen items. Though system previously achieved good results rudimentary, task-agnostic linguistic features two English-language tasks , performance Spanish-language Twitter data \HAHA less impressive. We tentatively attribute information loss involved conversion numeric annotations used task preference judgments required input method, fact normalize Twitter data match linguistic resources. Possible future work would include mitigating two problems , using additional, humour-specific features, including used past work well inspired prevailing linguistic theories humour. The benefits including word frequency also point possible improvements using -grams, tf--idf, task-agnostic linguistic features. \subsection*{Acknowledgments} This work supported German Federal Ministry Education Research promotional reference 01UG1816B , German Research Foundation part QA-EduInf project , DFG-funded research training group ``Adaptive Preparation Information Heterogeneous Sources'' , Austrian Science Fund project M\,2625-N31. The Austrian Research Institute Artificial Intelligence supported Austrian Federal Ministry Science, Research Economy.","     In this paper, we describe a methodology to predict sentiment in code-mixed tweets . Our team called verissimo.manoel in CodaLab\footnote{https://competitions.codalab.org/competitions/20654} developed an approach based on an ensemble of four models . The final classification algorithm was an ensemble of some predictions of all softmax values from these four models. This architecture was used and evaluated in the context of the SemEval 2020 challenge , and our system got 72.7\% on the F1 score."
"Keyphrases short pieces text summarize key points discussed document. They useful many natural language processing information retrieval tasks, as, text summarization , question answering , % information extraction , sentiment analysis , document retrieval , document categorization clustering , contextual advertisement , more. In automatic keyphrase generation task, input document, output set keyphrases categorized present absent keyphrases. Present keyphrases appear exactly target document, absent keyphrases semantically related partial overlap target document. We provide example target document keyphrases Figure . % document categorization , contextual advertisement Automatic keyphrase generation methods literature broadly divided extraction generation methods. A large pool prior works devoted extracting keyphrases selecting text spans phrases directly target document % ranking based importance . % mihalcea2004textrank However, due design principle, approaches cannot predict absent keyphrases. In recent years, neural sequence-to-sequence framework become fundamental building block neural keyphrase generation models widespread usage natural language generation tasks. The first deep neural keyphrase generation model, CopyRNN adopts Seq2Seq framework copy mechanism . % The copy attention mechanism enables decoder select words either according language model predefined vocabulary according probability distribution computed input text sequence. % Thus, copy enabled Seq2Seq methods capable generating present absent keyphrases. With copy attention mechanism, Seq2Seq models capable generating present absent keyphrases. A subsequent works extended CopyRNN enhance keyphrase generation. Although generative approaches capable generating present absent phrases, ignore advantages extractive solutions, e.g., extracted keyphrases indicate essential segments target document. % \citet{chen2019integrated} recently proposed combine extractor selects text spans present keyphrases generator generates absent keyphrases word word. % One advantage combined approach extractor help generator identify important segments target document. To generate comprehensive set keyphrases summarizes key points conveyed target document, reading full document content necessary. However, best knowledge, none previous neural methods provisioned read full content document thousands words long . Processing long documents deep neural networks requires high computational resources. Hence, existing neural methods truncate target document; take first hundred words input ignore rest document may contain salient information. To address aforementioned challenges, paper, propose SEG-Net two major components, sentence-selector selects salient sentences document extractor-generator predicts present keyphrases generates absent keyphrases jointly. The primary motivation design sentence-selector decompose long target document small segments, e.g., sentences, paragraphs, identify salient ones keyphrase generation. % One potential solution decompose target document small segments, e.g., sentences, paragraphs, identify salient ones keyphrase generation. For example, shown Figure , split document list sentences classify salient non-salient labels. In context, consider sentence salient contains present keyphrases overlaps absent keyphrases. In Figure , sample document consists six salient five non-salient sentences . A similar notion adopted prior works text summarization question answering . We employ Transformer backbone extractor-generator SEG-Net. We chose Transformer completely relies self-attention mechanism capable capturing longer range dependencies. We equip extractor-generator novel layer-wise coverage attention informed copy attention generated keyphrases summarize entire target document. The layer-wise coverage attention keeps track target document segments covered previously generated phrases guide self-attention mechanism Transformer attending encoded target document future generation steps. We revise standard copy mechanism propose ``informed'' copy attention keyphrase generation. Our revision based observation word different meaning context two different keyphrases. For example, Figure , word ``learning'' different meaning used ``computer assisted language learning'', ``integrated e learning'', ``learning foreign languages''. Hence, revise copy mechanism SEG-Net copy word present keyphrase generating absent keyphrase. Another motivation behind copy mechanism encourage model generate absent keyphrases summarize segments target document covered present keyphrases. % add 1 line We train SEG-Net via multi-task learning predict keyphrases well part-of-speech tags. We exploit multi-layer structure Transformer perform POS tagging keyphrase generation. We evaluate SEG-Net five benchmarks scientific articles two benchmarks web documents demonstrate effectiveness state-of-the-art neural generative methods domains. We perform thorough ablation analysis present noteworthy findings selecting salient sentences significantly improve present keyphrase extraction, layer-wise coverage attention informed copy mechanism facilitates absent keyphrase generation, jointly learning POS tagging phrase prediction reduces duplicate overlapping keyphrase generation. % \newcommand{\notexb}[1]{\Note{Purple}{\bf[#1 --xiao]}} \newcommand{\notesl}[1]{\Note{Orange}{\bf[#1 --soomin]}} \newcommand{\notekw}[1]{\Note{Red}{\bf[#1 --kai-wei]}} \newcommand{\notewa}[1]{\Note{Blue}{\bf[#1 --wasi]}} \usepackage{amsthm} % \theoremstyle{plain} \newtheorem{definition}{Task} \setcopyright{acmcopyright} \copyrightyear{20XX} \acmYear{20XX} \acmDOI{10.1145/1122445.1122456} %% These commands PROCEEDINGS abstract paper. \acmConference[Woodstock '18]{Woodstock '18: ACM Symposium Neural Gaze Detection}{June 03--05, 2018}{Woodstock, NY} \acmBooktitle{Woodstock '18: ACM Symposium Neural Gaze Detection, June 03--05, 2018, Woodstock, NY} \acmPrice{15.00} \acmISBN{xxx-x-xxxx-XXXX-X/X/X} %% %% end preamble, start body document source. %% %% The code generated tool http://dl.acm.org/ccs.cfm. %% Please copy paste code instead example below. %% % % \ccsdesc[500]{Computer systems organization~Embedded systems} % \ccsdesc[300]{Computer systems organization~Redundancy} % \ccsdesc{Computer systems organization~Robotics} % \ccsdesc[100]{Networks~Network reliability} %% %% Keywords. The author pick words accurately describe %% work presented. Separate keywords commas. \keywords{Neural keyphrase generation, sentence classification, multi-task learning, part-of-speech tagging, Transformer} %% %% The acknowledgments section defined using ""acks"" environment %% . This ensures proper %% identification section article metadata, %% consistent spelling heading. % %% %% The next two lines define bibliography style used, %% bibliography file. % \newpage % \bibliographystyle{acm-reference-format} \bibliography{acmart} %% %% If work appendix, place put it. % \appendix \end{document} \endinput %% %% End file `sample-sigconf.tex'. In paper, propose combination four models Semeval 2020 , team got 72.7\ F1 score competition. All models based using language models transfer learning. They alone performed well, together ensemble combination, performed even better. In applications, difficult use ensemble consisted four models, especially overhead coming time spent inference, culminating approach sometimes perform well. On hand, individual results four models close, meaning task, model used. It important note MultFit worst result, difference small, specific model takes lot less time train, lightest model ensemble. As future works, intend explore models Sentiment Analysis multilingual monolingual scenarios. include bib file like this:"," % \notexb{Xiao's note.} % \notesl{Soomin's note.} % \notewa{Wasi's note.} % \notekw{Kai-Wei's note.} \\  Generating a set of keyphrases that summarizes the core ideas discussed in a document has a significant impact on many applications, including document understanding, retrieval, advertising, and more. In recent years, deep neural sequence-to-sequence framework has demonstrated promising results in keyphrase generation. However, processing long documents using such deep neural networks requires high computational resources. To reduce the computational cost, the documents are typically truncated before given as inputs. As a result, the models may miss essential points conveyed in a document. Moreover, most of the existing methods are either extractive  or generative , and hence they do not benefit from the advantages of both modeling techniques. To address these challenges, we propose SEG-Net, a neural keyphrase generation model that is composed of two major components,  a selector that selects the salient sentences in a document, and  an extractor-generator that jointly extracts and generates keyphrases from the selected sentences. SEG-Net uses a self-attentive architecture, known as, Transformer as the building block with a couple of uniqueness. First, SEG-Net incorporates a novel layer-wise coverage attention to summarize most of the points discussed in the target document. Second, it uses an informed copy attention mechanism to encourage focusing on different segments of the document during keyphrase extraction and generation. Besides, SEG-Net jointly learns keyphrase generation and their part-of-speech tag prediction, where the later provides syntactic supervision to the former. The experimental results on seven keyphrase generation benchmarks from scientific and web documents demonstrate that SEG-Net outperforms the state-of-the-art neural generative methods by a large margin in both domains.  %Combining these two prevailing approaches in jointly learning to extract and generate can enable us to predict a comprehensive set of keyphrases.  % Relying on only one type of these prevailing approaches risks to miss important keyphrases.  % SEG-Net uses a self-attentive architecture, known as, Transformer to learn document representations for its ability to capture long-range dependencies.  % SEG-Net has a couple of uniqueness compared to the vanilla Transformer.  % during keyphrase generation to emphasize less on the document segments, from where the keyphrases are extracted. % such that the extracted and generated keyphrases remain mutually exclusive."
"%\url{}, accessed \printdate{} Reading increasingly carried means online multiple texts, simultaneously consist texts diverse genres, registers, authorships, credibilities etc.\ . % That is, learning takes place, speak, basis \enquote{document collages} whose components gathered constantly growing, nowadays mostly web-based information landscape space . % \textcolor{black}{Consequently, \citet[p.357]{Goldman:Braasch:Wiley:Graesser:Brodowinska:2012} speak reading intertextual process.} \citet{Britt:Rouet:Braasch:2012} assume intertext models represent selected constituents multiple texts \enquote{document entities} together entity-related information . % This supplemented three types links: % IM-related source-to-source , MM-related content-to-content source-to-content links . % A prediction DM, crucial work, probability generating intertext model result reading multiple text function number texts involved, authors, perspectives provide described situation \citep[p.\,171]{Britt:Rouet:Braasch:2012}, tasks accomplished contextual factors . % This suggests speak intertext model kind cognitive map underlying multiple text, MM abstracts textbase : % is, readers produce intertext models cognitive maps multiple texts parts underlying IL, groups communities readers produce distributed cognitive maps larger sections IL IL whole. % This duality small- large-scale reading processes leads object article. % That is, ask IL looks like perspective distributed cognitive maps vice versa, presents different reader communities. % The latter question focus article. Although DM takes necessary step generalizing CIM towards modeling multiple texts, largely single reader-oriented. % To broaden focus, generalize DM conceptually two steps: Step 2 concerns precisely viewpoint article. % That is, concerned central prerequisite alignable intertext models among readers members large communities. % This refers intertextual shape IL perspective different communities may different accesses \enquote{see} different landscapes, even situations opposite would assumed. % The DM related approaches model multiple texts extracted countless intertext models distributed totality ultimately represent, is, underlying multifaceted, highly dynamic IL, numerous document nodes relational, intertextual embeddings. According \citet [p.\,56]{Hartman:Hagerman:Leu:2018}, reading research mostly considers small amounts offline texts pre-selected experimenter rather open ILs users decide read. % But reading kind problem solving involves multiple search decision processes \citep[p.\,43]{Britt:Rouet:Durik:2018} , question arises limits processes imposed IL differ reader communities. % Apparently, approaches multiple texts focus micro-models leave corresponding macro-models, inform shape IL organizational laws, under-specified. % The present paper takes step direction filling gap: % develops macroscopic model IL examines shape appears perspective certain large-scale reader communities. % Our aim is, speak, impart knowledge \enquote{wild} sort reading takes place according \citet[p.\,535]{Braasch:McCrudden:Braten:2018} become subject reading research. % Thus, approach complementary current research intertext model: % study IL underlying construction intertext models macroscopic perspective, contrast reading research, starts microscopic perspective small groups individual readers . % \textcolor{black}{In terms integrated framework multiple texts concerned intertextuality information units cognitive strategies behavioral skills readers related.} % That is, modification fourth goal future research use multiple sources according \citet{Braasch:McCrudden:Braten:2018}, deal phenomenon different communities offered different information, especially context topic. % The extent phenomenon applies different language communities examined using example frequently used knowledge resource Web, Wikipedia \rrid{RRID:SCR\_004897}. %\textcolor{red}{Our contributions are:} The article organized follows: % Section explains relevance Wikipedia educational science gives overview related research. % Section explains research questions describes detail methods developed answer them. % In Section , describe experiments discuss results. % Finally, Section give conclusion outlook future work. In paper, present SEG-Net, keyphrase generation model identifies salient sentences target document utilize maximal information keyphrase prediction. SEG-Net jointly learns predict present absent keyphrases target document. In SEG-Net, incorporate novel layer-wise coverage informed copy attention cover critical points document diversify present absent keyphrases. We jointly train SEG-Net keyphrase generation part-of-speech tags prediction. We evaluate SEG-Net five benchmarks scientific articles two benchmarks web documents. The experiment results demonstrate effective state-of-the-art neural generative methods scientific web domains.","  We test the hypothesis that the extent to which one obtains information on a given topic through Wikipedia depends on the language in which it is consulted.  % Controlling the size factor, we investigate this hypothesis for a number of 25 subject areas.  % %That is, we test whether even in cases where two Wikipedias cover the same topic with approximately the same number of articles, they inform about this topic rather differently. % Since Wikipedia is a central part of the web-based information landscape, this indicates a language-related,  linguistic bias.  % The article therefore deals with the question of whether Wikipedia exhibits this kind of linguistic relativity or not. % From the perspective of educational science, the article develops a computational model of the information landscape from which multiple texts are drawn as typical input of web-based reading. % For this purpose, it develops a hybrid model of intra- and intertextual similarity of different parts of the information landscape and tests this model on the example of 35 languages and corresponding Wikipedias. % In this way the article builds a bridge between reading research, educational science, Wikipedia research and computational linguistics.  %%% Leave the Abstract empty if your article does not require one, please see the Summary Table for full details. %\section{} %\color{red} %Deadline: 15.\,05.~2020\par %For full guidelines regarding your manuscript please refer to \href{http://www.frontiersin.org/about/AuthorGuidelines}{Author Guidelines}.  %As a primary goal, the abstract should render the general significance and conceptual advance of the work clearly accessible to a broad readership. References should not be cited in the abstract. Leave the Abstract empty if your article does not require one, please see \href{http://www.frontiersin.org/about/AuthorGuidelines#SummaryTable}{Summary Table} for details according to article type.    \tiny  \keyFont{ \section{Keywords:} Multiple Texts, Information Landscape, Knowledge Graphs, Intratextual Similarity, Intertextual Similarity, Three-level Topic Model, Network Similarity Measurement, Linguistic Relativity}   %All article types: you may provide up to 8 keywords; at least 5 are mandatory."
"Context Scope: The US healthcare system complex setup governed managed state federal agencies. Managed Care health delivery system utilised Medicaid manage cost, utilization quality healthcare. The Managed Care system uses contract agreements Medicaid agencies Managed Care Organisations providing services. Some states even utilize system beyond traditional managed care initiatives care improvement chronic \& complex conditions, payment initiatives, etc. Contracts run gamut computer support janitorial services direct client services. HHS posts notifications new Request Proposal /solicitation releases, Requests Application Open Enrolments. RFPs bid requests consisting functional non-functional requirements different services. These also outline model contracts expected format proposals. The requirements mentioned form different questions/queries answered proposal/response RFPs. The procurement contracts entirely depends upon scores obtained response based predefined evaluation criteria. A contract generally awarded best scoring respondent. A typical RFP bid consists RFP advertisement, RFP itself, model contract, proposals/responses bidding entities scoring sheets submissions. RFPs supporting documents publicly available information. MCOs typically utilise historical submissions understand requirements respond better improve chances winning bid. Every RFP response typically runs several hundred pages spread across different websites data stores. Manual exploration historical bids time consuming iterative process. Given changing healthcare landscape, limited time-frame resources draft new responses, current process comprehensive enough extract insights derive competitive advantage. Challenges: Apart industry specific problem statement, work also poses unique challenge scoring entire documents. Most relevant efforts towards automatic scoring dealt short answers essays . Our work deals much larger sequence lengths, larger feature space capture. Another difference relevant literature RFPs written experts multiple iterations, opposed students writing essays evaluation. As such, removes need check superficial grammatical errors. Instead, need identify aspects text enhance scores diminish . Our Solution: In paper, propose automated framework using interpretable natural language processing techniques analyse RFP responses. The framework comprises two components: Text Processing Module Interpretable Scoring Model. RFP responses usually follow standard template/formatting available Portable Document Format PDF short. Moreover, understand content extract insights, text needs extracted granular level . These issues complicate text extraction process thus need develop Text Processing Module. We developed generic Text processing module would extract text different formats response. The extracted text analysed using Interpretable Scoring Model. The scoring model enables us identify terms/phrases auxiliary features impact section/question score positively negatively. We term positively impacting features enablers negatively impacting ones disablers. The framework also provides insights auxiliary features latently impact overall scoring. The framework also provides single portal/platform access historical bid responses similar details across bidders states. % Major Contribution work follows, have: We developed tested computational linguistic model assessment different resources manifesting formal informal learning media {as part learning opportunities Utilization-of-learning-opportunities models } means threshold concepts. This included distributive profiles 63 threshold concepts business education textbooks, newspapers, Wikipedia. We looked threshold concepts' frequency distributions, compound distributions, network structures within three kind resources. {Some effort taken order motivate introduce decidedly linguistic contribution kind educational research. {Since troublesome properties, especially troublesome language often emphasized, hardly systematically researched far, since research efforts generally limited learning processes associated threshold concepts \citep[e.g.][]{Brueckner:Zlatkin-Troitschanskaia:2018,Hoadley:Tickle:Wood:Kyng:2015,Sender:2017,Shanahan:Foster:Meyer:2006}, computational linguistic approach chosen present study. The computational linguistic perspective adopted present contribution pursues orientation which, terms educational research threshold concepts, two special features. On one hand, complements content analyses, classically used analyze textbooks, protocols textually graphically represented materials order work education-related meanings materials \citep[e.g.][]{Krippendorff:2013}. Here, codes set human coder automatic coding frequency relations used analyses. The often tedious lengthy manual evaluation limited number documents corresponding susceptibility errors matter fact limited small amount data. Computational linguistic analyses, contrary, process huge corpora. included codes set automatically using program code compared quickly. Secondly, so-called utilization-of-learning-opportunities models used model mechanisms action teaching-learning arrangements educational research \citep[e.g][]{Braun:Weiss:Seidel:2014}. These models show interactions learning-relevant aspects terms input-process-output paths. Very often learning outcomes analyzed connection different input factors . Significantly less frequently, however, learning potentials respective learning environments learning materials considered independently learner's assessment. With computational linguistic approach presented here, especially learning media used input learning processes processed large scale thus description learning environment presented considered informal well formal learning processes. Ultimately learning, meaning threshold concept expressions use text resources embraced within contour emerging research program -- encompassing specialized vocabularies, learning education, computational linguistics -- terms mental, referential differential meanings. The latter two used order derive hypotheses concerning formal informal learning contexts respect special class expressions, viz. threshold concepts. A second focus development computational linguistic model operationalizing threshold concepts analysis learning resources. In context, developed notion Threshold Concept Network quantified means alpha-cuts, taking account \enquote{web threshold concepts} . In way, able prove exceptional status threshold concepts textbooks, least node level. The main result formal informal resources indeed distinguished terms threshold concepts' profiles. Furthermore, Wikipedia turns first class formal learning resource. Continuing line research include least following steps: methodological considerations discussed Subsec. addressed. A lexical semantic analysis threshold concepts due. And, importantly, findings tied back education assessments learners. {For development tasks, already become clear studies , parts newspaper corpora used formulate tasks. If tasks formulated represent interconnectedness threshold concepts, present study help select suitable copora based density network structures. Thus, existing computational linguistic analyses also incorporated test development.At time, question selection corpora also arises context, expanded future studies. In present study, addition textbooks, SZ, Frankfurter Rundschau Wikipedia used hybrid corpora identify formal informal learning spaces and, course, represent part potential learning media.} Furthermore, experimental studies designed investigate systematically impact different resources learning. { Very often experimental studies developed assumptions tested themselves. On basis computational linguistic assessment, however, possible develop specific questions. Most notably, threshold concept acquisition learners compared depending media learn -- whereby, course, corresponding media competencies information literacy characteristics must also controlled . Furthermore, variety meanings threshold concepts examined follow-up studies. On learner side, investigation variation individual understanding already carried concept cost . However, studies yet available threshold concepts. The assessments study presented provide starting point experiments turn would round emerging research program sketched. Nevertheless, noted learning opportunities side analysed following, quality definitions semantic analyses disregarded. This important order avoid misinterpretations findings. For instance, although found linguistic characteristics concepts media used learning important design learning opportunities increasing, supplementary digitisation learning opportunities desirable, yet allow statements made underlying usage structures. It therefore likely learning media also used complementary way Wikipedia also used exam preparation addition text books.However, always clear trustworthy information detail, especially case collaboratively created wikis, whether students able uncover contradictions deal them. In case, future studies combine reading text comprehension research computational linguistic analyses order able compare objective quality threshold concept information individual understanding learner.}"," The Managed Care system within Medicaid  uses Request For Proposals  to award contracts for various healthcare and related services. RFP responses are very detailed documents  submitted by competing organisations to win contracts. Subject matter expertise and domain knowledge play an important role in preparing RFP responses along with analysis of historical submissions. Automated analysis of these responses through Natural Language Processing   systems can reduce time and effort needed to explore historical responses, and assisting in writing better responses.  Our work draws parallels between scoring RFPs and essay scoring models, while highlighting new challenges and the need for interpretability. Typical scoring models focus on word level impacts to grade essays and other short write-ups. We propose a novel Bi-LSTM based regression model, and provide deeper insight into phrases which latently impact scoring of responses. We contend the merits of our proposed methodology using extensive quantitative experiments. We also qualitatively asses the impact of important phrases using human evaluators. Finally, we introduce a novel problem statement that can be used to further improve the state of the art in NLP based automatic scoring systems."
"Language model pre-training shown great power improving many natural language processing tasks. Most pre-training models, despite variety, follow BERT architecture heavily relying multi-head self-attention learn comprehensive representations. It found 1) though self-attention module BERT highly non-local operator, large proportion attention heads indeed learn local dependencies due inherent property natural language; 2) removing attention heads fine-tuning downstream tasks degrade performance. The two findings indicate heavy computation redundancy exists current model design. In work, aim resolve intrinsic redundancy issue improve BERT w.r.t. efficiency downstream task performance. We consider question: reduce redundancy attention heads using naturally local operation replace them? We notice convolution successful extracting local features , thus propose use convolution layers efficient complement self-attention addressing local dependencies natural language. Specifically, propose integrate convolution self-attention form \nameofattention{} mechanism combines advantages two operations. Self-attention uses input tokens generate attention weights capturing global dependencies, expect perform local ``self-attention'', i.e., taking local span current token generate ``attention weights'' span capture local dependencies. To achieve this, rather deploying standard convolution fixed parameters shared input tokens, dynamic convolution good choice offers higher flexibility capturing local dependencies different tokens. As shown Fig.b, dynamic convolution uses kernel generator produce different kernels different input tokens. However, dynamic convolution cannot differentiate tokens within different context generate kernels . We thus develop \nameofconv{}, novel convolution produces adaptive convolution kernels receiving input span instead single token, enables discrimination generated kernels tokens within different context. For example, shown Fig.c, proposed \nameofconv{} produces different kernels different ``can'' tokens. With \nameofconv{}, build \nameofattention{} improve conventional self-attention, brings higher efficiency pre-training well better performance capturing global local information. To enhance performance efficiency, also add following new architecture design BERT. First, bottleneck structure designed reduce number attention heads embedding input tokens lower-dimensional space self-attention. This also relieves redundancy lies attention heads improves efficiency. Second, feed-forward module BERT consists two fully connected linear layers activation between, dimensionality inner-layer set much higher input output, promises good performance brings large parameter number computation. Thus devise grouped linear operator feed-forward module, reduces parameters without hurting representation power. Combining novelties together makes proposed model, termed ConvBERT, small efficient. Our contributions summarized follows. 1) We propose new \nameofattention{} replace self-attention modules BERT, leverages advantages convolution better capture local dependency. To best knowledge, first explore convolution enhancing BERT efficiency. 2) We introduce novel \nameofconv{} operation utilize multiple input tokens dynamically generate convolution kernel. 3) Based proposed \nameofconv{ }\nameofattention{}, build ConvBERT model. On GLUE benchmark, ConvBERTbase achieves 86.4 GLUE score 5.5 higher BERTbase 0.7 higher ELECTRAbase requiring less training cost parameters. 4) ConvBERT also incorporates new model designs including bottleneck attention grouped linear operator independent interest NLP model development. \documentclass{article} % need pass options natbib, use, e.g.: \PassOptionsToPackage{numbers, compress}{natbib} % loading neurips_2020 % ready submission % \usepackage{neurips_2020} % compile preprint version, e.g., submission arXiv, add add % [preprint] option: % \usepackage[preprint]{neurips_2020} % compile camera-ready version, add [final] option, e.g.: \usepackage[final]{neurips_2020} % avoid loading natbib package, add option nonatbib: % \usepackage[nonatbib]{neurips_2020} \usepackage[utf8]{inputenc} % allow utf-8 input \usepackage[T1]{fontenc} % use 8-bit T1 fonts \usepackage{hyperref} % hyperlinks \usepackage{url} % simple URL typesetting \usepackage{booktabs} % professional-quality tables \usepackage{amsfonts} % blackboard math symbols \usepackage{nicefrac} % compact symbols 1/2, etc. \usepackage{xcolor} \usepackage{microtype} % microtypography \usepackage{xspace} \usepackage{subcaption} \usepackage{graphicx} \usepackage{amsmath} \usepackage{pifont} \usepackage{floatrow} \floatsetup[table]{capposition=top} \usepackage{float} \usepackage{wrapfig} \usepackage{comment} \usepackage{caption} \definecolor{mypink}{RGB}{251,228, 234} \newcommand\bertbase{BERT\xspace} \newcommand\bertlarge{BERT\xspace} \newcommand{\nameofconv}{span-based dynamic convolution} \newcommand{\nameofattention}{mixed attention} \newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]} \title{ConvBERT: Improving BERT Span-based Dynamic Convolution} % The \author macro works number authors. There two commands % used separate names addresses multiple authors: \And \AND. % % Using \And authors leaves LaTeX determine break % lines. Using \AND forces line break point. So, LaTeX puts 3 4 % authors names first line, last second line, try using % \AND instead \And third author name. \author{Zihang Jiang\thanks{Work done internship Yitu Tech.}~~\thanks{Equal contribution.}~~, Weihao Yu\samethanks~~, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan \\ National University Singapore, Yitu Technology\\ @gmail.com,} \\ , ~ , ~ \\ } \begin{document} % % \newpage We introduced new problem statement NLP researchers, automatic scoring Request Proposals insurance industry. Using generic pdf parser, collected data 1300 RFP responses across multiple states US preprocessed analysed Natural Language Processing Pipelines. We built scoring system using Deep Learning approaches introduced interpretable system identification enabler disabler words phrases. These interpretations assist experts writing better RFP responses. Future work includes building multimodal system model aesthetic well content wise qualities proposal documents."," Pre-trained language models like BERT and its variants have recently achieved impressive performance in various natural language understanding tasks. However, BERT heavily relies on the global self-attention block and thus suffers large memory footprint and computation cost. Although all its attention heads query on the whole input sequence for generating the attention map  from a global perspective, we observe some heads only need to learn local dependencies, which means the existence of computation redundancy. We therefore propose a novel span-based dynamic convolution to replace these self-attention heads to directly model local dependencies. The novel convolution heads, together with the rest self-attention heads, form a new mixed attention block that is more efficient at both global and local context learning. We equip BERT with this mixed attention design and build a ConvBERT model. Experiments have shown that ConvBERT significantly outperforms BERT and its variants in various downstream tasks, with lower training costs and fewer model parameters. Remarkably, ConvBERTbase model achieves 86.4 GLUE score, 0.7 higher than ELECTRAbase, using less than $1/4$ training cost. \footnote{Code and pre-trained model will be released at \url{https://github.com/yitu-opensource/ConvBert}.}"
"One defining characteristics human languages productive. % We combine together concepts novel ways express ideas never thought before. % This good reason: children, observe little world must speak others, meaning even mundane language novel parroting back something already expressed us. % Similarly, even massive data collection efforts, deep models opportunity observe small subset possible utterances worlds. % This problem becomes especially acute models must drive behavior robot, misunderstanding command may pose serious safety hazard. Recently, number attempts probe understanding deep networks trained perform linguistic robotic tasks. % \citet{lake2018still} point generalization novel compositions concepts rather limited. % This matter amount data available; example, \citet{mccoy2019berts} find even networks test set performance different generalization abilities. % Most recently, \citet{ruis2020gscan} released gSCAN, dataset testing generalization abilities grounded robots. % In gSCAN, robotic agent must follow natural-language command 2D environment. % Commands specific types systematically held out; example, command particular adjective-noun combination might appear training set. % When concepts appear training set via random split training test set, performance phenomenal: 97\% commands executed correctly. % Yet, even simplest combinations missing training set, holding adjective-noun pair like ``yellow squares'', 35\% 55\% commands executed correctly. Guided notion compositionality central feature human languages deep networks failing internalize, construct compositional deep network guide behavior robots, thus building work \citet{kuo2020language}. % Given command, command-specific network assembled previously-trained components. % Components automatically discovered training set without annotation. % The structure combines components derived linguistic structure command. % In way, compositional structure language reflected compositional structure computations executed network. % This approach generalizes non-compositional networks fail thereby addressing many limitations pointed gSCAN prior work. % For example, compositional approach correctly executes 95\% commands involving novel adjective-noun pairs, compared 35\% 55\% reported prior work. Compositionality specific one dataset -- general principle -- implementation provide specific gSCAN. % Even though base network achieves 97\% performance state-of-the-art approaches gSCAN, generalizes significantly better number ways. % It outperforms state art test conditions explored gSCAN, generally large margin. % Where approach shines predicted well types compositionality exist network. % For example, novel combinations concepts related individual objects perform well. % However, compositionality exists level map directions, concepts combined, meaning novel combinations directions well understood; even case, approach outperforming state art 5\%. An additional benefit compositional approaches appears open door naturally including general linguistic principles. % For example, appears parses made equal. % In case, network structures derived semantic parser lead better performing agents compared structures derived constituency parser dependency parser. % We show another example idea incorporating lexical semantics words additional loss training network. % This incorporates general notions big small antonyms one another. Our approach forgoes popular mechanism increasing generalization performance neural networks: data augmentation. % Work gSCAN shows gains data augmentation appear low, still significant. % Data augmentation substantial drawbacks: arbitrary, slows runtime, dataset problem specific. % In addition, data augmentation introduces many parameters must tuned hand much knowledge must provided humans. % Instead, show generic principle compositionality replace data augmentation without drawbacks: human annotation required; additional systems parameters need introduced; training time affected. % It remains open question whether every data augmentation approach corresponding compositional structure supplant generalize it. % Compositional approaches could combined data augmentation, potentially raising performance even further. % % The state art performance dataset demonstrates stunning % performance drops., even novel adjective-noun combinations drop % command-following performance 97\% 38\%. % agent needs understand word means, disentangle meaning word % combine learned words interpret new command % need data acquire meanings new words Our work makes four contributions. %=============================================================================== We present novel span-based dynamic convolution operator integrate self-attention mechanism form mixed attention block language pre-training. We also devise bottleneck structure applied self-attention module grouped linear operation feed-forward module. Experiment results show ConvBERT model integrating novelties achieves consistent performance improvements costing much less pre-training computation.","   Humans are remarkably flexible when understanding new sentences that include   combinations of concepts they have never encountered before. Recent work has   shown that while deep networks can mimic some human language abilities when   presented with novel sentences, systematic variation uncovers the limitations   in the language-understanding abilities of neural networks. We demonstrate that these   limitations can be overcome by addressing the generalization challenges in a   recently-released dataset, gSCAN, which explicitly measures how well a robotic   agent is able to interpret novel ideas grounded in vision, e.g., novel   pairings of adjectives and nouns. The key principle we employ is   compositionality: that the compositional structure of networks should reflect   the compositional structure of the problem domain they address, while allowing   all other parameters and properties to be learned end-to-end with weak   supervision. We build a general-purpose mechanism that enables robots to   generalize their language understanding to compositional domains. Crucially,   our base network has the same state-of-the-art performance as prior work, 97\%   execution accuracy, while at the same time generalizing its knowledge when   prior work does not; for example, achieving 95\% accuracy on novel   adjective-noun compositions where previous work has 55\% average   accuracy. Robust language understanding without dramatic failures and without   corner causes is critical to building safe and fair robots; we demonstrate the   significant role that compositionality can play in achieving that goal."
"The proliferation disinformation online, commonly known ``fake news'', given rise lot research automatic fake news detection. However, efforts focused checking whether piece information factually correct, little attention paid propaganda techniques malicious actors use spread message. SemEval-2020 Task 11 aims bridge gap. It focused detecting use propaganda techniques news articles, creating dataset extends , offering two subtasks: Below, describe systems built two subtasks. At core systems RoBERTa , pre-trained model based Transformer architecture . However, improved RoBERTa adding extra layers neural network architecture, added post-processing steps. We applied transfer learning two subtasks, finally, combined different models ensemble. %The rest paper organized follows. Section discusses related work. Section describes general architecture models. Section provides details experimental setup. Section discusses results. Finally, Section offers conclusion. We presented model addresses many compositionality challenges found gSCAN, dataset specifically designed challenge neural networks. We show nothing inherently wrong seq2seq models: able generalize new compositional concepts; require helping hand so. When compositionality inherent problem reflected computation neural network, appears resulting network far better able understand target domain. This critical test time, generalizing new combinations, course, case relevant grounded robotics. An important caveat mechanism making representations sub-networks compatible one another key. Here constraining communication go attention maps. Other mechanisms may exist. Performance compositional approaches depends composed how. When compositionality capture part problem domain, condition D here, role meaningfully improve results. When compositionality relevant, appears supplant data augmentation provide faster, principled, dataset-agnostic method fewer parameters achieve even better results. When compositionality derived language, enables us include linguistic ideas fundamental part models, like notion synonyms antonyms. The suggestive admittedly tenuous implication work perhaps, grounded robotics develops, use approach test linguistic representations. Many formalisms exist linguistics encoding semantics sentences centuries debate brought us closer agreement. Without independent test formalism better, convergence one notion semantics unlikely. It appears grounded compositional models trained perform tasks, representations significantly better others. It also appears semantic representations are, i.e., abstract away surface syntax language, better resulting robotic model is. Perhaps future grounded robotics come full circle, starting borrowing ideas linguistics, contributing understanding language pinpointing language actually means. In meantime, robots continue deployed. As well conversational agents. It critical confidence systems input merely training set cause catastrophic failure. We demonstrate one step toward achieving goal: principled way enable networks generalize training set. Many open problems remain, key among them: way convert data augmentation approach network architecture sees problem generalizes better principled reason without data augmentation. This would powerful tool, suspect exists, yet found. The maximum paper length 8 pages excluding references acknowledgements, 10 pages including references acknowledgements \clearpage The acknowledgments automatically included final version paper. \acknowledgments{This work supported Center Brains, Minds Machines, NSF STC award 1231216, Toyota Research Institute, DARPA GAILA program, United States Air Force Research Laboratory Cooperative Agreement Number FA8750-19-2-1000, Office Naval Research Award Number N00014-20-1-2589, MIT CSAIL Systems Learn Initiative, CBMM-Siemens Graduate Fellowship.}","   We describe our system for SemEval-2020 Task 11 on Detection of Propaganda Techniques in News Articles. We developed ensemble models using RoBERTa-based neural architectures, additional CRF layers, transfer learning between the two subtasks, and advanced post-processing to handle the multi-label nature of the task, the consistency between nested spans, repetitions, and labels from similar spans in training. We achieved sizable improvements over baseline fine-tuned RoBERTa models, and the official evaluation ranked our system $\mathbf{3^{rd}}$  out of 36 teams on the span identification subtask with an F1 score of 0.491, and $\mathbf{2^{nd}}$  out of 31 teams on the technique classification subtask with an F1 score of 0.62."
"The 2019--2020 coronavirus pandemic disrupted lives, societies economies across globe. Its classification pandemic highlights global impact, touching people languages. Digital content types focused many weeks predominantly sanitary crisis effects infected people, families, healthcare workers society economy large. This calls large set tools help pandemic , also tools help digest analyze data ends. By analyzing representation reaction across countries different guidelines global trends, might possible inform policies prevention reaction future epidemics. % AB: ""span countries""? weird formulation Several institutions groups already started take snapshots digital content shared weeks~. However, global scale, digital content accessible variety different languages, existing NLP tools remain English-centric~. In paper describe release multilingual neural machine translation model used translate biomedical text. The model multi-domain multilingual, covering translation French, German, Spanish, Italian Korean English. Our contributions consist release of: This paper structured follows: Section overview previous work upon build; Section details model data settings, released test set; Section compares model public models state-of-the-art results academic competitions. The model downloaded \url{https://github.com/naver/covid19-nmt}: repository consists model checkpoint compatible Fairseq~, script preprocess input text. We described system developed SemEval-2020 Task 11 Detection Propaganda Techniques News Articles. We developed ensembles RoBERTa-based neural architectures, additional CRF layers, transfer learning two subtasks, advanced post-processing handle multi-label nature task, consistency nested spans, repetitions, labels similar spans training. We achieved sizable improvements baseline RoBERTa models, official evaluation ranked system 36 teams span identification subtask, 31 teams techniques classification subtask. In future work, plan explore neural architectures T5 GPT-3 . We want explore transfer learning tasks argumentation mining offensive language detection ."," We release a multilingual neural machine translation model, which can be used to translate text in the biomedical domain. The model can translate from 5 languages  into English. It is trained with large amounts of generic and biomedical data, using domain tags. Our benchmarks show that it performs near state-of-the-art both on news  and biomedical test sets, and that it outperforms the existing publicly released models. We believe that this release will help the large-scale multilingual analysis of the digital content of the COVID-19 crisis and of its effects on society, economy, and healthcare policies. We also release a test set of biomedical text for Korean-English. It consists of 758 sentences from official guidelines and recent papers, all about COVID-19."
"Query auto completion standard component search engines industry. Given prefix, system returns ranked list completed queries match users' intents. Query auto completion enhances user experience two ways: saves user keystrokes returns search results less time; guides users type ""better"" queries likely lead desirable search results. For example, given prefix ""soft"", ""software engineer"" better query ""software programmer"", since former commonly used job title. A typical QAC system takes generate-then-rank two-step framework. The candidate generation component returns frequent queries match prefix, memorizing mapping prefixes queries based search logs . The candidate ranking component extracts features candidates uses produce final ranking order. Both components involve intense computation whole process finished within several milliseconds, order meet online latency requirements. However, traditional approach fully exploit context query prefix. For example, generation phase, unseen prefixes, last word prefix used generate candidates ; ranking phase, effective feature query frequency collected search log, lacks deep semantics understanding. To enable semantic text understanding, neural networks based methods applied QAC. Early works focus ranking stage: Convolutional Neural Networks adopted measure semantic coherence query prefix suggested suffixes ranking phase. Recently, end-to-end approach generation ranking proposed: neural language model trained measure probability sequence tokens. During decoding, candidate generation ranking performed multiple iterations beam search . While neural language models show better sequence modeling power CNN, could take 1 second , making productionization infeasible. In work, goal build QAC system effective query context utilization real world search systems, meeting industry latency standard. We make improvements two-stage generate-then-rank framework: In candidate generation, extend previous work incorporating information unseen prefixes generate meaningful candidates. In candidate ranking, adopt neural language modeling, natural approach model coherence word previous sequence. To overcome latency challenge, optimize latency approximating computation word probability much efficient structure, reducing 95\% latency . Offline experiments public datasets show significant improvement terms relevance latency. We also train model LinkedIn job search dataset deploy production CPU serving. %Note neural language modeling, majority time spent computing probability next word, involves scoring words vocabulary. We hypothesize valid word probability required, long relative candidate ranking position correct. According hypothesis, In summary, contribution paper We describe release multilingual translation model supports translation general biomedical domains. Our model trained 350M sentences, covering French, Spanish, German, Italian Korean . Benchmarks public test sets show strength across domains. In particular, evaluated model biomedical domain, performs near state-of-the-art, advantage single model operates several languages. To address shortage Korean-English data, also release dataset 758 sentence pairs covering recent biomedical text COVID-19. Our aim support research studying international impact crisis causing, societal, economical healthcare level. \todo{run: NEWSTEST.it.en model, model Helsinki [DONE] new test data}"," Query Auto Completion , as the starting point of information retrieval tasks, is critical to user experience. Generally it has two steps: generating completed query candidates according to query prefixes, and ranking them based on extracted features. Three major challenges are observed for a query auto completion system:  QAC has a strict online latency requirement. For each keystroke, results must be returned within tens of milliseconds, which poses a significant challenge in designing sophisticated language models for it.  For unseen queries, generated candidates are of poor quality as contextual information is not fully utilized.  Traditional QAC systems heavily rely on handcrafted features such as the query candidate frequency in search logs, lacking sufficient semantic understanding of the candidate.  In this paper, we propose an efficient neural QAC system with effective context modeling to overcome these challenges. On the candidate generation side, this system uses as much information as possible in unseen prefixes to generate relevant candidates, increasing the recall by a large margin. On the candidate ranking side, an unnormalized language model is proposed, which effectively captures deep semantics of queries. This approach presents better ranking performance over state-of-the-art neural ranking methods and reduces $\sim$95\% latency compared to neural language modeling methods. The empirical results on public datasets show that our model achieves a good balance between accuracy and efficiency. This system is served in LinkedIn job search with significant product impact observed."
"Probabilistic topic models assume words generated latent topics inferred word co-occurrence patterns taking document global context. In recent years, various neural topic models proposed. Some built Variational Auto-Encoder utilizes deep neural networks approximate intractable posterior distribution observed words given latent topics . However, models take bag-of-words representation given document input VAE aim learn hidden topics used reconstruct original document. They learn word embeddings concurrently. Other topic modeling approaches explore pre-trained word embeddings extraction semantically coherent topics since word embeddings capture syntactic semantic regularities encoding local context word co-occurrence patterns. For example, topic-word generation process traditional topic models replaced generating word embeddings given latent topics two-component mixture Dirichlet multinomial component word embedding component . Alternatively, information derived word embeddings used promote semantically-related words Polya Urn sampling process topic models generate topic hierarchies . However, models use pre-trained word embeddings learn word embeddings jointly topics. While word embeddings could improve topic modeling results, conversely, topic information could also benefit word embedding learning. Early word embedding learning methods learn mapping function project word single vector embedding space. Such one-to-one mapping cannot deal word polysemy, word could multiple meanings depending context. For example, word `patient' two possible meanings `enduring trying circumstances even temper' `a person requires medical care'. When analyzing reviews restaurants health services, semantic meaning `patient' could inferred depending topic associated with. One solution first extract topics using standard Latent Dirichlet Allocation model incorporate topical information word embedding learning treating topic pseudo-word . Whereas aforementioned approaches adopt two-step process, either using pre-trained word embeddings improve topic extraction results topic modeling, incorporating topics extracted using standard topic model word embedding learning, \citet{shi2017jointly} developed Skip-Gram based model jointly learn topics word embeddings based Probabilistic Latent Semantic Analysis , word associated two matrices rather vector induce topic-dependent embeddings. This rather cumbersome setup. \citet{foulds2018mixed} used Skip-Gram imitate probabilistic topic model word represented importance vector topics context generation. In paper propose neural generative model built VAE, called Joint Topic Word-embedding model, jointly learning topics topic-specific word embeddings. More concretely, introduce topics tangible parameters shared across context windows. We assume pivot word generated hidden semantics encoding local context occurred. Then hidden semantics transformed topical distribution taking account global topics, enables generation context words. Our rationale context words generated hidden semantics pivot word together global topic matrix, captures notion word multiple meanings shared across corpus. We thus able learn topics generate topic-dependent word embeddings jointly. The results model also allow visualization word semantics topics visualized via top words words encoded distributions topics\footnote{Our source code made available \url{http://github.com/somethingx02/topical\_wordvec\_models}.}. In summary, contribution three-fold: This paper presents empirical study large-scale semi-supervised learning CTC acoustic models strong offline teacher model used generate pseudo-labels unlabelled data. The unlabelled data selected based confidence domain distribution well speaker content variability. Experimental results two different dialects reinforce efficacy teacher generated pseudo labels importance intelligent data selection methods. It observed domain-specific unlabelled data strong impact corresponding WER little cross-domain impact signifying importance sampling strategy boosting performance low resource domains. Future work direction would devise strategy leverage confidence well domain diversity combined data sampling strategy SSL. Another important future direction study impact word level decoding incorporates lexicon strong language model improving quality teacher generated pseudo-labels."," We propose a novel generative model to explore both local and global context for joint learning topics and topic-specific word embeddings. In particular, we assume that global latent topics are shared across documents, a word is generated by a hidden semantic vector encoding its contextual semantic meaning, and its context words are generated conditional on both the hidden semantic vector and global latent topics. Topics are trained jointly with the word embeddings. The trained model maps words to topic-dependent embeddings, which naturally addresses the issue of word polysemy. Experimental results show that the proposed model outperforms the word-level embedding methods in both word similarity evaluation and word sense disambiguation. Furthermore, the model also extracts more coherent topics compared with existing neural topic models or other models for joint learning of topics and word embeddings. Finally, the model can be easily integrated with existing deep contextualized word embedding learning methods to further improve the performance of downstream tasks such as sentiment classification."
"Relational databases prevalent many real-world applications. Typically, structured query language SQL required interact databases. However, mastering SQL queries generally difficult. It long standing goal enable users interacting relational databases via human natural languages. The general problem known ``Natural Language Interface Databases~'' database areas~. In recent years, surge interest deep learning-based approaches crucial problem NLIDBs: translating natural language query SQL, often referenced ``NL-to-SQL'' ``Text-to-SQL''~. In paper, focus Text-to-SQL problem experiment WikiSQL~~ dataset~. WikiSQL first large-scale dataset Text-to-SQL, 80K human annotated pairs NL question SQL query. WikiSQL constrained problem two factors -- question addressed single table, table known. This constrained setting guided research focus core elementary problem. Even though scope constrained, dataset still challenging tables questions diverse. Notably, 24K different tables associated dataset. In past, several approaches proposed WikiSQL dataset. They primarily share similar encoder-decoder architecture. The encoder encodes information NL question table schema hidden representation. Some encode NL question full table schema, e.g., concatenating NL question column names~, others encode NL question column separately~. There also work different layers~. The decoder decodes hidden representation SQL query. Some early work tried ``to sequence'' style one step decoding~ however found challenging guarantee output correct syntax. Later work breaks SQL query several parts like SELECT column, WHERE column, WHERE operator, WHERE value, etc. sub-decoders them~. In way, chance output violating syntax reduced. Beyond improving Text-to-SQL task models, recently work study leverage pre-trained language models~ get promising results~. All previous work reveal several major challenges Text-to-SQL WikiSQL: How fuse information NL question table schema, handled encoder; How make sure output SQL query executable accurate, handled decoder; How leverage pre-trained language models. This paper primarily motivated challenge , however, proposed approach contributes solving challenges. We argue previous approaches~ align task model well base language model, hence, power base language model compromised task model. Specifically, \citet{hwang2019sqlova} \citet{he2019xsql} concatenate full table schema NL question feed BERT-alike base language model. However, decoding stage, need hidden representation individual column. Thus, apply adhoc pooling tokens column name get vector column. \citet{hwang2019sqlova} simply pick vector first token ``shallow layer'' added another LSTM layers ``decoder-layer'' ``NL2SQ-Layer''. \citet{he2019xsql} apply weighted average vectors tokens. These ad-hoc pooling operations additional LSTM layers caused information loss bring unnecessary complexity. To solve dilemma encountered previous work, paper, choose encode pair NL question one individual column via exact BERT/RoBERTa model structure. Then multiple sub-tasks decoder stage: SELECT \& WHERE column ranking, condition operator, condition value span. Since decoder output final SQL query, apply straightforward rules assemble decoder outputs final SQL query. Our approach two benefits. First, inputs, i.e., question column pair align perfectly original sentence pair training tasks BERT/RoBERTa, hence, believe power BERT/RoBERTa utilized best. Second, encode one column, ``[CLS]'' vector BERT/RoBERTa output captures fused information question column, exactly ``column vector'' needed decoder. So need apply pooling additional complex layers. This makes model structure simple efficient. Since main philosophy approach ranking columns multi-task outputs, name Hybrid Ranking Network . In summary, contributions three folds. First, propose simple efficient network structure perfectly aligns Text-to-SQL task base language models, hence, power base language models best utilized. Second, base language model encoder directly encodes NL question column without additional pooling operations believed best encoder capturing question-column relation Text-to-SQL. Third, proposed hybrid ranking mechanism execution-guided decoding handle column-column relations effectively boost accuracy. In words, approach resolved aforementioned challenges time. Our approach achieves best result WikiSQL dataset, verifies effectiveness contributions. We argue that, WikiSQL task, columm-column interdependency quite rare. So, main relationship learn columns NL question. So, propose two stage approach paper:\\ In first stage, learn column-question relationships. We consider one column time, form input question-column pair train model classify whether column SELECT WHERE column. \\ In second stage, keep relevant columns first step via ranking use execution-guided decoding construct SQL query. Execution guided decoding takes necessary column-column relationships account. Our approach addresses issues leverages BERT recommended tasks setting input setting input length grow number candidate columns, hence able scale even large databases. \end{comment} There doubt state-of-the-art models text-to-SQL task must leverage pre-trained deep Transformer models like BERT, MT-DNN RoBERTa. However, previous approaches leverage pre-trained contextualized word representation concatenate column names table along NL question encoding layer order learn interdependencies columns SELECT clause WHERE conditions well columns different WHERE conditions . This several serious limitations: \\ BERT pre-trained sentence pairs, sentence sequences \\ prediction could instable column set different. Adding irrelevant column results different input embedding sequence. \\ Number columns could different, makes difficult static computation graph \\ Hard learn interdependencies. sparse. model consider those. exponetially complex serious limitations previously proposed models use contextualized word representations . To overcome issue, propose two stage approach: \\ Predict one attribute time slot: prediction independently. How likely column SELECT clause WHERE condition independent attributes. \\ Model interdependencies use execution guidance assemble: Model interdependies second step. Here, rules application introduced. Generic rules. Application specific rules. EG also used. \\ \end{comment} Driven motivation combining word embedding learning topic modeling mutually benefit other, propose probabilistic generative framework jointly discover semantically coherent latent topics global context also learn topic-specific word embeddings, naturally address problem word polysemy. Experimental results verify effectiveness model word similarity evaluation word sense disambiguation. Furthermore, model discover latent topics shared across documents, encoder JTW generate topical distribution word. This enables intuitive understanding word semantics. We also shown proposed JTW easily integrated deep contextualized word embeddings improve performance downstream tasks. In future work, explore discourse relationships context windows model, example, semantic shift neighboring sentences."," 		In this paper, we study how to leverage pre-trained language models in Text-to-SQL. We argue that previous approaches under utilize the base language models by concatenating all columns together with the NL question and feeding them into the base language model in the encoding stage. We propose a neat approach called Hybrid Ranking Network~ which breaks down the problem into column-wise ranking and decoding and finally assembles the column-wise outputs into a SQL query by straightforward rules. In this approach, the encoder is given a NL question and one individual column, which perfectly aligns with the original tasks BERT/RoBERTa is trained on, and hence we avoid any ad-hoc pooling or additional encoding layers which are necessary in prior approaches. Experiments on the WikiSQL dataset show that the proposed approach is very effective, achieving the top place on the leaderboard."
"\sockeye versatile toolkit research fast-moving field NMT. Since initial release, used least 25 scientific publications, including winning submissions WMT evaluations . \sockeye also powers \amt, showing industrial-strength performance addition flexibility needed academic environments. Moreover, excited see hardware manufacturers contributing optimizing \mxnet \sockeye speed. Intel demonstrated large performance gains \sockeye inference Intel Skylake processors. NVIDIA working significant performance improvements \sockeye's Transformer implementation fused operators optimized beam search. This paper discusses \sockeyeTwo's streamlined \gluon implementation , support state art architectures efficient decoding , improved model training . In paper, study leverage pre-trained language models like BERT WikiSQL task. We formulate text-to-SQL column-wise hybrid ranking problem propose neat network structure called HydraNet best utilizes pre-trained language models. The proposed model simple architecture achieves No.1 result WikiSQL leaderboard. We believe bring deep insight Text-to-SQL problem better utilize pre-trained language models. In future, extend capability HydraNet support full SQL grammar.","   We present \sockeyeTwo, a modernized and streamlined version of the \sockeye neural machine translation  toolkit.   New features include a simplified code base through the use of \mxnet's \gluon API, a focus on state of the art model architectures, distributed mixed precision training, and efficient CPU decoding with 8-bit quantization.   These improvements result in faster training and inference, higher automatic metric scores, and a shorter path from research to production."
"} {D}{eep} learning models revolutionized multiple fields information systems including natural language processing , computer vision, speech analysis. While enabled multiple tasks attain high accuracy values, model sizes prediction latencies increased significantly well. Specific text, Recurrent neural networks , Gated Recurrent Units long short term memory networks used quite time various natural language processing tasks. These models large especially input output embedding parameters. %For example, ClueWeb dataset, vocabulary contains 10M words. If embedding vectors 1024 dimensions dimension represented using 32 bits, size input-embedding matrix 40GB. Further considering output-embedding matrix weights hidden layers, RNN model 80GB. In past three years, field NLP made significant progress evident GLUE SuperGLUE leaderboards. Transformer based models like Bidirectional Encoder Representations Transformers , Generative Pre-training Transformer , Multi-task Deep Neural Network , Extra-Long Network , MegatronLM, Text-to-text transfer transformer , T-NLG GShard major contributors success. But models humongous size: BERT , GPT-2 , MegatronLM , T5 , T-NLG GShard . Bianco et al. Sanh et al. provide great overview sizes recent deep learning models computer vision NLP respectively. Deployment gigantic models difficult even high-end servers. Indeed large number real world applications run machines resource constrained environments, example, edge devices, sensors mobile phones. Edge devices could include offline medical equipment, modules drones, robots, glasses, etc. Often times, besides desiring small model size, low response times critical. For example, applications like driverless cars apps aid blind require processing speeds around 30 frames per second. Similarly, search engines need able process billions queries per day. Overall, following factors motivate us study compression deep learning models. Large networks fit on-chip storage hence require costly external DRAM accesses. Running 1 billion connection neural network, example, 30 frames per second would require = 19.2W DRAM access -- well beyond power envelope typical mobile device. This implies mobile phone running app could suffer fast draining battery, leading overheating phone. Han et al. discuss details power dissipation deep learning models. Another option avoid large RAM, high prediction times high power dissipation, run massive deep learning models cloud servers. But many real world applications, desirable run local client devices avoid network delay, guard user privacy avoid power dissipation terms input/output data communication. Small models indeed also lead low prediction latencies. For example, Diamos et al. showed small models, one cache RNN weights on-chip memory caches, block RAM, register files across multiple timesteps. This could lead much 146x speedup entire RNN layer stored registers rather GPU DRAM NVIDIA TitanX GPU. Finally, easier perform software development, deployment updates smaller models. Large models difficult handle. For example, impossible fine-tune pretrained BERT-large model GPU 12-16 GB RAM. This poses large barrier entry communities without resources purchase several large Graphic Processing Units . For large models, tuning various configuration parameters needs lots resources. Smaller models lead improved speed learning allow hyper-parameter configurations evaluated. Mobile-first companies dislike large apps. App stores sensitive size binary files. For example, iPhone App Store restriction ``apps 150 MB download connect Wi-Fi''. Smaller models easier use deploy real world systems. Large models need multiple server nodes. On hand, multiple instances smaller models run simultaneously machine leading higher QPS . Lastly, smaller models also decrease communication overhead distributed training models. Fortunately, large amount redundancy among weights large neural networks. A small subset weights sufficient reconstruct entire network. Denil et al. showed simply using 5\% weights, possible predict remaining weights without drop accuracy. This observation led large number research efforts across multiple communities compression large deep learning models. In survey, aim systematically explore large body literature NLP community organizing various categories. Figure shows broad organization model compression methods text. %The remainder survey organized follows. We provide broad overview popular approaches model compression Section. Further, delve deeper compression methods using pruning, quantization, knowledge distillation, parameter sharing, tensor decomposition linear Transformers Sections to. In Section, present benchmark text use cases model compression methods shown immense gains. Finally, conclude survey summary discussion future trends Section. In survey focus specific methods proposed communities like vision speech make use image/audio specific architectures hence cannot applied text. Also, discuss methods hardware optimizations reduce latency. While surveys broad area model compression, either old focus computer vision related problems. \sockeyeTwo provides out-of-the-box support quickly training strong Transformer models research production. Extensive configuration options simplified \gluon code base enable rapid development experimentation. As open source project, invite community contribute ideas \sockeyeTwo hope new programming model various performance improvements enable others conduct effective successful research. \small"," In recent years, the fields of natural language processing  and information retrieval  have made tremendous progress thanks to deep learning models like Recurrent Neural Networks , Gated Recurrent Units  and Long Short-Term Memory  networks, and Transformer based models like Bidirectional Encoder Representations from Transformers . %, Generative Pre-training Transformer , Multi-task Deep Neural Network , Extra-Long Network , Text-to-text transfer transformer , etc.  But these models are humongous in size. %: BERT , GPT-2 , T5 . On the other hand, real world applications demand small model size, low response times and low computational power wattage. In this survey, we discuss six different types of methods  for compression of such models to enable their deployment in real industry NLP  projects. Given the critical need of building applications with efficient and small models, and the large amount of recently published work in this area, we believe that this survey organizes the plethora of work done by the `deep learning for NLP' community in the past few years and presents it as a coherent story."
"In natural language processing, pre-trained contextual representations widely used help downstream tasks lack sufficient labeled training data. Previous works develop various self-supervised tasks obtain pre-trained contextual representations. Taking classic masked language modeling task used BERT example, first randomly chooses small number positions sentence, mask words position learns encoder restore them. As tasks require human supervision, size available training data could easily amount scale billions words. Pre-training large-scale data consumes exceptionally huge computational resources . In paper, tackle training efficiency issue develop novel variance-reduced algorithm better language pretraining. In particular, observe previous works use uniformly sampled positions mask constructing self-supervised tasks, inevitably inefficient optimization perspective. For instance, BERT training, find commonly used words punctuation easy learn, i.e., words correctly predicted model thousands training steps. Meanwhile, rare words phrases difficult predict even end training. If always uniformly sample positions mask, intuitively say, variance stochastic gradient large since positions gradually provide less informative signals not. Usually, learning large-variance gradient estimator inefficient ineffective. To formally characterize variance stochastic gradient, first introduce principled gradient variance decomposition theorem. The theorem shows gradient variance naturally decomposed two parts. One part concerns variance sentences sampled batch, part concerns variance masked positions. Our focus variance reduction second part. Importance sampling standard way variance reduction, suggests sample masks using proposal distribution instead uniform distribution. According theory, variance minimized probability mask sampled proposal distribution proportional gradient norm. However, brings chicken-egg problem: We never know gradient norm unless mask positions, feed masked sentence model back-propagate loss. As number possible masks huge, feeding possibilities network obtain gradients time expensive significantly slows training process. To address challenge, introduce meta-learning approach introducing MAsk Proposal Network takes whole sentence input outputs probability distribution positions sample masks. Both MAP-Net pretraining model jointly optimized adversarial manner. Given masked sentence sampled MAP-Net, model optimized recover masked sentence. At time, MAP-Net receives signals performance model masked sentence, improve itself. Instead using reinforcement learning, decouple learning objective make training MAP-Net easier. We show language generation tasks, use value loss instead value gradient norm. Therefore, goal MAP-Net find ``tough'' masked positions high losses challenge model, model attempts fulfill tasks generated MAP-Net. As obtain loss many masked positions, MAP-Net efficiently optimized pair-wise preference different positions. To demonstrate advantage proposed method, conduct several experiments using MAP-Net help training BERT, evaluate GLUE natural language understanding benchmark . Experiment results first indicate masked words generated MAP-NET meaningful informative training. Furthermore, variance sufficiently reduced, BERT model learned MAP-Net achieves better accuracy baselines tasks. In paper, attempt develop hybrid architecture extract different aspects semantic information linguistic data diverse types neural structures. Intriguingly, propose novel hierarchical multi-granularity attention mechanism, consisting syntactical attention symbolic level semantical attention embedding level, respectively. The experimental results show MahNN model achieves impressive performances variety benchmark datasets text classification task. Moreover, visualization attention distribution illustrates hierarchical multi-granularity attention mechanism effective capturing informative semantics different perspectives. We draw following conclusions work: There several future directions extend work. First, would investigate applying generative model obtain multichannel representations texts. Data augmentation way represent much richer semantics. Second, ConvNets require fixed-length inputs perform unnecessary convolution operations NLP tasks. It worthwhile explore novel ConvNet architecture processing variable length. Moreover, use simple calculating methods attention weights might able demonstrate full potential hierarchical multi-granularity attention mechanism. It would intriguing compute attention weights advanced approaches transfer learning reinforcement learning improve performance."," Self-supervised learning, a.k.a., pretraining, is important in natural language processing. Most of the pretraining methods first randomly mask some positions in a sentence and then train a model to recover the tokens at the masked positions. In such a way, the model can be trained without human labeling, and the massive data can be used with billion parameters. Therefore, the optimization efficiency becomes critical.  In this paper, we tackle the problem from the view of gradient variance reduction. In particular, we first propose a principled gradient variance decomposition theorem, which shows that the variance of the stochastic gradient of the language pretraining can be naturally decomposed into two terms: the variance that arises from the sample of data in a batch, and the variance that arises from the sampling of the mask. The second term is the key difference between self-supervised learning and supervised learning, which makes the pretraining slower. In order to reduce the variance of the second part, we leverage the importance sampling strategy, which aims at sampling the masks according to a proposal distribution instead of the uniform distribution. It can be shown that if the proposal distribution is proportional to the gradient norm, the variance of the sampling is reduced. To improve efficiency, we introduced a MAsk Proposal Network , which approximates the optimal mask proposal distribution and is trained end-to-end along with the model. According to the experimental result, our model converges much faster and achieves higher performance than the baseline BERT model."
"Written Chinese explicit word boundary, Chinese word segmentation serves upstream disambiguation step Chinese language processing. The task often viewed sequence labeling, character receives label indicating relative position segmented sentence. While traditional machine learning methods attained strong results, recent research focuses neural networks. \citet{shi-etal-2017-neural} first treat CWS neural machine translation . Nonetheless, \citet{zhao-etal-2018-chinese} point without extra resources, previous neural methods yet comparable non-neural state art \citet{zhao-kit-2008-unsupervised}, NMT method even behind. We note two advantages NMT: entire sentence encoded making decision, model jointly trains character embeddings sequence modeling. Thus, try bridge gap NMT approach SOTA, using low-resource techniques regularization data augmentation. Then, explore techniques commonly seen NMT. The translation-based method easy adopt without hassle feature engineering model design. In constrained test condition, method reaches top MSR dataset achieves strong result PKU dataset without tuning. As generic approach also used languages. In work, propose MAP-Net, uses mask proposal network reduce gradient variance pre-training. In particular, given sentence, MAP-Net outputs probability distribution positions sample masks. Then, BERT model trained recover masked sentence sampled MAP-Net, instead uniform distribution. Extensive experiments demonstrate MAP-Net better downstream tasks. It outperforms BERT GLUE tasks less training steps. In future, continue exploring methods reduce variance pretraining, e.g., smartly select batched sentences."," % Supervised Chinese word segmentation has been widely approached as sequence labeling or sequence modeling. Recently, some researchers attempted to treat it as character-level translation, but there is still a performance gap between the translation-based approach and other methods. In this work, we apply the best practices from low-resource neural machine translation to Chinese word segmentation. We build encoder-decoder models with attention, and examine a series of techniques including regularization, data augmentation, objective weighting, transfer learning and ensembling. When benchmarked on MSR corpus under closed test condition without additional data, our method achieves 97.6\% F1, which is on a par with the state of the art and remarkably better than previous translation-based methods. Supervised Chinese word segmentation has entered the deep learning era which reduces the hassle of feature engineering. Recently, some researchers attempted to treat it as character-level translation which further simplified model designing and building, but there is still a performance gap between the translation-based approach and other methods. In this work, we apply the best practices from low-resource neural machine translation to Chinese word segmentation. We build encoder-decoder models with attention, and examine a series of techniques including regularization, data augmentation, objective weighting, transfer learning and ensembling. Our method is generic for word segmentation, without the need for feature engineering or model implementation. In the closed test with constrained data, our method ties with the state of the art on the MSR dataset and is comparable to other methods on the PKU dataset."
"In era mobile reading, lot self-media platforms based user-generated-content mode emerged. People accustomed spending fragmented time reading online articles published self-media platforms conveniently get information knowledge via mobile devices. Different traditional documents essays, academic papers Wikipedia documents, self-media online articles diverse multimedia elements, addition text, usually existing pictures, videos, etc. The organization elements jointly affects users' perception. Besides, since creation forms self-media online articles free, articles unified format layout, usually vary diverse categories, styles content. Therefore, necessary integrate different multimedia elements comprehensively jointly process self-media online articles. The openness self-media platforms, user producer, however, results uneven quality online articles. Assessing quality self-media online articles critical issue many applications recommender systems online search find high-quality articles filter low-quality articles. It helpful increase user stickiness propose efficient solution automatic evaluation self-media online article quality. Considering nature self-media platforms, order engage users, quality self-media online articles reasonably defined level reading experience articles give users. This reflected article's content, writing norms, user perception, etc., factor also contains complicated elements, making self-media online article quality assessment much complex challenging task. The following question lies addressed: How establish unified framework effectively solve multivariate representation learning self-media online article quality? However, current studies document quality assessment mainly focus textual features . To best knowledge, prior work studied automatic self-media online article quality assessment. % % Our low-resource translation approach Chinese segmentation achieves strong performance easy adopt. Data augmentation, objective weighting ensembling beneficial. In future, worth extending languages.","   The automatic quality assessment of self-media online articles is an urgent and new issue, which is of great value to the online recommendation and search. Different from traditional and well-formed articles, self-media online articles are mainly created by users, which have the appearance characteristics of different text levels and multi-modal hybrid editing, along with the potential characteristics of diverse content, different styles, large semantic spans and good interactive experience requirements. To solve these challenges, we establish a joint model CoQAN in combination with the layout organization, writing characteristics and text semantics, designing different representation learning subnetworks, especially for the feature learning process and interactive reading habits on mobile terminals. It is more consistent with the cognitive style of expressing an expert's evaluation of articles. We have also constructed a large scale real-world assessment dataset. Extensive experimental results show that the proposed framework significantly outperforms state-of-the-art methods, and effectively learns and integrates different factors of the online article quality assessment."
"Dialogue state modules central component task-oriented dialogue system. Given user utterance existing dialogue history, dialogue system typically extracts dialogue states, according system response generated. An example shown Figure, given two turns dialogue, first user utterance ``I want expensive restaurant serves Turkish food.'', dialogue states consist slot-value pairs . As dialogue proceeds, dialogue state updated turn. After tow dialogue turns, dialogue state becomes , inform represents search constraints expressed user request represents search target user asking for. In example, user intention reserve restaurant. The business domain restaurant customer service. The dialogue state represents user looking current turn conversation. % While traditional dialogue state tracking modules extract user intention slot values pipeline [], recent work performances end-to-end DST extracting slot-value pairs only. {\bf While traditional dialogue state tracking performed spoken language understanding module , recent work perform end-to-end DST joint modeling SLU DST.} % Recent work focused scalability DST, unknown slot values problem multi-domain problem , aim simulate real-world applications. The first aims git rid limitation pre-defined ontology since often impossible enumerate possible slot values industry. The latter focuses knowledge sharing across domains handle samples unseen domains training. Prior work mostly followed manual labeling-train-test paradigm, begins design annotation guidelines, followed collection manual labeling training corpora, training model. The supervised learning task called Dialogue State Tracking . One limitation supervised learning manual labeling process slow costly given certain domain customer service. Available datasets labeled popular domains restaurant, taxi, train hotel. However, practice, number customer service domains ranges far beyond hundreds , makes infeasible manually label corpora every domain. In addition, shown ratio annotation errors high 30\% even 40\% DST task . %Due reason, relies manually annotated datasets, mostly constructed using Wizard-of-Oz approach based fixed pre-defined ontology. restricts ability learn large data since dialogue state difficult part annotate conversation. As shown figure ???, sample difficulty. Most existing task-oriented datasets originally benchmark DSTC2\&3 recently popular MultiWOZ instead real-word customer service conversations. Mostly recently proposed datasets Though recently proposed datasets including including DSTC2, KVRET MultiWOZ etc, SGD, Taskmaster-1 MultiDoGo ary trying increase magnitude domains, slots values. % Many human-human conversations accessible real life applications, nearly impossible annotate dialogue states huge complicated conversations even backend API database cannot provided. %for example, Twitter corpus released Kaggle \footnote{https://www.kaggle.com/thoughtvector/customer-support-on-twitter}, contains three million tweets replies customer support agents big companies customers Ubuntu dialogue corpus . %In occasions, gather dialogue state information solely based conversation utterances realistic. To address issue, useful automatically induce dialogue states raw dialogues. We assume large set dialogue records many different domains, without manual labeling dialogue states. Such data relatively easy obtain, example customer service call records different businesses. Consequently, propose task {\bf dialogue state induction} , automatically induce slot-value pairs raw dialogue data better used downstream dialogue tasks database query, act prediction response generation. The difference DSI DST illustrated Figure. Similar DST models, DSI outputs dialogue states slot-value pairs , price represents slot, expensive represents value. For requestable slots , request regarded slot area regarded value. During training, DST relies dialogue record manual labeling slot-value pairs dialogues. In contrast, task rely manual labeling generate slot-value pairs raw dialogues automatically. % During testing, existing methods rely ontology trained model label slot values dialogues, method rely ontology. {\bf existing methods considering unlabelled training data rely ontology, supervised models rely ontology.} % To address problem, introduce new task called dialogue state induction , aims automatically discover dialogue states human-human conversations without annotation. A similar work motivation dialogue intent induction, also try explore realistic dialogue system discovering intents user query utterances. Our task challenging compared since explore subdivided slot distribution one intention complex. %which still far away industry applications. From originally standard benchmark DSTC3 popular MultiWOZ. Most recently, However, real applications, small scale due reason. The popular dataset MultiWOZ expensive annotate large-scale dataset especially new task coming. Most recently, two datasets proposed solve annotation problem, provides two new perspectives DST. % We propose two generative models, induce slot-value pairs treating slots latent variables one-hot contextualized representations values generated. % % We experimented ELMo BERT [] contextualized representation. % The goal gather similar values slot. In models, Gaussian latent vector used model dialogue states, generates corresponding slot distribution. The second model additionally uses mixture Gaussian dialogue states model domain explicitly, similar values ``destination'' different domains mix slot. % % {\bf Similar recent end2end approaches DST, jointly model SLU DSI, case domain considered part slot induced together. Since different domains may contain slot, example, slot to\_location to\_location appear taxi bus domains, avoid similar values slot different domains mixed domain, second model additionally uses mixture Gaussian dialogue states model domain explicitly. If so, Figure corresponding descriptions changed.} % Both models trained variational inference. We introduce two neural latent variable models DSI treating whole state slot latent variables, values observed dialogue data generated. The goal induce slots according frequently co-occurring values dialogue contexts. In particular, value represented using sparse one-hot representation dense contextualized embedding representation. Both models generative probabilistic models, generate value first generating latent dialogue state vector, generating slot. The difference two models modeling service domains. We observe different service domains may contain slots similar contexts values. For example, taxi bus domains slot to\_location. In order mix structures large dialogue record, second model considers service domain explicitly taking dialogue state Mixture-of-Gaussians. We refer basic model DSI-base advanced model DSI-GM. % We build unsupervised generative model DSI. {\bf briefly description model} %, solely dependent plain dialogue utterances, also try handle unknown slot values multi-domain problems. Except scalability, model flexible since latent structure easily adjusted use additional annotation supervision. Experiments MultiWOZ SGD datasets show DSI models effectively induce dialogue states compared random select strategy. In addition, Gaussian mixture model gives significantly better results compared basic model. Finally, apply DSI recently proposed dialogue system, replacing dialogue state module DSI-GM model. Results show adding induced dialogue states gives significantly better results dialogue act prediction response BLEU compared dialogue system without considering dialogue states. In particular, BLEU score using DSI-GM model outputs better 2.1\% compared using dialogue states, lower 0.8\% compared using manual labeling dialogue states. This shows inducing dialogue states, improved dialogue systems obtained. To knowledge, first automatically induce dialogue states form slot-value pairs using neural latent variable model. Our models serve baselines research. We release code \url{https://github.com/taolusi/dialogue-state-induction}. % We want highlight four points: model solve unknown slot values problem multi-domain problem. model effectively reduce Inference Time Complexity since constrained number slots values pre-defined ontology list. although use external knowledge unsupervised model, easily extended external sources like slot values backend SLU system outputs. Annotated training data easily added models become semi-supervised. % {\bf clarify result dsi?} To demonstrate effective model, also apply model recently proposed pipeline system replacing dialogue state module DSI model. Results show compared empty dialogue states, dialogue states induced model help improve result downstream dialogue act prediction response generation obviously. To knowledge, first apply neural latent variable model inducing dialogue state. We release code XXX. % Similar \citet{rastogi2017scalable} \citet{goel2019hyst}, construct slot value candidate set given dialogue history certain turn. We build model ... % unknown slot values People tend solve small realistic research problems area. This infinite slot value problem. To summarize related work aimed solve problem. One ACl paper Chumenwenwen, many papers aimed solve this, maybe top conference papers selected shown. % Multi-domain Infinite slot value problem useful solving realistic application problem based current researches enough. Schema-Guided Dialogue State Tracking proposed solve problem. This applied similar domain DST, however, complete new domains, large semantic barrier hinder application. Besides, additional annotation sentence description also expensive quality affect results obviously. related work % However, still far away realistic application. The origin problems lie annotation difficulty cost. Thus, task-oriented dialogue corpus limited scale compared open-domain dialogue corpus. Thus, existing corpus based small ontology, still far away real world user cases. % comparable results compared supervised methods. % Useful response generation. For SOTA models without dialogue state, adding DSI results get improvements. % Existing task-oriented dialogue corpus domains include restaurant, flight, etc. Our method especially helpful totally new domains, like online shopping client service, plenty dialogue records, huge number slots values make even impossible annotate. shot learning, maybe still difficult split categories annotate. And methods based large amount dialogues find similarity. In work, propose novel method self-media online article quality classification. We design joint network CoQAN decouple modeling layout organization, writing characteristics text semantics, finally merge unified model. We innovatively propose explicitly learn presentational quality online articles, together text quality predict final quality online articles. The proposed framework integrate different features online article quality assessment, specially designed high-order interactive feature learning mobile browsing habits modeling solve problems field well. Evaluation results based real-world online article dataset demonstrate effectiveness superiority proposed CoQAN. Since self-media articles convey main content core ideas mainly text, considering complexity task efficiency practical applications, proposed network focuses understanding semantics text. More generalized assessments may need introduce semantic judgment pictures leave future work. If work appendix, place put it."," Dialogue state modules are a useful component in a task-oriented dialogue system. Traditional methods find dialogue states by manually labeling training corpora, upon which neural models are trained. However, the labeling process can be costly, slow, error-prone, and more importantly, cannot cover the vast range of domains in real-world dialogues for customer service. We propose the task of dialogue state induction, building two neural latent variable models that mine dialogue states automatically from unlabeled customer service dialogue records. Results show that the models can effectively find meaningful dialogue states. In addition, equipped with induced dialogue states, a state-of-the-art dialogue system gives better performance compared with not using a dialogue state module."
"Spoken dialog systems utilized voice assistants Alexa, Siri Google Home, typically consists sequential chain sub-systems, including Spoken Language Understanding , Dialog Management, Natural Language Generation Text-to-Speech. Generally, sub-systems perform cloud-based processing speech, following on-device wakeword detection. First, SLU system extracts natural language semantics utterance intent well associated named entities slot values speech segment. The appropriate application invoked execution finally responses processed text-to-speech system relayed user. An example intents slots utterance Table.\ . Conventionally, SLU system comprises two distinct stages: An Automatic Speech Recognition system obtains transcript text representation raw audio segment, A Natural Language Understanding system subsequently consumes transcript alternatively n-best hypotheses ASR system extracts semantics, particular, domain, intent slots. In work, consider neural end-to-end SLU models produce semantics intents slots audio. A primary motivation work arises deployment SLU systems devices resource limited cloud servers. For devices, neural E2E SLU model customized given resource constraints satisfy limited set intents use cases , deployed. Moving SLU computation cloud devices allows offline use, e.g. cars emergency situations latency gains placing computations closer user cost carbon savings reduced fleet sizes reducing communication payloads. E2E SLU provides alternative paradigm conventional approach compressing individual components ASR NLU systems satisfy on-device resource constraints. In latter approach, NLU subsystem exposed audio information prosody, ambiguity ASR decoding beyond n-best hypotheses. Errors ASR system cascade NLU tasks. Our approach developing E2E SLU systems leverages models developed ASR NLU communities replacing text interface neural network hidden interface layer. We term interpretable subclass E2E SLU models Joint SLU models produce intermediate transcript well NLU annotations. We show NLU metrics improve exposure richer interface also ASR metrics improve NLU feedback joint training. The multitask training models make use datasets transcribed audio well audio NLU annotations. \end{table} \subsection{Prior work} A prior works considered E2E SLU problem. In work Google , authors first develop problem note intermediate text representation improves performance. They consider encoder-decoder sequence networks predict transcript serialized form semantics multitask model decoders separate, two-stage model transcript obtained first, joint model single decoder predicts jointly. In contrast, formalize distinctions ASR NLU subsystems study impact end-to-end training, interfaces text hidden layers two. In , CTC based network used extract named entities French speech use attention based networks train larger corpora. Finally, works obtain single label directly speech segments. Our work goes beyond performs slot filling well. % TODO: detailed description E2E ASR systems needed? E2E SLU models similar multitask speech systems speech translation systems multilingual ASR systems. This work enabled advancements E2E ASR, systems shown outperform conventional RNN-HMM hybrid ASR systems trained large acoustic datasets. Connectionist Temporal Classification networks first neural E2E ASR model trained Recurrent Neural Network audio input features transcript label sequence different length considering possible alignments inputs labels. In Recurrent Neural Network-Transducer , authors extend CTC also modelling interdependencies input-output output-output distributions using added prediction network. In cases, efficient forward-backward computation enables loss computation backpropagation alignments. In contrast aforementioned streaming architectures, attention based sequence-to-sequence networks Listen Attend Spell , input features processed encoder networks produce hidden representation output feature. The decoder estimates element label sequence step using attention network focus fraction encoder network outputs. Extracting intent slots transcript long running problem NLU . In survey , authors compare DNN earlier feature engineering approach coupled conditional random fields softmax layers purpose named entity recognition. The interface ASR NLU systems traditionally best hypothesis sequence although richer interfaces lattices word confusion networks also well studied . In work, develop simple joint intent slot prediction network study impact text vs hidden layer interfaces ASR NLU. \subsection{Contributions} In Sec.\ , first present low-resource streaming model extracts utterance intent directly speech without intermediate text output. We present compositional model similar non-streaming pipelined two-stage SLU architecture, LAS based ASR system produces transcript consumed independently trained Neural NLU system. Finally, present aforementioned E2E differentiable Joint SLU models interface ASR NLU shared hidden layer. We restrict 1-best interfaces ASR NLU leave n-best hidden layer interfaces future work. We present experimental results, baselines, metrics variety datasets Sec.\ answer following questions: We proposed novel task dialogue state induction, automatically identify dialogue state slots values large set dialogue records. Compared existing research, task practically useful handling large variety services available industry, disallows scalable manual labeling dialogue states. We built two neural generative models latent variables. Results standard DST datasets show models effectively induce meaningful dialogue states raw dialogue data, improve results dialogue system compared without using dialogue states. Our methods serve baselines research task. We release code models . } ijcai20.tex \typeout{IJCAI--PRICAI--20 Instructions Authors} These instructions authors IJCAI-20. \documentclass{article} \pdfpagewidth=8.5in \pdfpageheight=11in The file ijcai20.sty NOT previous years' \usepackage{ijcai20} Use postscript times font! \usepackage{times} \usepackage{soul} \usepackage{url} \usepackage[hidelinks]{hyperref} \usepackage[utf8]{inputenc} \usepackage[small]{caption} \usepackage{graphicx} \usepackage{multirow} \usepackage{subfigure} \usepackage{amsmath} \usepackage{amsthm} \usepackage{booktabs} \usepackage{algorithm} \usepackage{algorithmic} \usepackage{amsfonts} \usepackage{color} \urlstyle{same} \newcommand{\newcite}[1]{\citeauthor{#1}} following package optional: \usepackage{latexsym} See https://www.overleaf.com/learn/latex/theorems_and_proofs nice explanation define new theorems, keep mind amsthm package already included template must *not* alter styling. \newtheorem{example}{Example} \newtheorem{theorem}{Theorem} Following comment ijcai97-submit.tex: The preparation files supported Schlumberger Palo Alto Research, AT\&T Bell Laboratories, Morgan Kaufmann Publishers. Shirley Jowell, Morgan Kaufmann Publishers, Peter F. Patel-Schneider, AT\&T Bell Laboratories collaborated preparation. These instructions modified used conferences long credit authors supporting agencies retained, notice changed, modification reuse restricted. Neither Shirley Jowell Peter F. Patel-Schneider listed contacts providing assistance without prior permission. To use conferences, change references files conference appropriate use authors, contacts, publishers, organizations. Also change deadline address returning papers length page charge instructions. Put files available appropriate places. \title{Dialogue State Induction Using Neural Latent Variable Models} Multiple author syntax Check ijcai20-multiauthor.tex file detailed instructions \author{ Qingkai Min \and Libo Qin\and Zhiyang Teng\and Xiao Liu\And Yue Zhang \affiliations School Engineering, Westlake University\\ Institute Advanced Technology, Westlake Institute Advanced Study\\ Research Center Social Computing Information Retrieval, Harbin Institute Technology\\ School Computer Science Technology, Beijing Institute Technology \emails \{minqingkai,tengzhiyang,zhangyue\}@westlake.edu.cn, lbqin@ir.hit.edu.cn, xiaoliu@bit.edu.cn } \begin{document} \maketitle"," We consider the problem of spoken language understanding  of extracting natural language intents and associated slot arguments or named entities from speech that is primarily directed at voice assistants. Such a system subsumes both automatic speech recognition  as well as natural language understanding . An end-to-end joint SLU model can be built to a required specification opening up the opportunity to deploy on hardware constrained scenarios like devices enabling voice assistants to work offline, in a privacy preserving manner, whilst also reducing server costs.   % removed word ""composed"" from joint system description We first present models that extract utterance intent directly from speech without intermediate text output. We then present a compositional model, which generates the transcript using the Listen Attend Spell ASR system and then extracts interpretation using a neural NLU model. Finally, we contrast these methods to a jointly trained end-to-end joint SLU model, consisting of ASR and NLU subsystems which are connected by a neural network based interface instead of text, that produces transcripts as well as NLU interpretation. We show that the jointly trained model shows improvements to ASR incorporating semantic information from NLU and also improves NLU by exposing it to ASR confusion encoded in the hidden layer."
"Modularized task-oriented dialogues systems core current smart speaker generation . The main modules systems Natural Language Understanding , Dialogue State Tracking , Dialogue Policy Natural Language Generation , trained separately using supervised and/or reinforcement learning. Thus data collection process required, tasks laborious expensive. For example, dialogue policy annotation done expert, better professional linguist. Therefore, model requires samples actually perform well tasks essential. The successful approach few-shot learning task-oriented dialogue systems notably transfer learning, large model firstly pre-trained large corpus fine-tuned specific tasks. For task-oriented dialogue systems, ~\citet{wu2020tod} proposed \href{https://github.com/jasonwu0731/ToD-BERT}{TOD-BERT} large pre-trained model achieve better performance BERT few-shots NLU, DST DP. \citet{liu2020coach} proposed two-step classification few-shot slot-filling, key task NLU module. Similarly, \citet{peng2020few} introduced benchmark few-shot NLG pre-trained language model specialized task. Further, template rewriting schema based T5 developed \citet{kale2020few} few-shot NLG two well-known datasets. \citet{peng2020soloist} proposed pre-trained language model end-to-end pipe-lined task-oriented dialogue systems. In experiments, showed promising few-shot learning performance MWoZ. Finally, several meta-learning approaches proposed DP, NLG/ACT, pipelined end-to-end models personalized dialogue systems. For performing few-shot learning, existing methods require set task-specific parameters since model fine-tuned samples. Differently, paper, perform few-shot learning priming LMs few-examples. In setting, parameters updated, thus allowing single model perform multiple tasks time. In paper, evaluate few-shot ability LM priming four task-oriented tasks previously mentioned . Currently, GPT-3 available public; thus experiment different sizes GPT-2 models SMALL , LARGE , XL . All experiments run single NVIDIA 1080Ti GPU. We developed models problem spoken language understanding extracting natural language intent named-entities slots directly speech. Such end-to-end model customized deployed resource constrained device enabling new offline privacy focussed use cases. We first developed audio intent model small footprint. We developed compositional model pretrained LAS ASR model whose outputs, transcription audio, fed pre-trained neural NLU model. Finally, end-to-end fully differentiable, interpretable Joint SLU model presented NLU system consumes transcript output LAS system neural network interface encodes ASR word confusion. These models trained multiple datasets affirmed following points: intent classification performance improves ASR outputs also produced replacing text wordpiece interface compositional ASR NLU systems neural network hidden joint training leads 2.7+\ relative improvement NLU metrics intents, slots mitigates downstream impact ASR errors joint training reduces ASR WER 3.8\ backpropagation NLU losses ASR layers."," Task-oriented dialogue systems use four connected modules, namely, Natural Language Understanding , a Dialogue State Tracking , Dialogue Policy  and Natural Language Generation . A research challenge is to learn each module with the least amount of samples  given the high cost related to the data collection. The most common and effective technique to solve this problem is transfer learning, where large language models, either pre-trained on text or task-specific data, are fine-tuned on the few samples. These methods require fine-tuning steps and a set of parameters for each task. Differently, language models, such as GPT-2 and GPT-3, allow few-shot learning by priming the model with few examples. In this paper, we evaluate the priming few-shot ability of language models in the NLU, DST, DP and NLG tasks. Importantly, we highlight the current limitations of this approach, and we discuss the possible implication to future work."
"Internet applications widely advancing towards social networks user generated content plays crucial role . This content mostly generated users, companies, news reporting agencies etc., consists text, image video noise naturally . This media forms foundation social media delivery systems Facebook, Youtube, Instagram, Twitter, etc. different views media orientations. Some social networks tend online social TV Youtube. Content networks generated video format users posted channels communication users possible commenting videos. Instagram started image short video sharing social network communication method implemented. Facebook another big social network, restrict users post specific media types also allows users directly communicate using direct chat recently available Instagram too. Among popular social media delivery systems, Twitter gained much attention recent years due simplicity user friendly interface. It also known best social media discovering ``what happening right now''. Twitter easily accessible almost device connected Internet web browser third-party/official application. The nature twitter short posts known tweets restrictions size first supposed gentle replacement SMS . In words, twitter best social media discovering news real world events . Reports show daily basis, 500 million new tweets monthly basis, 300 new user sign-ups happened twitter 2013; In 2018, estimated 700 million tweets posted day growth rate number tweets per year around 32\% 2013 . These statistics show users tend produce large volumes tweets variety types high velocity diverse veracity different values; Mentioned characteristics twitter streaming media takes account social big data respect 5'V model . Twitter rich data form tweets accessed using Twitter API. This API made researchers capable investigating social big data streaming way query based data retrieval fashion . Documentation API available twitter developer website. This API allows direct messaging, search, advertisement management, account activity control. It restriction prevent developers abusing service. For example, rate limit user application. This open free dataset used many researchers. Many advancements conducted multiple research areas related social big data opinion mining, topic detection tracking, user modeling, sentiment analysis, etc. But underlying conceptual perspective researcher problem makes different outcomes diverse real world applications. If data supposed treated streaming social big data problem would likely solved unsupervised semi-supervised methodology. Some researches try solve problem respect static corpus others tend streaming big social data respect . On hand, many previous works assumed correctness incoming data granted true many cases. For example, case analyzing tweets related regular users, many noisy content mistypes grammar errors exist case official tweets like news agency tweet, problem ignored. The missing part analysis related images videos tweets. To best knowledge, existing methods use multimodal data detect track topics. To overcome mentioned problems, present novel approach topic detection tracking problem. Our work consists three major parts: multimodal named entity recognizer, memory graph, Transformer based semantic similarity assignment. The combination three parts provides final results. Orientation presented study follows: Section covers related works problem hand. Section presents proposed methodology detecting tracking topics. The experimental results Section investigates performance work three distinct datasets. Finally, Section concludes whole paper. This paper proposes comprehensive framework modeling query intent search systems different product components. The proposed deep learning based models proven effective efficient online search applications. Discussions challenges deploying models production well insights making decisions provided. We hope framework well experiences journey could useful readers designing real-world query understanding text classification tasks. The next two lines define bibliography style used, bibliography file. \endinput"," Real time nature of social networks with bursty short messages and their respective large data scale spread among vast variety of topics are research interest of many researchers. These properties of social networks which are known as 5'Vs of big data has led to many unique and enlightenment algorithms and techniques applied to large social networking datasets and data streams. Many of these researches are based on detection and tracking of hot topics and trending social media events that help revealing many unanswered questions. These algorithms and in some cases software products mostly rely on the nature of the language itself. Although, other techniques such as unsupervised data mining methods are language independent but many requirements for a comprehensive solution are not met. Many research issues such as noisy sentences that adverse grammar and new online user invented words are challenging maintenance of a good social network topic detection and tracking methodology; The semantic relationship between words and in most cases, synonyms are also ignored by many of these researches. In this research, we use Transformers combined with an incremental community detection algorithm. Transformer in one hand, provides the semantic relation between words in different contexts. On the other hand, the proposed graph mining technique enhances the resulting topics with aid of simple structural rules. Named entity recognition from multimodal data, image and text, labels the named entities with entity type and the extracted topics are tuned using them. All operations of proposed system has been applied with big social data perspective under NoSQL technologies. In order to present a working and systematic solution, we combined MongoDB with Neo4j as two major database systems of our work. The proposed system shows higher precision and recall compared to other methods in three different datasets."
"A dialog system correctly understand speakers utterances respond natural language. Dialog act recognition sentiment classification two correlative tasks realize former. The goal DAR attach semantic labels utterance dialog identify underlying intentions . Meanwhile, sentiment classification detect sentiments implicated utterances help capture speakers intentions . In current research, used Transformers combined graph techniques provide topic detection system social media. Fine-tuning transformers realtime obtaining valuable semantic information rather using frequencies another novelty work. Our proposed model combined various modules including BERT, graph strategies multimodal named entity recognizer. This combination unique methodology called memory graph uses cognitive memorization human brain. Using hyper-parameters rate forgetting makes system available stream size machine running. Modularity work makes usable real life corporate based environments dealing big social data.The results shows superiority proposed method compared state art techniques. \caption{Twitter Topic/Event detection/tracking related studies}","         In dialog system, dialog act recognition and sentiment classification are two correlative tasks to capture speakers intentions, where dialog act and sentiment can indicate the explicit and the implicit intentions separately  \cite{kim2018integrated}. Most of the existing systems either treat them as separate tasks or just jointly model the two tasks by sharing parameters in an implicit way without explicitly  modeling mutual interaction and relation. To address this problem, we propose a Deep Co-Interactive Relation Network  to explicitly consider the cross-impact and model the interaction between the two tasks by introducing a co-interactive relation layer. In addition, the proposed relation layer can be stacked to gradually capture mutual knowledge with multiple steps of interaction. Especially, we thoroughly study different relation layers and their effects. Experimental results on two public datasets  show that  our model outperforms the state-of-the-art joint model by 4.3\% and 3.4\% in terms of F1 score on dialog act recognition task, 5.7\% and 12.4\% on sentiment classification respectively. Comprehensive analysis empirically verifies the effectiveness of explicitly modeling the relation between the two tasks and the multi-steps interaction mechanism. Finally, we employ the Bidirectional Encoder  Representation from Transformer  in our framework, which can further boost our performance in both tasks."
"Neural modeling approaches prominent research task-oriented open-domain dialog. Traditional sequence-to-sequence models used encoding dialog history predicting domains, intents, slot types, spans generally decoding full-fledged system responses. In recent years, large pre-trained Transformer-based models natural language understanding natural language generation become ubiquitous, leading tremendous advances fine-tuning towards dialog tasks. % In task-oriented speech-based dialog systems, effect ASR hypotheses widely studied techniques devised minimize resulting downstream NLU errors. More recently, end-to-end spoken language understanding approaches attempted sidestep problem. % There work address issues, using n-best lists. %It clear models would serve speech modality, ASR output contains recognition errors, structural information sentence boundaries punctuation. On hand, research open-domain dialog increasingly focusing large, monolithic end-to-end neural models like Google's Meena built using written data evaluated written interactions. Several written textual datasets created recently tackle various problems open-domain dialog, including persona-grounding, knowledge-grounding reasoning, state-of-the-art chatbots built using them. However, clear whether written text-based open-domain chatbots would seamlessly integrate ASR models serve speech modality, popular due ubiquity voice assistants like Alexa, Google Assistant Siri. Collecting large-scale written text-based dialog datasets cheaper practical collecting audio-based dialog datasets many ways. But speech-robustness factor consideration designing dialog system intended deployed speech modality, even absence audio-based training data. To bring attention important aspect open-domain dialog community, empirically study effects various types synthetic actual ASR hypotheses dialog history TransferTransfo , state-of-the-art neural open-domain dialog system based Generative Pre-trained Transformer NeurIPS ConvAI2 Conversational Intelligence Challenge. We build Topical-Chat dataset perform two augmentations study: one creating simulated ASR hypotheses entire dataset, another creating actual ASR hypotheses smaller audio-based analogue Topical-Chat test sets. We observe TF2 trained written textual data sensitive synthetic actual ASR hypotheses introduced dialog history inference time, sensitivity particularly prominent task response selection. As baseline mitigation strategy, introduce synthetic ASR hypotheses dialog history training observe marginal improvements, demonstrating need research techniques make end-to-end open-domain chatbots fully speech-robust. Figure shows sample snippet responses TF2 models trained written text synthetic ASR hypotheses fed speech-distorted dialog history. A close work spirit is, shows Transformer-based generative dialog models insensitive unrealistic perturbations like token-shuffling dialog history. Our work focused evaluating effects introducing realistic perturbations dialog history form synthetic actual ASR hypotheses. Our augmentation Topical-Chat, dubbed Topical-Chat ASR dataset, open-sourced\footnote{https://github.com/alexa/Topical-Chat/tree/master/TopicalChatASR/} enable open-domain dialog researchers perform speech-robustness evaluation fuel research novel techniques make monolithic neural open-domain dialog models speech-robust. % report impact automated metrics perplexity, unigram F1, recall diversity. In paper, propose sentiment-based stock index prediction system contains sentiment extractor distills polarity news articles towards market summarizer sums overall sentiment week predict index change next week. We propose discrete word feature called Polarity-Over-Time captures sentiment changes words according certain events different periods time. Both POT feature multi-task learning used improve performance sentiment extractor. We show model 10-year Reuters news dataset achieves considerable improvements compared baselines. In particular, demonstrate weekly-Monday framework provides space market react sentiment signals therefore, appropriate sentiment-based stock prediction. In future, explore following directions: The adaptation model daily price prediction. We explore adapt model predict daily price variations stronger denoising methods may applied sentiment extraction. The merge sentiment extractor summarizer one integrated neutral architecture BERT hierarchical CNN parameters jointly learned layers instead probability scores fed summarizer.","   Large end-to-end neural open-domain chatbots are becoming increasingly popular. However, research on building such chatbots has typically assumed that the user input is written in nature and it is not clear whether these chatbots would seamlessly integrate with automatic speech recognition  models to serve the speech modality. We aim to bring attention to this important question by empirically studying the effects of various types of synthetic and actual ASR hypotheses in the dialog history on TransferTransfo, a state-of-the-art Generative Pre-trained Transformer  based neural open-domain dialog system from the NeurIPS ConvAI2 challenge. We observe that TransferTransfo trained on written data is very sensitive to such hypotheses introduced to the dialog history during inference time.   %with the sensitivity being particularly prominent for the task of response selection.   As a baseline mitigation strategy, we introduce synthetic ASR hypotheses to the dialog history during training and observe marginal improvements, demonstrating the need for further research into techniques to make end-to-end open-domain chatbots fully speech-robust. To the best of our knowledge, this is the first study to evaluate the effects of synthetic and actual ASR hypotheses on a state-of-the-art neural open-domain dialog system and we hope it promotes speech-robustness as an evaluation criterion in open-domain dialog."
"Knowledge Graph consists facts form triplet using corresponding relation tail embeddings, calculate plausibility triplet measuring difference original reconstructed embeddings. These works either model relationship explainable way , utilize black-box expressive convolution operations . Another strand works considers link prediction semantic matching problem. They take embeddings head, relation tail input, output matching score elements triplet using bi-linear transformation , convolution etc. These works vary lot situations suitable for, like relation types KG, sparsity KG, etc. Therefore, choosing suitable architecture specific KG often requires careful analysis dataset model. To tackle issue, \citet{zhang2019autosf} proposes use AutoML greedily search optimal score functions distinct KGs. %To tackle issue, \citet{zhang2019autosf} proposes use AutoML find optimal structure specific KG. However, work constrained bilinear semantic matching models include reconstruction-based models search space. In paper, propose novel Neural Architecture Search framework search effective architecture given dataset. The framework entails general search space contains semantic matching models reconstructive models. Therefore, potential combine strength two model families. Instead searching discrete set candidate architectures, relax search space continuous, architecture optimized using efficient gradient-based search algorithm. As shown Fig., NAS framework contains two search modules. The representation search module aims refine embeddings head, relation, tail respectively multiple representation layers. The score function search module responsible selecting shallow architecture calculate plausibility score input triplet. While operators module broad range choices, work, primarily focus proof-of-concept two-level search space constraining operators architectures representatives existing link prediction models. Specifically, constrain search space representation search module reconstructive models, whose input output good compatibility module. As score function search module, select representative models mainstream model families link prediction task. To avoid overfitting, also add identity operation representation search module NAS algorithm could choose use original , even degenerate basic models score function search module necessary. On one hand, consider representation search module refines black-box way. On hand, output representation search module may also embed constraints modeled reconstruction-based models. Therefore, final score could potentially benefit cross-validation multiple models, likely lead better prediction results. We evaluate approach several popular benchmark datasets. Extensive experiments demonstrate approach good generalization ability different datasets, achieves better performance strong baseline models datasets. %% %% This file `sample-sigconf.tex', %% generated docstrip utility. %% %% The original source files were: %% %% samples.dtx %% %% IMPORTANT NOTICE: %% %% For copyright see source file. %% %% Any modified versions file must renamed %% new filenames distinct sample-sigconf.tex. %% %% For distribution original source see terms %% copying modification file samples.dtx. %% %% This generated file may distributed long %% original source files, listed above, part %% distribution. %% %% The first command LaTeX source must \documentclass command. \documentclass[sigconf]{acmart} %%%% As March 2017, [siggraph] longer used. Please use sigconf SIGGRAPH conferences. %%%% As May 2020, [sigchi] [sigchi-a] longer used. Please use sigconf SIGCHI conferences. %%%% Proceedings format SIGPLAN conferences % \documentclass[sigplan, anonymous, review]{acmart} %%%% Proceedings format conferences using one-column small layout % \documentclass[acmsmall,review]{acmart} %% %% \BibTeX command typeset BibTeX logo docs \AtBeginDocument{% \providecommand\BibTeX{{% \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}} %% Rights management information. This information sent %% complete rights form. These commands SAMPLE %% values them; responsibility author replace %% commands values provided %% complete rights form. \copyrightyear{2020} \acmYear{2020} \setcopyright{acmcopyright}\acmConference[CIKM '20]{Proceedings 29th ACM International Conference Information Knowledge Management}{October 19--23, 2020}{Virtual Event, Ireland} \acmBooktitle{Proceedings 29th ACM International Conference Information Knowledge Management , October 19--23, 2020, Virtual Event, Ireland} \acmPrice{15.00} \acmDOI{10.1145/3340531.3412104} \acmISBN{978-1-4503-6859-9/20/10} %% These commands PROCEEDINGS abstract paper. %\acmConference[Woodstock '18]{Woodstock '18: ACM Symposium Neural Gaze Detection}{June 03--05, 2018}{Woodstock, NY} %\acmBooktitle{Woodstock '18: ACM Symposium Neural Gaze Detection, June 03--05, 2018, Woodstock, NY} %\acmPrice{15.00} %\acmISBN{978-1-4503-XXXX-X/18/06} %% %% Submission ID. %% Use submitting article sponsored event. You'll %% receive unique submission ID organizers %% event, ID used parameter command. %%\acmSubmissionID{123-A56-BU3} %% %% The majority ACM publications use numbered citations %% references. The command \citestyle{authoryear} switches %% ""author year"" style. %% %% If preparing content event %% sponsored ACM SIGGRAPH, must use ""author year"" style %% citations references. %% Uncommenting %% next command enable style. %%\citestyle{acmauthoryear} %% %% end preamble, start body document source. \usepackage{microtype} \usepackage{url} \usepackage{mathrsfs} \usepackage[linesnumbered,ruled,vlined]{algorithm2e} \usepackage{multirow} \usepackage{graphicx} \usepackage{float} \usepackage{subfigure} \usepackage{color} \usepackage{enumerate} \usepackage{booktabs} \usepackage[normalem]{ulem} \usepackage[toc,page,title]{appendix} \newcommand{\tabincell}[2]{ } \usepackage{xcolor} \newcommand{\todo}[1]{\textcolor{red}{TODO: #1}\PackageWarning{TODO:}{#1!}} \usepackage{xspace} \newcommand{\cparagraph}[1]{#1} \ccsdesc[500]{Computing methodologies~Knowledge representation reasoning} %% %% Keywords. The author pick words accurately describe %% work presented. Separate keywords commas. \keywords{knowledge graph, kg embedding, neural architecture search} %% %% This command processes author affiliation title %% information builds first part formatted document. We empirically studied effects synthetic actual ASR hypotheses dialog history TF2, large state-of-the-art text-based neural open-domain dialog system NeurIPS ConvAI2 challenge. We observed TF2 trained written data sensitive hypotheses introduced dialog history inference time, demonstrating text-based neural open-domain chatbots may effective serving speech modality as-is. We observed training TF2 synthetic ASR hypotheses makes robust synthetic actual ASR hypotheses inference time, considerable room improvement left future work. Our augmentation Topical-Chat, dubbed Topical-Chat ASR dataset, open-sourced hope work sparks discussion research modality-centric modality-agnostic open-domain dialog systems."," Link prediction is the task of predicting missing connections between entities in the knowledge graph .  While various forms of models are proposed for the link prediction task, most of them are designed based on a few known relation patterns in several well-known datasets. Due to the diversity and complexity nature of the real-world KGs, it is inherently difficult to design a model that fits all datasets well. To address this issue, previous work has tried to use Automated Machine Learning  to search for the best model for a given dataset. However, their search space is limited only to bilinear model families. In this paper, we propose a novel Neural Architecture Search  framework for the link prediction task.  First, the embeddings of the input triplet are refined by the Representation Search Module. Then, the prediction score is searched within the Score Function Search Module. This framework entails a more general search space, which enables us to take advantage of several mainstream model families, and thus it can potentially achieve better performance.  We relax the search space to be continuous so that the architecture can be optimized efficiently using gradient-based search strategies. Experimental results on several benchmark datasets demonstrate the effectiveness of our method compared with several state-of-the-art approaches."
"The capacity neural network influences ability model complex functions. In particular, argued deeper models conducive expressive features . Very deep neural network models proved successful computer vision text classification . In neural machine translation , however, current state-of-the-art models Transformer typically employ 6-12 layers . Previous work shown difficult train deep Transformers, 12 layers . This due optimization challenges: variance output layer compounds get deeper, leading unstable gradients ultimately diverged training runs. In empirical study, re-investigate whether deeper Transformer models useful NMT. We apply recent initialization technique called ADMIN , remedies variance problem. This enables us train Transformers significantly deeper, e.g. 60 encoder layers 12 decoder layers.\footnote{We choose focus layer size since results maximum model size fit within single GPU system. The purpose study show feasible researchers experiment deep models; access massive GPU budgets requirement.} In contrast previous research, show indeed feasible train standard\footnote{Note architectural variants enable deeper models , discussed Sec . We focus standard architecture here.} Transformer many layers. These deep models significantly outperform 6-layer baseline, 2.5 BLEU improvement. Further, obtain state-of-the-art WMT'14 EN-FR WMT'14 EN-DE benchmarks.% !TEX encoding = UTF-8 % !TEX Root = Main.tex In paper, propose novel NAS framework link prediction task, combine strength reconstruction based semantic matching based models. Experimental results show NASE outperforms several state-of-the-art human-designed models AutoML based models datasets. In future work, would like explore possibility general search spaces include strong architectures.","  We explore the application of very deep Transformer models for Neural Machine Translation .  Using a simple yet effective initialization technique that stabilizes training, we show that it is feasible to build standard Transformer-based models with up to 60 encoder layers and 12 decoder layers.  These deep models outperform their baseline 6-layer counterparts by as much as 2.5 BLEU, and achieve new state-of-the-art benchmark results on  WMT14 English-French  and WMT14 English-German . To facilitate further research in Very Deep Transformers for NMT, we release the code and models: \url{https://github.com/namisan/exdeep-nmt}."
"Non-autoregressive transformer~ attracted wide attention neural machine translation~, generates sentences simultaneously rather sequentially. To enable parallel decoding, NAT imposes conditional independence assumption among words output sentences, leads significantly faster inference speed~ autoregressive Transformer~. However, NAT still falls behind autoregressive Transformer~ quality output sentences, BLEU~ machine translation. %\zhouh{move BLEU citations} We blame imposed conditional independence assumption, prevents NAT models explicitly learning word dependencies output sentence. Note word dependency crucial, explicitly learned AT model autoregressive language models~. Recently, \citet{mask_predict,levT} propose employ Masked Language Model~ NAT, includes word dependency modeling iterative fashion~, therefore yielding quite competitive results compared AT. Specifically, iterative models randomly mask words reference predict masked words conditioned unmasked ones training. In manner, iterative models trained explicitly capture dependencies masked words unmasked words. However, iterative approaches still produce poor results one decoding iteration perform multiple iterations inference, namely iteratively refining generated outputs previous iteration. Such iterative process quite time-consuming, partly sacrifices speed merit NAT. To date, remains open question iterative process abandoned, still preserving benefits explicitly modeling word dependencies NAT. In paper, argue major culprit problem mask language models used together iterative inference, sampling strategy masking words MLM. In particular, MLM employs fixed uniform strategy randomly masking words training, prevents model effectively learning word dependencies one-iteration generation. For example, beginning training NAT model still poorly tuned, mask fewer words. If not, would difficult NAT model correctly predict masked words. % In worst case, NAT model may stuck local minimums training, prevents NAT model well fitting training data. On contrary, mask little words end phase training, resulting NAT model rarely trained predict whole sentences, predict sentence fragments. In case, accurately generate whole sentence inference, NAT model generate sentence fragments iteratively. To end, sampling strategy crucial training NAT. To address issues, propose simple yet effective approach called Glancing Transformer~, equipped proposed Glancing Language Model~ non-iterative parallel text generation, achieving significant improvements upon strong baselines. Intuitively, GLM adopts adaptive glancing sampling strategy, glances fragments reference reference difficult fit training NAT. Correspondingly, model well tuned, adaptively reduce percentage glancing sampling, making sure resulting model could learn generate whole sentence one-iteration fashion. Specifically, proposed GLM differs MLM two aspects. Firstly, GLM proposes adaptive glancing sampling strategy, enables \method generate sentences one-iteration way, working gradual training instead iterative inference~. % Intuitively, GLM works like way Generally, GLM quite similar curriculum learning~ spirit, namely first learning generate fragments gradually moving learn whole sentences~. To achieve adaptive glancing sampling, GLM performs decoding twice training. The first decoding vanilla NAT, prediction accuracy indicates whether current reference ``difficult'' fitting. In second decoding, GLM gets words reference via glancing sampling according first decoding, learn predict remaining words sampled. Note second decoding update model parameters. Secondly, instead using token, GLM directly use representations encoder corresponding positions, natural could enhance interactions sampled words signals encoder. Experimental results show \method obtains significant improvements standard benchmarks compared vanilla NAT, without losing inference speed-up. \method achieves competitive results iterative approaches like Mask-Predict~, even outperforming Mask-Predict model WMT14 DE-EN WMT16 RO-EN. % Considering fully NAT models one-iteration simultaneous generation, \method achieves best BLEU scores. Compared strong AT baseline, \method still close performance gap within 1 BLEU point keeping speed-up. Empirically, find \method outperforms AT source input length less 20 WMT14 DE-EN. We speculate GLM could capture bidirectional context left-to-right LM unidirectional, indicates potential parallel generation models. We show feasible train Transformers depth previously believed difficult. Using ADMIN initialization, build Transformer-based models 60 encoder layers 12 decoder layers. On WMT'14 EN-FR WMT'14 EN-EN, deep models outperform conventional 6-layer Transformers 2.5 BLEU, obtain state-of-the-art results. We believe ability train deep models may open new avenues research NMT, including: Training extremely large noisy data, e.g. back-translation adversarial training , see exploited larger model capacity. Analyzing internal representations, see deeper networks indeed extract higher-level features syntax semantics . Compressing deep model via e.g. knowledge distillation , study trade-offs size translation quality. Analyzing deep models work theory."," %Non-autoregressive models generate all the tokens of the sequence in parallel.  Although non-autoregressive models with one-iteration generation achieve remarkable inference speed-up, they still fall behind their autoregressive counterparts in prediction accuracy. The non-autoregressive models with the best accuracy currently rely on multiple decoding iterations, which largely sacrifice the inference speed of non-autoregressive models.  Inspired by the way of learning word dependencies in autoregressive and iterative-decoding models, we propose Glancing Transformer~ with a glancing language model~, which learns to capture the word dependency gradually. Experiments on three benchmarks demonstrate that our approach can significantly improve the accuracy of non-autoregressive models without multiple decoding iterations. In particular, \method achieves state-of-the-art results among non-iterative models and even outperforms top iterative counterparts in some specific benchmarks."
"Neural NLP models typically embed sequence input tokens using lookup table learnable parameters, row represents token type dense vector . The embedding matrix often reused predict output language models . How essential embeddings model's success? Intuitively, one would expect critical, given ubiquitous use embeddings layers vast amount parameters typically consume. In work, show machine translation models trained without embedding parameters, rival sometimes even outperform standard embedding-based models. % Does play essential role models ability preform well do? In work put test removing trainable embedding matrix Neural Machine Translation models instead use fixed one hot encoding vocabulary. % Embedding widely used component modern NLP models, usually takes form learnable dense matrix, , vocabulary, hidden dimension, makes parameter consuming layer models architectures. Does play essential rule models ability preform well do? In work put test removing trainable embedding matrix Neural Machine Translation models instead use fixed one hot encoding vocabulary. We remove trainable embedding matrix standard transformer machine translation model, use constant one-hot encoding vocabulary instead. To limit dimensionality, use byte tokenization reading text unicode byte stream, represent virtually every text language 256 dimensions per token. Byte vocabularies obviate need preprocess text hand-crafted language-specific tokenizers subword induction algorithms, BPE . Machine translation experiments 10 language pairs show models without trainable embedding matrix perform par best embedding-based baselines. We find embeddingless models consistently achieve higher BLEU scores byte baselines, even yield slightly better performance embedding-based character models 80\% cases. % We observe similar yet weaker trend comparing character level models, embeddingless gets higher BLEU score 16 20 cases, smaller average gap. Although recent literature character-based transformers demonstrates superiority subword tokenization controlling network depth , experiments show removing embedding matrix byte-to-byte models makes perform least well standard subword models 9 20 cases. Overall, results suggest highly-parameterized embedding matrices might essential commonly perceived. % Interestingly, despite existing work MT showing sub word models consistently outperforming character byte level ones , Embeddingless models able get equal higher results subword models 9 20 cases despite significantly smaller number parameters. % Machine translation experiments 10 language pairs show models without trainable embedding matrix comparative best embedding based baseline. Moreover, We find embeddingless models achieve higher scores byte character level baselines 19 16 cases 20 respectively. Interestingly, despite existing work MT showing sub word models consistently outperforming character byte level ones , Embeddingless models able get equal higher scores sub word models 9 20 cases despite significantly smaller number parameters. % outperform byte baselines, achieve higher BLEU score 19 20 cases equal score ru-en. We observe similar weaker trend comparing character level models, embeddingless gets higher BLEU score 16 20 cases, smaller average gap. % \uri{new version - end} % Machine translation experiments 10 language pairs show removing embedding matrix consistently improves performance byte-to-byte models. % When translating English foreign language, embeddingless byte-to-byte models outperform character-based models 9 10 languages, subword models 7 10 languages. % This last result particularly surprising given recent literature machine translation, demonstrates consistent superiority subword models character models size . \omer{we need make last sentence bit subtle...} % \uri{Maybe: This last result particularly surprising given recent literature machine translation, shows sub word models consistently achieve higher BLEU scores character models size? } % However, observe subword models consistently better translating English. % % \uri{However, observe subword models able achieve higher BLEU scores translating English. ?} % \omer{We need explain this... maybe also blame BLEU sense? We're using ""better"" ""outperform"" quite bit here, diffs really huge, BLEU also super-dependent tokenizer. Maybe want soften whole tone paper say something like ""the models competitive"", ""very similar perfomance slight edge X""?} % \uri{I agree} % \omer{we need end positive note...} % \uri{the following old version last sentence bit crooked built different way. It doesnt say: ""we bad translating English"", explain end positive tune. Instead says ""we good conditions also subwords"": % While recent work machine translation demonstrates character byte level models often underperform sub word models , observe embeddingless models close gap even surpass sub word models cases, particularly tokenizer post-processes sub words model output optimized target language. % } %While recent work machine translation demonstrates character byte level models often underperform sub word models , observe embeddingless models close gap even surpass sub word models cases, particularly tokenizer post-processes sub words model output optimized target language. % \subsection{Old introduction} % Tokenization task segmenting string sequence tokens model process. % The current standard practice split string subwords using whitespaces, language-specific heuristics \uri{maybe also add list :https://arxiv.org/pdf/2008.05055.pdf}, Byte-Pair Encoding variants . % While subword tokenization works well practice many languages, also loses orthographic information may critical morphologically rich languages. % Moreover, assumption subword units must contiguous segments hold languages non-concatenative morphology Arabic Hebrew. % Intuitively, sufficiently parameterized models able leverage pure character tokenization access information original string; however, recent work machine translation demonstrates character-level models often underperform subword models practice . % We hypothesize part reason character-based models underperform embedding layer. This layer represents vocabulary item vector, implicitly capturing similarity function token types. % While similarity words subwords potentially useful, quite clear character similarity could benefit model. % We conjecture that, many languages, embedding layer allows character-based models learn misleading concept character similarity, propose remove it. % In work omit embedding matrix character-to-character machine translation models, using orthogonal one-hot representations instead. % Since languages thousands characters, use byte representations based UTF-8, represent information language 256 dimensions per token. % Machine translation experiments 10 language pairs show removing embedding matrix consistently improves performance byte-to-byte models. When translating English foreign language, embeddingless byte-to-byte models outperform character-based models 9 10 languages, subword-based models 7 10 languages \omer{one equal...} \uri{we win BPE in: zh,ar,ru,ja,tr,fa,he tie es lose de,vi}. However, observe subword models consistently better translating English. % We suspect opposing trends arise English-centric heuristics tokenizer. \omer{We show byte-based tokenization effectively learn ...} % \omer{I want say something Jieba experiment, finish byte-to-byte potentially learn language-specific tokenization without manually-engineered heuristics. Problem still see gap, need say something happening .} \uri{Maybe say believe gap due first sentence paragraph, linguistic knowledge injected Moses? } % Byte-to-byte machine translation models also theoretical advantage never encounter unknown token types . % Tremendous progress made Neural Machine Translation recent years [\uri{*cite*}]. In NMT, model gets input text source language, outputs translation target language. An important well studied question NMT text segmented fed models sequence tokens. A common approach build sub-words vocabulary Byte-Per Encoding one variants. BPE data compression algorithm given corpus wanted vocabulary size , merges greedily common bi-grams data sub-words, extracts vocabulary frequent sub-words corpus. As use sub-words currently allows achieve state-of-the-art results NMT [\uri{*cite*}], comes costs, main one lost orthographic information, especially morphologically rich languages \uri{X}, \uri{Y} \uri{Z}, consider example [\uri{*example*}].\\ % One way allowing models access information raw text holds use characters instead sub words base units. This choice additional advantages use words sub-words, presented . First, opposed sub-words based models, character based one suffer vocabulary tokens inference, second, need data preprocessing schemes like BPE also tend require hyper-parameters tuning themselves. Despite theoretical advantages, previous work [\uri{*cite*}] shown performance drops adopting current NMT models use character-to-character naive plug-and-play technique. That surprising, models sophisticated enough learn patterns given opportunity to. A part reason lies fact BPE creates shorter sequences, current models handle long sequences good shorter ones . Recently, lot work done order build models handle better long dependencies , impressive progress made area. In work tackle different aspect current models show improvement ability work non-sub-word [\uri{non-sub-word bit weird, I sure I write ""byte"" ""char"" here, model byte, story ""char"" stage}] data, outperforming BPE cases, using architecture tend struggle longer sequences. \\ % [\uri{not sure put following sentence}\\ % Recent improvements non-sub-words models suggested adding different new components existing models , contrary, suggest opposite.]\\ % One way allowing models access information raw text holds, use characters tokens, previous work [\uri{*cite*}] shown performance drops adopting current NMT models use character-to-character naive plug-and-play technique. That surprising, models sophisticated enough learn patterns given opportunity to. An important difference char-to-char form corpus opposed word sub-word form length sequences, sequence characters usually much longer sub-words form sub-word units, A widely common conjuncture char-to-char models underpreform current models handle long sequences good shorter sequences. A lot recent work done order build models handle better long dependencies , , though might part reason current models struggle characters tokens, work tackle different part modern models believe also culprit performance drop, show improvement non-sub-word models. \\ % The recent years, NMT Natural Language Processing general, one-model-fits-all approach, means building models preform well many tasks datasets, opposed designing different task specific algorithms architectures. % Typically, word sub-word embeddings language models reflect interesting relations lexical substitution, semantic similarity, etc. [\uri{*cite*}], though great qualities sub-word embeddings, argue trying find similarity function characters real justification, languages character encapsulate semantic meaning themselves. Therefore, function misleading rest model [\uri{someone might say, similarity vowels instance makes sense it?}] hurt ability generate high quality translations. \\ }\\ Subwords & \multicolumn{4}{@{}c@{}|}{@} & \multicolumn{5}{@{}c@{}|}{} & \multicolumn{6}{@{}c@{}|}{@} & \multicolumn{6}{@{}c@{}|}{} & .\\ Characters & \multicolumn{2}{@{}c@{}|}{} & \multicolumn{2}{@{}c@{}|}{} & \multicolumn{2}{@{}c@{}|}{} & \multicolumn{2}{@{}c@{}|}{} & & \multicolumn{2}{@{}c@{}|}{} & \multicolumn{2}{@{}c@{}|}{} & \multicolumn{2}{@{}c@{}|}{} & \multicolumn{2}{@{}c@{}|}{} & \multicolumn{2}{@{}c@{}|}{} & \multicolumn{2}{@{}c@{}|}{} & . \\ Bytes & \ D0 \ & \ 91 \ & \ D1 \ & \ 83 \ & \ D0 \ & \ B4 \ & \ D1 \ & \ 8C \ & \ 20 \ & \ D0 \ & \ B7 \ & \ D0 \ & \ B4 \ & \ D0 \ & \ BE \ & \ D1 \ & \ 80 \ & \ D0 \ & \ BE \ & \ D0 \ & \ B2 \ &\ 2E \\ \bottomrule \end{tabular} \end{figure*} In paper, proposed EmoGraph leverages graph neural networks model dependencies among different emotions. We consider emotion node construct emotion graph based co-occurrence statistics. different emotion classes. Our model EmoGraph outperforms existing strong multi-label classification baselines. Our analysis shows EmoGraph especially helpful low resource emotions large emotion space. An additional experiment shows also help single-label emotion classification task."," Many NLP models follow the embed-contextualize-predict paradigm, in which each sequence token is represented as a dense vector via an embedding matrix, and fed into a contextualization component that aggregates the information from the entire sequence in order to make a prediction.  Could NLP models work without the embedding component? To that end, we omit the input and output embeddings from a standard machine translation model, and represent text as a sequence of bytes via UTF-8 encoding, using a constant 256-dimension one-hot representation for each byte. Experiments on 10 language pairs show that removing the embedding matrix consistently improves the performance of byte-to-byte models, often outperforms character-to-character models, and sometimes even produces better translations than standard subword models.\footnote{Our code is publicly available at: \url{https://github.com/UriSha/EmbeddinglessNMT}}"
"Propagandist news articles misleading nature aim biasing audience towards particular point view using psychological rhetorical techniques, including loaded language, name calling, repetition, exaggeration, minimization, etc. With rapid growth number online sources information speed information spreads online, manual flagging propagandist news articles become untenable, leading ongoing need new research methods identifying articles automatically mitigate negative influence might users. Until recently, work area focused article-level detection. However, 2019, Da San Martino et al.~\shortcite{da-san-martino-etal-2019-fine} published corpus English news articles individual spans propaganda annotated addresses problem granular level. This corpus used shared tasks NLP4IF-2019 SemEval-2020. The 2020 shared task modified version prior year's task includes two subtasks: We present models tasks alongside discussions results ablations. This work tests importance embedding matrix neural machine translation models. Experiments 10 different languages show that, despite ubiquitous usage, competitive models trained without embeddings. Future work may investigate potential embeddingless models different NLP tasks, explore new methods improve training byte-level models. \pdfoutput=1 File eacl2021.tex Based style files ACL 2020, Based style files ACL 2018, NAACL 2018/19, Based style files ACL-2015, improvements taken NAACL-2016 style Based style files ACL-2014, were, turn, based ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009, EACL-2009, IJCNLP-2008... Based style files EACL 2006 e.agirre@ehu.es Sergi.Balari@uab.es ACL 08 Joakim Nivre Noah Smith \documentclass[11pt,a4paper]{article} \usepackage[hyperref]{eacl2021} \usepackage{times} \usepackage{latexsym} \renewcommand{\UrlFont}{\ttfamily\small} \usepackage{siunitx} \usepackage{multirow} \usepackage{microtype} \usepackage{graphicx} \usepackage{subfigure} \usepackage{booktabs} \usepackage{hyperref} \usepackage{amsmath} \newcommand{\theHalgorithm}{\arabic{algorithm}} \usepackage{amsbsy} \usepackage{bm} \usepackage{float} \usepackage{lipsum} \usepackage{hhline} This strictly necessary, may commented out, improve layout manuscript, typically save space. \usepackage{microtype} \usepackage[T2A,T1]{fontenc} \usepackage[utf8]{inputenc} \usepackage[russian,english]{babel} \usepackage{cleveref} \aclfinalcopy Uncomment line final submission \def\aclpaperid{***} Enter acl Paper ID \setlength\titlebox{5cm} You expand titlebox need extra space show authors. Please make titlebox smaller 5cm ; check camera-ready version ask change back. \newcommand\BibTeX{B\TeX} \newcommand\uri[1]{} \newcommand\omer[1]{} \newcommand\uri[1]{[{Uri: {#1}}]} \newcommand\omer[1]{[{Omer: {#1}}]} \title{Neural Machine Translation without Embeddings} \author{Uri Shaham \quad Omer Levy\\ \\ School Computer Science, Tel Aviv University \\ Facebook AI Research } \date{} \begin{document} \maketitle","   This paper presents our systems for SemEval 2020 Shared Task 11: Detection of Propaganda Techniques in News Articles. We participate in both the span identification and technique classification subtasks and report on experiments using different BERT-based models along with handcrafted features. Our models perform well above the baselines for both tasks, and we contribute ablation studies and discussion of our results to dissect the effectiveness of different features and techniques with the goal of aiding future studies in propaganda detection."
"%%%%%%%%%%%%%%%%%%%%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% We investigated several models combinations features identify propaganda spans text, classify techniques used within span. For span identification task, found LSTM-based model combining BERT predictions original features gives highest F1 score 39.2\"," \vspace*{-0.4cm} %The PAN 2020 authorship verification  shared task focuses on a cross-topic/closed-set setup of fictional texts . The topic  describes the principal subject matter of the document, which does not necessarily correlate with the author's writing style. For automated systems, the AV task is quite challenging when the texts to be investigated come from different thematic areas. %In this work, we present a hierarchical fusion of two well-known approaches into a single end-to-end learning procedure: A deep metric learning framework at the bottom aims to learn a pseudo-metric that maps a document of variable length onto a fixed-sized feature vector. At the top, we incorporate a probabilistic layer to perform Bayes factor scoring in the learned metric space.  %We also provide text preprocessing strategies to deal with the cross-topic issue.  The PAN 2020 authorship verification  challenge focuses on a cross-topic/closed-set AV task over a collection of fanfiction texts. Fanfiction is a fan-written extension of a storyline in which a so-called fandom topic describes the principal subject of the document. The data provided in the PAN 2020 AV task is quite challenging because authors of texts across multiple/different fandom topics are included. In this work, we present a hierarchical fusion of two well-known approaches into a single end-to-end learning procedure: A deep metric learning framework at the bottom aims to learn a pseudo-metric that maps a document of variable length onto a fixed-sized feature vector. At the top, we incorporate a probabilistic layer to perform Bayes factor scoring in the learned metric space. We also provide text preprocessing strategies to deal with the cross-topic issue."
"Machine Reading Comprehension become popular task NLP, aiming understand given passage answer relevant questions. With wide availability MRC datasets~ deep learning models~ , significant progress made. Despite success, majority MRC research focused open domains. For specific domains, however, construction high-quality MRC datasets, together design corresponding models considerably deficient~. The causes behind phenomenon threefold. Take medical domain example. i) Data annotators required medical backgrounds high standards. Hence, simple crowd-sourcing~ often leads poor annotation results. ii) Due domain sensitivity, people concerned reliability information sources answers extracted, explainability answers themselves~. This fundamentally different task requirements open-domain MRC. iii) From perspective model learning, difficult pre-trained language models understand meaning questions passages containing lot specialized terms~. Without help domain knowledge, state-of-the-art models perform poorly. As shown Figure, BERT~ MC-BERT ~ predict part correct answer, i.e.,~``torso"" ``buttocks"", instead generating complete answer medical question. %Compared strong baseline model, model predict complete answer injecting lot medical knowledge. % Our CMedBERT model extract complete answer fusing lot medical knowledge. % Only fuse medical knowledge pre-trained models, complete answers extracted. In paper, present comprehensive study Chinese medical MRC, including i) task formulated, ii) construction Chinese medical dataset iii) MRC model rich medical knowledge injected. To meet requirements medical MRC, aim predict answer spans medical question, support sentence passage, indicating source answer. The support sentences provide abundant evidence users learn medical knowledge, medical professionals assess trustworthiness model output results. %two MRC tasks Chinese medical dataset. %The first task predict answer content question second task predict index sentence supporting answer. For dataset, construct highly-quality Chinese medical MRC dataset, named Multi-task Chinese Medical MRC dataset . It contains 12,172 question, passage, answer, support sentence quads. Based analysis CMedMRC, summarize four special challenges Chinese medical MRC, including long-tail terminologies, synonym terminology, terminology combination paraphrasing. In addition, find comprehensive skills required MRC models answer medical questions correctly. For answer extraction CMedMRC, direct token matching required answering 31\% questions, co-reference resolution 11\%, multi-sentence reasoning 18\% implicit causality 22\%. In addition, answers remaining questions extremely difficult extract without rich medical background knowledge. To address medical MRC task, propose multi-task dynamic heterogeneous fusion network based MC-BERT~ model Chinese medical knowledge base . The technical contributions CMedBERT twofold: In experiments, compare CMedBERT four strong baselines. For answer prediction, compared strongest competitor, EM F1 scores increased +3.88\% +1.46\%, respectively. Meanwhile, support sentence prediction task result increased large margin, i.e., +7.81\% EM +4.07\% F1. The contributions summarized follows: % % \end{table*} \end{table*} In paper, presented system SemEval-2020 Task 11, leverages LSTM pretrained word embeddings without using human-engineered features representation learning. Our experimental results show LSTM model GloVe word embeddings get better performance according scores different neural network models integration models task. The main goal task detect propaganda techniques news articles fine-grained level, make coarse judgments whether news articles use propaganda techniques. It known neural networks perform well large training sets, sometimes large, accurately labeled dataset cannot obtained. For future work, development propaganda technology detection news articles greatly improved pretraining model integrated model architecture."," Machine Reading Comprehension  aims to extract answers to questions given a passage. It has been widely studied recently, especially in open domains. However, few efforts have been made on closed-domain MRC, mainly due to the lack of large-scale training data. In this paper, we introduce a multi-target MRC task for the medical domain, whose goal is to predict answers to medical questions and the corresponding support sentences from medical information sources simultaneously, in order to ensure the high reliability of medical knowledge serving. A high-quality dataset is manually constructed for the purpose, named Multi-task Chinese Medical MRC dataset , with detailed analysis conducted. We further propose the Chinese medical BERT model for the task , which fuses medical knowledge into pre-trained language models by the dynamic fusion mechanism of heterogeneous features and the multi-task learning strategy. Experiments show that CMedBERT consistently outperforms strong baselines by fusing context-aware and knowledge-aware token representations."
"Nowadays, volume biomedical literature biomedical web pages continues increase rapidly. Lots new articles web pages containing biomedical discoveries new insights continuously published. Indeed, increasingly high demand biomedical text mining. Recent progress biomedical text mining approach made possible development deep learning techniques used natural language processing . For example, pre-trained language models BERT , ERNIE , XLNet RoBERTa demonstrated remarkable successes modeling contextualized word representations utilizing massive amount training text. As fundamental technique natural language processing , language models pre-trained text could easily transferred learn downstream NLP tasks finetuning, achieve state-of-the-art performances many tasks including named entity recognition, paraphrase identification, question answering information retrieval. However, limitations apply state-of-the-art NLP methodologies biomedical text mining directly. Firstly, since recent representation models BERT trained tested mainly general domain datasets Wikipedia, difficult adapt biomedical datasets without losing performance. Moreover, word distributions general biomedical text quite different, problem biomedical text mining. In addition, exist long-tail concepts terminologies biomedical texts difficult learned via language models. For Chinese biomedical text, somewhat difficult due complex structure variety phrase combinations. To end, recent biomedical text mining models rely mostly adapted versions word representations . Considering whether possible automatically inject biomedical knowledge language representation learning Chinese medical corpus, hypothesize current state-of-the-art word representation models BERT trained biomedical corpora prior biomedical knowledge effective biomedical text mining tasks. However, exist two problems: retrieve biomedical domain knowledge; leverage knowledge representation learning. In paper, propose conceptualize representation learning approach Chinese biomedical language understanding. Specifically, propose coarse-to-fine masking strategies inject entity linguistic domain knowledge representation learning. As benchmarks Chinese Biomedical Language Understanding Evaluation, release first large scale benchmark including name entity recognition, paraphrase identification, question answering, information retrieval, intent detection, text classification. Experiments show approach achieves state-of-art results. In work, address medical MRC problem new dataset CMedMRC constructed. An in-depth analysis dataset conducted, including statistics, characteristics, required MRC skills, etc. Moreover, propose CMedBERT model, help pre-trained model better understand domain terms retrieving entities medical knowledge bases. Experimental results confirm effectiveness model. In future, explore knowledge improve performance models. \clearpage"," Biomedical text mining is becoming increasingly important as the number of biomedical documents and web data rapidly grows. Recently, word representation models such as  BERT has gained popularity among researchers. However, it is difficult to estimate their performance on datasets containing biomedical texts as the word distributions of general and biomedical corpora are quite different. Moreover, the medical domain has long-tail concepts and terminologies that are difficult to be learned via language models. For the Chinese biomedical text, it is more difficult due to its complex structure and the variety of phrase combinations. In this paper, we investigate how the recently introduced pre-trained language model BERT can be adapted for Chinese biomedical corpora and propose a novel conceptualized representation learning approach.  We also release a new  Chinese Biomedical Language Understanding Evaluation benchmark .  We examine the effectiveness of Chinese pre-trained models: BERT, BERT-wwm, RoBERTa, and our approach. Experimental results on the benchmark show that our approach could bring significant gain. We release the pre-trained model on GitHub: \url{https://github.com/alibaba-research/ChineseBLUE}."
"Digitalization facilitates management manipulation large-scale data sets, large collections documents, audio recordings, images. In large collections, finding specific objects efficiently possible computational tools. The predominant form searching based similarity objects, algorithm would identify rank list objects collection based similarity given query object. Similarity search requires object type specific similarity measures. For instance, form textual similarity may used searching document collections whereas measure time-series similarity would employed searching collections audio recordings. A particularly valuable type information object tables, recently received appropriate attention. Tables used present structured information two-dimensional matrix, extensively used scientific articles, business reports, product specifications, web pages etc. Research tables first class objects started roughly 10 years ago availability large table collections, mostly extracted web pages Wikipedia. %Research tables document collections lagged behind due difficulty extract tables document formats like PDF. This situation, however, recently improved considerably, document collections introduced proper table mark-up. This work concerned table similarity : Given pair tables, e.g. query table table table corpus, compute accurate estimate semantic similarity. TS fundamental operation prerequisite many applications, table clustering classification, table auto-completion, table fusion filling missing values databases. Despite importance TS, received little attention operation right far. Existing table similarity functions tightly integrated downstream application compared TS methods. For instance, previous works table augmentation, table union, table extension table imputation incorporate specific TS algorithms whose individual quality unknown. Note TS, define necessary applications, different related field table-keyword similarity. %cafarella2008webtables What lacking general robust method assess similarity two tables. Compared similarity types objects, TS own, specific properties. In contrast pure texts, sequence words, sentences, paragraphs conveys meaning, tables impose meaning arrangement values columns rows, often augmented header information. In contrast image similarity, relative positions pixels extremely important, table similarity often independent order rows columns - two tables patients two hospitals considered similar irrespective order patients appear rows, order metadata patients recorded. In paper, present TabSim, TS method employs deep learning techniques achieve two main objectives: a) generate suitable table representations, b) use representations learn accurate similarity function pairs tables. It based Siamese neural networks, known able learn similarity model given samples. TabSim require hand-crafted features, learns similarity function directly gold standard corpus. TabSim's network first generates representation table concatenation embeddings caption tabular content. For these, apply two different networks properly reflect diverging structures: A Bidirectional LSTM layer capable modeling sequences utilized capture semantic information captions order words caption carry semantic information. Tabular data represented order-invariant self-attention neural network, since order cells within column order columns within table often carry meaning. The two representations shared compared tables guarantee symmetry similarity score. Model parameters optimized contrastive loss function relies tables distances. To train evaluate TabSim, created novel corpus consisting 1500 table pairs extracted biomedical articles manually scored regarding pairwise degree similarity. To best knowledge, first publicly available gold standard corpus TS. We also evaluated approach two corpora originally developed different yet similar task allowed adaptation: a) tables extracted arXiv articles b) tables Wikipedia pages. Our evaluation three corpora shows that, average, TabSim outperforms baselines app. 7\% pp F1-score binary similarity classification setting app. 1.5\% pp ranking scenario using NDCG. The paper organized follows. In Section 2, review existing techniques TS. We explain TabSim neural architecture Section 3. Section 4 presents data preparation, used baselines, evaluation settings metrics. In Section 5, provide results evaluation conclude Section 6. %-------------------------------------------------------------------------- In article, introduce MC-BERT, pre-trained language representation model domain knowledge biomedical text mining. We show pretraining BERT biomedical corpora crucial applying biomedical domain. The ChineseBLUE MC-BERT soon available BioNLP community. Our motivation develop universal, GLUE-like, open platform Chinese BioNLP community composable generalized representation algorithm inject domain knowledge. Our work small step direction."," Tables are a popular and efficient means of presenting structured information. They are used extensively in various kinds of documents including web pages. Tables display information as a two-dimensional matrix, the semantics of which is conveyed by a mixture of structure , headers, caption, and content. Recent research has started to consider tables as first class objects, not just as an addendum to texts, yielding interesting results for problems like table matching, table completion, or value imputation. All of these problems inherently rely on an accurate measure for the semantic similarity of two tables. We present TabSim, a novel method to compute table similarity scores using deep neural networks. Conceptually, TabSim represents a table as a learned concatenation of embeddings of its caption, its content, and its structure. Given two tables in this representation, a Siamese neural network is trained to compute a score correlating with the tables' semantic similarity. To train and evaluate our method, we created a gold standard corpus consisting of 1500 table pairs extracted from biomedical articles and manually scored regarding their degree of similarity, and adopted two other corpora originally developed for a different yet similar task. Our evaluation shows that TabSim outperforms other table similarity measures on average by app. 7\% pp F1-score in a binary similarity classification setting and by app. 1.5\% pp in a ranking scenario."
"Spoken language understanding systems extract semantic information spoken utterance machine . The Air Travel Information System first SLU model built based cascade speech recognizer, language model, semantic extractor-SQL generator 1990 . Thirty years ATIS, designing end-to-end neural SLU replace ASR+NLU-based SLU technology still remains challenge . Ideally, would like all-neural model whose layers project audio signal hidden semantic representations, the-so-called ""thought vectors"" infer domain, intent, slots implied audio signal. To achieve goal, several groups conducted experiments using non-ASR awareness E2E SLU. These models usually apply multiple stack RNNs encode entire utterance vector fed fully connected feedforward neural network followed soft-mask max-pool layer identify domain, intent, slot. These models treat unique combination domain, intent, slots output label. For reason, call type E2E SLU classification-based approaches. The limitation classification-based approaches combination domains, intents, slots may grow exponentially, subsequently deal classification problem many number output labels; moreover, number intents usually fixed makes usability classification-based approaches limited. A natural approach deal variable-length output E2E SLU use sequence-to-sequence neural models . In , several seq2seq architectures proposed E2E SLU, among authors found model incorporates ASR-awareness module form multi-task learner delivers best performance. The finding supported recent proposed pre-trained ASR-awareness architecture . In ASR, input output sequences ordered monotonic. As such, need entire utterance decode transcription. In contrast, extracting semantic information audio signals, usually need scan entire audio. Similar neural machine translation , E2E SLU models massively benefit attention mechanism. In attention mechanism, encoder generates outputs incorporating hidden representations time steps hence allows output pay attention inputs time steps . The neural attention models demonstrated promising results ASR well . The transformers seq2seq, non-recurrent, self-attention neural models used neural machine translation well NLU great success . In paper, leverage transformer architecture E2E SLU. Neural transformers several distinct features make suitable candidate SLU task: The transformers use self-attention mechanism allows compute correlation sublayer pairs time steps encoder decoder. Sub-spaces projection self-attention helps extract semantic context audio frames. The transformers benefit distributed training linear transformation self-attention parallelized. Compared RNN models , transformers less number parameters. Our model works classification-based mode hierarchical mode allows decode variable length domain, intent slot vectors. We compare new architecture E2E SLU models use RNN CNN recently publicly released dataset called Fluent Speech Commands . Our results show transformer-based SLU outperforms RNN+CNN based model, less numbers parameters. The rest paper organized follows: Section 2 formulates problem. In Section 3, describe transformer based SLU details. Section 4 gives details experiments results. Finally, draw conclusion give future directions Section 5. We presented TabSim, new method assessing table similarity uses Siamese neural networks learn similarity measure gold standard corpus table pairs. We showed that, comparison five methods three also rooted applications based table similarity, TabSim attains considerably higher precision, recall, F1-score, accuracy measures three different corpora. Our results also demonstrate that, among different configurations TabSim, model uses self-attention neural networks achieve highest performance, probably is, different 2d-based CNN sequence-based Bi-LSTM, invariant row column permutations. As part research, also created first specific gold standard corpus table similarity research, containing 1500 table pairs manually scored regarding semantic similarity. Although corpus created way gives methods relying cosine similarity competitive advantage, TabSim also leads field corpus. A disadvantage TabSim high execution time; takes, average, 5 ms classify pair tables . This high runtime certainly appropriate using TabSim similarity function table similarity search engine, query table would compared every table corpus search time. In future work, plan focus designing scalable table search engines use TabSim core apply additional techniques early search space pruning.","  Spoken language understanding  refers to the process of inferring the semantic  information from audio signals.  While the neural transformers consistently deliver the best performance among the state-of-the-art neural architectures in field of natural language processing ,  their merits in a closely related field, i.e.,  spoken language understanding   have not beed investigated. In this paper, we introduce an end-to-end neural transformer-based SLU model that can predict the variable-length domain, intent, and slots  vectors embedded in an audio signal with no intermediate token prediction architecture. This new architecture leverages the self-attention mechanism by which the audio signal is transformed to various sub-subspaces allowing to extract the semantic context implied by an utterance. Our end-to-end  transformer SLU predicts the domains, intents  and slots in the Fluent Speech  Commands dataset with accuracy equal to 98.1 \%, 99.6 \%, and 99.6 \%, respectively and outperforms the SLU models that leverage a combination of recurrent and convolutional neural networks by 1.4 \%   while the size of our model is  25\% smaller than  that of these architectures.  Additionally, due to independent sub-space projections in the self-attention layer,  the model is highly parallelizable which makes it a good candidate for on-device SLU."
"Having clear picture students' perception classes, professors, university facilities enables educational institutions propose strategies improve many areas. It suggested many studies positive students' perception learning environment correlated higher academic achievement. Therefore, universities improve quality professors, class content well learning facilities, also improve --as consequence-- students' academic achievement, leading overall improvement education quality. The call action clear. However, order propose implement effective improvement strategies, one needs measure students' perception. Typical ways evaluations carried final stage academical period students grade professors several aspects. These evaluations normally consist online questionnaire closed questions, open questions students give opinions class professors. Closed questions questionnaire tedious students, leading low response rates. Closed questions helpful fast interpretation results statistical tools. These questions designed measure professors' performance specific topics engaging class is, punctuality, among others. On hand, open questions provide students free space express opinions. Of course, gathering interpreting data open questions responses much challenging task making statistics closed questions. Nonetheless, amount useful information found students' opinions valuable source rarely exploited. The latest advances machine learning natural language processing techniques used build tools facilitate analysis large amounts opinions generated students. Particularly, sentiment analysis suited identify quantify positively negatively students feel professors. These machine learning applications recently explored. For instance, Nae Bayes used classify students' opinions social media. Also, Latent Dirichlet Allocation used model topics along sentiment analysis explore opinions students. Some studies using tools machine learning conducted field students' perception analysis. The majority addressed issue performing sentiment analysis students' comments, others tried identify topics suggestions opinions left students , thus, develop joint approach state art tools NLP used perform sentiment analysis identify topics interest students' comments. Nonetheless, must stress researchers long worked similar problems assessing customer satisfaction written opinions including public election forecasting, sales trading prediction, marketing price prediction, among others. The common pipeline performing opinion mining consists following general steps: i) retrieval opinions public databases, ii) cleaning opinions , iii) prediction quantity interest polarity, sentiment strength, among others. In paper, combine state-of-the-art methods NLP-based pipeline classifying sentiment students' opinions. We use results predict ratings given professors students means supervised learning algorithms. Furthermore, perform LDA discover latent topics central students' opinions. With power question answering systems, envision students' perceptions surveys open questions fast answer, reaching high levels response rates, also extracting relevant information, comes students' opinions. These opinions mined methods like one propose analyse students truly feel professors classes. The structure paper follows. In section methods materials brief description data prepossessing presented. Next, results analysis model performance well statistical analysis obtained results scrutinised. Recommendations future perspectives research subject well outlined conclusions listed final part manuscript. The system description provided, compare achieved results official model several models, including baseline best team competition. In future work, investigated imbalanced dataset small inter-annotator agreement caused JokeMeter model focused prior probabilities grades input .","  Students' perception of classes measured through their opinions on teaching surveys allows to identify deficiencies and problems, both in the environment and in the learning methodologies. The purpose of this paper is to study, through sentiment analysis using natural language processing  and machine learning  techniques, those opinions in order to identify topics that are relevant for students, as well as predicting the associated sentiment via polarity analysis. As a result, it is implemented, trained and tested two algorithms to predict the associated sentiment as well as the relevant topics of such opinions. The combination of both approaches then becomes useful to identify specific properties of the students' opinions associated with each sentiment label  and topic. Furthermore, we explore the possibility that students' perception surveys are carried out without closed questions, relying on the information that students can provide through open questions where they express their opinions about their classes.  % of the comments using NLP techniques.  % In the present paper Students' perception of classes measure through teaching surveys allows to identify deficiencies and problems, both in the environment and in the learning methodologies. NLP techniques   %The abstract should briefly summarize the contents of the paper in 15--250 words.  \keywords{Students' satisfaction  \and Natural language processing \and polarity analysis}"
"In knowledge discovery representation, notion concept often used refer sense, i.e., `abstract entity' `abstract object' Fregean dichotomy sense vs. reference . In Natural Language Processing , task Concept Extraction deals identification language side concept coin, i.e., Frege's reference. Halliday offers syntactic interpretation reference. In terminology, ``classifying nominal group''. For instance, renewable energy nuclear energy classifying nominal groups: denote class energy, while, e.g., cheap energy affordable energy not: typify, rather qualify energy . CE crucial number downstream applications, including, e.g., language understanding, ontology population, semantic search, question answering; also key entity linking . In generic open domain subject-neutral discourse across different subjects, indexing longest possible nominal chunks head words located sequences tokens specified ``break words"" special dictionary lookups DBpedia Spotlight WAT common techniques. They generally reach outstanding precision, low recall due constant evolvement language vocabulary. Advanced deep learning models already dominate CE specialized closed domain discourse one limited range related subjects, e.g., biomedical discourse , also standard keyphrase extraction alternative. However, models need tremendous amount labeled data training. We present operational CE model utilizes pointer--generator networks bidirectional long short-term memory units retrieve concepts general discourse textual material.\footnote{We adopt Halliday's notion classifying nominal group definition concept.} Furthermore, since generic, domain-independent concept extraction model need sufficiently large training corpus covers vast variety topics annotated corpora available, opt distant supervision create sufficiently large diverse dataset. Distant supervision consists automatic labeling potentially useful data easy-to-handle algorithm obtain annotation likely noisy but, time, contain enough information train robust model . Two labeling schemes considered. Experiments carried dataset 250K+ Wikipedia pages show copies model trained differently joined ensemble significantly outperform standard techniques and, used top DBpedia Spotlight, improve performance nearly 10\%. Above proposed variants modern neural summarizaiton models we: Perform additional in-domain pretraining ; `decorate' inputs automatically extracted information ; sort inputs prioritize passing along large high-quality trials . We evaluated models across key aspects, including relevance, `semantic plausibility', factuality. All systems considered yielded highly fluent relevant summaries. But manual analysis generated corresponding reference summaries revealed factuality systems remains issue. The proposed decoration sorting strategies yielded modest statistically significant improvements assessed factuality. Annotators exhibited disagreement evaluating factuality. We believe part reflects inherent difficulty task, future work hope improve annotation protocol reduce subjectivity improve agreement. For example, explicit levels disagreement map onto specific numerical scores providing detailed instructions regarding may improve inter-rater agreement, might explicitly differentiating factuality strength evidence reported directionality finding. Separating factuality rating strength evidence direction conclusion seems promising route improve inter-rater agreement. ROUGE scores --- commonly used automatically evaluate summarization systems --- significantly correlate factuality assessments here. We proposed method automatically evaluating factuality narrative evidence syntheses, findings-JSD, using models infer reported directionality findings generated reference summaries. This measure significantly correlates manual assessments factuality. We view promising direction pursue going forward facilitate automatic evaluation evidence synopses, turn would support continued development automated summarization systems evidence synthesis. \section{Conclusions} We demonstrated modern neural abstractive summarization systems generate relevant fluent narrative summaries RCT evidence, struggle produce summaries accurately reflect underlying evidence, i.e., factual. We proposed new approaches modestly improve factuality system outputs, described metric attempts automatically measure factuality, suggesting directions future work. The multi-document summarization dataset available: . \section*{Acknowledgements} This work funded National Institutes Health National Library Medicine, grant R01-LM012086. We thank Ani Nenkova helpful comments concerning evaluation. \setlength\itemsep{-0.1em}"," Concept extraction is crucial for a number of downstream applications. However, surprisingly enough, straightforward single token/nominal chunk--concept alignment or dictionary lookup techniques such as DBpedia Spotlight still prevail. We propose a generic open-domain OOV-oriented extractive model that is based on distant supervision of a pointer--generator network leveraging bidirectional LSTMs and a copy mechanism. The model has been trained on a large annotated corpus compiled specifically for this task from 250K Wikipedia pages, and tested on regular pages, where the pointers to other pages are considered as ground truth concepts. The outcome of the experiments shows that our model significantly outperforms standard techniques and, when used on top of DBpedia Spotlight, further improves its performance. The experiments furthermore show that the model can be readily ported to other datasets on which it equally achieves a state-of-the-art performance.  \keywords{Open-domain discourse texts \and Concept extraction \and Pointer-generator neural network \and Distant supervision}"
"This paper describes socialbot open-domain conversation, Chirpy Cardinal, built research platform 2019 Alexa Prize competition. During competition, US-based Amazon Alexa users could give invocation phrase connected one competing socialbots . After receiving minimal orientation phrase beginning conversation, user talks socialbot decide end conversation -- point, invited provide rating comment. To provide convincing user experience, open-domain conversational agent must excel language understanding, language generation, emotional engagement, memory, world knowledge conversational planning, among desirable characteristics -- ambitious goal! Prior work within outside Alexa Prize competition taken successful strategy pushing progress along individual skills, forming ensemble sub-systems, excelling singular characteristic ignoring others. For instance, supporting user initiative open-domain conversations extremely challenging, requires understanding countless ways user take initiative, ability respond specificity. Faced difficulty, comes in-depth conversations, many previous dialogue systems rely primarily bot-initiative, driving users along carefully scripted paths. On hand, systems attempting higher user-initiative via non-scripted paths likely lead towards shallower conversations. Thus lot room innovation research trying simultaneously achieve two complementary characteristics; recurring theme throughout work. Our goal building socialbot offer natural-sounding emotionally engaging dialogue agent talk knowledgeably wide variety topics, also letting user take much initiative possible. Initiative -- ability drive direction conversation -- studied extensively context task-oriented dialogue. Mixed initiative , user bot share initiative, important quality successful dialogue system, provides user sense agency without making entirely responsible suggesting new topics directions. In order improve mixed initiative still providing acceptable conversational depth, designed initial system rely heavily system initiative, time explored several avenues increase user initiative controlled fashion. To support mixed initiative, system global navigational intent classifier entity tracker , allowing track high level topic changes user bot. Further, response priority system allows individual Response Generators interject user initiates change topic. High-coverage world knowledge important component open-domain conversation -- bot must able talk diverse range entities topics interest users, particularly wish respect user initiative. We use Alexa Knowledge Graph, The Washington Post, Reddit Twitter sources up-to-date knowledge particular domains, ensuring high coverage using Wikipedia Wikidata entities foundation entity-based conversations . However, world knowledge must delivered conversational style -- characteristic distinguishes socialbot virtual assistant. To achieve this, finetuned neural generative model TopicalChat dataset obtain conversational paraphrasing model adapts external text conversational style . A socialbot cannot focus solely external entities -- truly social, must able discuss personal experiences emotions. While ELIZA-like systems attempt via templated repetition user phrases, lack naturalness depth real human conversations. Our Neural Chat module invites user share everyday experiences current emotions, uses neural generative model respond empathetically. With it, attempt deep, sustained emotionally engaging conversation user's lives. In addition, Opinion module allows user express feelings expressing likes dislikes. To foster reciprocal atmosphere, bot also shares distinct feelings, experiences opinions. Lastly, note advent large-scale pretrained neural generative models substantially impacted possible open-domain socialbots. While last Alexa Prize competition, none top three socialbots used neural generation , found current GPT-2 models key tool support design goals. Neural generation enables natural phrasing emotional engagement, well flexible responsiveness , supporting higher user initiative. A limitation neural generation methods dialogue deterioration quality consistency long conversation, potentially overcome symbolic constraints. We explore ways bring best worlds -- long term consistency short term fluidity -- together. Despite first-time entrant, end competition system achieved rating 3.6/5.0, within 0.1 highest-ranked systems, capable detailed, sustained conversations interested users . Qualitatively, in-person interactions users, observed many innovations in-depth discussions everyday life, conversational styling informational content, opinionated exchanges received expressions pleasant surprise -- indicating steps right direction. In \secref{sec:analysis}, re-examine goals set achieve, empirically analyze bot's successes failures. In \secref{sec:discussion}, talk challenges faced, trade-offs made, conclusions avenues future work. We presented adaptation pointer--generator network model generic open-domain concept extraction. Due capacity cope OOV concept labels, outperforms dictionary lookup-based CE DBpedia Spotlight AIDA terms recall -score. It also shows advantage deep models focus NER since also covers non-named concept categories. However, combination pointer--generator model DBpedia Spotlight seems best solution since takes advantage neural model dictionary lookup. In order facilitate solid evaluation proposed model compare series baselines, utilized Wikipedia pages text snippet links sparsely concept-annotated dataset. To ensure model capable extracting generic concepts instead detecting texts page links, ignored sparse annotation training. Instead, compiled large densely concept-annotated dataset leveraging within distant supervision using algorithm described above. To best knowledge, dataset available far. In future, plan address problem multilingual concept extraction, using pre-trained multi-lingual embeddings compiling another large dataset contains higher percentage non-named entity concepts. The code running pretrained models available following GitHub repository: ."," We present Chirpy Cardinal, an open-domain dialogue agent, as a research platform for the 2019 Alexa Prize competition. Building an open-domain socialbot that talks to real people is challenging -- such a system must meet multiple user expectations such as broad world knowledge, conversational style, and emotional connection.  Our socialbot engages users on their terms -- prioritizing their interests, feelings and autonomy.  As a result, our socialbot provides a responsive, personalized user experience, capable of talking knowledgeably about a wide variety of topics, as well as chatting empathetically about ordinary life. Neural generation plays a key role in achieving these goals, providing the backbone for our conversational and emotional tone. At the end of the competition, Chirpy Cardinal progressed to the finals with an average rating of 3.6/5.0, a median conversation duration of 2 minutes 16 seconds, and a 90$^{\text{th}}$ percentile duration of over 12 minutes."
"Clinical Named Entity Recognition extracts patient information unstructured Electronic Health Records , important task clinical research. The main goal CNER identify clinical terminologies EHRs, diseases, symptoms, treatments, exams body parts. Accurate identification clinical concepts provide effective decision support patient care treatment. Compared English texts, CNER Chinese texts difficult since Chinese EHRs recorded without explicit word delimiters. In recent years, CNER attracted considerable research efforts, many methods proposed literature. Most deep learning methods. Although many advanced models developed CNER, performance still heavily depends manually-annotated training data. Labeling EHRs usually time-consuming expensive EHRs involve many complex clinical terminologies, labelers medical background qualified clinical annotation. It thus becomes rather difficult train effective model CNER since requires large number manually-annotated clinical texts. Active learning, iteratively selects informative samples labelers annotate, effective method reduce annotation cost. It widely used many Natural Language Processing tasks, text classification event recognition. In conventional active learning, one labeler algorithm queries labels selected instances labeler, always returns ground truth queried labels. However, many real settings, multiple labelers, usually provide diverse quality annotation different costs. Obviously, labeler offers better overall quality require higher cost query. The overall quality labelers assessed according previous annotation performance. Moreover, labelers may diverse expertise different instances. For example, CNER tasks, labelers may good labeling diseases, skilled symptoms. Therefore, need consider querying annotate selected instances keep trade-off quality cost. In past years, active learning multiple noisy labelers received significant attention achieved great success various applications. However, many works either ignored different expertise multiple labelers queried labeler instances globally neglected annotation costs different labelers. Recently, two methods considering diversity labelers expertise query costs proposed classification tasks. Experimental results demonstrate effectiveness two methods selecting cost-effective queries. We thus follow trend focus CNER task first time. In paper, propose Cost-Quality Adaptive Active Learning method CNER Chinese EHRs, selects cost-effective instance-labeler pairs obtain better annotation performance lower costs adaptive manner. Specifically, first combine three sampling strategies, namely uncertainty, entropy margin assess informativeness instances. We observe labeler low quality overall annotation still assign accurate labels specific instances real settings. Then, based fact, instance, select suitable labeler offers high-quality yet cheap annotations keep balance annotation quality, labeling costs, informativeness instances. The main contributions paper summarized follows: The rest paper organized follows. Section briefly reviews related work CNER active learning. Section presents proposed cost-quality adaptive active learning method Chinese CNER, followed experimental evaluations Section . Finally, conclusions potential research directions summarized Section . \customparagraph{Full Stack NLP} Most NLP research focuses self-contained tasks. However, open-domain socialbot, served diverse range customers widely different contexts, means self-contained task. Our socialbot tapestry many components, requiring deep understanding component work together -- setting call Full Stack NLP. Often inputs outputs components inter-dependent, leading cascading errors. We made many design choices delay hard decisions pipelines, maximize information exchange modules. Moving forward, next avenue advancing state-of-the-art would research models perform tasks jointly methods enable training multiple interdependent tasks small amount joint supervision. \customparagraph{Domain Shift} As recurring problem, found many existing NLP resources work well out-the-box. The main reason training data resources misaligned setting . However, deeper reason constantly changing nature dialogue agents themselves. Even extremely related resource , domain shift problem. Recent advances online- meta-learning could provide useful long term solution issue. \customparagraph{Conflict Intimacy} Bot-human conversations fundamentally different human-human conversations. Users adversarial, deliberately testing bot's boundaries. As socialbot designers, eager avoid disaster like Microsoft Tay, apply strict overly simplistic methods block sensitive topics . However, rules sincere conversation difficult topics. We observed users actually quite resilient conflict, find disagreement stimulating . We also found emotional intimacy reciprocal -- users inclined share feelings bot shared . Going forward, continue take seriously dangers speaking inappropriately, keep mind cost -- engagement intimacy -- engaging difficult topics. \customparagraph{Initiative} As part goal support user initiative, focused asking users questions find topics interested them. However, puts pressure user think response, especially given time constraints Alexa devices. Thus found attempts let user take initiative unfortunately led decision fatigue. Separately, ability support user initiative limited ability answer followup questions, correctly understand long unexpected user utterances. On balance, found asking user open-ended questions interesting topics good strategy -- easier handle spontaneous user questions, less pressuring asking users name topics. We see opportunity future work build systems listen user's knowledge, rather providing knowledge.","  Clinical Named Entity Recognition  aims to automatically identity clinical terminologies in Electronic Health Records , which is a fundamental and crucial step for clinical research. To train a high-performance model for CNER, it usually requires a large number of EHRs with high-quality labels. However, labeling EHRs, especially Chinese EHRs, is time-consuming and expensive. One effective solution to this is active learning, where a model asks labelers to annotate data which the model is uncertain of. Conventional active learning assumes a single labeler that always replies noiseless answers to queried labels. However, in real settings, multiple labelers provide diverse quality of annotation with varied costs and labelers with low overall annotation quality can still assign correct labels for some specific instances. In this paper, we propose a Cost-Quality Adaptive Active Learning  approach for CNER in Chinese EHRs, which maintains a balance between the annotation quality, labeling costs, and the informativeness of selected instances. Specifically, CQAAL selects cost-effective instance-labeler pairs to achieve better annotation quality with lower costs in an adaptive manner. Computational results on the CCKS-2017 Task 2 benchmark dataset demonstrate the superiority and effectiveness of the proposed CQAAL."
"% % The following footnote without marker needed camera-ready % version paper. % Comment instructions uncomment 8 lines % ""final paper"" variant English. % . % % % final paper: en-us version % % % space normally used marker % This work licensed Creative Commons % Attribution 4.0 International License. % License details: % \url{http://creativecommons.org/licenses/by/4.0/}. } %\footnotemark .} In digital era, users express personal thoughts opinions regarding wide range topics social media platforms blogs, micro-blogs , chats . Multilingual societies like India decent amount internet penetration widely adopted social media platforms. However, regional language influences proliferation Hindi-English Code-Mixed data. Sentiment analysis end-user data social media crucial resource commerce governance. However, contrast classical sentiment analysis methods, originally designed dealing well-written product reviews, CM texts social media often contain misspellings , badly cased words, letter substitutions, ambiguities, non standard abbreviations, improper use grammar, etc. CM poses several unseen difficulties natural language processing tasks word-level language identification, part-of-speech tagging, dependency parsing, machine translation semantic processing. In last years, number workshops Linguistic Code-Switching Workshops\footnote{https://code-switching.github.io/2020/} shared tasks Mixed Script Information Retrieval organized due emerging popularity code-mixing. To promote research area, Task 9 SemEval-2020 devoted CM sentiment analysis Twitter. The goal task automatically classify polarity given CM Twitter post one three predefined categories: positive, negative neutral. The CM languages English-Hindi English-Spanish; detailed description task see . In paper, present deep learning approach, using Recurrent Convolutional Neural Network task automatic CM sentiment classification tweets. % The rest paper structured follows. Section 2 provides background brief. Section 3 provides system overview Section 4 describes approach detail. In Section 5, discuss analysis evaluation results system. We conclude work Section 6. Deep learning powerful, data hungry. Various approaches proposed alleviate annotation bottleneck deep learning making model knowledge efficient. In thesis, also propose several approaches make deep learning models knowledge-efficient. Specifically, reviewed four work done direction: First, proposed knowledge rich deep learning model, unifying learning framework weak supervisions distant supervision, data programming joint inference; Second, applied knowledge rich deep learning model assist machine reading comprehension models find correct evidence sentences support decision; Third, investigate knowledge transfer techniques multilingual setting, proposed method improve pre-trained multilingual BERT based bilingual dictionary; Last, present episodic memory network language modelling, encode large external knowledge pre-trained GPT. I also show preliminary experimental results future work: enriching pre-trained model vector quantized memory layer. We tried best make deep learning models knowledge-efficient, even work present thesis up-to-date, they're early exploration direction. Given fact current large scale unsupervised pre-training began revolutionize NLP field, still, power unsupervised pre-training fully discovered yet lots research problems remain unsolved efficient transfer learning model compression etc. Most likely, future work center around large scale unsupervised pre-training.","   This paper describes the participation of LIMSI\_UPV team in SemEval-2020 Task 9: Sentiment Analysis for Code-Mixed Social Media Text. The proposed approach competed in SentiMix Hindi-English subtask, that addresses the problem of predicting the sentiment of a given Hindi-English code-mixed tweet.  We propose Recurrent Convolutional Neural Network that combines both the recurrent neural network and the convolutional network to better capture the semantics of the text, for code-mixed sentiment analysis.  The proposed system obtained 0.69  in terms of F1 score on the given test data and achieved the 9th place  in the SentiMix Hindi-English subtask."
"The performance many machine learning algorithms depends hyper-parameters. For example, prediction accuracy support vector machines depends kernel regularization hyper-parameters , deep neural networks sensitive wide range hyper-parameters, including number units per layer, learning rates, weight decay, dropout rates etc.. It well-known hyper-parameter settings often make difference mediocre state-of-the-art performance. As result, hyper-parameter optimization receiving increasingly amount attention NLP machine learning communities. However, identifying best model configuration often cumbersome process involve several trials errors optimal hyper-parameter setting found. Bayesian Optimization emerged efficient framework carrying model selection process, achieving impressive successes. For example, several studies, found better instantiations convolutional network hyper-parameters domain experts. The common theme perform set iterative hyper-parameter optimizations. In round, methods fit hyper-parameter response surface using probabilistic regression function Gaussian Process tree-based models, response surface maps hyper-parameter setting approximated accuracy. The learned regression model used surrogate response surface explore search space identify promising hyper-parameter candidates evaluate next order enhance validation accuracy. While methods enjoyed great success compared conventional random search grid search algorithms model selection, focus work largely optimizing effectiveness, ignoring resulting model's training efficiency. Given prediction accuracy model training time important real-world applications, models selected effectiveness may meet strict real-world efficiency requirements necessary deploy production environment. In addition, previous methods exclusively focus optimizing hyper-parameters given model class, ignoring important extrinsic hyper-parameters training set size influence speed accuracy. For example, model training time typically grows proportionally respect training set size, prediction accuracy also influenced amount training data used learning. If tolerance inefficient model training low, amount training data reduced adjusted rest intrinsic hyper-parameters meet stringent efficiency requirements. Given model effectiveness training time important real-world applications, work, propose unified Bayesian Optimization framework jointly selecting models prediction effectiveness training efficiency. First, propose objective captures tradeoff two metrics, demonstrate jointly optimize principled Bayesian Optimization framework. In addition, account extrinsic hyper-parameters training set size hyper-parameter optimization space. We demonstrate joint optimization measures enriched hyper-parameter space leads selecting efficient accurate models. It important point work fundamentally different previous Bayesian Optimization considers speed hyper-parameter search/model selection process -- focus model training efficiency, addition accuracy , focus hyper-parameter search efficiency. Our work viewed taking efficiency-centric view selecting effective models. Experiments model selection recommendation question answering tasks indicate models selected way significantly improves model training efficiency maintaining strong effectiveness compared state-of-the-art Bayesian Optimization algorithms. The remainder paper organized follows: We start discussion related work. Next, Section propose metrics quantifying tradeoff prediction accuracy training efficiency, discuss methods model selection based tradeoff metric. Section presents experimental results different tradeoff scenarios recommendation question answering tasks, concluding Section. { This paper describes approach proposed SemEval-2020 Task 9: Sentiment Analysis CM Social Media Text . In approach, pre-processed CM tweets proposed Recurrent Convolutional Neural Network sentiment analysis CM tweets. We submitted two runs obtaining promising results: best run obtained 0.691 F1 averaged across positives, negatives neutral. We observed proposed architecture occasionally strives separate positive negative polarities neutral vice versa. For future work, explore performance model larger corpora testing set. Also, would like investigate embedding choices BERT . Moreover, due impact irony sarcasm sentiment analysis would interesting apply deep learning techniques detect irony code-mixed scenario.","   The performance of many machine learning models depends on their hyper-parameter settings. Bayesian Optimization has become a successful tool for hyper-parameter optimization of machine learning algorithms, which aims to identify optimal hyper-parameters during an iterative sequential process. However, most of the Bayesian Optimization algorithms are designed to select models for effectiveness only and ignore the important issue of model training efficiency. Given that both model effectiveness and training time are important for real-world applications, models selected for effectiveness may not meet the strict training time requirements necessary to deploy in a production environment. In this work, we present a unified Bayesian Optimization framework for jointly optimizing models for both prediction effectiveness and training efficiency. We propose an objective that captures the tradeoff between these two metrics and demonstrate how we can jointly optimize them in a principled Bayesian Optimization framework. Experiments on model selection for recommendation tasks indicate models selected this way significantly improves model training efficiency while maintaining strong effectiveness as compared to state-of-the-art Bayesian Optimization algorithms."
"Attention-based transformer networks~ widely used sequence modeling tasks, including language modeling machine translation. To improve performance, models often scaled either wider, increasing dimension hidden layers, deeper, stacking transformer blocks. For example, T5 uses dimension 65K GPT-3 uses 96 transformer blocks. However, scaling increases number network parameters significantly , complicates learning, i.e., models either require large training corpora careful regularization . In paper, introduce new parameter-efficient attention-based architecture easily scaled wide deep. Our ep t-weight ransformer architecture, \arch, extends transformer architecture \citet{vaswani2017attention} delivers similar better performance significantly fewer parameters operations. At heart \arch~is \dextra~that uses group linear transformations \citet{mehta2018pyramidal} expand-reduce strategy varying width depth \arch~block efficiently. Since GLTs local nature, \dextra~uses feature shuffling, analogous channel shuffling convolutional networks , share information different groups. Such wide deep representations facilitate replacing multi-head attention feed-forward layers transformers single headed attention light-weight feed-forward layers, reducing total network parameters operations. Importantly, unlike transformers, \dextra~decouples depth width input size, allowing us allocate parameters efficiently across blocks using shallower narrower \arch~blocks near input deeper wider \arch~blocks near output. We demonstrate \arch~models achieve similar better performance transformer models significantly fewer parameters operations, two common sequence modeling tasks, machine translation language modeling. On low resource WMT'16 En-Ro machine translation dataset, \arch~attains transformer performance using fewer parameters. On high resource WMT'14 En-Fr dataset, \arch~delivers better performance fewer parameters baseline transformers. Similarly, language modeling, \arch~matches performance Transformer-XL~ fewer parameters WikiText-103 dataset. Our source code open-source available at: \textcolor{blue}{\url{https://github.com/sacmehta/delight}} We introduced unified Bayesian Optimization framework jointly optimizing models effectiveness training efficiency. We propose objective captures tradeoff accuracy training efficiency demonstrate jointly optimize measures principled framework. Experiments several real-world model selection rating prediction tasks indicate approach significantly improves model training efficiency maintaining strong effectiveness compared state-of-the-art baseline models."," We introduce a deep and light-weight transformer, \arch, that delivers similar or better performance than standard transformer-based models with significantly fewer parameters. \arch~more efficiently allocates parameters both  within each Transformer block using the \dextra, a deep and light-weight transformation and  across blocks using block-wise scaling, that allows for shallower and narrower \arch~blocks near the input and wider and deeper \arch~blocks near the output. Overall, \arch~networks are 2.5 to 4 times deeper than standard transformer models and yet have fewer parameters and operations. Experiments on benchmark machine translation and language modeling tasks show that \arch~matches or improves the performance of baseline Transformers with 2 to 3 times fewer parameters on average."
"The deep learning community looking alternatives recurrent neural networks storing information. For example, linear memory networks use linear autoencoder sequences memory . Additional memories RNNs like holographic reduced representations , tensor product representations classical associative memories suggested. Most approaches new memories based attention. The neural Turing machine equipped external memory attention process . Memory networks use attention first mapping query patterns space retrieving pattern largest dot product. End end memory networks make attention scheme differentiable replacing . EMN dot products became popular implement key-value attention self-attention. An enhancement EMN transformer extensions . The transformer great impact natural language processing community, particular via BERT models . {\bf Contribution work:} introducing novel deep learning layers equipped memory via modern Hopfield networks, introducing novel energy function novel update rule continuous modern Hopfield networks differentiable typically retrieve patterns one update. Differentiability required gradient descent parameter updates retrieval one update compatible activating layers deep networks. We suggest using modern Hopfield networks store information learned prototypes different layers neural networks. Binary Hopfield networks introduced associative memories store retrieve patterns . A query pattern retrieve pattern similar average similar patterns. Hopfield networks seem ancient technique, however, new energy functions improved properties. The stability spurious states metastable states sensibly reduced . The largest impactful successes reported increasing storage capacity Hopfield networks. In -dimensional space, standard Hopfield model store uncorrelated patterns without errors random patterns fixed stable pattern patterns stable . The bound holds nonlinear learning rules . Using tricks-of-trade allowing small retrieval errors, storage capacity . If learning rule related Hebb rule, patterns stored . For Hopfield networks non-zero diagonal matrices, storage increased . In contrast storage capacity, number energy minima Hopfield networks exponential . The standard binary Hopfield network energy function expressed sum interaction functions . Modern Hopfield networks, also called ``dense associative memory'' models, use energy function interaction functions form and, thereby, achieve storage capacity proportional . The energy function modern Hopfield networks makes robust adversarial attacks . Modern binary Hopfield networks energy functions based interaction functions form even lead storage capacity , stored binary patterns fixed points radius attraction vanishes . However, order integrate Hopfield networks deep learning architectures, necessary make differentiable, is, require continuous Hopfield networks . Therefore, generalize energy function \citet{Demircigil:17} builds exponential interaction functions continuous patterns states obtain new modern Hopfield network. We also propose new update rule ensures global convergence stationary points energy . We prove new modern Hopfield network typically retrieves patterns one update step exponentially low error storage capacity proportional . The retrieval patterns one update important integrate Hopfield networks deep learning architectures, layers activated once. Surprisingly, new update rule also key-value attention used transformer BERT models . Our modern Hopfield networks integrated new layer deep learning architectures pooling, memory, prototype learning, attention. We test new layers different benchmark datasets tasks like immune repertoire classification. This paper introduces deep light-weight transformer architecture, \arch, efficiently allocates parameters within \arch~block across \arch~blocks. Compared state-of-the-art transformer models, \arch~models deep light-weight deliver similar better performance. In future, plan apply \arch~to tasks, including language model pre-training, question answering, language generation. Acknowledgements: This research supported ONR N00014-18-1-2826, DARPA N66001-19-2-403, NSF , Allen Distinguished Investigator Award. Authors would also like thank members UW-NLP H2Lab The University Washington valuable feedback comments. \small{ } \clearpage"," We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store  exponentially  many patterns,  retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima :  global fixed point averaging over all patterns,   metastable states averaging over a subset of patterns, and   fixed points which store a single pattern. The new update rule  is equivalent to the attention mechanism used in transformers. This equivalence enables a  characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging  via metastable states.  The new modern Hopfield network can be integrated  into deep learning architectures  as layers to allow the storage of and access to  raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning,  beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks,  where deep learning methods typically struggle,  Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: \url{https://github.com/ml-jku/hopfield-layers}"
"% Computer Society journal papers something tad strange % first section heading . They place % ABOVE main text! IEEEtran.cls currently you. % However, You achieve effect making LaTeX jump % hoops via something like: % %\ifCLASSOPTIONcompsoc % \raisebox{2\baselineskip}[0pt][0pt]% % {\parbox{\columnwidth}{ In paper, propose DeText ranking framework BERT/CNN based ranking model practical usage industry. To accommodate requirements different ranking productions, DeText allows flexible configuration, input data, text embedding extraction, traditional feature handling, etc. These choices enable us experiment develop scalable neural network models strong relevance performance. Our offline experiments show DeText-LiBERT/DeText-CNN consistently outperforms strong production baselines. The resulting models deployed three vertical searches LinkedIn's commercial search engines. The next two lines define bibliography style used, bibliography file.", %\boldmath %The abstract goes here. %
"% COMPLETED. \acp{KG} represent structured collections facts describing world form typed relationships entities. These collections facts used wide range applications including Web search), cancer research, even entertainment. However, \acp{KG} Web far complete. For instance, birth place persons Freebase persons DBpedia found respective \acp{KG}. In addition, scientists DBpedia linked predicate describes known for. Identifying missing links referred link prediction. \ac{KGE} approaches map \acp{KG} continuous vector spaces proven highly effective efficient addressing task link prediction. In paper, propose \approach, simple effective new \ac{KGE} approach. \approach complex-valued convolutional neural model learns complex-valued vector representations given \ac{KG} combining 2D convolution operation Hermitian inner product. The motivation behind approach lies following considerations: We evaluate approach 37 state-of-the-art approaches four benchmark datasets often used literature. Overall, results suggest \approach outperforms current state-of-the-art approaches , terms \ac{MRR} Hits N .% -- standard measures link prediction task. %structure paper % The rest paper structured follows: % We provide overview state art \ac{KGE} in~\Cref{sec:related work}. Thereafter, notation preliminaries presented in~\Cref{sec:preliminaries}. Next, introduce \approach in~\Cref{sec:approach}. In~\Cref{sec:experiments}, explicate research question experimental settings.~\Cref{sec:results} reports results conducted experiments. Finally, conclude discussion ~\Cref{sec:conclusion}. The conclusion goes here. single appendix: [Proof Zonklar Equations] appendix heading use"," In this paper, we study the problem of learning continuous vector representations of knowledge graphs for predicting missing links. We present a new approach called \approach, which infers missing links by leveraging the composition of a 2D convolution with a Hermitian inner product of complex-valued embedding vectors. We evaluate \approach against state-of-the-art approaches on the WN18RR, FB15K-237, KINSHIP and UMLS benchmark datasets. Our experimental results show that \approach achieves a  performance superior to that of state-of-the-art approaches such as RotatE, QuatE and TuckER on the link prediction task on all datasets while requiring at least 8 times fewer parameters. We ensure the reproducibility of our results by providing an open-source implementation which includes the training, evaluation scripts along with pre-trained models at {\url{https://github.com/conex-kge/ConEx}.}   % \keywords{Knowledge Graph Embeddings \and Link Prediction \and Convolution \and Complex vector space}"
"Bipolar disorder recurrent chronic mental health condition occurs approximately 1\% global population . It characterised episodes low high mood cause significant interference everyday life. Borderline personality disorder characterised long-term pattern constantly variable mood, self-image behaviour. Although BD BPD two different conditions share similar symptoms mood instability impulsive behaviour . A recent study reported high prevalence comorbidity two conditions, 21.6\% individuals BD found comorbid BPD. As result difficult distinguish, accurate diagnosis crucial require different treatment . Standard diagnostic assessment involves psychiatrist asking series questions symptoms person retrospectively describe account symptoms. The success assessment also relies psychiatrist interprets verbal non-verbal cues drawn person's responses. In work, aim develop method extracts cues automatically interviews conducted non-clinical setting, assist existing assessment framework, expensive subjective. Recent studies explored data driven approaches automatically screen patients, incorporating features extracted multiple modalities clinical interviews, showing diagnostic value mental health conditions depression bipolar disorder . finds performance automatic mood detection much better clinical interactions personal conversations, significant differences features important type interaction. While existing studies BD focused recognising mood episodes, distinction BD BPD remains understudied. In paper, aim bridge gap presenting multi-modal dataset containing interviews non-clinical setting involving individuals diagnosis BD BPD, study automatic assessment two mental health conditions. Motivated study interaction interviewer participant course interview different aspects , investigate features extracted different modalities. Path signatures, initially introduced rough path theory branch stochastic analysis, shown successful range machine learning tasks involving modelling temporal dynamics . We propose apply path signatures summarising features extracted utterance, sentence speaker-turn interview-level feature representations, given ability naturally capture order events. By so, automatically include non-linear prior knowledge final feature set, leads effective classification, even simple linear classifier. The contributions work follows: We present new non-clinical interview dataset involving BD BPD patients; , We investigate different feature types propose using path signatures novel approach summarising turn-level features; We demonstrate good linear model learnt three classification tasks, provide insights distinction BD BPD analysing importance selected features. The superior performance \approach stems composition 2D convolution Hermitian inner product complex-valued embeddings. Applying 2D convolution complex-valued embeddings subjects predicates permits \approach recognize interactions subjects predicates form complex-valued feature maps. Through projection feature maps inclusion Hermitian inner product involving conjugate-transpose complex-valued embeddings objects, \approach accurately infer various types relations. For instance, \approach able model composition patterns without defining bijection mapping explicitly. This ability suggested since WN18RR FB15K-237 involve antisymmetric composite relations. Moreover, shows \approach requires significantly fewer parameters RotatE, QuatE TuckER WN18RR FB15K-237. two tables supplemental material explicitly show \approach able capture various types relations benchmark datasets. However, \approach inaccurately ranks entities \texttt{\_member\_of\_domain\_region} \texttt{\_member\_of\_domain\_usage}. This may indicate \approach able model triples subjects objects loosely semantically related. Overall, \approach expressive approaches solely apply 2D convolution solely apply inner products Hermitian Inner Products . \section{Conclusion future work} Completed. In work, introduce new approach addressing link prediction problem learning continuous vector representations knowledge graphs. \approach accurately infers various types relations leveraging composition 2D convolution Hermitian inner product complex-valued embeddings. \approach achieves state-of-the-art performances standard link prediction datasets requiring fewer parameters several state-of-the-art approaches---including QuatE, RotatE TuckER. In future work, plan explore combining 2D convolution Hamilton Quaternions."," Bipolar disorder  and borderline personality disorder  are both chronic psychiatric disorders. However, their overlapping symptoms and common comorbidity make it challenging for the clinicians to distinguish the two conditions on the basis of a clinical interview.  % Recent studies have explored data driven approaches to automatically screen patients, incorporating features extracted from clinical interviews, showing diagnostic value for mental health conditions such as depression and bipolar disorder.  In this work, we first present a new multi-modal dataset containing interviews involving individuals with BD or BPD being interviewed about a non-clinical topic . We investigate the automatic detection of the two conditions, and demonstrate a good linear classifier that can be learnt using a down-selected set of features from the different aspects of the interviews and a novel approach of summarising these features. Finally, we find that different sets of features  characterise BD and BPD, thus providing insights into the difference between the automatic screening of the two conditions."
"Despite impressive improvements neural machine translation , training large multilingual NMT model hundreds millions parameters usually requires collection parallel corpora large scale, order millions even billions aligned sentences~ supervised training. Although possible automatically crawl web~ collect parallel sentences high-resource language pairs German-English Chinese-English, often infeasible expensive manually translate large amounts documents low-resource language pairs, e.g., Nepali-English, Sinhala-English~. Much recent progress low-resource machine translation, driven idea universal machine translation , also known multilingual machine translation~, aims training one single NMT translate multiple source target languages. Typical UMT models leverage either single shared encoder language-specific encoders map source languages shared space, translate source sentences target language decoder. Inspired idea UMT, recent trend towards learning language-invariant embeddings multiple source languages shared latent space, eases cross-lingual generalization high-resource languages low-resource languages many tasks, e.g., parallel corpus mining~, sentence classification~, cross-lingual information retrieval~, dependency parsing~, name few. The idea finding abstract ``lingua franca'' intuitive empirical results impressive, yet theoretical understanding various aspects universal machine translation limited. In paper, particularly focus two basic questions: Toward answering first question, show completely assumption-free setup languages distribution data, impossible avoid making large translation error least one pair translation tasks. Informally highlight first theorem follows, provide formal statements Theorems . To answer second question, show fairly mild generative assumptions aligned documents pairwise translations, possible well pairwise translations, also able seeing aligned documents linear number languages, rather quadratic one. We summarize second theorem follows, provide formal statement Theorem. \paragraph{Notation Setup} We first introduce notation used throughout paper briefly describe problem setting universal machine translation. We use denote set possible languages, e.g., . For language , associate alphabet contains symbols . Note assume , , different languages could potentially share part alphabet. Given language , sentence sequence symbols , denote set sentences generated . Note since principle different languages could share alphabet, avoid ambiguity, language , unique token . The goal unique token used denote source sentence, sentence unique prefix indicate . Also, manuscript use sentence string interchangeably. Formally, let \footnote{We use denote set .} set source languages target language interested translating to. For pair languages , use denote joint distribution parallel sentence pairs . Given joint distribution, also use mean marginal distribution sentences . Likewise use denote corresponding marginal distribution sentences . Finally, two sets , use denote disjoint union . In particular, disjoint, disjoint union equals usual set union, i.e., . In paper, demonstrate potential using features extracted language speech non-clinical interviews assist assessment bipolar disorder BD borderline personality disorder BPD, challenging clinicians distinguish. It crucial diagnose two conditions accurately patients appropriate treatment. While many machine learning based studies learn clinical interviews automatically screen mental health conditions, detection BD BPD still understudied. We first presented non-clinical interview dataset, named AMoSS-I, conducted partially psychology graduates, task detecting BD BPD. We demonstrated good performance three classification tasks using down-selected features new way summarising features based path signatures. Lastly, showed importance linguistic features three tasks benefits feature fusion different modalities. For future work, plan learn acoustic features, investigate effect acoustic properties interviews impact recording environments."," The goal of universal machine translation is to learn to translate between any pair of languages, given a corpus of paired translated documents for a small subset of all pairs of languages. Despite impressive empirical results and an increasing interest in massively multilingual models, theoretical analysis on translation errors made by such universal machine translation models is only nascent.   In this paper, we formally prove certain impossibilities of this endeavour in general, as well as prove positive results in the presence of additional  structure of data.   For the former, we derive a lower bound on the translation error in the many-to-many translation setting, which shows that any algorithm aiming to learn shared sentence representations among multiple language pairs has to make a large translation error on at least one of the translation tasks, if no assumption on the structure of the languages is made.   For the latter, we show that if the paired documents in the corpus follow a natural encoder-decoder generative process, we can expect a natural notion of ``generalization'': a linear number of language pairs, rather than quadratic, suffices to learn a good representation. Our theory also explains what kinds of connection graphs between pairs of languages are better suited: ones with longer paths result in worse sample complexity in terms of the total number of documents per language pair needed.   We believe our theoretical insights and implications contribute to the future algorithmic design of universal machine translation."
"Knowledge Based Systems:\\ Systems incorporate human expertise making decisions knowledge-based systems . Traditionally knowledge-based system consists knowledge base data suitably collected organised human experts various fields, inference engine - relies knowledge base decision making, working memory handle operations. The inference engine rule-based, case-based, etc.\\ Deep Neural Networks:\\ Deep neural networks, hand, statistical modelling relies massive amounts data find statistical patterns, non-linear relationships able match prediction patterns given training set. It relies patterns infer conclusions new data well.\\ Time-series models:\\ A Recurrent neural network class neural network deals prediction temporal sequences. Long-Short term memory , Gated Recurrent Units Recurrent neural network architectures used time series forecasting.\\\\ Sequence Sequence models:\\ Sequence sequence models aims translate fixed-length input sequence fixed-length output sequence length input output may differ. It mainly three parts: encoder, intermediate vector decoder. In encoder, several stacks recurrent units combined unit accepts input element sequence propagates it, thus forming intermediate hidden state. This information hidden state consumed decoder part network turn consists sequences recurrent units produce sequence outputs. \subsection{Synergy Knowledge base systems Deep neural networks} Although Deep neural networks shown promising performance several fields, exist areas like interpretability, reasoning lack hence needs attention. On hand, Expert Systems built top characteristics Deep neural networks lack. Hence ways leverage strengths systems various principles. This paper discusses techniques integrating expert knowledge Deep Neural Networks attain kind synergy them. In paper, propose Context Reinforced Neural Topic Model address feature sparsity problem short texts. By introducing topic controller inference network, CRNTM infers topic word narrow range. Besides, pre-trained word embeddings incorporated multivariate Gaussian distributions Gaussian mixture distributions model enrich context information short messages. To quantitatively validate effectiveness CRNTM, conduct various experiments two benchmark datasets terms perplexity, topic coherence, text classification accuracy. The results indicate proposed model largely improves performance topic modeling enriching context information effectively."," In recent years, with the advent of massive computational power and the availability of huge amounts of data, Deep neural networks have enabled the exploration of uncharted areas in several domains. But at times, they under-perform due to insufficient data, poor data quality, data that might not be covering the domain broadly,  etc. Knowledge-based systems leverage expert knowledge for making decisions and suitably take actions. Such systems retain interpretability in the decision-making process. This paper focuses on exploring techniques to integrate expert knowledge to the Deep Neural Networks for sequence-to-sequence and time series models to improve their performance and interpretability.   \keywords{Deep Neural Network, TimeSeries, Sequence-to-Sequence models, Expert Knowledge}"
"A heuristic approach automated test case generation formal requirements specifications known learning-based testing introduced , . Learning-based testing iterative approach automate specification-based black-box testing. It encompasses test case generation, execution evaluation . The aim LBT automatically generate large number high-quality test cases combining model checking algorithm optimised model inference algorithm . For procedural reactive systems shown LBT significantly outperform random testing speed finds errors system test . This random test suites generally contain large degree redundancy, reduced using learning algorithms model checkers execute directed search software errors. An efficient practical implementation learning-based testing reactive systems developed LBTest tool . In paper describe IKL algorithm implemented LBTest. IKL algorithm active incremental learning deterministic Kripke structures. The reliability LBTest producing correct test results depends crucially correctness learning algorithm. So give formal definition IKL prove correctness. The IKL algorithm involves number optimisations necessary achieve scalability testing large software systems. We discuss optimisations perspective learning testing. The problems coverage, termination criteria black-box testing, complex different solutions proposed. In LBT, convergence learning sometimes used criterion terminate testing. However, heuristics needed estimate convergence context black box testing. We empirically evaluate reliability simple heuristic IKL. In remainder Section 1, discuss general paradigm LBT, specific requirements learning efficient testing reactive systems. In Section 2, review essential mathematical preliminaries. In Section 3, present architecture IKL learning algorithm main components. These three main components defined analysed detail Sections 4, 5 6. In Section 4, consider learning algorithm families DFA supports incremental learning projection . In Section 5, consider integrating family DFA single Kripke structure using subdirect product construction. In Section 6, consider efficient minimisation algorithm deterministic Kripke structures based Hopcroft's DFA minimisation algorithm . This needed IKL algorithm produce hypothesis models efficiently model checked. In Section 7, empirically evaluate black box heuristic detect convergence IKL, used test termination criterion. Finally, Section 8 draw conclusions suggest prospects research learning testing. \subsection{Learning-Based Testing} The basic LBT paradigm requires three components: \vskip 4pt system test , \vskip 4pt formal requirements specification , \vskip 4pt learned model . \vskip 4pt Now common specification-based testing, really distinctive. Learning-based testing heuristic iterative method automatically generate sequence test cases. The heuristic concept learn black-box system using tests queries. In general, LBT algorithm iterates following four steps: \vskip 4pt Suppose test case inputs executed yielding system outputs . The input/output observations synthesized learned model using incremental learning algorithm . This step involves generalization observed behaviour, possible behaviour. This generalisation step gives possibility predict previously unseen errors Step 2. \vskip 4pt The system requirements checked learned model derived Step 1 . This process searches counterexample requirements. \vskip 4pt The counterexample executed next test case , terminates output obtained. If fails test case \Reqi_{n+1}Si_{n+1}M_ni_{n+1}Si_{n+1}n+1 \c \ldots \c M_{n+1}SS1 \dotsM_0M_1M_2\ldotsSni_{n+1}i_{n+1}\ReqL\Aut_0 \c \Aut_1 \c \ldots \Aut\Aut\Aut_0 \c \Aut_1 \c \ldots \Aut\Aut_{i+1}L\Aut_{i}SS\Req\Req\Req\Req\ReqS\ReqS\Req\ReqSS\mathcal O\mathcal O\mathcal O$. Our generalisation Hopcroft's DFA minimisation algorithm deterministic Kripke structures Section 6 fairly simple straightforward. Nevertheless, algorithm previously published literature, represents another novel contribution. In paper, discussed techniques integrating expert knowledge form First-order Logic Rules, tuples, embeddings etc neural network time-series sequence-to-sequence models. While technique set pros cons, would optimal come scalable technique incorporate positive aspects above-discussed techniques."," Learning-based testing  is an emerging methodology to automate iterative black-box requirements testing of software systems. The methodology involves combining model inference with model checking techniques. However, a variety  of optimisations on model inference are necessary in order to achieve scalable testing for large systems.  In this paper we describe the IKL learning algorithm which is an active incremental learning algorithm for deterministic Kripke structures.  We formally prove the correctness of IKL. We discuss the optimisations it incorporates to achieve scalability of testing. We also evaluate a black box heuristic for test termination based on convergence of IKL learning."
"Since early attempts pretrain backbone model large-scale dataset transfer knowledge numerous computer vision tasks, pretraining become hallmark success deep learning. More recently, volume transformer-based Bert-style pretraining models grown tremendously research field natural language processing achieved state-of-the-art performance various NLP tasks. Likewise, success Bert-style pretraining techniques transferred research field intersection vision language . %--------------------------------fig------------------------- %--------------------------------fig end--------------------- Despite significant progress recent methods made initiative work ViLBert , part success traced back introduction in-domain pretraining datasets besides Conceptual Caption dataset. By in-domain, refer datasets used pretraining downstream tasks, MSCOCO , Visual Genome . However, out-of-domain pretraining, \ie, pretraining models out-of-domain datasets transferring learned knowledge downstream tasks unkown data distributions, essential research topic. In paper, focus out-of-domain pretraining learning generic representations ViLBert does. A fundamental requirement out-of-domain transfer learning mitigate biases pretraining data , may useful in-domain testing harmful out-of-domain testing due spurious correlation . To verify existence correlation biases, follow conduct toy experiment Conceptual Caption dataset. We observe conditional probability given large, \ie, , actually robust relationships them. Most previous works blame biased data collection without justification. However, reasonable since human living biased nature. In methodology, draw inspiration causal inference borrow idea backdoor adjustment mitigate biases. As shown Figure , traditional association-based learning fashion may lead spurious correlation two tokens common cause, \ie, confounder. By introducing backdoor adjustment , original conditional probability adjusted operator. The essence deconfounding control condition affected potential confounders assessing effect outcome given condition, \ie, intervention. In way, pure association-based pretraining becomes causal intervention-based pretraining. We note goal performing theoretically causal inference learning generic de-biased visio-linguistic representations well generalize downstream tasks unknown data distributions. We particularly targeting Bert-style pretraining models context-based proxy tasks supervision, masked language/object modeling . Context-based proxy tasks solely care association, \ie, co-occur anchor token without considering whether spurious correlations not. More formally, masked token modeling, abbreviated MTM, models conditional probability distribution observing . masked token denotes context information. The spurious correlation occurs confounded common cause , depicted Figure . Our goal model interventional operation , meaning distribution controlling mitigate correlation bias introduced before. Real-world cases concerning conditional probabilities corresponding intervention results Conceptual Captions dataset found Figure . In paper, propose several intervention-based BERT architectures help learn deconfouned visio-linguistic representations. We name kind architectures DeVLBert, refers extbf{Deconfounded Visio-Linguisitic Bert}. DeVLBert designed model-agnostic easily encapsulated Bert-style models. We conduct in-depth experiments discuss performance proposed DeVLBert architectures. Pretraining performed Conceptual Caption dataset downstream tasks built on, \ie, out-of-domain dataset. We evaluate effects architectures three downstream cross-modal tasks, including text-to-image retrieval , zero-shot text-to-image retrieval, visual question answering . We also conduct case studies evaluate DeVLBert human perspective, demonstrate mitigating dataset biases boosts generalization ability. The main contributions work summarized follows: We defined analysed learning algorithm IKL deterministic Kripke structures efficient applications software testing. This algorithm extends active incremental learning new features lazy learning projection. We formally proved correctness IKL algorithm main components. We also empirically evaluated black box heuristic detecting convergence learning, used terminate testing small systems test. Incremental learning projection combine make IKL scalable larger systems test. Also, incremental lazy learning combine support frequent generation hypothesis automata discover SUT errors much faster random testing model checking. These claims empirically evaluated supported . The IKL algorithm implemented LBTest tool learning based testing reactive systems. We believe efficiency learning-based testing even improved research model inference. For example, modular architecture IKL algorithm support experiment incremental DFA learning algorithms instead ID learning algorithm Section 4, . The impact frequency hypothesis automata generation testing efficiency could investigated. When hypothesis generation frequent overhead model checking high, overhead slow entire LBT process. However, generation infrequent, little use made model checker conduct directed search SUT errors using queries falsify user requirements. This also inefficient. More generally, could consider optimal tuning rate hypothesis automata generation, e.g. based estimated density SUT errors. The relationship computational learning software testing fruitful line research ever since Weyuker's thesis . Many fundamental questions remain within context learning-based testing. For example, execution automata learning algorithm always associated prefix tree construction based query set used. How influence choice breadth-first depth-first search SUT errors using prefix tree? This currently achieved ad-hoc pragmatic balance IKL model checker queries active learning queries. Another important question whether find techniques generate active learner queries besides congruence construction? Such techniques aimed reducing need random queries, inefficient practise. We gratefully acknowledge financial support research Swedish Research Council , Higher Education Commission Pakistan, European Union project HATS FP7-231620. ---- Bibliography ---- expects file ""myrefs.bib""","  In this paper, we propose to investigate the problem of out-of-domain visio-linguistic pretraining, where the pretraining data distribution differs from that of downstream data on which the pretrained model will be fine-tuned. Existing methods for this problem are purely likelihood-based, leading to the spurious correlations and hurt the generalization ability when transferred to out-of-domain downstream tasks. By spurious correlation, we mean that the conditional probability of one token  given another one can be high  without robust  relationships between them. To mitigate such dataset biases, we propose a Deconfounded Visio-Linguistic Bert framework, abbreviated as DeVLBert, to perform intervention-based learning. We borrow the idea of the backdoor adjustment from the research field of causality and propose several neural-network based architectures for Bert-style out-of-domain pretraining. The quantitative results on three downstream tasks, Image Retrieval , Zero-shot IR, and Visual Question Answering, show the effectiveness of DeVLBert by boosting generalization ability."
"% Thanks development generative modeling, algorithmic music generation made possible. % In recent years, instead relying rule-based systems plain time-series analysis, seen work using recurrent networks attention-based model generate music comparable human professional. % Despite capacity models, lack interpretability remains main obstacle controllable music generation ---in particular polymonic music much richer structures. With development artificial neural networks, deep learning become one popular techniques automated music generation. In particular, see recurrent attention-based models able generate creative human-like music without heavily handcrafted rules . %compared traditional time-series models rule-based algorithms. % lulebaseeviewer However, main drawback deep generative models behave like ``black boxes, difficult interpret musical meaning internal latent variables . Consequently, remains challenging task control generation process . This limitation restricts application scenario powerful deep generative models. In paper, improve model interpretability music generation via constrained representation learning. Inspired content-style disentanglement idea , enforce model learn two fundamental factors polyphonic music: chord texture . The former refers representation underlying chord progression, latter includes chord arrangement, rhythmic pattern, melody contour. The current design focuses learning 8-beat long piano composition segments variational autoencoder framework. The core model design lies encoder. We incorporate encoder two inductive biases successful chord-texture disentanglement. The former applies rule-based chord recognizer embeds information first half latent representation. The latter regards music 2-D images uses chord-invariant convolutional network extract texture information, storing second half latent representation. As decoder, adopt design PianoTree VAE , architecture reconstruct polyphonic music latent representation hierarchical manner. We show interpretable representations general-purpose, empowering wide spectrum controllable music generation. In study, explore following three scenarios: In sum, contributions paper follows: In paper, propose mitigate spurious correlations out-of-domain visio-linguistic pretraining. The fact output token connected input tokens Bert, pure association nature masked token modeling objective makes problem severe. We borrow idea back-door adjustment propose four novel Bert-style architectures DeVLBert out-of-domain pretraining. We conduct extensive quantitative evaluations well ablation studies discuss empirical effectiveness different architectures. The results show DeVLBert achieve promising numerical results compared baseline even in-domain visio-linguistic pretraining methods."," % While deep generative modeling has become promising in many domains, it remains a challenging task to algorithmically compose polymeric music,  essentially hindered by its rich structure. % Inspired by the recent work of disentanglement of factors of variations, we develop a novel architecture, under the VAE framework, that not only disentangles the chord and texture of an input polymeric segment, also provides a generation pathway leading to plausible music style transfer and analogy. % Through a wide spectrum of task validations, we show that the chord-texture resulted from our model enables several tasks including compositional style transfer, texture variation, chord progression interpolation and accompaniment arrangement. % By both automatic metrical and human-based evaluation, our method achieves the state-of-the-art quality on the music generation. While deep generative models have become the leading methods for algorithmic composition, it remains a challenging problem to control the generation process because the latent variables of most deep-learning models lack good interpretability. Inspired by the content-style disentanglement idea, we design a novel architecture, under the VAE framework, that effectively learns two interpretable latent factors of polyphonic music: chord and texture. The current model focuses on learning 8-beat long piano composition segments. We show that such chord-texture disentanglement provides a controllable generation pathway leading to a wide spectrum of applications, including compositional style transfer, texture variation, and accompaniment arrangement. Both objective and subjective evaluations show that our method achieves a successful disentanglement and high quality controlled music generation.\!\!\footnote{Code and demos can be accessed via \url{https://github.com/ZZWaang/polyphonic-chord-texture-disentanglement}}"
"Humans learn use language course lives interactions world people. Yet, prevailing dominant paradigm natural language processing research build fixed dataset train model freeze it, without ability model interact humans using language training time all. While need interaction order study human-machine communication full extent, constraints usually inhibit research. Firstly, conducting experiments costly. %, %for example research budgets paying crowdworkers mean data limit. Many datasets NLP collected crowdsourcing, whereby one pays crowdworkers perform interaction annotation tasks. This leads several issues, least research budgets paying crowdworkers mean data limit. %collecting large amount data difficult %Secondly, distribution motivated money, actual interest dialogues themselves. Secondly, crowdworkers motivated pay, interest actual tasks themselves, data distribution may match desired one . In work study ability open-domain}. %We show iterative collection-retraining/redeployment % open source everything %Finally, considerably challenging engage unpaid humans provide high quality dialogue conversing dialogue models. %* *Never-Ending Learning: show it improving* % * future ML/AI/NLP fixed datasets, continual interactive learning %* game purpose %* Side points: % * Price ) % * Distribution % * Deployment leads collecting data, natural place evaluate compare models % * games ideal testbed AI, w/ rich human interaction, grounding, sandbox % * things like alexa challenge allow fully deployed system, proprietary nature privacy concerns, research open reproducible. %Collect data, evaluate models. % In conclusion, contributed effective algorithm disentangle polyphonic music representation two interpretable factors, chord texture, VAE framework. Such interpretable representations serve intuitive human-computer co-creation interface, precisely manipulate individual factors control flow generated music. In paper, demonstrated three ways interact model, including compositional style transfer via swapping latent codes, texture variation sampling latent distribution, accompaniment arrangement using downstream conditional prediction, potentially many more. We hope work shed light field controllable algorithmic composition general, especially paradox model complexity model interpretability. We acknowledge learned music factors still basic. In future, plan extract abstract longer-range features using hierarchical models. We also plan explore ways control music generation practical usage. For bibtex users:"," Much of NLP research has focused on crowdsourced static datasets and the supervised learning paradigm of training once and then evaluating test performance.  As argued in \citet{de2020towards}, crowdsourced data has the issues  of lack of naturalness and relevance to real-world use cases, while the static dataset paradigm does not allow for a model to learn from its experiences of using language \cite{silver2013lifelong}. % %We posit that, in order to overcome these issues, machine learning must develop systems where models continually improve by interacting with humans and the world. % % In order to overcome these issues, machine learning must develop systems where models continually improve by interacting with humans and the world.  In contrast, one might hope for machine learning systems that become more useful as they interact with people. % In this work, we build and deploy a %  role-playing game, whereby human players converse with learning agents situated in an open-domain fantasy world. We show that by training models on the conversations they have with humans in  the game the models progressively improve, as measured by automatic metrics and online engagement scores. This learning is shown to be more efficient than crowdsourced data when applied to conversations with real users, as well as being far cheaper to collect.  %We are releasing the models and data from this work."
"Recently, deep learning witnessed great process . Video question answering become emerging task computer vision drawn increasing interests past years due vast potential applications artificial question answering system robot dialogue, video retrieval, etc. In task, robot required answer question watching video. Unlike well-studied Image Question Answering task focuses understanding static images, video QA practical since input visual information often change dynamically, shown Figure . Compared image QA, video QA much challenging due several reasons. Visual content complex video since may contain thousands frames, shown Figure . More importantly, frames may dominated strong background content however irrelevant questions. Videos often contain multiple actions, part interest questions. Questions video QA task often contain queries related temporal cues, implies consider temporal location objects complex interaction answer reasoning. For example Figure , answer question ``What man spinning bucket?"", robot recognize actions ``spin laptop'' ``spin bucket'' understanding interaction man objects different frames, also find temporal order actions answer reasoning along time axis. Taking video frames inputs, existing methods employ spatio-temporal attention mechanism frame features ask network ``where look''. However, methods often robust due complex background content videos. % However, extracting features whole frame makes model prone over-fit background content . Lei et al. tackle problem detecting objects frame processing sequence object features via LSTM. However, order input object sequence, may affect performance, difficult arrange. More importantly, processing objects recurrent manner inevitably neglect direct interaction nonadjacent objects. This critical video QA . In paper, introduce simple yet powerful network named Location-aware Graph Convolutional Networks model interaction objects related questions. We propose represent content video graph identify actions graph convolution. % propose explicitly detect salient objects videos model relationship constructing fully-connected graph. Specifically, objects interest first detected off-the-shelf object detector. Then, construct fully-connected graph node object edges nodes represent relationship. We incorporate spatial temporal object location information node, letting graph aware object locations. When performing graph convolution object graph, objects directly interact passing message edges. Last, output GCNs question features fed visual-question interaction module predict answer. Extensive experiments demonstrate effectiveness proposed location-aware graph. We achieve state-of-the-art results TGIF-QA, Youtube2Text-QA MSVD-QA datasets. The main contributions proposed method follows: propose explore actions video QA task learning interaction detected objects irrelevant background content explicitly excluded; propose model relationships objects GCNs objects able interact directly; propose incorporate object location information graph network aware location specific action; method achieves state-of-the-art performance TGIF-QA, Youtube2Text-QA MSVD-QA datasets. We presented fully realized system improving upon open-domain dialogue task open-domain dialogue utilizing deployed game lifelong learning. Detailed experiments showed one collect high quality data improves automatic offline metrics user engagement metrics used training models. We find exciting approach shows possible build continually improving models learn interacting humans wild , represents paradigm shift away limited static dataset setup prevalent much work community. Future work study resulting publicly released data explore methods lifelong learning, learning signals could extracted human utterances, example ideas \citet{hancock2019selffeeding}. Another possible direction, model performance begins saturate, exploit control game engine emphasize learning difficult cases ones learning signal, work adversarial collection . Finally, role-playing setup also applied generally, example incorporating dialogue actions, situated domains.","           We addressed the challenging task of video question answering, which requires machines to answer questions about videos in a natural language form. Previous state-of-the-art methods attempt to apply spatio-temporal attention mechanism on video frame features without explicitly modeling the location and relations among object interaction occurred in videos. However, the relations between object interaction and their location information are very critical for both action recognition and question reasoning.  In this work, we propose to represent the contents in the video as a location-aware graph by incorporating the location information of an object into the graph construction. Here, each node is associated with an object represented by its appearance and location features. Based on the constructed graph, we propose to use graph convolution to infer both the category and temporal locations of an action.  		As the graph is built on objects, our method is able to focus on the foreground action contents for better video question answering.  Lastly, we leverage an attention mechanism to combine the output of graph convolution and encoded question features for final answer reasoning. 		Extensive experiments demonstrate the effectiveness of the proposed methods. Specifically, our method significantly outperforms state-of-the-art methods on TGIF-QA, Youtube2Text-QA and MSVD-QA datasets. Code and pre-trained models are publicly available at:  % 		\textcolor{red}{\tt{https://github.com/SunDoge/L-GCN}}         \url{https://github.com/SunDoge/L-GCN}"
"The world seeing paradigm shift way conduct daily activities amidst ongoing coronavirus pandemic - online learning, way socialize, interact, conduct businesses shopping. Such global catastrophes direct effect social life; however, cultures react respond way given crisis. Even normal circumstances, research suggests people across different cultures reason differently . For instance, Nisbett book ""The geography thought: How Asians Westerners think differently... why"" stated East Asians think basis experience dialectically holistically, Westerners think logically, abstractly, analytically . This cultural behavior attitude mostly governed many factors, including socio-economic situation country, faith belief system, lifestyle. In fact, COVID-19 crisis showed greater cultural differences countries seem alike respect language, shared history culture. For example, even though Denmark Sweden two neighboring countries speak almost language share lot culture history, stand extreme ends spectrum comes way reacted coronavirus . Denmark Norway imposed robust lockdown measures closing borders, schools, restaurants, restricting gathering social contact, side, Sweden taken relaxed approach corona outbreak keeping schools, restaurants, borders open. Social media platforms play essential role extreme crisis individuals use communication channels share ideas, opinions, reactions others cope react crises. Therefore, study, focus exploring collective reactions events expressed social media. Particular emphasis given analyzing people's reactions global health-related events especially COVID-19 pandemic expressed Twitter's social media platform widespread popularity ease access using API. To end, tweets collected thousands Twitter users communicated within four weeks corona crisis analyzed understand different cultures reacting responding coronavirus. Additionally, extended version publicly available tweets dataset also used. A new model sentiment emotion analysis proposed. The model takes advantage natural language processing deep neural networks comprises two main stages. The first stage involves sentiment polarity classifier classifies tweets positive negative. The output first stage used input emotion classifier aims assign tweet either one positive emotions classes one negative emotions classes . Figure shows abstract model proposed system sentiment emotion analysis tweets' text. \subsection{Study Objective \& Research Questions} Our primary objective study understand different cultures behave react given global crisis. The state questions addressed cultural differences techno-social system reveals potentialities societal attitudinal, behavioral, emotional predictions. In present investigation, examine behavioral emotional factors describe societies react different circumstances, general objective analyze potential utilizing NLP-based sentiment emotional analysis techniques finding answers following research questions . \subsection{Contribution} The major contributions article following: {} The rest article organized follows. Section presents research design study dimensions. Related work presented section . Data collection procedure data preparation steps described section , whereas, sentiment emotion analysis model presented section . Section entails results followed discussion analysis section . Lastly, section concludes paper potential future research directions. In paper, proposed location-aware graph model relationships detected objects video QA task. Compared existing spatio-temporal attention mechanism, \algname able explicitly get rid influences irrelevant background content. Moreover, network aware spatial temporal location events, important predicting correct answer. Our method outperforms state-of-the-art techniques three benchmark datasets. , including TGIF-QA, Toutube2Text-QA MSVD-QA."," How different cultures react and respond given a crisis is predominant in a society's norms and political will to combat the situation. Often the decisions made are necessitated by events, social pressure, or the need of the hour, which may not represent the will of the nation. While some are pleased with it, others might show resentment. Coronavirus  brought a mix of similar emotions from the nations towards the decisions taken by their respective governments. Social media was bombarded with posts containing both positive and negative sentiments on the COVID-19, pandemic, lockdown, hashtags past couple of months. Despite geographically close, many neighboring countries reacted differently to one another. For instance, Denmark and Sweden, which share many similarities, stood poles apart on the decision taken by their respective governments. Yet, their nation's support was mostly unanimous, unlike the South Asian neighboring countries where people showed a lot of anxiety and resentment. This study tends to detect and analyze sentiment polarity and emotions demonstrated during the initial phase of the pandemic and the lockdown period employing natural language processing  and deep learning techniques on Twitter posts. Deep long short-term memory  models used for estimating the sentiment polarity and emotions from extracted tweets have been trained to achieve state-of-the-art accuracy on the sentiment140 dataset. The use of emoticons showed a unique and novel way of validating the supervised deep learning models on tweets extracted from Twitter."
"Knowledge transfer rapidly growing advancing research area AI. As humans, live world In practice, observed models become bigger performance grows. % Out commercial needs, committed improve privacy , security user experience . There many reasons reducing size models important, real-time inference efficiency deployment mobile devices. Transferring knowledge large, powerful models compact models proven practical solution. Clearly, knowledge transfer agents tasks important component towards AGI. %%%%%%%%%%%%%%%%%%%%%%%% %%%%%%%%%%%%%%%%%%%%%% What knowledge transfer? In general, regarded learning process learners acquire new modify existing knowledge interacting teachers. If represent knowledge probabilistic distribution, e.g., \nonumber \end{align} Thus, learning means get closer , distributions learner teacher. It's natural relate knowledge transfer KL-divergence measures much information lost approximating distribution another. Existing approaches categorized minimizing KL-divergence order. Learners infinite capacity would end behaving exact way teachers. In case, order matter. Unfortunately, true. In practice, learners typically restricted simple function family, e.g., Gaussian, parameterized neural networks finite layers hidden units. Meanwhile, teachers perfect. noisy especially high-dimensional action space. The question order performs better? Previous works attempt analyze difference restricting Gaussian distributions. However, learning process, i.e., learner interact teacher acquires knowledge, totally unclear. In paper, reinterpret KL-divergence minimization knowledge transfer provide in-depth analysis way learner acquires knowledge teacher, without applying constraints distributions. At high level, noticed encourages learners exposed distributions reinforced on-policy learning. Thus, propose minimize solving sequential decision making problems. To verify hypothesis, revisit Knowledge Distillation attempts distill knowledge teacher learner minimizing Neural Machine Translation task. The goal task generate sequence tokens target language given sentence source language. The generation procedure is: position , given previous actions , try produce token expected give maximum reward . Basically, NMT solve sequential decision making problem high-dimensional, discrete action space. Exposure bias known important issue due lacking exploration. We simply replace call approach Knowledge Acquisition . We describe learning process dialog, shown Fig.. Empirical results show +0.7-1.1 BLEU gains WMT'17 De-En IWSLT'15 Th-En tasks. % related work This paper aimed find correlation sentiments emotions people within neighboring countries amidst coronavirus outbreak tweets. Deep learning LSTM architecture utilizing pre-trained embedding models achieved state-of-the-art accuracy Sentiment140 dataset emotional tweet dataset used detecting sentiment polarity emotions users' tweets Twitter. Initial tweets right pandemic outbreak extracted tracking trending hasthtags\# February 2020. The study also utilized publicly available Kaggle tweet dataset March - April 2020. Tweets six neighboring countries analyzed, employing NLP-based sentiment analysis techniques. The paper also presents unique way validating proposed model's performance via emoticons extracted users' tweets. We cross-checked detected sentiment polarity emotions via various published sources number positive cases reported respective health ministries published statistics. Our findings showed high correlation tweets' polarity originating USA Canada, Pakistan India. Whereas, despite many cultural similarities, tweets posted following corona outbreak two Nordic countries, i.e., Sweden Norway, showed quite opposite polarity trend. Although joy fear dominated two countries, positive polarity dropped average Norway much earlier Swedes. This may due lockdown imposed Norway good month half Government decided ease restrictions, whereas, Swedish Government went herd immunity, equally supported Swedes. Nevertheless, average number positive tweets higher average number negative tweets Norway. The trend observed Pakistan Canada, positive tweets negative ones. We observed number negative positive tweets started dropping average sentiments first second week April six countries. This study also suggests NLP-based sentiment emotion detection help identify cross-cultural trends also plausible link actual events users' emotions expressed social platforms high certitude, despite socio-economic cultural differences, high correlation sentiments expressed given global crisis - case coronavirus pandemic. Deep learning models hand enriched semantically rich representations using ontology presented effectively grasping one's opinion tweets. Moreover, advanced seq2seq type language models word embedding explored future work. Till date , pandemic still rising parts world, including Brazil Russia. It would interesting observe extended patterns tweets across countries detect assert people's behavior dealing calamities. We hope believe study provide new perspective readers scientific community interested exploring cultural similarities differences public opinions given crisis, could influence decision makers transforming developing efficient policies better tackle situation, safe-guarding people's interest needs society. {}"," Knowledge Transfer has been applied in solving a wide variety of problems. For example, knowledge can be transferred between tasks  or between agents . Without loss of generality, we relate knowledge transfer to KL-divergence minimization, i.e., matching the  distributions of learners and teachers. The equivalence gives us a new perspective in understanding variants of the KL-divergence by looking at how learners structure their interaction with teachers in order to acquire knowledge.  In this paper, we provide an in-depth analysis of KL-divergence minimization in \texttt{Forward} and \texttt{Backward} orders, which shows that learners are reinforced via on-policy learning in \texttt{Backward}. In contrast, learners are supervised in \texttt{Forward}. Moreover, our analysis is gradient-based, so it can be generalized to arbitrary tasks and help to decide which order to minimize given the property of the task. By replacing \texttt{Forward} with \texttt{Backward} in Knowledge Distillation, we observed +0.7-1.1 BLEU gains on the WMT'17 De-En and IWSLT'15 Th-En machine translation tasks."
"Digital humanities transdisciplinary subject information technologies humanities, literary classics. For instance, Google makes contribution digital humanities promoting ``Google Books Library Project'' includes millions paper books scanned electronic text . Digital text easier researchers explore printed books, since development information technology provided numerous effective tools . In past decade, overwhelming data science techniques advanced research digital humanities; thus, components extracted analyzed literature. A review previous research reveals areas digital humanities remain unexplored. First, mainstream studies limited humanities works background Western world . It interesting constructive investigate humanities works oriental backgrounds. Second, comparative studies literature different styles story conducted . Particularly, previous researches focused longitudinal studies, wherein researchers usually adopt story series, Harry Potter Books 17, object study . A potential research interest story discovers varied features sentiments arise different literature, may driven literary genres authors opinion, among others. Third, network study essential social network story network possesses topological structure, help gain insight story characters based grand narration . To fill gap, paper introduces social network sentimental analysis work two different texts one famous Chinese story, The Three Kingdoms. In particular, leverage state-of-the-art natural language processing -based model extract social networks narratives two books. A series descriptive statistical analysis extracted networks conducted, discover homogeneity heterogeneity terms topological features networks. Additionally, adopt sentimental analysis compare evaluations main characters. The results reveal social network complicated narrative novel historical text . Consequently, concluded literariness stories tight relationship complexity social networks entail. The main contribution paper follows: The remainder paper organized follows. In Section , backgrounds text mining social network analysis researches presented. Section elaborates network extraction approach. We perform series empirical studies Section demonstrate thesis work. Finally, paper concluded Section summary potential future studies. In paper took close look knowledge transfer used improve capabilities neural network model sequence generation task using another model known stronger . While focused improving single learning model single teacher model, future work worth exploring joint learning system agents learners different roles, cooperate compete accomplish task. We explored details learning process optimizing KL-divergence forward backward orders. We found \texttt{Backward} allows learners acquire knowledge efficient way, especially solving sequential decision making problems. Our analysis general applicable tasks. We believe would guide us utilize KL-divergence effectively. \nocite{langley00} DELETE THIS PART. DO NOT PLACE CONTENT AFTER THE REFERENCES! special case"," Digital humanities is an important subject because it enables developments in history, literature, and films. In this paper, we perform an empirical study of a Chinese historical text, Records of the Three Kingdoms , and a historical novel of the same story, Romance of the Three Kingdoms . We employ natural language processing techniques to extract characters and their relationships. Then, we characterize the social networks and sentiments of the main characters in the historical text and the historical novel. We find that the social network in Romance is more complex and dynamic than that of Records, and the influence of the main characters differs. These findings shed light on the different styles of storytelling in the two literary genres and how the historical novel complicates the social networks of characters to enrich the literariness of the story."
"As unsupervised approach, topic modelling enjoyed great success automatic text analysis. In general, topic model aims discover set latent topics collection documents, describes interpretable semantic concept. Topic models like Latent Dirichlet Allocation ~ hierarchical/Bayesian extensions, e.g., in~\citet{blei2010nested,paisley2015nested,gan2015learning,zhou2016augmentable} achieved impressive performance document analysis. Recently, developments Variational AutoEncoders Autoencoding Variational Inference ~ facilitated proposal Neural Topic Models in~\citet{miao2016neural,srivastava2017autoencoding,krishnan2018challenges,burkhardt2019decoupling}. Inspired VAE, many NTMs use encoder takes Bag-of-Words representation document input approximates posterior distribution latent topics. The posterior samples input decoder reconstruct BoW representation. Compared conventional topic models, NTMs usually enjoy better flexibility scalability, important applications large-scale data. Despite promising performance recent popularity, several shortcomings existing NTMs, could hinder usefulness extensions. i) The training inference processes NTMs typically complex due prior posterior constructions latent topics. To encourage topic sparsity smoothness, Dirichlet~ gamma~ distributions usually used prior posterior topics, reparameterisation inapplicable them, thus, complex sampling schemes approximations used, could limit model flexibility. ii) A desideratum topic model generate better topical representations documents coherent diverse topics; many existing NTMs, hard achieve good document representation coherent/diverse topics time. This objective NTMs achieve lower reconstruction error, usually means topics less coherent diverse, observed analysed in~. iii) It well-known topic models degrade performance severely short documents tweets, news headlines product reviews, individual document contains insufficient word co-occurrence information. This issue exacerbated NTMs use encoder decoder networks, vulnerable data sparsity. To address shortcomings NTMs, paper propose neural topic model, built upon novel Optimal Transport framework derived new view topic modelling. For document, consider content encoded two representations: observed representation, , distribution words vocabulary latent representation, , distribution topics. obtained normalising document's word count vector needs learned model. For document collection, vocabulary size large one individual document usually consists tiny subset words. Therefore, sparse low-level representation semantic information document. As number topics much smaller vocabulary size, relatively dense high-level representation content. Therefore, learning topic model viewed process learning distribution close distribution possible. Accordingly, crucial investigate measure distance two distributions different supports. As optimal transport powerful tool measuring distance travelled transporting mass one distribution match another given specific cost function, recent development computational OT shown promising feasibility efficiently compute OT large-scale problems, natural us develop new NTM based minimisation OT. Specifically, model leverages encoder outputs topic distribution document taking word count vector input like standard NTMs, minimise OT distance , two discrete distributions support words topics, respectively. Notably, cost function OT distance specifies weights topics words, define distance embedding space, embed topics words represent semantics. By leveraging pretrained word embeddings, cost function function topic embeddings, learned jointly encoder. With advanced properties OT modelling geometric structures spaces probability distributions, model able achieve better balance obtaining good document representation generating coherent/diverse topics. In addition, model eases burden designing complex sampling schemes posterior NTMs. More interestingly, model natural way incorporating pretrained word embeddings, demonstrated able alleviate issue insufficient word co-occurrence information short texts~. With extensive experiments, model shown enjoy state-of-the-art performance terms topic quality document representations regular short texts. In work, proposed multitask learning-based approach predict depressed users Sina Weibo. First, based data collection script filtering manual labeling, built publish large Weibo User depression detection dataset - WU3D. The total number user samples reaches 30,000 user enriched information fields. This dataset quite sufficient used subsequent researchers complete research. Secondly, summarized manually extracted ten statistical features including text, social behavior, picture-based features. The experimental results showed varying degrees distribution differences normal users depressed users, contribute positively classification tasks. Our experimental results also proved feature engineering process text information vital part depression detection OSN. Furthermore, evaluated performance pretrained model XLNet embedding model solve downstream classification tasks. It showed appropriate embedding length selected, XLNet excellent performance efficiency handling long text sequences. Finally, implemented multitask learning DNN classifier, FusionNet, simultaneously handle word vector classification task statistical feature classification task. Benefit strategic advantages multitask learning, FusionNet reduced loss feature information caused transfer learning. Compared commonly used models existing work, FusionNet achieved significant performance improvement F1-Score 0.9772 showed best classification robustness training samples unbalanced. Thus, proven ideal classification model dealing multiple classification tasks time. For future work, two directions explored. The size dataset expanded. Larger datasets constructed training evaluating classifiers achieve better generalization performance. The characteristics behavior patterns depressed users analyzed. We propose effective feature solutions user-level depression detection OSN. Bibliography"," Recently, Neural Topic Models  inspired by variational autoencoders have obtained increasingly research interest due to their promising results on text analysis. However, it is usually hard for existing NTMs to achieve good document representation and coherent/diverse topics at the same time. Moreover, they often degrade their performance severely on short documents. The requirement of reparameterisation could also comprise their training quality and model flexibility. To address these shortcomings, we present a new neural topic model via the theory of optimal transport . Specifically, we propose to learn the topic distribution of a document by directly minimising its OT distance to the document's word distributions. Importantly, the cost matrix of the OT distance models the weights between topics and words, which is constructed by the distances between topics and words in an embedding space. Our proposed model can be trained efficiently with a differentiable loss. Extensive experiments show that our framework significantly outperforms the state-of-the-art NTMs on discovering more coherent and diverse topics and deriving better document representations for both regular and short texts."
"Even advent COVID-19 pandemic, people across world turning internet find answers medical concerns . Around 7\% Google daily searches health related, equivalent around 70,000 queries every minute . With emergence medical question-answering websites ADAM , WebMD , AskDocs HealthTap , people opportunity ask detailed questions find answers, experts, satisfied needs. COVID-19 done nothing accelerate trend. Almost every government agency healthcare organization tried meet informational need users building online FAQs try address many COVID-related topics possible %With ubiquity Internet emergence medical question-answering websites ADAM , WebMD , HealthTap , people increasingly searching online answers medical questions. Pew Internet Project surveys consistently find 75-83\% internet users look online health information . The examples already illustrate two important problems medical Q\&A collection: large number possible questions formulated different ways, easy user browse large collection pre-existing questions find one resembles need. A scalable solution overcome issues build system automatically match user formulated questions semantically similar answered questions, provide suggestions users. If similar answered questions exist, mark priority experts respond. This approach directly satisfies user needs allowing use words formulate question. It also provides avenue collecting unanswered questions users want answered, extremely important rapidly changing situation currrent COVID-19 pandemic. %However, number people asking medical questions online far exceeds number qualified experts -- i.e doctors -- answering them. A scalable solution overcome imbalance build system automatically match unanswered questions semantically similar answered questions, provide suggestions users. When similar answered questions exist, mark priority doctors respond. This approach uses doctor time efficiently, reducing number unanswered questions lowering cost providing online care. %Many individuals seeking medical advice online otherwise reluctant seek medical help due cost, convenience, embarrassment. For patients, accurate online system critical may medical advice receive. Of course, medical problems require in-person care, online system must indicate that. Other patients use internet addition in-person care either determine appointment needed follow visits lingering questions. For second group, answers see online match given doctors, less likely follow advice doctors , serious consequences. The problem matching general unanswered questions semantically similar answered questions well-studied context online user forums , community QA question answer archives . Typical approaches either assume large amount training data which, either statistics computed models learned. However, approaches fall short applied problem medical question similarity. First, medical questions imbibe large amount medical information single word completely change meaning question. As example, I pregnant I believe Ie infected coronavirus. What I know going hospital? Should I visit doctor I expecting think I might COVID-19? similar questions low overlap, Is safe take Vitamin D3 supplements build immunity Coronavirus? Is safe take Hydroxychloroquine build immunity Coronavirus? critically different couple words apart. Second, publicly available medical question-question similarity data scale differences effectively encoded order learn reliable similarity function. In fact, hypothesize constructing large datasets cover large functional space nuanced variations medical domain quite hard, scalable proposition. %Coming accurate algorithm finding similar medical questions, however, difficult. Simple heuristics word-overlap ineffective Can menstrual blood clot travel heart lungs like blood clots can? Can clots period cause stroke embolism? similar questions low overlap, Is candida retested treatment Is Chlamydia retested treatment? critically different one word apart. Machine learning good candidate complex tasks, requires labeled training data. As widely available data particular task exists, generate release dataset medical question pairs ones shown Table. % \TODO{Can least add COVID-19 related example?} \end{center} \end{table} % \footnote{We acknowledge fails edge cases. For instance, answers two questions ""Yes, correct"", mean questions similar.} In paper, tackle general problem medical question-question similarity, assuming small amount labeled data similarity pairs. We also apply general solution specific COVID-19 scenario many different questions different sources integrated user-friendly experience. Our proposed solution stems two key insights: First, whether two questions semantically similar akin asking whether answer one also answers other. This means answers answered questions contain wealth medical knowledge distilled model. The second insight infuse medical knowledge answers pretraining task within language model, capture relatedness words/concepts language. Recent success pretrained bi-directional transformer networks natural language processing non-medical fields supports insight . % In examples above, answer question Can clots period cause stroke embolism? talk about, instance, `menstrual blood' `bleeding' establishes relationships `period' `menstrual blood'. Similar connection established heart, lungs embolism. In contrast, answers around candida treatment likely discuss yeast around Chlamydia bacteria. %The second insight infuse medical knowledge answers pre-training task within language model, capture relatedness words/concepts language. Recent success pre-trained bi-directional transformer networks natural language processing non-medical fields supports insight . % \TODO{Can find COVID-19 related example?} %Given recent success pre-trained bi-directional transformer networks natural language processing outside medical field , research efforts medical NLP tried apply general language models medical tasks . However, models trained medical information, make errors reflect this. Our approach stems augmenting general language model BERT, medical knowledge process double fine-tuning first distills medical knowledge using large corpus relevant in-domain task medical question-answer pairs. Subsequently, fine-tunes available small corpus question-question similarity dataset. Our models pretrained medical question-answer pairs outperform models pretrained out-of-domain question similarity high statistical significance. In particular, pretraining tasks yield accuracy 78.7\% task, model achieves accuracy 82.6\% number training examples, accuracy 80.0\% much smaller training set, accuracy 84.5\% full corpus medical question-answer data used. % Furthermore, results show promise generalizing domains well. We present early results extensibilty approach another expert domain: question-question similarity context community driven question answer website Ubuntu operating system. %The task question-answer matching specifically chosen closely related question similarity; one component whether two questions semantically similar whether answer one also answers other. We show performance gains achieved particular task realized in-domain tasks, medical question-categorization medical answer completion. %However, labeled training data still one largest barriers supervised learning, particularly medical field expensive get doctor time hand-labeling data. } The main contributions paper are: The rest paper structured follows: \S describes methodology used creating dataset made publicly available. \S provides overview approach. \S describes used model build service matches user's COVID-19-related questions FAQs published online. \S describes experimental details key results, % \S gives peek application methodology domains. \S discusses related work end discussion future work. In paper, presented novel neural topic model based optimal transport, document endowed two representations: word distribution, , topic distribution, . An OT distance leveraged compare semantic distance two distributions, whose cost function defined according cosine similarities topics words embedding space. obtained encoder takes input trained minimising OT distance . With pretrained word embeddings, topic embeddings learned minimisation OT distance terms cost function. Our model shown appealing properties able overcome several shortcomings existing neural topic models. extensive experiments conducted, showing model achieves state-of-the-art performance discovering quality topics deriving useful document representations regular short texts. Thanks flexibility simplicity framework, future work developing extensions variants discover complex topic patterns e.g, like Correlated Topic Models~ Dynamic Topic Models~. \numberwithin{equation}{section} \counterwithin{figure}{section} \counterwithin{table}{section}"," People increasingly search online for answers to their medical questions but the rate at which medical questions are asked online significantly exceeds the capacity of qualified people to answer them. This leaves many questions unanswered or inadequately answered. Many of these questions are not unique, and reliable identification of similar questions would enable more efficient and effective question answering schema. COVID-19 has only exacerbated this problem. Almost every government agency and healthcare organization has tried to meet the informational need of users by building online FAQs, but there is no way for people to ask their question and know if it is answered on one of these pages. While many research efforts have focused on the problem of general question similarity, these approaches do not generalize well to domains that require expert knowledge to determine semantic similarity, such as the medical domain. In this paper, we show how a double fine-tuning approach of pretraining a neural network on medical question-answer pairs followed by fine-tuning on medical question-question pairs is a particularly useful intermediate task for the ultimate goal of determining medical question similarity. While other pretraining tasks yield an accuracy below 78.7\% on this task, our model achieves an accuracy of 82.6\% with the same number of training examples, an accuracy of 80.0\% with a much smaller training set, and an accuracy of 84.5\% when the full corpus of medical question-answer data is used. We also describe a currently live system that uses the trained model to match user questions to COVID-related FAQs. %We also present early experimental evidence suggesting the applicability of our proposed approach on another completely different domain: question-question similarity in the context of community driven question and answer website for the Ubuntu operating system."
"% Alternative first paragraph: %The goal acoustic scene classification identify class given audio recording, e.g., park, office, library. The ASC task challenging sounds within certain scenes similar characteristics, sound events overlap one another. The growing interest solving ASC problem, confirmed high participation researchers academia industry recent IEEE Detection Classification Acoustic Scenes Events challenge , justified impact robust ASC system several real-world applications. For instance, hearing aid devices modify behaviour accordingly different acoustic envijironments. % If paragraph becomes first paragraph, could reduced, simply say deep learning greatly improved performance ASC. Although many different solutions proposed years, interested reader referred official DCASE website, key elements successful ASC system CNN, data-augmentation, attention, mix-up. % Then need make clear device mismatch problem received less attention, proposal put forth, example . You clarify key problem right away say want address Knowledge distillation. The third paragraph look good needs polished perhaps trimmed bit. Instead, contribution must make stronger. What new work people pay attention it. %The goal acoustic scene classification identify class given audio recording, e.g., park, airport, metro station . The ASC task challenging sounds within certain scenes similar characteristics, sound events overlap one another. The growing interest solving ASC problem, indicated high participation researchers academia industry recent IEEE Detection Classification Acoustic Scenes Events challenges , justified impact robust ASC system real-world applications. For instance, hearing aid devices could modify behaviour accordingly different acoustic environments. In recent years, witnessed great progress acoustic scene classification task, demonstrated high participation IEEE Detection Classification Acoustic Scenes Events challenges . Top ASC systems use deep neural networks , main ingredient success application deep convolutional neural networks . Further boost ASC performance obtained introduction advanced deep learning techniques, attention mechanism , mix-up , Generative Adversial Network Variational Auto Encoder based data augmentation , deep feature learning . Nevertheless, ASC systems yet work well processing audios mismatched domain, e.g., audios recorded different devices . Device mismatch inevitable problem real production, therefore important aspect handle deploying ASC system. Indeed, new sub-task, namely Task1b, added DCASE 2018 foster research direction. The goal design system attain good performance 10-second audios segments collected target devices, either represented development phase, represented ASC system deployment scarce amount training material compared available source device. However, Task1b attracted minor interest among DCASE 2018 2019 participants, even fewer teams directly concerned device mismatch issue. %There exist approaches proposed tackle domain invariant problem ASC. For example, multi-instance learning , low-level mid-level feature learning , transfer knowledge across domains thereby tackle robustness issue broader sense. In literature, exist approaches tackle domain invariant problem ASC. For example, multi-instance learning , low-level mid-level feature learning , however address robustness issue broader sense. Less approaches instead proposed directly combat ASC device mismatch issue, actually focus present work. In particular, spectrum correction channel conversion build front-end module convert speech features source domain target domain feeding back-end classifier. Besides front-end features, mid-level feature based transfer systems, uses bottleneck features hidden layer representations adopted transfer knowledge source target domain. Adversarial training methods leverage extra domain discriminator solve device mismatch problem although key focus lack labeled target data. %Although mentioned techniques beneficial ASC robustness issue, yet clear gap source target device classification results. %In work, device mismatch problem investigated within Teacher-student learning, also named knowledge distillation , recently shown effective ASC domain adaptation speech tasks, e.g., . The key idea minimizes distance measurement teacher student model output distributions, i.e., information transferred soft-label level. %, namely class posterior probabilities, embedded structure relationships among output classes, usually used transfer knowledge teacher model student model. %In recent years, researchers propose relational learning . It directly models relationships sample pairs teacher student model. In , relational knowledge distillation demonstrated improve knowledge distillation process. RKD takes account relations outputs rather individual outputs themselves. %Independently whether relationships among outputs taken account, TS methods require effective soft labels accurately generated; otherwise, information encoded labels meaningless. %Among TS learning methods, There necessary condition get good effects, soft label must accurate enough, otherwise information encoded soft labels dose make senses. %As consequence, Unfortunately, conventional TS learning applied success if: source target data similar domain , source target data come pair although belong different domains . Neural label embedding , recently proposed , ingenues solution distill knowledge across domains neither aforementioned two requirements could met. %NLE embedding label level encode structural relationships among pair output classes deep neural models. Structural relationships turn represent measurements similarity dissimilarity among pairs objects distances points low-dimensional space. Label embedding viewed centroid soft labels class. NLE viewed centroid soft labels class. As extension soft labels, encodes knowledge distilled source domain teacher model, transferred target domain. %%More information build NLE given Section . In , NLE applied accent children's adaptation automatic speech recognition. %In work, extend NLE design started deploy NLE teacher-student adaptation approach combat ASC robustness problem presence source target device mismatch. In study, extend NLE adaptation scheme taking account relationships among different acoustic scenes adaptation. We achieve goal proposing relational teacher student learning approach based NLE ASC device mismatching problem. First, NLE learned relatively large-size source data set, i.e., collected source devices. %The source device data encodes structural relationships among different acoustic scenes. Next, ASC system adapted target device leveraging upon target domain data only, i.e., teacher-student learning unpaired data, set NLE, one per acoustic scene class. The proposed solution assessed DCASE 2018 Task1b data. Experimental results confirm intuitions demonstrate adaptation technique generates significant classification improvement target domain data. Indeed, NLE-based TS adaptation outperforms multi-device training strategies, conventional TS adaptation schemes. Furthermore, additional boost obtained TS adaptation carried leveraging structural information. %In work, solve device mismatching problem ASC systemss, focus structural relationship among scene classes. We propose novel NLE relational teacher student learning approach solve domain mismatch problem ASC. At first, label embedding learned relatively large-size source domain data, encode structural relationship information classes. Then system target domain data trained label embedding criterion including relationship loss. Our proposed approached evaluated DCASE2018 task1b development data. The experimental results verify methods obtain significant improvement target domain data. And visualization verify arguments structural relationships. %The rest work organized follows: Section describes NLE, including generation use. The relational TS learning framework described Section. Next, Section shows experimental results analysis. Finally, Section concludes work. \TODO{Expand slightly align cfp} \\ In work, release MQP, dataset medical question pairs generated labeled doctors based upon real, patient-asked questions. We also show double finetuning approach pretraining in-domain question-answer matching particularly useful difficult task identifying semantically similar questions. Furthermore, show choice in-domain task matters: choosing task provides ample signal capture domain knowledge needed able perform final task well. Although QA model outperforms out-of-domain same-task QQP model, examples QQP model seems learned information missing QA model. In future, explore whether two models learned independently useful information pretraining tasks. If did, hope able combine features one model multi-task learning. An additional benefit error analysis better understanding types mistakes even best model making. It therefore easier use weak supervision augmentation rules even active learning supplement datasets increase number training examples difficult regions data. Both improvements could improve performance task. Lastly, would note system deployed live also needs incorporate safety considerations respect identifying questions user's life threatened might need immediate attention doctor .\documentclass[sigconf]{acmart} \documentclass[sigconf, anonymous, review]{acmart} \pdfoutput=1 \BibTeX command typeset BibTeX logo docs \AtBeginDocument{ \providecommand\BibTeX{{ \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}} Rights management information. This information sent complete rights form. These commands SAMPLE values them; responsibility author replace commands values provided complete rights form. \setcopyright{acmcopyright} \copyrightyear{2020} \acmYear{2020} \setcopyright{acmlicensed}\acmConference[KDD '20]{Proceedings 26th ACM SIGKDD Conference Knowledge Discovery Data Mining}{August 23--27, 2020}{Virtual Event, CA, USA} \acmBooktitle{Proceedings 26th ACM SIGKDD Conference Knowledge Discovery Data Mining , August 23--27, 2020, Virtual Event, CA, USA} \acmPrice{15.00} \acmDOI{10.1145/3394486.3412861} \acmISBN{978-1-4503-7998-4/20/08} \acmDOI{10.1145/1122445.1122456} \settopmatter{printacmref=true} These commands PROCEEDINGS abstract paper. \acmConference[Woodstock '18]{Woodstock '18: ACM Symposium Neural Gaze Detection}{June 03--05, 2018}{Woodstock, NY} \acmBooktitle{Woodstock '18: ACM Symposium Neural Gaze Detection, June 03--05, 2018, Woodstock, NY} \acmPrice{15.00} \acmISBN{978-1-4503-XXXX-X/18/06} \usepackage{times} \usepackage{float} \usepackage{enumitem} \setlength{\parskip}{0cm} \setlength{\parindent}{1em} \setlength{\abovecaptionskip}{5pt} \setlength{\belowcaptionskip}{-5pt} \usepackage[compact]{titlesec} \usepackage[para]{footmisc} \titlespacing{"," %The device domain mismatch issue is an important problem of acoustic scene classification  for real-world applications. To leverage this problem, we focus on the knowledge transfer of the inner structural relationships between each classes. A label embedding with relational teacher student learning approach is proposed. Embedded labels are learned from the source domain data, which encodes the structural relationships. Then a relational teacher student learning framework is used to transfer knowledge. Our proposed approach is evaluated on DCASE2018 task1b data set. And the experimental and visualized results successfully verify our augment and proposed method, which significantly improve the classification accuracy on target device data, with the knowledge transferred from the source device data.  %  Alternative 1: %In this work, we use a model adaptation approach based on  neural label embedding  and  knowledge distillation to combat the accuracy drop in acoustic scene classification with deep neural networks caused by a mismatch between  development  and production  audio recording devices. The proposed adaptation approach works with unpaired source-target data and leverages upon NLE designed to take into account the relationships among acoustic scene classes. The NLE thereby not only condenses a representation of the DNN output distribution given all audio recordings aligned with the same output class but also captures the inherent relationships among acoustic scene classes. Device adaptation is carried out using relational teacher-student learning  solely based on target data, target labels, source DNN, and NLE. The latter serve as soft targets for DNN adaptation.  The proposed approach is assessed against the DCASE 2018 task1b dataset. Experimental evidence confirm the effectiveness our our approach, which compares favourably to conventional device adaptation, and traditional teacher-student based adaptation. Moreover, we observe that NLE based on structural information lead to superior ASC  results than NLE obtained with symmetric Kullback-Leibler divergence ,which do not take into account the relationships among acoustic scene classes.  % Alternative 2 In this paper, we propose a domain adaptation framework to address the device mismatch issue in acoustic scene classification leveraging upon neural label embedding  and relational teacher student learning .  Taking into account the structural relationships between acoustic scene classes, our proposed framework captures such relationships which are intrinsically device-independent. In the training stage, transferable knowledge is condensed in NLE from the source domain. Next in the adaptation stage, a novel RTSL strategy is adopted to learn adapted target models without using paired source-target data often required in conventional teacher student learning. The proposed framework is evaluated on the DCASE 2018 Task1b data set. Experimental results based on AlexNet-L deep classification models confirm the effectiveness of our proposed approach for mismatch situations. %when training with Device A data and testing with data recorded with Devices B and C.  NLE-alone adaptation compares favourably with the conventional device adaptation and teacher student based adaptation techniques. NLE with RTSL further improves the classification accuracy."
"%Motivate bit ASR side %Introduce bit punctuation problem. The output text generated automatic speech recognition systems typically devoid punctuation sentence formatting. Lack sentence segmentation punctuation makes difficult comprehend ASR output. For example, consider two sentences: ``Let's eat Grandma'' vs. ``Let's eat, Grandma!''. Punctuation restoration helps understand context text also greatly improves readability. Punctuated text often helps boosting performance several downstream natural language understanding tasks.% There plethora work done punctuation prediction past decades. While early methods punctuation prediction used finite state hidden markov models , techniques investigated probabilistic models like language modeling , conditional random fields maximum entropy models . As neural networks gained popularity, several approaches proposed based sequence labeling neural machine translation . These models widely used convolutional neural networks LSTM based architectures . More recently, attention transformer based architectures successfully applied wide variety tasks, shown perform well punctuation prediction. Although well explored problem literature, improvements directly translate domains. In particular, punctuation prediction conversational speech well explored . Also, number approaches proposed exploiting use acoustic features addition lexical features punctuation task, rather limited clearly address gap performance ASR outputs. In paper, focus multimodal semi-supervised deep learning approach punctuation prediction conversational speech leveraging pretrained lexical acoustic encoders. %two set approaches emerged. One approach tags every word punctuation following punctuation mark treating sequence labeling problem . The second approach uses machine translation based sequence sequence models generate punctuated text unpunctuated text . \subsection{Relation prior multimodal work} While several methodologies used either text acoustic information predicting punctuation, many studies show combining features yields best performance . Acoustic features widely used literature include prosodic information pause duration, phone duration, pitch related values like fundamental frequency, energy. shows using acoustic information lead increased recognition full stops. In , hierarchical encoder used encode per frame acoustic features word level features results show incorporating acoustic features significantly outperform purely lexical systems. However, trained large independent text corpus, lexical system outperformed multimodal system trained parallel audio/text corpora. To mitigate this, work introduced speech2vec embeddings vary respect acoustic context reference speech. In general, identify two potential shortcomings aforementioned multimodal systems. First, training still suboptimal due lack large-scale parallel audio/text corpora. Secondly, models trained reference text transcripts perform well ASR outputs, although incorporating acoustic features reduced gap extent. \subsection{Novelty work} %And tell approach different acoustic based approaches. %We therefore focus investigating benefits exploiting semi-supervised learning approach. In work, introduce novel framework multimodal fusion lexical acoustic embeddings punctuation prediction conversational speech. Specifically, investigate benefits using lexical acoustic encoders pretrained large amounts unpaired text audio data using unsupervised learning. The key idea learn contextual representations unsupervised training substantial amounts unlabeled data available improve performance downstream task like punctuation, amount data limited, leveraging learned representations. For multimodal fusion, explore attention mechanism automatically learn alignment word level lexical features frame level acoustic features absence explicit forced alignments. We also show adaptation proposed multimodal architecture streaming usecase limiting future context. We study effect pretrained encoders respect varying data sizes performance trained small amounts data. Finally, exploit N-best lists ASR perform data augmentation reduce gap performance tested ASR outputs. % We investigate following research questions paper: % %The rest paper organized follows: Section introduces semi-supervised learning approach pre-trained lexical acoustic encoder punctuation prediction. Section describes procedure fusion acoustic features lexical encoder. We discuss experimental setup Section results presented Section . Finally, Section , summarize conclusions. In paper, relational teacher student learning framework neural label embedding proposed resolve device mismatch issue acoustic scene classification. We explore similarities dissimilarities pairs classes. This structural relationship learned encoded NLE transferred source device domain target device domain via relational teacher-student approach. Our proposed framework assessed DCASE 2018 Task1b development data set, experimental results demonstrate viability approach, also significant improvement classification accuracy target device data obtained. Furthermore, visual analysis provided shed light key characteristics proposed neural label embedding concept. \clearpage","  In this work, we explore a multimodal semi-supervised learning approach for punctuation prediction by learning representations from large amounts of unlabelled audio and text data. Conventional approaches in speech processing typically use forced alignment to encoder per frame acoustic features to word level features and perform multimodal fusion of the resulting acoustic and lexical representations. As an alternative, we explore attention based multimodal fusion and compare its performance with forced alignment based fusion. Experiments conducted on the Fisher corpus show that our proposed approach achieves $\sim$6-9\% and $\sim$3-4\% absolute improvement  over the baseline BLSTM model on reference transcripts and ASR outputs respectively. We further improve the model robustness to ASR errors by performing data augmentation with N-best lists which achieves up to an additional $\sim$2-6\% improvement on ASR outputs. We also demonstrate the effectiveness of semi-supervised learning approach by performing ablation study on various sizes of the corpus. When trained on 1 hour of speech and text data, the proposed model achieved $\sim$9-18\% absolute improvement over baseline model.   %We also incorporate a pretrained lexical BERT encoder to further enhance the hidden representation of acoustic embedding when performed fusion with lexical embedding."
"Contemporary end-to-end speech synthesis systems achieve great results produce natural-sounding human-like speech even real time . They make possible efficient training put high demands quality, amount, preprocessing training data. Based advances, researchers aim at, example, expressiveness , controllability , few-shot voice cloning . When extending models support multiple languages, one may encounter obstacles different input representations pronunciations, imbalanced amounts training data per language. In work, examine cross-lingual knowledge-sharing aspects multilingual text-to-speech . We experiment languages simultaneously previous TTS work %\TN{nen moc siln tvrzen?}\OD{to nown us trochu omezuje :-)... ale mem napsat  works jestli chce known us. We summarize contributions follows: We propose scalable grapheme-based model utilizes idea contextual parameter generator network compare baseline models using different levels parameter sharing. We introduce new small dataset based Common Voice includes data five languages 84 speakers. We evaluate effectiveness compared models ten languages three different scripts show code-switching abilities five languages. For purposes evaluation, created new test set 400 bilingual code-switching sentences. % \OD{TODO v se chlubit} \TN{Je v?} % \TNdel{\OD{todo nov  dataset}} % \TNdel{\OD{nov code-switch test set}} Our source code, hyper-parameters, training evaluation data, samples, pre-trained models, interactive demos freely available GitHub.\footnote{\url{https://github.com/Tomiinek/Multilingual\_Text\_to\_Speech} } We introduced novel multimodal semi-supervised learning framework leverages large amounts unlabelled audio text data punctuation prediction. We proposed alternative attention based multimodal fusion mechanism effective, absence forced alignment word durations. Through data sizes ablation study, showed proposed model superior performance lexical models reference transcripts. In order address performance gaps ASR outputs, presented robust model less affected ASR errors performing data augmentation N-best lists.","   We introduce an approach to multilingual speech synthesis which uses the meta-learning concept of contextual parameter generation and produces natural-sounding multilingual speech using more languages and less training data than previous approaches. Our model is based on Tacotron~2 with a fully convolutional input text encoder whose weights are predicted by a separate parameter generator network. To boost voice cloning, the model uses an adversarial speaker classifier with a gradient reversal layer that removes speaker-specific information from the encoder.      We arranged two experiments to compare our model with baselines using various levels of cross-lingual parameter sharing, in order to evaluate:  stability and performance when training on low amounts of data,  pronunciation accuracy and voice quality of code-switching synthesis. For training, we used the CSS10 dataset and our new small dataset based on Common Voice recordings in five languages. Our model is shown to effectively share information across languages and according to a subjective evaluation test, it produces more natural and accurate code-switching speech than the baselines.      %The only drawback of this article is its unappealing abstract. It is probably because there includes no explicit description of the research objectives or the challenges that were dealt with in this study.  The reviewer thus thinks that improving the abstract will make this paper perfect.            %\OD{TODO pformulovat tak,  se soustde na n model, pdat info o datasetu}   %This work explores cross-lingual knowledge sharing in the context of speech synthesis. We compare three models based on Tacotron that utilize various levels of parameter sharing. Two of them follow recent multilingual text-to-speech systems. The first makes use of a fully-shared encoder and an adversarial classifier that ought to remove speaker-dependent information from the encoder. The other uses language-specific encoders. We introduce a new approach that combines the best of both previous methods. It enables effective parameter sharing using a meta-learning technique, preserves encoder flexibility, and actively removes speaker-specific information in the encoder.      %We compare the three models on two tasks. The first aims at joint multilingual training on ten languages. The second concerns code-switching or voice cloning. We show that our model effectively shares information across languages, and according to a subjective evaluation test, it produces more natural and accurate code-switching speech."
"Peking Opera, also known Beijing Opera Jingju, Chinese traditional performing art combines music, vocal performance, mime, dance acrobatics. Singing Peking Opera various styles, widely different depending different role type music styles. Strong personal styles also make actual singing different given music notes. Like dialect Mandarin, even unique way pronunciation. Moreover, melody singing often consist arias variation complex transitory vibratos, makes singing expressive difficult learn. Another difference normal singing note length great variance, sometime long note appear . All factors makes challenging modelling generating Peking Opera singing comparing normal singing. Although works focusing synthesis Peking Opera, broadly, opera, synthesis singing voice researched since 1962 Kelly Lochbaum used acoustic tube model synthesis singing voice success. Recently, several works use deep neural networks synthesis singing voice which, known parametric systems, process fundamental frequency harmonics features separately. As typical case among systems, Neural Parametric Singing Synthesizer using phoneme timing model, pitch model timbre model consist set neural networks generate acoustic parameters singing. In NPSS, Fitting Heuristic method introduced eliminate mismatch music note duration predicted phoneme duration. However, Fitting Heuristic method totally rule based requires locate principal vowel adjusting phoneme duration. This maybe acceptable English Japanese singing cases, cause huge duration error synthesizing Peking Opera. Different normal speech singing, Peking Opera, one syllable last long time contains long sequence phonemes, e.g. ``l-j-E-a-a-N"". More importantly, one can't simply tell phoneme amongst phonemes principle phone. There could multiple equally important phonemes Peking Opera singing. To better synthesize expressive Peking Opera, paper proposes Peking Opera singing synthesis system based Duration Informed Attention Network . The main contribution study lies two following points: 1) To tackle rhythm mismatch music note duration predicted phoneme duration, contextual based mixture density networks followed Lagrange Multiplier optimization proposed implemented duration modelling. This method completely data-driven, importantly, skips step locating principle phoneme conventional Fitting Heuristic method. 2) To deal melody mismatch original music score actual singing, also better model expressive variations vibratos Peking Opera, pseudo music score generated real singing fed input DurIAN model training. Experimental Results show proposed duration modeling prediction method outperforms Fitting Heuristic method large margin. And generated pitch contours also demonstrate system's ability synthesize singing variations vibratos Peking Opera. The following sections paper organized follows. Firstly, proposed model architecture introduced. Next, proposed Lagrange Multiplier-based duration prediction pseudo score generation introduced Section 2. In section 3 experiments conducted based unique Peking Opera database. Finally, quick discussion conclusion given Section 4. \TN{Prepsat todlecto vsecko} We introduced grapheme-based TTS model uses meta-learning approach. We compared two models two tasks . It shown scale work effectively even data-stress situations. It enables cross-lingual code-switching, basic voice cloning, limited pronunciation control five languages, demonstrated companion repository. We presented new grapheme-based model uses meta-learning multilingual TTS. We showed significantly outperforms multiple strong baselines two tasks: data-stress training code-switching, model favored voice fluency well pronunciation accuracy. Our code available GitHub.\textsuperscript{} For future work, consider changes model's attention module improve accuracy.","  Peking Opera has been the most dominant form of Chinese performing art since around 200 years ago. A Peking Opera singer usually exhibits a very strong personal style via introducing improvisation and expressiveness on stage which leads the actual rhythm and pitch contour to deviate significantly from the original music score. This inconsistency poses a great challenge in Peking Opera singing voice synthesis from a music score. In this work, we propose to deal with this issue and synthesize expressive Peking Opera singing from the music score based on the Duration Informed Attention Network  framework. To tackle the rhythm mismatch, Lagrange multiplier is used to find the optimal output phoneme duration sequence with the constraint of the given note duration from music score. As for the pitch contour mismatch, instead of directly inferring from music score, we adopt a pseudo music score generated from the real singing and feed it as input during training. The experiments demonstrate that with the proposed system we can synthesize Peking Opera singing voice with high-quality timbre, pitch and expressiveness."
"Many machine learning datasets label imbalance dataset bias problem. In many cases, either data harder collect certain classes data collection phase biased bias introduced collected dataset. Typical training algorithms, optimized order minimize error, tend exacerbating bias, e.g., providing higher recall precision majority class minority classes. Therefore, label imbalance problem raises concern fairness machine learning systems general. Spoken language understanding problems often suffer label imbalance, ways may hide important errors designers SLU systems. Consider SLU dataset Air Traffic Information Systems speech-to-intent detection problem dataset. About 75\% dataset carries intent searching flight, conversely, minority intent classes represented single training example; severe label imbalance problem. Suppose train model without concerns fairness imbalance. The model likely learn output `flight' intent time, give us accuracy 75\% low could acceptable depending application. Considering roughly 30 classes whole dataset, one class recall 1.0 precision 0.75 remaining 29 classes recall precision 0.0. In scenario, F-measure, harmonic average precision recall, 0.86 common class 0.0 rest, give average 0.03 acceptable many cases. % Previous work Fair ML, There recent interest introducing fairness training machine learning literature . Most studies applied benchmark datasets related socioeconomic problems, e.g., disparate impact equal opportunity . In studies, fairness defined task protecting use explicit implicit information protected attribute decisions machine learning algorithm, instance, framing problem constrained optimization problem introducing several penalties. In work, introduce fairness speech-related problem, namely SLU. We also propose positive generalized definition fairness, terms missed detection false alarm error rates suffered classes, regardless whether class definitions matters socioeconomic importance merely engineering convenience. % Previous methods F-measure optimization There several studies F-measure maximization . These models usually focus binary classification using non-neural-network models: situation problem F-measure optimization reduces problem learning threshold scores computed model make decision. We aware one study performs F-measure optimization convolutional neural networks, again, using system generates several binary classification outputs parallel; scenario, F-measure optimization reduces task tuning thresholds individual binary classifiers order maximize weighted log likelihood. However, true multi-class classification, using softmax output neural network, requires modified definition F-measure. There threshold tuned; instead, F-measure optimization requires optimizing model generate `better' scores terms F-measure. Model versus threshold optimization fundamental difference study previous ones. In work, goal design loss function maximize F-measure instead accuracy DNNs. Our methods tested two standard socioeconomic classification problems literature fairness , two SLU tasks . On SLU tasks, perform end-to-end SLU, i.e., directly map speech input labels instead performing automatic speech recognition followed natural language processing . We pose SLU problems multi-class classification tasks use softmax output DNN, making possible apply optimization criterion socioeconomic SLU learning problems. We approximate F-measure differentiable function softmax activations use standard backpropagation algorithm train DNN. Improvisation expressiveness Peking Opera singing makes extremely difficult synthesize classical performing art. With proposed MDN-based phoneme duration generation Lagrange Multiplier optimization, system generate accurate phoneme duration compared Fitting Heuristic phoneme duration scaling method. Pseudo music notes generated melody transcription algorithm solve score inconsistency problem training. Both objective average predicted phoneme duration error generated pitch contour show system performances well generating Peking Opera singing. And one see MOS generated samples still gap generated singing real performance terms naturalness. Our work includes collecting labelling Peking Opera singing data, conducting MOS test larger scale subjects musical background, improving quality pitch accuracy generated singing. \vfill\pagebreak References produced using bibtex program suitable BiBTeX files . The IEEEbib.bst bibliography style file IEEE produces unsorted bibliography list. ------------------------------------------------------------------------- \nocite{black2014automatic} \nocite{umbert2015expression}"," Spoken language understanding  datasets, like many other machine learning datasets, usually suffer from the label imbalance problem. Label imbalance usually causes the learned model to replicate similar biases at the output which raises the issue of unfairness to the minority classes in the dataset. In this work, we approach the fairness problem by maximizing the F-measure instead of accuracy in neural network model training. We propose a differentiable approximation to the F-measure and train the network with this objective using standard backpropagation. We perform experiments on two standard fairness datasets, Adult, and Communities and Crime, and also on speech-to-intent detection on the ATIS dataset and speech-to-image concept classification on the Speech-COCO dataset. In  all four of these tasks, F-measure maximization results in improved micro-F1 scores, with absolute improvements of up to 8\% absolute, as compared to models trained with the cross-entropy loss function.  In the two multi-class SLU tasks, the proposed approach significantly improves class coverage, i.e., the number of classes with positive recall."
"Recent neural text-to-speech systems based sequence-to-sequence approach, Tacotron~2 , brought considerable quality improvements, require relatively large amounts training data computational resources train operate. %Recent advances text-to-speech synthesis allowed integration high-quality speech synthesis systems products Alexa Google Assistant. However, adapting synthesis models custom domains requires access relatively large amounts training data computational resources. %Moreover, real-time synthesis may problematic due size systems sequential inference. Several works attempt reduce computational burden various ways , still tradeoff fast training times, fast inference, output quality. In paper, address training efficiency TTS systems well inference speed hardware requirements sustaining good quality synthesized audio. We propose fully convolutional, non-sequential approach speech synthesis %based combination ideas . %Similarly , system consisting teacher student network, similarly FastSpeech . The teacher network autoregressive %\OD{je tu potba kat uteregresivn, kdy se tak nikdy nepouv?} \todo{myslim, ze jo -- jde zpusob, jaky modeluje audio. Kdyby nemodeloval audio autoregresivne, tak nemel moc motivace naucit se spravny alignment} \OD{Fair enough.} convolutional network % based used extract correct alignments phonemes corresponding audio frames. The student network non-autoregressive, fully convolutional network %with residual connections % based encodes input phonemes, predicts duration one, decodes mel-scale spectrogram based phoneme encodings durations. %used synthesize spectrograms input phonemes. The student network first encodes input phonemes. Then duration prediction module predicts duration phoneme. Finally, phoneme encoding vectors expanded based durations fed decoder module synthesizes final spectrogram. We combine student network pretrained MelGAN vocoder achieve fast high-quality spectrogram inversion. Our model trained LJ~Speech data 40 hours single 8GB GPU generates high-quality audio samples faster real-time GPU CPU. %\OD{d se tohle rozdit na v bod ne 2?} \todo{ano :)}\OD{d :-)} Our contributions follows: We simplify teacher-student architecture FastSpeech provide fast stable training procedure. We use simpler, smaller faster-to-train convolutional teacher model single attention layer instead Transformer used FastSpeech. % . We show self-attention layers student network needed %necessary order achieve high-quality speech synthesis. We describe simple data augmentation technique used early training make teacher network robust sequential error propagation. We show model significantly outperforms strong baselines keeping speedy training inference. %Finally, provide results experiments various techniques batch normalization , dropout , positional encoding style loss functions. In work, proposed method maximize F-measure training DNN deal label imbalance problem frequently encountered many datasets. We approximated average using soft counts obtained softmax activations DNN. We compared proposed method cross-entropy based training experiments. We showed method applied different types DNNs, either fully-connected BLSTM based, long final layer softmax layer. In experiments two SLU problems, namely ATIS speech-to-intent detection problem Speech-COCO speech-to-image label classification task, showed deep F-measure maximization performs better cross-entropy model terms micro-, average- coverage classes. Especially, significantly increased coverage shows proposed method provides fair way treating minority classes. There several future directions research. One direction deal coverage versus accuracy trade-off, e.g., explore multi-task constrained learning methods might improve coverage fairness without harming performance majority class. Another issue would like address performance degradation high cases Speech-COCO. We also would like perform experiments larger datasets real speech instead synthesized speech."," %Recent breakthrough in in the quality of text-to-speech systems can be largely accounted to neural sequence-to-sequence models \cite{Tacotron2, WaveNet}. Extensive research has been conducted to improve the effectiveness of training \cite{DeepVoice3, EfficientTTS}, inference speed \cite{WaveRNN, ParallelWaveNet} and voice quality \cite{FastSpeech, TransformerTTS} of the speech synthesis systems.  While recent neural sequence-to-sequence models have greatly improved the quality of speech synthesis, % in the past years, %However, to our knowledge  there has not been a system capable of %fast and efficient training, speedy inference and fast training, fast inference and high-quality audio synthesis at the same time.  %However, none of the aforementioned systems excels in all of the traits.  %In this work,  We propose a student-teacher network %based on \cite{FastSpeech, DeepVoice3, EfficientTTS}  capable of high-quality faster-than-real-time spectrogram synthesis, with low requirements on computational resources and fast training time. We show that self-attention layers are not necessary for generation of high quality audio.  %In fact,  We utilize simple convolutional blocks with residual connections in both student and teacher networks and use only a single attention layer in the teacher model. Coupled with a MelGAN vocoder, our model's voice quality was rated significantly higher than Tacotron~2. Our model can be efficiently trained on a single GPU and can run in real time even on a  %4-core  CPU. We provide both our source code and audio samples in our GitHub repository.\footnote{\url{https://github.com/janvainer/speedyspeech}\label{fn:github}}"
"Automatic speaker verification several applications voice biometrics commercial applications, speaker detection surveillance, speaker diarization, etc. A speaker enrolled sample utterance, task ASV detect whether target speaker present given test utterance not. Several challenges organized years benchmarking advancing speaker verification technology NIST speaker recognition Evaluation challenge 2019 , VoxCeleb speaker recognition challenge VOiCES challenge . The major challenges speaker verification include language mismatch testing, short duration audio presence noise/reverberation speech data. %The field attracting lot participants, thereby rapidly updating state-of-the-art. The state-of-the-art systems speaker verification use model extract embeddings fixed dimension utterances variable duration. The earlier approaches based unsupervised Gaussian mixture model i-vector extractor recently replaced neural embedding extractors trained large amounts supervised speaker classification tasks. These fixed dimensional embeddings pre-processed length normalization technique followed probabilistic linear discriminant analysis based backend modeling approach . In previous work, explored discriminative neural PLDA approach backend modeling discriminative similarity function used. The learnable parameters NPLDA model optimized using approximation minimum detection cost function . This model also showed good improvements SRE evaluations VOiCES distance challenge . In paper, extend work propose joint modeling framework optimizes front-end x-vector embedding model backend NPLDA model single end-to-end neural framework. The proposed model initialized pre-trained x-vector time delay neural network . The NPLDA E2E fully trained pairs speech utterances starting directly mel-frequency cepstral coefficient features. The advantage method embedding extractor well final score computation optimized pairs utterances speaker verification metric. With experiments NIST SRE 2018 2019 datasets, show proposed NLPDA E2E model improves significantly baseline system using x-vectors generative PLDA modeling. % backend models trained embeddings output score. These scores scaled log-likelihood ratios using calibration methods. Speaker verification systems apply application specific threshold log-likelihood ratios output decision. Widely used examples embeddings i-vector, x-vector d-vector. I-vectors unsupervised embeddings representing alignment statistics utterance using Gaussian mixture universal background model , X-vectors d-vectors embeddings obtained Neural Network models trained objective speaker classification thousand speakers. The Probabilistic Linear Discriminant Analysis widely used backend model compute log-likelihood. Other backend models include DPLDA, pairwise Gaussian backend, SVMs, Neural PLDA. In majority systems, model extract embeddings trained separately backend model. % An area growing interest training End-to-End speaker verification systems, optimizes entire model verification objective function. In paper, extend prior work Neural PLDA model enable joint learning X-vector extractor NPLDA backend, fully end-to-end manner. We address GPU memory issues, analyse two straightforward methods sampling training trials batch. We provide comparisons different loss functions training. We presented convolutional model spectrogram synthesis phonemes supports speedy training inference, maintaining significantly better output voice quality strong baselines. Our source code audio samples available GitHub.\textsuperscript{} For future work, plan extend model support multi-speaker training data."," While deep learning models have made significant advances in supervised classification problems, the application of these models for out-of-set verification tasks like speaker recognition has been limited to deriving feature embeddings. The state-of-the-art x-vector PLDA based speaker verification systems use a generative model based on probabilistic linear discriminant analysis  for computing the verification score. Recently, we had proposed a neural network approach  for backend modeling in speaker verification called the neural PLDA  where the likelihood ratio score of the generative PLDA model is posed as a discriminative similarity function and the learnable parameters of the score function are optimized using a verification cost. In this paper, we extend this work to achieve joint optimization of the embedding neural network  with the NPLDA network in an end-to-end  fashion. This proposed end-to-end model is optimized directly from the acoustic features with a verification cost function and during testing, the model directly outputs the likelihood ratio score. With various experiments using the NIST speaker recognition evaluation  2018 and 2019 datasets, we show that the proposed E2E model improves significantly over the  x-vector PLDA baseline speaker verification system."
"With advent deep learning, end-to-end text-to-speech shown many advantages conventional TTS techniques . Tacotron-based approaches encoder-decoder architecture attention mechanism shown remarkable performance. The key idea integrate conventional TTS pipeline unified network learn mapping directly text-waveform pair . The recent progress neural vocoder also contributes improvement speech quality. Speech prosody includes affective prosody linguistic prosody. Affective prosody represents emotion speaker, linguistic prosody relates language content. They crucial speech communication. A TTS system expected synthesize right prosodic pattern right time. However, current end-to-end systems explicitly modeled speech prosody. Therefore, can't control well melodic rhythmic aspects generated speech. This usually leads monotonous speech, even models trained expressive speech datasets. In paper, would like study way enable Tacotron-based TTS expressive prosody generation. Multi-task learning learning paradigm leverages information multiple related tasks help improve overall performance . MTL inspired human learning activities people often apply knowledge learned many tasks learning new task, called inductive transfer. For example, learn read write together, experience reading strengthen writing vice versa. MTL widely used speech enhancement , speech recognition . It also used speech synthesis , statistical parametric speech synthesis GANs DNN-based speech synthesis stacked bottleneck features. In paper, apply multi-task learning Tacotron-based TTS prosody modeling. The study expressive speech synthesis focused prosody modeling , speech prosody generally refers intonation, stress, speaking rate, phrase breaks. Prosodic phrasing plays important role affective linguistic expressions. Inadequate phrase breaks may lead misperception speech communication. There recent studies prosody modeling end-to-end TTS system , example, improve prosodic phrasing using contextual information , syntactic features . They incorporated stage text preprocessing, therefore, optimized part synthesis processing. We propose novel two-task learning scheme Tacotron-based TTS model improve prosodic phrasing: 1) main task learns prediction speech spectrum parameters character-level embedding representation, 2) secondary task learns prediction word-level prosody embedding. During training, secondary task serves additional supervision Tacotron learn exquisite prosody structure associated input text. At run-time, prosody embedding serves local condition controls prosodic phrasing voice generation. The main contributions paper include: 1) novel Tacotron-based TTS architecture explicitly models prosodic phrasing; 2) multi-task learning scheme, optimizes model high quality speech spectrum, adequate prosodic phrasing time. The proposed system achieves remarkable voice quality Chinese Mandarin Mongolian. To best knowledge, first multi-task Tacotron implementation includes explicit prosodic model. This paper organized follows. Section recaps Tacotron TTS framework. We propose multi-task Tacotron Section report experiments Section. Section concludes discussion. In paper, explore transfer learning methods RNN-T models. Our motivation leverage well-trained en-US models bootstrap hi-IN RNN-T models also stabilize hi-IN RNN-T model training. We evaluated following transfer learning methods: a) en-US CE initialization b) en-US RNN-T initialization c) Two-stage transfer learning d) Encoder prediction network initialization. Based WER gains training convergence, propose Two-stage learning approach grapheme targets preferred transfer learning strategy. The experiments smaller data-sets training loss convergence reveal importance transfer learning low-resource RNN-T models. The methods discussed paper generalized low-resource languages well. In future, plan explore transfer learning methods extension multi-lingual RNN-T models. Recently, End-to-End automatic speech recognition system gained significant popularity ASR community. They replace acoustic model , language model pronunciation model conventional hybrid ASR system single neural network . One E2E architecture recurrent neural network transducer allow streaming input suitable online ASR applications. The E2E ASR systems well-suited on-device ASR applications model size much smaller hybrid ASR models. The popular E2E ASR system include Connectionist Temporal Classification , Attention-based Encoder-Decoder recurrent neural network transducer . The CTC Several works shown effectiveness TL hybrid ASR framework . In paper, TL also used RNN-T framework improve accuracy low-resource languages. -Several works shown importance transfer learning hybrid ASR systems []\\ -In case hybrid ASR system, transfer learning typically done initializing AM low resource language well-trained AM high resource language. \\ Merge two paragraphs There often disparity amount transcribed speech data available different languages. In cases, lot data available American English languages. The RNN-T model trained en-US data, referred hence-forth en-US RNN-T model, form encapsulates knowledge mapping input speech corresponding text learnt corresponding en-US data. TL approaches used transfer knowledge training models low resource languages. In paper, explore TL approaches benefit Hindi RNN-T model using en-US models. The knowledge en-US data embedded models trained en-US data, referred hence-forth en-US model. TL suitably used transfer knowledge models trained American English model trained low resources. TL enables sharing knowledge HR language LR language simply initializing LR model HR model. In paper, study TL methods American accent English data, referred henceforth en-US data, high resource data Indian accented Hindi data, referred hi-IN data, low resource data. - In case RNN-T framework, several possible combinations exist transfer learning. TL applied encoder well prediction network. \\ Review related work Encoder initialized following different ways: The key contributions work follows: In general TL also applied medium resource locales seek improvements certain new growing applications, corresponding training data may present source locale. It also applied locales language family well locales outside family."," Tacotron-based end-to-end speech synthesis has shown remarkable voice quality. However, the rendering of prosody in the synthesized speech remains to be improved, especially for long sentences, where prosodic phrasing errors can occur frequently. In this paper, we extend the Tacotron-based speech synthesis framework to explicitly model the prosodic phrase breaks. We propose a multi-task learning scheme for Tacotron training, that optimizes the system to predict both Mel spectrum and phrase breaks.  To our best knowledge, this is the first implementation of multi-task learning for Tacotron based TTS with a prosodic phrasing model. Experiments show that our proposed training scheme consistently improves the voice quality for both Chinese and Mongolian systems."
"% 1.  Speech synthesis, also known text-to-speech , attracted lot attention obtained satisfactory results recent years due advances deep learning. Several TTS systems based deep networks proposed, Char2Wav , Tacotron2 , DeepVoice3 , Transformer TTS , FastSpeech ParaNet . These systems usually first predict acoustic feature sequence input text sequence, generate waveform acoustic feature sequence using vocoder Griffin-Lim , WaveNet , WaveRNN , WaveGlow GAN-TTS . % 2.  el LSTM, Conv, transformer % According characteristics network strucutre, current mainstream TTS systems divided % three types: RNN-based, CNN-based Transformer-based. % The RNN-based TTS systems, Char2Wav , % Tacotron 2 Tacotron , % use recurrent neural network design main network structure, % attention mechanism applied model alignment % acoustic feature sequence text sequence, % nature RNN limits parallelism. % The CNN-based TTS systems, DeepVoice 3 % ParaNet , adopt convolution neural network model timing dependencies, % enable parallel processing training. % Especially ParaNet , iteratively refined attention mechanism proposed enable system % perform inference process parallel. % The Transformer-based TTS systems, Transformer TTS , FastSpeech AlignTTS , % apply architecture Transformer realize process speech synthesis. % FastSpeech uses self-attention structure Transformer design feed-forward network % predicting mel-spectrum parallel, needs guidance teacher autoregressive TTS model % due difficulty learning alignment text mel-spetrum. % AlignTTS proposes alignment loss make feed-forward TTS system capable model aligment % without guidance TTS systems. % 2.  Although current speech synthesis systems obtained high-quality speech, difficult achieve satisfactory results long text speech synthesis scenarios. In sequence-to-sequence TTS model, since monotonicity locality properties TTS alignment fully utilized, alignment procedure lacks robustness inference, leads skipping repeating words, incomplete synthesis, inability synthesize long utterances . To address issue, many monotonic attention mechanisms presented , alignment paths satisfying monotonic condition taken consideration decoder timestep. In , location-based GMM attention introduced also studied TTS systems generalize long utterances. Especially, AlignTTS proposes alignment loss model alignment text mel-spectrum, uses length regulator adjust alignment, solves instability problem alignment efficient. However, since self-attention Transformer used model dependencies input sequence elements AlignTTS, positional encodings required introduce positional information, limits maximum length input text. In paper, novel self-attention mechanism proposed remove need positional encodings lift restriction input text length. % In Tacotron , content-based attention mechanism introduced % used align text melspectrum, % exploit monotonicity TTS alignment. % Tacotron 2 uses hybrid attention meachnism % encourage attention alignment move forward consistently input sequence. % makes synthesis process instability. % long text sequence conducive % calcualtion attention mechanism TTS system, % affects prediction acoustic feature stop token inference. % FastSpeech AlignTTS use length regulator instead attention mechanism, % locational encoding Transformer also limits max length input text. % In order lift restriction, design novel self-attention mechanism % model timing dependencies TTS system. % 3.  On hand, prosody speech directly affects overall listening perception voice, especially long utterances. In order improve naturalness synthetic speech, necessary TTS systems model prosody information. In , prosody embedding introduced emotional expressive speech synthesis, enables fine-grained control speaking style. In , interpretable latent variable model prosody based Tacotron 2 presented model phoneme-level word-level prosody information speech. proposes quantized latent feature prosody speech, trains autoregressive prior model generate natural samples without reference speech. These prosody control methods enable us learn prosody speech fine-grained synthesized speech, still cannot effectively predict correct prosody according input text. One reason prosody information speech generally depends meaning text, phoneme information text used input current mainstream TTS systems, limits capabilities modeling prosody speech. In , textual knowledge BERT introduced TTS systems improve prosody speech, ignore variability prosody. For example, text may produce speech different prosody due pronunciation uncertainty. % 4.  In works, propose novel self-attention mechanism, named local attention, model timing dependencies, abandons positional encoding uses relative position matrix model influence positional relationship input sequence. At time, introduce prosody learning mechanism feed-forward TTS systems, prosody embedding phoneme learned mel-spectrum training. In addition, prosody predictor designed predict prosody embedding according text phoneme, pre-trained language model applied introduce meaning text. And main contributions works follows: We proposed novel multi-task Tacotron model model prosodic phrasing speech synthesis, word-level prosody generator introduced secondary task. The experiments show proposed MTL-Tacotron consistently outperforms contrastive systems. The modeling technique prosodic phrasing easily extended modeling melodic rhythmic aspects speech, intonation stress. & & \\ & & \\ \noalign{\smallskip}\hline {\footnotesize }","     Recent neural speech synthesis systems have gradually    focused on the control of prosody to improve the quality    of synthesized speech, but they rarely consider the    variability of prosody and the correlation between prosody    and semantics together. In this paper, a prosody learning    mechanism is proposed to model the prosody of speech based    on TTS system, where the prosody information of speech is    extracted from the mel-spectrum by a prosody learner and    combined with the phoneme sequence to reconstruct the    mel-spectrum. Meanwhile, the sematic features of text from    the pre-trained language model is introduced to improve the    prosody prediction results. In addition, a novel self-attention    structure, named as local attention, is proposed to lift    this restriction of input text length, where the relative    position information of the sequence is modeled by the    relative position matrices so that the position encodings    is no longer needed. Experiments on English and Mandarin show    that speech with more satisfactory prosody has obtained    in our model. Especially in Mandarin synthesis,    our proposed model outperforms baseline model with a MOS gap    of 0.08, and the overall naturalness of the synthesized    speech has been significantly improved."
"Conventional SLU pipeline mainly consists two components : Automatic Speech Recognition module generates transcriptions N-hypotheses, Natural Language Understanding module classifies transcriptions intents, speech recognition error propagation amplified sub-sequence NLU process. Although rapid development end-to-end speech recognition systems, performance SLU significant improved , still satisfy application requirements, due complexity scenarios. %The improved performance SLU mainly benefits increasing maturity ASR. The application deep neural networks acoustic models language models together rapid development end-to-end technique make ASR systems extend research domains . Usually errors speech recognition harm SLU module, errors impact eventual performance . The SLU component keeps attention keywords discarding irrelevant words . Thus joint optimization approach strengthen focus model improving transcription accuracy relates target events . Recently, many efforts dedicated end-to-end SLU domain intent predicted directly input audio . Previous researches shown large amount data determining factor excellent performance model . However, due lack audio ambiguity intents, difficult obtain sufficient in-domain labeled data. Transfer learning methodology become common strategy address insufficient data problem . %which vital technique generalizes models trained one setting task settings tasks. Different transfer learning strategies applied SLU model result competitive complementary results . In paper, strategy also applied amplify feature extraction capability encoder component, pre-train encoder large amount speech recognition labeled data, transfer encoder SLU model. Recently, proposed compared various encoder-decoder approaches optimize module SLU end-to-end manners proved intermediate text representation crucial SLU jointly training full model advantageous. Attention-based models widely used speech recognition provide impressive performance . Inspired this, propose Transformer based multi-task strategy adopt textual information SLU model. Since text information acts decoder component speech recognition task, treated adaptive regularizer adjust encoder parameters contributing improve intent prediction performance. It noticed lack textual corpus also major challenge training language models. To address problem, various methods carried expand corpus past decade . In addition, textual level transfer learning strategy merging pre-trained representation decoder also explored. The pre-trained representation obtained BERT model, designed pre-train deep bidirectional representations unlabeled text jointly conditioning left right context layers . Encoder decoder mutual independent connected attention block, get collaborated optimization training. To maximize performance, encoder decoder optimized transfer leaning strategies. In paper, first propose self-attention based end-to-end SLU structure, applied cross-lingual transfer learning method solve insufficient acoustic data problem. Then propose Transformer based multi-task strategy conducts intent classification speech recognition parallel. Finally, textual-level transfer learning structure designed aggregate pre-trained BERT model decoder component improves feature extraction capability decoder, indirectly. Based local attention, feed-forward text-to-speech system without limitation input text length designed. Meanwhile, prosody learning mechanism proposed model prosody speech, prosody information learned speech prosody learner training process. In order predict satisfactory prosody inference, pre-trained language model used introduce semantic feature. Experiments English synthesis Mandarin synthesis show significant improvement prosody speech obtained proposed TTS systems.","   End-to-end Spoken Language Understanding  models are made increasingly large and complex to achieve the state-of-the-art accuracy. However, the increased complexity of a model can also introduce high risk of over-fitting, which is a major challenge in SLU tasks due to the limitation of available data. In this paper, we propose an attention-based SLU model together with three encoder enhancement strategies to overcome data sparsity challenge. The first strategy focuses on the transfer-learning approach to improve feature extraction capability of the encoder. It is implemented by pre-training the encoder component with a quantity of Automatic Speech Recognition annotated data relying on the standard Transformer architecture and then fine-tuning the SLU model with a small amount of target labelled data. The second strategy adopts multi-task learning strategy, the SLU model integrates the speech recognition model by sharing the same underlying encoder, such that improving robustness and generalization ability. The third strategy, learning from Component Fusion  idea, involves a Bidirectional Encoder Representation from Transformer  model and aims to boost the capability of the decoder with an auxiliary network. It hence reduces the risk of over-fitting and augments the ability of the underlying encoder, indirectly. Experiments on the FluentAI dataset show that cross-language transfer learning and multi-task strategies have been improved by up to $4.52\%$ and $3.89\%$ respectively, compared to the baseline."
"Associative memory defined psychology ability remember many sets, called memories, unrelated items. Prompted large enough subset items taken one memory, animal computer associative memory retrieve rest items belonging memory. The diverse human cognitive abilities involve making appropriate responses stimulus patterns often understood operation associative memory, ``memories'' often distillations consolidations multiple experiences rather merely corresponding single event. The intuitive idea associative memory described using ``feature space''. In mathematical model abstracted neurobiology, presence particular feature denoted activity model neuron due directly driven feature signal. If possible features, distinct connections neural circuit involving neurons. Typical cortical synapses highly reliable, store bits information\footnote{For instance, recent study reports information content individual synapses ranging bits, based electron microscopy imaging, see also . These numbers refer structural accuracy synapses. There also electrical chemical noise synaptic currents induced biophysical details vesicle release neurotransmitter binding. The unreliability fusion pre-synaptic vesicles pre-synaptic neuron membrane dominant source trial-to-trial synaptic current variation . This noise decreases electrical information capacity individual synapses maximal value synaptic structure would otherwise provide.}. The description particular memory requires roughly bits information. Such system therefore store unrelated memories. Simple artificial neural network models associative memory exhibit limitation even precise synapses, limits memory storage less memories . Situations arise number small desired number memories far exceeds , see examples biological AI systems Section . In situations associative memory model would insufficient, since would able memorize required number patterns. At time, models associative memory large storage capacity considered paper, easily solve problems. The starting point paper machine learning approach associative memory based energy function attractor dynamics space variables, called Dense Associative Memory . This idea shown dramatically increase memory storage capacity corresponding neural network proposed useful increasing robustness neural networks adversarial attacks . Recently, extension idea continuous variables, called modern Hopfield network, demonstrated remarkably successful results immune repertoire classification , provided valuable insights properties attention heads Transformer architectures . Dense Associative Memories modern Hopfield networks, however, cannot describe biological neural networks terms true microscopic degrees freedom, since contain many-body interaction terms equations describing dynamics corresponding energy functions. To illustrate point consider two networks: conventional Hopfield network Dense Associative Memory cubic interaction term energy function . In conventional network dynamics encoded matrix , represents strengths synaptic connections feature neurons . Thus, network manifestly describable terms two-body synapses, approximately true many biological synapses. In contrast, Dense Associative Memory network cubic energy function naively requires synaptic connections tensors three indices, harder, although impossible, implement biologically. Many-body synapses become even problematic situations interaction term described complicated function simple power . Many-body synapses typically appear situations one starts microscopic theory described two-body synapses integrates degrees freedom . The argument described based counting information stored synapses conjunction fact modern Hopfield nets Dense Associative Memories huge storage capacity hints solution. The reason networks storage capacity much greater describe dynamics neurons, rather involve additional neurons synapses. Thus, remains theoretical question: hidden circuitry look like? Is possible introduce set hidden neurons appropriately chosen interaction terms activation functions resulting theory large memory storage capacity , and, time, manifestly describable terms two-body synapses? The main contributions current paper following. First, extend model continuous state variables continuous time, state network described system non-linear differential equations. Second, couple additional set ``complex neurons'' ``memory neurons'' hidden neurons feature neurons. When synaptic couplings neuron activation functions appropriately chosen, dynamical system variables energy function describing dynamics. The minima dynamics locations - dimensional feature subspace minima corresponding Dense Associative Memory system. Importantly, resulting dynamical system mathematical structure conventional recurrent neural network, neurons interact pairs two-body matrix synaptic connections. We study three limiting cases new theory, call models A, B, C. In one limit reduces Dense Associative Memory model depending choice activation function. In another limit model reduces network . Finally, present third limit call Spherical Memory model. To best knowledge model studied literature. However, high degree symmetry reason might useful future explorations various models large associative memory recurrent neural networks machine learning. For purposes paper defined ``biological plausiblity'' absence many-body synapses. It important note aspects model described equations biologically implausible. For instance, assumes strengths two physically different synapses equal. This assumption necessary existence energy function, makes easy prove convergence fixed point. It relaxed equations , makes even biological, but, time, difficult analyse. In paper, proposed various audio dequantization schemes implemented flow-based neural vocoder. For uniform dequantization, compressed range audio domain match conventional uniform dequantization method using mu-law companding compression. In addition, implemented iw dequantization resolve noise issue occurs lossy compression. For Gaussian dequantization, applied hyperbolic tangent normalization data-oriented Gaussian noise properly fit data within audio range. Lastly, modified flow block flow-based neural vocoder construct variational dequantization model apply flexible noise. From experiments, demonstrate implementing audio dequantization supplement flow-based neural vocoder produce better audio quality fewer artifacts. \clearpage","  Dense Associative Memories or modern Hopfield networks permit storage and reliable  retrieval of an exponentially large  number of memories. At the same time, their naive implementation is non-biological, since it seemingly requires the existence of many-body synaptic junctions between the neurons.   We show that these models are effective descriptions of a more microscopic  theory that has additional  neurons and only requires two-body interactions between them. For this reason our proposed microscopic theory is a valid model of large associative memory with a degree of biological plausibility. The dynamics of our network and its reduced dimensional equivalent both minimize energy  functions. When certain dynamical variables  are integrated out from our microscopic theory, one can recover many of the models that were previously discussed in the literature, e.g. the model presented in ``Hopfield Networks is All You Need'' paper. We also provide an alternative derivation of the energy function and the update rule proposed in the aforementioned paper and clarify the relationships between various models of this class."
"Over past years, developments sequence-to-sequence neural text-to-speech research led synthetic speech sounds almost indistinguishable human speech . However, large amounts high-quality recordings typically required professional voice talent train models quality, make prohibitively expensive produce. To counter issue, investigations S2S models facilitate multi-speaker data become popular topic research. %\EJ{I like starting sentence citation citation number brackets} \MK{Agreed} A study by, example, showed multi-speaker models perform well even better single-speaker models large amounts target speaker data available, single-speaker models perform better substantial amounts data used. Their research also showed amount data necessary additional speaker little 1250 2500 sentences without significantly reducing naturalness. With regards parametric synthesis, investigated effect several multi-speaker modeling strategies class imbalanced data. Their research found limited amounts speech, multi-speaker modeling oversampling could improve speech naturalness compared single speaker models, undersampling found generally harmful effect. They also showed ensemble methods improve naturalness, strategy comes considerable computational cost usually feasible S2S modeling. Although research shows multi-speaker modeling effective strategy reduce data requirements, suitable solution many languages large quantities high-quality multi-speaker data available. Multilingual multi-speaker synthesis aims address issue training multilingual model data multiple languages. Among first propose neural approach multilingual modeling was. Instead modeling languages separately, modeled language variation cluster adaptive training, mean tower well language basis towers trained. They found multilingual modeling harm naturalness high-resource languages, low-resource languages benefited multilingual modeling. Another study scaled number unseen low-resource languages twelve, similarly found multilingual models tend outperform single speaker models. More recently, multilingual modeling also adopted S2S architectures, however mostly purposes code-mixing cross-lingual synthesis. Language information typically represented either language embedding separate encoder language, applied approaches code-mixing accent conversion. With regards multilingual modeling, showed multilingual models attain naturalness speaker similarity comparable single speaker model high-resource target languages, research obtained promising results crosslingual transfer learning approach. While research S2S multilingual modeling clearly vibrant, appears exist little systematic research S2S multilingual models could used increase speech naturalness low-resource languages. To fill void, paper investigated extent results found S2S monolingual multi-speaker modeling transferable multilingual multi-speaker modeling, possible attain higher naturalness low-resource languages multilingual models single speaker models. Because multilingual modeling benefit inclusion large amounts non-target language data, also experimented several data addition strategies evaluated extent strategies effective improve naturalness low-resource languages. As research primarily addressing viability different approaches regards low-resource languages, focus much maximizing naturalness rather gaining better understanding different strategies work would potentially scale using larger amounts data. The rest paper organized follows. In Section, describe architecture used conduct experiments. In Section, describe experimental design give details training evaluation. In Section, provide experimental results. Finally, Section, discuss conclusions directions future research. A new approach audio laughter synthesis based seq2seq learning proposed inspired evolution TTS field. We proposed train deep learning system synthesize speech laughter transcriptions augmenting input phonemes laughter annotations. It allows leverage transfer learning patterns annotations acoustic features worlds. In paper, new approach audio laughter synthesis based seq2seq learning proposed inspired evolution TTS field. This system implemented leveraging patterns learned pass text acoustic features speech, learn laughter synthesis. We show using pretrained MelGAN model post waveform corrector allows remove audio artifacts generated Griffin-Lim algorithm. We also use pretrained MelGAN model post waveform corrector allows remove audio artifacts generated Griffin-Lim algorithm thus improve scores obtained MOS test. We believe several modifications could improve acoustic quality synthesis. First end-to-end training could help concerning accumulation errors several blocks: seq2seq system vocoder. This results strong improvement past methods audio laughter synthesis terms naturalness promising synthesizing speech-laughs thanks consistency latest speech synthesis technologies. This results strong improvement past methods audio laughter synthesis terms naturalness promising later use build amused speech synthesis systems. The promising results obtained here, allows us work incorporating laughter synthesis system fully functioning TTS control amusement level. The fact laughter synthesis system developed TTS context makes integration easier."," Recent advances in neural TTS have led to models that can produce high-quality synthetic speech. However, these models typically require large amounts of training data, which can make it costly to produce a new voice with the desired quality. Although multi-speaker modeling can reduce the data requirements necessary for a new voice, this approach is usually not viable for many low-resource languages for which abundant multi-speaker data is not available. In this paper, we therefore investigated to what extent multilingual multi-speaker modeling can be an alternative to monolingual multi-speaker modeling, and explored how data from foreign languages may best be combined with low-resource language data. We found that multilingual modeling can increase the naturalness of low-resource language speech, showed that multilingual models can produce speech with a naturalness comparable to monolingual multi-speaker models, and saw that the target language naturalness was affected by the strategy used to add foreign language data."
"% \dcrm{In standard Question Answering system, user enters natural language question,e.g., Who founded Tesla?}. Knowledge Graph based Question Answering systems use background Knowledge Graph answer queries posed user. Let us take following question example : Who founded Tesla?. The standard sequence steps traditional Entity Linking system follows: The system tries identify Tesla span interest. This task called Mention Detection Span Detection. Then attempt made link appropriate entity Knowledge Base. In work focus Knowledge Bases form graphs, hence entity linker case tries link Tesla appropriate node graph. For human, evident question looking person's name created organisation named Tesla, since text contains relation . Hence, important entity linker understands nuance ignores entity nodes Knowledge Graph also contain Tesla labels, e.g., considering example Wikidata knowledge graph. The task ignoring wrong candidate nodes, identifying right candidate node instead, called Entity Disambiguation . The cumulative process involving Mention Detection Entity Disambiguation called Entity Linking . Typically, MD ED stages implemented different machine learning models require separate training. Especially MD part, sentences marked entity spans requirement. In practice, data easily available. Moreover, errors introduced MD phase cascade ED phase. Hence, movement towards end-to-end Entity Linkers began . Such systems require labelled entity spans training. In spite benefits end-to-end models challenges remain: Due lack span detector initial phase, word sentence needs considered entity candidate disambiguation leads generation much larger number entity candidates. To re-rank candidates large amount time consumed, processing features candidates, also compiling features. %Some systems fetch neighbouring entities relations fly candidate entity, step take minute certain entities large KGs. In work, remain cognizant challenges design system completely avoids querying Knowledge Graph runtime. PNEL instead relies pre-computed pre-indexed TransE embeddings pre-indexed entity label description text set features given candidate entity. We demonstrate produces competitive performance maintaining lower response times compared VCG . While wide variety KG embeddings choose from, confine experiments pre-computed TransE Wikidata supplied PyTorch-BigGraph. Our choice based popularity ease availability embeddings. Traditionally, Knowledge Graphs choice Question Answering research DBpedia, Freebase YAGO. However, recent times Wikidata received significant attention owing fact covers large number entities . DBpedia, YAGO Wikidata source information Wikipedia, however DBpedia YAGO filter large percentage original entities, Wikidata not. While Wikidata larger number entities also adds noise challenge EL system. Wikidata also allows direct edits leading up-to-date information, DBpedia depends edits performed Wikipedia. Freebase discontinued portion merged Wikidata. Moreover DBpedia extracts data directly Wikidata, apart Wikipedia \footnote{https://databus.dbpedia.org/dbpedia/wikidata} . %Wikidata allows wiki based edits hence up-to-date. %Both DBpedia Freebase decided merge Wikidata form. Hence, decide base work Wikidata knowledge graph datasets evaluate based Wikidata.\\ In work contributions follows: The paper organised following sections: Related Work, outlining major contributions entity linking used question answering; PNEL, discuss pointer networks architecture PNEL Dataset used paper Evaluation, various evaluation criteria, results ablation test Error Analysis Discussion future direction. This paper aimed investigate effectiveness multilingual modeling improve speech naturalness low-resource language neural speech synthesis. speakers. Our results showed shown addition auxiliary non-target language data positively impact naturalness low-resource language speech viable alternative auxiliary target language data data readily available. We furthermore found target language data available, inclusion auxiliary non-target language data negatively affect naturalness. Although research compare multilingual models single speaker models even larger amounts target language data research, expect results multilingual modeling largely mimic effects observed studies monolingual multi-speaker modeling. Finally, explored several strategies including additional non-target language data. We showed data addition strategies equally effective, reported language diversity minimizing class imbalances appear important variables consider adding data. Based conclusions, identify several directions future research. First all, current research consider issue language proximity effect multilingual modeling. Although languages modeled separately encoders, language proximity may positively affect naturalness. Additionally, research evaluated low-resource language speech naturalness general level, may interesting focus naturalness language-specific characteristics language-specific phonemes stress patterns. We furthermore note amount auxiliary data used relatively limited experiments. Further analysis could done find whether findings hold scaled data. Finally, found MULT-2k+16x2k model effective improve naturalness target language speech, result clarify whether effect attributed large variation languages speakers, minimization class imbalances. It would interesting disentangle variables comparing model monolingual multi-speaker model similar amounts data per speaker."," Question Answering systems are generally modelled as a pipeline consisting of a sequence of steps. In such a pipeline, Entity Linking  is often the first step. Several EL models first perform span detection and then entity disambiguation. In such models errors from the span detection phase cascade to later steps and result in a drop of overall accuracy. Moreover, lack of gold entity spans in training data is a limiting factor for span detector training. Hence the movement towards end-to-end EL models began where no separate span detection step is involved. In this work we present a novel approach to end-to-end EL by applying the popular Pointer Network model, which achieves competitive performance. We demonstrate this in our evaluation over three datasets on the Wikidata Knowledge Graph.    \keywords{Entity Linking  \and Question Answering \and Knowledge Graphs \and Wikidata}"
"Slot filling one major challenging tasks spoken language understanding aims automatically extract semantic concepts assigning set task-related slots word sentence. first reported work applied recurrent neural network slot filling task encouraged follow-up deep learning work task. The next works focused deep learning: tried replace vanilla RNNs advanced RNN cells based long short-term memory bi-directional LSTM , focused recursive neural networks, utilizes attention-based RNN. In study, firstly generalize variational inference -based dropout regularization LSTM-RNNs advanced RNN architectures gated recurrent unit bi-directional LSTM/GRU. Then, RNN models VI-based dropout regularization employed slot filling task ATIS database. Compared , work presents slight modification LSTM-RNNs lead better baseline result, RNN architectures without VI-based dropout regularization tested experiments. As opposed , methods much easier implement attention-based RNN, similar results obtained practice. Since shown RNNs overfit quickly , various regularization methods, early stopping small under-specified models , used RNN training stage. Although dropout normally taken simple effective regularization overcome problem overfitting deep neural networks , concluded naive dropout regularization recurrent weights RNNs cannot reliably solve RNN overfitting problem noise added recurrent connections leads model instabilities . However, recent work shown dropout regularization variational approximation technique Bayesian learning. In addition, variational inference provides new variant dropout regularization, dropout masks separately shared along time embedding, decoding, recurrent weights, successfully applied recurrent layers RNNs. The remainder paper organized follows: Section presents VI-based dropout regularization RNNs. Section develops GRU bi-directional LSTM/GRU-based RNNs VI-based dropout regularization. Section shows experimental results ATIS database paper concluded Section . In work proposed PNEL, end-to-end Entity Linking system based Pointer Network model. We make modifications original Pointer Network model, identify utility problem statement EL, successfully model problem Pointer Network able find right set entities. We evaluate approach three datasets varying complexity report state art results two them. On third dataset, WebQSP, perform best precision lag behind recall. We select features require real time KG queries inference. This demonstrates Pointer Network model, choice features presented work, result practical deployable EL solution largest Knowledge Graph publicly available - Wikidata. \\ \dc{Our main design goal system speed deploybility; hence, refrain querying underlying Knowledge Graph inference. Instead, solely rely pre-trained TransE KG Embeddings, potentially encodes structural information Knowledge Graph. Additionally, also incorporate entity label description information PNEL benefits model. As evident results, PNEL exhibits state-of-the-art performance LC-QuAD 2.0, SimpleQuestions datasets best precision comparable F1-scores WebQSP well. Hence, concluded proposed feature sets encode KG information implicitly achieving comparable better performance systems explicitly relies KG structural information 1, 2-hop relation information. } The design goal models ""no KG query inference"", rely pre-computed TransE embeddings incorporate KG structural information. Additionally, use entity labels descriptions pre-indexed text database. It must noted consider neighbourhood relation information explicitly. Since system achieves state-of-the-art numbers datasets, performs competitively overall, said limited choice pre-indexed features performs par large variety systems also consider 1-hop 2-hop relation information explicitly. \\ For future work: PNEL based LSTM cell inevitably processes tokens sequentially increasing response times. This limitation could overcome using variant Transformer model instead, powerful model also able process tokens parallel. As future work would also like explore different entity embedding techniques investigate characteristics make embedding suitable entity linking task."," This paper proposes to generalize the variational recurrent neural network  with variational inference -based dropout regularization employed for the long short-term memory  cells to more advanced RNN architectures like gated recurrent unit  and bi-directional LSTM/GRU. The new variational RNNs are employed for slot filling, which is an intriguing but challenging task in spoken language understanding. The experiments on the ATIS dataset suggest that the variational RNNs with the VI-based dropout regularization can significantly improve the naive dropout regularization RNNs-based baseline systems in terms of F-measure. Particularly, the variational RNN with bi-directional LSTM/GRU obtains the best F-measure score."
"The percolation social media throughout world facilitated unprecedented ease access flow information. The rise internet availability also enabled every user consume, also contribute information flow. However, benefits ecosystems come cost mistrust veracity information. In recent years, social media scene witnessed proliferation false information campaigns, ordinary users intentionally otherwise consuming false news also spreading among communities. This phenomenon commonly referred fake news, broadly defined broadcasting information intentionally verifiably false . The rise fake news societal impact studied context numerous recent events, Brexit referendum 2016 US presidential elections . Fake news thus proven major threat democracy, journalism, freedom expression . The exposure users fake news shown numerous deleterious effects, instances include inducing attitudes inefficacy, alienation, trusting false propaganda, cynicism toward certain political candidates communities, times give rise violent events. For example, coordinated fake news propaganda campaigns Facebook considered key inciting Myanmar genocide 2016-2017 . Also, recent proliferation false information 5G communication networks cause novel Coronavirus outbreak resulted attacks employees infrastructure cellular careers UK . Fake news also affect financial markets, observed case fake news claiming Barack Obama injured explosion resulting loss \$130 billion stock value . Hence, growing need effective tools techniques detect control spread false information campaigns social media. Fake news classification process determining whether news contains false news misinformation not. Traditionally, classification performed subject-matter experts journalists via comparing claims article established facts cross-checking trusted alternative sources. However, high volume velocity information flow platforms render manual approaches infeasible. Therefore, recent efforts stakeholders research community focused automated techniques classification detection fake news. A promising solution domain leverage recent advances machine learning Natural Language Processing automated processing classification high-dimensional complex text news articles posts . %We purpose model news article classified dividing overall tasks three parts: Style-Based Classification, Knowledge-Based Classification, Propagation Credibility-Based Classification. This paper focus Style-based classification. % %Machine learning proven useful detecting fake news. The n-gram, part speech tagging probabilistic context free grammar widely used linguistic analysis neural networks. Mihalcea Strapparava used n-gram approach lie detection training Naive Bayes Support Vector Machine classifiers. They used crowd sourcing creating datasets three different topics, opinion abortions, opinion death penalty feelings best friend. They applied minimal pre-processing datasets tokenization stemming without performing feature selection stop words removal. They received average accuracy 70.8\% NB 70.1\% SVM, %Ott et al. trained SVM classifiers using relative POS tag frequencies texts features. They found probable relationship deceptive spam imaginative writing based POS distributional similarities. %Feng et al. investigated syntactic stylometry deception detection. They found features driven Context Free Grammar parse trees improved deception detection Ott et al. While literature applications machine learning fake news classification grown rapidly, body work classification short-text claims remains relatively thin. This issue paramount importance, many posts social media Twitter contain short claim extracted longer text news articles. The short form claims poses challenge classification task, provides limited information thus constrains applicability machine learning models trained full-length articles texts. Over past years, number datasets models proposed classification short-text claims, notable instances studies based LIAR dataset short statements . However, performance machine learning models trained dataset remain impractical levels, best accuracy values reported \~41.5\% . % reported study The problem non neural network approach news articles longer length using non neural network approach semantic syntactic features sentences cannot extracted exploited properly full extent non neural network approaches. The solution neural network methods. %Rashkin et al. trained LSTM model takes sequence words input predicts Politifact rating, found accurate NBC Maximum Entropy models. They also concatenated LSTM output Linguistic Inquiry Word Count features undergoing activation layers. The NBC Maximum Entropy models improved LIWC LSTM perform well. The reason might LSTM learn formations LIWC themselves. Wang used deep learning based CNN model LIAR dataset found better results non-neural network methods. %Qian et al. proposed two models, first one Two-Level Convolutional Neural Network variant CNN second one User Response Generator . The TCNN captured semantic information articles' text representing sentence word level. And URG learns generative responses news article text historical user responses assist classification. In paper, introduce Sentimental LIAR, extends LIAR dataset including new features based sentiment emotion analysis claims. Our extended dataset also proposes modified encoding textual attributes mitigate unintended bias modeling. Furthermore, propose novel deep learning architecture based BERT-Base language model classification claims genuine fake. Our results demonstrate proposed architecture trained Sentimental LIAR achieve accuracy 70\%, improvement ~30\% previously reported results LIAR benchmark. The Sentimental LIAR dataset proof-of-concept code made available GitHub. %In paper, present series experiments performed using BERT-Base extended LIAR datasets compare results. The base BERT-Base model modified adding linear neural net top modification done adding CNN model top. The modified models tested different version LIAR datasets. We modified LIAR dataset extending sentiment score sentiment statement. The extension done adding five emotions statement The remainder paper organized follows: Section presents technical background overview relevant datasets literature false claim classification. Section describes extended features Sentimental LIAR, details proposed deep learning architectures false claim detection. The experimental evaluation proposed techniques reported Section . Finally, concludes paper discussion results remarks future directions work. This work proposed variational inference-based dropout regularization RNNs LSTM, GRU, bi-directional LSTM/GRU cells. Contrary naive dropout regularization embedding decoding layers, VI-based dropout regularization applied RNN layers including recurrent layers sharing dropout masks RNN layers. The experiments slot filling task ATIS database showed variational RNN models obtain better results naive dropout regularization-based RNN models. In particular, variational bi-directional LSTM/GRU obtains best results terms F-measure. \vfill\pagebreak References produced using bibtex program suitable BiBTeX files . The IEEEbib.bst bibliography style file IEEE produces unsorted bibliography list. -------------------------------------------------------------------------"," The rampant integration of social media in our every day lives and culture has given rise to fast and easier access to the flow of information than ever in human history. However, the inherently unsupervised nature of social media platforms has also made it easier to spread false information and fake news. Furthermore, the high volume and velocity of information flow in such platforms make manual supervision and control of information propagation infeasible. This paper aims to address this issue by proposing a novel deep learning approach for automated detection of false short-text claims on social media. We first introduce Sentimental LIAR, which extends the LIAR dataset of short claims by adding features based on sentiment and emotion analysis of claims. Furthermore, we propose a novel deep learning architecture based on the BERT-Base language model for classification of claims as genuine or fake. Our results demonstrate that the proposed architecture trained on Sentimental LIAR can achieve an accuracy of 70\%, which is an improvement of ~30\% over previously reported results for the LIAR benchmark. %improve the previously reported accuracy of the task by     Focusing on the prevalent short-text format of claims on social media such as Twitter, our work   to an unprecedented challenge in  . Fake news is not only threatening to undermine democracy but equally has been proven to cause violence, disruption, and chaos in the world. Hence, in this research paper, we are going to use the machine learning approach to classify the fake news from the true ones. The rise of Natural Language Processing makes it possible to analyze the news articles. We are proposing a model composed of three perspectives. The first perspective is the Style Based Classification where we classify the article based on its intention is misleading or not, by analyzing the text pattern from the attribute-based and structure-based language features. The second perspective is Knowledge-based classification which is going to classify the news articles based on its authenticity by knowledge extraction and fact-checking. The third perspective is the Propagation and Credibility based classification by analyzing the propagation model of fake news and the credibility of the engaging users. This research paper currently focused on first perspective i.e. Style based classification by deception detection using deep neural networks where we performed experiments using LIAR Dataset by changing it into binary labels and BERT-Base."
"The ever-growing amount user-generated data social media platforms Facebook, Twitter, blogs electronic medium introduces new challenges terms automatic content moderation, especially regarding hate speech offensive language detection. Not hate speech likely happen Internet, anonymity easily obtained speakers psychologically distant audience, online nature also gives far-reaching determinative impact. User content mostly consists microposts, context post missing inferred current events. Manual verification posting human moderator infeasible due high amount postings created every day. Consequently, automated detection attacking postings feasible way counter kind hostility. However, task challenging natural language fraught ambiguities, language social media extremely noisy. The classification system would prepared task, needed generalized various test corpora well. In paper I described system consisting sequential pipeline text feature extraction classification main components. Firstly, bag-of-words model used encoding sentences corresponding integer sequence. Thereafter, vectors generated sequences fed series BiLSTM layers training. Then softmax layer used ternary classification corresponding offensive language categories. The rest paper organized follows. Section describes data, which, task performed. The methodology followed described Section . This followed results concluding remarks Section respectively. % % The following footnote without marker needed camera-ready % version paper. % Comment instructions uncomment 8 lines % ""final paper"" variant English. % %. % % % final paper: en-us version % % % space normally used marker % This work licensed Creative Commons % Attribution 4.0 International License. % License details: % \url{http://creativecommons.org/licenses/by/4.0/}. % } This paper introduced Sentimental LIAR extension LIAR dataset, proposed novel model architectures based BERT-Base fake claim detection short text. The proposed architectures extend BERT-Base adding feedforward neural network, CNN. The LIAR dataset extended adding emotions anger, sad, fear, anger disgust using IBM NLP API added sentiment score using Google NLP API. We also included speaker credit input attribute models. The experiments performed BERT-Base + feedforward NN, accuracy ranged 68.8\ 69\ within five experiments. These experiments performed changing input structure first three experiments changing hidden layers latter two experiments. A slight improvement 1\ observed accuracy improvements F1 Score. This suggests model may need revised handle complexity input data. Hence, CNN-based architecture investigated experiments. The experiments performed BERT-Base + CNN, accuracy ranged 68.82\ 70\ within six experiments, also major improvements observed F1 Score . The best performing model found one text attribute fed directly BERT-Base, output BERT-Base concatenated emotions, speaker's credit sentiments passed CNN. Undeutsch hypothesis four-factor theory supported intuition emotional sentimental attributes help distinguish fake claims, verified observation model performing better EMO SEN added. Adding SEN EMO BERT-Base output supplemented features boosted CNN model performance. For models, observed adding metadata increased accuracy model. Also, model accuracy F1 Score improved CNN-based architecture. The training loss VS validation Loss graphs BERT-Base + feedforward NN given Fig., BERT-Base + CNN Fig.. These plots suggest models overfitted 2 epochs, mostly due small size dataset. Also, must noted dataset imbalanced, 65\ data labeled false 35\ labeled true. These observations demonstrate need curation larger representative datasets short-text claims. Furthermore, results verify fake claims detected short-text according exaggerated expressions strong emotions demonstrated text. The proposed architecture also sets new state-of-the-art benchmark fake claim classification LIAR dataset accuracy 70\ ."," SemEval-2020 Task 12 was OffenseEval: Multilingual Offensive Language Identification in Social Media \cite{zampieri-etal-2020-semeval}. The task was subdivided into multiple languages and datasets were provided for each one. The task was further divided into three sub-tasks: offensive language identification, automatic categorization of offense types, and offense target identification. I have participated in the task-C, that is, offense target identification. For preparing the proposed system, I have made use of Deep Learning networks like LSTMs and frameworks like Keras which combine the bag of words model with automatically generated sequence based features and manually extracted features from the given dataset. My system on training on 25\% of the whole dataset achieves macro averaged f1 score of  47.763\%."
"The discourse structure document describes discourse relationships elements graph tree. Discourse parsing largely dominated greedy parsers~\cite[\eg.][]{braud2016multi,ji2014representation,yu2018transition,SogaardBC17}. Global parsing rarer dependency node's label internal split point make prediction computationally prohibitive. % resulting large grammar constant. % This expense comes dependency relation % labels assigned node % split point separates children, results % large constant global inference terms time % complexity, making inference process extremely slow. In work, propose CKY-based global parser tractable inference using new independence assumption loosens coupling identification best split point label prediction. % For particular node, first decide split % point without considering labels; based % split point, make decisions labels % current node. % However, apply recursion, total score % node sum scores split point label % assignments instead recursing split % score. % By making independence decisions split point label % assignment, remove large constant terms time % complexity; recursing sum scores, % dependency relations maintained. Doing gives us advantage search best tree larger space. % One side effect % need complex models represent EDUs. Greedy discourse parsers use complex models ensure step correct search space limited. For example, \citet{ji2014representation} manually crafted features feature transformations encode elementary discourse units ; \citet{yu2018transition} \citet{braud2016multi} used multi-task learning better EDU representation. Instead, work, use simple recurrent span representation build parser outperforms previous global parsers.% comparable state-of-art % greedy parsers. Our contributions are: %%% Local Variables: %%% mode: latex %%% TeX-master: ""main"" %%% End: % % File emnlp2019.tex % %% Based style files ACL 2019, %% Based style files EMNLP 2018, %% Based style files ACL 2018, %% Based style files ACL-2015, improvements %% taken NAACL-2016 style %% Based style files ACL-2014, were, turn, %% based ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009, %% EACL-2009, IJCNLP-2008... %% Based style files EACL 2006 %%e.agirre@ehu.es Sergi.Balari@uab.es %% ACL 08 Joakim Nivre Noah Smith \documentclass[11pt,a4paper]{article} \usepackage[hyperref]{emnlp-ijcnlp-2019} \usepackage{times} \usepackage{latexsym} \usepackage{mlsymbols} \usepackage{mystyle} \usepackage{symbol} \usepackage{comment} \usepackage{url} \aclfinalcopy % Uncomment line final submission % \setlength\titlebox{5cm} % You expand titlebox need extra space % show authors. Please make titlebox % smaller 5cm ; check % camera-ready version ask change back. \newcommand\BibTeX{B{\sc ib}\TeX} \newcommand\confname{EMNLP-IJCNLP 2019} \newcommand\conforg{SIGDAT} \title{A Simple Global Neural Discourse Parser} \author{ Yichu Zhou \\ University Utah \\ flyaway@cs.utah.edu \\\And% Omri Koshorek \\ Tel-Aviv University \\ omri.koshorek@cs.tau.ac.il \\\AND%\\ Vivek Srikumar \\ University Utah \\ svivek@cs.utah.edu \\\And% Jonathan Berant \\ Tel-Aviv University\\ joberant@cs.tau.ac.il } \date{} We evaluated embeddings four transfer learning models Mohler dataset task \ac{ASAG}. These transfer learning models explained breifly pretraining procedures. Besides, also elucidated \ac{ASAG} task's significance applications. The sentence embeddings created selected four transfer learning models desired student answers dataset. The encoding answers related words answers, irrespective order. The cosine similarity feature extracted every student answer desired answer. This feature trained three isotonic, linear ridge regression methods. \ac{ELMo} outperformed transfer learning models task best \ac{RMSE} score 0.978 Pearson correlation 0.485. With results, \ac{ELMo} competed conventional word embeddings, Word2Vec, GloVe FastText, without preprocessing multiple feature training. \ac{ELMo} performed comparatively better transfer learning models, \ac{BERT}, \ac{GPT} GPT-2. These transfer learning models exhibited poor results Mohler dataset compared conventional word embeddings. We also concluded ELMo achieve near state art results without training domain-specific data compelling preprocessing data.","     Discourse parsing is largely dominated by     greedy parsers with manually-designed     features, while global parsing is rare due to its     computational expense.  In this paper, we propose a     simple chart-based neural discourse parser that does not     require any manually-crafted features and is based on     learned span representations only. To overcome the     computational challenge, we propose an independence     assumption between the label assigned to a node in the     tree and the splitting point that separates its children,     which results in tractable decoding. We empirically     demonstrate that our model achieves the best performance     among global parsers, and comparable performance to     state-of-art greedy parsers, using only learned     span representations."
"Large-scale language model pretraining become increasingly prevalent achieving high performance variety natural language processing tasks. When applying models specific task, usually fine-tuned using supervised learning, often maximize log probability set human demonstrations. % Fundamentally, objectives weight every word equally lack humanimbued notion important get right less important . While strategy led markedly improved performance, still misalignment fine-tuning objective---maximizing likelihood human-written text---and care about---generating high-quality outputs determined humans. This misalignment several causes: maximum likelihood objective distinction important errors unimportant errors ; models incentivized place probability mass human demonstrations, including low-quality; distributional shift sampling degrade performance . Quality often improved significantly non-uniform sampling strategies beam search , lead repetition undesirable artifacts . Optimizing quality may principled approach overcoming problems. \footnotetext{Throughout paper, error bars represent 1 standard error.} Our goal paper advance methods training language models objectives closely capture behavior care about. To make short-term progress towards goal, focus abstractive English text summarization, long history NLP community , subjective task believe difficult quantify summary quality without human judgments. Indeed, existing automatic metrics evaluating summary quality, ROUGE , received criticism poor correlation human judgments . We follow works , fine-tune language models human feedback using reward learning . We first collect dataset human preferences pairs summaries, train reward model via supervised learning predict human-preferred summary. Finally, train policy via reinforcement learning maximize score given RM; policy generates token text `time step', updated using PPO algorithm based RM `reward' given entire generated summary. We gather human data using samples resulting policy, repeat process. We follow works use large pretrained GPT-3 models many 6.7 billion parameters. \iffalse \footnotetext{Throughout paper, error bars represent 1 standard error.} \fi Our main contributions four-fold. We show training human feedback significantly outperforms strong baselines English summarization. When applying methods version Reddit TL;DR dataset , train policies via human feedback produce better summaries much larger policies trained via supervised learning. Summaries human feedback models preferred labelers original human demonstrations dataset . We show human feedback models generalize much better new domains supervised models. Our Reddit-trained human feedback models also generate high-quality summaries news articles CNN/DailyMail dataset without news-specific fine-tuning, almost matching quality dataset reference summaries. We perform several checks ensure human preferences reflect real quality difference: consistently monitor agreement rates amongst labelers researchers, find researcher-labeler agreement rates nearly high researcher-researcher agreement rates , verify models merely optimizing simple metrics like length amount copying . We conduct extensive empirical analyses policy reward model. We examine impact model data size , study performance continue optimize given reward model , analyze reward model performance using synthetic human-written perturbations summaries . We confirm reward model outperforms metrics ROUGE predicting human preferences, optimizing reward model directly results better summaries optimizing ROUGE according humans . We publicly release human feedback dataset research. The dataset contains 64,832 summary comparisons TL;DR dataset, well evaluation data TL;DR CNN/DM . \iffalse \paragraph{Main result.} When applying methods version Reddit TL;DR dataset , train policies via human feedback produce better summaries much larger policies trained via supervised learning. Summaries human feedback models preferred labelers original human demonstrations dataset . These Reddit-trained human feedback models also generate high-quality summaries news articles CNN/DailyMail dataset without fine-tuning, almost matching quality dataset reference summaries. We perform several checks ensure strong human preferences reflect real quality difference: consistently monitor agreement rates amongst labelers researchers, find researcher-labeler agreement rates nearly high researcher-researcher agreement rates , verify models merely optimizing simple metrics like length amount copying . % we've performed qualitative evaluations policy outputs agree worker judgments ; we've spot-checked surprising conclusions, extractive baselines outperforming human-written summaries CNN/DM, found worker judgments seemed reasonable . \paragraph{Additional analysis.} We also examine impact model data size , study performance continue optimize given reward model , analyze reward model performance using synthetic human-written perturbations summaries , confirm reward model outperforms metrics ROUGE predicting human preferences . \fi The methods present paper motivated part longer-term concerns misalignment AI systems humans want do. When misaligned summarization models make facts, mistakes fairly low-risk easy spot. However, AI systems become powerful given increasingly important tasks, mistakes make likely become subtle safety-critical, making important area research. In work, propose new independence assumption global inference discourse parsing, makes globally optimal inference feasible RST trees. By using global inference, develop simple neural discourse parser. Our experiments show simple parser achieve comparable performance state-of-art parsers using learned span representations.","   As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. %, rather than by model understanding.   For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about---summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences.  We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts \cite{volske2017tl} and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles \cite{hermann2015teaching}, producing summaries nearly as good as the human reference without any news-specific fine-tuning.\footnote{Samples from all of our models can be viewed \href{https://openaipublic.blob.core.windows.net/summarize-from-feedback/website/index.html}{on our website}.}    We conduct extensive analyses to understand our human feedback dataset and fine-tuned models.\footnote{We provide inference code for our 1.3B models and baselines, as well as a model card and our human feedback dataset with over 64k summary comparisons,  \href{https://github.com/openai/summarize-from-feedback}{here}.}   We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans.   We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want."
"Language models exhibit one- few-shot learning growing interest machine learning applications adapt knowledge new information . One-shot language learning physical world also interest developmental psychologists; fast-mapping, ability bind new word unfamiliar object single exposure, much studied facet child language learning . Our goal enable embodied learning system perform fast-mapping, take step towards goal developing embodied agent situated 3D game environment learn names entirely unfamiliar objects single exposure, immediately apply knowledge carry instructions based objects. The agent observes world via active perception raw pixels, learns respond linguistic stimuli executing sequences motor actions. It trained combination conventional RL predictive learning. We find agent architecture consisting standard neural network components sufficient follow language instructions whose meaning preserved across episodes. However, learning fast-map novel names novel objects single episode relies semi-supervised prediction mechanisms novel form external memory, inspired dual-coding theory knowledge representation . With components, agent exhibit slow word learning fast-mapping. Moreover, agent exhibits emergent propensity integrate fast-mapped slowly acquired word meanings single episode, successfully executing instructions ``put dax box"" depend slow-learned fast-mapped word meanings. %An embodied learning system executed fast-mapping flexibility best large-scale text-based language models could lead similarly improved human-agent interaction users game-based agents, virtual-reality avatars robotic assistants. Via controlled generalization experiments, find agent reasonably robust degree variation number objects involved given fast-mapping task test time. The agent also exhibits above-chance success presented name particular object ShapeNet taxonomy instructed interact different exemplar object class, propensity enhanced specific meta-training. We find number unique objects observed agent training temporal aspect perceptual experience objects contribute critically ability generalize, particularly ability execute fast-mapping entirely novel objects. Finally, show dual-coding memory schema provide effective basis derive signal intrinsic motivation conventional memory. %Equipped intrinsic curiosity, agent resolve long episodes requiring fast-binding intermediate environment rewards stimulate requisite information discovery. \paragraph{Limitations.} One limitation work time cost required produce final models. Notably, fine-tuning 6.7B model RL required approximately 320 GPU-days. This adverse effect environment previously noted . Our data collection procedure also expensive compared prior work --- training set costing \textasciitilde \\mathrm{exp} \approx 1.6\mathcal{N})\gamma = 1\lambda = 0.95\pm\pm\pm\pm\pmr\beta\log - \frac{n - 1}{n}T=0$, indicating ROUGE correlates poorly human preferences. For supervised models, lowering temperature larger impact increasing model size. Interestingly, higher temperatures, feedback models actually outperform supervised counterparts . On CNN/DM, ROUGE agrees human evaluations human feedback models transfer better supervised models. However, unsurprisingly, supervised CNN/DM models still achieve much higher ROUGE. In Table, show ROUGE results CNN/DM 6.7B supervised baseline various models literature. We find model achieves ROUGE scores less T5 , slightly greater CNN-2sent-hieco-RBM model , SOTA abstractive summarization CNN/DM mid-2019 according NLP-progress leaderboard. In Table, show bigram overlap statistics models TL;DR CNN/DM datasets proxy much summaries copy frmo post. As Section, compute longest common subsequence bigrams original Reddit post news article, dividing number bigrams summary. We find models evaluated CNN/DM generally copy models evaluated TL;DR. Further, supervised human feedback models copy less pretrained models. \caption{Qualitative examples showing change reward reward model human-generated edits TL;DR summaries make summaries better. Examples randomly selected set edit distance less 5 magnitude change reward greater 0.5. Text strike-through removed original summary edit, text bold added. The reward model sensitive small semantically meaningful changes summary, although makes errors occasion. } We interested understanding relationship different metrics evaluating summaries. To this, compute agreement various metrics, including automatic metrics humans, different subsets data human evaluations. To remove policy quality confounding variable, summary comparisons generated policy temperature value. In Table, use samples 1.3B supervised model T=0.7 TL;DR; Table comparisons 6.7B supervised model T=0.7 TL;DR; Table comparisons 6.7B human feedback model T=0.7 TL;DR; Table comparisons 6.7B supervised baseline trained CNN/DM. Our 6.7B reward model generally agrees labelers much labelers, although ensemble labelers better. On hand, ROUGE generally poor agreement, log probability supervised baselines, simple heuristics like copying length often performing comparably. python reflection/x/jeffwu/merge_agreement_matrix.py cnndm --latex | pbcopy \section{Samples} Here provide non-cherry-picked samples human evaluations various models. In Tables- show samples TL;DR dataset, Tables- show samples CNN/DM dataset . See \href{https://openaipublic.blob.core.windows.net/summarize-from-feedback/website/index.html}{our website} uncurated policy samples. \caption{An example difficult comparison task TL;DR dataset. Summary A makes sound like author watching basketball TV kicked PC anger, whereas Summary B sounds like game froze up, rather author responsible GPU dead. } We show examples samples policy overoptimized rm3. The summaries, clearly long, low quality, full idiosyncrasies, still reflect rough gist post."," Recent work has shown that large text-based neural language models acquire a surprising propensity for one-shot learning. Here, we show that an agent situated in a simulated 3D world, and endowed with a novel dual-coding external memory, can exhibit similar one-shot word learning when trained with conventional RL algorithms. After a single introduction to a novel object via visual perception and language , the agent can manipulate the object as instructed , combining short-term, within-episode knowledge of the nonsense word with long-term lexical and motor knowledge. We find that, under certain training conditions and with a particular memory writing mechanism, the agent's one-shot word-object binding generalizes to novel exemplars within the same ShapeNet category, and is effective in settings with unfamiliar numbers of objects. We further show how dual-coding memory can be exploited as a signal for intrinsic motivation, stimulating the agent to seek names for objects that may be useful later. Together, the results demonstrate that deep neural networks can exploit meta-learning, episodic memory and an explicitly multi-modal environment to account for fast-mapping, a fundamental pillar of human cognitive development and a potentially transformative capacity for artificial agents."
"Emphasis selection emerging research problem natural language processing domain, involves automatic identification words phrases short text would serve good candidates visual emphasis. This research relevant visual media flyers, posters, ads, motivational messages certain words phrases visually emphasized use different color, font, typographic features. This type emphasis help expressing intent, providing clarity, drawing attention towards specific information text. Automatic emphasis selection therefore useful graphic design presentation applications assist users appropriate choice text layout. Prior works speech processing modeled word-level emphasis using acoustic prosodic features. Understanding emphasis speech critical many downstream applications text-to-speech synthesis , speech-to-speech translation , computer assisted pronunciation training . In computational linguistics, emphasis selection closely related problem keyphrase extraction . Keyphrases typically refer nouns noun-phrases capture salient topics long documents scientific articles , news articles , web pages , etc. In contrast, emphasis selection deals short texts , also emphasis could applied words belonging various parts speech. The goal SemEval 2020 - Task 10 design methods automatic emphasis selection short texts. To end, organizers provided dataset consisting 3,000 sentences annotated token-level emphasis multiple annotators. The authors employed standard I-O tagging schema, widely used annotation token-level tags. We approached emphasis selection sequence labeling task solved using Bidirectional Long Short-term Memory model, individual tokens represented using various contextual embedding models. We also employ label distribution learning approach, elegantly accounts disagreements annotators. In paper, proposed novel context-guided capsule network MMT. As significant extension conventional capsule network, DCCN utilizes timestep-specific source-side context vector dynamically guide extraction visual features different timesteps, semantic interactions modalities fully exploited MMT via context-guided dynamic routing mechanism. Moreover, employ DCCN extract visual features two complementary granularities: global visual features regional visual features, respectively. In English-to-German MMT task, model finally achieves +XXX improvement BLEU, +XXX METEOR -XXX TER. Experimental results English-to-German English-to-French MMT tasks strongly demonstrate effectiveness model. In future, plan apply DCCN multimodal tasks visual question answering multimodal text summarization."," This paper presents our submission to the SemEval 2020 - Task 10 on emphasis selection in written text. We approach this emphasis selection problem as a sequence labeling task where we represent the underlying text with various contextual embedding models. We also employ label distribution learning to account for annotator disagreements. We experiment with the choice of model architectures, trainability of layers, and different contextual embeddings. Our best performing architecture is an ensemble of different models, which achieved an overall matching score of 0.783, placing us 15th out of 31 participating teams. Lastly, we analyze the results in terms of parts of speech tags, sentence lengths, and word ordering. \blfootnote{*Authors contributed equally.}"
"The COVID-19 pandemic urged various science disciplines best contribute understanding relieving impact. Thus, scholars practitioners working information sciences dedicating significant effort help. Collecting analyzing data published social media platforms become focus respect. We joined community aims organizing data collected social media , informative uninformative. The WNUT-2020 Task 2 considers tweets recovered, suspected, confirmed death cases well location travel history cases informative. All tweets considered uninformative. The organizers share annotation manual baseline system made available, presumably prevent use manually annotated data encourage broad participation respectively.\footnote{\url{http://noisy-text.github.io/2020/covid19tweet-task.html}, accessed September 4, 2020.} The effort managed terms shared task, organizers share dataset consists annotated tweets conduct evaluation submissions. The task requires participating teams develop short-text classification systems facilitate training development data generalize test set release. Although gold labels training development data available participants, neither gold labels test data annotation guidelines part data shared participants. Moreover, test instances unknown participating teams. They hidden larger dataset. Each team allowed submit two outputs systems developed classifying tweets Codalab page task.\footnote{\url{https://competitions.codalab.org/competitions/25845}, accessed September 4, 2020.} The highest score terms F1 positive class team used rank leaderboard. Integrating automatically created machine learning based models manually formulated rules tackle text classification task promises best worlds. We pursued goal integrating output two deep learning models rule-based system team name COVCOR20. Although integration slightly improves total performance training development sets cross-validation setting, overall performance test data turned slightly worse best ML system. Our best submission ranked 22nd among 55 teams. The integration systems would ranked 27th score used final score team. The deep learning models rule-based system introduced Sections respectively. Next, Section describes integrate output systems. Then Section provide results discussion. Finally, conclude report share future plans continuing line research Section. We undertook comprehensive concept identification network analysis COVID-19. We demonstrated use novel concept recognition relationship discovery engine crafts latest advances natural language processing state-of-the-art solution biomedical entity recognition relationship discovery problem. Several new drugs uncovered studies many different treatment modalities brought surface. We envision solutions wide ranging impact length breadth drug discovery process spanning therapeutic areas."," In the scope of WNUT-2020 Task 2, we developed various text classification systems, using deep learning models and one using linguistically informed rules. While both of the deep learning systems outperformed the system using the linguistically informed rules, we found that through the integration of  the three systems a better performance could be achieved than the standalone performance of each approach in a cross-validation setting. However, on the test data the performance of the integration was slightly lower than our best performing deep learning model. These results hardly indicate any progress in line of integrating machine learning and expert rules driven systems. We expect that the release of the annotation manuals and gold labels of the test data after this workshop will shed light on these perplexing results."
"The phenomenon combining two languages message known code-switching code-mixing . Code-switching indicator bilingual competence , also motivated social cultural factors social status, race, age, etc. . % Instead consider indicator lack competence , cultural social factors motivate study . Although phenomenon studied extensively linguistics , still challenging machines process mixed natural languages. Code-switching notoriously present social media posts chats Twitter, Facebook WhatsApp; consequently making difficult process sentiment expressed contents. %Multilingual people, non-native English speakers, tend code-mix using English-based phonetic typing insertion anglicisms main language. %In addition mixing languages sentence level, fairly common find code-mixing behavior word level. %This linguistic phenomenon cannot tackled conventional NLP systems, based monolingual resources handle combination multiple languages. %Statistics show half messages Twitter language English. This evidence suggests languages, including multilingualism code-mixing, need considered NLP community. % \hl{Statistics show used code switching social media} In work, present Convolutional Neural Network system predict sentiment given code-mixed tweet. The sentiment labels either positive, negative, neutral, languages involved English Spanish. Our best model utilizes Spanish word embeddings tweets require manual feature engineering. % Before classification, English texts normalized anonymize entities, label stylistic patterns, transform words tackle typical issues texts Twitter. % We highlight contributions work follows: % %This paper structured six different sections. Section 2 contains dataset description. As section 3, contains literature review presents existing related work code-mixing. Section 4 depicts methodology. Section 5 devoted presentation discussion experimental results. Finally, recommendations future research opportunities along conclusion reported section 6. % ======================== Article section \section{Conclusion future work} NO: hyphens used state art used premodifying adjective We presented effort scope shared task aims pushing state art classifying short-texts , informative uninformative. We could extend training set cluster mining using Relevancer, use rule-based system extend training set, use rule-based system generate fine-grained data used multi-task setting. \section*{Acknowledgements} The authors Ko University funded European Research Council Starting Grant 714868 awarded Dr. Erdem Y project Emerging Welfare."," %   Code-switching is a phenomenon in which two or more languages are used in the same message. Nowadays, it is quite common to find messages with languages mixed in social media. This phenomenon presents a challenge for sentiment analysis. % forcing the models to use a mix of language resources. In this paper, we use a standard convolutional neural network model to predict the sentiment of tweets in a blend of Spanish and English languages. Our simple approach achieved a F1-score of $0.71$ on test set on the competition. We analyze our best model capabilities and perform error analysis to expose important difficulties for classifying sentiment in a code-switching setting."
". % % % final paper: en-us version % % space normally used marker This work licensed Creative Commons Attribution 4.0 International License. License details: \url{http://creativecommons.org/licenses/by/4.0/}. } Emphasis selection written text visual media proposed . The purpose shared task design automatic methods emphasis selection, i.e. choosing candidates emphasis short written text, enable automated design assistance authoring. For example, mentions technique applied graphic design applications Adobe Spark perform automatic text layout using templates include images text different fonts colors. The major challenge given thousands annotated short text data without context text visual background images, asked learn author- domain-specific emphatic short text. Besides, short text data annotated crowd-sourcing workers. And find different annotators different standards, increases difficulty task. To identify important words, model task sequential labeling problem. Our base models leverage different unsupervised language model ERNIE 2.0 , XLM-ROBERTA , ROBERTA ALBERT . These large unsupervised models pre-trained large amount unannotated data carry valuable lexical, syntactic, semantic information training corpora. Our approach follows: firstly, word-level output representations sentence computed pre-trained models fed designed downstream neural network word selections; secondly, finetune downstream networks together pre-trained models annotated training data; thirdly, investigate several different objective functions learn model; finally, apply feature engineering several data augmentation strategies improvement. The rest paper organized follows. In Section , briefly overview related works system. Section shows details approach. Our experiments shown Section , Section concludes. Code-switching interesting problem holding important presence social media, combined informal writing style increases challenges social media processing sentiment analysis. We experimented Sentimix Spanglish dataset using CNN model Spanish embeddings. We achieve precsion, recall, F1 score 0.80, 0.64, 0.71 respectively. reporting good enough results competition . Our analyses suggest deep learning model easily biased presence cue words vulgar expressions sentiment analysis. We found occurs mostly cue word English. This observation requires deeper analysis. \hl{the hard general ization contrast even code-switching scenario}. We also highlight need address complex language usage informality sarcasm. well dealing messages involving extralinguistic information usually needs world knowledge understanding processed. Furthermore, also pointed subjectivity annotation sentiment labels problem deserves addressed. We plan test contextual multilingual embeddings leverage language tags non-linguistic constructs hashtags emojis. ======================== Article section Do include section submitting paper review.","   This paper describes the system designed by ERNIE Team which achieved the first place in SemEval-2020 Task 10: Emphasis Selection For Written Text in Visual Media. Given a sentence, we are asked to find out the most important words as the suggestion for automated design. We leverage the unsupervised pre-training model and finetune these models on our task. After our investigation, we found that the following models achieved an excellent performance in this task: ERNIE 2.0, XLM-ROBERTA, ROBERTA and ALBERT. We combine a pointwise regression loss and a pairwise ranking loss which is more close to the final $Match_{m}$ metric to finetune our models. And we also find that additional feature engineering and data augmentation can help improve the performance. Our best model achieves the highest score of 0.823 and ranks first for all kinds of metrics."
"The 2020 edition Workshop Noisy User-generated Text hosted shared task `Identification Informative COVID-19 English Tweets'. The task involves automatically identifying whether English Tweet related novel coronavirus `informative' not. For tweet considered informative context, provide information recovered, suspected, confirmed, death cases well location travel history cases. The goal developing automated system help track development COVID-19 outbreak provide users information related virus, e.g. new suspicious/confirmed cases near/in users' regions. Aligned goals shared task, paper details use state-of-the-art natural language processing techniques task. %to identify informative tweets related COVID-19. We experiment variety methods, ranging feature-based classifiers leveraging recent advances pre-trained neural architectures . To improve performance, incorporate unlabelled tweets released COVID-19 via masked language modelling pseudo-labelling techniques. Our best performing model ensemble uses Logistic Regression combine output probabilities several base classifiers . We analyze impact pre-processing semi-supervision ablation studies. Through qualitative adversarial analysis, show predictions BERT model sensitive towards specific tokens `confirmed case' even locations numerals, also guides data pre-processing steps. In paper, present system ranks first SemEval-2020 Task 10. Our solution contains several strategies provide detailed experiments analyze effective. Our experiments show models empowered pre-trained language models effective, especially ERNIE 2.0. Besides, lexical features, pairwise loss, data augmentation also bring improvement models. include bib file like this:"," We describe our system for WNUT-$2020$ shared task on the identification of informative COVID-19 English tweets. Our system is an ensemble of various machine learning methods, leveraging both traditional feature-based classifiers as well as recent advances in pre-trained language models that help in capturing the syntactic, semantic, and contextual features from the tweets. We further employ pseudo-labelling to incorporate the unlabelled Twitter data released on the pandemic. Our best performing model achieves an F1-score of $0.9179$ on the provided validation set and $0.8805$ on the blind test-set."
"Text classification important problem natural language processing . The task assign document one predefined categories. It wide range applications sentiment analysis~, topic categorization~, email filtering~. Early machine learning approaches text classification based extraction bag-of-words features followed supervised classifier na\""ive Bayes~ linear SVM~. Later, better word representations introduced, latent semantic analysis~, skipgram~, fastText~, improved classification accuracy. Recently, recurrent convolutional neural network~ models introduced utilize word order grammatical structure. Many complex variations models proposed improve text classification accuracy, e.g. training one-hot CNN one-hot bidirectional LSTM network dynamic max-pooling . Current state-of-the-art approaches text classification involve using pretrained LSTMs complex computationally intensive models . DL15 argued randomly initialized LSTMs difficult optimize lead worse performance linear models. Therefore, improve performance, proposed pretraining LSTM either language model sequence auto-encoder. However, pretraining using complicated models time consuming, major disadvantage may always feasible. In paper, consider BiLSTM classifier model similar one proposed DL15 text classification. For simple BiLSTM model pretrained embeddings, propose training strategy achieve accuracy competitive previous purely supervised models, without extra pretraining step. We also perform ablation studies understand aspects proposed training strategy result improvement. Pretraining approaches often use extra unlabeled data addition labeled data. We explore applicability semi-supervised learning training framework, prior pretraining step. In regard, propose mixed objective function SSL utilize labeled unlabeled data obtain improvement classification. To summarize, contributions follows: In paper, describe system identify informative COVID-19 English tweets. We find ensemble model uses logistic regression combine predictions variety feature-based neural methods achieves best performance shared task. Our analysis shows incorporating unlabelled tweets results consistent performance gains. We show trained model sensitive specific tokens tweets, hence, advice exercising caution deploying machine learning models downstream monitoring applications."," \begin{quote}     In this paper, we study bidirectional LSTM network for the task of text classification using both supervised and semi-supervised approaches. Several prior works have suggested that either complex pretraining schemes using unsupervised methods such as language modeling or complicated models are necessary to achieve a high classification accuracy. However, we develop a training strategy that allows even a simple BiLSTM model, when trained with cross-entropy loss, to achieve competitive results compared with more complex approaches. Furthermore, in addition to cross-entropy loss, by using a combination of entropy minimization, adversarial, and virtual adversarial losses for both labeled and unlabeled data, we report state-of-the-art results for text classification task on several benchmark datasets. In particular, on the ACL-IMDB sentiment analysis and AG-News topic classification datasets, our method outperforms current approaches by a substantial margin. We also show the generality of the mixed objective function by improving the performance on relation extraction task.\footnote{\mycodeurl} \end{quote}"
"Coreference resolution aims identifying expressions refer entity text. It helps derive correct interpretation text binding antecedents pronouns together recognizing syntactic relationship among them. The coreference resolution considered critical preprocessing step various high-level natural language processing tasks including document summarization, question answering, information extraction . Existing coreference resolution approaches divided two major categories: mention-pair models entity-mention models . One main shortcomings mention-pair model making coreference decision without entity-level information. Moreover, lack information preceding clusters may result contradictory links. The entity-mention model tries make use non-local information encouraging sharing features across mentions point real-world entity. However, coreferent mentions usually spread far apart text, makes extremely difficult define effective global features. Previous studies either count long-term memory variants implicitly capture global features seek incorporate features clusters already formed determine whether mention coreferent preceding cluster . The former might miss important features specific pairwise predictions without help explicit entity-level features, latter may suffer error propagation false clusters used create entity-level features making future predictions. Taking text ``On November 3, 1992, Clinton elected 42nd president United States, following year Hillary Clinton became first lady. In 2013, Presidential Medal Freedom."" example, assume three mentions ``Clinton"", ``Hillary Clinton"", ``he"" well identified. The traditional mention-pair model likely group three mentions cluster shown Figure since ``Clinton"" ``Hillary Clinton"" share surname, ``he'' agrees ``Clinton"" gender number. To make use information clusters already formed, recent studies try better represent current mention incorporating features derived preceding cluster probably join . However, methods allow information shared forward fashion, i.e., antecedent expressions postcedent ones, prone reaching results shown Figure . The reason ``Hillary Clinton'' merged ``Clinton'' form cluster, pronoun ``he'' either joins formed cluster begins new one itself. Even though errors might recovered using proper decoding algorithm test time, maximum spanning tree algorithm, similar errors cannot completely eliminated. If information shared iteratively forward backward ways, disagreement gender ``Hillary Clinton'' ``he'' detected representation ``Clinton'' updated two possible co-references, helps find correct result Figure . Recently, graph neural network gained increasing popularity due ability modeling dependencies nodes graph . For coreference resolution, mentions linked via edges modeling likely two linked mentions refer entity. The features nodes shared direction message passing neighborhood aggregation iterative way. We found entity-centric features well captured GNN, achieving close state-of-the-art performance. To avoid contradictory links mention clustering results, propose use variant maximum spanning tree algorithm, second-order decoding algorithm instead traditional greedy search algorithm beam search algorithm . We factorize score tree sum arc-pair scores. A pair arcs link three different mentions, connected mentions viewed small cluster. Our global inference algorithm second-order features helps define powerful entity-level features clusters mentions aggregating scores small clusters. Traditional coreference resolution methods usually include three successive steps: mention detection, candidate pair generation, mention clustering . However, recent studies show joint solutions usually lead improved performance pipelined systems avoiding error propagation. We follow line research formulate coreference resolution joint manner. Our contributions summarized follows: graph neural networks introduced perform coreference resolution, aims better leverage entity-centric information encouraging sharing features across mentions refer entity; global inference algorithm second-order features presented optimally cluster mentions consistent groups; show GNN-based method combing second-order decoding algorithm achieved close state-of-the-art performance CoNLL-2012 coreference resolution benchmark. We show simple BiLSTM model using maximum likelihood training result competitive performance text classification tasks without need additional pretraining step. Also, addition maximum likelihood, using combination entropy minimization, adversarial, virtual adversarial training, report state-of-the-art results several text classification datasets. This mixed objective function also generalizes well tasks relation extraction outperforms current best models. \small"," One of the major challenges in coreference resolution is how to make use of entity-level features defined over clusters of mentions rather than mention pairs. However, coreferent mentions usually spread far apart in an entire text, which makes it extremely difficult to incorporate entity-level features. We propose a graph neural network-based coreference resolution method that can capture the entity-centric information by encouraging the sharing of features across all mentions that probably refer to the same real-world entity. Mentions are linked to each other via the edges modeling how likely two linked mentions point to the same entity.  Modeling by such graphs, the features between mentions can be shared by message passing operations in an entity-centric manner. A global inference algorithm up to second-order features is also presented to optimally cluster mentions into consistent groups. Experimental results show our graph neural network-based method combing with the second-order decoding algorithm  achieved close to state-of-the-art performance on the English CoNLL-2012 Shared Task dataset."
"Encoding linguistic units words, phrases sentences low-dimensional vectors core preliminary task deep learning natural language. The current language representation learning usually done different individual levels, typically, word sentence. The former includes pioneering works word2vec, GloVe fastText , latter includes recent so-called contextualized representations ELMo, GPT, BERT, XLNet ELECTRA . Nevertheless, works done uniformly learning representing linguistic units different hierarchies vector space. Actually, nearly existing work still focus individual granular language unit representation learning . However, universal representation among different levels linguistic units may offer great convenience needed handle free text language hierarchy unified way. As well known that, embedding representation certain linguistic unit enables linguistics-meaningful arithmetic calculation among different vectors, also known word analogy. For example, vector - vector + vector results vector . Thus universal representation may generalize good analogy features meaningful arithmetic operation onto free text language levels involved together. For example, Eat onion : Vegetable :: Eat pear : Fruit. In paper, explore regularities representations including words, phrases sentences vector space. To end, introduce universal analogy tasks derived Google's word analogy dataset. In addition, train Transformer-based model compare currently popular representation methods. Experimental results demonstrate well-trained Transformer-based models able map sequences variable lengths shared vector space similar sequences close other. Meanwhile, addition subtraction embeddings reflect semantic syntactic connections sequences. In addition, explore applicability characteristic retrieval-based chatbots evaluation insurance FAQ task, universal representation models significantly outperform TF-IDF BM25. We proposed coreference resolution system based graph neural networks enhanced second-order decoding algorithm. Modeling mentions relationships multiple-layer graph neural networks makes possible aggregate features mentions pointing entity iterative way, global inference algorithm second-order features helps produce optimal consistent clustering results. Experiments English CoNLL-2012 shared task dataset demonstrated model achieved close state-of-the-art performance coreference resolution task."," Despite the well-developed cut-edge representation learning for language, most language representation models usually focus on specific level of linguistic unit, which cause great inconvenience when being confronted with handling multiple layers of linguistic objects in a unified way. Thus this work introduces and explores the universal representation learning, i.e.,  embeddings of different levels of linguistic unit in a uniform vector space through a task-independent evaluation. We present our approach of constructing analogy datasets in terms of words, phrases and sentences and experiment with multiple representation models to examine geometric properties of the learned vector space. Then we empirically verify that well pre-trained Transformer models incorporated with appropriate training settings may effectively yield universal representation. Especially, our implementation of fine-tuning ALBERT on NLI and PPDB datasets achieves the highest accuracy on analogy tasks in different language levels. Further experiments on the insurance FAQ task show effectiveness of universal representation models in real-world applications."
"The ability learn tasks continuously lifetime limited supervision hallmark human intelligence. This enabled efficient transfer knowledge past experience. On contrary, current deep learning methods subjected learning new tasks sequential manner, suffer catastrophic forgetting , previous information lost due shift data distribution. Non-stationarity inevitable real world data continuously evolving. Thus, need design robust machine learning mechanisms deal catastrophic interference. Lifelong learning, also known continual learning , aims developing models continuously learn stream tasks sequence without forgetting existing knowledge rather building information acquired previously learned tasks order learn new tasks . One conceptualization accelerate learning positive transfer tasks minimizing interference respect network updates . Many approaches continual learning employ manually-designed techniques regularization gradient alignment mitigate catastrophic forgetting, shown effective computer vision reinforcement learning tasks. A recent trend continual learning, well machine learning general, directly learn generalizable solutions via meta-learning . Meta-learning aims learn new tasks quickly using limited number examples training many related tasks. In continual learning, meta-learning applied objective learning new tasks continually relatively small number examples per task traditional continual learning setup interleaving several past examples memory component, i.e. experience replay . While high rate experience replay usually mitigates catastrophic forgetting, comes closer multi-task learning lifelong learning setup computationally expensive learning data stream real-life applications. In natural language processing , continual learning still remains relatively unexplored . Despite success large pre-trained language models BERT , still require considerable amounts in-domain examples training new tasks prone catastrophic forgetting . Existing continual learning approaches language processing tasks include purely replay-based methods , meta-learning based method well generative replay-based method . However, approaches suffer several important limitations: require task identifiers, high rate replay multiple epochs training, deviates realistic lifelong learning scenario; tend expensive inference step . In paper, propose novel approach lifelong learning language processing tasks using meta-learning experience replay sparse time size. We consider realistic setting one pass training set possible task identifiers available. We extend two algorithms, namely online meta-learning neuromodulatory meta-learning algorithm domain NLP augment episodic memory module experience replay. While original objective continually learn new sequence tasks testing time, enhance conventional continual learning setup evaluation previously seen tasks, thus directly addressing problem catastrophic forgetting. Furthermore, realizing experience replay query set, directly optimize prevent forgetting. We show combining strong language model BERT along meta-learning sparse replay produces state-of-the-art performance lifelong text classification relation extraction benchmarks compared current methods realistic setting. To best knowledge, first meta-learning approach lifelong learning language tasks incorporates sparse replay. Through experiments, demonstrate approach considerably efficient previous work terms computational complexity well memory usage. To facilitate research field, make code publicly available. This work concentrates less concentrated language representation, seeking learn uniform vector form across different linguistic unit hierarchies. Far apart learning either word sentence representation, find training Transformer models large-scale corpus effectively learns universal representation words, phrases sentences. We especially provide universal analogy datasets \footnote{Our annotated datasets publicly released anonymous reviewing period.} insurance FAQ dataset evaluate models different perspectives. The well-trained universal representation model holds promise demonstrating accurate vector arithmetic regard words, phrases sentences applications FAQ retrieval tasks."," Lifelong learning requires models that can continuously learn from sequential streams of data without suffering catastrophic forgetting due to shifts in data distributions. Deep learning models have thrived in the non-sequential learning paradigm; however, when used to learn a sequence of tasks, they fail to retain past knowledge and learn incrementally. We propose a novel approach to lifelong learning of language tasks based on meta-learning with sparse experience replay that directly optimizes to prevent forgetting. We show that under the realistic setting of performing a single pass on a stream of tasks and without any task identifiers, our method obtains state-of-the-art results on lifelong text classification and relation extraction. We analyze the effectiveness of our approach and further demonstrate its low computational and space complexity."
"Humans possess ability encode express wide range intricate verbal non-verbal cues based goal context. This evolved complementary ability detect nuanced cues everyday communication. This ability result top-down processing based context learning humans able encode decode person person information flow efficiently. Context typically set communicated multi-modal cues. Inspired this, several studies shown multi-modal input systems improve accuracy tasks involving human communication, speech recognition , emotion recognition speaker recognition . Recently, use generalized feature representations become prevalent computer vision natural language research. Computer vision tasks like object detection semantic segmentation show improved accuracy features images extracted using models trained large amounts data like ImageNet . In natural learning literature, generalized embeddings like GloVe word2vec demonstrated state art performance several tasks like word similarity, word analogy named entity recognition. For speech applications like automatic speech recognition , speaker recognition paralinguistics still traditional use hand-crafted features like MFCCs, LFBEs features toolkits like openSMILE . However, also demonstrated features learned directly audio improve performance amount training data large enough . The research various domains demonstrated transfer learning models trained large datasets improve accuracy subsequent tasks. This especially important size labeled datasets large. There variety multi-modal tasks like emotion recognition still large amounts publicly available datasets. Motivated this, propose model learn embeddings combine features audio, video, text modalities improve performance downstream tasks. The main contribution paper understand leverage large datasets build representations outperform models built specific tasks datasets limited. For work, use emotion recognition downstream task evaluate embeddings. In practical applications, possible modalities available machine learning system inference. For example, applications use video web-based applications, disturbance communication network lead missing audio visual input. This leads second objective study; perform ablation studies understand impact missing modality, understand compensate it. This paper organized follows; Section , discuss prior work multi-modal tasks embedding generation techniques. Our proposed technique embedding extraction presented Section . In discuss training setup data. Finally, present results Section conclude Section . We showed pre-trained transformer-based language models, meta-learning sparse experience replay produce synergy improves lifelong learning language tasks. This important step moving away manually-designed solutions simpler, generalizable methods ultimately achieve human-like learning. Meta-learning could exploited combined setting few-shot lifelong learning. It might also promising learning distinct NLP tasks curriculum learning fashion.","     General embeddings like word2vec, GloVe and ELMo have shown a lot of success in natural language tasks. The embeddings are typically extracted from models that are built on general tasks such as skip-gram models and natural language generation. In this paper, we extend the work from natural language understanding to multi-modal architectures that use audio, visual and textual information for machine learning tasks. The embeddings in our network are extracted using the encoder of a  transformer model trained using multi-task training. We use person identification and automatic speech recognition as the tasks in our embedding generation framework. We tune and evaluate the embeddings on the downstream task of emotion recognition and demonstrate that on the CMU-MOSEI dataset, the embeddings can be used to improve over previous state of the art results."
"%============================================================================ Natural language understanding often requires ability comprehend reason expressions involving numbers. This produced recent rise interest build applications automatically solve math word problems~. These math problems consist textual description comprising numbers question guide reasoning process get numerical solution . This complex task The research community focused solving mainly two types mathematical word problems: arithmetic word problems algebraic word problems . Arithmetic word problems solved using basic mathematical operations involve single unknown variable. Algebraic word problems, hand, involve complex operators square root, exponential logarithm multiple unknown variables. In work, focus solving arithmetic word problems one illustrated \figref{fig:example}. This figure illustrates . The main idea paper explore use tree-based Recursive Neural Networks encode score expression tree . This contrasts predominantly sequential neural representations encode problem statement left right vice versa. By using Tree-RNN architectures, naturally embed equation inside tree structure link structure directly reflects various mathematical operations operands selected sequential textual input. We hypothesize structured approach efficiently capture semantic representations candidate equations solve complex arithmetic problems involving multiple and/or non-commutative operators. To test results, use recently introduced SingleEQ dataset . It contains collection 508 arithmetic word problems varying degrees complexity. This allows us track performance evaluated systems subsets require different reasoning capabilities. More concretely, subdivide initial dataset different subsets varying reasoning complexity non-commutative operations), investigate whether performance proposed architecture remains consistent across problems increasing complexity. \Figref{fig:conceptualview} provides high-level conceptual view interconnection main components proposed system. The processing flow consists two main steps. In first step, use candidate generator generate list potential candidate equations solving particular arithmetic word problem. To achieve this, employ Integer Linear Programming constraint optimization component proposed \mbox{} . In second step, candidate equations ranked candidate ranker, equation highest score chosen solution processed arithmetic word problem . In paper, focus second step exploring impact structural Tree-RNN-based sequential Long Short Term Memory-based candidate equation encoding methods. More specifically, define two Tree-RNN models inspired work \TreeLSTM{} models: . In rest manuscript refer general tree-structured architecture models \TreeLSTM{}. The main difference two that, \TLSTM{} child node representations summed up, \NTLSTM{} concatenated. Unlike representation used , input given word embeddings, Tree-LSTM models also take input operation embeddings represent arithmetic operators . This allows architecture distinguish different operators contained particular expression tree. We show \NTLSTM{} suitable deal equations involve non-commutative operators architecture able capture order operands. We also compare \TreeLSTM{} models sequential LSTM model call \BLSTM{}. All models take input contextualized representation numbers text produced bidirectional \LSTM{} layer . After conducting thorough multi-fold experimentation phase involving multiple random weight re-initializations order ensure validity results, show main added value \TreeLSTM{}-based models compared state-of-the-art methods lays increased performance complex arithmetic word problems. More concretely, contribution three-fold: %---------------------------------------------------------------------------- We proposed simple generative noise model generation adversarial examples training data augmentation NMT systems. Our results demonstrate NMT systems trained using adversarial examples resilient noisy input data. We show baseline NMT systems, noisy inputs cause substantial drop translation quality , systems trained using adversarial examples translation quality changes comparatively little . In terms translation robustness, systems trained adversarial examples average yield 50\ consistency improvement compared baselines trained clean data. Methods proposed useful achieving NMT robustness orthographic interpunctual variation input data. This especially beneficial use cases NMT systems used translate texts informal origins, chat conversations, social media posts web pages comment sections.","   Solving arithmetic word problems is a cornerstone task in assessing language understanding and reasoning capabilities in NLP systems. Recent works    use automatic extraction and ranking of candidate solution equations providing the answer to arithmetic word problems. In this work, we explore novel approaches to score such candidate solution equations using tree-structured   recursive neural network  configurations.    The advantage of this Tree-RNN approach over using more established sequential representations, is that it can naturally capture the structure of the equations. Our proposed method consists of transforming the mathematical expression of the equation into an expression tree. Further, we encode this tree into a Tree-RNN by using different \TreeLSTM{} architectures.      Experimental results show that our proposed method     \begin{enumerate*}[]     \item  improves overall performance with more than 3\% accuracy points compared to previous state-of-the-art, and with over 15\% points on a subset of problems that require more complex reasoning, and \item outperforms sequential LSTMs by 4\% accuracy points on such more complex problems.  \end{enumerate*}"
"Semantic role labeling , namely semantic parsing, shallow semantic parsing task aims recognize predicate-argument structure predicate sentence, whom, when, etc. Specifically, SRL seeks identify arguments label semantic roles given predicate. SRL important method obtaining semantic information beneficial wide range natural language processing tasks, including machine translation, question answering, discourse relation sense classification relation extraction. SRL split four subtasks: predicate detection, predicate disambiguation, argument identification, argument classification. For argument annotation, two formulizations . One based constituents , based dependencies. The other, proposed CoNLL-2008 shared task, also called semantic dependency parsing annotates heads arguments rather phrasal arguments. Figure shows example annotations. In prior SRL work, considerable attention paid feature engineering, struggles capture sufficient discriminative information compared neural network models, capable extracting features automatically. In particular, syntactic information, including syntactic tree features, known extremely beneficial SRL since large scale empirical verification of~\citet{punyakanok-etal-2008-importance}. Despite success, work suffered erroneous syntactic input, leading unsatisfactory performance. To alleviate issues, \citet{marcheggiani-etal-2017-simple,he-etal-2017-deep} proposed simple effective neural model SRL without syntactic input. Their work suggested neural SRL rely syntactic features, contradicting belief syntax necessary prerequisite SRL, believed early as~\citet{gildea-palmer-2002-necessity}. This dramatic contradiction motivated us make thorough exploration syntactic contribution SRL. Both span dependency effective formal representations semantics, though unknown form, span dependency, would better convenience effectiveness semantic machine learning later applications long time. This topic roughly discussed , concluded dependency SRL system clearly outperformed span-based system gold syntactic structure transformation; however, due different requirements downstream task applications, span dependency remain focuses research. Additionally, two forms SRL may benefit joint rather separated development. We, therefore, revisit syntax roles solid empirical basis explore syntax roles two styles syntax information equal quality, respectively. Recent works syntax contributions limited individual models ways syntax utilized. The conclusions drawn syntax roles therefore limitations. In order reduce limitations, explored three typical strong baseline models two categories syntactic utilization methods. In addition, pre-trained language models, ELMo BERT , build contextualized representations, continue provide gains NLP benchmarks, \citet{hewitt-manning-2019-structural} showed structure syntax information emerges deep models' word representation spaces. Whether neural SRL models benefit explicit syntax information addition implicit syntax information, however, another issue consider. %This paper focus semantic dependency parsing formulate SRL one two sequence tagging tasks predicate-specific encoding. With help proposed -order argument pruning algorithm syntactic tree, model obtains state-of-the-art scores CoNLL benchmarks English Chinese. Besides, SRL literature dedicated impressive performance gains English, multiple languages receive relatively little attention. Although human languages basic commonalities syntactic structure even different levels grammar, differences also obvious. The study syntactic roles needs examined context multiple languages verifying effectiveness applicability. In order quantitatively evaluate contribution syntax SRL, adopt ratios labeled F score semantic dependencies labeled attachment score syntactic dependencies, F score syntactic constituents. This ration first introduced CoNLL-2008 Shared Task evaluation metric. Considering various syntactic parsers contribute different syntactic inputs varying levels quality, ratio provides fairer comparison syntactically-driven SRL systems, empirical study surveys. ============================================================================ In work addressed reasoning component involved solving arithmetic word problems. We proposed recursive tree architecture encode underlying equations solving arithmetic word problems. More concretely, proposed use two different \TreeLSTM{} architectures task scoring candidate equations. We performed extensive experimental study SingleEQ dataset demonstrated consistent effectiveness models compared current state-of-the-art. We observed that, strong simple instances involving single operations, current feature-based state-of-the-art model exhibits significant gap performance mathematical problems whose solution comprises non-commutative and/or multiple operations. This reveals weakness method capture intricate nature reasoning necessary solve complex arithmetic problems. Furthermore, experiments show that, traditional sequential approach based recurrent encoding implemented using \biLSTMs{} equation proves robust baseline, outperformed recursive \TreeLSTM{} architecture encode candidate solution equation complicated problems require multiple operations solved. This difference performance becomes significant introduce additional noise set candidates adding incorrect equations contain non-commutative operations. ============================================================================"," Semantic role labeling  is dedicated to recognizing the semantic predicate-argument structure of a sentence.  Previous studies in terms of traditional models have shown syntactic information can make remarkable contributions to SRL performance; however, the necessity of syntactic information was challenged by a few recent neural SRL studies that demonstrate impressive performance without syntactic backbones and suggest that syntax information becomes much less important for neural semantic role labeling, especially when paired with recent deep neural network and large-scale pre-trained language models. Despite this notion, the neural SRL field still lacks a systematic and full investigation on the relevance of syntactic information in SRL, for both dependency and both monolingual and multilingual settings.  This paper intends to quantify the importance of syntactic information for neural SRL in the deep learning framework. We introduce three typical SRL frameworks , sequence-based, tree-based, and graph-based, which are accompanied by two categories of exploiting syntactic information: syntax pruning-based and syntax feature-based. Experiments are conducted on the CoNLL-2005, 2009, and 2012 benchmarks for all languages available, and results show that neural SRL models can still benefit from syntactic information under certain conditions. Furthermore, we show the quantitative significance of syntax to neural SRL models together with a thorough empirical survey using existing models."
"Building dialogue system converse people naturally meaningfully one challenging problems towards high-level artificial intelligence, drawing increasing interests academia industry area. Most existing dialogue systems either generation-based retrieval-based. Given dialogue context, generation-based approaches synthesize response word word conditional language model, retrieval-based methods select proper response candidate pool. In paper, focus retrieval-based approaches superior providing informative responses widely applied several famous commercial products XiaoIce Microsoft AliMe Assist Alibaba. We consider response selection task multi-turn dialogues, retrieval model ought select proper response measuring matching degree multi-turn dialogue context number response candidates. Earlier studies concatenate context single utterance calculate matching score utterance-level representations. Later, response selection models perform context-response matching within representation-matching-aggregation paradigm, turn utterance represented individually sequential information aggregated among sequence utterance-response matching features. To improve performance response selection, recent approaches consider multiple granularities representations matching propose complicated interaction mechanisms context response. Recently, wide range studies shown pre-trained language models , BERT, XLNET RoBERTa, large corpus learn universal language representations, helpful various downstream natural language processing tasks get rid training new model scratch. To adapt pre-trained models multi-turn response selection, \citet{whang2020domain} \citet{gu2020speaker} make first attempt utilize BERT learn matching model, context candidate response first concatenated fed PLMs calculating final matching score. These pre-trained language models well capture interaction information among inter-utterance intra-utterance multiple transformer layers. Although PLM-based response selection models demonstrate superior performance due strong representation ability, still challenging effectively learn task-related knowledge training process, especially size training corpora limited. Naturally, studies typically learn response selection model context-response matching task %learn matching model single response prediction task, overlook many potential training signals contained dialogue data. %come rich characteristics dialogue text. Such training signals might beneficial context understanding produce better features response prediction. Besides, response retrieved existing dialogue systems supervised conventional way still faces critical challenges, including incoherence inconsistency. On account issues, paper, instead configuring complex context-response matching models, propose learning context-response matching model auxiliary self-supervised tasks designed dialogue data based pre-trained language models . Specifically, introduce four self-supervised tasks including next session prediction, utterance restoration, incoherence detection consistency discrimination, jointly train PLM-based response selection model auxiliary tasks multi-task manner. On one hand, auxiliary tasks help improve capability response selection model understand dialogue context measure semantic relevance, consistency coherent context response candidates. On hand, guide matching model effectively learn task-related knowledge fixed amount train corpora produce better features response prediction. We conduct experiments two benchmark data sets multi-turn response selection: Ubuntu Dialog Corpus E-commerce Dialogue Corpus. Evaluation results show proposed approach significantly better state-of-the-art models datasets. Compared previous state-of-the-art methods, model achieves 2.9\% absolute improvement terms Ubuntu dataset 4.8\% absolute improvement E-commerce dataset. Furthermore, applied proposed self-supervised learning schema non-PLM-based response selection models, e.g., dual LSTM ESIM. Experimental results indicate learning schema also bring consistent significant improvement performance existing matching models. Surprisingly, self-supervised learning, simple ESIM even performs better BERT ubuntu dataset, demonstrating approach beneficial various matching architectures. % We publish source code later. In summary, contributions three-fold: We perform four generative tests asses learning reduplication deep convolutional networks: test proportion outputs latent codes manipulated marginal values, test interpolating latent variables, test reduplication unobserved data ciwGAN architecture, replication test reduplication unobserved data bare WaveGAN architecture. All four tests suggest deep convolutional networks learn simple identity-based pattern speech called reduplication, i.e.~a process copies phonological material express new meaning. The ciwGAN network learns encode meaningful representation --- presence reduplication latent codes. There near one-to-one correspondence two latent codes reduplication. By interpolating latent codes, cause bare form gradually turn reduplicated form major changes output majority cases. These results close would considered appearance symbolic computation algebraic rules. Additional evidence approximation symbolic computation emerges comes bare GAN replication experiment: substantial drop regression estimates first one two latent variables highest regression estimates, suggesting even without requirement produce informative data, network ``discretizes'' continuous highly variable phonetic feature --- presence reduplication --- uses small subset latent space represent phonetic/phonological property. Encoding identity-based pattern meaningful representation latent space emerges completely unsupervised manner ciwGAN architecture --- requirement Generator output informative data. Reduplicated unreduplicated forms never paired training data. The network fed bare reduplicated forms randomly. This unsupervised training approximates conditions language acquisition: human language learner needs represent reduplication pair bare reduplicated forms raw unlabeled acoustic data . The ciwGAN learns group reduplicated unreduplicated forms assign unique representation process reduplication. In fact, one-hot vector Generator learns associate reduplication training modeled representation unique meaning/function reduplication adds, line approach represent unique semantics one-hot vectors . The dependencies deep neural networks cannot learn ongoing line inquiry. The results computational experiments presented paper suggest Generator network learns extend learned identity-based patterns novel unobserved data. While network trained reduplicated items start [s], able elicit reduplication output following technique proposed . First, identify variables correspond phonetic/phonological representation presence [s], based , proposes setting single variables well training range reveal underlying value latent variable forces desired property output. We thus force [s] reduplication output simultaneously. For example, network outputs [\textipa{s@siju}] force reduplication [s] output; however, never sees [\textipa{s@siju}] training data --- [\textipa{siju}] reduplicated forms, none included [s]. We also excluded reduplicated unreduplicated items contain sequences acoustically similar [s]. This suggests network extends reduplication novel forms even absence acoustically similar reduplication patterns. Thus, experiments confirm network uses individual latent variables represent linguistically meaningful representations . Setting individual variables values well training interval reveals underlying values. By manipulating individual variables, explore representations learned well interactions different variables work . The results study make apparent deep convolutional network capable encoding different phonetic properties individual latent variables, also processes abstract copying reduplication. One advantages probing learning deep convolutional neural networks speech data trained GANs innovative outputs violate training data structured highly informative ways. The innovative output reduplication [s]-initial forms [\textipa{s@siju}] directly paralleled acoustic outputs read L1 speaker American English absent training data. Acoustic analysis shows high degree similarity generated reduplicated forms human recordings, meaning network learns output novel data linguistically interpretable resemble human speech processes even though absent training data. Thus, results experiments implications cognitive models speech acquisition. It appears one processes long held hallmark symbolic computation language, reduplication, emerge deep convolutional network without language-specific components model even trained raw acoustic inputs. The present paper tests simple partial reduplicative pattern CV copied appears base item. This perhaps computationally simplest reduplicative pattern. The world's languages feature large number reduplicative patterns C, CVC, types phonological content copied. Additionally, reduplication precede follow base inserted inside base item. This paper thus also appeal use well-understood identity-based patterns speech various degrees complexity test patterns deep convolutional networks cannot learn self-organization meaningful representations discretization continuous space emerges deep convolutional networks. \subsubsection*{Acknowledgements} This research funded grant new faculty University Washington. I would like thank Ella Deaton reading training data. \subsubsection*{Declaration interests} The author declares known competing financial interests personal relationships could appeared influence work reported paper. \iffalse \fi \clearpage { } \clearpage \section{Training data} } \caption{All items C voiceless voiced stop used training data. All items feature two unique repetitions read L1 American English speaker. The items transcription presented reader. } \caption{All items C [m], [n], [v] used training data. All items feature two unique repetitions read L1 American English speaker. The items transcription presented reader. } \caption{All items C [s] corresponding number unique repetitions training data per item. These items never reduplicated training data. All items read L1 American English speaker. The items transcription presented reader. }"," Building an intelligent dialogue system with the ability to select a proper response according to a multi-turn context is a great challenging task. Existing studies focus on building a context-response matching model with various neural architectures or PLMs and typically learning with a single response prediction task. These approaches overlook many potential training signals contained in dialogue data, which might be beneficial for context understanding and produce better features for response prediction.  Besides, the response retrieved from existing dialogue systems supervised by the conventional way still faces some critical challenges, including incoherence and inconsistency. To address these issues, in this paper, we propose learning a context-response matching model with auxiliary self-supervised tasks designed for the dialogue data based on pre-trained language models. Specifically, we introduce four self-supervised tasks including next session prediction, utterance restoration, incoherence detection and consistency discrimination, and jointly train the PLM-based response selection model with these auxiliary tasks in a multi-task manner.   By this means, the auxiliary tasks can guide the learning of the matching model to achieve a better local optimum and select a more proper response. Experiment results on two benchmarks indicate that the proposed auxiliary self-supervised tasks bring significant improvement for multi-turn response selection in retrieval-based dialogues, and our model achieves new state-of-the-art results on both datasets."
"Named Entity Recognition process identification named entities natural language text. The present paper concentrates three low resource languages : Bhojpuri, Maithili Magahi , belong Indo-Aryan language family. This work may seen first attempt develop NER tool Bhojpuri, Maithili Magahi. There previous work NER languages far know. The main aim present paper start insights NER systems developed Indian Languages resources based try develop NER System BMM. The NER module important component Natural Language Processing Information Extraction systems. It essential task computational purposes like Machine Translation , developing search engines, automatic indexing, document classification text summarization, questiona answering etc., possible build end-to-end Deep Learning systems languages due lack data. It also helpful many cross-linguistic applications relevant Indian Languages, particularly LRLs. The present study mainly focuses Named Entities BMM machine translation goal. \subsection{Named Entity Recognition} The concept Named Entity introduced Sixth Message Understanding Conference . It often seen part Information Extraction system, refers automatic extraction structured information entities, relationships entities attributes describing entities unstructured sources. The role NER system locate classify words text predefined categories names persons, organizations, locations, expressions times, quantities etc. The NEs could identified two conventional ways, recent success machine learning Deep Learning based techniques: It challenging task implement NER Indian languages due absence capitalization writing systems. On hand, systems phonetically organized designed, makes easily possible use phonetic features NER Indian languages. Preparing gazetteer list nouns impossible vast number unknown named entities world terms corpus versus language. Here, one important point noted much work reported NER Low Resource languages due insufficient lexical resources also due morphological richness. There efforts major Indian languages, i.e., Hindi, Tamil, Telugu, Urdu, Punjabi, efforts Low Resource Indian languages BMM. \subsection{Bhojpuri, Maithili, Magahi : An Introduction} Bhojpuri often considered major `sub-language' Hindi. It language spoken various states India countries well, viz. Nepal, Mauritius, Fiji, Surinam etc. The writing system Bhojpuri earlier Kaithi script Devanagari script used write Bhojpuri. According 2011 census~, 5,05,79,447 Bhojpuri speakers. Maithili belongs Indo-Aryan language family, Bhojpuri Magahi considered `sub-languages' Hindi mainly spoken Eastern Uttar Pradesh, Bihar Jharkhand states India. Maithili included 22 `scheduled' languages Republic India . Maithili added Constitution India 2003 92nd Constitutional Amendment Act. Maithili, sister language Hindi, spoken India, particularly Bihar, Jharkhand, Uttar Pradesh etc. well Nepal. It language Bihari sub-family included eighth schedule Indian constitution. There 1,35,83,464 Maithili speakers . It also one 122 recognised languages Nepal. In 2007, Maithili included interim Constitution Nepal March 2018, received second official language status Jharkhand state India. It earlier considered sub-language dialect. Magahi Magadhi, also considered major sub-language Hindi, chiefly spoken districts Bihar, Jharkhand, also Maldah district West Bengal. Magahi also written Kaithi script earlier days, present usually written Devanagari script. There 1,27,06,825 Magahi speakers . Earlier work machine translation reported proper handling named tokens improve translation quality performance. These named tokens would translated source target translation without NER module, NER module instead simply transliterated. The current BMM machine translation systems plan use NER module, based transfer-based approach machine translation. Even though MT systems based transfer approach, NER module based machine learning Deep Learning, rule-based approach. Due this, annotated corpus developed NER system three languages reported lower higher baseline results. The former based CRF latter combination Long Short Term Memory , Convolutional Neural Networ Conditional Randon Fields , called LSTM-CNNs-CRF. \subsection{Contributions} As prior work NER problem Bhojpuri, Maithili Magahi, contributions paper follows: In paper, propose learning context-response matching model four auxiliary self-supervised tasks designed dialogue data. Jointly trained auxiliary tasks, matching model effectively learn task-related knowledge contained dialogue data, achieve better local optimum produce better features response selection. Experiment results two benchmarks indicate proposed auxiliary self-supervised tasks bring significant improvement multi-turn response selection retrieval-based dialogues, PLM-based model achieves new state-of-the-art results datasets. In unusual situation want paper appear references without citing main text, use \nocite \nocite{langley00}"," 			In Natural Language Processing  pipelines, Named Entity Recognition  is one of the preliminary problems, which marks proper nouns and other named entities such as Location, Person, Organization, Disease etc. Such entities, without a NER module, adversely affect the performance of a machine translation system. NER helps in overcoming this problem by recognising and handling such entities separately, although it can be useful in Information Extraction systems also. Bhojpuri, Maithili and Magahi are low resource languages, usually known as Purvanchal languages. This paper focuses on the development of a NER benchmark dataset for the Machine Translation systems developed to translate from these languages to Hindi by annotating parts of their available corpora. Bhojpuri, Maithili and Magahi corpora of sizes 228373, 157468 and 56190 tokens, respectively, were annotated using 22 entity labels. The annotation considers coarse-grained annotation labels followed by the tagset used in one of the Hindi NER datasets. We also report a Deep Learning based baseline that uses an LSTM-CNNs-CRF model. The lower baseline F$_1$-scores from the NER tool obtained by using Conditional Random Fields models are 96.73 for Bhojpuri, 93.33 for Maithili and 95.04 for Magahi. The Deep Learning-based technique  achieved 96.25 for Bhojpuri, 93.33 for Maithili and 95.44 for Magahi."
"As fundamental task speech language processing, Automatic Speech Recognition aims generate transcripts human speech. Recently, successful application deep neural networks pushed accuracy end-to-end ASR models new level, brings significant challenges building large-scale, robust ASR systems, especially industrial applications. Major bottlenecks twofold: i) abundant labeled training data learning large, accurate ASR models; ii) efficient distributed, computing framework model training serving scale. In demo, present EasyASR, distributed machine learning platform address challenges. EasyASR built upon Machine Learning Platform AI Alibaba Cloud~\footnote{https://www.alibabacloud.com/product/machine-learning/}, provides ultra-scale, deep learning framework distributed GPU clusters. Our platform supports complete process training, evaluating serving ASR models. Additionally, integrated functionalities i) extract high-quality audio aligned transcripts massive video data ii) expand existing ASR training sets various augmentation policies. We designed easy-to-use PAI components enable users build run ASR models within lines command, hides complicated techniques starters. We also provide add-on configurations PAI commands allow advanced users customize network architectures models. On EasyASR, achieve state-of-the-art performance Mandarin speech recognition multiple public datasets. %In following, describe EasyASR detail. % Bhojpuri, Maithili Magahi Purvanchal languages often considered dialects Hindi, even though widely spoken parts India. Bhojpuri spoken even outside India. Partly due dialectal nature, show linguistic variations nominal case inflection, emphatic expressions. Like computational resources, lack NER system languages. We describe first attempt this. This attempt includes creation dataset well reporting results two baseline systems, one uses CRF uses LSTM-CNNs-CRF model. These NER systems planned used machine translation system Bhojpuri, Maithili Magahi Hindi. The NER dataset, prepared native speaker linguists, consists 228373, 157468 56190 tokens, 12351, 19809 7152 NE. The tagset used union ENAMEX, TIMEX NUMEX tagsets, total 22 labels. The results obtained 70.56\ 61.41\ Bhojpuri CRF LSTM-CNNs-CRF, respectively. The results Maithili 73.19\ 71.38\ Magahi, 84.18\ 86.39\ two models. Even though total data size Bhojpuri, scores lower number NEs dataset languages relatively much less languages. In words, results consistent number NEs datasets, rather total size dataset number tokens."," We present EasyASR, a distributed machine learning platform for training and serving large-scale Automatic Speech Recognition  models, as well as collecting and processing audio data at scale. Our platform is built upon the Machine Learning Platform for AI of Alibaba Cloud. Its main functionality is to support efficient learning and inference for end-to-end ASR models on distributed GPU clusters. It allows users to learn ASR models with either pre-defined or user-customized network architectures via simple user interface. On EasyASR, we have produced state-of-the-art results over several public datasets for Mandarin speech recognition."
"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Most neural machine translation systems autoregressive, hence decoding latency grows linearly respect length target sentence. For faster generation, several work proposed non-autoregressive models sub-linear decoding latency given sufficient parallel computation~. As challenging precisely model dependencies among tokens without autoregression, many existing non-autoregressive models first generate initial translation iteratively refined yield better output~. While various training objectives used admit refinement , generation process models similar refinement process happens discrete space sentences. Meanwhile, another line work proposed use continuous latent variables non-autoregressive translation, distribution target sentences factorized time given latent variables~. Unlike models discussed above, finding likely target sentence models requires searching continuous latent variables. To end, \citet{shu20latent} proposed EM-like inference procedure optimizes hybrid space consisting continuous discrete variables. By introducing deterministic delta posterior, maximizes proxy lowerbound alternating matching delta posterior original approximate posterior , finding target sentence maximizes proxy lowerbound . In work, propose iterative inference procedure latent variable non-autoregressive models purely operates continuous space.} Given latent variable model, train inference network estimate gradient marginal log probability target sentence, using latent variable input. At inference time, find target sentence approximately maximizes log probability initializing latent variable e.g. mean prior, following gradients estimated inference network. We compare proposed approach EM-like inference~ three machine translation datasets: {\wmtende}, {\wmtroen} {\iwsltdeen}. The advantages approach twofold: %We observe two advantages approach: refinement step twice fast, avoids discrete search large vocabulary, effective, giving higher marginal probabilities BLEU scores number refinement steps. Our procedure results significantly faster inference, instance giving speedup autoregressive baseline {\wmtende} expense BLEU score. %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% We shown possible achieve few-shot performance similar GPT-3 SuperGLUE LMs three orders magnitude fewer parameters. This achieved using \pet{}, method reformulates tasks cloze questions trains ensemble models different reformulations. We proposed simple yet effective modification \pet{} enables us use tasks require predicting multiple tokens. In extensive experiments, identified several factors responsible strong performance \pet{} combined pretrained ALBERT: possibility concurrently use multiple patterns transforming examples cloze questions, ability compensate patterns difficult understand, usage labeled data perform parameter updates opposed using priming , underlying LM itself. To enable comparisons work, make dataset few-shot training examples publicly available. For future work, would interesting see whether improvements possible using \pet{} multi-task setting."," We propose an efficient inference procedure for non-autoregressive machine translation that iteratively refines translation purely in the continuous space. Given a continuous latent variable model for machine translation, we train an inference network to approximate the gradient of the marginal log probability of the target sentence, using only the latent variable as input. This allows us to use gradient-based optimization to find the target sentence at inference time that approximately maximizes its marginal probability. As each refinement step only involves computation in the latent space of low dimensionality , we avoid computational overhead incurred by existing non-autoregressive inference procedures that often refine in token space. We compare our approach to a recently proposed EM-like inference procedure that optimizes in a hybrid space, consisting of both discrete and continuous variables. We evaluate our approach on {\wmtende}, {\wmtroen} and {\iwsltdeen}, and observe two advantages over the EM-like inference:  it is computationally efficient, i.e. each refinement step is twice as fast, and  it is more effective, resulting in higher marginal probabilities and BLEU scores with the same number of refinement steps. On {\wmtende}, for instance, our approach is able to decode $6.2$ times faster than the autoregressive model with minimal degradation to translation quality ."
"Deep learning methods revolutionized NLP field past ten years. Although LSTM networks around two decades, NLP community learned train use effectively past ten years. \citet{DBLP:journals/corr/ChoMGBSB14} introduced new sequence-to-sequence method, boosting field neural machine translation significantly . The year \citet{bahdanau2014neural} presented attention mechanism aimed focusing specific words within prefix, order make accurate prediction next word mapping one sequence another. During period new text representation methods adapted, complementing following representation methods: bag-of-words , tf-idf, one-hot vectors dense representations, prominent word2vec Glove embeddings, served go-to methods many works . \citet{devlin2018bert} introduced pre-trained transformer based attention mechanism without recurrent connections. BERT provided another advancement field pre-trained text representations, showing enhanced performance various NLP tasks . Many research directions shaped pre-trained word embeddings representations several software toolkits available training deep neural networks. While Keras toolkit widely used text classification padding, DyNet PyTorch toolkits excelled tasks dynamic computation graph recurrent networks exploited achieve better predictive performance sentences varying length . An important advancement dense representation area occurred introduction TensorFlow Hub 2018. According Google. We propose efficient inference procedure non-autoregressive machine translation refines translations purely continuous space. Given latent variable model machine translation, train inference network approximate gradient marginal log probability respect target sentence, using latent variable. This allows us use gradient based optimization find target sentence inference time approximately maximizes marginal log probability. As avoid discrete search large vocabulary, inference procedure efficient previous inference procedures refine token space. We compare approach recently proposed delta inference procedure optimizes jointly discrete continuous space three machine translation datasets: {\wmtende}, {\wmtroen} {\iwsltdeen}. With underlying latent variable model, proposed inference procedure using learned score function following advantages: twice fast delta inference, able find target sentences resulting higher marginal probabilities BLEU scores. While showed iterative inference learned score function effective spherical Gaussian priors, work required investigate approach also successful sophisticated priors, Gaussian mixtures normalizing flows. This particularly interesting, recent study showed latent variable models flexible prior give high test log-likelihoods, suffer poor generation quality inference challenging~."," One of the challenges in  the NLP field is training  large  classification  models, a task that is both difficult and tedious. It is even harder when GPU hardware is unavailable. The increased availability of pre-trained and off-the-shelf word embeddings, models, and modules aim at easing the process of training large models and achieving a competitive performance.   We explore the use of off-the-shelf BERT models and share the results of our experiments and compare their results to those of LSTM networks and more simple baselines. We show that the complexity and computational cost of BERT is not a guarantee for enhanced predictive performance in the classification tasks at hand."
"Autoregressive models ubiquitous natural language processing. Due sequential nature text generation, often tool choice tackling sequence-to-sequence problems translation , summarization , dialogue . Furthermore, form backbone several successful generative pre-training architectures . Two recent trends made autoregressive models cumbersome deploy real-world, natural language generation applications. First, state-of-the-art models grown larger larger, amounting hundreds millions even billions parameters . The increase size depth dramatically slows inference speed. Second, architecture choice autoregressive models seems shifted recurrent neural network Transformer . Though Transformer's self-attention mechanism improves performance, also increases computational complexity step-by-step generation algorithms used test time. Thus, trends contributed significantly increasing inference time costs, especially CPUs low-resource devices, hindering use production systems. % The increasing memory inference time costs enormous models make cumbersome deploy real-world settings. Inference CPU already quite slow, much less smartphone device. Thus, exists need scale large autoregressive models practical purposes. Knowledge distillation one popular method model compression. It transfers information learned large, pretrained teacher smaller, untrained student. In comparison methods weight pruning quantization, KD allows compressed model's architecture significantly differ original teacher. This feature enables models trained KD achieve high performance meeting particular inference requirements . Sequence-level knowledge distillation , proposed \citet{kim2016sequence}, dominant technique autoregressive KD current NLG literature, especially machine translation . This method trains student model using modified dataset generated teacher model standard negative log-likelihood objective. While SeqKD simple efficient, argue take advantage teacher's full potential. %This method two-step procedure 1) generates full sequences using teacher model produce modified dataset 2) trains student model modified dataset standard negative log-likelihood training. While seqKD conceptually simple efficient implement, argue reducing teacher's impact static dataset take advantage full potential. % Autoregressive models often trained way different used inference time. During training, true sequence available, model learns predict one-step-ahead given ground-truth context. However, inference time, model must generate entire sequence scratch repeatedly using outputs context subsequent steps. This training-inference inconsistency leads exposure bias problem, may manifested decrease sequence quality number generation steps increases. The seqKD algorithm simply NLL training modified dataset, also experiences issue. Training student model static dataset leads exposure bias problem. During training, student model learns predict next token given previous tokens provided data. However, inference time, student generates entire sequence scratch repeatedly using outputs context subsequent steps. This training-inference inconsistency causes decrease generation quality. Alternatively, propose student leverage teacher dynamic fashion learning process. % Our main contributions following: We recast distillation autoregressive models imitation learning problem, drawing parallels SeqKD behavioral cloning. From perspective, design new compression algorithm aimed addressing exposure bias autoregressive models called imitation-based knowledge distillation . We conduct several experiments translation summarization, demonstrating ImitKD especially suitable compressing deep Transformers achieve high performance shallow RNNs generate much faster inference time. %The key insight ImitKD treat teacher model oracle corrects student generations every step. Thus, student explicitly learns generate training. Our method consistently outperforms popular distillation algorithms, SeqKD. It yields student models beat models trained without teacher 1.4 4.8 points Bleu Rouge metrics. We devise new compression algorithm autoregressive models called imitation-based knowledge distillation . It inspired imitation learning perspective autoregressive distillation problem. Our algorithm trains student model within IL framework treating teacher oracle, allows student explore generation training. The teacher corrects student's generation every time step, thereby guiding student learning generate. % Experimental results translation summarization show ImitKD especially suitable compressing deep Transformer models achieve high performance shallow RNNs generate 14 times faster inference time. Our method consistently outperforms distillation algorithms , yields student models beat models trained without teacher 1.4 4.8 points generation metrics BLEU ROUGE. % We share results experimentation two different NLP tasks. In cases experimented small proprietary datasets domains suffer serious lack labeled data. In experiments used promising prominent BERT method off-the-shelf TensorFlow Hub modules aim outperforming several baselines tasks proper word choice political perspective identification. We used pre-trained off-the-shelf fine-tuned proprietary models. We failed outdo earlier folklore baselines well advanced LSTM-based baselines, straightforward systematic way applying BERT. Over 30 years ago, \citet{brooks1987no} argued software development process hard essence. They could envision advanced programming language capable solving complexity performing high-quality software development projects time. Analogously, user-friendly framework pre-trained models can't guarantee excellent predictive performance. Training high performing models essentially difficult. It requires deep understanding task data processing expertise. Fine-tuning pre-trained model might good starting point, developer still required delve deeply model's details order excel predictive performance. Please add following required packages document preamble: \usepackage{multirow}"," The performance of autoregressive models on natural language generation tasks has dramatically improved due to the adoption of deep, self-attentive architectures.  However, these gains have come at the cost of hindering inference speed, making state-of-the-art models cumbersome to deploy in real-world, time-sensitive settings.  We develop a compression technique for autoregressive models that is driven by an imitation learning perspective on knowledge distillation.  The algorithm is designed to address the exposure bias problem.     On prototypical language generation tasks such as translation and summarization, our method consistently outperforms other distillation algorithms, such as sequence-level knowledge distillation.  Student models trained with our method attain 1.4 to 4.8 BLEU/ROUGE points higher than those trained from scratch, while increasing inference speed by up to 14 times in comparison to the teacher model.\footnote{Our code can be found at \url{https://github.com/asappresearch/imitkd}.}"
"Extracting event temporal relations raw text data attracted surging attention NLP research community recent years fundamental task commonsense reasoning natural language understanding. It facilitates various downstream applications, forecasting social events tracking patients' medical history. Figure shows example task event extractor first needs identify events input relation classifier predicts pairwise relations among them, resulting temporal ordering illustrated figure. For example, \event{say} \temprel{before} \event{stop}; \event{buildup} \temprel{includes} \event{say}; temporal ordering \event{buildup} \event{stop} cannot decided context, relation \temprel{vague}. In work, developed new knowledge distillation technique inspired imitation learning compressing large cumbersome autoregressive models smaller faster counterparts. We demonstrated empirical success method popular baselines several natural language generation tasks. We excited several possible avenues future work. One branch ideas involves incorporating advanced IL algorithms beyond DAgger, LOLS , improve distillation process. Another possibility design imitation-based fine-tuning analogs SeqInter method. Finally, although experiments paper focused sequence-to-sequence settings, interested exploring use ImitKD compressing large language models aimed transfer learning.","  Extracting event temporal relations is a critical task for information extraction and plays an important role in natural language understanding. Prior systems leverage deep learning and pre-trained language models to improve the performance of the task. However, these systems often suffer from two shortcomings: 1) when performing maximum a posteriori  inference based on neural models, previous systems only used structured knowledge that is assumed to be absolutely correct, i.e., hard constraints; 2) biased predictions on dominant temporal relations when training with a limited amount of data. To address these issues, we propose a framework that enhances deep neural network with distributional constraints constructed by probabilistic domain knowledge. We solve the constrained inference problem via Lagrangian Relaxation and apply it to end-to-end event temporal relation extraction tasks. Experimental results show our framework is able to improve the baseline neural network models with strong statistical significance on two widely used datasets in news and clinical domains."
"Encoder-decoder architecture, uses encoder create representation source sequence decoder predict target sequence, established state art approaches neural machine translation . Recurrent neural network based model , convolutional neural network model self-attention network based model representative encoder-decoder models, NMT models variants combination three. NMT models based encoder-decoder architecture similar aspects, stack layers structure. Stack layers increases complexity model approximate nonlinear function. Viewing layers one function, every single layer captures different information input. Looking every single NMT model RNN-based model SAN-based model, models always try make representation one word containing information whole sentence every layer. However, empirically, one layer alone cannot result satisfactory result. It common regard sentence NMT model directed complete simple graph, views words nodes relationships words edges. However, perspective focuses relationship words, ignoring information, relationship phrases relationship different fragments sentences. As result, structure simple graph cannot fully reflect information. To overcome shortcomings simple graph, view sentence multigraph SAN-based model. In multigraph , multiple edges exist two nodes. Edge connects nodes also subgraphs reflects relationship different fragments sentences relationship word-pair. Encoding also regarded process generating multigraph approximate infinitely. Compared simple graph, multigraph explain th essence encoding comprehensively, explain relationship words general way. One layer NMT model capture incremental information automatically compared previous layer. Fusion previous incremental information makes representation rich thus benefits translation. From perspective multigraph, incremental information described set higher-order subgraphs generated layer. Even though current NMT models capture information subgraphs different orders, fusing representation fixed weight makes model difficulty pay attention really salient part. To solve problem, propose graph-based SAN empowered Graph-Transformer enhancing ability capturing subgraph information current NMT models. First all, generally define full representation fusing result concerned subgraph representations. Then let representation one layer split two parts, previous representation incremental representation. The previous representation reflects full representation previous layer, incremental representation reflects new information generated layer. Based this, encoding process modified adapt representation division. We split original self-attention three independent parts generate incremental representation. Our method accommodates subgraphs different orders different parts incremental representation, reduces information redundancy. To fuse full representation, We consider three fusing strategies terms different weighting schemes let model focus important parts representation. In experiments WMT14 English-to-German IWSLT14 German-to-English , results experiments prove model improve performance translation parameters increasing. Our model achieves performance outperforming Transformer improvement 1.1 BLEU points En-De 1.0 BLEU points De-En. In conclusion, propose general framework augments deep neural networks distributional constraints constructed using probabilistic domain knowledge. We apply setting end-to-end temporal relation extraction task event-type relation constraints show MAP inference distributional constraints significantly improve final results. We plan apply proposed framework various event reasoning tasks construct novel distributional constraints could leverage domain knowledge beyond corpus statistics, larger unlabeled data rich information contained knowledge bases."," Neural machine translation  usually works in a seq2seq learning way by viewing either source or target sentence as a linear sequence of words, which can be regarded as a special case of graph, taking words in the sequence as nodes and relationships between words as edges. In the light of the current NMT models more or less capture graph information among the sequence in a latent way, we present a graph-to-sequence model facilitating explicit graph information capturing. In detail, we propose a graph-based SAN-based NMT model called Graph-Transformer by capturing information of subgraphs of different orders in every layers. Subgraphs are put into different groups according to their orders, and every group of subgraphs respectively reflect different levels of dependency between words. For fusing subgraph representations, we empirically explore three methods which weight different groups of subgraphs of different orders. Results of experiments on WMT14 English-German and IWSLT14 German-English show that our method can effectively boost the Transformer with an improvement of 1.1 BLEU points on WMT14 English-German dataset and 1.0 BLEU points on IWSLT14 German-English dataset."
"Open-domain human-machine dialogue systems, especially generation-based genre, attracted extensive attention recently. Typically, following neural encoder-decoder paradigm, contemporary dialogue generation models~, often not, trained Maximum Likelihood Estimation principle mimic human context-response pairs training corpus. While notable gains achieved learning framework, prior art~ suggests naive MLE objective used training neural dialogue generation models effective enough tends result issues like dull response generation. By optimizing likelihood training dialogues, neural models inclined assign high probabilities ``safe'' responses, due fact vacuous responses like ``I know'' relatively high frequencies conversational datasets~. One promising training framework neural dialogue generation adversarial learning~, discriminator provides rewards generator contrastively distinguishing dialogues human-generated machine-generated. However, learning ability GANs text drastically limited due training instability model collapse~. First, discriminator usually unlikely fooled easily, generator hardly learn ineffective rewards. Second, generator sometimes encouraged mimic high-frequency generic responses training corpus, \checkhere{because cases, discriminator fails distinguish good response bad one: easily recognize contentful less-grammatical responses machine-generated, yet treat human-generated dull responses oracle.} In paper, introduce contrastive learning~ dialogue generation, model explicitly perceives difference well-chosen positive negative utterances. From perspective contrastive learning, discriminator adversarial learning considers human-generated responses positive utterances synthetic ones negative samples. Instead, work deems highly-matched context-response pairs positive samples mismatched training pairs negative samples. In particular, utilize pretrained baseline model reference. During contrastive learning, context response , target dialogue model trained give higher conditional probabilities positive samples, lower conditional probabilities negative samples, compared reference model. This training paradigm encourages model pull positive data points together push apart negative samples, exemplified Figure. As result, proposed training scheme explicitly takes semantic associations differences among training examples account dialogue modeling. Besides, contrastively characterizing distinctions relative strong reference, method implicitly enhances {distinctiveness} generated responses well, ensures overall performance target model inferior reference. Contrastively learning one pair positive negative samples quite straightforward, however, multi-mapping relations prevail human-human conversations, exist multiple appropriate responses given context, response sometimes fits well several contexts, known one-to-many many-to-one relations. Such complex multi-mapping relations overlooked previous learning framework, hampers effective dialogue response learning. Furthermore, potential highly-matched utterance pair treated negative sample outlier used positive sample, model may confused. Therefore, order consider multi-mapping phenomenon human conversations remedy potential problematic false learning samples, enhance training stability, augment contrastive learning group-wise dual sampling, groups positive negative instances sampled regarding context response, respectively. To depict subtle differences instances group, adapt instance importance matching scores, optimize \iffalse expected \fi weighted loss. We show illustration case understand learning framework Figure. Given training context-response pair . By mean, target model actually induced pull positive sample pairs together push mismatched pairs apart, thus learns distinctions positives negatives. The proposed group-wise contrastive learning framework suited training various neural dialogue generation models. We conduct extensive studies three large-scale conversation datasets using four popular dialogue models assess proposed approach. The experimental results confirm effectiveness learning framework favorable performance baseline training approaches.% % File emnlp2020.tex % %% Based style files ACL 2020, %% Based style files ACL 2018, NAACL 2018/19, %% Based style files ACL-2015, improvements %% taken NAACL-2016 style %% Based style files ACL-2014, were, turn, %% based ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009, %% EACL-2009, IJCNLP-2008... %% Based style files EACL 2006 %%e.agirre@ehu.es Sergi.Balari@uab.es %% ACL 08 Joakim Nivre Noah Smith \documentclass[11pt,a4paper]{article} \usepackage[hyperref]{emnlp2020-templates/emnlp2020} \usepackage{times} \usepackage{latexsym} \renewcommand{\UrlFont}{\ttfamily\small} % This strictly necessary, may commented out, % improve layout manuscript, % typically save space. \usepackage{microtype} % --------- packages added authors ---------> % \usepackage{booktabs} % For formal tables \usepackage{amsmath} \usepackage{amsfonts} \usepackage{bm} \usepackage{array} \usepackage{enumitem} % \usepackage{needspace} % \usepackage{adjustbox} \usepackage{tikz} \usepackage{pgfplots} \usepackage{multirow} % \usepackage{autobreak} \usepackage{makecell} \usepackage{xcolor} \usepackage{amssymb} % \usepackage{import} % \usepackage{subcaption} \usepackage{nicefrac} % compact symbols 1/2, etc. \usepackage{microtype} % microtypography % \usepackage{tabularx} % \usepackage{dcolumn} \usepackage{url} % <--------- packages added authors --------- % % --------- commands added author ----------> % % \newcommand\setrow[1]{\gdef\rowmac{#1}#1\ignorespaces} \newcommand\clearrow{\global\let\rowmac\relax} \clearrow \newcommand{\wideslash}{\text{ / }} \newcommand{\xxx}{\textcolor{red}{Placeholder}} % \newcommand{\alert}[1]{{\textcolor{red}{#1}}} \newcommand{\alert}[1]{{#1}} % \newcommand{\notice}[1]{\textcolor{red}{#1}} \newcommand{\notice}[1]{{#1}} % \newcommand{\checkhere}[1]{\textcolor{blue}{#1}} \newcommand{\checkhere}[1]{{#1}} \newcommand*{\rom}[1]{\romannumeral#1\relax} \newcommand{\centercell}[1]{\multicolumn{1}{c}{#1}} % \DeclareUnicodeCharacter{001D}{\textcolor{red}{CHECK THIS UNICODE CHAR!!!}} % <--------- commands added author ---------- % \aclfinalcopy % Uncomment line final submission \def\aclpaperid{690} % Enter acl Paper ID %\setlength\titlebox{5cm} % You expand titlebox need extra space % show authors. Please make titlebox % smaller 5cm ; check % camera-ready version ask change back. \newcommand\BibTeX{Bib\TeX} \title{Group-wise Contrastive Learning Neural Dialogue Generation} % \author{First Author \\ % Affiliation / Address line 1 \\ % Affiliation / Address line 2 \\ % Affiliation / Address line 3 \\ % \\\And % Second Author \\ % Affiliation / Address line 1 \\ % Affiliation / Address line 2 \\ % Affiliation / Address line 3 \\ % \\} \author{ Hengyi Cai\thanks{\ \ Work done JD.com.}, Hongshen Chen\\ {\bf Yonghao Song, Zhuoye Ding, Yongjun Bao, Weipeng Yan, Xiaofang Zhao} \\ {Institute Computing Technology, Chinese Academy Sciences, Beijing, China} \\ {University Chinese Academy Sciences, Beijing, China} \\ {JD.com, China} \\ {caihengyi@ict.ac.cn, ac@chenhongshen.com} \\ {\{songyonghao, zhaoxf\}@ict.ac.cn, \{dingzhuoye, baoyongjun, Paul.yan\}@jd.com} } \date{} % \hypersetup{draft} %%%%% NEW MATH DEFINITIONS %%%%% \usepackage{amsmath,amsfonts,bm} % Mark sections captions referring divisions figures \newcommand{\figleft}{} \newcommand{\figcenter}{} \newcommand{\figright}{} \newcommand{\figtop}{} \newcommand{\figbottom}{} \newcommand{\captiona}{} \newcommand{\captionb}{} \newcommand{\captionc}{} \newcommand{\captiond}{} % Highlight newly defined term \newcommand{\newterm}[1]{{\bf #1}} % Figure reference, lower-case. \def\figref#1{figure} % Figure reference, capital. For start sentence \def\Figref#1{Figure} \def\twofigref#1#2{figures } \def\quadfigref#1#2#3#4{figures , , } % Section reference, lower-case. \def\secref#1{section} % Section reference, capital. \def\Secref#1{Section} % Reference two sections. \def\twosecrefs#1#2{sections } % Reference three sections. \def\secrefs#1#2#3{sections , } % Reference equation, lower-case. \def\eqref#1{equation} % Reference equation, upper case \def\Eqref#1{Equation} % A raw reference equation---avoid using possible \def\plaineqref#1{} % Reference chapter, lower-case. \def\chapref#1{chapter} % Reference equation, upper case. \def\Chapref#1{Chapter} % Reference range chapters \def\rangechapref#1#2{chapters--} % Reference algorithm, lower-case. \def\algref#1{algorithm} % Reference algorithm, upper case. \def\Algref#1{Algorithm} \def\twoalgref#1#2{algorithms } \def\Twoalgref#1#2{Algorithms } % Reference part, lower case \def\partref#1{part} % Reference part, upper case \def\Partref#1{Part} \def\twopartref#1#2{parts } \def\ceil#1{\lceil #1 \rceil} \def\floor#1{\lfloor #1 \rfloor} \def\1{\bm{1}} \newcommand{\train}{\mathcal{D}} \newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}} \newcommand{\test}{\mathcal{D_{\mathrm{test}}}} \def\eps{{\epsilon}} % Random variables \def\reta{{\textnormal{}}} \def\ra{{\textnormal{a}}} \def\rb{{\textnormal{b}}} \def\rc{{\textnormal{c}}} \def\rd{{\textnormal{d}}} \def\re{{\textnormal{e}}} \def\rf{{\textnormal{f}}} \def\rg{{\textnormal{g}}} \def\rh{{\textnormal{h}}} \def\ri{{\textnormal{i}}} \def\rj{{\textnormal{j}}} \def\rk{{\textnormal{k}}} \def\rl{{\textnormal{l}}} % rm already command, name random variables \def\rn{{\textnormal{n}}} \def\ro{{\textnormal{o}}} \def\rp{{\textnormal{p}}} \def\rq{{\textnormal{q}}} \def\rr{{\textnormal{r}}} \def\rs{{\textnormal{s}}} \def\rt{{\textnormal{t}}} \def\ru{{\textnormal{u}}} \def\rv{{\textnormal{v}}} \def\rw{{\textnormal{w}}} \def\rx{{\textnormal{x}}} \def\ry{{\textnormal{y}}} \def\rz{{\textnormal{z}}} % Random vectors \def\rvepsilon{{\mathbf{\epsilon}}} \def\rvtheta{{\mathbf{\theta}}} \def\rva{{\mathbf{a}}} \def\rvb{{\mathbf{b}}} \def\rvc{{\mathbf{c}}} \def\rvd{{\mathbf{d}}} \def\rve{{\mathbf{e}}} \def\rvf{{\mathbf{f}}} \def\rvg{{\mathbf{g}}} \def\rvh{{\mathbf{h}}} \def\rvu{{\mathbf{i}}} \def\rvj{{\mathbf{j}}} \def\rvk{{\mathbf{k}}} \def\rvl{{\mathbf{l}}} \def\rvm{{\mathbf{m}}} \def\rvn{{\mathbf{n}}} \def\rvo{{\mathbf{o}}} \def\rvp{{\mathbf{p}}} \def\rvq{{\mathbf{q}}} \def\rvr{{\mathbf{r}}} \def\rvs{{\mathbf{s}}} \def\rvt{{\mathbf{t}}} \def\rvu{{\mathbf{u}}} \def\rvv{{\mathbf{v}}} \def\rvw{{\mathbf{w}}} \def\rvx{{\mathbf{x}}} \def\rvy{{\mathbf{y}}} \def\rvz{{\mathbf{z}}} % Elements random vectors \def\erva{{\textnormal{a}}} \def\ervb{{\textnormal{b}}} \def\ervc{{\textnormal{c}}} \def\ervd{{\textnormal{d}}} \def\erve{{\textnormal{e}}} \def\ervf{{\textnormal{f}}} \def\ervg{{\textnormal{g}}} \def\ervh{{\textnormal{h}}} \def\ervi{{\textnormal{i}}} \def\ervj{{\textnormal{j}}} \def\ervk{{\textnormal{k}}} \def\ervl{{\textnormal{l}}} \def\ervm{{\textnormal{m}}} \def\ervn{{\textnormal{n}}} \def\ervo{{\textnormal{o}}} \def\ervp{{\textnormal{p}}} \def\ervq{{\textnormal{q}}} \def\ervr{{\textnormal{r}}} \def\ervs{{\textnormal{s}}} \def\ervt{{\textnormal{t}}} \def\ervu{{\textnormal{u}}} \def\ervv{{\textnormal{v}}} \def\ervw{{\textnormal{w}}} \def\ervx{{\textnormal{x}}} \def\ervy{{\textnormal{y}}} \def\ervz{{\textnormal{z}}} % Random matrices \def\rmA{{\mathbf{A}}} \def\rmB{{\mathbf{B}}} \def\rmC{{\mathbf{C}}} \def\rmD{{\mathbf{D}}} \def\rmE{{\mathbf{E}}} \def\rmF{{\mathbf{F}}} \def\rmG{{\mathbf{G}}} \def\rmH{{\mathbf{H}}} \def\rmI{{\mathbf{I}}} \def\rmJ{{\mathbf{J}}} \def\rmK{{\mathbf{K}}} \def\rmL{{\mathbf{L}}} \def\rmM{{\mathbf{M}}} \def\rmN{{\mathbf{N}}} \def\rmO{{\mathbf{O}}} \def\rmP{{\mathbf{P}}} \def\rmQ{{\mathbf{Q}}} \def\rmR{{\mathbf{R}}} \def\rmS{{\mathbf{S}}} \def\rmT{{\mathbf{T}}} \def\rmU{{\mathbf{U}}} \def\rmV{{\mathbf{V}}} \def\rmW{{\mathbf{W}}} \def\rmX{{\mathbf{X}}} \def\rmY{{\mathbf{Y}}} \def\rmZ{{\mathbf{Z}}} % Elements random matrices \def\ermA{{\textnormal{A}}} \def\ermB{{\textnormal{B}}} \def\ermC{{\textnormal{C}}} \def\ermD{{\textnormal{D}}} \def\ermE{{\textnormal{E}}} \def\ermF{{\textnormal{F}}} \def\ermG{{\textnormal{G}}} \def\ermH{{\textnormal{H}}} \def\ermI{{\textnormal{I}}} \def\ermJ{{\textnormal{J}}} \def\ermK{{\textnormal{K}}} \def\ermL{{\textnormal{L}}} \def\ermM{{\textnormal{M}}} \def\ermN{{\textnormal{N}}} \def\ermO{{\textnormal{O}}} \def\ermP{{\textnormal{P}}} \def\ermQ{{\textnormal{Q}}} \def\ermR{{\textnormal{R}}} \def\ermS{{\textnormal{S}}} \def\ermT{{\textnormal{T}}} \def\ermU{{\textnormal{U}}} \def\ermV{{\textnormal{V}}} \def\ermW{{\textnormal{W}}} \def\ermX{{\textnormal{X}}} \def\ermY{{\textnormal{Y}}} \def\ermZ{{\textnormal{Z}}} % Vectors \def\vzero{{\bm{0}}} \def\vone{{\bm{1}}} \def\vmu{{\bm{\mu}}} \def\vtheta{{\bm{\theta}}} \def\va{{\bm{a}}} \def\vb{{\bm{b}}} \def\vc{{\bm{c}}} \def\vd{{\bm{d}}} \def\ve{{\bm{e}}} \def\vf{{\bm{f}}} \def\vg{{\bm{g}}} \def\vh{{\bm{h}}} \def\vi{{\bm{i}}} \def\vj{{\bm{j}}} \def\vk{{\bm{k}}} \def\vl{{\bm{l}}} \def\vm{{\bm{m}}} \def\vn{{\bm{n}}} \def\vo{{\bm{o}}} \def\vp{{\bm{p}}} \def\vq{{\bm{q}}} \def\vr{{\bm{r}}} \def\vs{{\bm{s}}} \def\vt{{\bm{t}}} \def\vu{{\bm{u}}} \def\vv{{\bm{v}}} \def\vw{{\bm{w}}} \def\vx{{\bm{x}}} \def\vy{{\bm{y}}} \def\vz{{\bm{z}}} % Elements vectors \def\evalpha{{\alpha}} \def\evbeta{{\beta}} \def\evepsilon{{\epsilon}} \def\evlambda{{\lambda}} \def\evomega{{\omega}} \def\evmu{{\mu}} \def\evpsi{{\psi}} \def\evsigma{{\sigma}} \def\evtheta{{\theta}} \def\eva{{a}} \def\evb{{b}} \def\evc{{c}} \def\evd{{d}} \def\eve{{e}} \def\evf{{f}} \def\evg{{g}} \def\evh{{h}} \def\evi{{i}} \def\evj{{j}} \def\evk{{k}} \def\evl{{l}} \def\evm{{m}} \def\evn{{n}} \def\evo{{o}} \def\evp{{p}} \def\evq{{q}} \def\evr{{r}} \def\evs{{s}} \def\evt{{t}} \def\evu{{u}} \def\evv{{v}} \def\evw{{w}} \def\evx{{x}} \def\evy{{y}} \def\evz{{z}} % Matrix \def\mA{{\bm{A}}} \def\mB{{\bm{B}}} \def\mC{{\bm{C}}} \def\mD{{\bm{D}}} \def\mE{{\bm{E}}} \def\mF{{\bm{F}}} \def\mG{{\bm{G}}} \def\mH{{\bm{H}}} \def\mI{{\bm{I}}} \def\mJ{{\bm{J}}} \def\mK{{\bm{K}}} \def\mL{{\bm{L}}} \def\mM{{\bm{M}}} \def\mN{{\bm{N}}} \def\mO{{\bm{O}}} \def\mP{{\bm{P}}} \def\mQ{{\bm{Q}}} \def\mR{{\bm{R}}} \def\mS{{\bm{S}}} \def\mT{{\bm{T}}} \def\mU{{\bm{U}}} \def\mV{{\bm{V}}} \def\mW{{\bm{W}}} \def\mX{{\bm{X}}} \def\mY{{\bm{Y}}} \def\mZ{{\bm{Z}}} \def\mBeta{{\bm{\beta}}} \def\mPhi{{\bm{\Phi}}} \def\mLambda{{\bm{\Lambda}}} \def\mSigma{{\bm{\Sigma}}} % Tensor \DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl} \SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n} \newcommand{\tens}[1]{\bm{\mathsfit{#1}}} \def\tA{{\tens{A}}} \def\tB{{\tens{B}}} \def\tC{{\tens{C}}} \def\tD{{\tens{D}}} \def\tE{{\tens{E}}} \def\tF{{\tens{F}}} \def\tG{{\tens{G}}} \def\tH{{\tens{H}}} \def\tI{{\tens{I}}} \def\tJ{{\tens{J}}} \def\tK{{\tens{K}}} \def\tL{{\tens{L}}} \def\tM{{\tens{M}}} \def\tN{{\tens{N}}} \def\tO{{\tens{O}}} \def\tP{{\tens{P}}} \def\tQ{{\tens{Q}}} \def\tR{{\tens{R}}} \def\tS{{\tens{S}}} \def\tT{{\tens{T}}} \def\tU{{\tens{U}}} \def\tV{{\tens{V}}} \def\tW{{\tens{W}}} \def\tX{{\tens{X}}} \def\tY{{\tens{Y}}} \def\tZ{{\tens{Z}}} % Graph \def\gA{{\mathcal{A}}} \def\gB{{\mathcal{B}}} \def\gC{{\mathcal{C}}} \def\gD{{\mathcal{D}}} \def\gE{{\mathcal{E}}} \def\gF{{\mathcal{F}}} \def\gG{{\mathcal{G}}} \def\gH{{\mathcal{H}}} \def\gI{{\mathcal{I}}} \def\gJ{{\mathcal{J}}} \def\gK{{\mathcal{K}}} \def\gL{{\mathcal{L}}} \def\gM{{\mathcal{M}}} \def\gN{{\mathcal{N}}} \def\gO{{\mathcal{O}}} \def\gP{{\mathcal{P}}} \def\gQ{{\mathcal{Q}}} \def\gR{{\mathcal{R}}} \def\gS{{\mathcal{S}}} \def\gT{{\mathcal{T}}} \def\gU{{\mathcal{U}}} \def\gV{{\mathcal{V}}} \def\gW{{\mathcal{W}}} \def\gX{{\mathcal{X}}} \def\gY{{\mathcal{Y}}} \def\gZ{{\mathcal{Z}}} % Sets \def\sA{{\mathbb{A}}} \def\sB{{\mathbb{B}}} \def\sC{{\mathbb{C}}} \def\sD{{\mathbb{D}}} % Don't use set called E, would symbol % expectation. \def\sF{{\mathbb{F}}} \def\sG{{\mathbb{G}}} \def\sH{{\mathbb{H}}} \def\sI{{\mathbb{I}}} \def\sJ{{\mathbb{J}}} \def\sK{{\mathbb{K}}} \def\sL{{\mathbb{L}}} \def\sM{{\mathbb{M}}} \def\sN{{\mathbb{N}}} \def\sO{{\mathbb{O}}} \def\sP{{\mathbb{P}}} \def\sQ{{\mathbb{Q}}} \def\sR{{\mathbb{R}}} \def\sS{{\mathbb{S}}} \def\sT{{\mathbb{T}}} \def\sU{{\mathbb{U}}} \def\sV{{\mathbb{V}}} \def\sW{{\mathbb{W}}} \def\sX{{\mathbb{X}}} \def\sY{{\mathbb{Y}}} \def\sZ{{\mathbb{Z}}} % Entries matrix \def\emLambda{{\Lambda}} \def\emA{{A}} \def\emB{{B}} \def\emC{{C}} \def\emD{{D}} \def\emE{{E}} \def\emF{{F}} \def\emG{{G}} \def\emH{{H}} \def\emI{{I}} \def\emJ{{J}} \def\emK{{K}} \def\emL{{L}} \def\emM{{M}} \def\emN{{N}} \def\emO{{O}} \def\emP{{P}} \def\emQ{{Q}} \def\emR{{R}} \def\emS{{S}} \def\emT{{T}} \def\emU{{U}} \def\emV{{V}} \def\emW{{W}} \def\emX{{X}} \def\emY{{Y}} \def\emZ{{Z}} \def\emSigma{{\Sigma}} % entries tensor % Same font tensor, without \bm wrapper \newcommand{\etens}[1]{\mathsfit{#1}} \def\etLambda{{\etens{\Lambda}}} \def\etA{{\etens{A}}} \def\etB{{\etens{B}}} \def\etC{{\etens{C}}} \def\etD{{\etens{D}}} \def\etE{{\etens{E}}} \def\etF{{\etens{F}}} \def\etG{{\etens{G}}} \def\etH{{\etens{H}}} \def\etI{{\etens{I}}} \def\etJ{{\etens{J}}} \def\etK{{\etens{K}}} \def\etL{{\etens{L}}} \def\etM{{\etens{M}}} \def\etN{{\etens{N}}} \def\etO{{\etens{O}}} \def\etP{{\etens{P}}} \def\etQ{{\etens{Q}}} \def\etR{{\etens{R}}} \def\etS{{\etens{S}}} \def\etT{{\etens{T}}} \def\etU{{\etens{U}}} \def\etV{{\etens{V}}} \def\etW{{\etens{W}}} \def\etX{{\etens{X}}} \def\etY{{\etens{Y}}} \def\etZ{{\etens{Z}}} % The true underlying data generating distribution \newcommand{\pdata}{p_{\rm{data}}} % The empirical distribution defined training set \newcommand{\ptrain}{\hat{p}_{\rm{data}}} \newcommand{\Ptrain}{\hat{P}_{\rm{data}}} % The model distribution \newcommand{\pmodel}{p_{\rm{model}}} \newcommand{\Pmodel}{P_{\rm{model}}} \newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}} % Stochastic autoencoder distributions \newcommand{\pencode}{p_{\rm{encoder}}} \newcommand{\pdecode}{p_{\rm{decoder}}} \newcommand{\precons}{p_{\rm{reconstruct}}} \newcommand{\laplace}{\mathrm{Laplace}} % Laplace distribution \newcommand{\E}{\mathbb{E}} \newcommand{\Ls}{\mathcal{L}} \newcommand{\R}{\mathbb{R}} \newcommand{\emp}{\tilde{p}} \newcommand{\lr}{\alpha} \newcommand{\reg}{\lambda} \newcommand{\rect}{\mathrm{rectifier}} \newcommand{\softmax}{\mathrm{softmax}} \newcommand{\sigmoid}{\sigma} \newcommand{\softplus}{\zeta} \newcommand{\KL}{D_{\mathrm{KL}}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\standarderror}{\mathrm{SE}} \newcommand{\Cov}{\mathrm{Cov}} % Wolfram Mathworld says function spaces vectors % But seem use vectors throughout site, % wikipedia. \newcommand{\normlzero}{L^0} \newcommand{\normlone}{L^1} \newcommand{\normltwo}{L^2} \newcommand{\normlp}{L^p} \newcommand{\normmax}{L^\infty} \newcommand{\parents}{Pa} % See usage notation.tex. Chosen match Daphne's book. \DeclareMathOperator*{\argmax}{arg\,max} \DeclareMathOperator*{\argmin}{arg\,min} \DeclareMathOperator{\sign}{sign} \DeclareMathOperator{\Tr}{Tr} \let\ab\allowbreak Instead treating MT seq2seq learning current NMT, work presents first graph-to-sequence NMT model, Graph-Transformer. Considering graph sequence generalized structure formalism, modeling graph information inside model may facilitate NMT model learn important subgraph information source. As multigraph defined sentence cannot immediately one part model one layer, assign every layer model learn subgraphs different orders, respectively. As model implementation, revise SAN may acquire explicit subgraph information introduced incremental representation. Results experiments show method effectively boost Transformer."," \checkhere{ Neural dialogue response generation has gained much popularity in recent years.  Maximum Likelihood Estimation  objective is widely adopted in existing dialogue model learning. However, models trained with MLE objective function are plagued by the low-diversity issue when it comes to the open-domain conversational setting. Inspired by the observation that humans not only learn from the positive signals but also benefit from correcting behaviors of undesirable actions, in this work, we introduce contrastive learning into dialogue generation, where the model explicitly perceives the difference between the well-chosen positive and negative utterances. Specifically, we employ a pretrained baseline model as a reference. During contrastive learning, the target dialogue model is trained to give higher conditional probabilities for the positive samples, and lower conditional probabilities for those negative samples, compared to the reference model. To manage the multi-mapping relations prevalent in human conversation, we augment contrastive dialogue learning with group-wise dual sampling. Extensive experimental results show that the proposed group-wise contrastive learning framework is suited for training a wide range of neural dialogue generation models with very favorable performance over the baseline training approaches. }"
"Spoken dialogue systems connect users computer applications human-machine conversations. The users achieve goals, finding restaurant, interacting task-oriented SDS multiple dialogue rounds turns. Dialogue state tracking important task SDS key function maintain state system track progress dialogue. In context work, state user's intention interest accumulated conversation history, user's intention interest turn referred turn-level state. %The state immediate state usually expressed terms set slot-value pairs. % For example, state contains two slot-value pairs, . In example, slots, , values. This state expresses system's belief user wants find cheap Italian restaurant. %At turn, system generates action, expressed user sentences natural language, user responds sentences, referred utterance. The system updates state. The problem dialogue state tracking learn predictor set training dialogues, specified sequences quadruples; learned predictor needs able predict state current turn dialogue history. %Various models developed DST. Traditional works usually deal task incorporating Spoken Language Understanding module. These works make limited progress rely hand-crafted features. The neural networks thus used DST achieve much success. Many neural-network models successfully applied DST. These models usually solve DST problem two approaches, Implicit Tracking Explicit Tracking. As shown Figure , Implicit Tracking models employs recurrent networks accumulate features extracted historical system action user utterance pairs. A classifier built upon accumulated features state prediction. Although Implicit Tracking captures temporal feature dependencies recurrent-network cells, state dependencies explicitly modeled. Only considering temporal feature dependencies insufficient accurate state prediction. This fact confirmed via ablation study experiment. Unlike Implicit Tracking, Explicit Tracking approaches, NBT GLAD, model state dependencies explicitly. From model structure Figure, Explicit Tracking approaches first build classifier predict turn-level state turn utilize state aggregator state aggregation. %One branch models handle DST problem building immediate-state predictor map historical immediate states accumulative state. To distinguish branch models another one ignore immediate state directly model state accumulation Recurrent Neural Networks , name former indirect models latter direct models. Unlike indirect models, Direct models, including, explicitly model dependencies immediate state accumulative state. Indirect models like NBT GALD, build immediate-state predictors using features extracted current turn's system action user uttrance , update state deterministic rule heuristics. They use Convolutional Neural Networks attention-based RNNs feature extraction achieve remarkable improvements upon previous models. Despite success, two limitations existing indirect models. Despite achieving remarkable improvements upon previous models, current Explicit Tracking models improved two aspects. One temporal feature dependencies considered model design. The Explicit Tracking models extract features current system action user utterance pair. In practice, slot-value pairs different turns highly dependent. For example, user specifies appears future turn-level states. \end{center} :Feature Extractor, CNNs, RNNs. {\bf RC}:Recurrent Cell, LSTM, GRU. {\bf CL}:Classifier. {\bf SA}:State Aggregator. The dotted arrowed lines emphasize modeling temporal feature dependencies. The dashed arrowed lines emphasize modeling temporal state dependencies.} \end{figure*} The uncertainties state aggregation expressively modeled. The state-aggregation approaches current Explicit Tracking models sub-optimal. The deterministic rule GLAD propagate errors future turns lead incorrect state aggregation. The heuristic aggregation NBT needs estimate best configuration coefficient . An approach reduce error propagation require less parameter estimation necessary state aggregation. % The uncertainties state aggregation expressively modeled. The deterministic state aggregation rule used GLAD sub-optimal errors caused hard decision propagate future turns lead incorrect aggregation future states. The NBT aware uncertainty deal using simple rule-based aggregation. And best configuration coefficient need estimated. %In Figure, see TEN TEN-X model correctly estimate state first turn second turn. At third turn, suppose predicted turn-label probabilities values , TEN-X model using deterministic state updating rule , output wrong state value . On contrary, TEN model uncertainty modeling still obtain true state value higher confidence . This example indicates significance uncertainty modeling state estimation. Although turn labels states related deterministic mapping , models estimating state deterministic rules fact sub-optimal. This estimation current state averaged uncertainties estimation previous states turn labels. Such averaging effect ignored deterministic rules used state updating. Models attempt update states heuristics leave state updating learning task also sub-optimal. Actually, due fact deterministic mapping already exists, heuristics learned function may under-perform deterministic mapping. A proper solution maintain deterministic mapping handle uncertainties simultaneously. % This training dialogue, system estimates current state, arguably accounted uncertainties estimating previous state estimating current turn label. That is, system predictive distribution current state result averaged uncertainties. Updating state using deterministic rules essentially ignores averaging effect. % dependency need taken account turn label state prediction. 2) inadequate consideration uncertainty state estimates. The prediction current state consider uncertainty estimating previous state uncertainty estimating current turn label. 3) less expressive modelling known dialogue dynamics. The known dynamics turn labels states model explicitly. A detailed discussion limitation existing models depicted ""Related Works"" Section. % %{\bf Less expressive modelling known dialogue dynamics.} Some works, including, build DST models recurrent networks. They ignore provided turn labels directly use states training target. In models, temporal feature dependency considered predicting states. They however, explicitly model known dynamics turn labels states. Demanding additional capacity, models may under-perform models use turn labels training target. The immediate state important source estimating accumulative state accumulative state changes immediate state changed. In study, propose novel Temporally Expressive Networks jointly model temporal feature dependencies temporal state dependencies ). Specifically, improve turn-level state prediction, exploits hierarchical recurrent networks capture temporal feature dependencies across dialogue turns. Furthermore, reduce state aggregation errors, introduce factor graphs formulate state dependencies, employ belief propagation handle uncertainties state aggregation. Evaluating DSTC2, WOZ MultiWoZ datasets, TEN shown improve accuracy turn-level state prediction state aggregation. The TEN model establishes new state-of-the-art model DSTC2 dataset state-of-the-art comparable model WOZ dataset. %The system take action respond users basis estimated state. % The State Updating task fully studied among three subtasks. Some works avoid Turn-level Prediction directly update dialogue state using Recurrent Neural Networks. It's natural use RNN state updating, supervision turn labels considered. Some works train models turn labels, update dialogue states rule, performs compatible better models updating states RNN. The limitations updating rules used recent works are: The updating rule GLAD makes hard decision state ignores rich probabilistic distribution information, thus turn-level error may propagate cumulatively. The updating rule NBT rely carefully tuned hyperparameters simple rule difficulty handling complex nature state updating. The MDT proposes RNN-based updating rule training using outputs turn-level prediction model. However, updating rule trained independently turn-level prediction model. % In work, propose novel state updating model factor graphs, call Factor Graph Tracker . The FGT implemented sum-product algorithm effectively reduce cumulative error propagation. It natural use sum-product algorithm summary previous states turn-level goal updating current state. The FGT easily built upon turn-level prediction model unified model; turn-level prediction state updating thus jointly learned. The state tracking may benefit joint learning strategy. % Another limitation existing models dependency turn-level goals ignored turn-level prediction stage. We argue dependency turn-level goals considered. For example, area slot usually requested food slot requested; slot-value pair already imformed, lower probability expressed user current turn. Instead directly building classifiers predict turn-level goals, work, use GRU turn-label-tracker handle dependency turn-level goals build classifiers upon outputs turn-label-tracker. % The GLAD introduce attention machinisum propose Global-Locally Self-Attentive encoder feature extraction. Specifically, global self attention model bidirectional LSTM used extract global features input slot, local BiLSTMATT extracting slot-specific features. We propose slot attention encoder simplify GLSA encoder significantly decreases model parameters. In work, propose group-wise contrastive dialogue learning approach, explicitly perceives difference well-chosen positive negative utterances, manages multi-mapping relations human conversations simultaneously. Given training instance, proposed learning framework first organizes group positive samples negative samples regarding context-response matching degrees, trains target dialogue model give higher conditional probabilities positive pairs lower probabilities negatives. Extensive experimental results show proposed learning framework brings solid favorable performance boost amongst various strong baseline approaches."," Dialogue state tracking  is an important part of a spoken dialogue system. Existing DST models either ignore temporal feature dependencies across dialogue turns or fail to explicitly model temporal state dependencies in a dialogue. In this work, we propose Temporally Expressive Networks  to jointly model the two types of temporal dependencies in DST. The TEN model utilizes the power of recurrent networks and probabilistic graphical models. Evaluating on standard datasets, TEN is demonstrated to improve the accuracy of turn-level-state prediction and the state aggregation.   %The existing models usually solve DST problem by two approaches, Implicit Tracking and Explicit Tracking. The Implicit Tracking employs recurrent networks to model the temporal feature dependencies across dialogue turns, but fails to consider the temporal state dependencies. While Explicit Tracking models state dependencies explicitly, it ignores the feature dependencies.  %Dialogue state tracking  is an important part of a spoken dialogue system. Some of the prior arts for DST build an momentary-state predictor and map the historical immediate states to the accumulative state. These models are however insufficiently modelling the temporal dependencies across dialogue turns and the uncertainties in state updating. In this work, we introduce a probabilistic graphical model to formulate the dialogue process and propose Temporally Expressive Networks  that utilizes hierarchical recurrent networks and belief propagation to deal with these issues. Evaluating on standard datasets, the proposed model is demonstrated to be significantly effective in improving accuracy for immediate-state prediction and reducing errors in state updating.  % a GRU  encoder sharing parameters across all slots to capture global information. The slot attention is adopted for each slot to capture local information. To alleviate the error propagation caused by hard decision on the turn-level goals, we propose a neural-network-based DST model with factor graphs and introduce the sum-product algorithm to handle the problem. Experimental stdies demonstrate that the proposed approach significantly improves the current art in tracking the joint goals."
"%%General subject \ac{NLG} process generating coherent natural language text non-linguistic data. Despite community agreement text speech output systems, far less consensus input be. A large number inputs hence employed \ac{NLG} systems, including images , numeric data, \ac{SW} data. Practical applications found domains weather forecasts , feedback car drivers , diet management . %%%specific problem subject Presently, generation natural language %\ac{SW}, precisely \ac{RDF} data gained substantial attention. The RDF-to-text task hence proposed investigate quality automatically generated texts \ac{RDF} \acp{KG}. %Moreover, \ac{RDF} demonstrated promising ability support creation \ac{NLG} benchmarks. With emergence neural methods, end-to-end data-to-text models introduced learn input-output mappings directly. These approaches rely much less %\todo{less comparative, ergo less what?} explicit intermediate representations compared rule-based approaches. Although Neural \ac{NLG} models achieving good results %\todo{cite paper shown} , English language widely targeted. % \todo[inline]{why important able generate different language text model} % \todo[inline]{What motivation behind investigating generation different language families?} In work, alleviate language limitation proposing multilingual approach, named NABU. The motivation behind multilingual models lies several directions, mainly transfer learning; low-resource language pairs trained together high-resource languages, translation quality improves; zero-shot translation, multilingual models able translate language pairs similar families never seen training; Easy deploy, multilingual model achieving performance many languages comparison several separate language-specific models much desirable companies terms deployment. Our approach, NABU, based fact knowledge graphs language-agnostic hence used encoder side generate multilingual text. NABU consists encoder-decoder architecture incorporates structural information RDF triples using encoding mechanism inspired \ac{GAT}. In contrast recent related work, NABU relies use reification %\todo{sure?} strategy modeling graph structure RDF input. The decoder part %\todo{do mean decoder?} based vanilla Transformer model along unsupervised tokenization model. %which implements \ac{BPE} unigram language model handling multilinguality. %\todo{Is statement really necessary here?Would make sense add details approach.} %Note NABU follows strategy recent literature multilingual \ac{NMT} models special token used encoder determine target language translate. %evaluation We evaluate NABU standard benchmarking WebNLG datasets three settings: monolingual, bilingual multilingual. For monolingual setting, compare NABU state-of-the-art English approaches also perform experiments Russian German. The goal bilingual setting analyze performance NABU language families. To achieve goal, train evaluate bilingual models using NABU English-German English-Russian. In multilingual setting, compare NABU multilingual Transformer model English, German Russian. %%%results Our results show NABU outperforms state-of-the-art approaches English achieves 66.21 BLEU. NABU also achieves consistent results across languages multilingual settings 56.04 BLEU. In addition, NABU presents promising results bilingual models 61.99 BLEU. %\todo{numbers?} Our findings suggest NABU able generate multilingual text similar quality generated humans. %conclusion The main contributions paper summarized follows: The version NABU used paper also experimental data publicly available.~\footnote{https://github.com/dice-group/NABU}. This paper studies problem state generation multi-domain dialogues. Existing generation-based models fail model dialogue dependencies ignore slot-overlapping problem MDST. To overcome limitation existing models, present novel Parallel Interactive Networks accurate robust dialogue state generation. The design PIN model inspired interactive nature dialogues overlapping slots ontology. The Interactive Encoder characterizes cross-turn dependencies in-turn dependencies. The slot-overlapping problem solved introducing slot-level context. Furthermore, distributed copy mechanism introduced perform selective copy either historical system utterances historical user utterances. Empirical studies two benchmark datasets demonstrate effectiveness PIN model. File emnlp2020.tex Based style files ACL 2020, Based style files ACL 2018, NAACL 2018/19, Based style files ACL-2015, improvements taken NAACL-2016 style Based style files ACL-2014, were, turn, based ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009, EACL-2009, IJCNLP-2008... Based style files EACL 2006 e.agirre@ehu.es Sergi.Balari@uab.es ACL 08 Joakim Nivre Noah Smith \documentclass[11pt,a4paper]{article} \usepackage[hyperref]{emnlp2020} \usepackage{times} \usepackage{latexsym} \usepackage{amsmath} \usepackage{amssymb} \usepackage{float} \usepackage{booktabs} \usepackage{enumerate} \usepackage{tikz} \usetikzlibrary{calc} \DeclareMathOperator{\softmax}{softmax} \include{tikzd} \renewcommand{\UrlFont}{\ttfamily\small} This strictly necessary, may commented out, improve layout manuscript, typically save space. \usepackage{microtype} \aclfinalcopy Uncomment line final submission \def\aclpaperid{***} Enter acl Paper ID \setlength\titlebox{7cm} You expand titlebox need extra space show authors. Please make titlebox smaller 5cm ; check camera-ready version ask change back. \newcommand\BibTeX{B\TeX} \title{Parallel Interactive Networks Multi-Domain Dialogue State Generation} \author{ Junfan Chen \\ BDBC SKLSDE\\ Beihang University, China \\ \texttt{chenjf@act.buaa.edu.cn} \\ \And Richong Zhang\thanks{Corresponding author}\\ BDBC SKLSDE\\ Beihang University, China \\ \texttt{zhangrc@act.buaa.edu.cn} \\ \AND Yongyi Mao \\ School EECS\\ University Ottawa, Canada \\ \texttt{ymao@uottawa.ca} \\ \And Jie Xu \\ School Computing\\ University Leeds, United Kingdom \\ \texttt{j.xu@leeds.ac.uk} \\ } \date{} \begin{document} \maketitle"," The RDF-to-text task has recently gained substantial attention due to continuous growth of Linked Data. In contrast to traditional pipeline models, recent studies have focused on neural models, which are now able to convert a set of RDF triples into text in an end-to-end style with promising results. However, English is the only language widely targeted. We address this research gap by presenting NABU, a multilingual graph-based neural model that verbalizes RDF data to German, Russian, and English. NABU is based on an encoder-decoder architecture, uses an encoder inspired by Graph Attention Networks and a Transformer as decoder. Our approach relies on the fact that knowledge graphs are language-agnostic and they hence can be used to generate multilingual text. We evaluate NABU in monolingual and multilingual settings on standard benchmarking WebNLG datasets. Our results show that NABU outperforms state-of-the-art approaches on English with 66.21 BLEU, and achieves consistent results across all languages on the multilingual scenario with 56.04 BLEU. %Moreover, we trained bilingual models for analyzing the capability of NABU to model jointly distinct language families such as English-Russian.  %\todo{Which conclusion did you reach from this training?} \keywords{Knowledge Graphs  \and Natural Language Generation \and Semantic Web.}"
"We digitally surrounded computational Language Models guide us writing reduce user effort, suggest different options words/sentences enhance style, fix grammatical/correctness errors accurately . Many keys press writing keyboard act part inputs compose new datasets models shape communicate others. Nevertheless, happen way write code? Succinctly, yes. According recent surveys found literature , Natural Language Processing subfield related programming language includes examples LMs used several tasks contexts. For example, authors used different techniques graph-based statistical LMs, probabilistic LMs, Deep Learning LMs suggest code programmers similarly auto-completer features IDEs. LMs used generate automated source code based sample code inputs pseudo-code evaluating generated code performs . Another exciting application NLP source code languages automatic translation different languages. The work reported explores different supervised unsupervised approaches migrate code different programming languages improve interoperability port codebases written obsolete deprecated languages . Another example found use Bayesian networks, attention mechanisms, pointer networks fill given code portion missings. There general understanding natural languages different characteristics NLP broad field. Since exist many research fields related human languages, richer background existing language characteristics. For example, much knowledge aspects like minimal representation units word specific language, used words language, word neologism not. Programming languages share syntax similarities spoken languages. However, restrictions sense common words neologisms , syntax restrictions features punctuation, format, style. Every programming language indeed reserved words symbols denote different actions, resources, syntax. However, essential part source code limited programmer imagination, conventions existing, guides good practices. As claims, In paper, Karampatsis Sutton \citeyear{karampatsis2019maybe} present segmenting words subword units improve source code modeling. Similarly, researchers dug representing source code vocabulary similar emphasis modeling words using sub-word units envisioning importance using neural networks . Nevertheless, word segmentation affect accuracy appropriateness code generated auto-completed modern LM using deep learning approaches? That kind question raises main goal paper: discover kinds associations different modern neural network architectures tokenization models produce best results creating LMs generate auto-complete source code. To pursue goal, research aims conduct experiments combining different deep neural network architectures different tokenization pre-trained models existing Python dataset. Using experimentation, want investigate combinations improve code generation auto-completion tasks checking outcomes tasks using metrics like accuracy human assessment. The rest paper follows: Section 2 presents different approaches followed research, DNNs used, software methods data employed. Section 3 describes results achieved research according different metrics tests, section 4 discusses findings implications results appropriate. Finally, Section 5 presents conclusions. \todo[inline]{BLEURT: Learning Robust Metrics Text Generation] https://arxiv.org/abs/2004.04696)} \todo[inline]{Say plan investigate generation different graphs [Kaffee et al. : Mind Gap: Generation Multilingual Wikipedia Summaries Wikidata ArticlePlaceholders]} We presented multilingual RDF verbalizer relies graph attention \ac{NN} along reification strategy. We carried extensive evaluation set certifying quality approach. Our experiments suggest approach, named NABU, outperforms state-of-the-art approaches English. Additionally, NABU presented consistent results across languages used evaluation. NABU language-agnostic, means ported easily languages considered paper. Moreover, reported challenges training bilingual models languages distinct families. To best knowledge, first approach exploit achieve multilinguality successfully RDF-to-text task. As future work, aim exploit graph-based neural architecture reification approaches improving NABU's performance. Additionally, plan investigate deal similarity relations combining language models new evaluation metrics. Moreover, plan investigate methodology context low-resource scenarios well different \acp{KG}."," In recent years, the use of deep learning in language models gained much attention. Some research projects claim that they can generate text that can be interpreted as human-writing, enabling new possibilities in many application areas. Among the different areas related to language processing, one of the most notable in applying this type of modeling is programming languages. For years, the Machine Learning community has been researching this software engineering area, pursuing goals like applying different approaches to auto-complete, generate, fix, or evaluate code programmed by humans. Considering the increasing popularity of the Deep-Learning-enabled language models approach, we detected a lack of empirical papers that compare different deep learning architectures to create and use language models based on programming code. This paper compares different neural network architectures like AWD-LSTMs, AWD-QRNNs, and Transformer while using transfer learning and different tokenizations to see how they behave in building language models using a Python dataset for code generation and filling mask tasks. Considering the results, we discuss each approach different strengths and weaknesses and what gaps we find to evaluate the language models or apply them in a real programming context."
"A dominant approach text generation use autoregressive models learned maximum likelihood estimation supervised data. However, approach introduces two well-known discrepancies training evaluation objectives lead undesired generations. % First, training loss negative log-likelihood, whereas evaluation based human judgment output quality. Under model misspecification, MLE tends over-generalize, assigning large probability mass high-quality low-quality sequences . Therefore, practice, must carefully select decoding algorithms produce high-quality outputs. Second, training, autoregressive model conditions gold history/prefix; however, inference time conditions model-generated history. This known exposure bias problem . In worst case, one incorrect prediction produce low-probability prefix gold data distribution, errors compound following steps . In practice, prior work observed problems repetition hallucination partly due exposure bias . We aim bridge gap training evaluation paper. To match training evaluation objectives, ideally maximize output quality given model-generated histories. This corresponds reinforcement learning objective: maximizing expected reward trajectories induced policy . However, optimizing objective notoriously difficult. Prior RL approaches mainly focus fine-tuning learned model optimize sequence-level metrics BLEU~, empirically remains unclear RL beneficial text generation . % Note many challenges RL arise exploring exponentially large space sequences, sparse rewards close reference. We thus propose learn reference sequences without interaction . Specifically, use off-policy policy gradient importance weighting , training examples higher probability model weighted higher. Further, reward functions approximate human judgment output quality estimating likely human would generated sequence. We call algorithm \algoname . Results news summarization, question generation, machine translation show \algoname leads better model performance MLE RL fine-tuning task metrics human-rated quality. Further, analysis shows \algoname learns high-precision models less sensitive decoding algorithms. In addition, alleviates exposure bias: output quality degrade much generation length increases. Considering results obtained, one could convincingly assert tokenization model used profoundly affects results generating automated source code. Although may accurate, must discuss carefully. First, overall results consistent existing literature . Sub-word tokenization works better case modeling source code, \citeyear{karampatsis2019maybe} stated. Every result obtained consistent sense. Even more, \citeyear{karpathy2016} envision, char tokenization probably best option try default dealing LMs source code. Furthermore, according results achieved, models GPT-2 -using tokenization model based BPE raw bytes- outperform LSTM/QRNN models like tested grasp better internals programming language. As showcased results, even GPT-2 best model terms accuracy, gave better code outputs ones selected comparison. As future work, would great check better textual output case GPT-2 a) much bigger better pre-trained model , b) related dataset's size quality, c) related causes or, d) related issues. Continuing comments accuracy, one may note textual outputs generated AWD-LSTM char, AWD-QRNN char, GPT-2 could polished accurate. The final training loss higher validation loss three selected DNN architectures, sign underfitting. We find issue BERT RoBERTa-based models. Whether purpose paper produce state-of-the-art results per se, continued training five epochs verify it. The improvement obtained extending training best approaches residual general, decided report results 1+30 epochs AWD-LSTMs AWD-QRNNs, 1+10 epochs Transformer. Regarding pre-training transfer learning, every pre-trained model got better accuracy non-pre-trained counterparts except word tokenization models. It seems strongly related statements introduced beginning paper, citing source code restrictions sense common words neologisms. In sense, conclusion comes mind rapidly: consider source code words ord units, probably fit fixed set words used human language like English. So, LM knowledge acquired pre-training entirely valid get fixed set words compose language. Most words programming language neologisms LMs pre-trained English, thus, needs incorporate relationships learned knowledge. For sub-word units, LM less sensible neologisms. Potentially, could robust divided word since set bytes chars straightforward chunks present richer constructions information units. Going deeper research, concerning pre-training effect LMs modeling source code, could worth researching relationship pre-training different human-spoken languages LM ability work existing source code specific programming languages. About tests made generating source code filling blanks using trained LMs, think that, general, textual results obtained good, yet informative LMs working improved. One things explain results dataset used. In case, used public dataset researchers use make results experiments comparable replicable. In literature, find standard dataset tasks compare easily. Other papers use custom datasets, find lack literature well-recognized code datasets use. Comparing recent papers NLP field used basis research , dataset may relatively small train big LM accomplish appropriately challenging tasks like generating source code auto-completing it. Future work may testing new approaches bigger datasets train big LMs focused modeling Python language checking whether results better. Recent examples LMs -such GPT-3 - claim produce accurate textual outputs even contexts trained. Part explanation given ability use gargantuan datasets combined Transformer attention-based architectures. So, approaches also relevant contexts like ours. Another line future research using datasets focused specific libraries Python aspects verify approaches specialize positively contexts DNN models used paper. Related evaluating code generated filled, observed literature different approaches . In context LMs modeling source code, many papers software libraries devoted translating programming languages typically evaluate text generation using methods metrics like BLEU , variants like SacreBLEU . Other papers like rely accuracy assess LM performance based deep learning. Some models even solve different tasks part existing benchmarks evaluated, checking perplexity . The current tendency large models evaluate using human intervention evaluate output quality . We assessed models using accuracy experiments evaluated models textual outputs based prior human knowledge. It would interesting future plan new evaluation processes involving larger cohorts source code experts evaluate models do. One potential new assessments usability tests conducted programmers. They compare code would write code proposed DNNs presented result common code auto-completion tools included integrated development environments. As outlined results section, relying metrics like accuracy enough. As case, accuracy metrics good indicator model performance, yet need verify LMs behavior quality using complementary methods like specialized metrics human evaluation. For tasks like auto-completion source code generation, existing specialized metrics , one future research lines improving evaluation LMs source code. Based existing ideas broad NLP, many opportunities explore sense. From new test suites language models used source code contexts behavioral testing human-centric evaluation models particular emphasis reproducible unbiased assessments, combinations automatic testing human-centric assessments. \section{Conclusions} This paper compares different approaches tokenization models, deep neural network architectures, pre-trained models, transfer learning affect results language models used generate source code auto-complete software pieces. We studied different DNN architectures like AWD-LSTM, AWD-QRNN, Transformer seek kind work better different tokenization models . Also, compared pre-training effect results given LMs training fine-tuning via transfer learning work languages . As result work, find small LMs , tokenization using char-sized chunks works better using tokenization models. In larger models like Transformer GPT-2, accuracy slightly worse architectures. However, GPT-2 raised better results source code generation tests . For source code auto-completion, tested transformer models like BERT RoBERTA. While accuracy models, perform well performing tasks proposed tests. In general, find pre-trained models work better, even trained initially programming language like Python . Finally, related evaluating tasks like automating source code generation source code auto-completion, raise concerns literature gaps propose research lines work future. \section{ Acknowledgments} We thank IBM Quantum team IBM Research ETX team insightful discussions research support received development research.","     Current approaches to text generation largely rely on autoregressive models and maximum likelihood estimation.     This paradigm leads to       diverse but low-quality samples due to mismatched learning objective and evaluation metric      and  exposure bias due to mismatched history distributions .      To alleviate these problems, we frame text generation as an offline reinforcement learning  problem with expert demonstrations ,     where the goal is to maximize quality given model-generated histories.      We propose \algoname :     an easy-to-optimize algorithm that learns from the demonstrations by importance weighting.      Intuitively, \algoname upweights confident tokens and downweights unconfident ones in the reference during training,      avoiding optimization issues faced by prior RL approaches that rely on online data collection.     According to both automatic and human evaluation,     models trained by \algoname outperform those trained by MLE and policy gradient      on summarization, question generation, and machine translation.      Further, our models are less sensitive to decoding algorithms     and alleviate exposure bias."
"\let\thefootnote\relax\footnote{ Corresponding author.} Recent years witnessed significant improvements vision language communities, consequently led substantial attention vision-language multi-modality tasks visual grounding , image captioning , visual question answering . Furthermore, video becomes ubiquitous, daily source information communication, video-language tasks video captioning , video moment retrieval , video question answering emerging important topics. Among topics, video QA especially challenging, requires fine-grained understanding video language. \sh{Figure shows example multiple-choice video QA TVQA dataset. The multiple-choice video QA task requires model select correct answer given question, corresponding video frames, subtitles.} We provide efficient algorithm addresses two train/test discrepancies MLE training text generation: likelihood learning objective vs. quality evaluation metric; gold history training vs. model-generated history inference. We demonstrated off-policy RL promising framework text generation, matched train/test objectives optimization advantages like MLE. We believe advanced off-policy learning techniques easily integrated text generation improve performance."," Video Question Answering  requires fine-grained understanding of both video and language modalities to answer the given questions. In this paper, we propose novel training schemes for multiple-choice video question answering with a self-supervised pre-training stage and a supervised contrastive learning in the main stage as an auxiliary learning. In the self-supervised pre-training stage, we transform the original problem format of predicting the correct answer into the one that predicts the relevant question to provide a model with broader contextual inputs without any further dataset or annotation. For contrastive learning in the main stage, we add a masking noise to the input corresponding to the ground-truth answer, and consider the original input of the ground-truth answer as a positive sample, while treating the rest as negative samples. By mapping the positive sample closer to the masked input, we show that the model performance is improved. We further employ locally aligned attention to focus more effectively on the video frames that are particularly relevant to the given corresponding subtitle sentences. We evaluate our proposed model on highly competitive benchmark datasets related to multiple-choice video QA: TVQA, TVQA+, and DramaQA. Experimental results show that our model achieves state-of-the-art performance on all datasets. We also validate our approaches through further analyses."
"Neural machine translation typically follows encoder-decoder framework, directly applies single neural network transform source sentence target sentence. With tens millions trainable parameters NMT model, translation tasks usually data-hungry, many low-resource even zero-resource terms training data. Following idea unsupervised self-supervised pre-training methods NLP area , works proposed improve NMT model pre-training, making full use widely available monolingual corpora . Typically, two different branches pre-training approaches proposed NMT: model-fusion parameter-initialization. The model-fusion approaches seek incorporate sentence representation provided pre-trained model, BERT, NMT model . These approaches able leverage publicly available pre-trained checkpoints website need change NMT model fuse sentence embedding calculated pre-trained model. Large-scale parameters pre-trained model significantly increase storage cost inference time, makes hard branch approaches directly used production. As opposed model-fusion approaches, parameter-initialization approaches aim directly pre-train whole part NMT model tailored objectives, initialize NMT model pre-trained parameters . These approaches production-ready since keep size structure model standard NMT systems. While achieving substantial improvements, pre-training approaches two main cons. Firstly, pointed \citet{yang2019xlnet}, artificial symbols like [mask] used approaches pre-training absent real data fine-tuning time, resulting pretrain-finetune discrepancy. Secondly, pre-training step involves sentences language, approaches unable make use cross-lingual alignment information contained source target monolingual corpus. We argue that, cross-lingual sequence generation task, NMT requires tailored pre-training objective capable making use cross-lingual alignment signals explicitly, e.g., word-pair information extracted source target monolingual corpus, improve performance. To address limitations mentioned above, propose Code-Switching Pre-training NMT. We extract word-pair alignment information source target monolingual corpus automatically, apply extracted alignment information enhance pre-training performance. The detailed training process CSP presented two steps: 1) perform lexicon induction get translation lexicons unsupervised word embedding mapping ; 2) randomly replace words input sentence translation words extracted translation lexicons train NMT model predict replaced words. CSP adopts encoder-decoder framework: encoder takes code-mixed sentence input, decoder predicts replaced fragments based context calculated encoder. By predicting sentence fragment replaced encoder side, CSP able either attend remaining words source language translation words replaced fragment target language. Therefore, CSP trains NMT model to: 1) learn build sentence representation input sentence traditional pre-training methods do; 2) learn perform cross-lingual translation extracted word-pair alignment information. In summary, mainly make following contributions: \footnotetext[1]{To used production easily, models need distilled student model structure size standard NMT systems.} \sh{Video QA requires fine-grained understanding video language modalities. To address this, focus training procedure could possibly take advantage given data.} In paper, propose novel training schemes \sh{that} specialize multiple-choice video QA. We first pre-train model transformed problem format \sh{of predicting questions contexts} better weight initialization. \sh{We} train \sh{our} model original QA problem format guided contrastive representation learning. Our model achieves state-of-the-art performance three highly challenging video QA datasets. We expect proposed method applied various multiple-choice video QA tasks, bringing performance improvement.","  This paper proposes a new pre-training method, called Code-Switching Pre-training  for Neural Machine Translation . Unlike traditional pre-training method which randomly masks some fragments of the input sentence,  the proposed CSP randomly replaces some words in the source sentence with their translation words in the target language. Specifically, we firstly perform lexicon induction with unsupervised word embedding mapping between the source and target languages, and then randomly replace some words in the input sentence with their translation words according to the extracted translation lexicons. CSP adopts the encoder-decoder framework: its encoder takes the code-mixed sentence as input, and its decoder predicts the replaced fragment of the input sentence. In this way, CSP is able to pre-train the NMT model by explicitly making the most of the cross-lingual alignment information extracted from the source and target monolingual corpus. Additionally,  we relieve the pretrain-finetune discrepancy caused by the artificial symbols like [mask].  To verify the effectiveness of the proposed method, we conduct extensive experiments on unsupervised and supervised NMT. Experimental results show that CSP achieves significant improvements over baselines without pre-training or with other pre-training methods."
"% \{-0.3em} % General introduction Belief tracking important component task-oriented dialog systems. The system tracks user goals multiple dialog turns, i.e. infers structured belief states expressed terms slots values , query external database . Different belief tracking models proposed recent years, either trained independently within end-to-end trainable dialog systems . % problem Existing belief trackers mainly depend supervised learning human annotations belief states every user utterance. However, collecting turn-level annotations labor-intensive time-consuming, often requires domain knowledge identify slots correctly. Building E2E trainable dialog systems, called E2E dialog systems short, even magnifies demand increased amounts labeled data . % idea Notably, often easily-available unlabeled dialog data customers trained human agents accumulated real-world customer services. In paper, interested reducing reliance belief state annotations building E2E task-oriented dialog systems, leveraging unlabeled dialog data towards semi-supervised learning. Intuitively, dialog data, even unlabeled, used enhance performance belief tracking thus benefit whole dialog system, cues user inputs system responses reveal belief states, shown Figure . %The underlying idea simple: system makes responses based belief user goals, able use system response infer corresponding belief state. %The correlation belief states system responses also reported previous works , shows learning belief tracking response generation together beneficial tasks. % mutual information % proposed model Technically, propose latent variable model task-oriented dialogs, called LAtent BElief State dialog model. The model generally consists multiple turns user inputs system responses observations, belief states latent variables. Basically, \modelname{} conditional generative model belief states system responses given user inputs, i.e. . Once built, model used infer belief states generate responses. More importantly, latent variable modeling enables us develop semi-supervised learning mix labeled unlabeled data principled variational learning framework . In manner, hope LABES model exploit cues belief tracking user inputs system responses. Furthermore, develop \modelname{}-S2S, specific model instantiation \modelname{}, employing copy-augmented Seq2Seq based conditional distributions implementing . %To leverage correlation, propose LAtent BElief State dialog model , conditional generative model models belief states system responses jointly given user inputs. %In particular, represent structured belief state discrete latent variables, e.g. sequence words defined vocabulary space. %With recent advances neural variational inference , effective methods proposed address structured latent representation learning , discrete latent variable modeling sequential inference . Inspired works, propose VI-based scheme learn latent belief states sequentially multiple dialog turns, employed unsupervised scenarios. Thus model conduct semi-supervised learning labeled unlabeled dialog data. We show advantage model compared E2E task-oriented dialog models, demonstrate effectiveness semi-supervised learning scheme three benchmark task-oriented datasets: CamRest676 , In-Car MultiWOZ across various scales domains. In supervised experiments, \modelname{}-S2S obtains state-of-the-art results CamRest676 In-Car, outperforms existing models leverage large pretrained language models MultiWOZ. In utilizing unlabeled dialog data, semi-supervised \modelname{}-S2S significantly outperforms supervised-only prior semi-supervised baselines. Remarkably, reduce annotation requirements 50\% without performance loss MultiWOZ, equivalent saving around 30,000 annotations. In paper, discussed importance precisely identifying candidates order resolve toponyms real-world referents. In particular, highlighted necessity working noisy non-standard texts . To foster research intermediary step, introduced DeezyMatch, flexible deep learning method candidate selection toponym matching. It based state-of-the-art neural network architectures tested different evaluation settings, considering various challenging scenarios comparison series well established baselines. DeezyMatch, evaluation framework presented paper, resources employed useful contributions researchers working intersection geospatial information retrieval digital humanities. The acknowledgments section defined using ""acks"" environment . This ensures proper identification section article metadata, consistent spelling heading. The next two lines define bibliography style used, bibliography file."," 		%ro 		Structured belief states are crucial for user goal tracking and database query in task-oriented dialog systems. However, training belief trackers often requires expensive turn-level annotations of every user utterance. 		In this paper we aim at alleviating the reliance on belief state labels in building end-to-end dialog systems, by leveraging unlabeled dialog data towards semi-supervised learning. 		We propose a probabilistic dialog model, called the LAtent BElief State  model, where belief states are represented as discrete latent variables and jointly modeled with system responses given user inputs. 		Such latent variable modeling enables us to develop semi-supervised learning under the principled variational learning framework. 		Furthermore, we introduce LABES-S2S, which is a copy-augmented Seq2Seq model instantiation of LABES\footnote{Code available at https://github.com/thu-spmi/LABES}. 		In supervised experiments, LABES-S2S obtains strong results on three benchmark datasets of different scales. In utilizing unlabeled dialog data, semi-supervised LABES-S2S significantly outperforms both supervised-only and semi-supervised baselines. 		Remarkably, we can reduce the annotation demands to 50\% without performance loss on MultiWOZ."
"Deep learning achieved significant successes, successes heavily rely massive annotated data. Few-Shot Learning one keys breaking shackle, commits learning new tasks examples . FSL made impressive progress many areas, computer vision . But progress FSL natural language processing much slower. One primary constraints lack unified benchmark few-shot NLP, thus new methods cannot easily compared iteratively improved. %Similar computer vision, Existing few-shot NLP researches mainly focus simple N-classification problems, text classification entity relation classification . However, one hand, works often report results constructed few-shot data, pretty inefficient results comparison thus hinders cumulative progress. On hand, simple N-classification problems cannot reflect complexity real-world NLP tasks. NLP tasks often face challenges structure prediction problems, sequence labeling parsing . More importantly, different NLP tasks often deeply related other, i.e. multi-task problems . One typical scenario complex NLP Dialogue Language Understanding problem, includes two sub-tasks: Intent Detection Slot Tagging . As multi-task problem, two sub-tasks proved strongly promote depend . One main obstacles constructing NLP FSL benchmark comes special evaluation paradigm FSL. Few-shot models usually first pre-trained data-rich domains tested unseen few-shot domains. %where pre-training test tasks need related . Thus, FSL evaluations always need lot different domains conquer result-randomness domain selection limited learning shots. But often hard gather enough domains NLP tasks. To solve this, existing works construct fake domains single dataset. They split labels training labels testing labels. Then, construct fake pre-training testing domains training testing labels respectively, testing labels unseen pre-training. Such simulation yield plenty related domains, lacks reality works label set large. Actually, splitting labels impractical many real-world NLP problems. For example Name Entity Recognition, label sets often small split . In paper, present FewJoint, novel FSL benchmark joint multi-task learning, promote FSL research NLP area. To reflect real word NLP complexities beyond simple N-classification, adopt sophisticated important NLP problem benchmark: Task-oriented Dialogue Language Understanding. Task-oriented Dialogue rising research area develops dialogue systems help users achieve goals, booking tickets. Language Understanding fundamental module Task-oriented Dialogue extracts semantic frames user utterances . It contains two sub-tasks: Intent Detection Slot Tagging. With Slot Tagging task, benchmark covers one common structure prediction problems: sequence labeling. Besides, thanks natural dependency Intent Detection Slot Tagging, benchmark embody multi-task challenge NLP problems. %Fig shows example few-shot joint language understanding. To conquer randomness make adequate evaluation, include 59 different dialogue domains real industrial API, considerable domain amount compared existing few-shot dialogue data. We also provide Few-shot Learning platform ease experiment set comparison. In summary, contribution three-fold: We present novel Few-shot learning benchmark 59 real-world domains, allows evaluating few-shot models without constructing fake domains. We propose reflect real-world NLP complexities covering structure prediction problems multi-task learning problems. We propose Few-shot Learning platform ease comparison implement few-shot methods. % \end{figure*} % In paper interested reducing belief state annotation cost building E2E task-oriented dialog systems. We propose conditional generative model dialogs - \modelname{}, belief states modeled latent variables, unlabeled dialog data effectively leveraged improve belief tracking semi-supervised variational learning. Furthermore, develop LABES-S2S, copy-augmented Seq2Seq model instantiation LABES. We show strong benchmark performance \modelname{}-S2S effectiveness semi-supervised learning method three benchmark datasets. In experiments MultiWOZ, save around 50\ , i.e. around 30,000 belief state annotations without performance loss. There interesting directions future work. First, \modelname{} model general enhanced by, e.g. incorporating large-scale pre-trained language models, allowing options belief state decoder response decoder Transformer based. Second, \modelname{} provides principled framework build E2E dialog systems less reliance annotations, enable semi-supervised learning and, explain below, reinforcement learning well. fix form backbone model implement conditional probabilities , may simply substitute copy-augmented Seq2Seq implementation sophisticated models GPT-2 train via semi-supervised learning principles achieve better performance. Second, analogously introduce dialog acts latent variables define joint distribution , trained semi-supervised learning reinforcement learning well."," Few-shot learning  is one of the key future steps in machine learning and has raised a lot of attention. However, in contrast to the rapid development in other domains, such as Computer Vision, the progress of FSL in Nature Language Processing  is much slower.  One of the key reasons for this is the lacking of public benchmarks.  NLP FSL researches always report new results on their own constructed few-shot datasets, which is pretty inefficient in results comparison and thus impedes cumulative progress. In this paper, we present FewJoint, a novel Few-Shot Learning benchmark for NLP.  Different from most NLP FSL research that only focus on simple N-classification problems, our benchmark introduces few-shot joint dialogue language understanding, which additionally covers the structure prediction and multi-task reliance problems.  This allows our benchmark to reflect the real-word NLP complexity beyond simple N-classification.  Our benchmark is used in the few-shot learning contest of SMP2020-ECDT task-1.\footnote{The Eighth China National Conference on Social Media Processing. Link: \url{https://smp2020.aconf.cn/smp.html}}  We also provide a compatible FSL platform to ease experiment set-up.\footnote{The dataset and platform is available at \url{https://github.com/AtmaHou/MetaDialog}}"
"Event coreference resolution aims identify event mentions document refer event . For example, two event mentions Figure , departing leave, refer EndPosition event Nokia's CEO. Traditional event coreference resolution methods usually rely series upstream components , entity recognition event detection. Such pipeline framework, unfortunately, often suffers error propagation problem. For instance, best event detection system KBP 2017 achieved 56 F1 , undoubtedly limit performance follow-up event coreference task . Furthermore, previous approaches use hand-crafted features , heavily depend NLP components thus hard generalize new languages/domains/datasets. In paper, propose End-to-End Event Coreference method -- neural network, predict event chains raw text end-to-end manner. For example, taking raw text Figure input, directly output two event coreference chains, \{departing, leave, goodbye\} \{rejoin\}. By jointly modeling event detection event coreference, neural network require prior components, representations/pieces evidence different tasks different decisions shared reinforced. Besides, learned end-to-end manner, inherently resolve error propagation problem. End-to-end event coreference, however, challenging due mention diversity long-distance coreference. First, event mentions highly diversified , may variety syntactic objects, including nouns, verbs, even adjectives. For example, EndPosition event triggered departing, leave, goodbye former. By contrast, mentions entity coreference mostly noun phrases . Second, coreferential event mentions commonly appear long-distance sentences, therefore event coreference intricately governed long-distance, semantic-dependent decisions . For example, Figure closest antecedent\footnote{In paper, antecedents coreferential mentions appear earlier document.} mention goodbye -- leave, far it. To resolve coreference two distant, diverse event mentions, system rely semantic meanings, i.e., describe EndPosition event different perspectives. By contrast, entity mentions' closest antecedents immediately preceding sentence , resolved easily using local syntactic clues. To resolve mention diversity problem long-distance coreference problem, paper proposes type-guided mechanism neural network. This mechanism bridges distant, diverse event mentions exploiting event type information three folds: 1) type-informed antecedent network enables capture semantic information event mentions predicting coreferential scores type scores simultaneously; 2) type-refined mention representation enhances mention representation type information, therefore even lexically dissimilar mentions bridged together, two diverse EndPosition mentions goodbye departing; 3) type-guided decoding algorithm exploit global type consistency accurate event chains. The main contributions paper are: 1. We propose end-to-end neural network event coreference resolution - neural network. jointly model event detection event coreference, learn automatically extract features raw text. To best knowledge, first end-to-end neural event coreference model achieve state-of-the-art performance. 2. We design type-guided mechanism event coreference, effectively resolve mention diversity problem long-distance coreference problem event coreference resolution. 3. We conduct experiments two standard datasets: KBP 2016 KBP 2017, show achieves new state-of-the-art performance. And additional ablation experiments verify effectiveness proposed type-guided mechanism. In paper, present novel few-shot learning benchmark NLP tasks, first few-shot NLP benchmark joint multi-task learning. Compared existing few-shot learning data, benchmark reflects real-world NLP complexities better covering structure prediction problem multi-task learning problem. Also, benchmark consists 59 real dialogue domains. This allows evaluate few-shot model without constructing fake domain like existing works.","   Traditional event coreference systems usually rely on pipeline framework and hand-crafted features, which often face error propagation problem and have poor generalization ability.   In this paper, we propose an End-to-End Event Coreference approach -- $\text{E}^{3}\text{C}$ neural network, which can jointly model event detection and event coreference resolution tasks, and learn to extract features from raw text automatically.   Furthermore, because event mentions are highly diversified and event coreference is intricately governed by long-distance, semantic-dependent decisions, a type-guided event coreference mechanism is further proposed in our $\text{E}^{3}\text{C}$ neural network.   Experiments show that our method achieves new state-of-the-art performance on two standard datasets."
"Sequence labeling assigns token label sequence. Tasks Named Entity Recognition , Part-Of-Speech tagging chunking formulated sequence labeling tasks. BiLSTM-CRF one successful neural sequence labeling architectures. It feeds pretrained word representations single layer bi-directional LSTM encoder extract contextual features feeds features CRF decoder layer produce final predictions. The CRF layer linear-chain structure models relation neighboring labels. In traditional CRF approach, exact probabilistic inference algorithms forward-backward Viterbi algorithms applied training prediction respectively. %The Viterbi algorithm applied exactly find best label sequence inference forward-backward algorithm applied compute posterior marginal distributions exactly position training. In many sequence labeling tasks, CRF layer leads better results simpler method predicting label independently. In practice, sometimes require fast sequence labelers training prediction . The BiLSTM encoder CRF layer contain sequential computation require time input words even parallelized GPU. A common practice improve speed encoder replace BiLSTM CNN structure , distill larger encoders smaller ones settings . The CRF layer, however, difficult replace superior accuracy compared faster alternatives many tasks. %More recently, \citet{cui-zhang-2019-hierarchically} proposed BiLSTM-LAN replace CRF layer, lower time complexity, network introduces 3 additional LSTM layers require sequential computations. The CRF layer still necessary better accuracy many tasks, limits speed. % showed algorithm unfolded RNN grid-structure, expand work sequence structure unfold MFVI algorithm RNN In order achieve sublinear time complexity CRF layer, must parallelize CRF prediction tokens. In paper, apply Mean-Field Variational Inference approximately decode linear-chain CRF. MFVI iteratively passes messages among neighboring labels update distributions locally. Unlike exact probabilistic inference algorithms, MFVI parallelized different positions sequence, achieving time complexity constant full parallelization. %Similar \citet{zheng2015conditional}, show algorithm unfolded RNN, Previous work showed algorithm unfolded RNN grid CRF structure. We expand work linear-chain CRF structure unfold algorithm RNN connected encoder form end-to-end neural network amenable parallelization training prediction. We call unfolded RNN approximate inference network . In addition linear-chain CRFs, also apply AIN factorized second-order CRF models, consider relations neighboring labels. Our empirical results show AIN significantly improves speed achieves competitive accuracy traditional CRF approach 4 tasks 15 datasets. This paper proposes state-of-the-art, end-to-end neural network event coreference resolution -- neural network, jointly models event detection event coreference, learns extract features raw text directly. A type-guided mechanism proposed resolving mention diversity problem long-distance coreference problem, which: 1) informs coreference prediction type scoring, 2) refines mention representation using type information, 3) guides decoding type consistency. Experiments show method achieves state-of-the-art performances KBP 2016 KBP 2017. For future work, focus bottleneck event coreference, e.g., event detection argument modeling."," %with pretrained word embeddings and contextual feature extractors such as RNN or CNN  The linear-chain Conditional Random Field  model is one of the most widely-used neural sequence labeling approaches. Exact probabilistic inference algorithms such as the forward-backward and Viterbi algorithms are typically applied in training and prediction stages of the CRF model. However, these algorithms require sequential computation that makes parallelization impossible. In this paper, we propose to employ a parallelizable approximate variational inference algorithm for the CRF model. Based on this algorithm, we design an approximate inference network that can be connected with the encoder of the neural CRF model to form an end-to-end network, which is amenable to parallelization for faster training and prediction. The empirical results show that our proposed approaches achieve a 12.7-fold improvement in decoding speed with long sentences and a competitive accuracy compared with the traditional CRF approach."
"Historically, metrics evaluating quality machine translation relied assessing similarity MT-generated hypothesis human-generated reference translation target language. Traditional metrics focused basic, lexical-level features counting number matching n-grams MT hypothesis reference translation. Metrics {\sc Bleu} {\sc Meteor} remain popular means evaluating MT systems due light-weight fast computation. Modern neural approaches MT result much higher quality translation often deviates monotonic lexical transfer languages. %A single reference translation might always sufficient accommodate expressiveness translations. For reason, become increasingly evident longer rely metrics {\sc Bleu} provide accurate estimate quality MT . While increased research interest neural methods training MT models systems resulted recent, dramatic improvement MT quality, MT evaluation fallen behind. The MT research community still relies largely outdated metrics new, widely-adopted standard emerged. In 2019, WMT News Translation Shared Task received total 153 MT system submissions . The Metrics Shared Task year saw 24 submissions, almost half entrants Quality Estimation Shared Task, adapted metrics . The findings above-mentioned task highlight two major challenges MT evaluation seek address herein . Namely, current metrics struggle accurately correlate human judgement segment level fail adequately differentiate highest performing MT systems. %The findings Metrics Shared Task highlight segment-level evaluation strong neural MT systems major challenges, none submitted metrics achieving satisfactory levels correlation human judgements . In paper, present {\sc Comet}\footnote{Crosslingual Optimized Metric Evaluation Translation.}, PyTorch-based framework training highly multilingual adaptable MT evaluation models function metrics. Our framework takes advantage recent breakthroughs cross-lingual language modeling generate prediction estimates human judgments Direct Assessments , Human-mediated Translation Edit Rate metrics compliant Multidimensional Quality Metric framework . Inspired recent work Quality Estimation demonstrated possible achieve high levels correlation human judgements even without reference translation , propose novel approach incorporating source-language input MT evaluation models. Traditionally QE models made use source input, whereas MT evaluation metrics rely instead reference translation. As , show using multilingual embedding space allows us leverage information three inputs demonstrate value added source input MT evaluation models. To illustrate effectiveness flexibility {\sc Comet} framework, train three models estimate different types human judgements show promising progress towards better correlation segment level robustness high-quality MT. %The rest paper organized follows. Section presents overview related literature. Section describes corpora used. Section describes different model architectures training regimes. Section describes conducted experiments evaluation metrics. Section reports corresponding results achieved. Finally, Section presents relevant conclusions, pinpoints possible future directions. We release {\sc Comet} framework trained MT evaluation models described paper research community upon publication. Future Work [TODO]} In paper, explore comprehensive document-level neural machine translation. Assuming document-level clues indeed helpful better translation, kept open problem finding good way effectively introduce helpful clues sentence-independent NMT. Taking document embedding default representation document-level clues, distinguish two types document embeddings, global local, targetedly capture general information whole document scope specific detailed information surrounding text. For concerned document-level NMT, first time survey multiple ways generating document embeddings conduct extensive experiments. Taking strong Transformer baseline, experimental results show global local document embeddings may effectively enhance baseline systems, showing sufficient richer document clues indeed greatly help standard sentence-independent NMT. In future work, apply context attention decoder investigate effect different memory sizes. TBD \clearpage"," We present {\sc Comet}, a neural framework for training multilingual machine translation evaluation models which obtains new state-of-the-art levels of correlation with human judgements. Our framework leverages recent breakthroughs in cross-lingual pretrained language modeling resulting in highly multilingual and adaptable MT evaluation models that exploit information from both the source input and a target-language reference translation in order to more accurately predict MT quality. To showcase our framework, we train three models with different types of human judgements: Direct Assessments, Human-mediated Translation Edit Rate and Multidimensional Quality Metrics. Our models achieve new state-of-the-art performance on the WMT 2019 Metrics shared task and demonstrate robustness to high-performing systems. %Furthermore, they show promising results towards solving the current challenges of accurate segment-level evaluation and robustness to top performing systems."
"Aspect detection, vital component aspect-based sentiment analysis , aims identifying predefined aspect categories discussed segments online reviews. Table shows example review television several different aspects, Image, Sound, Ease Use. With large number reviews, automatic aspect detection allows people efficiently retrieve review segments aspects interested in. It also benefits many downstream tasks, review summarization recommendation justification . } \end{table} There several research directions aspect detection. Supervised approaches leverage annotated labels aspect categories suffer domain adaptation problems . Another research direction consists unsupervised approaches gained lot attention recent years. Early unsupervised systems dominated Latent Dirichlet Allocation based topic models . However, several recent studies revealed LDA-based approaches perform well aspect detection extracted aspects poor quality . Compared LDA-based approaches, deep learning models, aspect-based autoencoder , shown excellent performance extracting coherent aspects identifying aspect categories review segments. However, models require human effort manually map model discovered aspects aspects interest, may lead inaccuracies mapping especially model discovered aspects noisy. Another research direction based weakly supervised approaches leverage small number aspect representative words fine-grained aspect detection . Although models outperform unsupervised approaches, make use human annotated data extract high-quality aspect seed words, may limit application. In addition, able automatically discover new aspects review corpus. We focus problem unsupervised aspect detection since massive amount reviews generated every day many newer products. It difficult humans efficiently capture new aspects manually annotate segments scale. Motivated ABAE, learn interpretable aspects mapping aspect embeddings word embedding space, aspects interpreted nearest words. To learn better representations aspects review segments, formulate UAD self-supervised representation learning problem solve using contrastive learning algorithm, inspired success self-supervised contrastive learning visual representations . In addition learning algorithm, also resolve two problems deteriorate performance ABAE, including self-attention mechanism segment representations aspect mapping strategy . Finally, discover quality aspect detection improved knowledge distillation . The contributions paper summarized follows: % } % % % \end{table} \def\year{2021}\relax %File: formatting-instructions-latex-2021.tex %release 2021.1 \documentclass[letterpaper]{article} % DO NOT CHANGE THIS \usepackage{aaai21} % DO NOT CHANGE THIS \usepackage{times} % DO NOT CHANGE THIS \usepackage{helvet} % DO NOT CHANGE THIS \usepackage{courier} % DO NOT CHANGE THIS \usepackage[hyphens]{url} % DO NOT CHANGE THIS \usepackage{graphicx} % DO NOT CHANGE THIS \urlstyle{rm} % DO NOT CHANGE THIS \def\UrlFont{\rm} % DO NOT CHANGE THIS \usepackage{natbib} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT \usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT \frenchspacing % DO NOT CHANGE THIS \setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS \setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS %new added start \usepackage{booktabs} \usepackage{footnote} \usepackage{amsmath,amssymb,mathrsfs} \usepackage[ruled,linesnumbered]{algorithm2e} \usepackage{epstopdf} \usepackage{multirow} \usepackage[skip=0pt]{subcaption} \usepackage{soul} \usepackage{tabularx} \renewcommand\tabularxcolumn[1]{m{#1}} \usepackage{enumitem} \renewcommand\vec[1]{\overrightarrow{#1}} \newcommand\cev[1]{\overleftarrow{#1}} \newcommand{\etal}{et~al.} \usepackage{microtype} \usepackage[switch]{lineno} %new added end %\nocopyright %PDF Info Is REQUIRED. % For /Author, add authors within parentheses, separated commas. No accents commands. % For /Title, add Title Mixed Case. No accents commands. Retain parentheses. \pdfinfo{ /Title /Author /TemplateVersion } %Leave % /Title % Put actual complete title within parentheses mixed case % Leave space \Title beginning parenthesis alone % /Author % Put actual complete list authors within parentheses mixed case. % Each author comma. If name contains accents, remove them. If LaTeX commands, % remove them. % DISALLOWED PACKAGES % \usepackage{authblk} -- This package specifically forbidden % \usepackage{balance} -- This package specifically forbidden % \usepackage{color % \usepackage{CJK} -- This package specifically forbidden % \usepackage{float} -- This package specifically forbidden % \usepackage{flushend} -- This package specifically forbidden % \usepackage{fontenc} -- This package specifically forbidden % \usepackage{fullpage} -- This package specifically forbidden % \usepackage{geometry} -- This package specifically forbidden % \usepackage{grffile} -- This package specifically forbidden % \usepackage{hyperref} -- This package specifically forbidden % \usepackage{navigator} -- This package specifically forbidden % % \indentfirst} -- This package specifically forbidden % \layout} -- This package specifically forbidden % \multicol} -- This package specifically forbidden % \nameref} -- This package specifically forbidden % \usepackage{savetrees} -- This package specifically forbidden % \usepackage{setspace} -- This package specifically forbidden % \usepackage{stfloats} -- This package specifically forbidden % \usepackage{tabu} -- This package specifically forbidden % \usepackage{titlesec} -- This package specifically forbidden % \usepackage{tocbibind} -- This package specifically forbidden % \usepackage{ulem} -- This package specifically forbidden % \usepackage{wrapfig} -- This package specifically forbidden % DISALLOWED COMMANDS % \nocopyright -- Your paper published use command % \addtolength -- This command may used % \balance -- This command may used % \baselinestretch -- Your paper published use command % \clearpage -- No page breaks kind may used final version paper % \columnsep -- This command may used % \newpage -- No page breaks kind may used final version paper % \pagebreak -- No page breaks kind may used final version paperr % \pagestyle -- This command may used % \tiny -- This acceptable font size. % {0} %May changed 1 2 section numbers desired. % The file aaai21.sty style file AAAI Press % proceedings, working notes, technical reports. % % Title % Your title must mixed case, sentence case. % That means verbs , % nouns, adverbs, adjectives capitalized, including words hyphenated terms, % articles, conjunctions, prepositions lower case unless % directly follow colon long dash \title{A Simple Effective Self-Supervised Contrastive Learning Framework\\ Aspect Detection} \author{ %Authors % All authors must font size format. Tian Shi\textsuperscript{\rm 1}, Liuqing Li\textsuperscript{\rm 2}, Ping Wang\textsuperscript{\rm 1}, Chandan K. Reddy\textsuperscript{\rm 1}\\ } \affiliations{ %Afiliations \textsuperscript{\rm 1}Department Computer Science, Virginia Tech\\ \textsuperscript{\rm 2}Verizon Media\\ tshi@vt.edu, liuqing.li@verizonmedia.com, ping@vt.edu, reddy@cs.vt.edu % See examples next } \iffalse %Example, Single Author, ->> remove \iffalse,\fi place surrounding AAAI title use \title{My Publication Title --- Single Author} \author { % Author Author Name \\ } \affiliations{ Affiliation \\ Affiliation Line 2 \\ name@example.com } \fi \iffalse %Example, Multiple Authors, ->> remove \iffalse,\fi place surrounding AAAI title use \title{My Publication Title --- Multiple Authors} \author { % Authors First Author Name,\textsuperscript{\rm 1} Second Author Name, \textsuperscript{\rm 2} Third Author Name \textsuperscript{\rm 1} \\ } \affiliations { % Affiliations \textsuperscript{\rm 1} Affiliation 1 \\ \textsuperscript{\rm 2} Affiliation 2 \\ firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com } \fi In paper present {\sc Comet}, novel neural framework training MT evaluation models serve automatic metrics easily adapted optimized different types human judgements MT quality. To showcase effectiveness framework, sought address challenges reported 2019 WMT Metrics Shared Task . We trained three distinct models achieve new state-of-the-art results segment-level correlation human judgments, show promising ability better differentiate high-performing systems. One challenges leveraging power pretrained models burdensome weight parameters inference time. A primary avenue future work {\sc Comet} look impact compact solutions DistilBERT . Additionally, whilst outline potential importance source text above, note {\sc Comet-rank} model weighs source reference differently inference equally training loss function. Future work investigate optimality formulation examine interdependence different inputs. \section*{Acknowledgments} We grateful Andr\'e Martins, Austin Matthews, Fabio Kepler, Daan Van Stigt, Miguel Vera, reviewers, valuable feedback discussions. This work supported part P2020 Program projects MAIA Unbabel4EU, supervised ANI contract numbers 045909 042671, respectively. \clearpage \section{Appendices} \begin{table*}[!ht] \centering"," Unsupervised aspect detection  aims at automatically extracting interpretable aspects and identifying aspect-specific segments  from online reviews. However, recent deep learning based topic models, specifically aspect-based autoencoder, suffer from several problems such as extracting noisy aspects and poorly mapping aspects discovered by models to the aspects of interest. To tackle these challenges, in this paper, we first propose a self-supervised contrastive learning framework and an attention-based model equipped with a novel smooth self-attention  module for the UAD task in order to learn better representations for aspects and review segments. Secondly, we introduce a high-resolution selective mapping  method to efficiently assign aspects discovered by the model to the aspects of interest. We also propose using a knowledge distillation technique to further improve the aspect detection performance. Our methods outperform several recent unsupervised and weakly supervised approaches on publicly available benchmark user review datasets. Aspect interpretation results show that extracted aspects are meaningful, have a good coverage, and can be easily mapped to aspects of interest. Ablation studies and attention weight visualization also demonstrate effectiveness of SSA and the knowledge distillation method."
"There several recent studies aim predict aspect ratings using deep neural network based models multi-task learning framework . In setting, rating predictions different aspects, typically highly correlated share review encoder, treated different tasks. However, models rely hand-crafted aspect keywords aid rating/sentiment predictions . Thus, results, especially case studies reviews, biased towards pre-defined aspect keywords. In addition, models focus improving prediction accuracy, however, knowledge discovery review corpus still relies unsupervised rule-based methods , limits applications current MARP models . In past years, model uncertainty deep neural network classifiers received increasing attention , identify low-confidence regions input space give reliable predictions. Uncertainty models also applied deep neural networks text classification . However, existing uncertainty methods used improve overall prediction accuracy multi-task learning models crowd-sourcing annotation involved MARP task. In paper, attempt tackle mentioned issues. The primary contributions paper follows: The rest paper organized follows: In Section , introduce related work MARP task uncertainty estimation methods. In Section , present details proposed FEDAR model, AKR method LEAD uncertainty estimation approach. In Section , introduce different MARP datasets, baseline methods implementation details, well analyze experimental results. Our discussion concludes Section.%% %% This file `sample-sigconf.tex', %% generated docstrip utility. %% %% The original source files were: %% %% samples.dtx %% %% IMPORTANT NOTICE: %% %% For copyright see source file. %% %% Any modified versions file must renamed %% new filenames distinct sample-sigconf.tex. %% %% For distribution original source see terms %% copying modification file samples.dtx. %% %% This generated file may distributed long %% original source files, listed above, part %% distribution. %% %% The first command LaTeX source must \documentclass command. \documentclass[sigconf]{acmart} \usepackage{amsmath,amssymb,multicol,mathrsfs} \usepackage[ruled,linesnumbered]{algorithm2e} \usepackage{graphicx} \usepackage{balance} \usepackage{epstopdf} \usepackage{multirow} \usepackage{color,soul} \usepackage{tabularx} \renewcommand\tabularxcolumn[1]{m{#1}} \usepackage[normalem]{ulem} \usepackage{enumitem} \setlist{leftmargin=5.5mm} \usepackage{flushend} \usepackage{tikz} \usepackage{pgf} \usepackage[eulergreek]{sansmath} \usepackage{graphicx} \usepackage{subcaption} \renewcommand\vec[1]{\overrightarrow{#1}} \newcommand\cev[1]{\overleftarrow{#1}} \newcommand{\etal}{et~al. } \usepackage{url} \newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}} %%%% As March 2017, [siggraph] longer used. Please use sigconf SIGGRAPH conferences. %%%% As May 2020, [sigchi] [sigchi-a] longer used. Please use sigconf SIGCHI conferences. %%%% Proceedings format SIGPLAN conferences % \documentclass[sigplan, anonymous, review]{acmart} %%%% Proceedings format conferences using one-column small layout % \documentclass[acmsmall,review]{acmart} %% %% \BibTeX command typeset BibTeX logo docs \AtBeginDocument{% \providecommand\BibTeX{{% \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}} %% Rights management information. This information sent %% complete rights form. These commands SAMPLE %% values them; responsibility author replace %% commands values provided %% complete rights form. \setcopyright{acmcopyright} \copyrightyear{2018} \acmYear{2018} \acmDOI{10.1145/1122445.1122456} %% These commands PROCEEDINGS abstract paper. \acmConference[]{}{}{} \acmBooktitle{} \acmPrice{15.00} \acmISBN{978-1-4503-XXXX-X/18/06} %% %% Submission ID. %% Use submitting article sponsored event. You'll %% receive unique submission ID organizers %% event, ID used parameter command. %%\acmSubmissionID{123-A56-BU3} %% %% The majority ACM publications use numbered citations %% references. The command \citestyle{authoryear} switches %% ""author year"" style. %% %% If preparing content event %% sponsored ACM SIGGRAPH, must use ""author year"" style %% citations references. %% Uncommenting %% next command enable style. %%\citestyle{acmauthoryear} %% %% end preamble, start body document source. %% %% The code generated tool http://dl.acm.org/ccs.cfm. %% Please copy paste code instead example below. %% \ccsdesc[500]{Information systems~Sentiment analysis} \ccsdesc[500]{Information systems~Clustering classification} \ccsdesc[300]{Information systems~Information extraction} %% %% Keywords. The author pick words accurately describe %% work presented. Separate keywords commas. \keywords{Multi-task learning, model uncertainty, deep neural network, dropout, classification, online reviews} %% A ""teaser"" image appears author affiliation %% information body document, typically spans %% page. % %% %% This command processes author affiliation title %% information builds first part formatted document. %% %% The acknowledgments section defined using ""acks"" environment %% . This ensures proper %% identification section article metadata, %% consistent spelling heading. % \newpage \balance %% %% The next two lines define bibliography style used, %% bibliography file. \bibliographystyle{ACM-Reference-Format} \bibliography{ref} \end{document} \endinput %% %% End file `sample-sigconf.tex'. In paper, propose self-supervised contrastive learning framework aspect detection. Our model equipped two attention modules, allows us represent every segment word embeddings aspect embeddings, map aspect embeddings word embedding space contrastive learning mechanism. In attention module word embeddings, introduce SSA mechanism. Thus, model learn robust representations, since SSA encourages model capture phrases multiple keywords segments. In addition, propose HRSMap method aspect mapping, dramatically increases accuracy segment aspect predictions ABAE model. Finally, improve performance aspect detection knowledge distillation. BERT-based student models benefit pretrained encoders overcome disadvantages data preprocessing teacher model. During training, introduce entropy filters loss function ensure student models focus high confidence training samples. Our models shown better performance compared several recent unsupervised weakly-supervised models several publicly available review datasets across different domains. Aspect interpretation results show extracted aspects meaningful, good coverage, easily mapped gold-standard aspects. Ablation studies visualization attention weights demonstrate effectiveness SSA entropy filters.","  In recent years, several online platforms have seen a rapid increase in the number of review systems that request users to provide aspect-level feedback. Multi-Aspect Rating Prediction , where the goal is to predict the ratings from a review at an individual aspect level, has become a challenging and an imminent problem. To tackle this challenge, we propose a deliberate self-attention deep neural network model, named as FEDAR, for the MARP problem, which can achieve competitive performance while also being able to interpret the predictions made. As opposed to the previous studies, which make use of hand-crafted keywords to determine aspects in sentiment predictions, our model does not suffer from human bias issues since aspect keywords are automatically detected through a self-attention mechanism. FEDAR is equipped with a highway word embedding layer to transfer knowledge from pre-trained word embeddings, an RNN encoder layer with output features enriched by pooling and factorization techniques, and a deliberate self-attention layer. In addition, we also propose an Attention-driven Keywords Ranking  method, which can automatically extract aspect-level sentiment-related keywords from the review corpus based on the attention weights. Since crowdsourcing annotation can be an alternate way to recover missing ratings of reviews, we propose a LEcture-AuDience  strategy to estimate model uncertainty in the context of multi-task learning, so that valuable human resources can focus on the most uncertain predictions. Our extensive set of experiments on different DMSC datasets demonstrate the superiority of the proposed FEDAR and LEAD models. Visualization of aspect-level sentiment keywords demonstrate the interpretability of our model and effectiveness of our AKR method."
"% However, gap machine translation systems human translators needs manually closed post-editing. % The recent success deep learning algorithms heavily relies increasing availability crowdsourcing services either data annotations human evaluations, ImageNet dataset Amazon Mechanical Turk. % Through power crowd, data requesters expect obtain large amounts data relatively low cost. % However, growing demand quality data requires technical expertise. % For example, cornerstone multilingual researches natural language processing typically lies multilingual paralleled corpus. % Post-editing crowdsourcing efficient way produce high-quality translations. % However, de facto demanding human intelligence generally requires crowdsourcing participants educated certain ability language translation, consequently leads substantial increase spent expense elapsed time. The explosive advances sequence sequence model enable deep learning based neural machine translation approximate even achieve human parity specific language pairs scenarios. Instead translating scratch human translators, new translation paradigm emerged: computer assisted translation system, includes machine translation human post-editing. The post-editing process whereby humans amend machine-generated translations achieve acceptable final product. %, necessarily reference generated another human translator. Practically, estimated average translation time reduced 17.4\% . However, utilizing NMT poses two key challenges. First, neural machine translation quality still continues vary great deal across different domains genres, less proportion availability paralleled training corpora. % Many experiments show large scale in-domain data boost performance NMT . Second, zero tolerance policy common choice vast majority important applications. For example, business legal documents translated, even single incorrect word could bring serious financial property losses. Therefore, subsequent human post-editing indispensable situations like this. Unfortunately, NMT systems saves time providing preliminary translations, time spent error corrections humans remains substantial extent offsets efficiency gained NMT systems. In paper, explore automatic post-editing deep learning framework. Specifically, adopt imitation learning approach, model first screens translation candidates quality prediction decides whether post edit generation atomic operation method. % Figure demonstrates example system atomic operation post-editing, quality estimation step illustrates errors found original machine translation APE step proceeds proposed corrections. % The benefits crowdsourcing system faster error detection automatic QE system faster error correction automatic correction suggestion system. % In pilot system one shown Figure, observe improved human efficiency aid automatic systems especially systems produce top-quality error corrections, requiring actions human. Starting wide range features used CAT system, carefully analyze human post-editing results narrow framework design three key modules: quality estimation , generative post-editing atomic operation post-editing. These modules tightly integrated transformer neural networks . Our main innovation % adaptive crowdsourcing system two modular post-editing algorithms either independently conditionally used. hierarchical model two modular post-editing algorithms conditionally used based novel fine-grained quality estimation model. % In iteration f For machine translation, %the system model i) runs QE model predict detailed token level errors, summarized overall quality score decide whether machine translation quality high not, ii) conditional previous decision, employs atomic operation post-editing algorithm high quality sentence generative model rephrase translation low one. We examine approach public English--German dataset WMT 2017 APE shared task. Our system outperforms top ranked methods BLEU TER metrics. In addition, following standard human evaluation process aimed achieving impartiality respect efficiency CAT system, ask several certified translators edit machine translation outputs without APE assistance. Evaluation results show system significantly improves translators' efficiency. In paper, proposed multi-task deep learning model, namely FEDAR, problem multi-aspect review rating prediction. Different previous studies, model require hand-crafted aspect-specific keywords guide attention boost model performance task rating prediction. Instead, model relies highway word embedding layer transfer knowledge pre-trained word vectors large corpus, sequential encoder layer whose output features enriched pooling feature factorization techniques, deliberate self-attention layer maintains interpretability model. Experiments various MARP datasets demonstrated superior performance model. In addition, also developed Attention-driven Keywords Ranking method, automatically extract aspect sentiment keywords review corpus based attention weights. Aspect-level sentiment word-cloud visualization results demonstrated interpretability model effectiveness AKR method. Finally, also proposed LEcture-AuDience method measure uncertainty deep neural networks, including FEDAR model, context multi-task learning. Our experimental results multiple real-world datasets demonstrate effectiveness proposed work."," % Text translation is a difficult and expensive task in crowdsourcing, since it requires the expertise of at least two languages.  With the advent of neural machine translation, there has been a marked shift towards leveraging and consuming the machine translation results.  However, the gap between machine translation systems and human translators needs to be manually closed by post-editing.  In this paper, we propose an end-to-end deep learning framework of %as a computer assisted crowdsourcing system for  the quality estimation and automatic post-editing of the machine translation output.  Our goal is to provide error correction suggestions and to further relieve the burden of human translators through an interpretable model.  To imitate the behavior of human translators, we design three efficient delegation modules -- quality estimation, generative post-editing, and atomic operation post-editing and construct a hierarchical model based on them.  %When the quality estimation model predicts the translation to be poor, the generative post-editing module is called to completely rephrase the translation.  % In contrast, the translation quality is high, the output only needs several atomic operations, such as deletion, insertion, or substitution.  We examine this approach with the English--German dataset from WMT 2017 APE shared task and our experimental results can achieve the state-of-the-art performance.  We also verify that the certified translators can significantly expedite their post-editing processing with our model in human evaluation."
"Recent advances deep learning led significant improvement Neural Machine Translation . Particularly, performance sentence-level translation low- high- resource language pairs dramatically improved . However, translating text long-range dependencies, conversations documents, original mode translating one sentence time ignores discourse phenomena , introducing undesirable behaviors inconsistent pronouns across different translated sentences. Document-level NMT, realistic translation task scenarios, systematically investigated machine translation community. Most literatures focused looking back fixed number previous source target sentences document-level context . Some latest works innovatively attempted either get entire document context dynamically select suitable context . Because scarcity document training data, benefit gained approach, reflected BLEU, usually limited. We therefore elect pay attention context previous sentences small number usually cover entire document. Almost latest studies chose standard transformer model baseline translates sentence document model trained sentence-level data. The cohesion consistency general poor. A reasonable baseline train transformer context prepended, modification could simply implemented via data preprocessing. \citet{bawden2018evaluating} conducted detailed analysis RNN-based NMT models topic whether include extended context. Consistency precision often viewed trade-off other. We conduct detailed analysis effect document context consistency transformer architecture accepting multi-sentence input. When comes leveraging contextual information, common approach model interaction sentence context specially designed attention modules . Such works tend include one encoder decoder, substantial number parameters additional computations. In work, reduce contextual regular attention modules one single encoder decoder. Our idea motivated one transformer decoder two-stream self-attention . % In particular, maintain two different sets hidden states employ two different masking matrices capture long short term dependencies. The contributions paper threefold: i) extensively research performance standard transformer setting multi-sentence input output; ii) propose simple effective modification adapting transformer document NMT aim ameliorating effect error accumulation; iii) experiments demonstrate even simple baseline achieve comparable results. Our studies show offer efficient computer assisted translation crowdsourcing system reduce time cost human labor. An interesting point mention difference training inference algorithms. The latter exclusively enjoys benefits iterative update former not. So natural question whether iterative update QE atomic operation APE successfully plugged training process. The short answer yes. Because simplest way replace current inner loop model specialization Algorithm similar iterative update used inference, rationales data augmentation. Particularly, model parameters encoder-memory encoder updated via loss minimization, infer current atomic operation APE assign machine translation, {\itshape i.e.} substituting new tuple Line 4 Algorithm. We implemented idea found results became worse. We hypothesize probably original model specialization bring diversity errors machine translation models learning. Therefore, theoretically reasonable modification combine original model specialization iterative update together, computationally feasible training cost significantly increase several folds. It might argued final hierarchical system, automatically selects suitable model inference, special case conventional two-model ensemble, not. The standard ensemble method requires inference trained models, combines predicted distribution token. In contrast, hierarchical model need infer selected model once. If for-loop exits early Algorithm, generative model applied. If iterative update desired, atomic operation model runs 5 iterations, usually faster generative model. This difference hierarchical APE system standard ensemble multiple single models makes system faster. But that, motivation propose hierarchical model better model editing behaviors human translators. The decision making process allows discriminatory APE machine translation outputs vary qualities."," Many document-level neural machine translation  systems have explored the utility of context-aware architecture, usually requiring an increasing number of parameters and computational complexity.  However, few attention is paid to the baseline model.  In this paper, we research extensively the pros and cons of the standard transformer in document-level translation, and find that the auto-regressive property can simultaneously bring both the advantage of the consistency and the disadvantage of error accumulation.  Therefore, we propose a surprisingly simple long-short term masking self-attention on top of the standard transformer to both effectively capture the long-range dependence and reduce the propagation of errors.  We examine our approach on the two publicly available document-level datasets.  We can achieve a strong result in BLEU and capture discourse phenomena."
"Knowledge Distillation popular model acceleration compression approach . It assumes lightweight network learn generalize way large network . To end, simple method train student network predicted probabilities teacher network targets. In KD, student network ``copycat'' teacher network knowledge learned teacher prediction. Rather, straightforward way transfer knowledge parameters two networks, parameters sources predictions. Such idea recently found effective pre-training fine-tuning paradigm . For example, parameters learned large-scale unlabeled data used good start train complex network target task. However, parameter reuse applicable model acceleration compression teacher student networks might different width depth\footnote{In multi-layer neural network, number neurons hidden layer referred network width, number stacked layers referred network depth.}. In paper, propose Weight Distillation transfer parameters teacher network student network. We design parameter generator model transformation teacher network parameters student network parameters, even different sized weight matrices. After that, fine-tuning process performed improve quality transferred parameters. See \fig{fig:compare} comparison KD WD. We test WD method well-tuned Transformer-based machine translation system. The experiments run three machine translation tasks, including WMT16 English-Roman , NIST12 Chinese-English , WMT14 English-German . With similar speedup, student network trained WD 0.511.82 BLEU higher KD. With similar BLEU performance, student network trained WD 1.111.39 faster KD. More interestingly, found WD effective improve student network model size close teacher network. On WMT14 En-De data, WD-based system establishes new state-of-the-art 1.88 faster big teacher network. %% predictions \node[teacher prob,minimum height=0.6cm,anchor=south] {}; \node[teacher prob,minimum height=1cm,anchor=south east] {}; \node[teacher prob,minimum height=0.8cm,anchor=south east] {}; \node[teacher prob,minimum height=0.4cm,anchor=south west] {}; \node[teacher prob,minimum height=0.2cm,anchor=south west] {}; % Student \node[student weight,anchor=west] {}; \node[font=\small,anchor=south,inner sep=0pt] {}; \node[student weight,anchor=south] {}; %% predictions \node[student prob,minimum height=0.4cm,anchor=south] {}; \node[student prob,minimum height=0.8cm,anchor=south east] {}; \node[student prob,minimum height=0.6cm,anchor=south east] {}; \node[student prob,minimum height=0.1cm,anchor=south west] {}; \node[student prob,minimum height=0.3cm,anchor=south west] {}; %% ground truth \node[ground truth prob,minimum height=0.1cm,anchor=south] {}; \node[ground truth prob,minimum height=1cm,anchor=south east] {}; \node[ground truth prob,minimum height=0.1cm,anchor=south east] {}; \node[ground truth prob,minimum height=0.1cm,anchor=south west] {}; \node[ground truth prob,minimum height=0.1cm,anchor=south west] {}; % Connections \draw[-latex',red] .. controls + + .. ; \draw[-latex',red] ; \draw[-latex,densely dashed] ; \draw[-latex,densely dashed] ; \draw[-latex,densely dashed] ; \draw[-latex,densely dashed] ; \draw[-latex,densely dashed] ; \draw[-latex,densely dashed] ; \end{tikzpicture} } \hfill \subfigure[Weight Distillation] { %% predictions \node[teacher prob,minimum height=0.6cm,anchor=south] {}; \node[teacher prob,minimum height=1cm,anchor=south east] {}; \node[teacher prob,minimum height=0.8cm,anchor=south east] {}; \node[teacher prob,minimum height=0.4cm,anchor=south west] {}; \node[teacher prob,minimum height=0.2cm,anchor=south west] {}; % Student \node[student weight,anchor=west] {}; \node[font=\small,anchor=south,inner sep=0pt] {}; \node[student weight,anchor=south] {}; %% predictions \node[student prob,minimum height=0.1cm,anchor=south] {}; \node[student prob,minimum height=0.8cm,anchor=south east] {}; \node[student prob,minimum height=0.1cm,anchor=south east] {}; \node[student prob,minimum height=0.3cm,anchor=south west] {}; \node[student prob,minimum height=0.1cm,anchor=south west] {}; %% ground truth \node[ground truth prob,minimum height=0.1cm,anchor=south] {}; \node[ground truth prob,minimum height=1cm,anchor=south east] {}; \node[ground truth prob,minimum height=0.1cm,anchor=south east] {}; \node[ground truth prob,minimum height=0.1cm,anchor=south west] {}; \node[ground truth prob,minimum height=0.1cm,anchor=south west] {}; % Parameter Generator \coordinate ; \node[pgnode] }]pgmid) {}; \node[pgnode] }]pgmid) {}; \node[pgnode] }]pgmid) {}; \node[pgnode] }]pgmid) {}; \draw[-latex,lyyblue] ; \draw[-latex,lyyblue] ; \draw[-latex,lyyblue] ; \draw[-latex,lyyblue] ; % Connections \draw[-latex',red] .. controls + + .. ; \draw[-latex',red] ; \draw[-latex',red] .. controls + + .. ; \draw[-latex',red] .. controls + + .. ; \draw[-latex',red] .. controls + + .. ; \draw[-latex',red] .. controls + + .. ; \draw[-latex',red] .. controls + + .. ; \draw[-latex,densely dashed] ; \draw[-latex,densely dashed] ; \draw[-latex,densely dashed] ; \draw[-latex,densely dashed] ; \draw[-latex,densely dashed] ; \draw[-latex,densely dashed] ; \end{tikzpicture} } \hspace*{\fill} \end{figure*} We propose multi-level contrastive learning paradigm exploit fine-grained response quality calibrate training response generation models. We design Rank-aware Calibration network construct contrastive optimization objectives. We build Knowledge Inference component capture keyword knowledge reference training exploit information encourage generation informative words. We evaluate proposed model carefully annotated short-text conversation dataset results suggest model generate relevant diverse responses compared baseline models.","   Knowledge distillation has been proven to be effective in model acceleration and compression. It allows a small network to learn to generalize in the same way as a large network. Recent successes in pre-training suggest the effectiveness of transferring model parameters. Inspired by this, we investigate methods of model acceleration and compression in another line of research. We propose Weight Distillation to transfer the knowledge in the large network parameters through a parameter generator. Our experiments on WMT16 En-Ro, NIST12 Zh-En, and WMT14 En-De machine translation tasks show that weight distillation can train a small network that is 1.88$\sim$2.94$\times$ faster than the large network but with competitive performance. With the same sized small network, weight distillation can outperform knowledge distillation by 0.51$\sim$1.82 BLEU points."
"% ============================================================================== The CLEVR dataset modern 3D incarnation historically significant shapes-based datasets like SHRDLU , used demonstrating AI efficacy language understanding . Although originally aimed visual question answering problem , versatility seen use diverse ML domains, including extensions physics simulation engines language augmented hierarchical reinforcement learning causal reasoning . Parallelly, research interest geometric learning GNN based techniques seen dramatic surge recent deep learning zeitgeist. In focused paper, present library allows easy integration application geometric representation learning CLEVR dataset tasks - enabling NLP research community apply GNN based techniques research . The library three main components: 1. Parser: allows extraction graph structured relationships among objects environment -- textual questions, semantic image scene graphs, 2. Embedder: allows generation latent embeddings using models desired backend choice , 3. Visualizer: provides tools visualizing structural graphs latent embeddings. % %Thus, release library, hope enable greater adoption geometric learning NLP community, lowering initial learning curve and/or rapid prototyping integration GNNs NLP research domains like language grounded RL, visual reasoning, language compositionality etc. . % ============================================================================== In work, propose weight distillation transfer knowledge parameters teacher network student network. It generates student network teacher network via parameter generator. Our experiments show weight distillation consistently outperforms knowledge distillation producing faster better student network three machine translation tasks."," The CLEVR dataset has been used extensively in language grounded visual reasoning in  Machine Learning  and Natural Language Processing  domains. We present a graph parser library for CLEVR, that provides functionalities for object-centric attributes and relationships extraction, and construction of structural graph representations for dual modalities. Structural order-invariant representations enable geometric learning and can aid in downstream tasks like language grounding to vision, robotics, compositionality, interpretability, and computational grammar construction. We provide three extensible main components -- parser, embedder, and visualizer that can be tailored to suit specific learning setups. We also provide out-of-the-box functionality for seamless integration with popular deep graph neural network  libraries. Additionally, we discuss downstream usage and applications of the library, and how it accelerates research for the NLP research community\footnote{Code is available at - \url{https://github.com/raeidsaqur/clevr-parser}}."
"Deep neural networks proved vulnerable adversarial attacks, maliciously craft adversarial examples fool victim model . For instance, highly poisonous phrases minor modification easily deceive Google's toxic comment detection system . With broad use DNN-based natural language processing systems, spam filtering malware detection , growing concern security. As result, research textual adversarial attacking becomes increasingly important. %by perturbing original input In recent years plenty adversarial attack models proposed . Nevertheless, work satisfactorily real-world attack situations. Existing adversarial attack models roughly classified four categories according accessibility victim model: gradient-based, score-based, decision-based blind models. First, gradient-based models, also known white-box models, require full knowledge victim model perform gradient computation . % attack models work white-box setting , full knowledge victim model required gradient computation. Unfortunately, hardly know architecture victim model real-world attack situations, let alone compute gradients. Second, blind models need know anything victim model, attack performance usually good enough, precisely complete ignorance victim model. Specifically, existing blind models either implement character-level random perturbations conduct sentence-level distracting paraphrasing . However, character-level attacks easy repulse , sentence-level attacks cannot guarantee attack validity, i.e, keeping ground-truth label adversarial example original input. More importantly, attack success rates blind models unsatisfactory. % adversarial example quality, including grammaticality language naturality. % % inclined craft invalid adversarial examples, different ground-truth labels original input, Finally, score- decision-based attack models seem suitable real-world adversarial attack situations. They need know output victim models -- former requires prediction scores latter needs final prediction decision. % normally practicable real-world adversarial attacking situations % Attack models two kinds models seem suitable real-world situation adversarial attacking, usually able invoke victim model obtain output. Existing score- decision-based attack models achieved great attack performance , significant problem. To craft adversarial example, models iteratively make perturbations query victim model many times, e.g., recent score-based model needs query victim model times average generate adversarial example . % They utilize victim model output guidance iteratively conduct perturbations finding adversarial example . % PWWSteratively % Although achieving good attacking performance, models usually need invoke victim model many times, e.g., attack model \citet{zang2020word} needs invoke victim model times average attack one instance. It neither efficient practical invoke victim model many times real-world situations adversarial attacking. We argue low efficiency existing score- decision-based attack models results learning ability simply follow certain fixed optimization rules attack, e.g., greedy algorithm , genetic algorithm particle swarm optimization . % model -> score- decision-based models? % For instance, start attack scratch. % And lessons learned previous attacks. %For example, \citet{zang2020word} ? To solve problem, propose build attack model possessing learning ability, learn lessons attack history store parameters improve attack efficiency. % learn weak sides data victim model% data? % history launch deadly attacks efficiently. Considering labeled data available adversarial attacking, design model following reinforcement learning paradigm. There two main operations model, including identifying key words original sentences crucially influence decision victim model, selecting appropriate substitutes replace them. Our model aimed learning optimal policy series substitution operations iteratively conducted generate adversarial examples. % The prober aimed locating vulnerable sentence, i.e., word sentence easiest attack. % The attacker supposed find fatal attack, i.e., word replace vulnerable word original input. %Our attack model highly adaptable combined different word substitution methods. In experiments, evaluate attack model benchmark datasets three typical NLP tasks including sentiment analysis, text classification natural language inference. The victim models respective state-of-the-art models datasets, namely ALBERT , XLNet RoBERTa , two open APIs. Since model work score- decision-based attack settings, carry experiments two settings. Experimental results show attack model consistently outperforms baseline methods datasets terms attack success rate attack efficiency. % within whatever limit number victim model query times. We also find model bring robustness improvement victim model adversarial training. % conduct quantitative analyses exhibit learning ability model. %  % scoreecision based % & In work, proposed improve aggressive language detection jointly performing text normalization , via adversarial multi-task learning framework. The private encoders ALD TN focused task-specific feature retrieving, respectively, shared encoder learned underlying common features two tasks. During adversarial training, task discriminator distinguished separate learning ALD TN. Experimental results four ALD datasets showed model outperformed baselines large margins differing settings, demonstrating necessity joint learning TN ALD."," Adversarial attacking aims to fool deep neural networks with adversarial examples. In the field of natural language processing, various textual adversarial attack models have been proposed, varying in the accessibility to the victim model. Among them, the attack models that only require the output of the victim model are more fit for real-world situations of adversarial attacking. However, to achieve high attack performance, these models usually need to query the victim model too many times, which is neither efficient nor viable in practice. To tackle this problem, we propose a reinforcement learning based attack model, which can learn from attack history and launch attacks more efficiently. In experiments, we evaluate our model by attacking several state-of-the-art models on the benchmark datasets of multiple tasks including sentiment analysis, text classification and natural language inference. Experimental results demonstrate that our model consistently achieves both better attack performance and higher efficiency than recently proposed baseline methods. We also find our attack model can bring more robustness improvement to the victim model by adversarial training. All the code and data of this paper will be made public."
"In natural languages, lexical items often used multiple word classes without overt changes word form. For instance, word buru Mundari used noun denote `mountain', verb denote `to heap up' . Known word class flexibility, phenomenon considered one challenging topics linguistic typology . We present computational methodology quantify regularity word class flexibility across languages. There extensive literature languages vary word class flexibility, either directly related notions word class conversion . However, existing studies tend rely analyses small sets lexical items may representative word class flexibility broad lexicon. Critically lacking systematic analyses word class flexibility across many languages, existing typological studies focused qualitative comparisons word class systems. We take knowledge first step towards computational quantification word class flexibility \NumLanguages languages, taken Universal Dependencies project . We focus lexical items used nouns verbs, i.e., noun-verb flexibility. This choice motivated fact distinction nouns verbs stable word class systems across languages: language makes distinction word classes all, likely distinction nouns verbs . However, understanding cross-linguistic regularity noun-verb flexibility impoverished. We operationalize word class flexibility property lemmas. We define lemma flexible occurrences tagged nouns others verbs. Flexible lemmas sorted noun dominant lemmas, occur frequently nouns, verb dominant lemmas occur frequently verbs. Our methodology builds contextualized word embedding models quantify semantic shift grammatical classes lemma, within single language. This methodology also help quantify metrics flexibility lexicon across languages. We use methodology address one fundamental questions study word class flexibility: phenomenon analyzed directional word-formation process similar derivation, form underspecification? Derived words commonly argued lower frequency use narrower range meaning compared base . If word class flexibility directional process, expect flexible lemmas subject semantic variation dominant word class less frequent class. We also test claim noun-to-verb flexibility involves semantic shift verb-to-noun flexibility. While previous work explored questions, remains challenging quantify semantic shift semantic variation, particularly across different languages. We present novel probing task reveals ability deep contextualized models capture semantic information across word classes. Our utilization deep contextual models predicts human judgment spectrum noun-verb flexible usages including homonymy , polysemy , word class flexibility. We find BERT outperforms ELMo non-contextual word embeddings, upper layers BERT capture semantic information, resonates existing probing studies . In paper, propose reinforcement learning-based textual adversarial attack model aimed real-world adversarial attack situations. It work score- decision-based attack settings possesses learning ability launch attacks efficiently. We also find model bring robustness improvement victim model adversarial training compared existing baselines. In future, work towards enhancing attack efficiency improving attack performance situation extremely limited victim model queries. In addition, explore make model robust adversarial training methods."," Word class flexibility refers to the phenomenon whereby a single word form is used across different grammatical categories. Extensive work in linguistic typology has sought to characterize word class flexibility across languages, but quantifying this phenomenon accurately and at scale has been fraught with difficulties. We propose a principled methodology to explore regularity in word class flexibility. Our method builds on recent work in contextualized word embeddings to quantify semantic shift between word classes , and we apply this method to \NumLanguages languages\footnote{Code and data to reproduce the experimental findings are available at: \url{https://github.com/SPOClab-ca/word-class-flexibility}.}. We find that contextualized embeddings not only capture human judgment of  class variation within words in English, but also uncover shared tendencies in class flexibility across languages. Specifically, we find greater semantic variation when flexible lemmas are used in their dominant word class, supporting the view that word class flexibility is a directional process. Our work highlights the utility of deep contextualized models in linguistic typology."
"Coreference resolution task identifying mentions document co-refer entity. It important task facilitating many applications question answering text summarization. \citet{lee-etal-2017-end} proposed first neural end-to-end architecture coreference resolution. Most recent state-of-the-art systems use backbone utilizing better scoring functions, pruning procedures, pre-trained token representations. Despite usage, knowledge, in-depth analysis done better understand inner workings influential system. This understanding important: example, \citet{kummerfeld-klein-2013-error}'s analysis then-best classical coreference systems inspired many important follow-up works . However, unknown observations classical feature-based often highly pipelined systems extend current end-to-end models. In paper, empirically analyze best instantiation model family, SpanBERT + c2f-coref, investigating interaction two components: mention detector mention linker. Specifically, study errors independently jointly affect final clustering. Using CoNLL-2012 PreCo datasets, highlight low-precision, high-recall nature detector. While traditionally recall emphasized detector design decision , show huge degradation noisy mentions that, perhaps surprisingly, increasing number candidates considered baseline linker deteriorates performance. While classical coreference pipelines focused detector precision, rarely emphasized modern end-to-end systems. We hence stress importance precision-recall balance detector demonstrate pruning hyperparameters, addition reducing computational complexity, help control trade-off. However, show difficulty obtaining high-precision detector demonstrating importance anaphoricity decisions inability detector make decisions. Finally, highlight high potential linker remaining errors besides anaphoricity decisions mainly involve pronoun resolution. We hope findings shed light internal mechanism mainstream coreference system lay empirical foundation future research. We use contextual language models examine shared tendencies word class flexibility across languages. We find majority class often exhibits semantic variation minority class, supporting view word class flexibility directional process. We also find English, noun-to-verb flexibility associated semantic shift verb-to-noun flexibility, case languages. Our probing task reveals upper layers BERT contextual embeddings best reflect human judgment semantic similarity. We obtain similar results different datasets language models English support robustness method. This work demonstrates utility deep contextualized models linguistic typology, especially characterizing cross-linguistic semantic phenomena otherwise difficult quantify.","  Coreference resolution is an important task for discourse-level natural language understanding. However, despite significant recent progress, the quality of current state-of-the-art systems still considerably trails behind human-level performance. Using the CoNLL-2012 and PreCo datasets, we dissect the best instantiation of the mainstream end-to-end coreference resolution model that underlies most current best-performing coreference systems, and empirically analyze the behavior of its two components: the mention detector and mention linker. While the detector traditionally focuses heavily on recall as a design decision, we demonstrate the importance of precision, calling for their balance. However, we point out the difficulty in building a precise detector due to its inability to make important anaphoricity decisions. We also highlight the enormous room for improving the linker and that the rest of its errors mainly involve pronoun resolution. We hope our findings will help future research in building coreference resolution systems."
"Neural machine translation enables end-to-end training translation models known give state-of-the-art results large variety language pairs. NMT high-resource language pairs straightforward: choose NMT architecture implementation, train model existing data. In contrast, low-resource language pairs, work well due inability neural networks generalize small amounts data. One reason strong over-fitting potential neural models . There several solutions address issue two effective ones transfer learning model regularization. Transfer learning sometimes considered data regularization comes form monolingual cross-lingual transfer learning , pseudo-parallel data generation , multi-task learning . On hand, model regularization techniques place constraints learning model parameters order aid model learn robust representations positively impact model performance. Among existing model regularization methods, dropout commonly used known effective regardless size data. We thus focus designing technique complement dropout especially extremely low-resource situation. The common way train NMT models minimize softmax cross-entropy loss, i.e., cross-entropy softmax distribution smoothed label distribution typically represented one-hot vector. In words, NMT model trained produce softmax distribution similar label. In high-resource settings, may never happen due diversity label sequences. However, low-resource settings, due lack diversity, high chance occurring over-fitting said take place. We consider simple manipulation softmax distribution may help prevent it. This paper presents investigation softmax tempering training NMT models order address over-fitting issue. Softmax tempering realized simply dividing pre-softmax logits positive real number greater 1.0. This leads smoother softmax probability distribution, used compute cross-entropy loss. Softmax tempering devised used regularly knowledge distillation , albeit different purposes. We regard softmax tempering means deliberately making softmax distribution noisy training expectation positive impact final translation quality. We primarily evaluate utility softmax tempering extremely low-resource settings involving English 11 languages Asian Languages Treebank . Our experiments reveal softmax tempering reasonably high temperature improves translation quality. Furthermore, makes greedy search performance models trained softmax tempering comparable better performance beam search using models trained without softmax tempering, enabling faster decoding. We expand scope study high-resource settings, taking WMT 2019 English-to-German translation task, well multilingual settings using ALT data. We also show softmax tempering improves performance NMT models using recurrently stacked layers heavily share parameters. Furthermore, clarify relationship softmax tempering dropout, i.e., widely used effective regularization mechanism. Finally, analyze impact softmax tempering softmax distributions gradient flows training. We analyzed complex interaction mention detector linker mainstream coarse-to-fine coreference system. Using oracle experiments, showed that, detector recall important, higher non-singleton mention precision would lead dramatically better linker performance, though achieving difficult. We also demonstrated oracle linker performance near perfect vast majority remaining linker errors besides anaphoricity decisions pronoun resolution. We hope discoveries help future research coreference systems. \clearpage"," Neural machine translation  models are typically trained using a softmax cross-entropy loss where the softmax distribution is compared against smoothed gold labels. In low-resource scenarios, NMT models tend to over-fit because the softmax distribution quickly approaches the gold label distribution. To address this issue, we propose to divide the logits by a temperature coefficient, prior to applying softmax, during training. In our experiments on 11 language pairs in the Asian Language Treebank dataset and the WMT 2019 English-to-German translation task, we observed significant improvements in translation quality by up to 3.9 BLEU points. Furthermore, softmax tempering makes the greedy search to be as good as beam search decoding in terms of translation quality, enabling 1.5 to 3.5 times speed-up. We also study the impact of softmax tempering on multilingual NMT and recurrently stacked NMT, both of which aim to reduce the NMT model size by parameter sharing thereby verifying the utility of temperature in developing compact NMT models. Finally, an analysis of softmax entropies and gradients reveal the impact of our method on the internal behavior of NMT models."
"Neural text generation one extensively studied tasks natural language processing , forms basis dialogue systems, machine translation, text summarization. However, often monotonous dull, texts generated existing methods fully reflect rich diversity expression human language. In particular, models tend overproduce words frequently appearing data, hardly utilizing informative words . % along fast-evolving model architectures pre-training techniques. \dkp{} % \st{However, existing methods neural text generation stop short resolving problem degeneration machine-generated texts either monotonous repeating words phrases, often failing complete proper sentence.} Even pre-training techniques large corpora fail resolve issue. %\dkpc{Current logic: three causes, choose solve last Better logic: one cause - drawback, second cause - drawback, third cause - directly addressing model, thus optimal!} Possible causes text degeneration illuminated, defect specific model architectures discrepancy training data true distribution. Recently, emphasis placed investigating flaws maximum likelihood objective. Concretely, likelihood training pays little attention top ranks terms target token probabilities, maximizing likelihood adequately reflect human language processing. Therefore, maximum likelihood-based training, models learn produce tokens frequently appearing data often. We argue, however, primary reason behind sub-optimal performance likelihood objective essentially imbalanced token distribution inherent natural language. Natural language extremely skewed distribution, top hundred frequently-used words occupy nearly half total corpus following Zipf's law. Training classifier inherently imbalanced data maximum likelihood estimation leads biased classification boundaries favor majority classes. In words, models play difficult role learning imbalanced label distribution. % \st{Our analysis comparing word frequencies corpus texts generated MLE model reveals model uses top-100 words 40\% often original data.} % Although might contributing factors, found word distribution provides clues text degeneration. %We set work line last category. However, unlike previous approaches, claim data distribution provide clues text degenerates. %Prior studies report number main causes text degeneration. First, attributed Attention mechanisms, \dkp{bulabula}. Another reason lies training machine fixed corpora distribution agree real-world language distribution. Lastly, maximum-likelihood objective, machine trained with, questioned. Following maximum-likelihood, little attention made top ranks next \dkpc{next what?} token probabilities. \dkpc{this explained crystal clear, since phenomenon directly related model}. differs human behavior. % We hypothesize text generation enriched balancing training data distribution. To end, introduce F-Softmax , Section), factorizes probability distribution target token product two conditional probabilities frequency class, token target frequency class. It ensures training balanced data, since frequency classes designed distribution close uniformity, token distributions within class confined subsets vocabularies grouped similar frequencies. To end, unique tokens assigned frequency class prior training, novel mean efficiency maximization , Section). MefMax evaluates maximizes class-labeling performance normalized entropy , probability distributions learned uniform possible. % Athe probability distributions learned model uniform possible, without introducing hyperparameter. % \st{which assigns tokens, order decreasing frequency, unique frequency class given condition classes share frequency mass. In way, prediction pipelines frequency classes tokens, respectively, performed based well-defined data near-uniform class distribution.} % We propose factorized softmax achieves introducing concept classes decomposing output probabilities using classes. It computes probability distributions tokens factorized manner; probability distribution class conditional probability distribution next token given class. The probability next token computed within subset vocabulary, rather full vocabulary. Well structured subsets vocabulary allows model benefit balanced output distributions. We assign token unique class utilizing proposed mean efficiency maximization algorithm data distribution trained distribution uniform possible. We conduct extensive performance evaluations seven relevant metrics quantify diversity quality generated texts. In terms diversity generated texts, approach significantly outperforms MLE baseline also diversity-promoting alternatives . We also achieve state-of-the-art results quality performances. In paper, explored utility softmax tempering training NMT models. Our experiments low-resource high-resource settings revealed softmax tempering lead improvement decoding quality also bridges gap greedy beam search performance. Consequently, use greedy search achieving better translation quality non-tempered models leading 1.5 3.5 times faster decoding. We also explored compatibility softmax tempering multilingualism extreme parameter sharing, explicitly investigated complementarity softmax tempering dropout, show softmax tempering alternative dropout high-resource settings, complementary dropout low-resource settings. Our analysis softmax entropies gradients training confirms tempering gives precise softmaxes enabling model learn strong gradient signals even late training stages. In future, explore effectiveness softmax tempering natural language processing tasks."," %Despite recent advances in neural text generation, encoding the rich diversity in human language remains elusive. We argue that the sub-optimal text generation is largely attributable to the imbalanced token distribution, which particularly misdirects the learning model when trained with the maximum-likelihood objective. As a simple yet effective remedy, we propose F$^2$-Softmax to enable a balanced training over the tokens with skewed frequency distribution. \dkp{By decomposing the softmax function, F$^2$-Softmax confines probability distribution to subsets of vocabularies which are more uniformly distributed. The subsets are further optimized by our novel mean efficiency maximization , without introducing any hyperparameter.} Significant performance gains across generation quality metrics suggest that our methods achieve human-like diversity in text generation.  Despite recent advances in neural text generation, encoding the rich diversity in human language remains elusive. We argue that the sub-optimal text generation is mainly attributable to the imbalanced token distribution, which particularly misdirects the learning model when trained with the maximum-likelihood objective. As a simple yet effective remedy, we propose two novel methods, F$^2$-Softmax and MefMax, for a balanced training even with the skewed frequency distribution. MefMax assigns tokens uniquely to frequency classes, trying to group tokens with similar frequencies and equalize frequency mass between the classes. F$^2$-Softmax then decomposes a probability distribution of the target token into a product of two conditional probabilities of  frequency class, and  token from the target frequency class. Models learn more uniform probability distributions because they are confined to subsets of vocabularies.  Significant performance gains on seven relevant metrics suggest the supremacy of our approach in improving not only the diversity but also the quality of generated texts.  % Despite recent advances in neural text generation, encoding the rich diversity in human language remains elusive. We argue that the sub-optimal text generation is mainly attributable to the imbalanced token distribution, which particularly misdirects the learning model when trained with the maximum-likelihood objective. As a simple yet effective remedy, we propose F$^2$-Softmax for a balanced training even with the skewed frequency distribution. F$^2$-Softmax decomposes a probability distribution of the target token into a product of two conditional probabilities of  frequency class  token from the target frequency class. Models learn more uniform probability distributions because they are confined to subsets of vocabularies. The subsets are further optimized by our novel mean efficiency maximization . It maximizes the . Significant performance gains across generation quality metrics suggest that our method achieves human-like diversity in text generation, without compromising the quality of generated texts. Significant performance gains on seven relevant metrics suggest the supremacy of our approach improves both diversity and quality in text generation."
"Natural language understanding key component conversational dialogue systems, converting user's utterances corresponding semantic representations certain narrow domain . As core task NLU, slot tagging usually formulated sequence labeling problem. Recently, motivated commercial applications like Amazon Alexa, Apple Siri, Google Assistant, Microsoft Cortana, great interest attached rapid domain transfer adaptation samples. Few-shot learning approaches become appealing scenario , general model learned existing domains transferred new domains rapidly merely examples . The similarity-based few-shot learning methods widely analyzed classification problems, classify item according similarity representation class. These methods learn domain-general encoder extract feature vectors items existing domains, utilize encoder obtain representation new class labeled samples . This scenario successfully adopted slot tagging task considering word-label similarity temporal dependency target labels. Nonetheless, still challenge devise appropriate word-label similarity metrics generalization capability. In work, vector projection network proposed few-shot slot tagging task NLU. To eliminate impact unrelated label vectors large norm, exploit projections contextual word embeddings normalized label vector word-label similarity. Moreover, half norm label vector utilized threshold, help reduce false positive errors. %It first normalizes vector representation label unit vector, exploits projections contextual word embeddings unit label vectors word-label similarities. One-shot five-shot experiments slot tagging named entity recognition tasks show method outperform various few-shot learning baselines, enhance existing advanced methods like TapNet prototypical network, achieve state-of-the-art performances. Our contributions summarized follows: In paper, propose unified multi-task learning approach exploits capabilities adversarial learning approach relation extraction biomedical domain. We first experimented three benchmark biomedical relation extraction tasks, i.e., protein-protein interaction, drug-drug interaction, clinical relation extraction. For that, utilized four popular datasets: AIMed, BioInfer, SemEval 2013 DDI shared task dataset i2b2-2010 clinical relation dataset. We demonstrated model shows superior performance compared state-of-the-art models tasks. \\ Although model shown significant improvements state-of-the-art methods tasks, observed supervised model generalize well class label small instances. In future, would like develop zero-shot learning method could assist model huge class imbalance issue. {\bf Acknowledgement}: Dr. Sriparna Saha gratefully acknowledges Young Faculty Research Fellowship Award, supported Visvesvaraya Ph.D. Scheme Electronics IT, Ministry Electronics Information Technology , Government India, implemented Digital India Corporation carrying research. single appendix: [Proof Zonklar Equations] appendix heading use","  Few-shot slot tagging becomes appealing for rapid domain transfer and adaptation, motivated by the tremendous development of conversational dialogue systems. In this paper, we propose a vector projection network for few-shot slot tagging, which exploits projections of contextual word embeddings on each target label vector as word-label similarities. Essentially, this approach is equivalent to a normalized linear model with an adaptive bias. The contrastive experiment demonstrates that our proposed vector projection based similarity metric can significantly surpass other variants. Specifically, in the five-shot setting on benchmarks SNIPS and NER, our method outperforms the strongest few-shot learning baseline by $6.30$ and $13.79$ points on F$_1$ score, respectively. Our code will be released at \url{https://github.com/sz128/few_shot_slot_tagging_and_NER}."
"% Over last decade increasing number people access news online, use social networking platforms engage, consume propagate content social circles. Social networks provide easy means distribute news commentary, resulting sharp increase number media outlets, representing wide range perspectives ideologies. However, despite diversity, content often shared among people hold similar beliefs ideologies, resulting highly segregated information communities, often referred ``echo chambers''. % To date, works studying phenomenon either focused linguistic aspects biased polarized content, social aspects connecting users content providers way documents spread. Our main observation paper modeling aspects needed order understand analyze information communities. %focused analyzing interactions news sources users social networks % Our goal paper formalize connections perspectives expressed text, users share them, social interactions information communities emerging connections. We suggest novel, minimally supervised, approach embedding information communities, allows us map news media landscape politically divisive issues, capture ideological biases perspectives expressed news content. We analyze differences communities based position continuous conservative-liberal ideological spectrum, observe differences perspectives expressed documents shared communities three issues-- immigration, gun-control abortion. % %Identifying perspective difference making explicit help strengthen trust newly-formed information landscape ensure perspectives represented. It also help lay foundation automatic detection false content rumors help identify information echo-chambers single perspective highlighted. %ribeiro2018media \subsection{Issue-Framing Political Perspective} To help clarify objectives, consider two articles highly polarized immigration issue. \\ % Example 1: Different Perspectives Immigration %Difference frame usage party. Topic: Immigration, Frame used: Economic % colback=blue!5!white,colframe=blue!75!black \end{tcbraster} The two articles capture opposite political perspectives, liberal conservative . They directly contradict other, rather focus discussion different aspects helping argue case. The first emphasizing contribution immigrants community tax revenue, second emphasizing implication wages U.S. workers. This process known framing. % Our goal capture political perspective associated articles, explain perspective identifying framing dimensions text, associated perspective support it. Previous work by~\citet{boydstun2014tracking} studied policy issue framing news media suggested 15 broad frames analyze issues framed, include Economic, Morality Security, among others. These framing dimensions help capture ideological splits. For example, framing immigration issue using morality frame using security frame, reader primed accept liberal conservative perspectives, respectively. However, shown Example 1, cases analysis coarse grained, articles frame issue using economic frame, suggesting finer grained analysis needed capture differences perspective. To help resolve issue, suggest data-driven refinement frames, described section, identifying repeating expressions used context different frames, grouping form sub-frames, separate different usages frame express different political perspectives . % Our goal capture ideological perspective associated documents, identify issues framed support perspectives, represent political meaning frames ideological spectrum. \subsection{Embedding Information Communities} %Identifying political perspectives typically framed text-categorization The analysis discussed typically framed text classification, e.g., biased language analysis, political ideology identification framing analysis. Given highly dynamic nature political events strategies used discuss them, methods would require continuous adaptation. % Instead, take different approach driven principal social homophily, referring tendency individuals form social ties others share views. This phenomenon previously used help overcome language variation issues. In settings, follow observation political perspectives attitudes expressed text reflected behavior users engaging it. We identify similar patterns exploit distant supervision construct information communities consisting documents users, holding similar views focusing similar aspects issues. Figure describes example immigration issue. We define communities information graph, modeling interaction news document nodes, users share Twitter, political influencers, politicians, followed-by sharing users. % %Given graph connecting Twitter users nodes, via activity-links news article nodes , via social-links politically affiliated users , Our algorithm groups users news-article nodes together communities, associates political meaning communities observing social links politicians, identifies repeating themes perspectives observing topic framed news articles associated community. %TODO: explain - embedding space, allowing share representation, creates common language evaluating relationship elements, connecting frames political labels, documents . A community defines probability distribution, elements, belong cluster. The assignments overlapping. The community represented using centroid, allowing us create embedding community, evaluated embedding space - observing similarity elements, labels, perspectives, explain community. % % To accomplish that, define latent space embedding users, documents, political influencers, frames ideology-labels. Intuitively, embedding space shaped textual content documents, engagement patterns users documents social ties politicians. We take community embedding approach, define communities multivariate Gaussian distribution latent space. We suggest EM-style learning approach, augmenting graph embedding objective, based first-order graph relations, global-view derived inferred community assignments. Unlike related work analyzing community behavior conflicts, analyze observed community structures, rather goal construct communities, way characterize different issues discussed align social information text. Recent work ~\citet{li2019encoding} exploits social supervision detecting political bias documents. We take broader view work, aim characterize discussion, rather individual articles. %TODO - experiments designed todo? One practical thing evaluate document classification, show adding communities help. More broadly - sanity check communities. communities properties. We conduct extensive experiments evaluate inferred community structures, three politically divisive issues, namely, immigration, abortion gun-control. We show approach used detect political ideology documents, even little social information available, well characterize cohesive information communities, focusing different aspects issues. % % TODO : discuss disconnected docs % todo: discuss ""beyond binary labels"" % todo : discuss no-supervision settings - similar topic models etc. %Eval: %Extenral Indicators sanity - % frame/subframe label correlation+visualization %indicator correlation. % classifier results - subframes good features, comparable BERT. %Internal Sanity check - % took top-K articles SF, evaluated correspond definition . % TODO: add example couple paragraphs. % Sample 10 side, topic = 60 documents. Compare two lists SF predicted text - GLDA vs. embedding %Applying SF analyzing data - %event based table In paper, propose vector projection network few-shot slot tagging task, interpreted normalized linear model adaptive bias. Experimental results demonstrate method significantly outperform strongest few-shot learning baseline SNIPS NER datasets 1-shot 5-shot settings. Furthermore, proposed vector projection based similarity metric remarkably surpass others variants. For future work, would like add learnable scale factor bias Eqn. ."," In this paper we suggest a minimally-supervised approach for identifying nuanced frames in news article coverage of politically divisive topics. We suggest to break the broad policy frames suggested by, \citeyear{boydstun2014tracking} into fine-grained subframes which can capture differences in political ideology in a better way. We evaluate the suggested subframes and their embedding, learned using minimal supervision, over three topics, namely, immigration, gun-control and abortion. We demonstrate the ability of the subframes to capture ideological differences and analyze political discourse in news media.  % a method for characterizing the perspectives on news media on several politically divisive issues, such as immigration, gun-control and abortion."
"Two widely-known formalisms commonly used represent syntactic structure sentences human languages: constituent dependency representations. Constituent trees, commonly used tasks span information crucial, describe syntax sentence terms constituents hierarchical order. We find two kinds constituent trees: continuous discontinuous , respectively). The latter extend former allowing %the representation crossing branches constituents gaps middle. These necessary describing wh-movement, long-distance extractions, dislocations, cross-serial dependencies linguistic phenomena common free word order languages German . On hand, dependency tree straightforwardly connects word sentence dependent another, considered head word. This structure composed binary syntactic dependencies known representing information closer semantic relations classified projective non-projective , respectively). %, Non-projective dependency trees %are complex structure allows allow crossing dependencies, model linguistic phenomena described discontinuous constituent trees. Since information described %regular constituent tree cannot fully represented %regular dependency tree vice versa , %typically parsers typically parsers exclusively trained produce either dependency constituent structures and, cases, %they restricted less complex continuous/projective representations. %, supporting one four syntactic structures described before. \carlos{There exceptions, i.e., approaches trained generate constituents dependencies. For instance, chart parser \citet{zhou-zhao-2019-head} generates continuous projective structures single model, sequence labeling parser \citet{strzyz19} combines continuous constituents non-projective dependency structures. In cases, discussed detail Section, representations shown benefit terms accuracy.} \carlos{However, knowledge, joint training approaches defined support non-projective dependency trees discontinuous constituents; accurate least computationally complex models formalisms single-representation approaches: graph-based transition-based models non-projective dependencies, transition-based parsers discontinuous phrase-structure trees.} \carlos{In order fill gap,} propose novel multitask transition-based parser efficiently generate unrestricted constituent dependency structures \carlos{} single trained model. We design encoder-decoder neural architecture jointly trained across syntactic information represented two formalisms following multitask learning strategy . Inspired , model constituent trees augmented dependency structures use two separate task-specific decoders produce regular augmented dependency trees. Each decoder relies Pointer Networks biaffine classifier incrementally generate labelled dependencies left right, proposed \citet{L2RPointer}. Finally, decoding runtime $) required memory space multi-representational approach remains single-task dependency parser \citet{L2RPointer}, since single model trained multitask learning strategy impact decoding time, allowing decoders run parallel. We test multi-representational neural model\footnote{Source code available \url{https://github.com/danifg/MultiPointer}.} continuous English Chinese Penn Treebanks discontinuous NEGRA TIGER datasets. In benchmarks, approach outperforms single-task parsers , proves learning across regular dependency trees constituent information leads gains accuracy tasks, obtaining competitive results cases surpassing current state art %by wide margin several datasets. \deproot{2}{} \depedge[edge unit distance=4ex]{2}{1}{ROOT+S\#2} \depedge{2}{3}{VP\#1} \depedge[edge unit distance=3ex]{2}{4}{VP\#1} \depedge[edge unit distance=3ex]{2}{5}{ROOT+S\#2} \end{dependency} % \deproot[edge unit distance=2ex]{4}{} \depedge[edge unit distance=5ex]{4}{1}{nsubj} \depedge[edge unit distance=4ex]{4}{2}{cop} \depedge{4}{3}{advmod} \depedge[edge unit distance=4ex]{4}{5}{punct} \end{dependency}\\ % {\tiny a) Continuous constituent tree.} {\tiny b) Projective augmented dependency tree.} {\tiny c) Projective dependency tree.}\\ %%%%%%%%%%%%%%%%%%%%%%%%% \includegraphics[width=0.3\textwidth]{treedisc.png} % \deproot[edge unit distance=4ex]{2}{} \depedge[edge unit distance=2ex]{4}{1}{NP\#2} \depedge{4}{3}{NP\#1} \depedge[edge unit distance=4.5ex]{2}{4}{S\#1} \depedge[edge unit distance=4ex]{2}{5}{VROOT\#2} \end{dependency} % \deproot[edge unit distance=2.5ex]{2}{} \depedge[edge unit distance=2ex]{4}{1}{APP} \depedge{4}{3}{DET} \depedge[edge unit distance=4.5ex]{2}{4}{SUBJ} \depedge[edge unit distance=4ex]{2}{5}{punct} \end{dependency}\\ % {\tiny d) Discontinuous constituent tree.} {\tiny e) Non-projective augmented dependency tree.} {\tiny f) Non-projective dependency tree.} \end{figure*} In paper, empirically validate inequality attention heads Transformer come assumption imbalanced training. Correspondingly, propose specific method two ways resolve issue. Experiments show improvements multiple language pairs. And detailed analysis shows alleviation problem effectiveness techniques."," We propose a transition-based approach that, by training a single model, can efficiently parse any input sentence with both constituent and dependency trees, supporting both continuous/projective and discontinuous/non-projective syntactic structures. To that end, we develop a Pointer Network architecture with two separate task-specific decoders and a common encoder, and follow a multitask learning strategy to jointly train them. The resulting quadratic system, not only becomes the first parser that can jointly produce both unrestricted constituent  and dependency   trees from a single model, but also proves that both syntactic formalisms can benefit from each other during training, achieving state-of-the-art accuracies in several widely-used benchmarks such as the continuous English and Chinese Penn Treebanks, as well as the discontinuous German NEGRA and TIGER datasets."
"%GOAL: introducing rule based% %Traditional solutions task-oriented dialogue systems decompose task building complete task-oriented dialogue system several sequential steps, including \ac{LU}, \ac{DM} \ac{NLG}~ . In paper focus dialogue policy key component dialogue management; decides actions system take time step according context user feedback. The aim dialogue policies \ac{TDS} select appropriate actions time step according current context conversation user feedback~. In early work, dialogue policies manually designed set rules map dialogue context corresponding system action~. %That feasible domain complex. %, approach suffers limited task scalability inability easily updated user behavior changes. %When task domain complex possible conversation scenarios predefined explicitly, dialogue policy represented set rules map dialogue context corresponding system action . The ability rule-based solutions limited domain complexity task scalability. Moreover, design maintenance rules require lot effort domain knowledge. %GOAL: introducing supervised learning disadvantages% Due recent advantages deep learning availability labeled conversational datasets, supervised learning employed dialogue policy training overcome disadvantages rule-based systems. %Dialogue context-action pairs fed model infer underlying relation dialogue context corresponding dialogue actions supervised learning methods. \todo{It looks obvious task first sentence paragraph} The downside supervised learning approach dialogues observed datasets unlikely represent possible conversation scenarios; extreme cases, required conversational dataset cannot collected acquiring might cost-prohibitive. %GOAL: introducing RL disadvantages% The success \ac{RL} areas holds promises dialogue \ac{PL}~. Using \ac{RL} techniques, train dialogue policies optimize automatically, scratch utilizing interactions users~. % Handcrafting complex rules essential anymore expense pressure maintaining policy time alleviated. In \ac{RL}-based solutions, dialogue system takes actions controlled dialogue policy, user feedback , provided dialogue finished, utilized adjust initial policy~. %These methods assume system access reward signal end dialogue. In practice, reward signals always available may inconsistent~. As practical ask explicit user feedback dialogue policy training, different strategies proposed design rule-based user simulator along reward function approximate real reward function exists user's mind. Designing appropriate user simulator accurate reward function requires strong domain knowledge. This process disadvantages rule-based dialog systems~. The difference rule-based approaches system design meet problem dialogue agent side rule-based user simulators need solve environment side. %To train dialogue agent reinforcement learning, handcraft rule-based user simulator suffer problem rule-based dialogue agent task becoming complex. %The difference one approach meets problem dialogue agent side another one solve environment side . %%GOAL: Describing bottleneck% If task simple easy solve, build rule-based system rather user-simulator used \ac{RL} techniques train dialogue system, uncontrollable factors involved? And task domain complex hard solve, easier design maintain complicated rule-based user simulator build rule-based dialogue agent? % Training model-based user simulator~ real human dialogue dataset alternative solution still data-hungry. %Besides, guarantee human-designed model-based simulator cover possible dialogue scenarios. % With respect comparison reinforcement learning supervised learning, Supervised learning methods suffer issues require labeled conversational data; exceptional cases, data cannot collected privacy reasons, \ac{RL} solution. However, collecting labeled data feasible many applications~. % Therefore work seek answer following research question: Are really making progress c{TDSs focusing purely advancing \ac{RL}-based methods?} To address question, introduce three dialogue \ac{PL} methods require user simulator. The proposed methods achieve comparable even higher performance compared \ac{SOTA} \ac{RL} methods. The first method utilizes action decoder predict dialogue combinations. %The sequential decision setup make use dependency information different atomic dialogue actions response. The second method regards dialogue \ac{PL} task multi-label classification problem. Unlike previous work, assign dense layer action label action space. % This change provides dialogue agent stable higher performance. Based second method, propose adversarial learning method dialogue \ac{PL} without utilizing \ac{RL}. To backpropagate loss reward model policy model, utilize Gumbel-Softmax connect policy model reward model third method. % We compare methods \ac{RL} adversarial \ac{RL} based dialogue training solutions show achieve comparable performance without utilizing costly user simulator. To summarize, contributions are: We propose novel encoder-decoder neural architecture based Pointer Networks that, jointly trained regular constituent-based dependency trees, syntactically parse sentence two extended formalisms. constituent dependency trees. Apart requiring train single model, approach produce simplest continuous/projective trees, also discontinuous/non-projective structures runtime. We test parser main dependency constituent benchmarks, obtaining competitive results cases reporting state-of-the-art accuracies several datasets. As future work, plan perform auxiliary-task learning train separate model task, testing different weights loss computation. This lose advantage training single model undertake tasks, certainly lead improvements accuracy."," %\todo[maybe a more interesting title? like ``rethinking supervised learning and reinforcement learning in dialogue policy learning""] Dialogue policy learning for \ac{TDSs} has enjoyed great progress recently mostly through employing \ac{RL} methods. However, these approaches have become very sophisticated. It is time to re-evaluate it. Are we really making progress developing dialogue agents only based on \ac{RL}? We demonstrate how ~traditional supervised learning together with ~a simulator-free adversarial learning method can be used to achieve performance comparable to \ac{SOTA} \ac{RL}-based methods.  First, we introduce a simple dialogue action decoder to predict the appropriate actions. Then, the traditional multi-label classification solution for dialogue policy learning is extended by adding dense layers to improve the dialogue agent performance. Finally, we employ the Gumbel-Softmax estimator to alternatively train the dialogue agent and the dialogue reward model without using \ac{RL}.  Based on our extensive experimentation, we can conclude the proposed methods can achieve more stable and higher performance with fewer efforts, such as the domain knowledge required to design a user simulator and the intractable parameter tuning in reinforcement learning. Our main goal is not to beat \ac{RL} with supervised learning, but to demonstrate the value of rethinking the role of \ac{RL} and supervised learning in optimizing \ac{TDSs}."
"Despite many recent advances Natural Language Generation, successful creative narrative composition remains elusive. Current neural approaches plagued difficulty mastering structure, veer topics, lack long-range cohesion. They successfully imitate fluency style human writing, closer inspection sentences fit together form whole, reader left impression generation content. % \np{I think abi see good work cite.} This lack structure also degrades relevance generations conditioned prompt source text - strong language model repeat key phrases given prompt remain topic. These issues illustrated Naive Generated Story Table , many sentences individually fine, fit together one story, relate prompt. We hypothesise problem addressed focus deeper latent narrative structures. In Aristotle's Poetics, one enduring treatises craft writing good stories, philosopher lays elements story order importance. They are: An amateur masters skills later list, mastery event choice event arrangement distinguishes good writer . Next character, relevance, finally style diction matter. %A good writer must begin events, solidified transform events surface forms - names, details, natural language. This philosophical framework fits remarkably well traditional Natural Language Generation Pipeline approach emphasizes Content Planning . The pipeline divides generation three steps: Content Planning, Microplanning Surface Realization, step input modified refined, getting closer final textual output. %Abstract concepts get grounded, synonyms chosen actual wording selected, order convey semantic information present input. Incorporating plot order generate stories viewed proxy Content Planning/MicroPlanning language model makes use convert readable grammatically correct natural language output . Inspired Aristotelian Content Planning Frameworks, develop novel system story generation. We focus developing system learn expertly select events, characters, relevant content, write good plot structures. After work plot complete, large language model best fill descriptions, details, local specifics story. For plot generation, employ event-choice event-arrangement rescoring models assist building arc cohesion plot, character rescoring model helps select characters appear where, relevance model responsible keeping plot structure story topic. As improving plot-generation via rescoring using Aristotelian framework neural generation novel concepts, previous work implement practice. % \np{may contribution list 1) propose leverage principled Aristotelian framework content planning. 2) propose implementation framework using revise-based approach several rescoring models. 3) strong experimental results. } propose leverage principled Aristotelian framework content planning, 2) propose implementation framework using revision-based approach via several rescoring models 3) show strong experimental results 4 baselines. % Our contributions are: 1) We build system rescoring models enforce Aristotelian principles content planning process, 2) We experiment various different architectures rescoring model, ways create training examples encode Aristotelian concept, 3) We evaluate best system two state-of-the-art story generation systems dataset, well two ablated versions itself, find system improved relevance overall quality. %- neither best train models learn select correct events, arrange right order. There similarly work best teach model incorporate character. \\ In work, proposed two supervised learning approaches one adversarial learning method train dialogue policy \acp{TDS} without building user simulators. The proposed methods achieve state-of-the-art performance suggested existing approaches based \acf{RL} adversarial learning. However, demonstrated methods require fewer training efforts, namely domain knowledge needed design user simulator intractable parameter tuning \ac{RL} adversarial learning. Our findings questioned full potential supervised learning dialogue \acf{PL} exerted \ac{RL} methods used appropriate \ac{TDS} scenarios. \clearpage"," Long-form narrative text generated from large language models manages a fluent impersonation of human writing, but only at the local sentence level, and lacks structure or global cohesion. We posit that many of the problems of story generation can be addressed via high-quality content planning, and present a system that focuses on how to learn good plot structures to guide story generation. We utilize a plot-generation language model along with an ensemble of rescoring models that each implement an aspect of good story-writing as detailed in Aristotle's Poetics. We find that stories written with our more principled plot-structure are both more relevant to a given prompt and higher quality than baselines that do not content plan, or that plan in an unprincipled way.\footnote{Code at \url{https://github.com/PlusLabNLP/story-gen-BART}} %may need to talk about using the plot scaffolding to write stories later more directly - just trying to keep the focus on storyline % also could add a focus on ""long form text"" to differentiate from dialogue systems etc"
"% What neural keyphrase generation general Keyphrases phrases summarize highlight important information piece text. Keyphrase generation task automatically predicting keyphrases given source text. The task easily misunderstood trivialized yet another natural language generation task like summarization translation, failing recognize one key aspect distinguishes KPG: multiplicity generation targets; input sequence, KPG system expected output multiple keyphrases, mini-sequence multiple word tokens. % Typically, one source text associated multiple keyphrases, % may either present absent source text. % This property task, along others, pushes community investigate leveraging deep neural networks handle task. % There quite work KPGen task, people mainly use two popular frameworks: one2one one2seq % However, previous literature, comparison two frameworks, effects architectural hyper-parameter choice remain unclear. % Keyphrase generation essentially natural language generation task. Despite unique nature, KPG essentially ``brute-forced'' sequence-to-sequence framework existing literature .%,sun2019divgraphpointer,ye2018kp_semi}. % Seq2Seq models encoder-decoder neural networks, encoder reads source text form hidden representation, decoder generates target sequence word word conditioned source text representation passed encoder. The community approached unique challenges much ingenuity problem formulation, model design, evaluation. For example, multiple target phrases reformulated either splitting one phrase per data point joining single sequence delimiters , allowing straightforward applications existing neural techniques Seq2Seq. In accordance tremendous success demonstrated effectiveness neural approaches, steady progress made past years --- least empirically --- across various domains, including sub-areas previously shown rather difficult . Meanwhile, myriad KPG's unique challenges comes ever-growing collection studies that, albeit novel practical, may quickly proliferate overwhelm. We therefore motivated present study --- best knowledge --- first systematic investigation challenges well effect interplay among solutions. We hope study serve practical guide help researchers gain holistic view task, profit empirical results investigations variety topics KPG including model design, evaluation, hyper-parameter selection. %data processing, % Based training paradigms, keyphrase generation models introduced prior works fall two categories, namely \onetoone \onetoseq . % Models achieved improved performance texts various types, including scientific publications , news articles , forum postings . % However, unaware existing systematic comprehensive empirical analysis neural keyphrase generation, particularly examining effects fundamental factors shared various model designs. % In empirical study, exhaustive experiments provide comprehensive analysis. % To facilitate future research community clarifying, % In work, present comprehensive empirical study neural keyphrase generation extensive experiments, aiming characterize key factors keyphrase generation models, quantitatively analyze impacts model performance, compare wide range baseline variants. % We hope study serves practical guide help researchers architecture, methods, hyper-parameter selection. % We also hope provide new insights community. % Based extensive experiments, provide comprehensive analyses number factors affect training generalization performance keyphrase generation models. % Thus contributions are: The rest paper organized follows. We first enumerate specific challenges KPG due multiplicity target, describe general setups experiments. We subsequently present experimental results discussions answer three main questions:\\ 1. How well KPG models generalize various testing distributions?\\ 2. Does order target keyphrases matter training \onetoseq ?\\ 3. Are larger training data helpful? How better make use them? % We shown Content Planning via interim plot structure representation combined use rescoring models inject Aristotelian story-writing principles plot. We found results stories relevant higher quality stories generated directly prompts use plots without Aristotelian rescoring. Our findings also suggest future work additional ways incorporate story principles plot generation. Although Aristotelian plots improved naive plot, remains gaps quality generated gold plot structures. There also work done investigating models best able incorporate plots, would enable plot improvements even effective."," Recent years have seen a flourishing of neural keyphrase generation  works, including the release of several large-scale datasets and a host of new models to tackle them. Model performance on KPG tasks has increased significantly with evolving deep learning research. % \todo{Among the growing number of neural models competing on this track, we observe that most of them fall into two categories --- \onetoone and \onetoseq --- based on their training paradigms.} % However, there lacks a comprehensive comparison among different model designs, and an investigation on related factors  that may affect a keyphrase generation system's performance. However, there lacks a comprehensive comparison among different model designs, and a thorough investigation on related factors that may affect a KPG system's generalization performance. In this empirical study, we aim to fill this gap by providing extensive experimental results and analyzing the most crucial factors impacting the generalizability of KPG models. We hope this study can help clarify some of the uncertainties surrounding the KPG task and facilitate future research on this topic."
"Causal explanation detection aims detect whether causal explanation given message . Linguistically, coherence relations messages explain meaning different textual units combine jointly build discourse meaning larger unit. The explanation important relation coherence refers textual unit message expresses explanatory coherent semantics . As shown Figure , M1 divided three discourses, D2 explanation expresses reason advantageous equipment operate temperatures. CED important tasks require understanding textual expression . For example, question answering, answers questions likely group sentences contains causal explanations . Furthermore, summarization event descriptions improved selecting causally motivated sentences . Therefore, CED problem worthy study. The existing methods mostly regard task classification problem . At present, mainly two kinds methods, feature-based methods neural-based methods, similar semantic understanding tasks discourse granularity, opinion sentiment classification discourse parsing . The feature-based methods extract feature relation discourses. However, methods deal well implicit instances lack explicit features. For CED, shown Figure , D2 lacks explicit features of, due to, features tenses, friendly feature-based methods. The methods based neural network mainly Tree-LSTM model hierarchical Bi-LSTM model . The Tree-LSTM models learn relations words capture semantics discourses accurately lack understanding semantics discourses. The hierarchical Bi-LSTM models employ sequence structure implicitly learn relations words discourses. However, previous work shows compared Tree-LSTM, Bi-LSTM lacks direct understanding dependency relations words. Therefore, method implicit learning inter-word relations prominent tasks related understanding semantic relations messages . Therefore, directly learn relations words effectively consider discourse-level correlation filter key information valuable point worth studying. Further analysis, relations words imply semantics message discourses? From view computational semantics, meaning text meaning words also relation, order, aggregation words. In simple words meaning text partially based syntactic structure . In detail, CED, core subsidiary words discourses contain basic semantics. For example, D1 shown Figure , according word order syntactic structure, capture ability temperature advantageous. We understand basic semantic D1 expresses kind ability advantageous via root words advantageous affiliated words. Additionally, correlation key information discourse level important capture causal explanatory semantics message? Through observation, different discourse different status explanatory semantics message. For example, M1, combined D1, D2 expresses explanatory semantics ability work temperatures advantageous, D3 expresses semantic transition. In detail, D1 D2 keys explanatory semantics M1, treated D1, D2, D3 differently, transitional semantic D3 affect understanding explanatory semantic M1. Therefore, make better use information keywords syntactic structure pay attention discourses key explanatory semantics problem solved. To end, propose Pyramid Salient-Aware Networks utilizes keywords syntactic structure discourse focuses key discourses critical explanatory semantics detect causal explanation messages. First, keywords syntactic structure? From perspective syntactic dependency, root word central element dominates words, dominated words, subordinate root word . From that, root subsidiary words dependency structure keywords syntax level discourse. Specifically, sample 100 positive sentences training data illuminate whether keywords obtained syntactic dependency contain causal explanatory semantics. And find causal explanatory semantics 80\% sentences captured keywords dependency structure\footnote{Five Ph.D. students majoring NLP judge whether sentences could identified containing causal explanatory semantics root word surrounding words syntactic dependency, agreement consistency 0.8}. Therefore, extract root word surrounding words syntactic dependency discourse keywords. Next, need consider make better use information keywords contained syntactic structure. To pay attention keywords, common way using attention mechanisms increase attention weight them. However, implicitly learned attention interpretable. Inspired previous researches , propose bottom graph-based word-level salient network merges syntactic dependency capture salient semantics discourses contained keywords. Finally, consider correlation discourse level pay attention discourses key explanatory semantics? Inspired previous work , propose top attention-based discourse-level salient network focus key discourses terms explanatory semantics. In summary, contributions paper follows: In paper described development novel, holistic methodology measuring hate speech scalable, debiased, explainable manner. Based prior literature, theorized eight qualitative levels scale ranging genocidal hate speech counterspeech collected empirical observations examples level . We developed labeling instrument record ordinal ratings 10 components hate speech reviewing process. We collected online comments three major social media platforms sampled way focus labeling comments likely hate speech counterspeech, maintaining generalizability ensuring collected comments positive probability selection sampling procedure. We created crowdsourcing-based labeling procedure allocate comments reviewers yield network linking reviewers overlapping comment reviews, facilitating estimation survey interpretation bias reviewer . We fit faceted Rasch partial credit model create sample-invariant scale hate speech placed comments, survey instrument items, raters continuous metric, adjusted estimated comment hate speech score estimated survey interpretation bias raters happened rate comment. The statistical diagnostics Rasch model allowed us evaluate quality reviewer remove crowdsource workers low-quality responses. Finally, applied supervised, multitask, Transformer-based deep learning rater bias auxiliary input , followed IRT nonlinear post-processsing transformation plausible value sampling, learn estimator maps raw text hate speech score robust, explainable manner. That deep learning model encouraged gain general understanding language training data three separate social media platforms. Separately, steps represents novel contribution hate speech literature. In combination, believe methodology proposes paradigm shift understanding measurement hate speech, supervised learning human-labeled data broadly. We hope work encourage researchers adopt Constructing Measures-style theoretical development \& measurement study complex social phenomena, including transition dichotomous ordinal outcomes continuous, linear scales estimated via Rasch-based item response modeling, corrected survey interpretation bias reviewers, integrated explainable multitask deep learning architectures. For future updates project, data, models, please visit \href{https://hatespeech.berkeley.edu/}{hatespeech.berkeley.edu}."," 		Causal explanation analysis  can assist us to understand the reasons behind daily events, which has been found very helpful for understanding the coherence of messages. In this paper, we focus on Causal Explanation Detection, an important subtask of causal explanation analysis, which determines whether a causal explanation exists in one message. We design a Pyramid Salient-Aware Network  to detect causal explanations on messages. PSAN can assist in causal explanation detection via capturing the salient semantics of discourses contained in their keywords with a bottom graph-based word-level salient network. Furthermore, PSAN can modify the dominance of discourses via a top attention-based discourse-level salient network to enhance explanatory semantics of messages. The experiments on the commonly used dataset of CEA shows that the PSAN outperforms the state-of-the-art method by 1.8\% F1 value on the Causal Explanation Detection task."
"Event coreference resolution task determining event mentions document refer real-world event. Event coreference resolution important part NLP systems summarization, text-level event extraction, question answering on. Besides, compared considerable research entity coreference resolution, less attention event coreference resolution. Therefore, event coreference resolution still challenging task performance improved. Event mentions refer event occur within document across multiple documents . We focus WD event coreference paper WD event coreference basic work CD event coreference. The main task WD event coreference judging whether pair events coreferential not. Figure shows two coreferential event pairs two documents. The first event pair D1 shooting event second event pair D2 fire event. In order judge coreference event pair, approaches solving event coreference resolution relied various linguistic properties especially event argument, contains {spatio-temporal} information events. For instances, Figure, words red front events. And words blue, green orange front participant, time, location events respectively. Although event arguments contain useful information event coreference resolution, two problems using event arguments information event coreference resolution. Firstly, difficult extract event arguments accurately due diversity expression event arguments. The performance event argument extraction 55.7 ACE corpus. For instance, D1, arguments shooting event two sentences expressed differently. In details, D1, participant, time, location shooting event worker/2 women, 8:30 p.m. kraft S1, women, Friday evening building S2 respectively. Secondly, every event mention contains arguments one event may make model confused coreference two events event pair. For instance, D2, Wasilla Bible Church location fire event S1 S2. Besides, D2, devoid event arguments, burned event fire event coreferential context. As aforementioned, arguments events difficult extract. It also difficult use arguments solve problems event coreference resolution even extracted. Thus, context event mentions important effective event coreference resolution. In order use context information efficiently, propose multi-loss neural network model need argument information accomplish within-document event coreference resolution task. We propose two sub-models use context information detect coreference two events event pair train jointly. One classifier predicts whether two events one pair coreferential, another scorer calculates similarity scores assist infer coreference. The final stage event coreference resolution event clustering. After event pairs predicted scored, filter event pairs according results classifier scorer. Then, use dynamic connectivity algorithm construct graph event clustering. Each node graph event mention edge two nodes represent whether two event coreferential not. Finally, events connected one graph considered one event cluster. We evaluate model ECB+ corpus use B, CEAF, MUC CoNLL measures. The experimental results show model achieve significant improvement compared state-of-the-art methods use event argument features. In paper, devise pyramid salient-aware network detect causal explanations messages. PSAN effectively learn key relation words word level filter key information discourse level terms explanatory semantics. Specifically, propose bottom word-level salient-aware module capture salient semantics discourses contained keywords based syntactic-centric graph. We also propose top discourse-level salient-aware module modify dominance different discourses terms global explanatory semantic constraint via attention mechanism. Experimental results open-accessed commonly used datasets show model achieves best performance.","Event coreference resolution is an important task in Natural Language Processing  and nearly all the existing approaches to this task  rely on event argument information. However, these methods tend to suffer from error propagation from the stage of event argument extraction. Besides, not every event mention contains all arguments of an event and argument information may confuse the model that events have arguments to detect event coreference in real text. Furthermore, the context information of an event is useful to infer coreference between events. Thus, in order to reduce the errors propagated from event argument extraction and use context information effectively, we propose a multi-loss neural network model which does not need any argument information to do the within-document event coreference resolution task and achieve a significant performance than the state-of-the-art methods."
"A task-oriented spoken dialogue system usually consists three modules: input,output control, shown Fig.. The input module consists automatic speech recognition spoken language understanding extracts semantic-level user dialogue actions user speech signal. The control module two missions. One maintain dialogue state, encoding machine's understanding conversation. Once information input module received, dialogue state updated dialogue state tracking . The choose semantic-level machine dialogue action response user, called dialogue decision policy. The output consists natural language generation text-to-speech synthesis, convert dialogue action audio. Dialogue management important part dialogue system. Nevertheless, inevitable ASR SLU errors make hard track true dialogue state make decision. In recent statistical dialogue system, distribution dialogue state, i.e. belief state, tracked. A well-founded theory belief tracking decision making offered partially observable Markov Decision Process framework. Previous DST algorithms divided three families: hand-crafted rules, generative models, discriminative models. Recently, since Dialog State Tracking Challenges provided labelled dialog state tracking data common evaluation framework test-bed, variety machine learning methods DST proposed. These methods rely strictly set labelled off-line data. Since labelled data off-line, learning process supervised learning methods independent dialogue policy module. The key issues supervised learning methods poor generalization over-tuning. Due lack labels, approaches easily used on-line update DST. This work marks first step towards employing deep reinforcement learning method dialogue state tracking module. The performance DST module optimized conversation user dialogue system. We call DRL-based DST module tracking agent. In order bound search space tracking agent, propose companion teaching framework . Furthermore, framework, train tracking agent dialogue policy agent jointly respective deep reinforcement learning algorithms order make two agents adaptive other. % And two main types DST systems current dialogue system. One semantic-based dialogue state tracking system, text-based dialogue state tracking system implicitly explicitly removes spoken language understanding module. In paper, explain proposed tracking agent framework based semantic-based dialogue state tracking system. The paper two main contributions: The rest paper organized follows. Section gives overview related work. In Section , framework on-line DST presented. The implementation detail represented Section . In Section , joint training process introduced. Section presents experiments conducted evaluate proposed framework, followed conclusion Section . We present multi-layer feed-forward neural network event mention extraction multi-loss neural network model within-document event coreference resolution respectively. We use information event argument system. Additionally, test system ECB+ corpus achieve significant improvement state-of-the-art methods. Due incomplete annotation propagation errors event mentions arguments extraction pipeline systems, try design joint model accomplish event extraction, argument extraction, event coreference resolution tasks jointly future. Acknowledgements.  \Acknowledgements{This work supported Natural Science Foundation China . This work also supported Alibaba Group Alibaba Innovative Research Program Huawei Tech. Ltm Huawei Innovation Research Program.).} Supplements. ,  \Supplements{Appendix A.} Reference section.  citation content using ""some words"". ~ needed make reference number line word it. Appendix sections. ,  \begin{appendix}"," Dialogue state tracking  is a crucial module in dialogue management. It is usually cast as a supervised training problem, which is not convenient for on-line optimization. In this paper, a novel companion teaching based deep reinforcement learning  framework for on-line DST optimization is proposed. To the best of our knowledge, this is the first effort to optimize the DST module within DRL framework for on-line task-oriented spoken dialogue systems. In addition, dialogue policy can be further jointly updated. Experiments show that on-line DST optimization can effectively improve the dialogue manager performance while keeping the flexibility of using predefined policy. Joint training of both DST and policy can further improve the performance."
"{T}{he} task-oriented spoken dialogue system aims assist human user accomplishing specific task . The dialogue management core part SDS. There two main missions dialogue management: dialogue belief state tracking dialogue decision-making . In work, focus devising policy chooses dialogue action respond user. The sequential system decision-making process abstracted partially observable Markov decision process . Under framework, reinforcement learning approaches used automated policy optimization. In past years, many deep reinforcement learning algorithms use neural networks function approximators, investigated dialogue policy . Most approaches focus dialogue policy optimization single dialogue task. However, real-life scenarios, dialogue agent asked many users different dialogue tasks, e.g., Apple Siri support many dialogue tasks . In multi-task setup, traditional DRL-based approaches train individual policy dialogue task. It means dialogue policy independent model parameters, whose scale increase proportionally number tasks. One solution train generic policy dialogue tasks . However, two obstacles traditional DRL-based approaches. In paper, propose Structured Actor-Critic Reinforcement Learning Universal Dialogue Management address two problems. It use data collected different dialogue tasks train generic policy. To tackle scalability problem, utilize recently proposed structured dialogue policy , dialogue policy represented graph neural network . With scalability GNN , single set parameters used different dialogue tasks. That makes possible train generic policy among multiple dialogue tasks. To tackle efficiency problem, deploy advanced off-policy actor-critic algorithm, combines decoupled acting learning novel off-policy correction method called V-trace. Combining improved optimization algorithm structured dialogue policy, make generic policy learning process stable efficient original GNN-based dialogue policy . We evaluate performance STRAC PyDial benchmark, includes six environments three dialogue domains. Results show unified dialogue agent STRAC gets best performance 18 tasks benchmark. This paper provides DRL-based companion teaching framework optimize DST module dialogue system. Under framework, tracker learned conversations user SDS rather produced off-line methods. We also choose jointly train dialogue policy agent tracking agent framework. The experiments showed proposed companion teaching framework on-line DST system achieved promising performances DSTC2 DSTC3."," Traditional dialogue policy needs to be trained independently for each dialogue task. In this work, we aim to solve a collection of independent dialogue tasks using a unified dialogue agent. The unified policy is parallelly trained using the conversation data from all of the distributed dialogue tasks. However, there are two key challenges: the design of a unified dialogue model to adapt to different dialogue tasks;  finding a robust reinforcement learning method to keep the efficiency and the stability of the training process. Here we propose a novel structured actor-critic approach to implement structured deep reinforcement learning , which not only can learn parallelly from data of different dialogue tasks\footnote{In the experimental setup of this work, each dialogue task has only one dialogue domain.} but also achieves stable and sample-efficient learning. We demonstrate the effectiveness of the proposed approach on 18 tasks of PyDial benchmark. The results show that our method is able to achieve state-of-the-art performance."
"%When building dialogue system, complex tasks require information exchange often challenging. One example handle restaurant reservation consultation multiple areas single conversation. Specifically, type task needs complete subtasks order finish conversation called composite task. Composite tasks different multi-domain dialogue tasks. The latter often mentioned papers focusing transfer learning. In case, multi-domain dialogue tasks involve one domain single dialogue, performance one domain model tested different domains order highlight transferability. On contrary, composite dialogue tasks may involve multiple domains single dialogue, agent must complete subtasks order get positive feedback. Consider process completing composite task . An agent first chooses subtask , make sequence decisions gather related information information required users provided subtasks completed, choose next subtask complete. The state-action space increase number subtasks. Thus, dialogue policy learning composite task needs exploration, needs take dialogue turn agent user complete composite task. The sparse reward problem magnified. Solving composite tasks using method one solving single domain tasks may hit obstacles. The complexity composite task makes hard agent learn acceptable strategy. While hierarchical deep reinforcement learning shows promising power, introducing framework options Markov Decision Process , original task decomposed two parts: deciding subtask solve solve one subtask, thus simplifying problem. However, previous works, multi-layer perceptrons often used DQN estimate Q-value. MLPs use concatenation flatten dialogue state inputs. In way, cannot capture structural information semantic slots state easily, results low sampling efficiency. In work, propose ComNet, makes use Graph Neural Network better leverage graph structure observations coherent HDRL method. Our main contributions three-fold: 1. We propose new framework ComNet combining HDRL GNN solve composite tasks achieving sample efficiency. 2. We test ComNet based PyDial benchmark show result over-performed vanilla HDRL systems robust noise environment. 3. We test transferability framework prove framework, efficient accurate transfer possible. This paper proposed scalable distributed dialogue policy STRAC train generic dialogue policy available data collected different dialogue tasks. STRAC increased scalability, stability efficiency NN-based policy combining structured dialogue policy effective off-policy actor-critic algorithm. Compared traditional approaches, STRAC-M trained parallel multiple tasks gets better performance, especially data-limited situation. Compared another GNN-based policy optimization approach FM-GNN, training process STRAC stable efficient. The final gains considerable complex environments. Compared recent proposed generic policy DQNDIP-M, STRAC-M trained using data available dialogue tasks also model relations among sub-agents. In future work, test STRAC real users instead agenda-based user simulator. single appendix: [Proof Zonklar Equations] appendix heading use"," Dialogue policy training for composite tasks, such as restaurant reservation in multiple places, is a practically important and challenging problem. Recently, hierarchical deep reinforcement learning  methods have achieved good performance in composite tasks. However, in vanilla HDRL, both top-level and low-level policies are all represented by multi-layer perceptrons  which take the concatenation of all observations from the environment as the input for predicting actions. Thus, traditional HDRL approach often suffers from low sampling efficiency and poor transferability. In this paper, we address these problems by utilizing the flexibility of graph neural networks . A novel ComNet is proposed to model the structure of a hierarchical agent. The performance of ComNet is tested on composited tasks of the PyDial benchmark. Experiments show that ComNet outperforms vanilla HDRL systems with performance close to the upper bound. It not only achieves sample efficiency but also is more robust to noise while maintaining the transferability to other composite tasks."
"Relation extraction aims identify semantic relations named entities text. While previous work focuses extracting relations within sentence, a.k.a.~sentence-level RE, recent studies escalated document level, since large amount relations entities usually span across multiple sentences real world. According analysis Wikipedia corpus , least 40.7\% relations extracted document level. Compared sentence-level RE, document-level RE requires complex reasoning, logical reasoning, coreference reasoning common-sense reasoning. A document often contains many entities, entities multiple mentions phrase alias. To identify relations entities appearing different sentences, document-level RE models must capable modeling complex interactions multiple entities synthesizing context information multiple mentions. Figure shows example document-level RE. Assume one wants extract relation ``Surfers Riverwalk"" S11 ``Queensland"" S1. One find ``Surfers Riverwalk"" contains ``Pacific Fair"" , ``Pacific Fair"" located ``Queensland"" . This chain interactions helps infer inter-sentential relation ``located in"" ``Surfers Riverwalk"" ``Queensland"". %==================== In paper, propose ComNet, structured hierarchical dialogue policy represented two graph neural networks . By replacing MLPs traditional HDRL methods, ComNet makes better use structural information dialogue state separately feeding observations top-level decision slot-dependent, slot-independent subtask nodes exchange message nodes. We evaluate framework modified PyDial benchmark show high efficiency, robustness transferability settings."," Relation extraction  aims to identify the semantic relations between named entities in text. Recent years have witnessed it raised to the document level, which requires complex reasoning with entities and mentions throughout an entire document. In this paper, we propose a novel model to document-level RE, by encoding the document information in terms of entity global and local representations as well as context relation representations. Entity global representations model the semantic information of all entities in the document, entity local representations aggregate the contextual information of multiple mentions of specific entities, and context relation representations encode the topic information of other relations. Experimental results demonstrate that our model achieves superior performance on two public datasets for document-level RE. It is particularly effective in extracting relations between entities of long distance and having multiple mentions."
"Dialogue state tracker core part task-oriented dialogue system, records dialogue state. The dialogue state consists set domain-slot-value triples, specific value represents user goal, e.g., . The dialogue system responds user based dialogue state. Thus, order make dialogue process natural fluent, essential extract dialogue state dialogue context accurately. However, paucity annotated data main challenge field. In work, solve key problem learn unlabeled data DST task. We design dual learning framework DST task, dialogue state tracker primal agent dual agent utterance generator. Within dual learning framework, two primal-dual agents help update external reward signals reconstruction errors using unlabeled data. It needs labeled dialogue data warm two primal-dual agents. However, two main challenges combining dual learning framework previous dialogue state tracking methods: How represent dialogue state dual learning framework? Dual learning method first proposed neural machine translation task. The outputs primal-dual agents NMT task sequential natural languages. However, DST task, output dialogue state tracker consists isolated domain-slot-value triples. The traditional DST task formulated classification problem given ontology, possible values corresponding slot listed. Under problem definition, previous classification methods choose right value slot. The recent innovated tracker TRADE directly generates values slot slot using copy mechanism dialogue context. However, tracker methods get slot values independently. During dual learning loop, hard get reward signal independent slot values. The reward signal dual utterance generator also hard allocate isolated value generation processes. Since relations predicted values modeled assumed independent other, would face serious reward sparse problem. In work, reformulate dialogue state tracking task sequential generation task. The whole dialogue state represented sequence structured information. For example, state represented ``\textless\textgreater \textless\textgreater \textless\textgreater \textless\textgreater \textless\textgreater \textless\textgreater \textless\textgreater''. Is reasonable generating whole dialogue context dialogue state? The intuitive dual task state tracker dialogue context generation. However, MultiWOZ 2.1 dataset, dialogue context 10 turns average average length sentence 10 tokens. It difficult generating accurately dialogue context dialogue state. Because dialogue context long, hard guarantee generated dialogue context contains semantics given state. In work, simplify dual task user utterance generation task ignores specific values given state. The input dual task composed two parts , output delexicalized user utterance. The delexicalized script copied released code \footnote{https://github.com/ConvLab/ConvLab}. The system utterance user utterance lexicalized respectively according given turn state. We get new pseudo-labeled dialogue turn. In order produce multi-turn pseudo-labeled data, sample labeled dialogue data combine pseudo-labeled dialogue turn, dialogue turn directly adds end sampled dialogue context turn state covers label sampled state. Finally, get new dialogue context pseudo label state, intuitive dual-task does. The main contributions paper summarized follows: In paper, proposed GLRE, global-to-local neural network document-level RE. Entity global representations model semantic information entire document R-GCN, entity local representations aggregate contextual information mentions selectively using multi-head attention. Moreover, context relation representations encode topic information relations using self-attention. Our experiments demonstrated superiority GLRE many comparative models, especially big leads extracting relations entities long distance multiple mentions. In future work, plan integrate knowledge graphs explore document graph modeling ways improve performance.\\ Acknowledgments. This work supported partially National Key R\&D Program China , National Natural Science Foundation China , Water Resource Science \& Technology Project Jiangsu Province . The next two lines define bibliography style used, bibliography file."," In task-oriented multi-turn dialogue systems, dialogue state refers to a compact representation of the user goal in the context of dialogue history. Dialogue state tracking  is to estimate the dialogue state at each turn. Due to the dependency on complicated dialogue history contexts, DST data annotation is more expensive than single-sentence language understanding, which makes the task more challenging. In this work, we formulate DST as a sequence generation problem and propose a novel dual-learning framework to make full use of unlabeled data. In the dual-learning framework, there are two agents: the primal tracker agent  and the dual utterance generator agent . Compared with traditional supervised learning framework, dual learning can iteratively update both agents through the reconstruction error and reward signal respectively without labeled data. Reward sparsity problem is hard to solve in previous DST methods. In this work, the reformulation of DST as a sequence generation model effectively alleviates this problem. We call this primal tracker agent dual-DST. Experimental results on MultiWOZ2.1 dataset show that the proposed dual-DST works very well, especially when labelled data is limited. It achieves comparable performance to the system where labeled data is fully used."
"% P1 intro {T}{ask-oriented} dialog system aims users achieve goals finding attractions booking restaurants. Developing system typically requires following dialog components construct pipeline illustrated \fig : natural language understanding extract user's intents slot-values , dialog state tracking update belief states , querying database, dialog policy decide system's next action , natural language generation generate system responses . Although recent advances neural approaches natural language domain greatly improved performance individual dialog components, errors component accumulated pipelined system, resulting degradation overall performance. Therefore, designing effective architecture optimizing entire dialog system end-to-end fashion still challenging. % P2 % e2e Recently, several end-to-end neural dialog systems proposed . % Modularized % seq2seq Sequence-to-sequence approaches directly generate system responses given user utterance inputs, limitations querying external database unavailable , system actions interpretable . % RL e2e Moreover, previous researchers investigated dialog policy optimization reinforcement learning end-to-end neural task-oriented dialog systems . % GPT-2 Meanwhile, recent approaches transfer general linguistic knowledge large pre-trained language model, GPT-2 , goal-oriented dialog shown remarkable improvements . They employed GPT-2 backbone is, fine-tuned model auto-regressively generate dialog states, system actions, responses sequence. Although leveraging rich knowledge allows models generate natural appropriate responses, reinforcement learning transformer-based architectures reported unstable , learning dialog policy models explored yet. % P4 approaches In paper, present end-to-end trainable neural dialog system reinforcement learning multi-domain task-completion tasks, SUMBT+LaRL, consists two components: extended version SUMBT word-level dialog state tracker LaRL word-level policy model. In addition SUMBT updates belief states employing slot-utterance matching mechanism, SUMBT+ predicts domains user-intents user utterance. Then given predictions SUMBT+, LaRL models categorical latent system action spaces generates system responses. In training framework, emphasize importance separately pre-train SUMBT+ LaRL fine-tune entire model end-to-end fashion. Then, trained dialog policy optimized off-line reinforcement learning using REINFORCE algorithm succeed dialog tasks. During reinforcement training, policy gradients latent actions decouple discourse-level decision making language generation decoder, enabling stable effective reinforcement learning. We propose new success criteria system respond requestable slots calculate match performance using belief state estimated SUMBT+. We demonstrated efficacy proposed system MultiWOZ2.0, implementing ConvLab platform user simulator-based evaluations. Our extensive experimental results corpus simulator-based evaluation shows effectiveness proposed pretraining end-to-end fine-tuning framework well reinforcement learning latent action space. From results qualitative analysis simulated dialog examples, also present discrepancy problem corpus automatic evaluations, limitations off-line reinforcement learning dialog systems, needs advanced reward design success criteria. Our model achieved new state-of-the-art success rate end-to-end corpus-based evaluation MultiWOZ2.0, well outperformed challenge winner 8th dialog system technology challenge challenge simulator-based evaluation. % P5 contribution summary In summary, main contributions paper three-fold: % section intro \sect 2 briefly reviews end-to-end multi-domain task-completion dialog systems DSTC8 Challenge. \sect 3 explains detailed architecture proposed SUMBT+LaRL training procedures. Related work described \sect 4 experimental results presented \sect 5. %\newpage %\clearpage In work, first reformulate dialogue state tracking task sequence generation task. Then adopt coarse-to-fine decoding method directly generate structured state sequence. The proposed coarse-to-fine tracker achieves best performance among BERT-free methods. The main contribution work lies building dual learning framework multi-domain DST task. The experimental results indicate proposed dual learning method efficiently improve pretrained tracker unlabeled data. In future work, improve state tracking model dual utterance generation model using pretrained models, e.g. BERT. ijcai20.tex \typeout{IJCAI--PRICAI--20 Instructions Authors} These instructions authors IJCAI-20. \documentclass{article} \pdfpagewidth=8.5in \pdfpageheight=11in The file ijcai20.sty NOT previous years' \usepackage{ijcai20} Use postscript times font! \usepackage{times} \usepackage{soul} \usepackage{url} \usepackage[hidelinks]{hyperref} \usepackage[utf8]{inputenc} \usepackage[small]{caption} \usepackage{graphicx} \usepackage{amsmath} \usepackage{amsthm} \usepackage{booktabs} \usepackage{algorithm} \usepackage{algorithmic} \urlstyle{same} \usepackage[bookmarks=false]{hyperref} following package optional: \usepackage{latexsym} See https://www.overleaf.com/learn/latex/theorems_and_proofs nice explanation define new theorems, keep mind amsthm package already included template must *not* alter styling. \newtheorem{example}{Example} \newtheorem{theorem}{Theorem} Following comment ijcai97-submit.tex: The preparation files supported Schlumberger Palo Alto Research, AT\&T Bell Laboratories, Morgan Kaufmann Publishers. Shirley Jowell, Morgan Kaufmann Publishers, Peter F. Patel-Schneider, AT\&T Bell Laboratories collaborated preparation. These instructions modified used conferences long credit authors supporting agencies retained, notice changed, modification reuse restricted. Neither Shirley Jowell Peter F. Patel-Schneider listed contacts providing assistance without prior permission. To use conferences, change references files conference appropriate use authors, contacts, publishers, organizations. Also change deadline address returning papers length page charge instructions. Put files available appropriate places. \title{IJCAI--PRICAI--20 Formatting Instructions} Single author syntax \author{ Christian Bessiere \affiliations CNRS, University Montpellier, France \emails pcchair@ijcai20.org } Multiple author syntax Check ijcai20-multiauthor.tex file detailed instructions \iffalse \author{ First Author \and Second Author\and Third Author\And Fourth Author \affiliations First Affiliation\\ Second Affiliation\\ Third Affiliation\\ Fourth Affiliation \emails \{first, second\}@example.com, third@other.example.com, fourth@example.com } \fi"," The recent advent of neural approaches for developing each dialog component in task-oriented dialog systems has remarkably improved, yet optimizing the overall system performance remains a challenge. In this paper, we propose an end-to-end trainable neural dialog system with reinforcement learning, named SUMBT+LaRL. The SUMBT+ estimates user-acts as well as dialog belief states, and the LaRL models latent system action spaces and generates responses given the estimated contexts. We experimentally demonstrate that the training framework in which the SUMBT+ and LaRL are separately pretrained and then the entire system is fine-tuned significantly increases dialog success rates. We propose new success criteria for reinforcement learning to the end-to-end dialog system as well as provide experimental analysis on a different result aspect depending on the success criteria and evaluation methods. Consequently, our model achieved the new state-of-the-art success rate of 85.4\% on corpus-based evaluation, and a comparable success rate of 81.40\% on simulator-based evaluation provided by the DSTC8 challenge.  %The recent advent of neural approaches for developing each dialog component in task-oriented dialog systems has remarkably improved, yet optimizing the overall system performance remains a challenge. In this paper, we propose an end-to-end trainable neural dialog system with reinforcement learning, named SUMBT+LaRL. The SUMBT+ estimates user-acts as well as dialog belief states, and the LaRL models latent system action spaces and generates responses given the estimated contexts. We experimentally demonstrate that the training framework in which the SUMBT+ and LaRL are separately pretrained and then the entire system is fine-tuned significantly increases dialog success rates. We propose new success criteria for reinforcement learning to the end-to-end dialog system as well as provide experimental analysis on a different result aspect depending on the success criteria and evaluation methods. Consequently, our model achieved the new state-of-the-art success rate of 85.4% on corpus-based evaluation, and a comparable success rate of 81.40% on simulator-based evaluation provided by the DSTC8 challenge."
"The massive rise user-generated web content, alongside freedom speech social media anonymity users brought increase online offensive content anti-social behavior. The consequences behavior genuine users social media become serious concern researchers Natural Language Processing related fields recent years. The shared task number 6 SemEval 2019, OffensEval , proposes model task offensive language identification hierarchically, means identifying offensive content, whether targeted, so, target offense. In OffensEval, offensive language defined ``any form non-acceptable language targeted offense, veiled direct'' includes ``insults, threats, posts containing profane language swear words'' . We participated first two subtasks OffensEval proposed approach deep model consisting Recurrent Neural Network word-level Convolutional Neural Network character-level processing. Character-level processing beneficial, offensive comments likely follow unorthodox writing styles, contain obfuscated words, irregular word separation leads tokenization issues . We also experimented two methods, Support Vector Machine TFIDF count features another SVM BERT -encoded sentences input, lower performances comparing deep model. After overviewing related work section , discuss methodology data details section , results section . In section , analyze results conclude paper section . In paper, propose end-to-end trainable task-oriented dialog system reinforcement learning consists SUMBT+ LaRL. The SUMBT+ estimates user-acts well dialog belief states, LaRL models latent system action spaces generates responses given estimated contexts. We experimentally demonstrated training framework SUMBT+ LaRL pretrained, entire system fine-tuned significantly increases dialog success rates. Besides, end-to-end fine-tuning showed supervisions next system response also encourages improve dialog state tracking, achieving performance comparable state-of-the-art joint accuracy. We trained model REINFORCE using reward levels success criteria designed. Using estimated belief states assess reward level 1 attained best success rates end-to-end corpus evaluation, whereas oracle belief states reward level 2 achieved highest automatic evaluations. The experiment results qualitative analysis simulated dialog examples present discrepancy problem corpus automatic evaluations, limitations off-line reinforcement learning dialog policy, needs advanced reward design success criteria. Our model achieved new state-of-the-art success rate 85.4\ end-to-end corpus-based evaluation MultiWOZ corpus, well outperformed challenge winner DSTC8 Track 1 challenge showing 81.4\ simulator-based evaluation. single appendix: [Proof Zonklar Equations] appendix heading use"," This paper presents the models submitted by Ghmerti team for subtasks A and B of the OffensEval shared task at SemEval 2019. OffensEval addresses the problem of identifying and categorizing offensive language in social media in three subtasks; whether or not a content is offensive , whether it is targeted  towards an individual, a group, or other entities . The proposed approach includes character-level Convolutional Neural Network, word-level Recurrent Neural Network, and some preprocessing. The performance achieved by the proposed model for subtask A is 77.93\% macro-averaged F\textsubscript{1}-score."
"A wide range Natural Language Processing tasks, Machine Translation , speech recognition, information retrieval, data mining, creating text resources low-resource languages benefit upstream task language identification. The Cuneiform Language Identification task VarDial 2019 tries address problem identifying languages dialects texts written cuneiform symbols. Identifying languages dialects cuneiform texts difficult task, since languages lack resources also problem tokenization. Although work addressing problem tokenization languages dialects, universal method tool available tokenization cuneiform texts, task depends rules language, simply cuneiform writing system syllabic well logographic one. As result, endeavors paper based character-level features. This work investigates different machine learning methods proven effective text classification compares obtained F\textsubscript{1}-score, accuracy, training time. In paper, first review literature language identification work languages written using cuneiform writing system , introduce models used tackle problem identifying languages dialects , describe training data , discuss results . % You begin brief description task overview approach. % We would like ensure future readers paper find relevant task description, data results. So, ask cite shared task report paper introduction. In paper, introduced Ghmerti team's approach problems `offensive language identification' `automatic categorization offense type' shared task 6 SemEval 2019, OffensEval. In subtask A, neural network-based model outperformed methods, including SVM word TFIDF character count features another SVM BERT-encoded tweets input. Furthermore, analysis results indicates sarcastic language, inability discern emotions anger, ethnic racial slurs constitute considerable portion errors. Such deficiencies demand larger training corpora variety features, information sarcasm, emotion, personality, etc."," Identification of the languages written using cuneiform symbols is a difficult task due to the lack of resources and the problem of tokenization. The Cuneiform Language Identification task in VarDial 2019 addresses the problem of identifying seven languages and dialects written in cuneiform; Sumerian and six dialects of Akkadian language: Old Babylonian, Middle Babylonian Peripheral, Standard Babylonian, Neo-Babylonian, Late Babylonian, and Neo-Assyrian. This paper describes the approaches taken by  \tt{SharifCL} \normalfont team to this problem in VarDial 2019. The best result belongs to an ensemble of Support Vector Machines and a naive Bayes classifier, both working on character-level features, with macro-averaged F\textsubscript{1}-score of 72.10\%."
"In recent years, growing interest hierarchical multi-label classification applied wide range applications International Patent Classification , product annotation advertising recommendation . In common flat classification problem, input sample associated single label set disjoint labels. However, HMC problem, labels organized form tree Directed Acyclic Graph input sample usually associated multiple labels, made challenging. The straight-forward approach dealing HMC problem convert flat multi-label classification problem simply ignoring relevance labels . The main disadvantage loss useful hierarchical information. Alternatively, local approach designed perform multi-label classification, classifications carried level label hierarchy . The overall classification results generated based local predictions. While hierarchical information better utilized local approaches, misclassifications easily propagated next levels . Global approaches proposed learn single global model labels reduce model size consider entire label hierarchy . These global classifiers typically built flat classifiers modifications made integrate hierarchical information labels model. Recently, algorithms combine local global approaches proposed . All algorithms introduced focus design hierarchical classifier ignoring hierarchical features may extracted important HMC well. \citet{HARNN'2019} \citet{rojas2020efficient} consider hierarchical feature extraction work. However, extraction process designed fulfilled applying typical attention mechanism whole text. Since HMC problem text may associated multiple labels hierarchy level, features extracted typical attention may diluted. We believe reasonable hypothesize label-based attention, information extraction performed based different labels different hierarchical levels, would allow model interpretable overall better performance accuracy. Given motivations, propose LA-HCN --- HMTC model label-based attention facilitate label-based hierarchical feature extraction, introduce concept mechanism component intermediate representation helps bridge latent association words labels label-based attention. \paragraph{Contribution} Main contributions work: In paper, investigated different machine learning methods, SVM neural networks, compared performance task language dialect identification cuneiform texts. The best performance achieved combination SVM naive Bayes, using character-level features. It shown characters enough obtain least 72.10\ F\textsubscript{1}-score. However, best model able achieve good result classifying dialects indicates need kinds features, word-level ones, and/or embedded transferred knowledge languages dialects used training deep models. Here conclude paper. The readers interested system performance also could learned submission.\\ You also include ideas future work."," Hierarchical multi-label text classification  has been gaining popularity in recent years thanks to its applicability to a plethora of real-world applications. The existing HMTC algorithms largely focus on the design of classifiers, such as the local, global, or a combination of them. However, very few studies have focused on hierarchical feature extraction and explore the association between the hierarchical labels and the text. In this paper, we propose a Label-based Attention for Hierarchical Mutlti-label Text Classification Neural Network  , where the novel label-based attention module is designed to hierarchically extract important information from the text based on the labels from different hierarchy levels. Besides, hierarchical information is shared across levels while preserving the hierarchical label-based information. Separate local and global document embeddings are obtained and used to facilitate the respective local and global classifications. In our experiments, LA-HCN outperforms other state-of-the-art neural network-based HMTC algorithms on four public HMTC datasets. The ablation study also demonstrates the effectiveness of the proposed label-based attention module as well as the novel local and global embeddings and classifications. By visualizing the learned attention , we find that LA-HCN is able to extract meaningful information corresponding to the different labels which provides explainability that may be helpful for the human analyst."
"Multi-task learning problem minimizing average error across tasks, measured held-out samples, motivated observation sometimes learning single model partially shared parameters performs better single-task models. In learning-to-learn setting, worry error task . Both settings apply randomly initialized base learners, well architectures pre-trained yet another task. In learning-to-learn, new task assumed come ambiguity set defined tasks. Unsurprisingly, approaches multi-task learning minimize average loss across training samples available tasks. This always lead best solution, however, since relations loss error may differ across tasks. Several off- online methods normalizing relations proposed , even this, minimizing average loss across tasks two disadvantages: Performance outlier tasks may poor ; learning-to-learn setting, minimizing average loss optimal task selection unbiased . Minimizing worst-case loss across tasks instead average loss, theory solves two problems, approach popular algorithmic fairness domain adaptation covariate shift assumptions . In multi-task settings, possible directly modify loss minimized multi-task learning , example possible common approach multi-task learning batch sampled one tasks random . We present general approach multi-task learning worst-case-aware loss minimization, instead relying automated curriculum learning . \paragraph{Contributions} We present automated curriculum learning approach robust multi-task transfer learning. Our approach general parameterizes family worst-case-aware objectives, minimax loss-proportional minimization two extremes. In series experiments GLUE multi-task benchmark , show several objectives lead better performance benchmark itself, importantly, also lead much better generalization out-of-domain data sets. %Finally, show shared models learned using worst-case-aware curriculum learning also perform better learning-to-learn settings. We proposed LA-HCN, novel algorithm HMTC, label-based attention learned text different hierarchical levels. Furthermore, local global text embeddings generated local global classification respectively. At different levels, meaningful attention learned based different labels. Comprehensive experiments demonstrate effectiveness LA-HCN, outperforms neural network-based state-of-the-art HMTC algorithms across four benchmark datasets different properties. Moreover, visualization learned label-based attentions reveals interpretability. However, practical meaning learned components well explored far may considered future work, explicit label structure may also taken consideration design components."," Multi-task transfer learning based on pre-trained language encoders achieves state-of-the-art performance across a range of tasks. Standard approaches implicitly assume the tasks, for which we have training data, are equally representative of the tasks we are interested in, an assumption which is often hard to justify. This paper presents a more agnostic approach to multi-task transfer learning, %, relying on $\alpha$-ball  which uses automated curriculum learning to minimize a new family of worst-case-aware losses across tasks. Not only do these losses lead to better performance on outlier tasks; they also lead to better performance in zero-shot and few-shot transfer settings."
"Building robust task-oriented dialogue systems challenging due complex system design limited availability human-annotated data. A dialogue agent expected learn dialogue reasoning, decision making, language generation, require large amount training data. However, collecting annotating data training dialogue system time-intensive transferable among domains. One possible workaround leverage pre-trained language model reduce human supervision. Recent progress pre-training language models shown promising alleviating data scarcity problem. Such models typically pre-trained large-scale plain text self-supervised objectives, e.g., language modeling language denoising. Fine tuning pre-trained language models improves wide range natural language processing applications, notably machine translation, personalized dialogue response generation. However, adapting pre-trained language models task-oriented dialogue systems trivial. Current state-of-the-art approaches task-oriented dialogue rely several tasks-specific modules, State Operation Predictor dialogue state tracking, CopyNet end-to-end dialogue task completion. Such modules usually absent pre-training stage. Therefore, tasks-specific architecture modifications required order adapt pre-trained language models different dialogue tasks. In work, aim simplify process transferring prior knowledge pre-trained language models improving task-oriented dialogue systems. We propose Minimalist Transfer Learning , simple yet effective transfer learning framework allows plug-and-play pre-trained sequence-to-sequence models jointly learn dialogue state tracking dialogue response generation. Unlike previous approaches, use copy mechanism ``carryover'' previous dialogue states generate new dialogue states, introduce Levenshtein belief spans models difference old states new states. In practice, MinTL first decodes updating previous dialogue state; then, updated state used search external knowledge base; finally, response decoder decodes response conditioning dialogue context knowledge base match result. MinTL easy set using different pre-trained seq2seq backbones. We conduct extensive experiments DST end-to-end dialogue response generation tasks two pre-trained seq2seq models, T5 BART. The experimental result large-scale task-oriented dialogue benchmark MultiWOZ suggests proposed method significantly improves SOTA performance full data simulated low resource setting. Our contributions summarized follows: We presented \texttt{N-LTP}, Python natural language processing toolkit Chinese. To best knowledge, first neural Chinese toolkit supports Chinese fundamental NLP tasks. \texttt{N-LTP} evaluated five fundamental Chinese NLP tasks obtains state-of-the-art performance. We hope \texttt{N-LTP} facilitate Chinese NLP research applications. In future, keep extending \texttt{N-LTP} adding new state-of-the-art models going forward. \nocite{*}"," In this paper, we propose Minimalist Transfer Learning  to simplify the system design process of task-oriented dialogue systems and alleviate the over-dependency on annotated data. MinTL is a simple yet effective transfer learning framework, which allows us to plug-and-play pre-trained seq2seq models, and jointly learn dialogue state tracking and dialogue response generation.  Unlike previous approaches, which use a copy mechanism to ``carryover'' the old dialogue states to the new one, we introduce Levenshtein belief spans , that allows efficient dialogue state tracking with a minimal generation length. We instantiate our learning framework with two pre-trained backbones: T5 and BART, and evaluate them on MultiWOZ. Extensive experiments demonstrate that: 1) our systems establish new state-of-the-art results on end-to-end response generation, 2) MinTL-based systems are more robust than baseline methods in the low resource setting, and they achieve competitive results with only 20\% training data, and 3) $Lev$ greatly improves the inference efficiency\footnote{Code available in \url{https://github.com/zlinao/MinTL}}."
"Recent advancements area generative modeling helped increase fluency generative models. However, several issues persist: coherence output semblance mere repetition/hallucination tokens training data . One reason could generation task typically construed end-to-end system. This contrast traditional approaches, incorporate sequence steps NLG system, including content determination, sentence planning, surface realization . A review literature psycholinguistics cognitive science also provides strong empirical evidence human language production process monolith . Prior approaches indeed incorporated content planning NLG system, example data-to-text generation problems well classic works include planning, based speech acts . Our work closely follows prior approaches, one crucial difference: planners based dialogue acts speech acts. Consider example Fig.. An input utterance Person B, statement , followed question , effectively responded using plans, learned generated, prior realization phase. The realization output include mention provides relief, consistent generated plan . Dialogue acts , nature, encompass wide variety realized output, hence cannot sufficiently constrain language model generation process. Research addressed issue adapting existing taxonomies towards goals . We instead use adapted extended form lexical-conceptual structures help constrain realization output effectively . Our work makes following contributions: \\ % \end{inparaenum} In paper, proposed MinTL, simple general transfer learning framework effectively leverages pre-trained language models jointly learn DST dialogue response generation. The proposed reducing DST complexity improving inference efficiency. In addition, two pre-trained Seq2Seq language models: T5 BART incorporated framework. Experimental results MultiWOZ shows that, using MinTL, systems achieve new SOTA result dialogue state tracking end-to-end response generation also improves inference efficiency. In future work, plan explore task-oriented dialogues domain-adaptive pre-training methods enhance language model backbones, extend framework mixed chit-chat task-oriented dialogue agents."," Achieving true human-like ability to conduct a conversation remains an elusive goal for open-ended dialogue systems. We posit this is because extant approaches towards natural language generation  are typically construed as end-to-end architectures that do not adequately model human generation processes.    To investigate, we decouple generation into two separate phases: planning and realization. In the planning phase, we train two planners to generate plans for response utterances. The realization phase uses response plans to produce an appropriate response. Through rigorous evaluations, both automated and human, we demonstrate that decoupling the process into planning and realization performs better than an end-to-end approach."
"A metaphor figurative form expression compares word phrase object action literally applicable helps explain idea suggest likeness analogy them. Metaphors used extensively types literature writings, especially poetry songs communicate complex feelings, emotions, visuals present text readers effectively. Metaphors ubiquitous natural language help structuring understanding world even without conscious realization presence. Given prevalence significance metaphorical language, effective detection metaphors plays essential role many natural language processing applications, example, language understanding, information extraction, sentiment analysis, etc. However, automated detection metaphorical phrases difficult problem primarily due three reasons. First, subjective component involved: metaphoricity expression may vary across humans. Second, metaphors domain context dependent. And third, lack annotated data, required train supervised machine learning algorithms facilitate automated detection accurately. Most previous approaches detection metaphorical phrases, either relied manual lexical detection requires heavily handcrafted features built linguistic resources, costly obtain greatly limits applicability used supervised machine learning based algorithms limited forms linguistic context, example using subject verb objects triplets . Although techniques automate detection metaphorical phrases, however, prediction accuracies good prediction accuracies techniques text classification tasks. Inspired recent works field NLP transfer learning, paper, present end-to-end method composed deep contextualized word embeddings, bidirectional LSTMs multi-head attention mechanism address limitations aforementioned. Our method notable sense unlike many existing approaches, requires raw text sequences input depend complex fine-tuned feature pipelines. We address task natural language generation open-ended dialogue systems. We test hypothesis decoupling generation process planning realization achieve better performance end-to-end approach. In planning phase, explore three methods generate response plans, including Symbolic Planner two learned planners, Context Attention Pseudo Self Attention models. Through linguist expert evaluation, able determine efficacy response plans towards realization. In realization phase, use Pseudo Self Attention model make use learned response plans generate responses. extbf{Our key finding two separate human crowdsourced studies decoupling realization, planning phases outperforms end-to-end No Planner system across three metrics .} In work, taken initial step towards goal replicating human language generation processes. Thorough rigorous evaluations required fully support claims, e.g., including additional metrics diverse corpora. In work, limit types GIVE, GAIN, LOSE, PERFORM. However, restrict ask action target all. Also, since symbolic planner used obtain silver standard training data, straightforward changes like adding additional lexicons would enable us generalize corpora well include additional ask types pipeline. Another natural extension would explore training planning realization phases together hierarchical process . This would, principle, validate efficacy approach."," %% Text of abstract Metaphors are ubiquitous in natural language, and their detection plays an essential role in many natural language processing tasks, such as language understanding, sentiment analysis, etc. Most existing approaches for metaphor detection rely on complex, hand-crafted and fine-tuned feature pipelines, which greatly limit their applicability. In this work, we present an end-to-end method composed of deep contextualized word embeddings, bidirectional LSTMs and multi-head attention mechanism to address the task of automatic metaphor detection. Our method, unlike many other existing approaches, requires only the raw text sequences as input features to detect the metaphoricity of a phrase. We compare the performance of our method against the existing baselines on two benchmark datasets, TroFi, and MOH-X respectively. Experimental evaluations confirm the effectiveness of our approach."
"For text editing, sequence-to-sequence framework applied text simplification , punctuation restoration , grammatical error correction , machine translation post-editing , etc. We observe current inference methods roughly grouped two categories: End-to-end Tagging . For models categories, encoders extract encode information source text sequence. Yet, goal decoders different End2end Tagging. Upon receiving encoder's hidden states comprise source text information, decoder End2end directly decodes hidden states generates completely edited target text sequence. But, decoder Tagging produces sequence editing operations, deletion insertion, later applied source text yield edited text via realization step . The mechanisms End2end Tagging illustrated Figure . In work, presented end-to-end method composed deep contextualized word embeddings, bidirectional LSTMs, multi-head attention mechanism address task automatic metaphor detection classification. Our method requires raw text sequences input depend complex fine-tuned feature pipelines. Our method established new state-of-the-art datasets metaphor detection. The Appendices part started command ; appendix sections done normal sections","  In neural text editing, prevalent sequence-to-sequence based approaches directly map the unedited text either to the edited text or the editing operations, in which the performance is degraded by the limited source text encoding and long, varying decoding steps. To address this problem, we propose a new inference method, Recurrence, that iteratively performs editing actions, significantly narrowing the problem space. In each iteration, encoding the partially edited text, Recurrence decodes the latent representation, generates an action of short, fixed-length, and applies the action to complete a single edit. For a comprehensive comparison, we introduce three types of text editing tasks: Arithmetic Operators Restoration , Arithmetic Equation Simplification , Arithmetic Equation Correction . Extensive experiments on these tasks with varying difficulties demonstrate that Recurrence achieves improvements over conventional inference methods."
"There broad consensus among grammar formalisms composition form meaning natural language resource-sensitive process, words making phrase contributing exactly resulting whole. The sentence ``the Mad Hatter offered'' ill-formed lack grammatical material, ``offer'' ditransitive verb; ``the Cheshire Cat grinned Alice cup tea'' hand ill-formed excess material, intransitive verb ``grin'' cannot accommodate. Given resource-sensitive nature language, comes surprise Linear Logic , particular intuitionistic version ILL, plays central role current logic-based grammar formalisms. Abstract Categorial Grammars Lambda Grammars use ILL ``as-is'' characterize abstract level grammatical structure surface form semantic interpretation obtained means compositional translations. Modern typelogical grammars tradition Lambek Calculus, e.g.~Multimodal TLG, Displacement Calculus, Hybrid TLG, refine type language account syntactic aspects word order constituency; ILL target logic semantic interpretation, reached homomorphism relating types derivations syntactic calculus semantic counterparts. A common feature aforementioned formalisms adoption parsing-as-deduction method: determining whether phrase syntactically well-formed seen outcome process logical deduction. This logical deduction automatically gives rise program meaning composition, thanks remarkable correspondence logical proof computation known Curry-Howard isomorphism, natural manifestation syntax-semantics interface. The Curry-Howard -terms associated derivations neutral respect particular semantic theory one wants adopt, accommodating truth-conditional view formal semantics vector-based distributional view, among others. Despite formal appeal, grammars based variants linear logic fallen favour within NLP community, owing scarcity large-scale datasets, also due difficulties aligning established high-performance neural toolkit. Seeking bridge gap formal theory applied practice, focus proof nets linear logic, lean graphical calculus away bureaucratic symbol-manipulation overhead characteristic conventional prooftheoretic presentations . Integrating proof nets recent advances neural processing, propose novel approach linear logic proof search eliminates issues commonly associated higher-order types hypothetical reasoning, greatly reducing computational costs structure manipulation, backtracking iterative processing burden standard parsing techniques . Our proposed methodology relies two key components. The first encoder/decoder-based supertagger converts raw text sentences linear logic judgements dynamically constructing contextual type assignments, one primitive symbol time. The second bi-modal encoder contextualizes generated judgement conjunction input sentence. The contextualized representations fed Sinkhorn layer, tasked finding valid permutation brings primitive symbol occurrences alignment. The architecture induced trained labeled data, assumes role formally grounded yet highly accurate parser, transforms raw text sentences linear logic proofs computational terms simply typed linear -calculus, decorated dependency annotations allow reconstruction underlying dependency graph . We propose recurrent inference method, Recurrence, edits given text sequence iteratively iteration programmer determines single step editing action interpreter executes action. Our method outperforms two inference methods, End2end Tagging, three arithmetic equation editing tasks introduced. For future work, plan apply Recurrence open-domain natural language data investigate relax need intermediate editing steps extra supervision signals. We also wish experiment applying pointer attention replace position component actions."," Linear logic and the linear $\lambda$-calculus have a long standing tradition in the study of natural language form and meaning. Among the proof calculi of linear logic, proof nets are of particular interest, offering an attractive geometric representation of derivations that is unburdened by the bureaucratic complications of conventional prooftheoretic formats. Building on recent advances in set-theoretic learning, we propose a neural variant of proof nets based on Sinkhorn networks, which allows us to translate parsing as the problem of extracting syntactic primitives and permuting them into alignment. Our methodology induces a batch-efficient, end-to-end differentiable architecture that actualizes a formally grounded yet highly efficient neuro-symbolic parser. We test our approach on {\AE}thel, a dataset of type-logical derivations for written Dutch, where it manages to correctly transcribe raw text sentences into proofs and terms of the linear $\lambda$-calculus with an accuracy of as high as $70\%$."
"Autoregressive language models functions estimate probability distribution next word sequence past words, . This requires capturing statistical dependencies words short timescales, syntactic information likely dominates , well long timescales, semantic narrative information likely dominate . Because probability distribution grows exponentially sequence length, approaches simplify problem ignoring long-range dependencies. Classical -gram models, example, assume word independent last words, typical . Hidden Markov models assume influence previous words decays exponentially distance current word . In contrast, neural network language models recurrent transformer networks include longer-range interactions, simplify problem working lower-dimensional representational spaces. Attention-based networks combine position content-based information small number attention heads flexibly capture different types dependencies within sequence . Gated recurrent neural networks compress information past words fixed-length state vector . The influence word state vector tends decay exponentially time. However, element state vector different exponential time constant, ``timescale'' , enabling gated RNNs like long short-term memory network flexibly learn many different types temporal relationships . Stacked LSTM networks reduce single layer , showing network depth insignificant influence LSTM captures temporal relationships. %Yet types networks flexibility comes cost, since models must learn shape dependencies data. %\ahcomment{rewrote substantially. think quite bit better now?} Yet networks shape temporal dependencies must learned directly data. This seems particularly problematic long-range dependencies, sparsely informative . This raises two related questions: temporal dependencies language model look like? And information incorporated neural network language model? To answer first question, look empirical theoretical work explored dependency statistics natural language. \citet{tegmark} quantified temporal dependencies English French language corpora measuring mutual information tokens function distance them. They observed mutual information decays power law, i.e.\ constant . This behavior common hierarchically structured natural languages well sequences generated probabilistic context-free grammars . %While precise shape dependency function may vary languages corpora, shall take given mathematical form follows power law. Now second question: temporal dependencies natural language follow power law, information incorporated neural network language models? To knowledge, little work explored control temporal dependencies learned attention-based models. However, many approaches proposed controlling gated RNNs, including updating different groups units different intervals , gating units across layers , explicitly controlling input forget gates determine information stored removed memory . Yet none proposals incorporate specific shape temporal dependencies based known statistics natural language. %\vvcomment{Rewrote last sentence -- claiming unclear relate theoretical stuff seems bit odd since directly control input/forget gates relate theory :D} \ahcomment{LOVE IT}%Yet unclear relate largely practical modifications theoretical properties models capture temporal dependencies. %\ahcomment{SHOULD WE SAY SOMETHING ABOUT TRANSFORMERS HERE? I'D LIKE TO, BUT I'M NOT EXACTLY SURE WHAT!} %\vvcomment{I'm tempted claim top paragraph nobody's really thought measure, let alone control, transformers encoding temporal dependencies words. So good candidate incorporating information need way measure specific, controllable mechanism group mechanismin model encode temporal dependencies words. Then say thought put RNNs...etc.} %\ahcomment{good suggestion!} In work, build framework \citet{chrono} develop theory memory mechanism LSTM language models capture temporal dependencies follow power law. This relies defining timescale individual LSTM unit based unit retains forgets information. We show theory predicts distribution unit timescales LSTM %We show theory predicts specific characteristics--the distribution timescales across LSTM units--of models trained natural English formal languages . Further, show forcing models follow theoretical distribution improves language modeling performance. These results highlight importance combining theoretical modeling understanding language models capture temporal dependencies multiple scales. %dependencies multiple timescales. %Effective language models capture statistical properties natural language, including information varies multiple timescales. For example, syntactic effects evolve timescale words, whereas semantics, emotions, narratives evolve much longer timescales tens hundreds thousands words. The importance long timescale information evident results showing neural networks outperformed classical n-gram models many language modeling benchmarks . This difference attributed networks' ability capture long timescale dependencies impossible n-gram models. Yet difficult interpret neural language models represent information different timescales, unclear timescale representations controlled yield better interpretable models. %One popular architecture neural language models recurrent neural networks, particular Long Short-Term Memory . Efforts interpret representations learned LSTMs using probing tasks shown LSTM language models capable learning short timescale information word order ~, long timescale semantic information~. Other methods attempted interpret timescale LSTM representations using predictive models brain responses natural language~. Yet question information different timescales maintained LSTM representations still satisfying answer. %One alternative interpreting representations existing models construct language models different layers groups units explicitly constrained operate different timescales. Several approaches proposed building explicitly multi-timescale models, including updating different groups units different intervals , gating units across layers , including explicit control input forget gates determine information stored removed memory . These approaches ease interpretation controlling timescales represented different units. Yet raises new concern: unlike standard LSTMs, explicitly multi-timescale models unable flexibly learn statistics natural language. This decrease performance models diminish utility. Thus, constructing explicitly multi-timescale language models important consider timescales present natural language. %\citet{tegmark} quantified distribution timescales natural language measuring mutual information tokens function distance them. They observed mutual information decays power law, common many hierarchical structures . It would desirable language model retain temporal information mimics statistics. However, clear attain power law using LSTMs, fundamentally designed decay information exponentially across time . %In work, present method control timescales information represented unit LSTM language model, resulting interpretable multi-timescale representations. Building theoretical grounding \citet{chrono}, quantify timescale represented unit using forget gate activations. We use framework analyze existing LSTM language model show different layers model retain information across time. Next, use framework construct explicitly multi-timescale language models timescale LSTM unit controlled setting forget input gate biases. To determine distribution timescales within model used prior mimics power law statistical properties natural language combination exponential timescales. Finally, show prior creates interpretable representations long short timescale information selectively routed different parts network. We introduced neural proof nets, data-driven perspective proof nets ILL, successfully employed demanding task transcribing raw text proofs computational terms linear -calculus. The terms construed constitute type-safe abstract program skeletons free interpret within arbitrary domains, fulfilling role practical intermediary text meaning. Used as-is, find direct application logic-driven models natural language inference. Our architecture marks departure parsing approaches, owing novel use Sinkhorn operator, renders fully parallel backtrack-free, also logically grounded. It general enough apply variety grammar formalisms inheriting linear logic; augmented Gumbel sampling, provide probabilistic means account derivational ambiguity. Viewed means exposing deep tecto-grammatic structure, paves way graph-theoretic approaches syntax-aware sentential meaning representations.","  Language models must capture statistical dependencies between words at timescales ranging from very short to very long. %, but how much information is needed for each timescale? Earlier work has demonstrated that dependencies in natural language tend to decay with distance between words according to a power law. However, it is unclear how this knowledge can be used for analyzing or designing neural network language models. %However, it is unclear how power law decay of information should manifest in neural network language models. In this work, we derived a theory for how the memory gating mechanism in long short-term memory  language models can capture power law decay. We found that unit timescales within an LSTM, which are determined by the forget gate bias, should follow an Inverse Gamma distribution. Experiments then showed that LSTM language models trained on natural English text learn to approximate this theoretical distribution. Further, we found that explicitly imposing the theoretical distribution upon the model during training yielded better language model perplexity overall, with particular improvements for predicting low-frequency  words. Moreover, the explicit multi-timescale model selectively routes information about different types of words through units with different timescales, potentially improving model interpretability. These results demonstrate the importance of careful, theoretically-motivated analysis of memory and timescale in language models.   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % EMNLP version %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %Although neural language models are effective at capturing statistics of natural language, their representations are challenging to interpret. In particular, it is unclear how these models retain information over multiple timescales.  %In this work, we construct explicitly multi-timescale language models by manipulating the input and forget gate biases in a long short-term memory  network. %%In this work, we quantify and control the timescale of each unit in a LSTM language model via the the input and forget gate biases.  %The distribution of timescales is selected to approximate power law statistics of natural language through a combination of exponentially decaying memory cells. %%We then design a prior based on statistical properties of natural language and construct a multi-timescale LSTM language model.  %We then empirically analyze the timescale of information routed through each part of the model using word ablation experiments and forget gate visualizations. %%Next, we propose word ablation experiments and forget gate visualizations to interpret the timescale of information routing through the different parts of a model.  %These experiments show that the multi-timescale model successfully learns representations at the desired timescales, and that the distribution includes longer timescales than a standard LSTM.  %%Moreover, it outperforms the standard model on the language modeling task on the Penn Treebank and WikiText-2 datasets, especially on rare words. \ahcomment{change last sentence to point about interpretability} %Further, information about high-, mid-, and low-frequency words is routed preferentially through units with the appropriate timescales. %Thus we show how to construct language models with interpretable representations of different information timescales.  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %Shivangi's version %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % Language models should ideally capture the statistical properties of natural language varying over multiple timescales. However, representations within language models  are challenging to interpret. Hence, it is unclear how different layers of an LSTM LM retain information over different timescales.  % % In this paper, we propose a mechanism to interpret and control the timescale of information routing through an LSTM unit. We observed that a standard LSTM LM favors representations of short timescale information . We then introduce a prior based on the statistical properties of language to control the distribution of the timescales across LSTM units to achieve an effective multi-timescale language model. In addition to this, we present a word ablation experiment and forget gate visualization to interpret the timescale of information routing through the different parts of the model. % % The proposed model learns representations of both short as well as long-timescale information. It also achieves better prediction performance than a standard LSTM LM on Penn Treebank and WikiText-2 datasets, especially on rare words."
"The advancement field Computer Vision ~ Natural Language Processing ~ last decade, introduced several interesting machine learning techniques. %problems convenient. The problems object detection~, segmentation~, image classification~ CV, machine translation~, question answering~, biomedical clinical text mining~ , speech recognition~ NLP, solved much efficiently ever before. This facilitated researchers indulge solving interdisciplinary problems demand knowledge fields. Visual Question Answering ~ emerged one problem. In VQA, task poised questions asked respect image, machine needs learn generate answers questions based learned features input image. In contrast typical CV tasks largely focus %have singular solving problems %restricted problems ~ Inception-Resnet-v2~, respectively. We fuse representations together pass specific answer prediction model leaf node. For task question classification root node, propose question segregation technique. We use Support Vector Machine ~ classifier hand-engineered word frequency-based features QS. We use machine learning technique QS, rule-based strategy suffers problem defining many rules may extend datasets~. The following examples RAdiology Data ~ show difficulty rule-based approach medical domain. Careful analysis question reveals first example expects descriptive type answer list facts indicate kidney hemorrhage , second example expects confirm presence/absence spleen . The presence anomalies question acts hindrance formation robust rules classification questions correct type. We perform experiments RAD ImageCLEF2018 VQA-Med 2018 datasets, perfectly capture problem statement intend solve. Detailed discussion dataset found Section. Experimental evaluation demonstrates promising results, showing effectiveness proposed approach. %'s efficiency. Additionally, error analysis system's outputs %error analysis shows future direction research area addressing different kinds errors. The organization paper follows. We first discuss related work VQA. Then present details methodologies implemented solve specific problem. In particular, explain proposed HQS-VQA models detail. Basically, discussed technique used question segregation module VQA components used generate query-specific answers. Details experiments along evaluation results necessary analysis reported. %We perform experiments show results qualitative quantitative analysis. Finally, conclude provide future directions work. \subsection{Motivation} The motivation behind work stemmed following facts: %of medical visual question answering listed follows: \end{adjustwidth} \end{table} \item We identify need, propose SVM-based question segregation technique segregate questions. We use information propose hierarchical deep multi-modal network generate answers. \end{itemize} \subsection{Contributions} In paper developed theory LSTM language models capture power law temporal dependencies. We showed theory predicts distribution timescales LSTM language models trained natural formal languages. We also found explicit multi-timescale models forced follow theoretical distribution give better performance, particularly long timescales. Finally, show evidence information dependent different timescales routed specific units, demonstrating unit timescales highly interpretable. This enhanced interpretability makes possible use LSTM activations predict brain data, , estimate processing timescales different brain regions . These results highlight importance theoretical modeling understanding language models capture dependencies multiple timescales. \subsubsection*{Acknowledgments} We would like thank Shailee Jain valuable feedback manuscript useful discussions, anonymous reviewers insights suggestions. Funding support work came Burroughs Wellcome Fund Career Award Scientific Interface , Intel Research Award, Alfred P. Sloan Foundation Research Fellowship. anthology \clearpage anthology","        {        Visual Question Answering in Medical domain  plays an important role in providing medical assistance to the end-users. These users are expected to raise either a straightforward question with a Yes/No answer or a challenging question that requires a detailed and descriptive answer. The existing techniques in VQA-Med fail to distinguish between the different question types sometimes complicates the simpler problems, or over-simplifies the complicated ones. It is certainly true that for different question types, several distinct systems can lead to confusion and discomfort for the end-users. To address this issue, we propose a hierarchical deep multi-modal network that analyzes and classifies end-user questions/queries and then incorporates a query-specific approach for answer prediction. We refer our proposed approach as Hierarchical Question Segregation based Visual Question Answering, in short HQS-VQA.      %   We first use the Support Vector Machine  with the hand-engineered features to classify the questions into yes/no and descriptive types.    % Then, based on the question types, we employ different strategies to provide the answer. The Yes/No type questions are treated as a binary classification problem. We generate the answer from a fixed vocabulary for the descriptive type question.     Our contributions are three-fold, viz. firstly, we propose a question segregation  technique for VQA-Med; secondly, we integrate the QS model to the hierarchical deep multi-modal neural network to generate proper answers to the queries related to medical images; and thirdly, we study the impact of QS in Medical-VQA by comparing the performance of the proposed model with QS and a model without QS. We evaluate the performance of our proposed model on two benchmark datasets, viz. RAD and CLEF18. Experimental results show that our proposed HQS-VQA technique outperforms the baseline models with significant margins. We also conduct a detailed quantitative and qualitative analysis of the obtained results and discover potential causes of errors and their solutions.         }"
"The following instructions directed authors papers submitted EMNLP 2020 accepted publication proceedings. All authors required adhere specifications. Authors required provide Portable Document Format version papers. The proceedings designed printing A4 paper. In paper, propose hierarchical multi-modal approach tackle VQA problem medical domain. In particular, use question segregation module top level hierarchy divide input questions two different types , followed individual independent models leaf level, dedicated type question segregated previous level. Our proposed approach applied related problem segregation possible require non-trivial changes architecture. We use SVM QS based requirements rigorous QS techniques implemented. To evaluate usefulness proposed model, conduct experiments two different datasets, RAD CLEF18. We also perform experiments combined data two datasets show generalisability approach. Models, trained proposed hierarchy QS, scored better, outperforming stated baseline models. It suggests questions different types learn better isolation individual learning paths. Experimental results indicate effectiveness work, depicting value VQA medical domain. We also find even simple versions model competitive. Further analysis obtained results reveals evaluation metric needs improvement evaluate VQA medical domain. For future work, plan investigate better evaluation strategy evaluating task apart devising detailed scheme QS. We also plan introduce better individual models handle leaf node tasks."," This document contains the instructions for preparing a manuscript for the proceedings of EMNLP 2020. The document itself conforms to its own specifications, and is therefore an example of what your manuscript should look like. These instructions should be used for both papers submitted for review and for final versions of accepted papers. Authors are asked to conform to all the directions reported in this document."
"The rapid development science technology world created vast amount data. In particular, growth social networks continuously creates huge amount comments posts valuable sources exploit analyze digital era. Text classification prerequisite works analyzing user opinion network environment, filtering removing malicious information, detecting criminal risk. With great potential, text classification attracted much attention experts natural language processing community worldwide. In English, easily search range text classification publications many fields. However, relatively researches done Vietnamese text. Most published articles focus binary classification. However, large amount information today requires analysis many aspects . The lack knowledge techniques Vietnamese language makes us decide conduct research classify multi-class text Vietnamese social media datasets. These datasets provided VLSP share-task publications text classification. In particular, various social media textual datasets UIT-VSMEC emotion recognition UIT-VSFC students' feedback classification HSD-VLSP hate speech detection . These datasets multi-label imbalance labels published recently. They suitable requirements would like study. The emergence deep neural networks word embeddings made text classification efficient. Pre-trained word embeddings accurately capture semantics assist deep learning models improve efficiency classification. In study, implement deep learning models CNN , LSTM variants solve classification problems. Besides, implement BERT model , state-of-the-art model many natural language processing tasks recent years. BERT trained transformer two-dimensional context . BERT contrast previous deep learning models looked text sequence left right combined left-to-right right-to-left training. To improve word representation, create normalized words dictionary, helps recognize words included pre-trained embedding represented due misspellings. As result, CNN model combined fastText's pre-trained embedding , remarkably performance Vietnamese social media datasets. Our study also proves efficiency BERT Vietnamese students' feedback dataset. Besides, combine single models increase efficiency classification. As result, ensemble model accomplishes higher results single model. Compared previous studies done datasets, models achieve better results. In work, focus problem mitigating gender bias neural dialogue models. We propose adversarial training framework Debiased-Chat reduce bias dialogue model training process. With help disentanglement model, design adversarial learning framework trains dialogue models cleverly include unbiased gender features exclude biased gender features responses. Experiments two human conversation datasets demonstrate model successfully mitigates gender bias dialogue models outperforms baselines producing engaging, diverse, gender-specific responses. In future, investigate debiasing retrieval-based dialogue models complicated pipeline-based dialogue systems. File emnlp2020.tex Based style files ACL 2020, Based style files ACL 2018, NAACL 2018/19, Based style files ACL-2015, improvements taken NAACL-2016 style Based style files ACL-2014, were, turn, based ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009, EACL-2009, IJCNLP-2008... Based style files EACL 2006 e.agirre@ehu.es Sergi.Balari@uab.es ACL 08 Joakim Nivre Noah Smith \documentclass[11pt,a4paper]{article} \usepackage[hyperref]{emnlp2020} \usepackage{times} \usepackage{latexsym} \renewcommand{\UrlFont}{\ttfamily\small} This strictly necessary, may commented out, improve layout manuscript, typically save space. \usepackage{microtype} \aclfinalcopy Uncomment line final submission \def\aclpaperid{***} Enter acl Paper ID \setlength\titlebox{5cm} You expand titlebox need extra space show authors. Please make titlebox smaller 5cm ; check camera-ready version ask change back. \newcommand\BibTeX{B\TeX} \title{Instructions EMNLP 2020 Proceedings} \author{First Author \\ Affiliation / Address line 1 \\ Affiliation / Address line 2 \\ Affiliation / Address line 3 \\ \texttt{email@domain} \\\And Second Author \\ Affiliation / Address line 1 \\ Affiliation / Address line 2 \\ Affiliation / Address line 3 \\ \texttt{email@domain} \\} \date{}"," Text classification is a popular topic of natural language processing, which has currently attracted numerous research efforts worldwide. The significant increase of data in social media requires the vast attention of researchers to analyze such data. There are various studies in this field in many languages but limited to the Vietnamese language. Therefore, this study aims to classify Vietnamese texts on social media from three different Vietnamese benchmark datasets. Advanced deep learning models are used and optimized in this study, including CNN, LSTM, and their variants. We also implement the BERT, which has never been applied to the datasets. Our experiments find a suitable model for classification tasks on each specific dataset. To take advantage of single models, we propose an ensemble model, combining the highest-performance models. Our single models reach positive results on each dataset. Moreover, our ensemble model achieves the best performance on all three datasets. We reach 86.96\% of F1-score for the HSD-VLSP dataset, 65.79\% of F1-score for the UIT-VSMEC dataset, 92.79\% and 89.70\% for sentiments and topics on the UIT-VSFC dataset, respectively. Therefore, our models achieve better performances as compared to previous studies on these datasets."
"In recent years, Transformers defined state-of-the-art performance variety NLP tasks, including machine translation language modeling. While large Transformer models learn uniquely rich representations, also highly overparameterized . Several studies therefore attempted prune Transformers training retaining much performance possible . Some methods fairly successful, achieving compression ratios 10 depending downstream task. Looking beyond task performance, however, remains unclear widely-used pruning methods affect model's learned representations. For example, pruned Transformer may translate text BLEU, pruning affect model ways unaccounted metric? Motivated question, apply recent analysis techniques study representations increasingly sparse Transformers trained MT. We perform magnitude pruning iterative, lottery-ticket fashion identify Transformers competitive sparsities drop task performance . We examine internal structures models sparsity increases, specifically addressing following questions: Using iterative magnitude pruning , train En-De Transformer retains 99.4\% BLEU 66.4\% sparsity. During IMP, obtain eight Transformer models varying levels sparsity, along original unpruned model. We probe models' representations learned linguistic knowledge eighteen auxiliary syntactic semantic tasks . We perform unsupervised comparison representations attention distributions dense sparse models, adopting metrics posed \citet{wu_similarity_2020}. Our key conclusions follows: We introduce energy-based re-ranking improve performance autoregressive neural machine translation. Still, performance gap output EBR oracle re-ranker significant. This gap indicates Joint-EBM model introduced paper cannot perfectly distinguish samples target sentences source sentence. Exploring different energy models Joint-EBM target future work reduce gap. proreweighted NMT augments autoregressive NMT energy-based models. We introduced training algorithm energy-based model translation tasks experimentally show effectiveness ERNMT single-source multi-source translation tasks. Specifically, showed multi-source ERNMT consistently improves performanc BaseNMT: . \clearpage"," Recent work on the lottery ticket hypothesis has produced highly sparse Transformers for NMT while maintaining BLEU. However, it is unclear how such pruning techniques affect a model's learned representations. By probing Transformers with more and more low-magnitude weights pruned away, we find that complex semantic information is first to be degraded. Analysis of internal activations reveals that higher layers diverge most over the course of pruning, gradually becoming less complex than their dense counterparts. Meanwhile, early layers of sparse models begin to perform more encoding. Attention mechanisms remain remarkably consistent as sparsity increases."
"ACM's consolidated article template, introduced 2017, provides consistent \LaTeX\ style use across ACM publications, incorporates accessibility metadata-extraction functionality necessary future Digital Library endeavors. Numerous ACM SIG-specific \LaTeX\ templates examined, unique features incorporated single new template. If new publishing ACM, document valuable guide process preparing work publication. If published ACM before, document provides insight instruction recent changes article template. The ``\verb|acmart|'' document class used prepare articles ACM publication --- conference journal, stage publication, review final ``camera-ready'' copy, author's version, {\itshape very} changes source. A consistent theme analysis behavioral shift early layers , occurs gradually sparsity increases. Our probing results find lower layers sparse models directly encode POS syntax information compared dense models, even though performance final encoder representations similar . Moreover, similarity analyses conclude early layer encoder hidden representations attention distributions trend closer towards respective final representations sparse models. Information-theoretically, sparse layers less maximum capacity encoding, individual layer must shoulder load final representations remain predictively salient. Conversely, overparameterized dense model compensate weak lower layer representations upper layers. Indeed, upper FC layers pruned lower FC layers , reflecting shift modeling power away higher layers. We also observe gradual loss information stored model representations weights pruned, especially later layers. Individual neurons diverge dense counterparts , causing drop overall representational complexity encoder decoder. Correspondingly, sparse models perform worse higher-order semantic tasks less relevant BLEU . The reduced overall complexity sparse representations may partially explain final layers observed closer early layers . Finally, find sparse models' attention distributions remain largely similar values dense model. This ability reduce weights attention modules maintaining nearly identical representations affirms lines work . Of three attention types, encoder-decoder pruned least , varies across sparsities, exhibits within-model, inter-layer heterogeneity . These results corroborate existing evidence unique importance . Meanwhile, decoder self-attention extremely homogenous across layers sparsities, perhaps encoder-decoder attention relevant creating rich representations. \paragraph{Limitations.} Our work focuses pruned Transformers BLEU remains similar original model. However, BLEU imperfect measure translation quality , possible pruned models actually perform worse task lower sparsities suggested BLEU. Still, think work relevant given sparse models typically held standard matching unpruned task performance. Next, emphasize work focuses solely magnitude pruning, may representative pruning methods impact Transformers. We chose style pruning primarily allows higher overall sparsity without drop performance . Further, might expected pruning entire neurons attention heads would substantially change e.g.~the distributions model's outputs, found less existing work specifically measuring effects magnitude pruning. This dearth analysis seemed particularly egregious given recent growth work unstructured sparsity . Finally, note probing classifiers: widely discussed community , probes measure correlation model outputs auxilliary information. Differences probe performance necessarily imply anything information actually uses forward pass. Especially since find evidence suggesting sparse models may encoding information across layers, possible differing structure may explain worse probe performance, opposed fundamentally weaker linguistic feature extraction. We hope future work supplements results analyzing model's encoded knowledge ways. \section{Conclusions} We evaluate unstructured pruning affects behavior Transformers task performance maintained. We use probing classifiers demonstrate pruning degrades semantic knowledge affecting BLEU, early layers sparse models better encode low-level linguistic information. Unsupervised similarity analysis reveals pruning induces representational changes encoder decoder, particularly higher layers, early sparse representations similar final representations. Meanwhile, attention distributions remain remarkably similar, even high sparsities. \section*{Acknowledgements} We thank Yonatan Belinkov Jonathan Frankle advice initial stages project. We thank Nelson F. Liu providing access preprocessed probing datasets. \clearpage \renewcommand\thefigure{\thesection\arabic{figure}} \setcounter{figure}{0} \renewcommand\thetable{\thesection\arabic{table}} \setcounter{table}{0} \section{Appendix} For initial experiments, trained linear probe mapping 1024-dimension token representations number output classes . For subsequent MLP probing, use MLP one 1024-neuron hidden layer ReLU activation. All weights trained using Adam learning rate 50 epochs 3 epochs early stopping patience. For complete task descriptions, please refer \citet{liu_linguistic_2019}. Of eighteen tasks, five pairwise, i.e. involve predicting property pair tokens. These tasks syntactic arc prediction, syntactic arc classification, semantic arc prediction, semantic arc classification, coreference resolution . For prediction tasks involving pairs tokens, input two token embeddings addition elementwise product . Because model uses Moses tokenization byte-pair encoding, source tokens preprocessed probing datasets split subtokens model. We aggregate subtoken representations averaging representations. Finally, noticed tasks smaller train/test sets displayed run-to-run variability, tasks , report averaged metrics across five replicate runs different random seeds . } & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\ \midrule LTH0 & 0.000 & 0.000 & 27.77 & 27.77 \\ LTH1 & 0.168 & 0.200 & 28.04 & 27.59 \\ LTH2 & 0.302 & 0.360 & 28.00 & 27.81 \\ LTH3 & 0.410 & 0.488 & 27.70 & 27.46 \\ LTH4 & 0.496 & 0.590 & 27.93 & 27.24 \\ LTH5 & 0.565 & 0.672 & 27.80 & 26.90 \\ LTH6 & 0.620 & 0.738 & 27.76 & 26.51 \\ LTH7 & 0.664 & 0.790 & 27.61 & 26.14 \\ LTH8 & 0.669 & 0.832 & 27.19 & 25.82 \\ LTH9 & 0.727 & 0.865 & 27.16 & 25.33 \\ \bottomrule"," With the ever-increasing growth of online recruitment data, job-resume matching has become an important task to automatically match jobs with suitable resumes. This task is typically casted as a supervised text matching problem. Supervised learning is powerful when the labeled data is sufficient. However, on online recruitment platforms, job-resume interaction data is sparse and noisy, which affects the performance of job-resume match algorithms.  \ignore{This task is typically casted as a supervised text matching problem.While supervised learning is powerful when the labeled data is sufficient and clean, the job-resume interaction in practice is usually sparse and noisy, which brings difficulties to effective text representation learning.}  To alleviate these problems, in this paper, we propose a novel multi-view co-teaching network from sparse interaction data for job-resume matching.  Our network consists of two major components, namely text-based matching model and relation-based matching model.  The two parts capture semantic compatibility in two different views, and complement each other.  In order to address the challenges from sparse and noisy data, we design two specific  strategies  to combine the two components. First, two components share the learned parameters or representations, so that the original representations of each component can be enhanced. More importantly, we adopt a co-teaching mechanism to reduce the influence of noise in training data. The core idea is to let the two components help each other by selecting more reliable training instances. The two strategies focus on representation enhancement and data enhancement, respectively.  Compared with pure text-based matching models, the proposed approach is able to learn better data representations from limited or even sparse interaction data, which is more resistible to noise in training data.  Experiment results have demonstrated that our model is able to outperform state-of-the-art methods for job-resume matching.  %In such a way, compared with pure text-based match models, the proposed approach is able to learn better representations from limited or even sparse interaction data, and is more resistible to noise in training data."
"In rule-based machine translation , linguist formalises linguistic knowledge lexicons grammar rules, used system analyse sentences source language translate them. While approach require parallel corpora training grants control translations created system, process encoding linguistic knowledge requires great amount expert time. Notable examples RBMT systems original, rule-based Systran , Lucy LT Apertium platform . Instead, corpus-based machine translation systems learn translate examples, usually form sentence-level aligned corpora. On one hand, approach generally computationally expensive offers limited control generated translations. Furthermore, feasible language pairs limited available parallel resources. On hand, parallel resources available, boasts much higher coverage targeted language pair. Examples corpus-based MT paradigms phrase-based statistical machine translation neural machine translation . In work, focused leveraging RBMT knowledge improving performance NMT systems under-resourced scenario. Namely, used information provided Lucy LT, RBMT system linguistic knowledge formalised human linguists computational grammars, monolingual bilingual lexicons. Grammars collections transformations annotated trees. Monolingual lexicons collections lexical entries, lexical entry set feature-value pairs containing morphological, syntactic semantic information. Bilingual lexicon entries include source-target lexical correspondences and, optionally, contextual conditions actions. The Lucy LT system divides translation process three sequential phases: analysis, transfer, generation. During analysis phase, source sentence morphologically analysed using lexicon identifies surface form plausible morphological readings. Next, Lucy LT chart parser together analysis grammar consisting augmented syntactic rules extracts underlying syntax tree structure annotates it. The transfer generation grammars applied succession tree, undergoes multiple annotations transformations add information equivalences target language adapt source language structures appropriate ones target language. Finally, terminal nodes generation tree assembled translated sentence. We focused analysis phase, special interest two features used: morphological category inflexion class classes lexical entries. %%% NE/TERM Additionally, focused two language phenomena easily addressable using RBMT present challenge using corpus-based MT: named entities terminological expressions. A named entity word sequence words unequivocally refer real-world object, proper nouns, toponyms, numbers dates. In context MT, NEs present different challenges. For example, English sentence starts word Smith, know priori dealing name profession, translated, proper noun may left untranslated, maybe transliterated different script. A second issue may arise using subword units: word-level models may accidentally preserve out-of-vocabulary NE, subword level model generate translation it. NEs one main out-of-vocabulary word classes, often cause translation problems seriously affect meaning sentence . Similarly, terminological expression consist single word sequence words may different meaning depending context domain appear. Hence, translation term might different depending context domain. Moreover, different contexts domains may impose additional restrictions language used, different modes use active passive voice, presence particular terminology may suggest translation acceptable even meaning source sentence preserved. Accurate terminology translation crucial produce adequate translations . In work extend analyse injection morphological information technique proposed previous word propose approach NEs terminology rely particular technology applied MT approach using kind resource detect translate NEs terminological expressions. To test proposed approach, focused English-Spanish , English-Basque, English-Irish English-Simplified Chinese language pairs under-resourced scenario, using corpora around one million parallel entries per language pair domain. Additional test sets contain several examples terms, NEs rich morphology also selected used explore performance proposed approaches. Results suggest that, obtaining results statistically significantly different baseline several scenarios, proposed approaches show appropriate behaviours keeping passive voice characteristic domains. %Results suggested adding morphological information source language effective using subword units particular setting. This paper presented multi-view co-teaching network able learn sparse, noisy interaction data job-resume matching. We considered two views developing matching algorithm, namely text- relation-based models. Furthermore, two models integrated unified approach able combine merits task. We designed two strategies model integration, namely representation enhancement data enhancement. Representation enhancement referred sharing learned parameters representations across two models; data enhancement referred process filtering re-weighting training instances according quality, implemented co-teaching algorithms. Extensive experiments showed proposed approach able achieve better matching performance sparse noisy interaction data comparing several competitive baselines. In paper, focus macro interaction behaviors, \ie acceptation interview rejection. While, intuitive kinds micro interactive actions also useful matching task, click dwell time. We investigate topic develop comprehensive interaction model. Besides, also consider applying approach categories study domain adaptation problem across different categories."," Rule-based machine translation is a machine translation paradigm where linguistic knowledge is encoded by an expert in the form of rules that translate text from source to target language. While this approach grants extensive control over the output of the system, the cost of formalising the needed linguistic knowledge is much higher than training a corpus-based system, where a machine learning approach is used to automatically learn to translate from examples. In this paper, we describe different approaches to leverage the information contained in rule-based machine translation systems to improve a corpus-based one, namely, a neural machine translation model, with a focus on a low-resource scenario. Three different kinds of information were used: morphological information, named entities and terminology. In addition to evaluating the general performance of the system, we systematically analysed the performance of the proposed approaches when dealing with the targeted phenomena. Our results suggest that the proposed models have limited ability to learn from external information, and most approaches do not significantly alter the results of the automatic evaluation, but our preliminary qualitative evaluation shows that in certain cases the hypothesis generated by our system exhibit favourable behaviour such as keeping the use of passive voice. %Our results suggest that adding morphological information to the source language is as effective as using subword units in this particular setting."
"% Spoken Language Understanding technology plays crucial part goal-oriented dialogue systems. It typically involves intent detection slot filling tasks. As names imply, intent detection aims identify users intents, slot filling focuses capturing semantic constituents user utterances . As shown Fig., given user query ook restaurant next fall 5, sampled SNIPS dataset , intent BookRestaurant assigned whole sentence, token sentence corresponds one specific slot type. Due process interdependence SLU subsequent dialogue components, dialogue manager natural language generator, performance two tasks, i.e., ID SF, determines upper limit utility dialogue system . Intuitively, intent detection slot filling associated , observed Fig.. For instance, intent utterance extit{PlayMusic, slots utterance likely artist rather cuisine, vice versa % . . As accumulation annotated queries, co-occurrence characteristic slot tags intent labels become prominent perceptible, providing hints mutual dependence ID SF. Hence, promising achieve complementary effect modeling two tasks joint fashion sharing knowledge them. % proposed using CNN based triangular CRF joint intent detection slot filling. % Some works simply rely shared parameters model co-occurrence characteristic implicit way. Some works proposed model intent-slot relation sharing parameters, outperforming previous separated models large margin. % With rise RNN-based methods attention mechanisms, practice working relationship intents slots joint models likely get sophisticated. More recently, gate mechanism attention mechanism also introduced RNN-based models , provides new perspective joint ID SF modeling. % proposed using slot-gated mechanism enhance slot filling performance intent information. To take one step further, proposed Stack-Propagation Framework incorporate token-level intent information better guide slot prediction process. This stacking neural network model could provide better interpretability slot-gated mechanism. However, methods still suffer various limitations. For one thing, local context information fully exploited models, ignoring intuition local context useful architectural inductive prior SF. For another thing, methods fail take full advantage supervised signals due implicit unidirectional modeling style intent-slot relations. Those limitations hinder improvement SLU systems, especially overall accuracy, highly depends joint performance ID SF. In work, propose novel Parallel Interactive Network address issues. For first issue, Gaussian self-attentive encoder introduced better capture local structure contextual information token, incorporates valuable inductive prior knowledge SF. For second issue, design Intent2Slot module Slot2Intent module model bidirectional information flow SF ID. Specifically, inspired Dual Process Theory neurocognitive science, divide information processing modules two stages: implicit interaction stage explicit interaction stage. These two stages correspond two different processing styles human brain operates: implicit , unconscious learning explicit , conscious learning. In implicit interaction stage, relationships intents slots implicitly captured parameters shared encoder, utilized intuitive decoders obtain token-level intent distribution slot label distribution. In explicit interaction stage, distribution information obtained former stage explicitly utilized rational decoders reduce solution space. Finally, cooperation mechanism, comprehensively considers information two stages, performed reduce prediction bias thereby improve precision accuracy model predictions. To verify effectiveness proposed method, conduct experiments two real-world datasets, i.e., ATIS SNIPS , popularly used benchmarks recent works. Empirical results show method achieves competent performance intent error rate, slot F1-score, sentence-level semantic frame accuracy compared baselines. % ert In addition, Bidirectional Encoder Representation Transformer explored improve performance model. In summary, key contributions follows: In work, explored use rule-based machine translation knowledge improve performance neural machine translation models under-resourced scenario, showing models limited ability learn external information. adding morphological information source language effective using subword units particular setting. We also found RBMT translations often adequate BLEU TER poorly reflected this, often scoring worse incorrect NMT-generated translations. We also tested different approaches inject named entities terminological expressions contained RBMT model NMT. The approaches treat NMT model black box, is, way need know modify inner workings system, thus applicable model, implementation architecture. Only approaches injecting terminology word-based models improved baseline, albeit statistically significantly. In scenarios, use approaches led translations that, significantly different automatic evaluation score, appear closer style targeted text; namely, case terminology translation, strategies managed retain passive voice corpus. One paths future work focus sophisticated extraction RBMT knowledge. Namely, plan use transfer rules improve performance NMT model. One paths future work focus extraction RBMT knowledge inclusion transfer rules improve performance NMT model. The model trained following structure parse tree able properly deal information, generally performed worse rest; integrating information differently might produce better results. Use second encoder RBMT output input. A second path using approaches modify architecture neural network. For example, using multiple encoders take source sentence output RBMT system. This approach used improve performance NMT. As previously mentioned, corpus-based MT gives limited control output user, especially dealing homographs terminology; instead, RBMT gives total control. Combining source sentence RBMT output contains user-selected translations might lead improvements domain-specific low resource scenarios. A second improvement path would using multiple encoders. This approach used improve performance NMT, but, scenario, one inputs would output RBMT system. As previously mentioned, corpus-based machine translation gives limited control output user, specially dealing homographs terminology; instead, RBMT gives total control. Combining source sentence RBMT output contains user-selected translations might lead improvements domain-specific low resource scenarios. Use sources information . Finally, also plan leverage information contained freely available RBMT systems, Apertium, contains features similar ones used work. While Apertium shallow-transfer system,>Apertium deep transfer meaning less syntactic information, features similar ones used work available Apertium."," %  Spoken Language Understanding  is an essential part of the spoken dialogue system, which typically consists of intent detection  and slot filling  tasks. Recently, recurrent neural networks  based methods achieved the state-of-the-art for SLU. It is noted that, in the existing RNN-based approaches, ID and SF tasks are often jointly modeled to utilize the correlation information between them. However, we noted that, so far, the efforts to obtain better performance by supporting bidirectional and explicit information exchange between ID and SF are not well studied. % However, we note that, so far, the explicit and bidirectional information flow for ID and SF tasks has not been explored to improve the performance of SLU.  % In addition, the utilization of the local context information will enhance the performance of SF.  In addition, few studies attempt to capture the local context information to enhance the performance of SF. Motivated by these findings, in this paper, Parallel Interactive Network  is proposed to model the mutual guidance between ID and SF. Specifically, given an utterance, a Gaussian self-attentive encoder is introduced to generate the context-aware feature embedding of the utterance which is able to capture local context information. Taking the feature embedding of the utterance, Slot2Intent module and Intent2Slot module are developed to capture the bidirectional information flow for ID and SF tasks. Finally, a cooperation mechanism is constructed to fuse the information obtained from Slot2Intent and Intent2Slot modules to further reduce the prediction bias. The experiments on two benchmark datasets, i.e., SNIPS and ATIS, demonstrate the effectiveness of our approach, which achieves a competitive result with state-of-the-art models. More encouragingly, by using the feature embedding of the utterance generated by the pre-trained language model BERT, our method achieves the state-of-the-art among all comparison approaches. %  % Spoken Language Understanding  is an essential part of the spoken dialogue system, which typically consists of intent detection  and slot filling  tasks. Recurrent neural networks  based methods have achieved the state-of-the-art in SLU field. It is noted that, in those approaches, ID and SF are often jointly modeled due to the correlation between them.  % However, most existing joint models fall short of supporting bidirectional and explicit information exchange between ID and SF, which hinders the overall improvement of SLU systems.  % In addition, few studies have taken into account the explicit attention on local context, which is a useful structural inductive prior for SF task. Motivated by these findings, in this paper, Parallel Interactive Network  is proposed to model the mutual guidance between ID and SF. Specifically, given an utterance, we introduce a gaussian self-attentive encoder to extract context-aware features aiming at enhancing local structure information. Then these features are simultaneously fed to the Slot2Intent module and Intent2Slot module to build two-stage interactions where the semantic knowledge is both implicitly and explicitly shared between ID and SF tasks. Finally, a cooperation mechanism is proposed to fuse the information obtained from the two-stage interaction and further reduce the prediction bias. % The experiments on two benchmark datasets, i.e., SNIPS and ATIS, demonstrate the effectiveness of our approach, which achieves a competitive result with state-of-the-art models. % More encouragingly, by incorporating our approach to the pre-trained language model BERT, we outperform all comparison approaches and establish the new state-of-the-art performances in terms of slot F1-score and overall accuracy."
"Task-oriented dialogue systems designed help users achieve predefined goals, booking restaurants movie recommendations via natural language interactions. These systems deeply connected external Knowledge Bases since system responses guided output KB dialogue history. The current state-of-the-arts end-to-end pipelined systems rely Dialogue State Tracking Speech Act annotations. Aside annotation cost, knowingly high, pipelined systems must predict valid DST querying KB, execute query, generate response template, finally fulfill retrieved information. The resulting systems usually overly complicated, require multiple steps, including direct interaction KB. On end spectrum, end-to-end trainable models use KB dialogue history input, directly generate system responses. Most implementations use either Gold KB input intermediate API call retrieve part KB . These systems require least DST annotation generating API calls select gold KB. Moreover, even advanced transformer architecture, end-to-end models struggle input becomes large. For example, MWOZ, 22K entities one domains. Interested readers refer Appendix C overview different task-oriented methodologies. On hand, \citet{petroni2019language} discovered simple yet effective way query factual knowledge BERT. Later on, \citet{roberts2020much} fine-tuned pre-trained language model, T5, question-answers pairs, without letting model access external context knowledge. These results suggest actual knowledge stored model parameters. However, task-oriented dialogue systems, KB entities appear news articles Wikipedia, e.g., hotel addresses postcodes, thus aforementioned methods cannot straightforwardly applied, especially KB dynamically changes . In paper, propose method store KB directly model parameters using novel Knowledge Embedded approach. The resulting model use DST template responses, KB input inference time, used dynamically changing KBs via fine-tuning. The KE approach consists newly defined user goal query generates equivalents KE dialogues KB using minimal annotation effort. Figure shows high level overview approach. To verify effectiveness proposed methodology, extensively experiment, using automatic human metrics, five task-oriented datasets small, medium, large KBs. Our experiments show end-to-end models effectively embed knowledge bases parameters achieve competitive performance five datasets. % Additionally, show end-to-end models perform well pipelined modularized systems uses DST S-ACT. In paper, propose novel Parallel Interactive Network jointly modeling intent detection slot filling. In model, Gaussian self-attentive encoder first introduced better capture local context information utterance, two modules introduced model mutual guidance ID SF. Finally, cooperation mechanism proposed improve performance robustness proposed PIN. Experiment results two benchmark datasets show proposed PIN achieves competent performance compared baselines, demonstrating effectiveness proposed PIN. In addition, incorporating pre-trained language model BERT, method achieves state-of-the-art among comparison approaches. For future work, extend model handle cold start problem data samples provided training process. conference papers normally appendix use section* acknowledgment"," %Task-Oriented Dialogue Systems are either modularized with separate dialog %state tracking  and management steps, or end-to-end trainable. In either case, %, and they can be very large. Task-oriented dialogue systems are either modularized with separate dialogue state tracking  and management steps or end-to-end trainable. In either case, the knowledge base  plays an essential role in fulfilling user requests. Modularized systems rely on DST to interact with the KB, which is expensive in terms of annotation and inference time. End-to-end systems use the KB directly as input, but they cannot scale when the KB is larger than a few hundred entries. In this paper, we propose a method to embed the KB, of any size, directly into the model parameters. The resulting model does not require any DST or template responses, nor the KB as input, and it can dynamically update its KB via fine-tuning. We evaluate our solution in five task-oriented dialogue datasets with small, medium, and large KB size. Our experiments show that end-to-end models can effectively embed knowledge bases in their parameters and achieve competitive performance in all evaluated datasets\footnote{Code available in \url{https://github.com/HLTCHKUST/ke-dialogue}}. % The resulting model do not access any external resource during the user interaction, and do not require any KB as input. % to learn to embed structured knowledge of any size directly with model parameters. % % We propose to fine-tune large pre-trained models for task-oriented dialog system with our approach to learning task-specific structured knowledge.   %This has the advantage of  as part of the input nor as an external source during the user interaction."
"Open domain question answering~ involves finding answers questions open corpus. The task led growing interest scalable end-to-end retrieval systems question answering. Recent neural retrieval models shown rapid improvements, surpassing traditional information retrieval~ methods BM25. When QA formulated reading comprehension task, cross-attention models like BERT achieved better-than-human performance benchmarks Stanford Question Answering Dataset . Cross-attention models especially well suited problems involving comparisons paired textual inputs, provide early fusion fine-grained information within pair. This encourages careful comparison integration details across within two texts. However, early fusion across questions answers poor fit retrieval, since prevents pre-computation answer representations. Rather, neural retrieval models independently compute embeddings questions answers typically using dual encoders fast scalable search. Using dual encoders results late fusion within shared embedding space. For machine reading, early fusion using cross-attention introduces inductive bias compare fine grained text spans within questions answers. This inductive bias missing single dot-product based scoring operation dual encoder retrieval models. Without equivalent inductive bias, late fusion expected require additional training data learn necessary representations fine grained comparisons. To support learning improved representations retrieval, explore supervised data augmentation approach leveraging complex classification model cross-attention question-answer pairs. Given gold question passage pairs, first train cross-attention classification model supervisor. Then collection questions used mine potential question passage pairs supervision cross-attention model. The retrieval model training benefits additional training pairs annotated graded predictions cross-attention model augmenting, existing gold data. Experiments reported MultiReQA-SQuAD MultiReQA-NQ, retrieval models establishing significant improvements Precision Mean Reciprocal Rank metrics. In paper, propose learn KB directly model parameters using novel Knowledge Embedded approach, fundamentally different giving KB input using DST querying KB. We demonstrate approach scalable different KB sizes used dynamically changing KBs via fine-tuning. Automatic human evaluations confirm models embedded KBs achieve competitive performance evaluated datasets. Finally show, first time, end-to-end models perform well pipelined modularized systems MWoZ single domain dataset."," Neural models that independently project questions and answers into a shared embedding space allow for efficient continuous space retrieval from large corpora. Independently computing embeddings for questions and answers results in late fusion of information related to matching questions to their answers. While critical for efficient retrieval, late fusion underperforms models that make use of early fusion . We present a supervised data mining method using an accurate early fusion model to improve the training of an efficient late fusion retrieval model. We first train an accurate classification model with cross-attention between questions and answers. The accurate cross-attention model is then used to annotate additional passages in order to generate weighted training examples for a neural retrieval model. The resulting retrieval model with additional data significantly outperforms retrieval models directly trained with gold annotations on Precision at $N$  and Mean Reciprocal Rank ."
"Topic models, Latent Dirichlet Allocation , aim discover underlying topics semantic structures text collections. Due interpretability effectiveness, LDA extended many Natural Language Processing tasks . Most models employ mean-field variational inference collapsed Gibbs sampling model inference result intractable posteriors. However, inference algorithms model specific require dedicated derivations. To address limitation, neural topic models black-box inference explored, flexible training schemes. Inspired variational autoencoder , \citet{miao2016nvdm} proposed Neural Variational Document Model interprets latent code VAE topics. Following way, \citet{srivastava2017prodlda} adopted logistic normal prior rather Gaussian mimic simplex properties topic distribution. Logistic normal Laplace approximation Dirichlet distribution . However, logistic normal exhibit multiple peaks vertices simplex Dirichlet distribution. Therefore, less capable capturing multi-modality crucial topic modeling . To overcome limitation, \citet{wang2019atm} proposed Adversarial-neural Topic Model , topic model based Generative Adversarial Networks sampling topics directly Dirichlet distribution impose Dirichlet prior. ATM employs generator transforming randomly sampled topic distributions word distributions, adversarially trained discriminator estimating probability word distribution came training data rather generator. Although ATM shown effective discovering coherent topics, used induce topic distribution given document due absence topic inference module. Such limitation hinders application downstream tasks, text classification. Moreover, ATM fails deal document labels help extract coherent topics. For example, document labeled `sports' likely belongs topics `basketball' `football' rather `economics' `politics'. To address limitations ATM, propose novel neural topic modeling approach, named Topic Modeling Cycle-consistent Adversarial Training . In ToMCAT, topic modeling cast transformation topic distributions word distributions. Specifically, transformation topic distributions word distributions used interpret topics, reverse transformation used infer underlying topics given document. Under formulation, ToMCAT employs generator transform topic distributions randomly sampled Dirichlet prior corresponding word distributions, encoder reversely transform documents represented word distributions topic distributions. To encourage generator/encoder produce realistic target samples, discriminators word/topic distributions introduced enable adversarial training. Additional cycle-consistency constraints utilized align learning encoder generator prevent contradicting other. Furthermore, documents labels, propose sToMCAT introduces extra classifier regularize topic modeling process. The main contributions paper are: In paper, propose novel approach making use early fusion classification model improve late fusion retrieval models. The early fusion model used supervised data mining augments training data later model. The proposed approach mines 53\ ~ 12\ ~ examples MultiRQA-NQ MultiRQA-SQuAD, respectively. The resulting retrieval models improve +8.6\ +1.0\ P@1 NQ SQuAD, respectively. The current pipeline assumes exists annotated in-domain question answer pairs train cross-attention model. With strong general purpose cross-attention model, supervised data mining method could modified train in-domain retrieval models without gold question answer pairs. We leave direction future work.","   Advances on deep generative models have attracted significant research interest in neural topic modeling.   The recently proposed Adversarial-neural Topic Model models topics with   an adversarially trained generator network   and employs Dirichlet prior to capture the semantic patterns in latent topics.   It is effective in discovering coherent topics but unable to infer topic distributions for given documents   or utilize available document labels.   To overcome such limitations, we propose Topic Modeling with Cycle-consistent Adversarial Training    and its supervised version sToMCAT.   ToMCAT employs a generator network to interpret topics and an encoder network to infer document topics.   Adversarial training and cycle-consistent constraints are used to   encourage the generator and the encoder to produce realistic samples that coordinate with each other.   sToMCAT extends ToMCAT by incorporating document labels   into the topic modeling process to help discover more coherent topics.   The effectiveness of the proposed models is evaluated on unsupervised/supervised topic modeling and   text classification.   The experimental results show that our models can produce both coherent and informative topics,   outperforming a number of competitive baselines."
"Probabilistic topic models tools discovering main themes large corpora. The popular Latent Dirichlet Allocation variants effective extracting coherent topics interpretable manner, usually cost designing sophisticated model-specific learning algorithm. Recently, neural topic modeling utilizes neural-network-based black-box inference main research direction field. Notably, NVDM employs variational autoencoder model topic inference document generation. Specifically, NVDM consists encoder inferring topics documents decoder generating documents topics, latent topics constrained Gaussian prior. \citet{srivastava2017prodlda} argued Dirichlet distribution appropriate prior topic modeling Gaussian NVDM proposed ProdLDA approximates Dirichlet prior logistic normal. There also attempts directly enforced Dirichlet prior document topics. W-LDA models topics Wasserstein autoencoders framework achieves distribution matching minimizing Maximum Mean Discrepancy , adversarial topic model directly generates documents Dirichlet prior process adversarially trained discriminator framework Generative Adversarial Network . Recently, due effectiveness Graph Neural Networks embedding graph structures, surge interests applying GNN natural language processing tasks . For example, GraphBTM neural topic model incorporates graph representation document capture biterm co-occurrences document. To construct graph, sliding window document employed word pairs window connected. A limitation GraphBTM word relationships considered ignoring document relationships. Since topic possessed subset documents corpus, believe topical neighborhood document, i.e., documents similar topics, would help determine topics document. To end, propose Graph Topic Model , neural topic model corpus represented document relationship graph documents words corpus nodes connected based document-word co-occurrences. In GTM, topical representation document node aggregated multi-hop neighborhood, including document word nodes, using Graph Convolutional Network . As GCN able capture high-order neighborhood relationships, GTM essentially capable modeling word-word doc-doc relationships. In specific, relationships relevant documents established shared words, desirable topic modeling documents belonging one topic typically similar word distributions. The main contributions paper are: We presented ToMCAT, neural topic model adversarial cycle-consistent objectives, supervised extension, sToMCAT. ToMCAT employs generator capture semantic patterns topics encoder encode documents corresponding topics. sToMCAT incorporates document labels topic modeling. The effectiveness ToMCAT sToMCAT verified experiments topic modeling text classification. In future, plan extend model cope external word document semantics. It would also interesting explore alternative architectures CycleGAN formulation topic modeling.","   Graph Neural Networks    that capture the relationships between graph nodes via message passing   have been a hot research direction   in the natural language processing community.   In this paper, we propose Graph Topic Model , a GNN based neural topic model   that represents a corpus as a document relationship graph.   Documents and words in the corpus become nodes in the graph and   are connected based on document-word co-occurrences.   By introducing the graph structure,   the relationships between documents are established through their shared words   and thus the topical representation of a document is enriched by   aggregating information from its neighboring nodes using graph convolution.   Extensive experiments on three datasets were conducted   and the results demonstrate the effectiveness of the proposed approach."
"% {\color{red}jiaqi: outlines} % \ys{Need put Covid information here. Logic need covid 19 rather information so. } In work, report system architecture results team TEST\_POSITIVE competition W-NUT 2020 sharred Task-3: extracting COVID-19 event Twitter. Since February 2020, pandemic COVID-19 spreading world, posing significant threat mankind every aspect. The information sharing pandemic critical stopping virus spreading. With recent advance social networks machine learning, able automatically detect potential events COVID cases, identify key information prepare ahead. % \kenneth{I would probably make explicit ``this paper reports system architecture results team ABC XYZ competition IMWUT 2020''.} % Users share wide range information social networks. Large platforms, Twitter Facebook, provide sufficient user-generated content natural language processing applications. For example, massive tweet data posted users nourished variety applications, e.g. sentiment analysis ~, disaster monitoring ~, event extraction ~ etc. We interested COVID-19 related event extraction tweets. With prevalence coronavirus, Twitter valuable source news information. Twitter users share COVID-19 related topics personal narratives news social media . The information could helpful doctors, epidemiologists, policymakers controlling pandemic. However, manual extracting useful information tremendous amount tweets impossible. Hence, aim develop system automatically extract structured knowledge Twitter. % \ys{According Chieh-Yang, using global model solved issue limited annotation, using various types tasks use event data training.} Extracting COVID-19 related events Twitter non-trivial due following challenges: \\ How deal limited annotations heterogeneous events subtasks?. The creation annotated data relies completely human labors, thus limited amount data obtained event categories. There variety types events subtasks. % Due sparsity positive samples, % \cc{why due sparsity positve samples} annotation cannot scale properly thus limited amount data obtained. % The training dataset relies manual annotation. Hence, obtain limited number training data. Many existing works solve low resource problem different approaches, inlcuding crowdsourcing , unsupervised training , multi-task learning . Here adopt multi-task training paradigm benefit inter-event intra-event information sharing. In way, \ours learns shared embedding network globally events data. In way, implicitly augment dataset global training fine-tuning language model. % events subtasks share similarities % make use fundamental relations across different subtasks events learning global embedding network. % Heterogeneous types events subtasks. How make type-aware predictions? Existing work encode information different subtask types model, could useful suggesting candidate slot entity type. In order make type-aware predictions, propose NER-based post-processing procedure end \ours pipeline. We use NER automatically tag candidate slots remove candidate whose entity type match corresponding subtask type. For example, shown Figure, subtask ``Who'', ``my wife's grandmother'' valid candidate slot, ``old persons home'', tagged location entity, would replaced ``Not Specified'' post-processing. % UK valid slot subtask ho,as ho would require human-related descrip-tion K tagged location-related entityby NER. % % % trains % tackles event separately trains multiple models different events. % \cy{Didn't see reason become challenge.} % To tackle aforementioned challenges, propose \underline{Jo}int \underline{E}vent Mu\underline{l}ti-task Learn\underline{in}g model. % Built upon joint event multi-task learning framework, \ours benefits training data across event types. % In way, implicitly augment dataset global training fine-tuning embedding parameters. % Furthermore, design type-aware post-processing step automatically remove predictions whose entities match corresponding subtask types leveraging named entity recognition . % For example, ``UK'' valid slot subtask ``who'', ``who'' would require human-related description ``UK'' tagged location-related entity NER. % \kenneth{This example quite confusing. Need make clear.} % For example, predicted slot subtask ``who'' tagged location related entity, invalidate prediction ``Not Specified''. % In summary, \ours enabled following technical contributions:\\ % % % covid-19 wide spreading % To automatically extract structured knowledge events % related COVID-19 Twitter useful epidemiologists, journalist policymakers. % Challenges: Noisy text Twitter; Limited training data; % In work, propose joint event multi-task learning model noisy text slot filling tasks limited training data. % Our Contributions: % % We introduced Graph Topic Model, neural topic model incorporates corpus-level neighboring context using graph convolutions enrich document representations facilitate topic inference. Both quantitative qualitative results presented experiments demonstrate effectiveness proposed approach. In future, would like extend GTM corpora explicit doc-doc interactions, e.g., scientific documents citations social media posts user relationships. Replacing GCN GTM advanced graph neural networks another promising research direction.","  The competition of extracting COVID-19 events from Twitter is to develop systems that can automatically extract related events from tweets. The built system should identify different pre-defined slots for each event, in order to answer important questions . To tackle these challenges, we propose the \underline{Jo}int \underline{E}vent Mu\underline{l}ti-task Learn\underline{in}g  model. Through a unified global learning framework, we make use of all the training data across different events to learn and fine-tune the language model.  Moreover, we implement a type-aware post-processing procedure using named entity recognition  to further filter the predictions. \ours outperforms the BERT baseline by $17.2\%$ in micro F1.\footnote{\url{https://github.com/Chacha-Chen/JOELIN}}    % Extracting structured knowledge from Twitter is non-trivial because:  Limited annotated data: structured knowledge needs to be annotated manually;   Various types of tasks: there are different types of slot filling tasks for different events and subtasks.   % To tackle these challenges, we propose \underline{Jo}int \underline{E}vent Mu\underline{l}ti-task Learn\underline{in}g  model. Through a unified global learning framework, we make use of all the training data across different events to learn and fine-tune the language embedding parameters.  Moreover, we implement a type-aware post-processing procedure using NER-based techniques to further filter the predictions.\footnote{\url{https://github.com/Chacha-Chen/JOELIN}} % \kenneth{ I would give one or two examples about the event types-- what is an ``event relevant to COVID-19''? It's unclear in the abstract alone.  I would probably just say the performance numbers in the abstract. What is the performance of the proposed method?} % \jq{Thanks! Kenneth}"
"In era digitization, businesses turning towards leveraging artificial intelligence techniques exploit information contained business documents. Traditional information extraction approaches utilize Natural Language Processing methods process information documents expressed form natural language text . However, documents contain rich multi-modal information includes text document layout. The document layout organises textual information different formats sections, paragraphs, tables, multi-column etc. utilising different font-types/colors/positions/sizes/styles. Further, important visual cues also indicated figures/charts/logos etc. overall document page appearance. In general, information document spans multiple pages gives rise variety complex document layouts observed scientific articles, invoices, receipts, emails, contracts, presentations, blogs, etc. Analyzing understanding documents challenging endeavor requires multi-disciplinary perspective combining NLP, computer vision , knowledge-representation learn generic document representation suitable different downstream applications . Recent approaches towards document analysis explored frameworks utilize information document text, document layout document image different capacities specific document tasks. proposed joint training document text structure task IE form-like documents, combine text image information task semantic segmentation documents. Their proposed frameworks optimize network performance respect downstream task suitable tasks. To address limitation, proposed pre-training technique based BERT transformer architecture , combine text layout information scanned documents. They showcase applicability pre-trained network different downstream tasks utilizing image information fine-tuning task. Although presents pre-trained framework learn document representation, two limitations approach - framework allows single page documents proposed pre-training tasks cannot utilize image information learning document representation. In real-world scenario, multi-page documents common different pages potentially containing different information across text, layout, image dimensions. Also, page image captures overall layout beyond appearance text tokens document. Thus, serving different documents tasks, unified pre-training framework learns generic document representation three modalities works multi-page documents necessary. In paper, propose generic document representation learning framework takes input document text, layout, image information applicable different document tasks. Specifically, encode multi-modal document information - text position embeddings similar BERT text token 2D position embeddings capture layout, text token image embeddings capture appearance, document page image position embeddings learn document representation capable handling multi-page documents. In order handle large token sequences courtesy multi-page documents, utilize Longformer model proposed backbone framework introduces attention mechanism scales linearly sequence length. Following work , utilize Masked Visual Language Modelling task document classification task enforces joint pre-training input embeddings. To ensure network learns image embeddings, introduce two additional self-supervised pre-training tasks framework - document topic modeling document shuffle prediction . Similar work , mine latent topics document text train framework predict topic distribution using document page image embeddings task DTM. On hand, DSP involves shuffling page image order keeping embeddings intact randomly sampled documents training identify document tampered with. While DSP task enforces joint pre-training image embeddings text layout embeddings, DTM task helps learn richer page image embeddings. As explored different approaches prior art , employ multi-task learning framework simultaneously train multiple objectives different pre-training tasks learn shared representations across text, layout, image modalities documents. We train network publicly available ArXiv dataset contains millions research articles spanning variety STEM domains mathematics, physics, computer science, etc. Fig. signifies applicability pre-trained embeddings different document tasks. We evaluate performance framework following tasks datasets - Form Understanding IE scanned forms Document Classification Table Token Classification Document Retrieval . We conduct exhaustive set experiments analyze performance pre-trained embeddings state-of-the-art baselines ablations framework. We're able beat SOTA baselines trained comparable dataset size network parameters tasks. In summary, main contributions work are: % We're able beat SOTA performance certain tasks achieve comparable performance cases utilizing pre-trained embeddings fine-tuning task. In summary, main contributions work In work, build \ours upon joint event multi-task learning framework. We use NER-based post-processing generate type-aware predictions. The results show \ours significantly boosts performance extracting COVID-19 events noisy tweets BERT CT-BERT baselines. In future, would like extend \ours open domain event extraction tasks, challenging requires general pipeline. \kenneth{Say one two sentence future work. What's next?} \clearpage"," In this paper, we propose a multi-task learning-based framework that utilizes a combination of self-supervised and supervised pre-training tasks to learn a generic document representation. We design the network architecture and the pre-training tasks to incorporate the multi-modal document information across text, layout, and image dimensions and allow the network to work with multi-page documents. We showcase the applicability of our pre-training framework on a variety of different real-world document tasks such as document classification, document information extraction, and document retrieval. We conduct exhaustive experiments to compare performance against different ablations of our framework and state-of-the-art baselines. We discuss the current limitations and next steps for our work and make the code available to promote future research in this direction.   % In this paper, we propose a multi-task learning-based framework that utilizes a combination of self-supervised and supervised pre-training tasks to learn a generic document representation. We design the network architecture and the pre-training tasks to incorporate the multi-modal document information across text, structure, and image dimensions and allow the network to work with multi-page documents. We showcase the applicability of our pre-training framework on a variety of different real-world document tasks such as document classification, document information extraction, document table structure detection, and document retrieval. We conduct exhaustive experiments to compare performance against different ablations of our framework and SOTA baselines.  % To the best of our knowledge, this is the first approach in which multiple pages \& token-level visual information is encoded along with text and layout during pre-training.  % Our model outperforms existing SOTA baselines pre-trained on comparable dataset sizes across various downstream tasks. We discuss the current limitations and next steps for our work and make the code available to promote future research in this direction."
"Discourse coherence subject much research Computational Linguistics thanks widespread applications . Most current methods described either stemming explicit representations based Centering Theory , deep learning approaches learn without use hand-crafted linguistic features. Our work explores third research avenue based Rhetorical Structure Theory . We hypothesize texts low/high coherence tend adhere different discourse structures. Thus, pose using even silver-standard RST features help separating coherent texts incoherent ones. This stems definition coherence - writer document needs follow specific rules building clear narrative argument structure role constituent document appropriate respect local global context, even existing discourse parsers able predict plausible structure consistent across coherent documents. However, parser difficulty interpreting given document, likely produce unrealistic trees improbable patterns discourse relations constituents. This idea first explored \citeauthor{feng-etal-2014-impact} \shortcite{feng-etal-2014-impact}, followed approach similar \citeauthor{Barzilay-Entity-Grid} \shortcite{Barzilay-Entity-Grid} estimating entity transition likelihoods, instead using discourse relations entities participate opposed grammatical roles. Their method achieved significant improvements performance even using silver-standard discourse trees, showing potential use parsed RST features classifying textual coherence. Our work, however, first develop test neural approach leveraging RST discourse representations coherence evaluation. Furthermore, \citet{feng-etal-2014-impact} tested proposal sentence permutation task, involves ranking sentence-permuted text original. As noted \citet{lai-grammerly}, accurate proxy realistic coherence evaluation. We evaluate method realistic Grammarly Corpus Of Discourse Coherence , model needs classify naturally produced text one three levels coherence. Our contributions involve: RST-Recursive, RST-based neural tree-recursive method coherence evaluation achieves 2\% state art performance GCDC 62\% fewer parameters. When ensembled current state art, namely Parseq , achieve notable improvement plain ParSeq model. We demonstrate usefulness silver-standard RST features coherence classification, establish results lower-bound performance improvements gained using RST features. We present multi-modal pre-training framework utilizes multi-task learning learn generic document representation. Our framework encodes visual, layout textual information supports real-world multi-page documents. Our network pre-trained publically available Arxiv dataset utilizing self-supervised tasks promote learning multi-modal shared representations. We fine-tune pre-trained network showcase state-of-the-art performance different document tasks document classification, information extraction document retrieval. In future, investigate pre-training large datasets PublayNet analyze performance gain different tasks explore new architecture designs enable document image tasks object detection/segmentation using framework. We present multi-modal neural network architecture utilizes multi-task learning learn generic document representation. Our proposed architecture encode multiple pages encoding visual, layout textual components ubiquitous real-world PDF documents. We finetune architecture across various downstream tasks, compare results existing baselines. Our model significantly outperforms existing baselines FUNSD, attains comparable scores RVL-CDIP, even pretrained much smaller dataset compared LayoutLM. We also demonstrate model capable table token detection document retrieval tasks. Novel approach, architecture utilize visual, layout \& textual components pretraining hence generalize better even pretrained smaller dataset. We also introduce two novel pretraining tasks helps learn richer visual representations enforces joint representation learning visual language modalities. Hence, model pretrained four pretraining tasks acheives highest performance across downstream tasks. We also conduct ablation demonstrate efficacy two proposed tasks. In future research, investigate pretraining architecture larger subset Arxiv dataset use larger PublayNet dataset . Add future work \section{Ethical Impact} The framework proposed paper learning generic document representation enables system read, understand interpret digital documents. Such framework applicable variety enterprise settings. Typical enterprise applications depend experts put hours work collecting, filtering, reading, searching analysing business documents mine useful insights business. Common examples include government officers validating user submitted documents passport application, loan officers analysing user business documents ascertain income status owner, corporate lawyers analysing contracts identify loopholes etc. For different scenarios, upside using proposed framework huge since dramatically reduces manual effort different experts conducting routine tasks. For e.g., framework fine-tuned dataset passport applications capable analysing extracting submitted fields applicant application. A system based framework deployed concerned government agency would assist officials quickly go fields approve/reject application. Additionally, officials need acquire specialised skills undergo training understand system works. On hand, difficult come scenario proposed framework ill-used without malicious intent. Users potentially utilize framework mine personal information applicants/employees enterprise documents. For e.g., corporate human resources officer could keep database applicants mining personal information submitted resumes using framework fine-tuned dataset resumes. Hence, opinion, proposed framework enables decision making different users providing document insights used positive negative impact. \section{Introduction} In era digitization, businesses turning towards leveraging artificial intelligence techniques exploit information contained business documents. Traditional information extraction approaches utilize Natural Language Processing methods process information documents expressed form natural language text . However, documents contain rich multi-modal information includes text document layout. The document layout organises textual information different formats sections, paragraphs, tables, multi-column etc. utilising different font-types/colors/positions/sizes/styles. Further, important visual cues also indicated figures/charts/logos etc. overall document page appearance. In general, information document spans multiple pages gives rise variety complex document layouts observed scientific articles, invoices, receipts, emails, contracts, presentations, blogs, etc. Analyzing understanding documents challenging endeavor requires multi-disciplinary perspective combining NLP, computer vision , knowledge-representation learn generic document representation suitable different downstream applications . Recent approaches towards document analysis explored frameworks utilize information document text, document layout document image different capacities specific document tasks. proposed joint training document text structure task IE form-like documents, combine text image information task semantic segmentation documents. Their proposed frameworks optimize network performance respect downstream task suitable tasks. To address limitation, proposed pre-training technique based BERT transformer architecture , combine text layout information scanned documents. They showcase applicability pre-trained network different downstream tasks utilizing image information fine-tuning task. Although presents pre-trained framework learn document representation, two limitations approach - framework allows single page documents proposed pre-training tasks cannot utilize image information learning document representation. In real-world scenario, multi-page documents common different pages potentially containing different information across text, layout, image dimensions. Also, page image captures overall layout beyond appearance text tokens document. Thus, serving different documents tasks, unified pre-training framework learns generic document representation three modalities works multi-page documents necessary. In paper, propose generic document representation learning framework takes input document text, layout, image information applicable different document tasks. Specifically, encode multi-modal document information - text position embeddings similar BERT text token 2D position embeddings capture layout, text token image embeddings capture appearance, document page image position embeddings learn document representation capable handling multi-page documents. In order handle large token sequences courtesy multi-page documents, utilize Longformer model proposed backbone framework introduces attention mechanism scales linearly sequence length. Following work , utilize Masked Visual Language Modelling task document classification task enforces joint pre-training input embeddings. To ensure network learns image embeddings, introduce two additional self-supervised pre-training tasks framework - document topic modeling document shuffle prediction . Similar work , mine latent topics document text train framework predict topic distribution using document page image embeddings task DTM. On hand, DSP involves shuffling page image order keeping embeddings intact randomly sampled documents training identify document tampered with. While DSP task enforces joint pre-training image embeddings text layout embeddings, DTM task helps learn richer page image embeddings. As explored different approaches prior art , employ multi-task learning framework simultaneously train multiple objectives different pre-training tasks learn shared representations across text, layout, image modalities documents. We train network publicly available ArXiv dataset contains millions research articles spanning variety STEM domains mathematics, physics, computer science, etc. Fig. signifies applicability pre-trained embeddings different document tasks. We evaluate performance framework following tasks datasets - Form Understanding IE scanned forms Document Classification Table Token Classification Document Retrieval . We conduct exhaustive set experiments analyze performance pre-trained embeddings state-of-the-art baselines ablations framework. We're able beat SOTA baselines trained comparable dataset size network parameters tasks. In summary, main contributions work are: We're able beat SOTA performance certain tasks achieve comparable performance cases utilizing pre-trained embeddings fine-tuning task. In summary, main contributions work \section{Introduction} In era digitization, businesses turning towards leveraging artificial intelligence techniques exploit information contained business documents. Traditional information extraction approaches utilize Natural Language Processing methods process information documents expressed form natural language text . However, documents contain rich multi-modal information includes text document structure. The document structure organises textual information different formats sections, paragraphs, tables, multi-column etc. utilising different font-types/colors/positions/sizes/styles. Further, important visual cues also indicated figures/charts/logos, etc. In general, information documents spans multiple pages different document structures give rise variety complex document layouts observed scientific articles, invoices, receipts, emails, contracts, presentations, blogs, etc. Analyzing understanding documents challenging endeavor requires multi-disciplinary perspective combining NLP, computer vision , knowledge-representation learn generic document representation suitable different downstream applications . Although traditional approaches document processing involve analysing textual information document classification, summarisation etc., recent approaches explored frameworks utilize information text, document structure document image different capacities specific downstream tasks. proposed joint training document text structure task IE form-like documents, combine text image information task semantic segmentation documents. These approaches propose framework objective optimizing network performance w.r.t. downstream task learn generic document representation applicable different downstream tasks. To address limitations, proposed pre-training technique based BERT transformer architecture , combine text structure information scanned documents. They incorporate modifications BERT pre-training tasks make suitable training documents showcase applicability different downstream tasks utilizing image information fine-tuning task. Although presents pre-trained framework learn document representation, two limitations approach - framework allows single page documents proposed pre-training tasks cannot utilize image information learning document representation. In real-world scenario, multi-page documents common different pages potentially containing different information across text, structure, image dimensions. Also, page image contains important visual cues different document elements tables/figures/charts, etc. overall layout beyond appearance text tokens document. Thus, serving different documents tasks, unified pre-training framework learns generic document representation three modalities works multi-page documents necessary. In paper, propose generic document representation learning framework takes input document text, structure, image information applicable different document tasks. Specifically, encode multi-modal document information - text position embeddings similar BERT text token 2D position embeddings capture structure, text token image embeddings capture appearance, document page image position embeddings learn document representation capable handling multi-page documents. In order handle large token sequences courtesy multi-page documents, utilize Longformer model proposed backbone framework introduces attention mechanism scales linearly sequence length. Following work , utilize Masked Visual Language Modelling task enforces joint pre-training text, structure, page embeddings document classification task enforces joint pre-training input embeddings. To ensure network learns overall page image embeddings, introduce two additional self-supervised pre-training tasks framework - topic modeling document shuffle prediction . Similar work , mine latent topics document text train framework predict topic distribution using document page image embeddings task DTM. On hand, DSP involves shuffling page image order keeping embeddings intact randomly sampled documents training identify document tampered with. Both tasks enforce joint pre-training page image embeddings text structure embeddings. As explored different approaches prior art , employ multi-task learning framework simultaneously train multiple objectives different pre-training tasks learn shared representations across text, structure, image modalities documents. We train network publicly available ArXiv dataset contains millions research articles spanning variety STEM domains mathematics, physics, computer science, etc. Fig. signifies applicability pre-trained embeddings different document tasks. We evaluate performance framework following tasks datasets - Form Understanding IE scanned forms Document Classification Table Token Classification Document Retrieval . We conduct exhaustive set experiments analyze performance pre-trained embeddings state-of-the-art baselines ablations framework. We're able beat SOTA baselines trained comparable dataset size network parameters tasks. In summary, main contributions work are: We're able beat SOTA performance certain tasks achieve comparable performance cases utilizing pre-trained embeddings fine-tuning task. In summary, main contributions work \def\year{2021}\relax File: formatting-instructions-latex-2021.tex release 2021.1 \documentclass[letterpaper]{article} DO NOT CHANGE THIS \usepackage[switch]{lineno} \usepackage{aaai21} DO NOT CHANGE THIS \usepackage{times} DO NOT CHANGE THIS \usepackage{helvet} DO NOT CHANGE THIS \usepackage{courier} DO NOT CHANGE THIS \usepackage[hyphens]{url} DO NOT CHANGE THIS \usepackage{graphicx} DO NOT CHANGE THIS \usepackage{fixltx2e} \urlstyle{rm} DO NOT CHANGE THIS \def\UrlFont{\rm} DO NOT CHANGE THIS \usepackage{natbib} DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT \usepackage{caption} DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT \frenchspacing DO NOT CHANGE THIS \setlength{\pdfpagewidth}{8.5in} DO NOT CHANGE THIS \setlength{\pdfpageheight}{11in} DO NOT CHANGE THIS \nocopyright PDF Info Is REQUIRED. For /Author, add authors within parentheses, separated commas. No accents commands. For /Title, add Title Mixed Case. No accents commands. Retain parentheses. \pdfinfo{ /Title /Author /TemplateVersion } Leave /Title Put actual complete title within parentheses mixed case Leave space \Title beginning parenthesis alone /Author Put actual complete list authors within parentheses mixed case. Each author comma. If name contains accents, remove them. If LaTeX commands, remove them. \usepackage{amsfonts} \copyrighttext{} DISALLOWED PACKAGES \usepackage{authblk} -- This package specifically forbidden \usepackage{balance} -- This package specifically forbidden \usepackage{color \usepackage{CJK} -- This package specifically forbidden \usepackage{float} -- This package specifically forbidden \usepackage{flushend} -- This package specifically forbidden \usepackage{fontenc} -- This package specifically forbidden \usepackage{fullpage} -- This package specifically forbidden \usepackage{geometry} -- This package specifically forbidden \usepackage{grffile} -- This package specifically forbidden \usepackage{hyperref} -- This package specifically forbidden \usepackage{navigator} -- This package specifically forbidden \indentfirst} -- This package specifically forbidden \layout} -- This package specifically forbidden \multicol} -- This package specifically forbidden \nameref} -- This package specifically forbidden \usepackage{savetrees} -- This package specifically forbidden \usepackage{setspace} -- This package specifically forbidden \usepackage{stfloats} -- This package specifically forbidden \usepackage{tabu} -- This package specifically forbidden \usepackage{titlesec} -- This package specifically forbidden \usepackage{tocbibind} -- This package specifically forbidden \usepackage{ulem} -- This package specifically forbidden \usepackage{wrapfig} -- This package specifically forbidden DISALLOWED COMMANDS \nocopyright -- Your paper published use command \addtolength -- This command may used \balance -- This command may used \baselinestretch -- Your paper published use command \clearpage -- No page breaks kind may used final version paper \columnsep -- This command may used -- No page breaks kind may used final version paper \pagebreak -- No page breaks kind may used final version paperr \pagestyle -- This command may used \tiny -- This acceptable font size. \usepackage{multirow} \usepackage[switch]{lineno} \setcounter{secnumdepth}{2} May changed 1 2 section numbers desired. The file aaai21.sty style file AAAI Press proceedings, working notes, technical reports. Title Your title must mixed case, sentence case. That means verbs , nouns, adverbs, adjectives capitalized, including words hyphenated terms, articles, conjunctions, prepositions lower case unless directly follow colon long dash \title{Towards Multi-modal, Multi-task Learning based Pre-training Framework Document Representation Learning} \author { Author Anonymous authors \\ } \iffalse Example, Single Author, ->> remove \iffalse,\fi place surrounding AAAI title use \iffalse Example, Multiple Authors, ->> remove \iffalse,\fi place surrounding AAAI title use \title{Towards Multi-modal, Multi-task Learning based Pre-training Framework Document Representation Learning} \author { Authors Subhojeet Pramanik\thanks{Equal Contribution}\textsuperscript{\rm 1}, Shashank Mujumdar\textsuperscript{*\rm 2}, Hima Patel\textsuperscript{\rm 2}\\ } \affiliations{ \textsuperscript{1}IBM Cloud, India \\ \textsuperscript{2}IBM Research, India\\ \{subhojeet,shamujum,himapatel\}@in.ibm.com } \fi"," This paper evaluates the utility of Rhetorical Structure Theory  trees and relations in discourse coherence evaluation. We show that incorporating silver-standard RST features can increase accuracy when classifying coherence. We demonstrate this through our tree-recursive neural model, namely RST-Recursive, which takes advantage of the text's RST features produced by a state of the art RST parser. We evaluate our approach on the Grammarly Corpus for Discourse Coherence  and show that when ensembled with the current state of the art, we can achieve the new state of the art accuracy on this benchmark. Furthermore, when deployed alone, RST-Recursive achieves competitive accuracy while having 62\% fewer parameters.  %This paper explores the impact of silver-standard Rhetorical Structure Theory  trees and relations on discourse coherence evaluation. We show that incorporating discourse features benefits the previous state of the art model and also propose three models based on Recursive Neural Networks. We evaluate our models on the Grammarly Corpus for Discourse Coherence , showing promising results with one model achieving new state of the art performance on discourse classification, and another nearing previous state of the art accuracy. In addition, we provide valuable insights with respect to the application and behaviour of RST relations and trees in discourse analysis, and motivate future work in this area."
"Medical code assignment categorizes clinical documents sets codes facilitate hospital management improve health record searching~. These clinical texts comprise physiological signals, laboratory tests, physician notes, International Classification Diseases coding system widely used annotation. Most hospitals rely manual coding human coders assign standard diagnosis codes discharge summaries billing purposes. However, work error-prone~. Incorrect coding cause billing mistakes mislead general practitioners patients readmitted. Intelligent automated coding systems could act recommendation system help coders allocate correct medical codes clinical notes. Automatic medical code assignment intensively researched past decades~. Recent advances natural language processing deep learning techniques inspired many methods automatic medical code assignment~. \citet{zhang2019learning} incorporated structured knowledge medical text representations preserving translational property concept embeddings. However, several challenges remain medical text understanding. Diagnosis notes contain complex diagnosis information, includes large number professional medical vocabulary noisy information non-standard synonyms misspellings. Free text clinical notes lengthy documents, usually hundreds thousands tokens. Thus, medical text understanding requires effective feature representation learning complex cognitive process enable multiple diagnosis code assignment. Previous neural methods medical text encoding generally fall two categories. Medical text modeling commonly regarded synonym recurrent neural networks capture sequential dependency. Such works include AttentiveLSTM~, Bi-GRU~ HA-GRU~. The category uses convolutional neural networks CAML~ MultiResCNN~. These methods capture locality achieved optimal predictive performance medical code assignment. Inspired generic temporal convolutional network architecture~, consider medical text modeling causal constraints, encoding current token depends previous tokens, using dilated convolutional network. We combine label attention network fine-grained information aggregation. \paragraph{Distinction Our Model} The MultiResNet currently state-of-the-art model. It applies multi-channel CNN different filters learn features concatenates features produce final prediction. In contrast, model extends TCN sequence modeling uses single filter dilation operation control receptive field. In addition, instead weight tying used TCN, customize label attention pooling extract relevant rich features. \paragraph{Our Contributions} We contribute literature three ways. We consider medical text modeling perspective imposing sequential causal constraint medical code assignment using dilated convolutions, effectively captures long sequential dependencies learns contextual representations long clinical notes. We propose dilated convolutional attention network , coupling residual dilated convolution, label attention network effective efficient medical text modeling. Experiments real-world medical data show improvement state art. Compared multi-channel CNN RNN models, model also offers smaller computational cost. In paper, explore usefulness silver-standard parsed RST features neural coherence classification. We propose two new methods, RST-Recursive Ensemble. The former achieves reasonably good performance, 2\ short state art, robust 62\ fewer parameters. The latter demonstrates added advantage RST features improving classification accuracy existing state art methods setting new state art performance modest promising margin. This signifies document's rhetorical structure important aspect perceived clarity. Naturally, improvement performance bounded quality parsed RST features could increase better discourse parsers developed. In future, exploring RST-based architectures coherence classification, well better RST ensemble schemes improving RST parsing avenues potentially fruitful research. Additional research multipronged approaches draw Centering Theory, RST deep learning together also value."," Medical code assignment, which predicts medical codes from clinical texts, is a fundamental task of intelligent medical information systems. The emergence of deep models in natural language processing has boosted the development of automatic assignment methods.  However, recent advanced neural architectures with flat convolutions or multi-channel feature concatenation ignore the sequential causal constraint within a text sequence and may not learn meaningful clinical text representations, especially for lengthy clinical notes with long-term sequential dependency. This paper proposes a Dilated Convolutional Attention Network , integrating dilated convolutions, residual connections, and label attention, for medical code assignment. It adopts dilated convolutions to capture complex medical patterns with a receptive field which increases exponentially with dilation size. Experiments on a real-world clinical dataset empirically show that our model improves the state of the art."
"The Transformer translation model , outperformed previous RNN/CNN based sequence-to-sequence models, based multi-head attention networks. The multi-head attention mechanism, computes several scaled dot-product attention parallel, efficiently parallelized sequence level RNNs , addressing drawback CNNs model contexts inside fixed window. Even though advantages parallelization multi-head attention mechanism, recent studies suggest computation scaled dot-product attention sufficiently efficient, especially handling long sequences, due quadratic increasing size attention matrix. In paper, study accelerate inference scaled dot-product attention another perspective. Specifically, propose learn hard retrieval attention attends one position sequence rather tokens simplify computation scaled dot-product attention. Since hard attention mechanism attends one token, matrix multiplication attention probabilities value sequence standard scaled dot-product attention achieved simple efficient retrieval operation. Our contributions follows: Recent years extensively studies automatic medical code assignment. Neural clinical text encoding models use CNNs extract local features RNNs preserve sequential dependency. This paper combines using dilated convolution. The dilated convolutional attention network consists dilated convolution layers, residual connections, label attention layer. The DCAN model obeys causal constraint sequence encoding learns rich representations capture label-aware importance. Through experiments MIMIC-III dataset, model shows better predictive performance state-of-the-art methods."," The Transformer translation model that based on the multi-head attention mechanism can be parallelized easily and lead to competitive performance in machine translation. The multi-head attention network performs the scaled dot-product attention function in parallel, empowering the model by jointly attending to information from different representation subspaces at different positions. Though its advantages in parallelization, many previous works suggest the computation of the attention mechanism is not sufficiently efficient, especially when processing long sequences, and propose approaches to improve its efficiency with long sentences. In this paper, we accelerate the inference of the scaled dot-product attention in another perspective. Specifically, instead of squeezing the sequence to attend, we simplify the computation of the scaled dot-product attention by learning a hard retrieval attention which only attends to one token in the sentence rather than all tokens. Since the hard attention mechanism only attends to one position, the matrix multiplication between attention probabilities and the value sequence in the standard scaled dot-product attention can be replaced by a simple and efficient retrieval operation. As a result, our hard retrieval attention mechanism can empirically accelerate the scaled dot-product attention for both long and short sequences by $66.5\%$, while performing competitively in a wide range of machine translation tasks when using for cross attention networks."
"Neural Machine Translation opened new opportunities transfer learning high-resource low-resource language pairs . While transfer learning shown great promise, transfer languages different scripts brings additional challenges. For successful transfer embedding layer, parent child model use partially overlapping vocabulary . It common merge two vocabularies aligning identical subwords randomly assigning remaining subwords child vocabulary positions parent vocabulary . This works well transfer languages use script, child language written unseen script, vocabulary positions replaced random subwords. This significantly reduces transfer embedding layer. \citet{gheini2019universal} argue romanization improve transfer languages unseen scripts. However, romanization also introduce information loss might hurt translation quality. In work, study usefulness romanization transfer many-to-many multilingual MT models low-resource languages different scripts. Our contributions following: We propose accelerate inference scaled dot-product attention learning hard retrieval attention attends one token sentence rather tokens. With one-on-one hard attention matrix, matrix multiplication attention probabilities value sequence standard scaled dot-product attention replaced simple efficient retrieval operation. Our hard retrieval attention mechanism accelerate long short sequences times fast scaled dot-product attention. In experiments wide range machine translation tasks, demonstrate using hard retrieval attention cross attention networks lead competitive performance."," Transfer learning is a popular strategy to improve the quality of low-resource machine translation. For an optimal transfer of the embedding layer, the child and parent model should share a substantial part of the vocabulary.   This is not the case when transferring to languages with a different script. We explore the benefit of romanization in this scenario. Our results show that romanization entails information loss and is thus not always superior to simpler vocabulary transfer methods, but can improve the transfer between related languages with different scripts. We compare two romanization tools and find that they exhibit different degrees of information loss, which affects translation quality. Finally, we extend romanization to the target side, showing that this can be a successful strategy when coupled with a simple deromanization model."
"Machine learning models used practice today predominantly supervised models rely large datasets labeled training. However, cost collecting maintaining labeled training data remains bottleneck training high-capacity supervised models. Data programming aims address difficulty collecting labeled data using programmatic approach weak supervision heuristics, domain experts expected provide data programs incorporating domain knowledge. Prior work data programming focuses modeling aggregating labeling functions written manually generated automatically denoise labeling functions. % However, little known user experience % writing labeling functions improve it. Writing data programs be, however, challenging time consuming. Most domain experts lay users little programming literacy, even proficient programmers, often difficult convert domain knowledge set rules writing programs. % By extending data programming programming example, bridge gap scalable training data generation domain experts. To address challenges, introduce data programming demonstration , new framework aims make creating labeling functions easier learning users' interactive visual demonstrations. DPBD moves burden writing labeling functions intelligent synthesizer enabling users steer synthesis process multiple semantic levels, providing rationales relevant labeling choices interactively filtering proposed functions. DPBD draws two lines prior research; programming demonstration example , e.g.,, aims make programming easier synthesizing based user interactions input output examples, interactive learning user-provided features rationales . We operationalize framework \system, interactive system enables accessible data programming create labeled training datasets document classification. \system automatically generates document level labeling rules span-level annotations relations specific examples provided users. Through user study conducted 10 data scientists, evaluate \system alongside manual data programming using Snorkel. We measure predictive performances models created participants two common labeling tasks, sentiment classification spam detection. We also elicit ratings qualitative feedback participants multiple measures, including ease use, ease learning, expressivity, overall satisfaction. We find \system facilitates accessible creation labeling functions without loss quality learned labeling models. Tagging token level classification text documents another widely used task benefit DPBD. Here also briefly discuss work progress \tagruler, DPBD system learns token labeling functions user interaction create training datasets tagging models. % Tagging span-level classification text documents another widely used task benefit DPBD. Here also briefly discuss work progress \tagruler, DPBD system enables interactive generation token labeling functions order create labeled training data tagging models. % On hand, \tagruler synthesizes token classification rules based users. In summary, contribute DPBD, general data independent framework learning labeling rules interactive demonstration; \system, interactive system operationalizing framework document classification tasks; comparative user study conducted data scientists performing real world tasks evaluate \system conventional data programming. We made research artifacts, including \system code demo, publicly available~. % along materials anonymized results user study % \documentclass[sigconf]{acmart} \usepackage[moderate]{savetrees} \usepackage{booktabs} % For formal tables \usepackage{listings} \usepackage{latexsym} \usepackage[sets]{cryptocode} \usepackage{amsmath} \usepackage{amssymb} \usepackage{graphicx} \usepackage{setspace} \usepackage{fullpage} \usepackage{xspace} \usepackage{xcolor} \usepackage{caption} \usepackage{subfigure} \usepackage{courier} \usepackage{enumitem} \usepackage[font=normal,skip=2pt]{caption} \usepackage{times} \usepackage{microtype} \usepackage{balance} % better equalize last page \usepackage{xcolor} \usepackage[hang,flushmargin]{footmisc} \setlength{\textfloatsep}{8pt plus 2pt minus 2.0pt} \setlength{\intextsep}{3.0pt plus 1.0pt minus 1.0pt} % \textfloatsep: 20.0pt plus 2.0pt minus 4.0pt; % \floatsep: 12.0pt plus 2.0pt minus 2.0pt; % \intextsep: 12.0pt plus 2.0pt minus 2.0pt. \renewcommand{\UrlFont}{\ttfamily\small} \renewcommand % search images % Copyright \setcopyright{none} \acmConference[]{}{} %% %% Submission ID. %% Use submitting article sponsored event. You'll %% receive unique submission ID organizers %% event, ID used parameter command. %%\acmSubmissionID{123-A56-BU3} %% %% The majority ACM publications use numbered citations %% references. The command \citestyle{authoryear} switches %% ""author year"" style. %% %% If preparing content event %% sponsored ACM SIGGRAPH, must use ""author year"" style %% citations references. %% Uncommenting %% next command enable style. %%\citestyle{acmauthoryear} %% %% end preamble, start body document source. %Conference %\acmYear{1997} %\copyrightyear{2016} %\acmArticle{4} %\acmPrice{15.00} %% These commands optional %%\acmBooktitle{Transactions ACM Woodstock conference} %\editor{Jennifer B. Sartor} %\editor{Theo D'Hondt} %\editor{Wolfgang De Meuter} \definecolor{tomato}{rgb}{1,0.2,0} \definecolor{turqoise}{rgb}{0.03, 0.91, 0.87} \definecolor{grey}{rgb}{0.4,0.4,0.4} \newif\ifnotes \notestrue \DeclareRobustCommand{\cagatay}[1]{\ifnotes{\small[\textcolor{grey}{\c{C}a\u{g}atay:}\textcolor{tomato}{#1}]}\fi} \DeclareRobustCommand{\sara}[1]{\ifnotes{\small[\textcolor{grey}{Sara:}\textcolor{turqoise}{#1}]}\fi} \DeclareRobustCommand{\subhead}[1]{#1} \DeclareRobustCommand{\system}{\mbox{\sc Ruler}\xspace} \DeclareRobustCommand{\ruler}{\mbox{\sc Ruler}\xspace} \DeclareRobustCommand{\tagruler}{\mbox{\sc TagRuler}\xspace} \DeclareRobustCommand{\snorkel}{\mbox{\sc Snorkel}\xspace} \DeclareRobustCommand{\babblelabble}{\mbox{\sc BabbleLabble}\xspace} \DeclareRobustCommand{\thenum}{ten\xspace} \newcommand{\eat}[1]{} \newcommand{\example}[1]{{\underline{Example:} #1\qed}} \newcommand{\stitle}[1]{\smallskip {#1}} \newcommand{\sstitle}[1]{\smallskip {\underline{#1}}} \DeclareRobustCommand{\subhead}[1]{#1} \newcommand{\squishlist}{ } \renewcommand{\shortauthors}{} \settopmatter{printacmref=false,printfolios=true,printccs=false} We analyzed value romanization transferring multilingual models low-resource languages different scripts. While cannot recommend romanization default strategy multilingual models transfer learning across scripts information loss inherent it, find benefits transfer related languages use different scripts. The \texttt{uconv} romanization tool outperforms \texttt{uroman} preserves information encoded original script consequently causes less information loss. Furthermore, demonstrated romanization also successful target side followed additional, learned deromanization step. We hope results provide valuable insights future work transfer learning practical applications low-resource languages unseen scripts."," % problem & importance   Data programming is a programmatic weak supervision approach to efficiently curate large-scale labeled training data. Writing data programs  requires, however, both programming literacy and domain expertise. Many subject matter experts have neither programming proficiency nor time to effectively write data programs. Furthermore, regardless of one's expertise in coding or machine learning, transferring domain expertise into labeling functions by enumerating rules and thresholds is not only time consuming but also inherently difficult.  % proposed solution  Here we propose a new framework, data programming by demonstration , to generate labeling rules using interactive demonstrations of users. DPBD aims to relieve the burden of writing labeling functions from users, enabling them to focus on higher-level semantics such as identifying relevant signals for labeling tasks.  We operationalize our framework with \system, an interactive system that synthesizes labeling rules for document classification by using span-level annotations of users on document examples.  % evidence that it works  We compare \system with conventional data programming  through a user study conducted with 10 data scientists creating labeling functions for sentiment and spam classification tasks.  We find that \system is easier to use and learn  and offers higher overall satisfaction, while providing discriminative model performances comparable to ones achieved by conventional data programming."
"Deep neural networks typically trained large amount single task data time-consuming optimization phase. This assumes distribution data points fixed. However, neural models scale complex, realistic environments prone distributional shifts adversarial data points. Online learning hand make distributional assumption naturally involves adversarial scenario. However, due larger number training parameters non-convex optimization landscape, deep neural networks hard train online settings. % data points made available time streaming fashion. \vskip -0.45in \end{wrapfigure} Meta-learning emerged promising technique fast training deep neural networks acquiring transferring knowledge across different tasks learned learning algorithm. This work proposes meta-learning approach learn sequential adaptation algorithms deep neural networks. We introduce sparse variant Meta Networks perform online continual fast adaptation deep neural networks data stream non-stationary distribution. In Sparse Meta Networks , fast-weights generated sparsely step meta-learner accumulated across multiple steps. When sparse fast-weights accumulated way, across different tasks, together act mixture multiple experts single Sparse-MetaNet model. Such sparsely generated recurrent fast-weights computationally efficient; thus applied large scale deep neural networks, also crucial maintain far past memory streaming data. To demonstrate effectiveness approach, introduce new vision based benchmark called Online Cifar. In Online Cifar setup, Sparse-MetaNet shows better flexibility less catastrophic interference, achieves best classification accuracy compared gradient based baselines. We also evaluate Sparse-MetaNet Wisconsin Card Sorting Test , simple online reinforcement learning problem adapted human cognitive test large scale language modelling benchmarks. When used along Transformer-XL adaptive language modelling, Sparse-MetaNet achieves 1.00 bpc enwik8 22.67 perplexity WikiText-103 datasets, improving upon original Transformer-XL result 1.06 bpc 24.0 perplexity, respectively. \vskip -0.45in \end{wrapfigure} Accessibility key wider adoption technology machine learning exception. Here introduced data programming demonstration , general human-in-the-loop framework aims ease writing labeling functions, improving accessibility efficiency data programming. We presented \system, DPBD system, easily generating labeling functions create training datasets document-level classification tasks. \system converts user rationales interactively expressed span-level annotations relations among labeling rules using DPBD framework. We also reported progress developing \tagruler, second DPBD system focusing labeling functions tagging. Through user study 10 data scientists performing real world labeling tasks classification, evaluated \system together conventional data programming found \system enables accessible data programming without loss performance labeling models created. We believe DPBD systems useful data scientists well subject matter experts. We release \system open source software support future applications extended research. \system prioritizes accessibility expressivity. Is trade-off inevitable? The expressivity \system enhanced extended semantic syntactic analysis document context user demonstrations. Enabling manual revision synthesized labeling functions multiple levels abstraction also useful. In context, improving expressivity \system use cases without diminishing accessibility important area future research. Deriving additional insights users limited programming proficiency would use \system another area future work, open sourcing \system step forward direction. Future research also includes developing fast search ranking algorithms experimenting different active learning strategies effectively search navigate vast joint space labeling functions data examples. In paper presented \system, data programming demonstration system easily generating labeling functions create training datasets document-level classification tasks. \system converts user rationales interactively expressed span-level annotations relations labeling rules using DPBD framework. DPDB general human-in-the-loop framework aims ease writing labeling functions, improving accessibility efficiency data programming. Through user study 10 data scientists performing real world labeling tasks classification, evaluated \system together conventional data programming found \system enables accessible data programming without loss performance labeling models created. Results study also suggested that, even skilled programmers, majority functions write captured easily visual interactions using system. We release \system open source software support future applications extended research. \section{Evaluation} We evaluate \system alongside manual data programming using Snorkel. Our goal better understand trade-offs afforded method. To end, conducted user study data scientists measured task performance accuracy completing two labeling tasks. In addition task performance, also analyzed accessibility expressivity methods using qualitative feedback elicited participants. Note \system used programmers non-programmer domain experts alike, fair comparison Snorkel requires proficiency conventional programming. \subhead{Participants} We recruited 10 participants Python programming experience professional network. All participants significant programming experience . Their experience Python programming ranged years average years . \subhead{Experimental Design} We carried study using within-subjects experiment design, participants performed tasks using conditions . The sole independent variable controlled method creating labeling functions. We counterbalanced order tools used, well classification task performed tool. \subhead{Tasks Procedure} We asked participants write labeling functions two prevalent labeling tasks: spam detection sentiment classification. They performed two tasks YouTube Comments Amazon Reviews, respectively. Participants received 15 mins instruction use tool, using topic classification task newsgroup dataset example. We asked participants write many functions considered necessary goal task. There given 30 mins complete task recorded labeling functions created functions' individual aggregate performances. After completing tasks, participants also filled exit survey, providing qualitative feedback. For manual programming condition, provided Jupyter notebook interface based Snorkel tutorial. The notebook section writing functions, section diverse analysis tools, section train logistic regression model labels generated. \section{Evaluation} We evaluate \system alongside manual data programming using Snorkel. Our goal better understand trade-offs afforded method. To end, conducted user study 10 data scientists measured task performance accuracy completing two labeling tasks. In addition task performance, also analyzed accessibility expressivity methods using qualitative feedback elicited participants. \subhead{Participants} We recruited participants Python programming experience professional network . Note \system used programmers non-programmer domain experts alike, fair comparison Snorkel requires proficiency conventional programming. All participants significant programming experience . Their experience Python programming ranged years average years . \subhead{Experimental Design} We carried study using within-subjects experiment design, participants performed tasks using conditions . The sole independent variable controlled method creating labeling functions. We counterbalanced order tools used, well classification task performed tool. \subhead{Tasks Procedure} We asked participants write labeling functions two prevalent labeling tasks: spam detection sentiment classification. They performed two tasks YouTube Comments Amazon Reviews, respectively. Participants received 15 mins instruction use tool, using topic classification task newsgroup dataset example. We asked participants write many functions considered necessary goal task. There given 30 mins complete task recorded labeling functions created functions' individual aggregate performances. After completing tasks, participants also filled exit survey, providing qualitative feedback. For manual programming condition, iteratively developed Jupyter notebook interface based Snorkel tutorial. We provided section writing functions, section diverse analysis tools, section train logistic regression model labels generated. . \section{Evaluation} We evaluate framework baseline manual programming labeling functions . Our primary goal better understand trade offs afforded method based quantitative performance qualitative feedback participants. To end, conducted user study \thenum participants measured task performance accuracy two labeling tasks two different corpora, YouTube Comments Amazon Reviews. In addition task performance, also analyzed accessibility, expressivity, interpretability methods using qualitative feedback elicited participants observations gathered study sessions. \subhead{Experimental Design} We wanted sure user opportunity try tools could fairly compare two, still minimizing knowledge transfer tasks. We also wanted evaluate methods different types tasks. To achieve this, divided participant pool two random groups five participants each. We randomly assigned task/tool pairings group. To avoid ordering effects, counterbalanced presentation tasks within group. The sole independent variable controlled method creating labeling function, two conditions; \snorkel, \system. Note two tools correspond two different forms creating labeling functions, manual , using visual interactive demonstration, respectively. Babble Labble mention In pilot version study also tested Babble Labble, system generation labeling functions natural language explanations. In general participants performed worse found system less expressive, omitted study. Our takeaway Babble may useful collecting functions scale from, example, crowdsource workers, less suited individual machine learning engineer domain expert. \subhead{Participants} For fair comparison, wanted make sure participants skilled programmers, well familiar training machine learning models able interpret statistics like precision recall. Because difficulty recruiting subjects skill set, recruited participants employees interns lab. None participants involved work. Although believe Ruler used programmers non-programmer domain experts alike, purpose study compare Ruler existing methods, recruited programmers skilled Python. Participants either ``research scientist'' ``software engineer'' job title. Five held PhDs one held BS, computer science. \todo{Mention titles, prev experience participants} All participants significant programming experience . Their experience Python programming ranged years average years . Only two participants used data programming past, experience training supervised models collecting training data. \subhead{Tasks Data} We asked participants write labeling functions two prevalent labeling tasks, spam detection sentiment classification. They performed two tasks YouTube Comments Amazon Reviews, respectively. We asked participants write many functions considered necessary goal task. They given 30 mins complete task. Participants also tutored 15 mins writing labeling functions using topic classification task newsgroup dataset. \subhead{Procedure} Before experiment began, users asked complete questionnaire elicited information educational background programming model development experiences. This way could ensure treatment groups reasonably balanced across several dimensions. Each user scheduled complete two sessions, never day 3 days apart. These sessions conducted zoom began 15 minute tutorial learn use tool practicing newsgroup dataset. Next, user given 30 minutes complete assigned task. They allowed ask questions access internet desired. Before tutorial task, user given much time wanted read task description consisting short paragraph describing task 5 examples class. We cannot expect 30 minute experiment realistic representation generating training data labels like, likely good approximation first 30 minutes generating training data like. Given constrained resources, consider best method evaluation. Throughout task, recorded labeling functions created participants functions' individual aggregate performances task. At end session, participants completed exit survey provide qualitative feedback. After second session, user asked complete final survey comparing two tools. For instance, spam detection assigned first task completed user, Snorkel first tool, first session participant would complete Snorkel tutorial, spam detection task using Snorkel survey. On later day, would complete Ruler tutorial, followed sentiment analysis task using Ruler, survey Ruler, survey comparing two tools. \cagatay{If space permits, consider adding figure illustrating experiment flow.} \sara{consider adding figure showing users' experience}\section{DPBD Framework} \stitle{Problem Statement} Given dataset data records set labels , aim develop framework enables human labelers interactively assign label data record efficiently sampled , demonstrating rationales label assignments visual interaction. Given triplet data record, visual interaction labeler, label assigned, want framework effectively synthesize propose labeling rules labeler choose from. Finally, want framework optimally aggregate chosen rules order create labeled training set probabilistic labels order subsequently train discriminative models it. \stitle{Framework Overview} The data programming demonstration framework two input sources: human labeler, data labeled. The labeler subject matter expert sufficient domain understanding extract useful signals data. Given dataset, framework enables labeler label record categorical label, providing labeling rationales interactively marking relevant parts record specifying semantics relationships among them. The output labeling model, trained automatically produce labels large set unlabeled data. The DPBD framework four main components, labeling interface, synthesizer, modeler, active sampler. The labeler interacts data via labeling interface. The labeling interface records labeler's interaction compiles interaction labeling rule. The synthesizer synthesizes labeling rules translates chosen labeler program functions. Third, selected functions passed modeler, builds labeling model optimally aggregating generated functions. Until certain stopping criterion met labeler decides exit, active sampler selects next data record present labeler. In rest section, describe details component. The labeling interface workplace labeler encodes domain knowledge labeling rules. It provides way express noisy explanations labeling decisions using visual interaction language, allows user express domain knowledge without formalize ideas computer programs natural language explanations. This allows focus patterns data abstracting away implementation concerns. \stitle{Generalized Labeling Model} Inspired entity-relationship model database modeling, generalized labeling model models data records concepts relationships. The GLM views data record series tokens, token continuous subset record semantics attached. For example, text data, token span data record; image data record, would 2D region, rectangular free form; audio data record, would 1D window data record . A concept group tokens labeler believes share common semantics. For instance, text data, labeler might define concept positive adjectives consisting set tokens, imply positive review. When labeling audio data, labeler might create concept aggregate clips express excitement, specific speaker. This abstraction allows user teach GLM generalizations relevant task. A relationship represents binary correlation token-token, token-concept, concept-concept. Some examples membership , co-existence , positional . \stitle{Mapping GLM Elements Operations} Given GLM specification described above, framework also defines operations applied GLM elements. Table lists GLM elements corresponding operations. The implementation labeling interface operations described Table would vary across data types token definitions. To add expressivity, GLM may also perform transformations set tokens, describe next section. \stitle{Compiling Operations Labeling Rules} Once labeler finishes annotating example using provided operations, selects label, tokens extracted annotation used initial set conditions build rules. The synthesizer combines conditions labeling rules selecting subsets conditions combined different conjunctive formulas, according relationships user annotated. The synthesizer extends initial set labeling rules presents extended labeling rules labeler select from, choosing desired ones based domain knowledge. A labeling rule serves intermediate language, interpretable labeler synthesizer. In framework, adopt notation domain relational calculus represent rules, expressed as: . The variable \texttt{tokens} sequence tokens existential quantification, \texttt{conditions} conjunctive formula boolean predicates tested \texttt{tokens} data record. The predicates first-order expressions, expressed tuple . optional transformation function token identifier, process mapping raw token generalized forms. Some example transformations word lemmatization text labeling, speech-to-text detection audio labeling, object recognition image labeling. token, either token, literal set. If denotes token, transformation function may also apply . operator whose type depends type . If token literal, detects positional equality relationship. Otherwise, set, one set operators . Since \texttt{conditions} conjunctive form, order labeler's interactions matter. \example{ Consider following review binary sentiment classification task: \texttt{This book great! I loved read many times I soon buy new copy.} If labeler thinks data record positive sentiment, express decision rationale using GLM. First, may select two tokens related sentiment: \texttt{book} \texttt{great}. Assume two concepts labeler previously created: \texttt{itembook, electronics}; \texttt{padjwonderful}. The labeler realizes token \texttt{great} generalized \texttt{padj} concept, means labeling rule still valid token replaced tokens concept, adds token concept. Finally, labeler creates positional relationship \texttt{book} token \texttt{great} indicate appear sentence, completing labeling process. These operations compile labeling rule . } This rule sent synthesizer expansion program synthesis. Given compiled labeling rule labeling interface, synthesizer extends one single labeling rule labeler's interaction set general labeling rules; translates labeling rules computer programs. It straightforward translate rules executable computer programs , section, focus synthesize extended labeling rules. Given labeling rule compiled labeler's interaction, synthesizer generates labeling rules optimizing two competing goals: maximizing generalization, data accurately labeled; maximizing coverage labeler's interaction, simply labeler's interaction valuable signal labeling based domain knowledge. Of course, larger set annotations interaction, larger set labeling functions synthesized. To keep rule selection easy possible user, case prioritize rules cover interaction, assuming little redundancy. labeler's interaction. We achieve generalization given rules using following heuristics: substituting tokens concepts; replacing general coexistence relationships position-specific ones; applying available transformations tokens . Since labeling rule GLM conjunctive conditions, Algorithm generalizes predicate conditions. Inside, Line Line substitute token concept. Line implemented explicitly matching token concept set, well sophisticated data-dependent processing via transformation . For example, system text labeling , addition matching values labeler defined concepts, also apply named-entity recognition named-entities implicit concepts token member of. Line Line replace positional co-occurrence relationship removing condition specifies positional context. The conditions extended labeling rules conjunctive combination single predicates, one extended condition set . In addition, special cases binary labeling, algorithm also considers rule flips label adding negation conditions . Once extended rules generated, rules ranked generalization score---a measurement applicable certain rule is. We define data-independent generalization score labeling rule as: . Intuitively, calculated counting many different data instances used. It prefers labeling rules using large sets match tokens data record. \example{ Continuing Amazon review example, synthesizer derive following labeling rules using heuristics: Labeling rule~ generated using heuristics . Labeling rule~ and~ synthesized using heuristics , respectively. Note labeling rule~ general than~ and~ data records labeled by~ and~ labeled way using labeling rule~. Labeling rules~ due flipping binary label heuristics . } Once extended labeling rules generated, labeler help confirm validity order achieve faster convergence. The top-k candidates ranked generalization score displayed labeling interface labeler accept reject. The modeler component trains model used automatically annotate unlabeled datasets. Naively aggregating labeling functions either inaccurate , scale large set unlabeled data. Instead, modeler encapsulates ideas traditional data programming first build generative model denoise labeling functions, train discriminative model leverage features beyond expressed labeling functions. To improve model quality faster rates, framework uses active sampler choose next data record labeling. The active sampler plugged custom active learning policy. By default, selects data record highest entropy : probability example belongs class , predicted trained label model. \section{DPBD Framework} \stitle{Problem Statement} Given dataset data records set labels , aim develop framework enables human labelers interactively assign label data record efficiently sampled , demonstrating rationales label assignments visual interaction. Given triplet data record, visual interaction labeler, label assigned, want framework effectively synthesize propose labeling rules labeler choose from. Finally, want framework optimally aggregate chosen rules order create labeled training set probabilistic labels order subsequently train discriminative models it. \stitle{Framework Overview} The data programming demonstration framework two input sources: human labeler, data labeled. The labeler subject matter expert sufficient domain understanding extract useful signals data. Given dataset, framework enables labeler label record categorical label, providing labeling rationales interactively marking relevant parts record specifying semantics relationships among them. The output labeling model, trained automatically produce labels large set unlabeled data. Inherited traditional data programming~, framework also assumes set labeled data available tuning model hyperparameters. The DPBD framework four main components. The labeler interacts data via labeling interface. The labeling interface records labeler's interaction compiles interaction labeling rule. The synthesizer synthesizes labeling rules translates chosen labeler program functions. Third, selected functions passed modeler, builds labeling model optimally aggregating generated functions. Until certain stopping criterion met labeler decides exit, active sampler selects next data record present labeler. In rest section, describe details component. The labeling interface workplace labeler encodes domain knowledge labeling rules. It provides way express noisy explanations labeling decisions using visual interaction language, allows user express domain knowledge without formalize ideas computer programs natural language explanations. This allows focus patterns data abstracting away implementation concerns. Inspired entity-relationship model database modeling, generalized labeling model models data records concepts relationships. The GLM views data record series tokens, token continuous subset record semantics attached. For example, text data, token span data record; image data record, would 2D region, rectangular free form; audio data record, would 1D window data record . A concept group tokens labeler believes share common semantics. For instance, text data, labeler might define concept positive adjectives consisting set tokens, imply positive review. When labeling audio data, labeler might create concept aggregate clips express excitement, specific speaker. This abstraction allows user teach GLM generalizations relevant task. A relationship represents binary correlation token-token, token-concept, concept-concept. Some examples membership , co-existence , positional . \subhead{Mapping GLM Elements Operations} Given GLM specification described above, framework also defines operations applied GLM elements. Table lists GLM elements corresponding operations. The implementation labeling interface operations described Table would vary across data types token definitions. To add expressivity, GLM may also perform transformations set tokens, describe next section. \subhead{Compiling Operations Labeling Rules} Once labeler finishes annotating example using provided operations, selects label, tokens extracted annotation used initial set conditions build rules. The synthesizer combines conditions labeling rules selecting subsets conditions combined different conjunctive formulas, according relationships user annotated. The synthesizer extends initial set labeling rules presents extended labeling rules labeler select from, choosing desired ones based domain knowledge. A labeling rule serves intermediate language, interpretable labeler synthesizer. In framework, adapt notation domain relational calculus represent rules, expressed as: . The variable \texttt{tokens} sequence tokens existential quantification, \texttt{conditions} conjunctive formula boolean predicates tested \texttt{tokens} data record. The predicates first-order expressions, expressed tuple . optional transformation function token identifier, process mapping raw token generalized forms. Some example transformations word lemmatization text labeling, speech-to-text detection audio labeling, object recognition image labeling. token, either token, literal set. If denotes token, transformation function may also apply . operator whose type depends type . If token literal, detects positional equality relationship. Otherwise, set, one set operators . Since \texttt{conditions} conjunctive form, order labeler's interactions matter. \example{ Consider binary sentiment classification task Amazon review data. Observe following review: \texttt{This book great! I loved read many times I soon buy new copy.} If labeler thinks data record positive sentiment, express decision rationale using GLM. First, may select two tokens related sentiment: \texttt{book} \texttt{great}. Assume two concepts labeler previously created: \texttt{itembook, electronics}; \texttt{padjwonderful}. The labeler realizes token \texttt{great} generalized \texttt{padj} concept, means labeling rule still valid token replaced tokens concept, adds token concept. Finally, labeler creates positional relationship \texttt{book} token \texttt{great} indicate appear sentence, completing labeling process. These operations compile labeling rule . } This rule sent synthesizer expansion program synthesis. Given compiled labeling rule labeling interface, synthesizer extends one single labeling rule labeler's interaction set general labeling rules; translates labeling rules computer programs. It straightforward translate rules executable computer programs , section, focus synthesize extended labeling rules. Given labeling rule compiled labeler's interaction, synthesizer generates labeling rules optimizing two competing goals: maximizing generalization, data accurately labeled; maximizing coverage labeler's interaction, simply labeler's interaction valuable signal labeling based domain knowledge. Of course, larger set annotations interaction, larger set labeling functions synthesized. To keep rule selection easy possible user, case prioritize rules cover interaction, assuming little redundancy. labeler's interaction. We achieve generalization given rules using following heuristics: substituting tokens concepts; replacing general co-existence relationships position-specific ones; applying available transformations tokens . Since labeling rule GLM conjunctive conditions, Algorithm generalizes predicate conditions. Inside, Line Line substitute token concept. Line implemented explicitly matching token concept set, well sophisticated data-dependent processing via transformation . For example, system text labeling , addition matching values labeler defined concepts, also apply named-entity recognition named-entities implicit concepts token member of. Line Line replace positional co-occurrence relationship removing condition specifies positional context. The conditions extended labeling rules conjunctive combination single predicates, one extended condition set . In addition, special case binary labeling, algorithm also considers rule flips label adding negation conditions . Once extended rules generated, rules ranked generalization score---a measurement applicable certain rule is. We define data-independent generalization score labeling rule as: . Intuitively, calculated counting many different data instances used. It prefers labeling rules using large sets match tokens data record. \example{ Continuing Amazon review example, synthesizer derive following labeling rules using heuristics: Labeling rule~ generated using heuristics . Labeling rule~ and~ synthesized using heuristics , respectively. Note labeling rule~ general than~ and~ data records labeled by~ and~ labeled way using labeling rule~. Labeling rules~ due flipping binary label heuristics . } Once extended labeling rules generated, labeler help confirm validity order achieve faster convergence. The top-k candidates ranked generalization score displayed labeling interface labeler accept reject. The modeler component trains model used automatically annotate unlabeled datasets. Naively aggregating labeling functions either inaccurate , scale large set unlabeled data. This simply labeling functions noisy: may overlap, conflict even depends other, provide limited signals weak supervision. Instead, modeler encapsulates ideas traditional data programming first build generative model denoise labeling functions, train discriminative model leverage features beyond expressed labeling functions. To improve model quality faster rates, framework uses active sampler choose next data record labeling. The active sampler plug custom active learning policy. By default, selects data record highest entropy : probability example belongs class , predicted trained label model. \section{Introduction} Machine learning models used practice today predominantly supervised models rely large datasets labeled training. However, cost collecting maintaining labeled training data remains bottleneck training high-capacity supervised models. Data programming aims address difficulty collecting labeled data using programmatic approach weak supervision heuristics, domain experts expected provide data programs incorporating domain knowledge. Prior work data programming focuses modeling aggregating labeling functions written manually generated automatically denoise labeling functions. However, little known user experience writing labeling functions improve it. Writing data programs be, however, challenging time consuming. Most domain experts lay users little programming literacy, even proficient programmers, often difficult convert domain knowledge set rules writing programs. By extending data programming programming example, bridge gap scalable training data generation domain experts. To address challenges, introduce data programming demonstration , new framework aims make creating labeling functions easier learning users' interactive visual demonstrations. DPBD moves burden writing labeling functions intelligent synthesizer enabling users steer synthesis process multiple semantic levels, providing rationales relevant labeling choices interactively filtering proposed functions. DPBD draws two lines prior research; programming demonstration example , e.g.,, aims make programming easier synthesizing based user interactions input output examples, interactive learning user-provided features rationales . We operationalize framework \system, interactive system enables accessible data programming create labeled training datasets document classification. \system automatically generates document level labeling rules span-level annotations relations specific examples provided users. Through user study conducted 10 data scientists, evaluate \system alongside manual data programming using Snorkel. We measure predictive performances models created participants two common labeling tasks, sentiment classification spam detection. We also elicit ratings qualitative feedback participants multiple measures, including ease use, ease learning, expressivity, overall satisfaction. We find \system facilitates accessible creation labeling functions without loss quality learned labeling models. Tagging token level classification text documents another widely used task benefit DPBD. Here also briefly discuss work progress \tagruler, DPBD system learns token labeling functions user interaction create training datasets tagging models. Tagging span-level classification text documents another widely used task benefit DPBD. Here also briefly discuss work progress \tagruler, DPBD system enables interactive generation token labeling functions order create labeled training data tagging models. On hand, \tagruler synthesizes token classification rules based users. In summary, contribute DPBD, general data independent framework learning labeling rules interactive demonstration; \system, interactive system operationalizing framework document classification tasks; comparative user study conducted data scientists performing real world tasks evaluate \system conventional data programming. We made research artifacts, including \system code demo, publicly available~ search images Copyright \setcopyright{none} \acmConference[]{}{} Submission ID. Use submitting article sponsored event. You'll receive unique submission ID organizers event, ID used parameter command. \acmSubmissionID{123-A56-BU3} The majority ACM publications use numbered citations references. The command \citestyle{authoryear} switches ""author year"" style. If preparing content event sponsored ACM SIGGRAPH, must use ""author year"" style citations references. Uncommenting next command enable style. \citestyle{acmauthoryear} end preamble, start body document source. Conference \acmYear{1997} \copyrightyear{2016} \acmArticle{4} \acmPrice{15.00} These commands optional \acmBooktitle{Transactions ACM Woodstock conference} \editor{Jennifer B. Sartor} \editor{Theo D'Hondt} \editor{Wolfgang De Meuter} \definecolor{tomato}{rgb}{1,0.2,0} \definecolor{turqoise}{rgb}{0.03, 0.91, 0.87} \definecolor{grey}{rgb}{0.4,0.4,0.4} \newif\ifnotes \notestrue \DeclareRobustCommand{\cagatay}[1]{\ifnotes{\small[{\c{C}a\u{g}atay:}{#1}]}\fi} \DeclareRobustCommand{\sara}[1]{\ifnotes{\small[{Sara:}{#1}]}\fi} \DeclareRobustCommand{\subhead}[1]{#1} \DeclareRobustCommand{\system}{\mbox{\sc Ruler}\xspace} \DeclareRobustCommand{\ruler}{\mbox{\sc Ruler}\xspace} \DeclareRobustCommand{\tagruler}{\mbox{\sc TagRuler}\xspace} \DeclareRobustCommand{\snorkel}{\mbox{\sc Snorkel}\xspace} \DeclareRobustCommand{\babblelabble}{\mbox{\sc BabbleLabble}\xspace} \DeclareRobustCommand{\thenum}{ten\xspace} \newcommand{\eat}[1]{} \newcommand{\example}[1]{{\underline{Example:} #1\qed}} \newcommand{\stitle}[1]{\smallskip {#1}} \newcommand{\sstitle}[1]{\smallskip {\underline{#1}}} \DeclareRobustCommand{\subhead}[1]{#1} \newcommand{\squishlist}{ } \renewcommand{\shortauthors}{} \settopmatter{printacmref=false,printfolios=true,printccs=false} \begin{document} \title[]{Data Programming Demonstration:\\A Framework Interactively Learning Labeling Functions} \author{Sara Evensen} \affiliation{ \institution{Megagon Labs} } \email{} \author{Chang Ge} \authornote{Work done internship Megagon Labs.} \affiliation{ \institution{University Waterloo} } \email{} \author{Dongjin Choi} \authornotemark[1] \affiliation{ \institution{Georgia Tech} } \email{} \author{\c{C}a\u{g}atay Demiralp} \affiliation{ \institution{Megagon Labs} } \email{} \renewcommand{\shortauthors}{B. Trovato et al.} \renewcommand{\shortauthors}{Evensen et al.} \keywords{} \maketitle \pagestyle{plain}","     Training a deep neural network requires a large amount of single-task data and involves a long time-consuming optimization phase. This is not scalable to complex, realistic environments with new unexpected changes.      Humans can perform fast incremental learning on the fly and memory systems in the brain play a critical role.     We introduce Sparse Meta Networks -- a meta-learning approach to learn online sequential adaptation algorithms for deep neural networks, by using deep neural networks.      We augment a deep neural network with a layer-specific fast-weight memory. The fast-weights are generated sparsely at each time step and accumulated incrementally through time providing a useful inductive bias for online continual adaptation. We demonstrate strong performance on a variety of sequential adaptation scenarios, from a simple online reinforcement learning to a large scale adaptive language modelling."
"The advent open-source software question answering websites contributed improving way developers produce code. Nowadays, code search permeates development activities. Developers spend 15\% time searching online piece code works, fix bug, use API . According \citet{sadowski-how-developers-search-for-code-case-study:2015}, Google, developers search code 12 times day, clicking 2 3 results average per search session. Most developers use general-purpose search engines look code , uses page rank indexes tactics optimized searching code. Then, general-purpose search engines adequately find code snippets unless accompanying descriptions. According \citet{masudur-developers-use-google-code-retrieval:2018}, developers spend time, visit pages, change queries often code-related searches. In particular, newcomers project greatly benefit semantic search since face variety entrance barriers . GitHub, popular source code hosting platform, attempted build semantic code search. They extracted millions lines code repositories matched code snippet docstring. The final results satisfactory tool could find relevant code snippet user provided query matched docstring description . According \citet{cambronero-deep-code-search-2019}, users' intents better matched questions collected question-answering sites related programming, e.g., Stack Overflow. Those sites allow users ask question approve best answer it. Other users vote helpful answer mark wrong helpful ones. Those collective actions curate organize information. Initial code search studies based deductive-logic rules manually extracted features . The recent success artificial neural networks shifted recent works machine learning-based approach. \citet{cambronero-deep-code-search-2019} coined name, neural code search, i.e., code search based neural networks. Recent works applied neural networks summarize retrieve code snippets. \citet{cambronero-deep-code-search-2019} proposed neural network attention mechanism \citet{Gu-deep-code-search:2018} presented recurrent neural network. Our novel approach based Convolutional Neural Networks . For best knowledge, CNNs yet used search code, achieved good results selecting answers . CNNs prioritize local interactions translation invariant, important traits task. In study, answer following research questions: The universal learning algorithm aware humans learn. Human learning robust flexible -- relies causality, ability fast sequential adaptation balances memory encoding active forgetting, across large number familiar unfamiliar scenarios. Meta-learning offers promising computational paradigm learn universal learning algorithm data-driven way. In work, proposed meta-learning approach learn sequential adaptation algorithm arbitrary deep neural network architectures. Our approach performs sequential adaptation bounded compute memory across changing environment tasks. The proposed Online Cifar setup serve useful benchmark studying flexible models algorithms go beyond fixed distribution regime. In current state Sparse-MetaNet method, sparsity mask sampled fixed distribution. A future work explore learning-based approaches conditional mask distribution, Sparse-MetaNet model selectively encode fast-weight memory past gradients. The current work limited focus catastrophic interference issue neural networks. A future work extend Sparse-MetaNet approach mitigating issue."," Software developers routinely search for code using general-purpose search engines. However, these search engines cannot find code semantically unless it has an accompanying description. We propose a technique for semantic code search: A Convolutional Neural Network approach to code retrieval . Our technique aims to find the code snippet that most closely matches the developer's intent, expressed in natural language. We evaluated our approach's efficacy on a dataset composed of questions and code snippets collected from Stack Overflow. Our preliminary results showed that our technique, which prioritizes local interactions , improved the state-of-the-art  by 5\% on average, retrieving the most relevant code snippets in the top 3  positions by almost 80\% of the time. Therefore, our technique is promising and can improve the efficacy of semantic code retrieval."
"In recent years, deep learning methods become standard solving information retrieval tasks. These methods effectively map words phrases vector representations. These representations facilitate better matching phrases similar meanings. Phrases closer meaning represented closer vector space. In information retrieval, many ways develop relevance scores used, counting word overlap query document. Recently, complex machine learning models use human-verified datasets train models assign similarity scores used rankings. Applying deep learning Natural Language Processing problems given rise new approaches better represent sentence meaning using neural networks. For instance, Long Short Term Memory models attention mechanism allow word relationships constructed different sentences thus words better placed context, rather examining words closest them. A breakthrough development Natural Language Processing, BERT architecture, extracts word consequently sentence representations masking words throughout sentence predicting omitted words, using self-attention encode entire sentence once. Within BERT framework, model also trained predict next sentence choices, given input sentence. \\ Even advances, deep learning methods still struggle inherent difficulties IR tasks. These challenges result discrepancies query document vocabulary, limited size data used training, weaknesses given human-generated query. In effort mitigate effects, team approach inspired existing method, doc2query, given input document uses transformer model architecture predict plausible queries leading document. Although shown expanded documents indeed allowed improved retrieval performance downstream ranking model, approach requires documents collection interest first ``pre-indexed'' feeding input transformer model, practical. Instead, propose query2query method takes given query input generates several queries similar meaning. The hope create powerful query augmenting generated queries given query single representation, used match desired passage. To complete architecture, feed expanded queries pre-trained BERT model predict similarity scores queries documents produce final ranking. The goal approach reduce surface form oise within certain query generating queries ask information, different ways. By different representations ame query, hope create holistic queries result obtain end-to-end method generalize better potentially reduce problems modern IR faces. Our model, CoNCRA, achieved MRR score 5\ higher average Unif, state-of-the-art technique. We could rank relevant code snippet among first 3 positions 78\ cases. Our technique achieved TOP-1 accuracy 60\ , techniques achieved 50\ . The results seem promising, plan investigation check model invariant datasets. We also investigate transfer learning, e.g., checking model trained Stack Overflow dataset find relevant code snippets GitHub corpus . Our approach based NLP technique proposed \citet{feng-2015}, future work may use transformers autoencoders, techniques showed good results many NLP tasks."," This paper describes Brown University's submission to the TREC 2019 Deep Learning track. We followed a 2-phase method for producing a ranking of passages for a given input query: In the the first phase, the user's query is expanded by appending 3 queries generated by a transformer model which was trained to rephrase an input query into semantically similar queries. The expanded query can exhibit greater similarity in surface form and vocabulary overlap with the passages of interest and can therefore serve as enriched input to any downstream information retrieval method. In the second phase, we use a BERT-based model pre-trained for language modeling but fine-tuned for query - document relevance prediction to compute relevance scores for a set of 1000 candidate passages per query and subsequently obtain a ranking of passages by sorting them based on the predicted relevance scores.  According to the results published in the official Overview of the TREC Deep Learning Track 2019, our team ranked 3rd in the passage retrieval task , and 2nd when considering only re-ranking submissions."
"% Background Collecting sufficient amount electronic health records challenging task various factors . Due problem, researchers medical field often provided small amount data given. Owing fact deep learning techniques perform better large amounts data, number studies using machine learning techniques conducted solve specific medical problems, regarding limited number data . Dementia also one many medical symptoms facing situation. % Alzheimer's Dementia Dementia, syndrome deterioration cognitive function beyond might expected normal ageing, mostly affected Alzheimer Disease . % Although studies Dementia also faces problem lacking dataset, There previous researches various approaches recognize Alzheimer's Dementia , shown excellent performance. % However, dataset used works adequete quantity one used paper. However, datasets used works sufficient quantity one used paper. % The ADReSS challenge The ADReSS challenge INTERSPEECH 2020 hosts two tasks: Alzheimer Dementia classification Mini Mental Status Examination regression, providing refined dataset. The dataset equally balanced AD non-AD participants metadata age gender. % Each data conversation participant investigator composed acoustic textual information. % Each data conversation participant investigator participant spontaneously describes picture given investigator. % Each data conversation participant spontaneously describes picture given investigator acoustic textual modality. Each data conversation participants, audio text modalities, spontaneously describes picture given investigator. % proposing work Participants challenge suggested solve hosted tasks using given data, numbers train test data 108 48, respectively. For recognizing AD small amounts data, determined would beneficial use acoustic textual features. % why? % thought would best use many information possible recognizing AD  ? Furthermore, leverage models pre-trained large scale datasets feature extractor get better representation. To end, paper focus exploiting various multi-modal features, design suitable network architecture. %       We compare 3 4 different acoustic textual features, respectively, use hand-crafted feature part-of-speech tagging additional inputs. The usage POS HC influenced previous research, approved using features gained transcript improve performance . The proposed network modified version Convolutional Recurrent Neural Network ; capable computing conversations variable lengths, implemented methods fit small amount data. Also, model able compute using acoustic feature only, without metadata, efficient considering real-world situation. Our experimental results show using features pre-trained network leads performance gain raw, regression results imply potential network classifying classes cognitive impairment based MMSE score. This report describes Brown University's entry TREC 2019 Deep Learning Track, produced final ranking set 1000 candidate passages given queries. Our method aims enriching meaning surface form query expanding similar queries, hopes subsequent ranking process, expanded query would provide extra semantic information vocabulary overlap would facilitate retrieval relevant documents. \\ We found retrieval method promising terms retrieval results, albeit significant margins future improvement. A natural focus point future work improving semantic similarity generated queries original query. In work, simply use top 3 output beams terms estimated log-likelihood. However, different metrics could used re-order prioritize larger number generated outputs. In addition, investigation carried terms various ways synthesizing query information condensing documents' representation."," % The ADReSS Challenge at INTERSPEECH 2020 regards to discern patients suspicious of Alzheimer Dementia by providing acoustic and textual data. Since the given training dataset only comprised of 108 conversations, leveraging pre-trained models is effective than fitting from scratch. Therefore, this paper aims to recognize Alzheimer Dementia by exploiting various multi-modal features from pre-trained networks. With the given dataset of conversational form, we modify a Convolutional Recurrent Neural Network based structure to compute input modalities. Our model performs classification and regression tasks simultaneously and is capable of computing conversations with variable lengths. For the classification task, the best test accuracy using only acoustic input is 72.92\%, while using both modality results in 81.25\%. For the regression task, we achieved an RMSE score of 3.7749 . Additionally, our 5-fold cross-validation result for the regression task shows the possibility of classifying 4 classes of cognitive impairment, categorized by the MMSE score, with an accuracy of 78.70\%.   %We use 5-fold cross-validation for measuring model performance. For the classification task, the best F1 score using only acoustic input is 86.28\%, while using both modality results in 94.54\%. For the regression task, the best RMSE score is 3.3493.   Collecting and accessing a large amount of medical data is very time-consuming and laborious, not only because it is difficult to find specific patients but also because it is required to resolve the confidentiality of a patient's medical records. On the other hand, there are deep learning models, trained on easily collectible, large scale datasets such as Youtube or Wikipedia, offering useful representations. It could therefore be very advantageous to utilize the features from these pre-trained networks for handling a small amount of data at hand. In this work, we exploit various multi-modal features extracted from pre-trained networks to recognize Alzheimer's Dementia using a neural network, with a small dataset provided by the ADReSS Challenge at INTERSPEECH 2020. The challenge regards to discern patients suspicious of Alzheimer Dementia by providing acoustic and textual data. % With the given dataset, we assess features extracted from the pre-trained networks using a neural network. With the multi-modal features, we modify a Convolutional Recurrent Neural Network based structure to perform classification and regression tasks simultaneously and is capable of computing conversations with variable lengths. % Our model performs classification and regression tasks simultaneously and is capable of computing conversations with variable lengths. % For the classification task, the best test accuracy using only acoustic input is 72.92\%, while using both modality results in 81.25\%. For the regression task, we achieved an RMSE score of 3.7749 . Additionally, our 5-fold cross-validation result for the regression task shows the possibility of classifying 4 classes of cognitive impairment, categorized by the MMSE score, with an accuracy of 78.70\%. Our test results surpass baseline's accuracy by 18.75\%, and our validation result for the regression task shows the possibility of classifying 4 classes of cognitive impairment with an accuracy of 78.70\%."
"Transformer one state-of-the-art approaches Neural Machine Translation , hence, widely accepted. For example, WMT19 machine translation tasks, reported 80\% submitted systems adopted Transformer architecture . Note high translation quality Transformer models entails large number parameters. Moreover, Transformer model inherently much slower conventional machine translation approaches mainly due auto-regressive inference scheme incrementally generating token. As result, deploying Transformer model mobile devices limited resources involves numerous practical implementation issues. To address implementation challenges little degradation translation quality, study low-bit quantization strategy Transformer accomplish high-performance on-device NMT. We note previous studies compress Transformer models utilize uniform quantization . While uniform quantization may effective memory footprint savings, would face various issues improve inference time maintain reasonable BLEU score. For example, even integer arithmetic units inference operations present limited speed resulting BLEU score quantized Transformer substantially degraded low-bit quantization INT4 . While determining number quantization bits Transformer, crucial consider component Transformer may exhibit varied sensitivity quantization error toward degradation translation quality . Accordingly, mixed precision quantization suggested effort assign different numbers quantization bits depending component quantization sensitive loss function. In addition, illustrate later, even assigning different quantization bits row embedding block reduce overall number quantization bits entire Transformer model. Our proposed quantization strategy, thus, provides finer-grained mixed precision approach compared previous methods, consider layer-wise matrix-wise mixed precision. % One important aspect block Transformer contributes inference computation translation accuracy differently. Transformer consists three major blocks: embedding, encoder, decoder. The embedding block huge number parameters due dependence vocabulary size, easily scale tens thousands. On contrary, matrices encoder decoder relatively small since independent vocabulary size. As result, embedding block causes major memory latency consumption. Since decoding steps parallelizable inference time, also contributes largely inference computation. % In consideration these, propose mixed precision quantization strategy Transformer quantization efficient inference computation reasonable accuracy loss. Accommodating distinguished implementation properties component Transformer, propose following methodologies decide precision block: 1) case embedding block, statistical importance word taken account 2) encoder decoder blocks, sensitivity quantized sub-layer considered. The main contributions paper follows: For classification task, best test accuracy using acoustic input 72.92\ , using modality results 81.25\ . For regression task, achieved RMSE score 3.7749 . Additionally, 5-fold cross-validation result regression task shows possibility classifying 4 classes cognitive impairment, categorized MMSE score, accuracy 78.70\ . This paper demonstrates extracted features pre-trained networks satisfactory handling small amounts data, recognize Alzheimer's Dementia. The proposed model compute variable lengths dialogue also introduce productive methods fit network little amount data. Furthermore, model require metadata also perform well without transcript, may practical real-world situations. Our test result outperforms baseline's tasks, regression results imply potential network classifying classes cognitive impairment based MMSE score. validation  unimodal  modality   post  , audio  text ,    .    bimodal network  modality merge             ,     future work    .      VGGish & Transformer-XL VGGish wrong - 18 Transformer-XL wrong - 5 bimodial wrong - 9 future work improve multi-modal network Some modifications model architecture done merge different modalities beneficial effects future work. There validation samples overlapping error results unimodal network modality, bimodal network using modality features able reach accurate answer typical samples. Yet, samples unimodal networks could deduce correctly wrong bimodal network. Accordingly, mechanisms effectively fusioning divergent features applied expectation performance gain . For future work, modifications model architecture done merge different modalities mechanisms effectively fusioning divergent features applied expectation performance gain . For future work, expectation performance gain, mechanisms effectively fusioning different modality features applied model architecture. Acknowledgements","  The deployment of widely used Transformer architecture is challenging because of heavy computation load and memory overhead during inference, especially when the target device is limited in computational resources such as mobile or edge devices. Quantization is an effective technique to address such challenges. Our analysis shows that for a given number of quantization bits, each block of Transformer contributes to translation quality and inference computations in different manners. Moreover, even inside an embedding block, each word presents vastly different contributions. Correspondingly, we propose a mixed precision quantization strategy to represent Transformer weights by an extremely low number of bits . For example, for each word in an embedding block, we assign different quantization bits based on statistical property. Our quantized Transformer model achieves 11.8$\times$ smaller model size than the baseline model, with less than -0.5 BLEU. We achieve 8.3$\times$ reduction in run-time memory footprints and 3.5$\times$ speed up  such that our proposed compression strategy enables efficient implementation for on-device NMT."
"The rapid progression generative models computer vision natural language processing led increasing likelihood realistic-looking news articles generated Artificial Intelligence . The malicious use technology could present major societal problem. \citet{zellers2019defending} report humans easily deceived AI-generated propaganda. By manipulating technology, adversaries would able disseminate large amounts online disinformation rapidly. While promising pretrained generative models best defense , often challenging aware models utilized adversaries beforehand. More importantly, ignores fact news articles often accompanied images captions . %We argue visual context provides vital clues discriminating machine-generated articles. In paper, present first line defence neural fake news images captions. To best knowledge, first address challenging realistic problem. Premised assumption adversarial text generator unknown beforehand, propose evaluate articles based semantic consistency linguistic visual components. While state-of-the-art approaches bidirectional image-sentence retrieval leveraged visual-semantic consistency great success standard datasets MSCOCO Flickr30K , show Appendix able reason effectively objects image named entities present caption article body. This due discrepancies distribution datasets, captions standard datasets usually contain general terms including woman dog opposed named entities Mrs Betram Golden Retriever, commonly contained news article captions. Moreover, images often directly related articles associated with. For example, Figure , article contains mentions British Prime Minister. Yet, contains image United Kingdom flag. To circumvent problem, present DIDAN, simple yet surprisingly effective approach exploits possible semantic inconsistencies text image/captions detect machine-generated articles. For example, notice article caption Fig. actually mention different Prime Ministers. Besides evaluating semantic relevance images captions article, DIDAN also exploits co-occurrences named entities article captions determine authenticity score. The authenticity score thought probability article human-generated. We adopt learning paradigm commonly used image-sentence retrieval models trained reason dissimilarities images non-matching captions. In instance, negative samples constitute articles non-corresponding image-caption pairs. Not reasonable approach adversarial generative model unknown, show empirically crucial detecting machine-generated articles high confidence even access machine-generated samples training. More importantly, means DIDAN easily trained abundance online news articles without additional costly annotations. To study threat, construct NeuralNews dataset contains human machine-generated articles. These articles contain title, main body well images captions. The human-generated articles sourced GoodNews dataset. Using titles main article bodies context, use GROVER generate articles. Instead using GAN-generated images easy detect even without exposure training time , consider much harder setting articles completed original images. We include real generated captions generated SOTA entity-aware image captioning model . We present results findings series empirical well user study experiments. In user study experiments, use 4 types articles including real generated news determine humans susceptible to. The insights derived findings help identify possible weaknesses adversaries exploit produce neural fake news serve valuable reference defending threat. Last least, experimental results provide competitive baseline future research area. In summary, contributions multi-fold: In work, analyze block sub-layer Transformer propose extremely low-bit quantization strategy Transformer architecture. Our 2.6-bit quantized Transformer model achieves 11.8 model compression ratio reasonable -0.5 BLEU. We also achieve compression ratio 8.3 memory footprints 3.5 speed mobile device . \clearpage"," Large-scale dissemination of disinformation online intended to mislead or deceive the general population is a major societal problem. Rapid progression in image, video, and natural language generative models has only exacerbated this situation and intensified our need for an effective defense mechanism. While existing approaches have been proposed to defend against neural fake news, they are generally constrained to the very limited setting where articles only have text and metadata such as the title and authors. In this paper, we introduce the more realistic and challenging task of defending against machine-generated news that also includes images and captions. To identify the possible weaknesses that adversaries can exploit, we create a NeuralNews dataset composed of 4 different types of generated articles as well as conduct a series of human user study experiments based on this dataset. In addition to the valuable insights gleaned from our user study experiments, we provide a relatively effective approach based on detecting visual-semantic inconsistencies, which will serve as an effective first line of defense and a useful reference for future work in defending against machine-generated disinformation."
"As neural networks adopted solve real-world problems, parts network may easy develop, unknown aspects hyperparameters, clear method derivation. Ongoing research focuses developing new network architectures training methods. When developing neural networks, question hand set hyperparameter values maximize results set training configuration. For network architecture design, important hyperparameters include type network, number layers, number units per layer, unit type. For training configurations, important hyperparameters include learning algorithm, learning rate, dropout ratio. All hyperparameters interact affect performance neural networks. This interaction hyperparameters referred epistasis. Thus need tuned simultaneously get optimum results.\\ The motivation behind research replace tedious manual tuning hyperparameters automatic method performed computers. Current methods optimization limited trivial methods like Grid search. Grid search simple method hyperparameter optimization. However, number hyperparameters increases, Grid search becomes time consuming computationally taxing. This number lattice points increases exponential way increase number hyperparameters . For example, ten hyperparameters tuned try five values parameter, alone requires 9 Million evaluations: . For reason, grid search feasible certain applications. To solve this, look GA higher-performing less computationally taxing solution. The use GA neural network hyperparameter optimization explored previously . \\ We present empirical study GAs neural network models machine translation natural language specifically Japanese English. We describe experiment setup Section 2, GA method Section 3, results Section 4. The preliminary findings suggest simple GA encoding potential find optimum network architectures compared random search baseline. In paper, define new task, full-line code completion, studied performance neural language models task. Apart token-based BPE-based approaches, already evaluated token-level code completion tasks, additional conduct experiments ASDL syntax-based models. Our experiments show Transformer language model token sequences currently performs best datasets. In future, plan improve effectiveness language models full-line code completion training data using models larger parameter size. Meanwhile, aim utilize powerful software analyzing tools narrow output space model, e.g., adding restrictions variable names API usage. Furthermore, would like improve neural model incorporate syntax structures like parent-child links ASTs incorporate BPE copy mechanism tackle out-of-vocabulary problem. \bigskip","  With neural networks having demonstrated their versatility and benefits, the need for their optimal performance is as prevalent as ever. A defining characteristic, hyperparameters, can greatly affect its performance. Thus engineers go through a process, tuning, to identify and implement optimal hyperparameters. That being said, excess amounts of manual effort are required for tuning network architectures, training configurations, and preprocessing settings such as Byte Pair Encoding . In this study, we propose an automatic tuning method modeled after Darwin's Survival of the Fittest Theory via a Genetic Algorithm . Research results show that the proposed method, a GA, outperforms a random selection of hyperparameters."
"In past decades, knowledge graph construction applications rapidly developed achieved significant outcomes. For better relevancy web search, Google leveraging knowledge graph represents real-world entities relationships one another since 2012. %, also large amount publicly available knowledge graphs, freebase, Dbpedia, YAGO constructed used many real-world intelligent applications. To identify entities text, named entity recognition techniques extensively studied applied many areas including e-commerce search . Such NER systems usually work well defined ontology classify tokens sequence words . A comprehensive domain-specific PT ontology beneficial product search discovery e-commerce platform . At The Home Depot , PT ontology used tremendously online search improve query understanding product retrieval. For example, Figure shows snippet PT ontology consists known PT classes. The PTs ontology serve entity reference NER task well classes SKU-PT mapping catalog side facilitates retrieval relevant products. %. Kutiyanawala et al. also proposed product ontology framework created specially e-commerce search retrieval . %comprehensive domain-specific Ontology required order better understand customers intent account expanding catalog. The Ontology enrichment proved effective boost search relevancy. For example, given customer query ""shower curtain hook"", system would also return ""shower curtain"" products since failed infer proper product type due lack knowledge. By introducing new product type ""shower curtain hook"", system able remove noise provide relevant results. % % \end{equation*} % \[ % z = \overbrace[1pt][5pt]{ge}^{brand}\ \overbrace{7.3\:cu\:ft}^{dim} \quad\overbrace{dryer}^{product}\quad\overbrace{gas}^{attribute} % \] %In domain e-commerce, strong well-structured knowledge graph also plays pivotal roles business business communications customer search navigation experience. %A structured standardized product ontology define product description, catalog formats business documents support electric data exchange vendors buyers. %The Home Depot world leading home improvement retailer customers business. Orange Graph repository access point THD domain-specific knowledge, includes rich product information, project information relationships. By adopting well-structured knowledge graph, high-level search quality, project-based buying features, marketing customer services offered THD e-commerce enterprise systems. % } % % % \end{table} Discovering valid PTs key task build expand PT ontology fundamental challenge regarding definition PT. % given concept instead fact. A PT defined demand side atomic keywords/phrase describes customers look supply side semantic tag/label uniquely identifies product. Within THD, also practical guidelines distinguish valid invalid PTs like %Product type essential component PT ontology. %it widely used e-commerce domain group similar products together. For instance, consider Appliances category, goal discover distinct types refrigerators case could be: ""Side By Side"", ""French Door"", etc. %Although different definitions valid PT, In paper, define valid PT leaf-level description entity. common attributes like color, brand, material, style etc PTs requires significant differences form, functionality usage location make new PT comparing existing ones . %Another determiner whether adding token product type makes new product type addition new token changes form, function usage location. In example, cordless change drill, utility sink. Obviously, neither definition definite guidelines exhaustive enough always complicated cases exceptions human judgement based knowledge merchandising, customer preference common sense required. %without involving human knowledge usually expensive term time monetary cost. %automatically determine candidate . %Although aforementioned definition would generally help distinguish valid invalid PTs, several challenges task %as depicted Table. %First foremost, crucial determine right level granularity discovered PTs. Very generic PTs generally ambiguous could attributable broad set products different use cases. For example, PT chairs ambiguous comprise outdoor chairs, office chairs, dining chairs chairs types different usage location. %Specifically, domain experts great advantage For example, generic PT range broken granular ones fuel type like gas range, electric range attribute like induction range, convention range. The word ""wood"" material wood rolling pin usage wood glue. % Moreover, often subjective determine level granularity PT discovery stopped based criteria generic PT broken granular PTs. For instance, given generic PT ranges break fuel type features . In example, consider one PT one attribute; alternatively combined construct granular PT. % Another challenge automatically identify token PT attribute not. As example, consider wood rolling pin wood glue; token wood latter change use case glue, former material. However, leveraging human knowledge large scale problems usually timely expensive. To reduce cost, paper proposes %The main contribution paper follows: proposing active learning framework minimizes human effort PT discovery 1) identifying high quality candidates using phrase mining user behavior. 2) limiting number PT candidates human validation. %%%%%%% This work introduces advanced GA hyperparameter optimization applies machine translation optimization. We demonstrate optimization hyperparameters via GA outperform random selection hyperparameters. Specifically, outperform defined ability algorithm arrive goal less individuals added. Finally, propose future research directions expected provide additional gains efficacy GAs."," Entity-based semantic search has been widely adopted in modern search engines to improve search accuracy by understanding users' intent. %behind the search terms.  %In e-commerce domain, product type  is a central concept in intent understanding as well as catalog organization. %indicating customers' intent in their search queries.  %be identified from customers' queries for understanding  In e-commerce, an accurate and complete product type  ontology is essential for recognizing product entities in queries and retrieving relevant products from catalog.  However, finding product types  to construct such an ontology is usually expensive due to the considerable amount of human efforts it may involve.  In this work, we propose an active learning framework that efficiently utilizes domain experts' knowledge for PT discovery.  We also show the quality and coverage of the resulting PTs in the experiment results."
"Distributional word representations trained large-scale corpora widely used modern natural language processing systems, aims describe meaning words sentences vectorized representations . Recent studies addressed state-of-the-art word embedding performance various NLP tasks, start focus evaluate performance different word embeddings accurately. However, \citet{Tsvetkov15} \citet{Chiu16} demonstrated even word embedding, existing evaluation methods provide constantly correlative results intrinsic evaluation extrinsic evaluation. Therefore, evaluating performance word embeddings unified metric challenging NLP tasks. \citet{Hollenstein19} proposed new evaluation framework called CogniVal, applied traditional neural networks regression considered intrinsic extrinsic measurements based collected human natural language processing-related cognitive data sources across three modalities: electroencephalography , functional magnetic resonance imaging , eye-tracking. CogniVal potentially identified pioneer multi-modal cognitive word embedding evaluation framework, conducts vectorized word embeddings evaluation predicting much reflect semantic representations cognitive data sources recorded human processing natural language. However, CogniVal framework ignored measure characteristics human physiological signals. Specifically, three modalities cognitive data used experiment featuring non-stationary non-linear motions . Inspired \citet{Zekri08,Bodyanskiy13}, assume neural networks fuzzy systems computational intelligence methods suitable tools modelling expert knowledge dealing uncertain non-linear processes non-stationary time series dynamic system, approximate reasoning characteristics fuzzy systems could present practical model handle uncertainty disturbances real data complex hybrid non-linear non-stationary problems . For reason, proposed fuzzy-based neural network framework evaluating word embeddings cognitive datasets, name CogniFNN, expects enhance quality evaluating performance word embeddings cognitive data sources , achieve higher ratio significant results random word embeddings well. \paragraph{Contributions} The main contributions study shown follows: In work, propose active learning framework product type discovery leverage domain expertise efficient way. The effectiveness framework demonstrated quality coverage resulting product types experiments well positive business impact. Experiment results also show training data denoising significantly beneficial method performance. There two kinds future work including: 1) Feature engineering PT classifier exploiting textual and/or image data 2) Design denoise procedure add additional component framework."," Word embeddings can reflect the semantic representations, and the embedding qualities can be comprehensively evaluated with human natural reading-related cognitive data sources. In this paper, we proposed the CogniFNN framework, which is the first attempt at using fuzzy neural networks to extract non-linear and non-stationary characteristics for evaluations of English word embeddings against the corresponding cognitive datasets. In our experiment, we used 15 human cognitive datasets across three modalities: EEG, fMRI, and eye-tracking, and selected the mean square error and multiple hypotheses testing as metrics to evaluate our proposed CogniFNN framework. Compared to the recent pioneer framework, our proposed CogniFNN showed smaller prediction errors of both context-independent  and context-sensitive  word embeddings, and achieved higher significant ratios with randomly generated word embeddings. Our findings suggested that the CogniFNN framework could provide a more accurate and comprehensive evaluation of cognitive word embeddings. It will potentially be beneficial to the further word embeddings evaluation on extrinsic natural language processing tasks."
"Reinforcement Learning~ methods increasingly used solving sequential decision-making problems natural language inputs, like text-based games chat-bots personal conversation assistants. In work, focus Text-Based Games~, require solving goals like ``Obtain coin kitchen'', based natural language description agent's observation environment. To interact environment, agent issues text-based action commands~ upon receives reward signal used training RL agent. % generalization problem Traditional text-based RL methods focus problems partial observability large action spaces. However, topic generalization unseen TBGs less explored literature. We show previous RL methods TBGs often show poor generalization unseen test games. We hypothesize overfitting caused due presence irrelevant tokens observation text, might lead action memorization. % ~(eg. every time agent. To alleviate problem, propose CREST, first trains overfitted base model original observation text training games using Q-learning. Subsequently, apply observation pruning that, episode training games, remove observation tokens semantically related base policy's action tokens. Finally, re-train bootstrapped policy pruned observation text using Q-learning improves generalization removing irrelevant tokens. Figure shows illustrative example method. Experimental results TextWorld games show proposed method generalizes unseen games using almost x-x fewer training games compared SOTA methods; features significantly faster learning. In paper, proposed CogniFNN framework using fuzzy-based neural networks explore non-linear non-stationary characteristics physiological signals improving evaluation performance word embeddings cognitive datasets recorded subjects understanding natural language . Our findings showed CogniFNN achieved smaller prediction errors higher significant ratios context-independent context-sensitive word embeddings 15 cognitive data sources across EEG, fMRI eye-tracking. Our contributions could useful evaluation strategy beneficial exhaustive investigation word embedding evaluations corresponding cognitive features."," 		We show that Reinforcement Learning~ methods for solving Text-Based Games~ often fail to generalize on unseen games, especially in small data regimes. To address this issue, we propose Context Relevant Episodic State Truncation~ for irrelevant token removal in observation text for improved generalization. Our method first trains a base model using Q-learning, which typically overfits the training games. The base model's action token distribution is used to perform observation pruning that removes irrelevant tokens. A second bootstrapped model is then retrained on the pruned observation text. Our bootstrapped agent shows improved generalization in solving unseen TextWorld games, using $10$x-$20$x fewer training games compared to previous state-of-the-art~ methods despite requiring less number of training episodes."
"As key step constructing knowledge graph, relation extraction task extract relation entities expressed sentence. Previous work largely focused intra-sentence binary relation extraction, goal extract relation entity pair sentence. However, relations require two entities may span multiple sentences, defined n-ary cross-sentence relation extraction. As example shown Table, relation ``educate'' includes four entities, person's ""name``, ""academic degree``, ""academic major`` ""school``. In addition, relation spans four sentences example. Some prior works applied supervised learning approach tackle task, require large-scale labeled training data. \end{table} To obtain large-scale annotated data, work assumes consecutive sentences contain entities relation knowledge base, sentences whole describe relation. This assumption referred distant supervision n-ary cross-sentence relation extraction task. Even though methods based distant supervision quickly annotate sentences, still two main limitations: 1) suffer noisy labeling problem; 2) strong distant supervision assumption consider non-consecutive sentences, reduces generalizability trained model. As example shown Table, sentences 18th 20th positions describe fact labeled using distant supervision consecutive. The first sentence incorrectly labeled noisy labeled data, describes Alan Turing's work instead education. To address first limitation, propose train sentence distribution estimator , two-level agent reinforcement learning model. This provides well-trained model select high-quality labeled sentence groups alleviate impact noisy data. There previous works applying reinforcement learning remove binary intra-sentence noisy data achieve state-of-the-art performance. When applying RL n-ary cross-sentence relation extraction, key challenge RL model learn sentence features, also know context relation sentence. In paper, process selecting sentences influenced feature sentence itself, also indicators defined , measure semantic relationship sentences. Moreover, whether sentence selected state going affect decision next state. This state transition property provides ability choose best combination sentences sentence group. To address second limitation, relax strong distant supervision assumption lies heart prior work replacing weaker distant supervision assumption. The assumption sentence least one main entity two supplementary entities annotated relation entities. We follow Wikidata Knowledge Base scheme, main entity ``value'' fact supplementary entity ``qualifer'' fact. This assumption introduces non-consecutive sentences propose novel universal relation extractor encode consecutive non-consecutive sentence groups. This relation extractor self-attention soft attention mechanism layer, compares similarity word-level features relation query vectors. The relation extractor also encodes sentence via Piece-wise Convolution Neural Network layer. The PCNN output used learn information transforms sentences via non-linear transformation layer. We present method improving generalization TBGs using irrelevant token removal observation texts. Our bootstrapped model trained salient observation tokens obtains generalization performance similar SOTA methods, x-x fewer training games, due better generalization; shows accelerated convergence. In paper, restricted analysis TBGs feature similar domain distributions training test games. In future, wish handle topic generalization presence domain differences novel objects, goal statements test games seen training."," The models of n-ary cross sentence relation extraction based on distant supervision assume that consecutive sentences mentioning $n$ entities describe the relation of these $n$ entities. However, on one hand, this assumption introduces noisy labeled data and harms the models' performance. On the other hand, some non-consecutive sentences also describe one relation and these sentences cannot be labeled under this assumption. In this paper, we relax this strong assumption by a weaker distant supervision assumption to address the second issue and propose a novel sentence distribution estimator model to address the first problem. This estimator selects correctly labeled sentences to alleviate the effect of noisy data is a two-level agent reinforcement learning model. In addition, a novel universal relation extractor with a hybrid approach of attention mechanism and PCNN is proposed such that it can be deployed in any tasks, including consecutive and non-consecutive sentences. Experiments demonstrate that the proposed model can reduce the impact of noisy data and achieve better performance on general n-ary cross sentence relation extraction task compared to baseline models."
"Healthcare information systems store huge volumes electronic health records contain detailed visit information patients period time. The data structured three levels top bottom: patient journey, individual visit medical code. Fig. provides typical example structure. An anonymous patient visits his/her doctor, pathology lab admitted hospital different days. The procedures diagnoses performed visits recorded industry-standard medical codes. Each medical code, i.e. International Classification Diseases Current Procedure Terminology , lowest level, records independent observation set codes higher level depict medical conditions patient given time point. At top level, occurrences medical events different time-stamps chained together patient journey, offers informative details. Predicting sequential medical outcomes based patient's journey, hospital re-admissions diagnoses, core research task significantly benefits healthcare management hospitals governments. For example, re-admission statistics could used measure quality care; Diagnoses used understand fully patient's problems relevant medical research. However, researchers encountered many challenges attempts represent patient journeys predict medical outcomes EHR data characteristics temporality, high-dimensionality irregularity. Recurrent neural networks widely used analyze sequential data, unsurprisingly including medical events modelling clinical prediction. For example, Choi et al. proposed multi-level representation learning, integrates visits medical concepts based visit sequences co-occurrence medical concepts. They indirectly exploited RNN embed visit sequences patient representation downstream prediction tasks. Some research works directly employed RNNs model time-ordered patient visits predicting diagnoses. However, length patient visit sequence grows, RNN-based models restricted less expressive power RNNs, vanishing gradient forgetfulness. However, RNN-based models constrained forgetfulness, i.e., predictive power drops significantly sequence patient visits grows long. To memorize historical records, LSTM GRU developed utilize memory gate mechanism mitigating issues. To go further, Song et al. proposed utilise attention mechanism deep framework model sequential medical events. It worth noting sequences medical events often found lengthy, especially patient suffers chronic disease. Hence, due restricted ability RNNs long-term dependency modeling , traditional RNNs, even memory cells gates, usually underperform cases long sequence medical events. In light this, neural model overcome performance bottleneck RNN-based models particularly desirable medical predictions based longitudinal EHR data. %%%%%%%% WHAT THE RELATION BETWEEN SHEN2018DISAN AND THIS ONE?? Directional self-attention networks alleviate long sequence problems improve accuracy predictions, models trained available input information - past future.. CAN WE COME TO THE CONCLUSION: ONE OF CONTRIBUTION IS WE HAVE FULLY CONSIDERED ALL MEDICAL EVENTS COMPARING TO OTHER WORKS THAT CAN ONLY PARTIALLY CONSIDER. % Recently, attention mechanism integrated RNNs model sequential EHRs data, achieves good prediction accuracy. Although attention-based RNNs relatively improves prediction performance, limitations RNNs weaken advantage attention mechanism. In natural language processing , sole attention mechanism used construct sequence sequence model achieves state-of-the-art quality score neural machine translation task. The attention mechanism flexibility sequence length RNN, task/data-driven modeling dependencies. Unlike sequential models, computation easily significantly accelerated existing distributed/parallel computing schemes. However, best knowledge, neural net entirely based attention designed patient journey EHRs data. Most recently, attention mechanisms sprung fore effective integrations RNNs modeling sequential EHR data. So far, approaches shown satisfactory prediction accuracy, argue power attention RNN limited weaknesses RNN . In particular, Vaswani et al. used sole attention mechanism, i.e., multi-head attention self-attention, construct sequence-to-sequence model neural machine translation tasks achieved state-of-the-art quality score. And according Shen et al., self-attention mechanism allows flexibility sequence lengths RNNs task/data-driven modeling contextual dependencies. Unlike recurrent models, attention procedure easy compute computation also significantly accelerated distributed/parallel computing schemes. For example, Song et al. proposed employ 1D CNN model local context use attention mechanism capture long-term dependency sequential medical events. However, applied EHR data instead regular sequential data , current attention models cannot appropriately deal aspects EHR data, arbitrary time-stamps hierarchical data format. Hence, best knowledge, neural network-based entirely attention never designed analytics EHR data. To bridge gap literature address open issues listed above, propose novel attention mechanism called Masked Encoder temporal context fusion. It uses self-attention capture contextual information temporal dependencies patient's visits. Then, propose end-to-end neural network, called Bidirectional temporal encoder Network , predict medical outcomes leveraging learned representation patient journey, representation generated solely proposed attention mechanism, MasEnc. BiteNet constructs multi-level self-attention network represent visits patient journeys simultaneously, using attention pooling stacked MasEnc layers. It worth noting that, compared existed RNN-based methods, BiteNet yield better prediction performance long sequences medical records. Experiments conducted two supervised prediction two unsupervised clustering tasks real-world EHR datasets demonstrate proposed BiteNet model superior prior state-of-the-art baseline methods. To summarize, main contributions are: % The remainders paper organized follows. Section reviews related studies. In Section, briefly discuss preliminary, details model presented Section. In Section, demonstrate experimental results conducted real-world datasets. Lastly, conclude study Section.%and outline future work % We proposed sentence distribution estimator alleviate impact noisy distant supervision labeled data n-ary cross-sentence relation extraction; weaker distant supervision assumption, considers non-consecutive sentences; universal relation extractor, hybrid model attention mechanism non-linear transformation layer encodes non-consecutive consecutive sentence groups. The experiments showed proposed model reduces impact noisy data achieves significantly better performance n-ary cross sentence relation extraction compared SotA models."," Electronic health records  are longitudinal records of a patient's interactions with healthcare systems. A patient's EHR data is organized as a three-level hierarchy from top to bottom: patient journey - all the experiences of diagnoses and treatments over a period of time; individual visit - a set of medical codes in a particular visit; and medical code - a specific record in the form of medical codes. As EHRs begin to amass in millions, the potential benefits, which these data might hold for medical research and medical outcome prediction, are staggering - including, for example, predicting future admissions to hospitals, diagnosing illnesses or determining the efficacy of medical treatments. Each of these analytics tasks requires a domain knowledge extraction method to transform the hierarchical patient journey into a vector representation for further prediction procedure. The representations should embed a sequence of visits and a set of medical codes with a specific timestamp, which are crucial to any downstream prediction tasks. Hence, expressively powerful representations are appealing to boost learning performance. To this end, we propose a novel self-attention mechanism that captures the contextual dependency and temporal relationships within a patient's healthcare journey. An end-to-end bidirectional temporal encoder network  then learns representations of the patient's journeys, based solely on the proposed attention mechanism. We have evaluated the effectiveness of our methods on two supervised prediction and two unsupervised clustering tasks with a real-world EHR dataset. The empirical results demonstrate the proposed BiteNet model produces higher-quality representations than state-of-the-art baseline methods."
"The International Classification Diseases establishes standardized fine-grained classification system broad range diseases, disorders, injuries, symptoms, related health conditions . It primarily intended use healthcare workers, policymakers, insurers national health program managers. The United States incurs administrative costs billions dollars annually arising complex billing infrastructure . Specifically, ICD code assignment typically manual process, consuming average 25 43 minutes per patient depending ICD version . It also prone errors resulting inexperienced coders, variation coders, incorrect grouping codes mistakes patient discharge summaries. These errors costly one report estimating preventable errors ICD coding cost Medicare system 31.6 billion FY2018 .\\\\ Recent work tried automate task ICD code assignment using deep learning. Typically framed multilabel classification problem, researchers trained Convolutional Neural Networks , Recurrent Neural Networks , Transformer models predict ICD-9 codes patient discharge summaries. These models outperformed rule-based approaches utilizing conventional algorithms Logistic Regression, Support Vector Machines, Random Forests etc., achieving competitive micro F1-scores range 42\% - 68\%. Amongst models, based CNNs achieved best performance. Neural network models revolutionized field NLP SOTA models various NLP tasks involve deep neural network models BERT, Bidirectional RNN CNN-based methods. Recent works shown particular vulnerability deep models adversarial examples often produced adding small imperceptible perturbations input data. The state art models NLP exceptions perturbations. provides review different adversarial attacks defense strategies NLP literature. Based granularity perturbation, adversarial attack strategies NLP classified three types - character-level attacks, word-level attacks sentence-level attacks. In character-level attack strategy, model induces noise character level. Character-level noise induced due naturally occurring reasons typos misspellings due intentional modification malicious third-party. existing character-level attack strategies NLP. To accurately model naturally occurring typos, restrict typos distribution based character constraints found standard English keyboard. We follow strategy work. Furthermore, assume white-box setting adversary access gradients loss function wrt model inputs. To knowledge, first work investigate effects adversarial samples clinical NLP domain. In paper, proposed novel prediction model called BiteNet. The model framework comprises MasEnc module captures contextual information temporal relationships visits patient's healthcare journey attention pooling construct hierarchical structure three-levelled EHR data. The output representation patient journey that, learned model, used predict medical outcomes end-to-end sole self-attention network. We evaluated BiteNet's performance model several baseline methods supervised unsupervised tasks, conducted ablation study examine contributions component. The results show BiteNet produces accurate predictions baseline methods.","   Manual annotation of ICD-9 codes is a time consuming and error-prone process. Deep learning based systems tackling the problem of automated ICD-9 coding have achieved competitive performance. Given the increased proliferation of electronic medical records, such automated systems are expected to eventually replace human coders. In this work, we investigate how a simple typo-based adversarial attack strategy can impact the performance of state-of-the-art models for the task of predicting the top 50 most frequent ICD-9 codes from discharge summaries. Preliminary results indicate that a malicious adversary, using gradient information, can craft specific perturbations, that appear as regular human typos, for less than $3\%$ of words in the discharge summary to significantly affect the performance of the baseline model."
"Systematic Generalization characterized capacity understand produce potentially infinite number novel combinations known components . For example, Figure, model could exposed set facts , possible facts inferred combination known components . More recent work examined systematic generalization terms ability ``a model manipulate concepts new combinations trained concepts, limited set combinations'' . This view systematic generalization shifts emphasis reasoning learning. %If model able perfectly accomplish task leveraging existing facts infer new ones, deem model generalizing systematically. Here examine systematic generalization measuring ability model reason new inference step combinations despite trained limited subset them. %, conditioning upon small subset active relationships inference time. Recent developments natural language processing shown Transformer Language Models able capture linguistic knowledge , yield state-of-the-art performance many NLP tasks , including limited answering reading comprehension questions generating factual knowledge little task supervision. These models optimized large corpora predict next words set masked words sentence. While yielding impressive results, clear TLMs rely many superficial patterns data actually learn re-usable skills, enabling generalize new tasks leveraging compositionality skills . Training massive data give certain advantages respect understanding meanings words, conjecture data gives models less experience reasoning inference chains. In work, study less understood issues related well TLMs able perform long chains reasoning. In particular, use TLMs task theorem proving, facts proofs specified natural language. Using theorem proving, test TLMs generate interpretable proofs logically consistent language modeling main objective. % In setting, language models various attractive properties: require logical rule engineering still interpretable, need human annotations, easy extend data. % Language models many advantages theorem provers: require rule engineering, allow practitioners query open class relations, easy extend data, require human supervision train. In particular, study behavior logical reasoners text analyzing generated proofs final answer. This setup allows us evaluate reasoning generalization capabilities TLMs. Recent work suggest language models treated knowledge bases. This directly motivates us investigate language models also learn certain reasoning strategies. Studying abilities give us insights facilitate use models dynamic knowledge bases could infer new knowledge even seen pre-training. For natural language theorem proving, use question answering CLUTRR benchmark suite perform controlled studies. This dataset interest because: compositional nature tasks involved make well suited evaluating systematic generalization, question--answer pair accompanied proof used explain arrive answer. %Our goal obtain state-of-the-art results dataset, rather, We use dataset medium understand reasoning capacity TLMs. Our experiments reveal following: To best knowledge, first use language modeling objective interpretable theorem proving Transformer. We hope work shed light reasoning capacity TLMs inspire future research design models greater reasoning capacity. This work first step exploring robustness NLP models used automatic ICD-9 code classification. Clinical documents different regular documents typically generated fast-paced environment higher average typos non-standard acronyms. As result, clinical NLP models susceptible adversarial samples compared regular NLP model trained standard English dataset. A key extension work would consider dictionary learnt clinical documents biomedical literature defense character-level perturbations. Although might mitigate decrease performance, completely solve it. A rigorous way deal would account tokenization strategy. It easy push word vocabulary using tokenization strategies like word2vec GloVe. Other strategies model words unseen training dataset word-piece byte-pair encoding also break typos introduced models learn sub words standard dictionary. Therefore, defense must account typos fundamental tokenization strategy. An interesting direction would learn word similarity metric map unknown word closer word vocabulary given input word context appears. Building robust tokenization strategy would first step towards robust NLP model character-level adversarial attacks. \medskip \small"," We are interested in understanding how well Transformer language models  can perform reasoning tasks when trained on knowledge encoded in the form of natural language. We investigate their systematic generalization abilities on a logical reasoning task in natural language, which involves reasoning over relationships between entities grounded in first-order logical proofs. Specifically, we perform soft theorem-proving by leveraging TLMs to generate natural language proofs. We test the generated proofs for logical consistency, along with the accuracy of the final inference. We observe length-generalization issues when evaluated on longer-than-trained sequences. However, we observe TLMs improve their generalization performance after being exposed to longer, exhaustive proofs. In addition, we discover that TLMs are able to generalize better using backward-chaining proofs compared to their forward-chaining counterparts, while they find it easier to generate forward chaining proofs. We observe that models that are not trained to generate proofs are better at generalizing to problems based on longer proofs. This suggests that Transformers have efficient internal reasoning strategies that are harder to interpret. These results highlight the systematic generalization behavior of TLMs in the context of logical reasoning, and we believe this work motivates deeper inspection of their underlying reasoning strategies."
"Singing voice synthesis aims synthesize high-quality expressive singing voices based musical score information, attracts lot attention industry academia ~. Singing voice synthesis shares similar pipeline text speech synthesis, achieved rapid progress~ techniques developed text speech synthesis~. Most previous works SVS~ adopt sampling rate used text speech, frequency bands sampling data points enough convey expression emotion high-fidelity singing voices. However, simply increasing sampling rate cause several challenges singing modeling. First, audio higher sampling rate contains wider higher frequency bands\footnote{According Nyquist-Shannon sampling theorem~, sampling rate cover frequency band . Therefore, frequency band audio 48kHz sampling rate spans 024kHz 012kHz 24kHz sampling rate. The additional high frequency band 1224kHz increases difficulty modeling since high-frequency signals complicated less predictive.}, throws challenges predicting frequency spectrums acoustic model. Second, audio higher sampling rate contains longer waveform points much fine-grained fluctuations fixed period time\footnote{For example, 1 second audio waveform contains 48,000 sampling points sampling rate 48kHz.}, also increases difficulty vocoder modeling time domain. As consequence, even previous works~ adopt higher sampling rate , either leverage coarse-grained MFCC~ acoustic features slow autoregressive neural vocoder~, use non-neural vocoder Griffin-Lim~ WORLD~ generate waveform, fully exploit potential high sampling rate thus cannot yield good voice quality. In paper, develop HiFiSinger, SVS system towards high-fidelity singing voices. HiFiSinger adopts FastSpeech~ acoustic model Parallel WaveGAN~ vocoder since popular speech synthesis~ ensure fast training inference speed also high quality. %. Instead using Griffin-Lim, WORLD autoregressive neural model WaveRNN WaveNet vocoder, HiFiSinger leverages To address challenges high sampling rate singing modeling , design multi-scale adversarial training acoustic model vocoder, introduce several additional systematic designs findings crucial improve singing modeling: We conduct experiments internal singing voice synthesis datasets contain 11 hours high-fidelity singing recordings 48kHz sampling rate. Experiment results demonstrate advantages developed HiFiSinger previous singing voice synthesis system. Further ablation studies verify effectiveness design HiFiSinger generate high-fidelity voices. ======================================================== interested understanding current limitations Transformers In work, carefully crafted series experiments understand systematic generalization capacity Transformer language models symbolic reasoning question answering dataset. While powerful language modelers, believe Transformers part future personal assistants, able capture logical statements expressed natural language extrapolate unseen proofs. TLMs state art models wide variety natural language processing tasks. Given widespread use, important understand limits ability reason knowledge expressed natural language extrapolate learned inference procedures unseen problem instances. Our explorations reveal multiple insights. Firstly, TLMs suffer length-generalization issues generating proofs. Secondly, TLMs get better reasoning trained longer, exhaustive proofs. TLMs also generalize better leveraging backward-chaining proofs forward-chaining proofs. properties Transformers provides first important evaluation setting. led us insights allowing us dramatically increase ability systematically generalize simple named entity transformation. In addition, fact backward-chaining proof models perform better forward-chaining ones makes us believe backward-chaining strategies easier use albeit harder generate. Moreover, find no-proof models perform better trained produce proofs. We conjecture benefiting naturally stated logical proof statements requires complex internal representations. At time, believe cases, people would prefer interpretable system cost slightly lower accuracy. We explore directions future research projects. Recent work developing position-agnostic attention mechanisms Transformers useful future direction develop generalizable models. Furthermore, results motivates use neuro-symbolic methods Neural Theorem Provers alternative avenue achieving systems systematically generalize logical compositional reasoning tasks. Combining approaches large pre-trained language models left future research. We hope work inspire research systematic generalization capacity language models motivate study creation neural models greater reasoning capacity. rather greater number parameters training data. shed light symbolic reasoning capacity Transformers inspire future research directions"," High-fidelity singing voices usually require higher sampling rate  with large range of frequency to convey expression and emotion. However, higher sampling rate causes the wider frequency band and longer waveform sequences and throws challenges for singing modeling in both frequency and time domains in singing voice synthesis . Conventional SVS systems that adopt moderate sampling rate  cannot well address the above challenges. In this paper, we develop HiFiSinger, an SVS system towards high-fidelity singing voice using 48kHz sampling rate. HiFiSinger consists of a FastSpeech based neural acoustic model and a Parallel WaveGAN based neural vocoder to ensure fast training and inference and also high voice quality. To tackle the difficulty of singing modeling caused by high sampling rate , we introduce multi-scale adversarial training in both the acoustic model and vocoder to improve singing modeling. Specifically, 1) To handle the larger range of frequencies caused by higher sampling rate , we propose a novel sub-frequency GAN  on mel-spectrogram generation, which splits the full 80-dimensional mel-frequency into multiple sub-bands  and models each sub-band with a separate discriminator. 2) To model longer waveform sequences caused by higher sampling rate, we propose a multi-length GAN  for waveform generation to model different lengths of waveform sequences with separate discriminators. 3) We also introduce several additional designs and findings in HiFiSinger that are crucial for high-fidelity voices, such as adding F0  and V/UV  as acoustic features, choosing an appropriate window/hop size for mel-spectrogram, and increasing the receptive field in vocoder for long vowel modeling in singing voices. Experiment results show that HiFiSinger synthesizes high-fidelity singing voices with much higher quality: 0.32/0.44 MOS gain over 48kHz/24kHz baseline and 0.83 MOS gain over previous SVS systems. Audio samples are available at \url{https://speechresearch.github.io/hifisinger/}."
"Deep speech representation learning subject large number past works. Many techniques developed employed extracting representations speech related tasks speaker recognition speech emotion recognition using deep learning. A significant number deep learning models based Convolutional Neural Networks SR SER . The common approach training CNN models speech-related tasks use time-frequency inputs spectrograms derived raw audio signals. Given sufficient data, deep learning models enable extraction better speech representations compared methods i-Vectors . Attention mechanisms shown positive impact extracting effective deep representations input data, instance speech signals. Considerable improvements accuracy emotion recognition models speaker recognition models examples demonstrate potential benefits using attention mechanisms representation learning. Attention models uphold memory-query paradigm, memory set information items CNN embeddings region spectral representation speech-related tasks , part utterance embedded recurrent cell recurrent neural network . The query derived hidden state model either modality different one . The majority attention models used speech-related tasks, use features extracted utterances using deep neural network information items memory, last hidden layer model query . The general purpose attention model generating deep representations speech signals focus information item individually. The information items considered attention model define granularity model focus on. The spectral representation utterance enables deep learning models consider fine-grained features frequency bins short time-frames. However, typical attention models used audio signals utilize embedding obtained CNN model memory final embedding model query. Using embeddings obtained CNNs, limits granularity attention models large regions spectral representation. On hand, improving granularity CNN embeddings utterance leads large attention models harder train prone over-fitting. While number studies investigating various attention models using CNN embeddings utterances , limited number studies aim use fine-grained attention models spectral representation utterance. In paper, address challenge improving granularity attention models introducing fine-grained attention mechanism audio signals. This mechanism enables deep learning models focus individual frequency bins spectrogram without drawbacks complex models typically involve large number parameters. The aim model attend frequency bin spectrogram representation order boost contribution salient bins. This mechanism also helps reduce importance bins useful information leading accurate representations, also lead robustness respect existing noise input audio. The performance proposed attention mechanism tested using select set prominent CNN architectures two tasks SR SER. The experimental results show deploying fine-grained frequency attention mechanism improves performance benchmark networks substantially less impacted added noise. Our contributions paper follows: The rest paper organized follows. First, discuss related work area speech representation learning followed particular approaches used attention mechanisms purpose. Next, present proposed attention mechanism. In following section, discuss experiments along implementation details. Next, provide results work. And finally, summarize conclude paper. In paper, developed HiFiSinger, SVS system synthesize high-fidelity singing voice. To address challenges caused high sampling rate, designed SF-GAN acoustic model better model wider frequency band, ML-GAN vocoder better model longer waveform sequences, introduced several systematic designs findings important improve singing modeling. Experiment results show HFiSinger synthesizes singing voices much higher quality previous systems. For future work, continue close quality gap synthesized voices recordings, also apply fidelity solution HiFiSinger text speech synthesis."," Deep learning techniques have considerably improved speech processing in recent years. Speech representations extracted by deep learning models are being used in a wide range of tasks such as speech recognition, speaker recognition, and speech emotion recognition. Attention models play an important role in improving deep learning models. However current attention mechanisms are unable to attend to fine-grained information items. In this paper we propose the novel Fine-grained Early Frequency Attention  for speech signals. This model is capable of focusing on information items as small as frequency bins. We evaluate the proposed model on two popular tasks of speaker recognition and speech emotion recognition. Two widely used public datasets, VoxCeleb and IEMOCAP, are used for our experiments. The model is implemented on top of several prominent deep models as backbone networks to evaluate its impact on performance compared to the original networks and other related work. Our experiments show that by adding FEFA to different CNN architectures, performance is consistently improved by substantial margins, even setting a new state-of-the-art for the speaker recognition task. We also tested our model against different levels of added noise showing improvements in robustness and less sensitivity compared to the backbone networks."
"Text summarization aims produce condensed summaries covering salient non-redundant information source documents. Recent studies single-document summarization benefit advances neural sequence learning well pre-trained language models make great progress. However, multi-document summarization tasks, neural models still facing challenges often underperform classical statistical methods built upon handcrafted features. We observe two major challenges adapting advanced neural SDS methods MDS: Large search space. MDS aims producing summaries multiple source documents, exceeds capacity neural SDS models sets learning obstacles adequate representations, especially considering MDS labeled data limited. For example, 287K training samples CNN/Daily Mail SDS dataset 30 DUC 2003 MDS dataset . High redundancy. In MDS, statement even sentence spread across different documents. Although SDS models adopt attention mechanisms implicit measures reduce redundancy, fail handle much higher redundancy MDS effectively . There attempts solve aforementioned challenges MDS. Regarding large search space, prior studies perform sentence filtering using sentence ranker take top-ranked sentences. However, hard cutoff search space makes approaches insufficient exploration labeled data limited ranker since sentences discarded,\footnote{ set 7 in~\citet{lebanoff-etal-2018-adapting} 15 in~\citet{zhang-etal-2018-adapting}. One document set DUC 2004, example, averages 265.4 sentences.} albeit discarded sentences important could favored. As result, although studies perform better directly applying base SDS models MDS, outperform state-of-the-art MDS methods. Regarding high redundancy, various redundancy measures proposed, including heuristic post-processing counting new bi-grams cosine similarity, dynamic scoring compares source sentence current summary like Maximal Marginal Relevance . Nevertheless, methods still use lexical features without semantic representation learning. One extension studies uses capsule networks improve redundancy measures. However, capsule networks pre-trained SDS fixed feature inputs classical methods without end-to-end representation learning. In paper, present deep RL framework, MMR-guided Reinforcement Learning MDS, unifies advances SDS one classical MDS approach, MMR end-to-end learning. \ours addresses MDS challenges follows: \ours overcomes large search space soft attention. Compared hard cutoff, soft attention favors top-ranked candidates sentence ranker . However, discard low-ranked ones, ranker imperfect, sentences ranked low may also contribute high-quality summary. Soft attention restrains search space allowing exploration limited labeled data, leading better representation learning. Specifically, \ours infuses entire prediction MMR neural module attending important sentences downplaying rest instead completely discarding them. \ours resolves high redundancy MDS unified way: explicit redundancy measure MMR incorporated neural representation current state, two modules coordinated RL reward optimization, encourages non-redundant summaries. We conduct extensive experiments ablation studies examine effectiveness \ours. Experimental results show \ours achieves state-of-the-art performance DUC 2004 TAC 2011 datasets . A comparison various combination mechanisms demonstrates benefits soft attention large search space MDS . In addition, ablation manual studies confirm \ours superior applying either RL MMR MDS alone, MMR guidance effective redundancy avoidance . \start{Contributions} We present RL-based MDS framework combines advances classical MDS neural SDS methods via end-to-end learning. We show proposed soft attention better hard cutoff previous methods learning adequate neural representations. Also, infusing neural representation current summary explicit MMR measures significantly reduces summary redundancy. We demonstrate \ours achieves new state-of-the-art results benchmark MDS datasets. \section{Conclusions}"," While neural sequence learning methods have made significant progress in single-document summarization , they produce unsatisfactory results on multi-document summarization . We observe two major challenges when adapting SDS advances to MDS:  MDS involves larger search space and yet more limited training data, setting obstacles for neural methods to learn adequate representations;  MDS needs to resolve higher information redundancy among the source documents, which SDS methods are less effective to handle. To close the gap, we present \ours, Maximal Margin Relevance-guided Reinforcement Learning for MDS, which unifies advanced neural SDS methods and statistical measures used in classical MDS.  \ours casts MMR guidance on fewer promising candidates, which restrains the search space and thus leads to better representation learning. Additionally, the explicit redundancy measure in MMR helps the neural representation of the summary to better capture redundancy. Extensive experiments demonstrate that \ours achieves state-of-the-art performance on benchmark MDS datasets. In particular, we show the benefits of incorporating MMR into end-to-end learning when adapting SDS to MDS in terms of both learning effectiveness and efficiency.\footnote{Code can be found at \url{https://github.com/morningmoni/RL-MMR}.}"
"Natural language generators task-oriented dialogue take meaning representations inputs, i.e. set dialogue acts attributes values, output natural language utterances realizing MR. Current NLGs trained end-to-end corpus MR/utterance pairs MRs cover specific set dialogue acts domain attributes. Creation datasets labor intensive time consuming. However, building NLG new domain ontology, possible re-use data built existing domain ontologies. If possible, would speed development new dialogue systems significantly. \end{footnotesize} blue} NYC {\color{darkred}red}. Some attributes shared sources: unique dialogue acts attributes source underlined E1 E2. E3 illustrates MR target test set dub COM. All MRs COM combine dialogue acts attributes E2E NYC. There training data corresponding E3. %: goal task re-use %the existing training data E2E NYC %and train NLG %can generalize unseen combinations %as shown E3. The MRs illustrate attribute values, e.g. {\sc restaurant name, point-of-interest}, delexicalized improve generalization.} \end{figure*} Here experiment one version task building new domain ontology based {\bf combining} two existing ontologies, utilizing training data. Each dataset based different domain ontology restaurant domain, novel attributes dialogue acts seen dataset, e.g. one attributes representing family friendly rating information, one attributes decor service. Our aim NLG engine realize utterances extended {\bf combined} ontology seen training data, e.g. MRs specify values family friendly, rating, decor service. Figure illustrates task. Example E1 training set referred NYC, previous work controllable sentence planning NLG , E2 E2E NLG shared task . As describe detail Section, E1 E2 based two distinct ontologies. Example E3 %in Figure illustrates task addressed paper: create test set novel MRs combined ontology, train model generate high quality outputs individual sentences realize attributes ontologies. To knowledge, completely novel task. While common practice NLG construct test sets MRs realize attribute combinations seen training, initial experiments showed task surprisingly adversarial. However, methods supporting type generalization extension new cases would great benefit task-oriented dialogue systems, common start restricted set attributes enlarge domain ontology time. New attributes constantly added databases restaurants, hotels entities support better recommendations better search. Our experiments test whether existing data covers subset attributes used produce NLG enlarged ontology. We describe create test set --- call {\sc com} --- combined MRs test different methods creating NLG. A baseline sequence-to-sequence NLG model slot error rate .45 produces semantically perfect outputs 3.5\% time. To improve performance, experiment three different ways conditioning model incorporating side constraints encode source attributes MR . %, i.e.,whether E2E NYC both. However, increases proportion semantically perfect model outputs 3.5\% 5.5\% . We propose motivate novel self-training method greatly improves performance learning model mistakes. An error analysis shows models {\bf do} produce many {\bf combined} outputs, errorful semantics. We develop rule-based text-to-meaning semantic extractor automatically creates novel correct MR/text training instances errorful model outputs, use self-training experiments, thus learning mistakes . We validate text-to-meaning extractor human evaluation. We find model trained process produces SERs .03, semantically perfect outputs 81\% time . A human evaluation shows outputs also natural, coherent grammatical. Our contributions are: We start Section defining task detail, describe models metrics Section, results Section. We discuss related work throughout paper relevant conclusion Section. We present reinforcement learning framework MDS unifies neural SDS advances Maximal Marginal Relevance end-to-end learning. The proposed framework leverages benefits neural sequence learning statistical measures, bridging gap SDS MDS. We conduct extensive experiments benchmark MDS datasets demonstrate superior performance proposed framework, especially handling large search space high redundancy MDS. In future, investigate feasibility incorporating classical MDS guidance abstractive models large-scale pre-training challenging settings document set may contain hundreds even thousands documents."," Natural language generators  for task-oriented dialogue typically take a meaning representation  as input, and are trained end-to-end with a corpus of MR/utterance pairs, where the MRs cover a specific set of dialogue acts and domain attributes. Creation of such datasets is labor intensive and time consuming. Therefore, dialogue systems for new domain ontologies would benefit from using data for pre-existing ontologies.   Here we explore, for the first time, whether  it  is possible to train  an NLG for a new {\bf larger} ontology using  existing training sets for the restaurant domain, where each set is based on a {\bf different} ontology. We create a new, larger {\bf combined}  ontology,  and then  train an NLG  to produce utterances covering it. For example, if one dataset has  attributes for family friendly and   rating information, and the other has attributes for decor and service, our aim is an NLG for the combined ontology that can produce utterances that realize values for family friendly, rating, decor and   service.   Initial experiments with a baseline neural sequence-to-sequence model show that this task is surprisingly  challenging. %, and that in many cases the models do not produce combine attributes from the two original ontologies, and when they do the semantics are highly errorful. We then develop a  novel {\bf self-training} method that identifies  model outputs, automatically  constructs a corrected MR input to form a new  training pair, and then repeatedly adds these new instances back into the training data. %that combine attributes from both sources %and then automatically construct an MR that matches the string that %was actually generated .   %We repeatedly construct and add these %new instances back into training, resulting in a self-trained %model that produces semantically perfect outputs 83\% of the time. %We repeatedly construct and add these %new instances back into training, resulting  We then test the resulting model on a new test set. The result is a self-trained model whose performance is an absolute 75.4\% improvement over the baseline model.  %can produce semantically perfect outputs 83\% of the time. %improves the proportion of semantically perfect outputs for the new combined ontology  from 5.5\% to 83\%.  We also report a human qualitative evaluation of the final  model showing that it achieves high naturalness, semantic coherence and grammaticality."
"In recent years, neural LMs shown profound abilities generate texts could almost indistinguishable human writings . Neural LMs could used generate concise summaries , coherent stories , complete documents given prompts . It natural question source extent rhetorical knowledge: What makes neural LMs articulate, how? While recent works query linguistic knowledge , open question remain unanswered. We hypothesize contextualized neural LMs encode rhetorical knowledge intermediate representations, would like quantify extent encode rhetorical knowledge. To verify hypothesis, hand-craft set 24 rhetorical features including used examine rhetorical capacities students , evaluate well neural LMs encode rhetorical features representations encoding texts. Recent work started evaluate encoded features hidden representations. Among them, probing popular choice. Previous work probed morphological , agreement , syntactic features . Probing involves optimizing simple projection model representations features. The loss optimization measures difficulty decode features representations. In work, use probe containing self attention mechanism. We first project variable-length embeddings fixed-length latent representation per document. Then, apply simple diagnostic classifier detect rhetorical features latent representation. This design probe reduces total number parameters, enable us better understand model's ability encode rhetorical knowledge. We find that: These observations allow us investigate mechanisms neural LMs better understand degree encode linguistic knowledge. We demonstrate discourse-level features queried analyzed neural LMs. All code parsed tree data available github. \nocite{KedzieMcKeown19,shah2018bootstrapping} \nocite{budzianowski2018multiwoz,eric2019multiwoz,gavsic2015policy,hakkani2016multi,Cervoneetal19,shah2018bootstrapping,ultes2017pydial,chen2017deep} This paper presents first experiments training NLG extended domain ontology re-using existing within-domain training data. We show combine two training datasets restaurant domain, different ontologies, relying distinct sets dialogue acts attributes, generate output combines attributes sources, applying combination neural supervision novel self-training method. While common practice construct test sets unseen attribute combinations, know prior work based constructing new combined ontology. Our experiments show task surprisingly adversarial, consistent recent work suggesting neural models often fail generalize . Work domain transfer shares similar goals experiments presented , methods produce NLG outputs integrate attributes two different sources sentence. Our final results show ability self-training method automatically construct new training instances results high quality natural, coherent grammatical outputs high semantic accuracy. In future, hope generalize novel self-training method build NLG combine two distinct domains, e.g. hotels movies combined restaurants multi-domain dialogue . Ideally systems cover multiple domains able produce utterances seamlessly integrate domains, data exists domain independently. However, may additional challenges combinations. Our results require initial neural models generate {\bf some} combined outputs. It clear whether aspects experimental setup facilitate this, e.g. may require attributes shared across two initial ontologies, shared vocabulary. Thus possible initial models two distinct domains may produce combined outputs, may necessary seed self-training experiments small number combined training instances. We leave issues future work. In future work plan investigate use novel self-training method building NLG combining two distinct domains hotel restaurant information, training data exists hotels alone restaurants alone, e.g. generate The Ritz great place stay rooms lovely restaurant serves excellent nouvelle cuisine, utterance combines attributes domains. This may {\color{red} say assumptions make per one review rebuttal, say method relies least SOME output make combination. worst case collect small amount. May also training methods try force combinations.} In We also plan investigate whether stylistic attributes one source injected utterances another source."," Recently, neural language models  have demonstrated impressive abilities in generating high-quality discourse. While many recent papers have analyzed the syntactic aspects encoded in LMs, to date, there has been no analysis of the inter-sentential, rhetorical knowledge.  In this paper, we propose a method that quantitatively evaluates the rhetorical  capacities of neural LMs. We examine the capacities of neural LMs understanding the rhetoric of discourse by evaluating their abilities to encode a set of linguistic features derived from Rhetorical Structure Theory . Our experiments show that BERT-based LMs outperform other Transformer LMs, revealing the richer discourse knowledge in their intermediate layer representations. In addition, GPT-2 and XLNet apparently encode less rhetorical knowledge, and we suggest an explanation drawing from linguistic philosophy. Our method presents an avenue towards quantifying the rhetorical capacities of neural LMs."
"Our WeChat AI team participates WMT 2020 shared news translation task ChineseEnglish. In year translation task, mainly focus exploiting several effective model architectures, better data augmentation, training model ensemble strategies. For model architectures, mainly exploit two different architectures approaches, namely Transformers RNMT. For Transformers, implement Deeper transformer Pre-Norm, Wider Transformer larger filter-size average attention based transformer. For RNMT, use deep transition based DTMT model. We finally ensemble four kinds models system. For synthetic data generation, explore various methods out-of-domain in-domain data generation. For out-of-domain data generation, explore back-translation method leverage target side monolingual data knowledge distillation method leverage source side golden parallel data. For in-domain data generation, employ iterative in-domain knowledge transfer leverage source side monolingual data golden parallel data. Furthermore, data augmentation methods, including noisy fake data sampling, used training robust NMT models. %We also apply techniques corresponding side golden parallel data. For training strategies, mainly focus parallel scheduled sampling, target denoising minimum risk training algorithm in-domain finetuning. We also exploit self-bleu based model ensemble approach enhance system. As result, constrained ChineseEnglish system achieves highest case-sensitive BLEU score among submitted systems. In remainder paper, start overview model architectures Section. Section describes details systems training strategies. Then Section shows experimental settings results. Finally, conclude work Section. In paper, propose method quantitatively analyze amount rhetorical information encoded neural language models. We compute features based Rhetorical Structure Theory probe RST features contextualized representations neural LMs. Among six popular neural LMs, find contextualization helps generally improve rhetorical capacities LMs, individual models may vary quality. In general, LMs attending contexts directions encode rhetorical knowledge stable manner using uni-directional contexts permuted contexts . Our method presents avenue towards quantitatively describing rhetorical capacities neural language models based unlabeled, target-domain corpus. This method may used selecting suitable LMs tasks including rhetorical acts classifications, discourse modeling, response generation."," We participate in the WMT 2020 shared news translation task on Chinese$\to$English. Our system is based on the Transformer with effective variants and the DTMT architecture. In our experiments, we employ data selection, several synthetic data generation approaches ,  advanced finetuning approaches and self-bleu based model ensemble. Our constrained Chinese$\to$English system achieves 36.9 case-sensitive BLEU score, which is the highest among all submissions."
"Social media become essential element society people communicate exchange information daily basis. The strong influence social media internet users great benefit many individuals, businesses, organizations. Many companies organizations nowadays use social media reach customers, promote products, ensure customer satisfaction. Despite benefits associated widespread use social media, remain vulnerable ill-intentioned activities, openness, anonymity, informal structure platforms contributed spread harmful violent content. \par Although social media service providers policies control ill-intentioned behaviors, rules rarely followed users. Social media providers also allow users report inappropriate content, unreported content may discovered due huge volume data platforms. Some countries restricted use social media, others taken legal action regarding violent harmful content might target particular individuals communities. However, violations might end unpunished due anonymous nature platforms, allowing ill-intentioned users fearlessly share harmful content using nicknames fake identities. One most-shared harmful content social media hate content, might take different forms text, photos, and/or video. Hate speech expression encourages, promotes, justifies violence, hatred, discrimination person group individuals based characteristics color, gender, race, sexual orientation, nationality, religion, attributes. Online hate speech rapidly increasing entire world, nearly \% world population communicates social media. Studies shown nearly \% Americans experienced online hate harassment. This result \% higher results comparable questionnaire conducted . For younger people, results show \% teenagers frequently encounter hate speech social media. \par One dangerous influential forms online hate speech led spread supporters extreme ideologies target racial groups minorities. White supremacists one ideological groups believe people white race superior dominant people races; also referred white nationalism radical ideologies. White supremacists claim undermined dark skin people, Jews, multicultural Muslims, want restore white people power, violently necessary. They also claimed responsibility many violent incidents happened s, including bank robberies, bombings, murders. The white supremacist ideology adopted right-wing left-wing extremists combine white supremacy political movements. \par White supremacist hate speech become significant threat community, either influencing young people hateful ideas creating movements implement goals real world. A study also suggested links hate speech hate crimes others . Several recent brutal attacks also committed supporters radical white supremacists active members social media. The mass shootings New Zealand, Texas, Norway committed white supremacists shared opinions ideologies social media. The attacker two mosques Christchurch, New Zealand, 28 year old man identified white nationalist hero, posted manifesto discussed intent kill people way reinforce sovereignty white extremists. From psychological point view, violent attack must preceded warning behaviors, includes behavior shows violent attack associated it, certain situations predict it. Warning behaviors either real-world markers linguistic markers signs happen real life and/or online. \par Automatic detection white supremacist content social media used predict hate crimes violent events. Perpetrators caught attacks happen examining online posts give strong indications intent make attack. Predicting violent attacks based monitoring online behavior would helpful crime prevention, detecting hateful speech social media also help reduce hatred incivility among social media users, especially younger generations. \par Studies investigated detection different kinds hate speech detecting cyberbullying , offensive language , targeted hate speech general distinguishing types hate speech neutral expressions. Others dealt problem detecting specific types hate speech, anti-religion, jihadist, sexist, racist. However, less attention given detecting white supremacism particular, limited studies. \par White supremacist extremists tend use rhetoric language. They also use specific vocabulary, abbreviations, coded words express beliefs intent promote hatred encourage violence avoid detected traditional detection methods. They mostly use hate speech races religions, claim races undermining them. Figure shows example white supremacist tweet. \par \subsection{Research goal contributions} In paper, aim detect white supremacist tweets based textual features using deep learning techniques. We collected tweets white supremacist accounts hashtags extract word embeddings, labeled subsets data corpus build white supremacist dataset. We applied two approaches: first uses domain-specific word embedding learned corpus classifies tweets using Bidirectional LSTM-based deep model. This approach evaluated multiple dataset achieved different results depending datasets ranged \% \% F1-score. The second approach uses pre-trained language model fine-tune white supremacist dataset using Neural Network dense layer. The BERT language model F1-scores ranged \% \%. Thus, research contribution summarized follow: \par The rest paper proceeds Background Section , provides information methodology used, related studies Literature Review section , detailed description methods Methodology section , details used datasets Dataset section , specifications methodologies results approach Experiments Results section , observations analysis performance approach Discussion section , finally, Conclusion Future Work section . In paper, introduce system WeChat submitted WMT 2020 shared task ChineseEnglish news translation. Our system based Transformer different variants DTMT architecture. Data selection, several effective synthetic data generation approaches , advanced finetuning approaches self-bleu based model ensemble employed proven effective experiments. Our constrained ChineseEnglish system achieved 36.9 case-sensitive BLEU score highest among submissions.","  White supremacists embrace a radical ideology that considers white people superior to people of other races. The critical influence of these groups is no longer limited to social media; they also have a significant effect on society in many ways by promoting racial hatred and violence. White supremacist hate speech is one of the most recently observed harmful content on social media. Traditional channels of reporting hate speech have proved inadequate due to the tremendous explosion of information, and therefore, it is necessary to find an automatic way to detect such speech in a timely manner. This research investigates the viability of automatically detecting white supremacist hate speech on Twitter by using deep learning and natural language processing techniques. Through our experiments, we used two approaches, the first approach is by using domain-specific embeddings which are extracted from white supremacist corpus in order to catch the meaning of this white supremacist slang with bidirectional Long Short-Term Memory  deep learning model, this approach reached a 0.74890 F1-score. The second approach is by using the one of the most recent language model which is BERT, BERT model provides the state of the art of most NLP tasks. It reached to a 0.79605 F1-score. Both approaches are tested on a balanced dataset given that our experiments were based on textual data only. The dataset was combined from dataset created from Twitter and a Stormfront dataset compiled from that white supremacist forum."
"Graph Neural Networks recent years shown provide scalable highly performant means incorporating linguistic information structural biases NLP models. They applied various kinds representations shown effective range tasks, including relation extraction~, question answering~, syntactic semantic parsing tasks~, summarization ~, machine translation~ abusive language detection social networks~. While GNNs often yield strong performance, % models % complex, difficult understand `reasoning' behind predictions. For NLP practitioners, highly desirable know linguistic information given model encodes encoding happens~. The difficulty interpreting GNNs represents barrier analysis. % Furthermore, opaqueness decreases user trust% , impedes discovery harmful biases, complicates error analysis% ~, issue GNNs seemingly small implementation differences make break models~. In work, focus post-hoc analysis GNNs formulate desiderata interpretation method: A simple way perform interpretation use erasure search~, approach wherein attribution happens searching maximal subset features entirely removed without affecting model predictions. % The removal guarantees information discarded features ignored model. This contrasts approaches use heuristics define feature importance, example attention-based methods~ back-propagation techniques~. They guarantee model ignores low-scoring features, attracting criticism recent years . % The trust erasure search reflected literature methods % motivated approximations erasure~, new attribution techniques % evaluated using erasure search ground truth~. Applied GNNs, erasure search would involve search largest subgraph completely discarded. Besides faithfulness considerations conceptual simplicity, discrete attributions would also simplify comparison relevance paths; contrast continuous attribution edges, straightforward extract visualize important paths. Furthermore, contrast techniques based artificial gradients~, erasure search would provide implementation invariance~. This important NLP, models commonly use highly parametrized decoders top GNNs, e.g.~\citet{koncel-kedziorski-etal-2019-text}. While arguably satisfying criteria desiderata, erasure search unfortunately fails tractability. In practical scenarios, infeasible, even approximations, remove one feature time~ underestimate contribution due saturation~, remain prohibitively expensive. Our GraphMask aims meeting desiderata achieving benefits erasure search scalable manner. That is, method makes easily interpretable hard choices whether retain discard edges discarded edges relevance model predictions, remaining tractable model-agnostic~. GraphMask understood differentiable form subset erasure, where, instead finding optimal subset erase every given example, learn erasure function predicts every edge every layer whether connection retained. Given example graph , method returns layer subgraph faithfully claim edges outside influence predictions model. To enable gradient-based optimization erasure function, rely sparse stochastic gates~. In erasure search, optimization happens individually example. This result form overfitting even non-superfluous edges aggressively pruned, similar prediction could made using alternative smaller subgraph; refer problem hindsight bias. % Because model relies parametrized erasure function rather individual per-edge choice, address issue amortizing parameter learning training dataset process similar readout bottleneck introduced in~\citet{schulz2020restricting}. As demonstrate Section, strategy avoids hindsight bias. \paragraph{Contributions} Our contributions follows: The first approach domain-specific experiments , results show domain-specific embedding Bidirectional LSTM model outperforms results used randomly initialized word embedding LSTM. Their accuracy \ , accuracy \ . Although model exceeds accuracy, expected much higher accuracy 2 points, means random initialization perform badly. It important mention white supremacist corpus pretrained word embedding 1 million tweets, increasing corpus size would provide better performance, limited Twitter policies. This experiment shows Bidirectional LSTM based deep model gave good performance white supremacy detection, contradicts, said LSTM give good performance length tweets limited 180 characters; however, 280 characters. \par From feature perspective comparison, Table shows WSW2V performs comparison domain-agnostic models using classifier datasets; WSW2V outperforms models Stormfront Balanced datasets, GloVe Twitter outperforms WSW2V, big size difference data trained on, i.e, GloVe Twitter WSW2V. From classifier perspective comparison, Bidirectional LSTM-based deep model outperforms LR two datasets , LR outperforms Bidirectional LSTM-based deep model Twitter dataset. \par The second experiment involved using BERT model dataset assess performance white supremacist hate speech classification task. As shown Table, BERT outperforms distributional-based embeddings Bidirectional LSTM-based deep model Table. This means BERT model gives closer meaningful vector words due training strategy large corpus trained on. The BERT language model combines advantages domain-agnostic domain-specific embeddings training strategy, petrained large corpus add extra layer training specific task. \par Finally, narcissists often use first-person singular pronouns profane aggressive language social media communications , individuals argumentative personality often comment people posts frequently post similar topics prove point. White supremacists usually associate radical groups either identifying member profiles encouraging promoting ideological perspectives. This study focuses tweets textual features detect white supremacy, account profile features. Thus, focus tweet features help identify white supremacists characteristics. Further account analysis included future work. \section {Conclusion Future work} From experiments, shown combination word embedding, deep learning perform well problem white supremacist hate speech. Some datasets imbalanced simulate real-world data, others balanced assess model performance ideal situation. The BERT model also proved provides state art problem. For future work, corpus size maximized order generate meaningful embeddings, experiments done multiclass problems instead binary class problems combining Google Word2Vec domain-specific Word2Vec. \section {Acknowledgement} I would like thank researchers made resources available research community."," Graph neural networks  have become a popular approach to integrating structural inductive biases into NLP models. However, there has been little work on interpreting them, and specifically on understanding which parts of the graphs  contribute to a prediction. In this work, we introduce a post-hoc method for interpreting the predictions of GNNs which identifies unnecessary edges. % Given a trained GNN model, we learn a simple classifier that, for every edge in every layer, predicts if that edge can be dropped. We demonstrate that such a classifier can be trained in a fully differentiable fashion, employing stochastic gates and encouraging sparsity through the expected  $L_0$ norm. We use our technique as an attribution method to analyze GNN models for two tasks -- question answering and semantic role labeling -- providing insights into the information flow in these models. We show that we can drop a large proportion of edges without deteriorating the performance of the model, while we can analyse the remaining edges for interpreting model predictions."
"Modern methods natural language processing based complex neural network architectures, language units represented metric space . Such phenomenon allows us express linguistic features mathematically. The method obtaining representation interpretations described multiple overview works. Almeida Xex\'eo surveyed different types static word embeddings , Liu et al. focused contextual representations found recent neural models. Belinkov Glass surveyed strategies interpreting latent representation. Best knowledge, first focus syntactic morphological abilities word representations. We also cover latest approaches, go beyond interpretation latent vectors analyze attentions present state-of-the-art Transformer models. %analyzed matrix representation neural networks. %. %\tltodo{Maybe use ToC instroduction section remove here} %The survey organized following way: %In Section, introduce several types NLP models going analyzed. Section shortly describes metrics used evaluate syntactic information captured models. The observations results static contextual word embeddings presented Section. The observations attention matrices different Transformer architectures described Section. We summarize findings Section. %for attention matrices Transformer models. %We conclude survey mentioning supervised approaches enhance syntactic signal. % In paper, propose LUKE, new pretrained contextualized representations words entities based transformer. LUKE outputs contextualized representations words entities using improved transformer architecture using novel entity-aware self-attention mechanism. The experimental results prove effectiveness various entity-related tasks. Future work involves applying LUKE domain-specific tasks, biomedical legal domains.","  Neural networks trained on natural language processing tasks capture syntax even though it is not provided as a supervision signal. %The syntax is captured by the natural language processing models even when not provided as a supervision signal. This %This phenomenon  indicates that syntactic analysis is essential to the understating of language in artificial intelligence systems. % This overview paper covers approaches of evaluating the amount of syntactic information included in the representations of words for different neural network architectures. %This overview paper covers approaches to evaluating of syntactic information in the representation of words in neural networks. We compare the spectrum of model architectures and the training data. We mainly summarize research on English monolingual data on language modeling tasks and multilingual data for neural machine translation systems and multilingual language models.   %Particularly we consider corpora in one language, mainly English used for training Language Models, and multilingual data for Machine Translation Systems and Multilingual Language Models. We describe which pre-trained models and representations of language are best suited for transfer to syntactic tasks.  % We hope that our comparison will help in finding pretrained model for transfer   % The survey covers the research on producing representation of language and evaluation of captured syntactic information. I focus on the works that do not use syntactic supervision during training of the representation, and are obtained on large mono or multilingual corpora.  % The aim of this work is to examine to what extent syntactic features can be extracted from plain text and how it can be compared to expert annotations."
"Texts represent main source knowledge society. However, written various manners, thus creating barrier readers ideas intend convey. Therefore, document comprehension main challenge users overcome, understanding meaning behind troublesome words becoming familiar them. Complex Word Identification task intends identify hard-to-understand tokens, highlighting clarification assisting users grasping contents document. Motivation. Each culture includes exclusive ideas, available ones pass obstacle language. However, properly understanding language prove difficult task. By identifying complex words, users make consistent steps towards adapting culture accessing knowledge offer. As example, entries like ""mayoritariamente"" ""gobernatura"" Spanish environment create understanding problems non-native Spanish speakers, thus requiring users familiarize particular terms. Challenges. The identification task becomes increasingly difficult, proper complex word identification guaranteed. For example, use human identification techniques, language learners may consider new word complex, others might share opinion relying prior knowledge language. Therefore, universal annotation techniques required, ground truth established set words considered complex context. Proposed Approach. We consider state-of-the-art solutions, namely multilingual Transformer-based approaches, address CWI challenge. First, apply zero-shot learning approach. This performed training Recurrent Neural Networks Transformer-based models source language corpus, followed validating testing corpus target language, different source language. A second experiment consists one-shot learning approach considers training three languages , keeping one entry target language, validating testing English, German, Spanish, French, respectively. In addition, performed few-shot learning experiments validating testing language, training others, addition small number training entries target language. The model learns sample structures language and, general, performs better applied multiple entries. Furthermore, training process help model adapt situations number training inputs scarce. The dataset provided CWI Shared Task 2018 used perform experiments. This paper structured follows. The second section describes related work impact CWI task. The third section describes corpus outlines method based multilingual embeddings Transformer-based models, together corresponding experimental setup. The fourth section details results, alongside discussion error analysis. The fifth section concludes paper outlines main ideas, together potential extensions. Main observations: 1. The unsupervised neural networks capture syntax. 2. Contextual embeddings suited probing syntactic features static word embedding. 3. Static word embeddings perform better task require contextual information, syntactic analogies retrival 4. An easy pre-training task, auto-encoding, requires syntactic information lesser extent, therefore worse captured word embeddings. 5. Embeddings obtained language models machine translation system give similar results probed part speech trained corpora size. However, performance rises amount data typically language models trained larger corpora, therfore yield better results transfer learning syntactic probing. 6. Some attention matrices Transformer architecture aligned dependency relations. 7. Usually middle layer Language Models syntactic. 8. In Machine Translation top layers encoder syntactic. This may fact model's output predicted directly output latent representation syntactic. Word Embeddings Neural Networks trained large corpora capture syntactic information. This phenomena In overview, survey syntactic structures latently learned neural models natural language processing tasks. naturally underlay natural language reflected unsupervised models. We compared multiple approaches others described features affect ability capture syntax. The following aspects tend improve performance syntactic tasks POS tagging: Our meta-analysis latent states showed syntactic representation could found middle layers model. They tend capture complex relations initial layers, representations less dependent pretraining objectives top layers. In work We shown extent systems trained non-syntactic task learn grammatical structures. The question leave research whether providing explicit syntactic information model improve performance NLP tasks. ?"," Complex Word Identification  is a task centered on detecting hard-to-understand words, or groups of words, in texts from different areas of expertise. The purpose of CWI is to highlight problematic structures that non-native speakers would usually find difficult to understand. Our approach uses zero-shot, one-shot, and few-shot learning techniques, alongside state-of-the-art solutions for Natural Language Processing  tasks . Our aim is to provide evidence that the proposed models can learn the characteristics of complex words in a multilingual environment by relying on the CWI shared task 2018 dataset available for four different languages . Our approach surpasses state-of-the-art cross-lingual results in terms of macro F1-score on English , German , and Spanish  languages, for the zero-shot learning scenario. At the same time, our model also outperforms the state-of-the-art monolingual result for German ."
"Aspect based sentiment analysis fine-grained sentiment analysis task. ABSA contains several subtasks, four aspect category detection detecting aspect categories mentioned sentences, aspect category sentiment analysis predicting sentiments detected aspect categories, aspect term extraction identifying aspect terms presenting sentences aspect term sentiment analysis classifying sentiments toward identified aspect terms. While aspect categories mentioned sentence predefined categories may occur sentence, aspect terms explicitly appear sentences. Fig. shows example. ACD detects two aspect categories food service ACSA predicts positive negative sentiments toward them. ATE identifies two aspect terms ``taste'' ``service'' ATSA classifies positive negative sentiments toward them. In paper, concentrate ACSA task. The ACD task auxiliary used find aspect category-related nodes sentence constituency parse trees ACSA task. Since sentence usually discusses one aspect categories expresses different sentiments toward them, various attention-based methods developed allocate appropriate sentiment words given aspect categories. Wang et al. first explore attention mechanism ACSA task proposed attention based LSTM . For given sentence aspect category mentioned sentence, AT-LSTM first models sentence via LSTM model, combines hidden states LSTM representation aspect category generate aspect category-specific word representations, finally applies attention mechanism word representations find aspect category-related sentiment words, used predict sentiment aspect category. The constrained attention networks handles multiple aspect categories sentence simultaneously introduces orthogonal sparse regularizations constrain attention weight allocation. The aspect-level sentiment capsules model performs ACD ACSA simultaneously, also uses attention mechanism find aspect category related sentiment words achieves state-of-the-art performances ACSA task. However, models directly use given aspect category find aspect category-related sentiment words, may cause mismatching sentiment words aspect categories unrelated sentiment word semantically meaningful given aspect category. For example Fig., ``Great'' ``bad'' used interchangeably. It hard attention-based methods distinguish word associated aspect category food service among ``good'' ``bad''. To solve problem, The HiErarchical ATtention network first finds aspect terms indicating given aspect cagegory, finds aspect category-related sentiment words depending position information semantics aspect terms. Although HEAT obtains good results, train HEAT, additionally need annotate aspect terms indicating given aspect category, time-consuming expensive. To mitigate mismatch problem, propose Sentence Constituent-Aware Network aspect-category sentiment analysis require additional annotation. SCAN contains two graph attention networks interactive loss function. Given sentence, first use Berkeley Neural Parser generate constituency parse tree. The two GATs generate representations nodes sentence constituency parse tree ACD task ACSA task, respectively. The GAT ACD mainly attends words indicating aspect categories, GAT ACSA mainly attends sentiment words. For given aspect category, interactive loss function helps ACD task find nodes predict aspect category can predict aspect categories. The sentiment words nodes used predict sentiment polarity aspect category ACSA task. Fig. shows constituency parse tree sentence ``Greate taste bad service.''. For aspect category food, SCAN first finds yellow nodes ``Greate taste'' ``taste'', predict sentiment food based sentiment word ``Great'' node ``Great taste''. SCAN excludes blue node ``Great taste bad service.'' food, predict food also service. The main contributions work summarized follows: Complex Word Indentification challenging task, even using state-of-the-art Transformer-based solutions. In work, introduce approach improves previous results cross-lingual monolingual CWI shared task 2018 using multilingual language-specific Transformer models, multilingual word embeddings , different fine-tuning techniques. Fine-tuning model data two different languages creates opportunity grasping features empower better recognize complex words certain contexts, even different language. In addition, zero-shot, one-shot, few-shot learning strategies provide good results, surpassing strong baselines proposing alternative help non-native speakers properly understand difficult aspects certain language. For future work, intend improve results monolingual tasks integrating additional models, XLNet techniques like adversarial training multi-task learning. Furthermore, intend experiment pretraining techniques specific Transformer models, results French benefit cross-lingual transfer learning."," Aspect category sentiment analysis  aims to predict the sentiment polarities of the aspect categories discussed in sentences. Since a sentence usually discusses one or more aspect categories and expresses different sentiments toward them, various attention-based methods have been developed to allocate the appropriate sentiment words for the given aspect category and obtain promising results. However, most of these methods directly use the given aspect category to find the aspect category-related sentiment words, which may cause mismatching between the sentiment words and the aspect categories when an unrelated sentiment word is semantically meaningful for the given aspect category. To mitigate this problem, we propose a Sentence Constituent-Aware Network  for aspect-category sentiment analysis. SCAN contains two graph attention modules and an interactive loss function. The graph attention modules generate representations of the nodes in sentence constituency parse trees for the aspect category detection  task and the ACSA task, respectively. ACD aims to detect aspect categories discussed in sentences and is a auxiliary task. For a given aspect category, the interactive loss function helps the ACD task to find the nodes which can predict the aspect category but can predict other aspect categories. The sentiment words in the nodes then are used to predict the sentiment polarity of the aspect category by the ACSA task. The experimental results on five public datasets demonstrate the effectiveness of SCAN. \footnote{Data and code can be found at https://github.com/l294265421/SCAN}  \keywords{Aspect Category Sentiment Analysis  \and Aspect Based Sentiment Analysis \and Graph Attention Network.}"
"With rapid development e-commerce, online reviews written users become increasingly important reflecting real customer experiences. To ease process review writing, task personalized review generation~ proposed automatically produce review text conditioned necessary context data, \eg users, items, ratings. As mainstream solution, RNN-based models widely applied PRG task. Standard RNN models mainly model sequential dependency among tokens, cannot effectively generate high-quality review text. Many efforts devoted improving kind architecture PRG task, including context utilization, long text generation, writing style enrichment. These studies improved performance PRG task extent. However, two major issues still remain solved. First, generated text likely uninformative, lacking factual description product information. Although several studies try incorporate structural semantic features , mainly extract features review text. Using review data alone, difficult fully capture diverse comprehensive facts unstructured text. Second, studies focus word-level generation, makes difficult directly model user preference higher level. For example, given product, user may focus price, another user may emphasize look. To address issues, propose improve PRG task external knowledge graph . By associating online items KG entities, able obtain rich attribute feature information items, potentially useful PRG task. Although idea intuitive, easy fully utilize knowledge information generating review text task. KG typically organizes facts triples, describing relation two involved entities. It may suitable simply integrate KG information enhance text representations capture user preference due varying intrinsic characteristics different data signals. In order bridge semantic gap, augment original KG user word nodes, construct heterogeneous knowledge graph adding user-item links entity-word links. User-item links formed according user-item interactions, entity-word links formed according co-occurrence review sentences. We seek learn unified semantic space able encode different kinds nodes. Figure presents illustrative example HKG. Given graph, focus two kinds useful information PRG task. First, associated facts regarding item incorporated enrich review content. Second, considering users target nodes, utilize graph infer users' preference specific relation aspect . The two kinds information reflect word- aspect-level enrichment, respectively. To utilize semantics two levels, decompose review generation process two stages, namely aspect sequence generation sentence generation. We aim inject multi-granularity KG information different generation stages improving PRG task. To end, paper, propose KG-enhanced personalized review generation model based capsule graph neural networks~. Compared existing GNN-based methods representing graphs individual scalar features, Caps-GNN extract underlying characteristics graphs capsules graph level dynamic routing mechanism capsule reflects graph properties different aspects. Based constructed HKG, utilize Caps-GNN extract graph properties different aspects graph capsules, may helpful infer aspect- word-level user preference. For aspect sequence generation, propose novel adaptive learning algorithm able capture personalized user preference aspect level, called aspect capsules, graph capsules. We associate aspect capsule unique aspect unsupervised topic models. Furthermore, generation sentences, utilize learned aspect capsules capture personalized user preference word level. Specially, design graph-based copy mechanism generate related entities words copying HKG, enrich review contents. In way, KG information effectively utilized aspect word levels model. %To knowledge, first utilize knowledge graph generate personalized review text, able capture aspect- word-level KG semantics learning user preference. To knowledge, first utilize KG capture aspect- word-level user preference generating personalized review text. For evaluation, constructed three review datasets associating items KG entities. Extensive experiments demonstrate effectiveness KG information model. %%Our code dataset released review period. In paper, We propose Sentence Constituent-Aware Network aspect-category sentiment analysis. The two graph attention modules interactive loss function SCAN form complete solution alleviate mismatch problem. The experimental results five public datasets demonstrate effectiveness SCAN. Future work could consider making representations leaf nodes richer using syntactic information dependency tree sentence modelling inter-aspect category dependencies. ---- Bibliography ---- BibTeX users specify bibliography style 'splncs04'. References sorted formatted correct style."," Personalized review generation  aims to automatically produce review text reflecting user preference, which is a challenging natural language generation task. Most of previous studies do not explicitly model  factual description of products, tending to generate uninformative content. Moreover, they mainly focus on word-level generation, but cannot accurately reflect more abstractive  user preference in multiple aspects.  To address the above issues, we propose a novel knowledge-enhanced PRG model  based on capsule graph neural network~. We first  construct a heterogeneous knowledge graph  for utilizing rich item attributes. We adopt  Caps-GNN to learn graph capsules for encoding underlying characteristics from the HKG. Our generation process contains two major steps, namely aspect sequence generation and sentence generation. First, based on graph capsules, we adaptively learn aspect capsules for inferring the aspect sequence.   Then, conditioned on the inferred aspect label, we design a graph-based copy mechanism to generate sentences by incorporating related entities or words from HKG. To our knowledge, we are the first to utilize knowledge graph for the PRG task. The incorporated KG information is able to enhance user preference at both aspect and word levels. Extensive experiments on three real-world datasets have demonstrated the effectiveness of our model on the PRG task."
"As mentioned Chapter , models trained simply obtain high accuracy held-out sets often learn rely shallow input statistics, resulting brittle models. % susceptible adversarial attacks. For example, \citet{lime} present document classifier distinguishes Christianity Atheism test accuracy . However, close inspection, model spuriously separates classes based words contained headers, ``Posting'', ``Host'', ``Re''. Spurious correlations training test sets allow undesired models obtain high accuracies. Much complex hidden correlations may present arbitrarily large human-annotated dataset . Such correlations may difficult spot, even one identifies them, open question mitigate . In chapter, I investigate direction potential steer neural models away relying spurious correlations provide explanations predictions models. This direction enhancing neural models capability learn natural language explanations training time generate explanations test time. For humans, shown explanations play key role structuring conceptual representations categorisation generalisation . Humans also benefit tremendously reading explanations acting environment first time . Thus, explanations may also used set model better initial position learn correct functionality. Meanwhile, test time, generating correct argumentation addition obtaining high accuracy potential endow model higher level transparency trust. %In work, introduce new dataset models exploiting generating explanations task recognizing textual entailment. Incorporating external knowledge neural model shown result robust models . % show models achieving high accuracies SNLI, , show dramatically reduced performance simpler dataset, model \citet{kim} robust due incorporating external knowledge. Free-form natural language explanations form external knowledge following advantages formal language. First, easy humans provide free-form language, eliminating additional effort learning produce formal language, thus making simpler collect datasets. Secondly, natural language explanations might potentially mined existing large-scale free-form text. Finally, natural language readily comprehensible end-user needs assert reliability model. %Thirdly, formal languages chosen researchers may differ work work therefore models constructed one formal language might trivially transferred another. Meanwhile free-form explanations generic applicable diverse areas research, natural language processing, computer vision, policy learning. Despite potential natural language explanations improve learning transparency, scarcity datasets community, discussed Section . To address deficiency, I collected large corpus K human-annotated explanations SNLI dataset~. I chose SNLI constitutes influential corpus natural language understanding requires deep assimilation fine-grained nuances commonsense knowledge. %A plethora models developed dataset, including previous state-of-the-art universal sentence representations , demonstrates power task dataset. I call explanation-augmented dataset e-SNLI, I release publicly\footnote{The dataset found \url{https://github.com/OanaMariaCamburu/e-SNLI}.} advance research direction training generation free-form natural language explanations. %To demonstrate efficacy e-SNLI dataset, %I show much difficult neural models produce correct natural language explanations based spurious correlations produce correct labels. Further, I develop models predict label generate explanation prediction. I also investigate presence natural language explanations training time guide neural models learning better universal sentence representations better capabilities solve out-of-domain instances. Secondly, I show much difficult neural model produce correct natural language explanations based spurious correlations produce correct labels based correlations. Thirdly, I develop models predict label generate explanation prediction, I investigate correctness generated explanations. Finally, I investigate whether training neural model natural language explanations result better universal sentence representations produced model better performance out-of-domain datasets. \paragraph{Remark.} In chapter, I use concept correct explanation refer correct argumentation ground-truth label instance. This confused concept faithful explanation, refers accuracy explanation describes decision-making process model, described Section . The capability neural model generate correct explanations important aspect development models. For example, correct argumentation may sometimes needed practice, alongside correct final answer. Hence, chapter, I inspect correctness explanations generated introduced neural models. In next chapter, I take step towards verifying faithfulness explanations.% given Chapter . In paper, propose structured meta-learning algorithm open domain dialogue generation infrequent sentence functions. To tackle low-resource issue, proposed model, based recently proposed model-agnostic meta-learning, find transferable internal representations sensible parameters produce large improvement adaptation steps. Moreover, explore structure across fine-grained sentence functions model balance knowledge generalization knowledge customization. Extensive experiments show structured meta-learning algorithm outperforms existing approaches low-resource setting.","  Deep neural networks are becoming more and more popular due to their revolutionary success in diverse areas, such as computer vision, natural language processing, and speech recognition. However, the decision-making processes of these models are generally not interpretable to users. In various domains, such as healthcare, finance, or law, it is critical to know the reasons behind a decision made by an artificial intelligence system. Therefore, several directions for explaining neural models have recently been explored.   In this thesis, I investigate two major directions for explaining deep neural networks. The first direction consists of feature-based post-hoc explanatory methods, that is, methods that aim to explain an already trained and fixed model , and that provide explanations in terms of input features, such as tokens for text and superpixels for images . The second direction consists of self-explanatory neural models that generate natural language explanations, that is, models that have a built-in module that generates explanations for the predictions of the model. The contributions in these directions are as follows.   % In this thesis, I investigate the topic of explaining deep neural networks. This topic is crucial nowadays as neural model are becoming more and more employed in real-world applications due to their high performance in diverse areas, such as computer vision, natural language processing, and speech recognition. However, the decision-making processes learned by these models are not generally human-interpretable. In various real-world applications, such as healthcare, finance, or criminal justice, it is critical to know the reasons behind a decision made by an artificial intelligence system. Therefore, several directions for explaining neural models have recently been explored.   % a series of methods have recently been developed to provide explanations for the predictions of neural models. This thesis brings contributions to two major directions for explaining deep neural networks: feature-based post-hoc explanatory methods and self-explanatory neural models that generate natural language explanations for their predictions. The contributions are as follows.   %However, it is still an open question how to verify whether the explanations provided by these methods are faithfully describing the decision-making processes of the models that they aim to explain. Secondly, it is also an open question whether neural networks can learn from human-provided natural language explanations for the ground-truth labels at training time, as well as support their predictions with natural language explanations at test time, just like humans do.    First, I reveal certain difficulties of explaining even trivial models using only input features. I show that, despite the apparent implicit assumption that explanatory methods should look for one specific ground-truth feature-based explanation, there is often more than one such explanation for a prediction. I also show that two prevalent classes of explanatory methods target different types of ground-truth explanations without explicitly mentioning it. Moreover, I show that, sometimes, neither of these explanations is enough to provide a complete view of a decision-making process on an instance. %These findings can have an important impact on how users choose explanatory methods to best suit their needs.    Second, I introduce a framework for automatically verifying the faithfulness with which feature-based post-hoc explanatory methods describe the decision-making processes of the models that they aim to explain. This framework relies on the use of a particular type of model that is expected to provide insight into its decision-making process. I analyse potential limitations of this approach and introduce ways to alleviate them.  % The introduced verification framework is generic and can be instantiated on different tasks and domains to provide off-the-shelf sanity tests that can be used to test feature-based post-hoc explanatory methods. I instantiate this framework on a task of sentiment analysis and provide sanity tests\footnote{The sanity tests are available at \\ \url{https://github.com/OanaMariaCamburu/CanITrustTheExplainer}.} %to test any feature-based post-hoc explanatory method. Furthermore,  on which I present the performances of three popular explanatory methods. %The results show that these methods may provide unfaithful explanations.  %I also discuss ways in which the current limitations of the framework can further be addressed to lead to more robust and flexible verifications.    %In the process of developing this framework, I uncover several ways in which a particular type of model that is expected to provide insight into its decision-making process can provide misleading such insight. I also introduce checks that can be done to account for this misleading insight in order to use this type of model in the proposed framework.  % %%%%%%%% BEFORE %%%%%%%%%%The framework is generic and can be instantiated on different tasks and domains. I instantiate it on a task of sentiment analysis and provide sanity tests that can be used off-the-shelf\footnote{The tests are available at \url{https://github.com/OanaMariaCamburu/CanITrustTheExplainer}.} to test any feature-based post-hoc explanatory method. Furthermore, I present preliminary results of three explanatory methods on these tests, which raise awareness of the unfaithful explanations that these methods may provide. %I discuss ways in which the limitations of this verification framework can further be addressed and open the path towards more robust and flexible verification frameworks that can be adapted to users' needs.  %%%% this framework relies on the use of a particular type of model that is expected to provide insight into its decision-making process. I analyse the potential limitations of this approach and introduce ways to overcome them. By constructions   %In addition, as a step towards addressing the question of verifying if explanatory methods faithfully describe the decision-making processes learned by the models they aim to explain, I investigate a particular type of self-explanatory neural model and I show three ways in which this type of model can provide misleading explanations. % on its decision-making process.    %Secondly, I present a novel verification framework that can generate a multitude of sanity tests for explanatory methods. I instantiate this framework on the task of sentiment analysis and provide three sanity tests, which can be used off-the-shelf.\footnote{The tests are available at \url{https://github.com/OanaMariaCamburu/CanITrustTheExplainer}.} I present the results of three explanatory methods on these tests. I discuss ways in which the limitations of this verification framework can further be addressed and open the path towards more robust and flexible verification frameworks that can be adapted to users' needs.  % improve their behaviour and performance %exhibit improved behaviour  % if they are additionally given natural language explanations for the ground-truth label at training time  Third, to explore the direction of self-explanatory neural models that generate natural language explanations for their predictions, I collected a large dataset of $\sim\!\!570$K human-written natural language explanations on top of the influential Stanford Natural Language Inference  dataset. I call this explanation-augmented dataset e-SNLI.\footnote{The dataset is publicly available at \url{https://github.com/OanaMariaCamburu/e-SNLI}.} %, which I release publicly\footnote{The dataset is available at \url{https://github.com/OanaMariaCamburu/e-SNLI}.} %to advance research in the direction of training with and generation of natural language explanations.  % Further, I provide empirical evidence that models generating correct explanations are more reliable than models that just predict the correct labels.  % I also train different neural models that generate natural language explanations at test time, and I measure the success of these models to generate correct explanations. I also investigate whether the presence of natural language explanations at training time can lead a model to produce better universal sentence representations and to perform better on out-of-domain datasets. I do a series of experiments that investigate both the capabilities of neural models to generate correct natural language explanations at test time, and the benefits of providing natural language explanations at training time.  Fourth, I show that current self-explanatory models that generate natural language explanations for their own predictions may generate inconsistent explanations, such as ``There is a dog in the image.'' and ``There is no dog in the [same] image.''. Inconsistent explanations reveal either that the explanations are not faithfully describing the decision-making process of the model or that the model learned a flawed decision-making process.  I introduce a simple yet effective adversarial framework for sanity checking models against the generation of inconsistent natural language explanations. Moreover, as part of the framework, I address the problem of adversarial attacks with exact target sequences, a scenario that was not previously addressed in sequence-to-sequence attacks, and which can be useful for other tasks in natural language processing. I apply the framework on a state of the art neural model on e-SNLI and show that this model can generate a significant number of inconsistencies.  This work paves the way for obtaining more robust neural models accompanied by faithful explanations for their predictions.  %My hope is that in the future feature-based post-hoc explanatory methods will be superseded  by robust and accurate neural models that faithfully explain themselves to their human users in natural language."
"We use sequence vectors represent sentence, vector consists semantic-role tag, part-of-speech tag, syntactic semantic tags, refer sequence \textsl{meta sequence}. We present application using meta-sequence learning generate, given article, adequate QAPs form multiple-choice questions. In particular, develop scheme called MetaQA learn meta sequences declarative sentences corresponding interrogative sentences training dataset. % consisting sentences. Combining removing redundant meta sequences yields set called MSDIP , element pair MD corresponding MI, MD MI stand for, respectively, meta sequence declarative sentence interrogative sentence. A trained MetaQA model generates QAPs given declarative sentence follows: Generate meta sequence , find best-matched MD MSDIP, generates meta sequences interrogative sentences according corresponding MIs meta sequence , identifies meta-sequence answer MI, coverts back text form QAP. \begin{comment} Our work put forwards opinion triplet extraction perspective aspect-based sentiment analysis. Existing works applicable opinion triplet extraction shown insufficient, owing use unified aspect-sentiment tagging scheme ignorance interaction elements triplet. Thus, propose multi-task learning framework address limitations highlighting uses joint training, decoupled aspect sentiment prediction, regularization among correlated tasks learning. Experimental results verify effectiveness framework comparison wide range strong baselines. Comparison results different variants proposed framework signify necessity core components framework. Based observations case study error analysis, plan carry research following aspects: robust taggers aspect opinion extraction, flexible evaluation metric triplet extraction, mighty triplet interaction mechanism . File emnlp2020.tex Based style files ACL 2020, Based style files ACL 2018, NAACL 2018/19, Based style files ACL-2015, improvements taken NAACL-2016 style Based style files ACL-2014, were, turn, based ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009, EACL-2009, IJCNLP-2008... Based style files EACL 2006 e.agirre@ehu.es Sergi.Balari@uab.es ACL 08 Joakim Nivre Noah Smith \documentclass[11pt,a4paper]{article} \usepackage[hyperref]{emnlp2020} \usepackage{times} \usepackage{latexsym} \renewcommand{\UrlFont}{\ttfamily\small} \usepackage{booktabs} \usepackage{color} \usepackage{tikz-dependency} \usepackage{amsfonts} \usepackage{amsmath} \usepackage{multirow} \usepackage{makecell} \usepackage{pifont} \newcommand{\cmark}{\ding{51}} \newcommand{\xmark}{\ding{55}} \usepackage[noend]{algpseudocode} \usepackage{algorithmicx,algorithm} This strictly necessary, may commented out, improve layout manuscript, typically save space. \usepackage{microtype} \aclfinalcopy Uncomment line final submission \def\aclpaperid{704} Enter acl Paper ID \setlength\titlebox{5cm} You expand titlebox need extra space show authors. Please make titlebox smaller 5cm ; check camera-ready version ask change back. \newcommand\BibTeX{B\TeX} \title{A Multi-task Learning Framework Opinion Triplet Extraction} \author{Chen Zhang\textsuperscript{1}, Qiuchi Li\textsuperscript{2}, Dawei Song\textsuperscript{1}\Thanks{ Dawei Song corresponding author.}, Benyou Wang\textsuperscript{2} \\ \textsuperscript{1} Beijing Institute Technology, Beijing, China. \\ \textsuperscript{2} University Padova, Padova, Italy. \\ \texttt{\{czhang,dwsong\}@bit.edu.cn}, \texttt{\{qiuchili,wang\}@dei.unipd.it} \\} \date{}"," %Creating multiple-choice questions to assess reading comprehension of a given article %involves generating question-answer pairs  on the main points of the document. We present a meta-sequence representation of sentences and demonstrate how to use meta-sequence learning to generate adequate question-answer pairs  over a given article. %learning scheme to generate adequate QAPs  %via meta-sequence representations of sentences.   %without handcrafted features.  A meta sequence is a sequence of vectors of semantic and syntactic tags. %In particular, %we devise a scheme called MetaQA to %learn meta sequences from training data to form  %pairs of a meta sequence for a declarative sentence   %and a corresponding  interrogative sentences . % indexed for fast retrieval,  On a given declarative sentence, a trained model  converts it to a meta sequence,  finds a matched meta sequence in its learned database,  and   uses the corresponding meta sequence for interrogative sentence to generate QAPs. %We implement MetaQA for the English language using  %semantic-role labeling,  %part-of-speech tagging, and  named-entity recognition, We show that, trained on a small dataset,  our method generates efficiently, on the official SAT practice reading tests, a large number of syntactically and semantically correct QAPs with high accuracy."
"The desire human-like interfaces technical systems, evidenced growing use intelligent assistants, belies need conversational AI systems accomplish wide range tasks, booking restaurants, trains, flights, IT help desk accessing financial accounts transaction records. The wide range tasks necessitated need flexible scalable dialogue system support variety use cases minimal development maintenance effort. Existing dialogue systems broken two major categories, open-domain dialogue systems, focus non-task related conversations, task-oriented dialogue systems, focus user task completion. A typical open-domain system uses end-to-end neural architecture often trained input output utterances human-to-human conversations . While open-domain systems optimized engaging human-like conversation, lack inherent ability interface systems behalf conversation partner. Whereas, typical task-oriented dialogue system seeks understand human intents execute them. This done adopting modularized pipeline architecture three modules sequentially connected shown Fig. . A natural language understanding module recognizes user intents extract useful entity information . The dialogue management module contains two submodules, dialogue state tracker dialogue action policy modules. The DST module tracks mapping entities slots relevant required completing user tasks . The POL module decides actions execute via API. Finally, natural language generation module generates user response based user aspects system actions . In cases, multiple modules combined together, e.g. systems composite NLU DST module , systems composite POL NLG module maps previous utterances dialogue states system response . Despite research advances modular neural approaches, hardly used practice. Industrial dialogue systems, though modularized, still use expensive expert driven rule-based heuristics implemented several lines codes hand-crafted templates, therefore difficult scale number use cases grows. More recently, renewed effort apply single end-to-end neural architecture model task-oriented dialogue use autoregressive transformer architecture . This led reformulation dialogue system design text generation sequence modeling task. While efforts obtained state-of-the-art performance publicly available task-oriented dialogue datasets, still room improvement, especially areas generality practicality. First, problem formulation fails reconcile open-domain task-oriented dialogue model architecture. Also, many cases, address complexity action policy especially towards back-end API system. Finally, fully incorporate control, verification explanation capabilities make modularized approaches attractive. To resolve shortcomings, propose DLGNet-Task, end-to-end neural network simultaneously handles open-domain task-oriented dialogue, way model outputs controllable, verifiable, explainable module level. This system compatible data driven expert driven rule-based approaches. That is, approach simultaneously modular end-to-end, drop-in replacement traditional modular task-oriented dialogue systems. To best knowledge, expressive approach date achieving objective. In summary, able model individual behavior NLU, DM NLG components single neural network model trained end-to-end. Still, model flexible enough allow individual modules separately trained validated line traditional TOD system. % Validation module level provide information additional training needed. It could also help balancing contribution module model finetuned module-level objectives. % The DLGNet-Task model based autoregressive transformer architecture similar DLGNet GPT-2/3 models. To evaluate performance DLGNet-Task, trained model system-level training objective modified MultiWoz2.1 dataset. The dataset modification done mainly support DLGNet-Task design framework . Based widely used TOD metrics, inform rate, success rate, BLEU score , experiments show DLGNet-Task produces comparable performance state-of-the-art approaches MultiWoz2.1 dataset. % addition controllable, verifiable, explainable model's intermediate outputs. In paper, presented technique optimal synthesis multimodal specifications. On benchmark complex regex synthesis problems, showed approach substantially accurate past models, synthesis algorithm finds model-optimal program frequently compared beam search. While evaluated method context regular expressions, technique also applicable synthesis tasks."," Task oriented dialogue  requires the complex interleaving of a number of individually controllable components with strong guarantees for explainability and verifiability. This has made it difficult to adopt the multi-turn multi-domain dialogue generation capabilities of streamlined end-to-end open-domain dialogue systems. In this paper, we present a new framework, DLGNet-Task, a unified task-oriented dialogue system which employs autoregressive transformer networks such as DLGNet and GPT-2/3 to complete user tasks in multi-turn multi-domain conversations. Our framework enjoys the controllable, verifiable, and explainable outputs of modular approaches, and the low development, deployment and maintenance cost of end-to-end systems. Treating open-domain system components as additional TOD system modules allows DLGNet-Task to learn the joint distribution of the inputs and outputs of all the functional blocks of existing modular approaches such as, natural language understanding , state tracking, action policy, as well as natural language generation . Rather than training the modules individually, as is common in real-world systems, we trained them jointly  with appropriate module separations. When evaluated on the MultiWOZ2.1 dataset, DLGNet-Task shows comparable performance to the existing state-of-the-art approaches. Furthermore, using DLGNet-Task in conversational AI systems reduces the level of effort required for developing, deploying, and maintaining intelligent assistants at scale.  % significant improvement over existing approaches, and achieves state-of-the-art performance at both the module and system levels."
"Knowledge graphs represent knowledge world relationships entities, i.e., triples form . Such knowledge resource provides clean structured evidence many downstream applications question answering. KGs usually constructed human experts, time-consuming leads highly incomplete graphs . Therefore automatic KG completion proposed infer missing link relationship head entity tail entity . Existing KG completion work mainly makes use two types information: 1) co-occurrence entities relations 2) deducible reasoning paths tuples. KG embeddings encode entities relations, first type information, together continuous vector space low-rank tensor approximations~. Ours approach utilizes second type information, reasoning path tuples deduced target tuple~. Here reasoning path starts head entity ends tail entity \e{t}: \e{h \overset{r_1}{\rightarrow} e_1 \overset{r_k}{\rightarrow} e_k \overset{r_N}{\rightarrow} t}, \e{r_1 \wedge ... \wedge r_N} forms relation chain infers existence . Therefore methods also referred multi-hop reasoning KGs, learns multi-hop chain rule deduce target . An example chain given Figurea infer whether athlete plays location. Multi-hop reasoning approaches usually utilize richer evidence self-justifiable terms reasoning path rules used predictions, making prediction missing relations interpretable. Despite advantages success multi-hop reasoning approach , target relationship may perfectly inferred single relation chain. There could exist multiple weak relation chains correlate target relation. Figure gives examples cases. These multiple chains could leveraged following ways: reasoning process naturally relies logic conjunction multiple chains ; commonly, instances none chains accurate, aggregating multiple pieces evidence improves confidence , also observed case-based study works. Inspired observations, propose concept multi-chain multi-hop rule set. Here, instead treating single multi-hop chain rule, learn rules consisting small set multi-hop chains. Therefore inference target relationships becomes joint scoring set chains. {We treat set chains one rule and, since different query pairs follow different rules, together set rules reason relation.} Learning generalized multi-hop rule set combinatorial search problem. We address challenge game-theoretic approach inspired by. Our approach consists two steps: selecting generalized multi-hop rule set employing Multi-Layer Perceptron candidate chains; reasoning generalized rule set, uses another MLP model conditional probability target relationship given selected relation chains. The nonlinearity MLP reasoner provides potential model logic conjunction among selected chains rule set. We demonstrate advantage method KG completion tasks FB15K-237 NELL-995. Our method outperforms existing single-chain approaches, showing defined generalized rules necessary many reasoning tasks. In paper, proposed DLGNet-Task, end-to-end neural network framework modeling multi-turn multi-domain task-oriented dialogue. The DLGNet-Task model learns joint distribution nodes dialogue flow graph capable representing task-oriented open-domain dialogue systems. For TOD specific applications, DLGNet-Task also capable learning action policy towards back-end API. Experimental results show DLGNet-Task gives comparable performance existing approaches practical focus. The results also showed performance DLGNet hampered errors original MultiWoz dataset well noise introduced data processing. While DLGNet-task framework capable learning controllable, verifiable explainable end-to-end model. This also shows need consistent TOD datasets properly defined dialogue flow graph. We hope explore direction part future work terms dataset generation data processing pipeline. We also hope improve DLGNet-Task model performance adversarial reinforcement learning. \clearpage \break \iffalse \iftrue \setcounter{table}{0} \renewcommand{\thetable}{A\arabic{table}}"," Multi-hop reasoning approaches over knowledge graphs infer a missing relationship between entities with a multi-hop rule, which corresponds to a chain of relationships. We extend existing works to consider a generalized form of multi-hop rules, where each rule is a set of relation chains.  To learn such generalized rules efficiently, we propose a two-step approach that first selects a small set of relation chains as a rule and then evaluates the confidence of the target relationship by jointly scoring the selected chains. A game-theoretical framework is proposed to this end to simultaneously optimize the rule selection and prediction steps. Empirical results show that our multi-chain multi-hop  rules result in superior results compared to the standard single-chain approaches, justifying both our formulation  of  generalized rules  and the effectiveness of the proposed learning framework."
"Generating text conforms syntactic semantic constraints benefits many NLP applications. To name few, paired data limited, \citet{yang-etal-2019-low} build templates large-scale unpaired data aid training dialog generation model; \citet{Niu2017ASO} \citet{liu-etal-2019-rhetorically} apply style constraints adjust formality rhetoric utterances; \citet{iyyer2018adversarial} \citet{li-etal-2019-Insufficient} augment dataset using controlled generation improve model performance. We study problem syntactically controlled text generation, aims generate target text pre-defined syntactic guidance. Most recent studies topic use sentences exemplars specify syntactic guidance. However, guidance specified sentence vague, syntactic semantic factors tangled. Different them, use constituency parse trees explicit syntactic constraints. As providing full-fledged parse trees target text impractical, require template parse tree sketches top levels full tree . Figure shows pipeline. \citet{iyyer2018adversarial} adopt setting ours. Their proposed SCPN model uses two LSTM encoders respectively encode source text parse tree, connects one decoder additional attention pointer structures. Nonetheless, recurrent encoders suffer information loss compressing whole sequence one vector also incapable properly modeling tree structure constituency parse well. Consequently, network tends ``translate'' parse tree, instead learning real syntactic structures it. % \zc{this sentence still unclear.} We propose Transformer-based syntax-guided text generation method, named \ours. It first expands template constituency parse tree full-fledged parse tree tailored input source text, uses full tree guide text generation. To capture tree structure syntax, apply path attention mechanism text generation model. It forces one node attend nodes located path instead nodes tree. Such mechanism limits information flow among nodes constituency tree direct ancestor-descendant relationship, forcing parent nodes carry information children. In cooperation path attention, linearize constituency trees compact node-level format . Moreover, address challenge properly integrating semantic syntactic information, design multi-encoder attention mechanism . It enables Transformer decoder accept outputs multiple encoders simultaneously. We evaluated model controlled paraphrasing task. The experiment results show \ours outperforms state-of-the-art SCPN method syntactic quality semantic quality. % \zc{ use absolute improvements instead relative ones} Human evaluations prove method generates semantically syntactically superior sentences, semantic syntactic score improvements. % \zc{also give concrete numbers here, much improvements?} Further, find multi-encoder attention mechanism enhances Transformer's ability deal multiple inputs, path attention mechanism significantly contributes model's semantic performance . Our contributions include: 1) multi-encoder attention mechanism allows Transformer decoder attend multiple encoders; 2) path attention mechanism designed better incorporate tree-structured syntax guidance special tree linearization format; 3) syntax-guided text generation method \ours achieves new state-of-the-art semantic syntactic performance. We propose new approach multi-chain multi-hop rule learning knowledge graph completion tasks. First, formalize concept multi-hop rule sets multiple relation chains knowledge graphs. Second, propose game-theoretical learning approach efficiently select predictive relation chains query relation. Our formulation learning method demonstrate advantages two benchmark datasets existing single-chain based approaches. For future work, plan investigate rules beyond chains, well integrate KG embeddings framework.","   We study the problem of using  constituency parse trees as syntactic guidance for controlled text generation. Existing approaches to this problem use recurrent structures, which not only suffer from the long-term dependency problem but also falls short in modeling the tree structure of the syntactic guidance. We propose to leverage the parallelism of Transformer to better incorporate parse trees. Our method first expands a partial template constituency parse tree to a full-fledged parse tree tailored for the input source text, and then uses the expanded tree to guide text generation. The effectiveness of our model in this process hinges upon two new attention mechanisms: 1) a path attention mechanism that forces one node to attend to only other nodes located in its path in the syntax tree to better incorporate syntax guidance; 2) a multi-encoder attention mechanism that allows the decoder to dynamically attend to information from multiple encoders. Our experiments in the controlled paraphrasing task show that our method outperforms SOTA models both semantically and syntactically, improving the best baseline's BLEU score from $11.83$ to $26.27$."
"Recently, great success automatic text summarization generation. To better compare improve performance models, evaluation systems problem interest. The selection evaluation metrics greatly affect assessed quality generated summary thus affect evaluation summarization models. The ideal metric definitely human judgement, often treated gold standard. But human evaluation time-consuming labor-intensive, automatic evaluation metric cannot save human resources also simulate ability human judgement crucial importance. Most existing automatic evaluation methods assess summary comparing reference texts written humans. Some model-free simply use hand-crafted matching functions calculate similarity candidate summary reference . These methods consider reference candidate sequence tokens n-gram blocks. For instance, de facto standard evaluation metric, ROUGE calculates n-gram overlap machine-generated summaries reference summaries. Although methods advantage interpretability efficiency, found correlate poorly human evaluation. To reduce requirement exact word matching, recent work tried match reference candidate summary embedding space words sentences . For instance, BERTScore uses contextual word embeddings generated BERT performs greedy matching obtain maximum cosine similarity two texts. %\citeauthor{clarketal2019sentence} designed metric combines sentence-level embeddings word mover distance calculate distance moving candidate sequence reference transforms distance similarity score, MoverScore combines n-gram embeddings WMD. These methods proved correlate better human judgement ROUGE many datasets, demonstrates effectiveness using contextual embeddings. } , three dimensions focus evaluating linguistic quality summaries.} \end{table*} However, aforementioned methods intrinsic drawbacks: methods always need least one human-generated reference assess candidate summary. References written humans costly obtain. In addition, consider semantic similarities references, i.e. semantic qualities summaries, ignores linguistic qualities important aspects. In paper, propose new unsupervised contrastive learning framework automatically evaluating summary qualities without comparing reference summaries training human ratings. Specifically, design evaluator consider linguistic semantic aspects summary. Then aspect create set negative samples perturbing training samples. We compare scores original training samples negative samples obtain contrastive loss function learn evaluator. The experiments Newsroom CNN/Daily Mail demonstrate new evaluation method much higher correlation human judgement. We summarize contributions follows: We proposed novel syntactically guided text generation method \ours.~ \newcommand{\x}{\bm{x}} \newcommand{\y}{\mathbf{y}} \newcommand{\y}{\bm{y}} \newcommand{\s}{\bm{s}} \newcommand{\bt}{\bm{t}} \newcommand{\z}{\bm{z}} \newcommand{\Z}{\mathcal{Z}} \newcommand{\R}{\mathbb{R}} \newcommand{\prob}{\mathbb{P}} \newcommand{\D}{\mathcal{D}} \newcommand{\E}{\mathbb{E}} \newcommand{\N}{\mathcal{N}} \newcommand{\h}{\mathbf{h}} \newcommand{\Qp}{Q_\phi} \newcommand{\Pt}{P_\theta} \newcommand{\bmu}{\bm{\mu}} \newcommand{\ba}{\bm{\alpha}} \newcommand{\V}{\mathcal{V}} \newcommand{\tsrc}{\bm{s}_{\rm src}} \newcommand{\ttgt}{\bm{s}_{\rm tgt}} \newcommand{\spred}{\hat{\bm{x}}_{\rm tgt}} \newcommand{\stgt}{\bm{x}_{\rm tgt}} \newcommand{\stmpl}{\bm{x}_{\rm tmpl}} \newcommand{\ssrc}{\bm{x}_{\rm src}} \newcommand{\hsrc}{\bm{h}_{\rm src}} \newcommand{\htmpl}{\bm{h}_{\rm tmpl}} \newcommand{\hscr}[1]{\h^{}} \newcommand{\uscr}[2]{#1^{}} \newcommand{\ours}{{GuiG}\xspace} \newcommand{\synexpan}{{\ours.SE}\xspace} \newcommand{\guigen}{{\ours.TG}\xspace} \newcommand{\zc}[1]{{[#1]}} \aclfinalcopy Uncomment line final submission \def\aclpaperid{***} Enter acl Paper ID \setlength\titlebox{5cm} You expand titlebox need extra space show authors. Please make titlebox smaller 5cm ; check camera-ready version ask change back. \newcommand\BibTeX{B\TeX} \title{Transformer-Based Neural Text Generation Syntactic Guidance} \author{Yinghao Li \\ Georgia Institute Technology \\ \texttt{yinghaoli@gatech.edu} \\\And Rui Feng \\ Georgia Institute Technology \\ \texttt{rfeng@gatech.edu} \\\AND Isaac Rehg \\ Georgia Institute Technology \\ \texttt{isaacrehg@gatech.edu} \\\And Chao Zhang \\ Georgia Institute Technology \\ \texttt{chaozhang@gatech.edu} \\} \date{} \begin{document} \maketitle"," Evaluation of a document summarization system has been a critical factor to impact the success of the summarization task. Previous approaches, such as ROUGE, mainly consider the informativeness of the assessed summary and require human-generated references for each test summary. In this work, we propose to evaluate the summary qualities without reference summaries by unsupervised contrastive learning. Specifically, we design a new metric which covers both linguistic qualities and semantic informativeness based on BERT. To learn the metric, for each summary, we construct different types of negative samples with respect to different aspects of the summary qualities, and train our model with a ranking loss. Experiments on Newsroom and CNN/Daily Mail demonstrate that our new evaluation method outperforms other metrics even without reference summaries. Furthermore, we show that our method is general and transferable across datasets."
"Part-of-speech tags dependency parsing formed long-standing union NLP. But equally long-standing question efficacy. % union. %POS tags features parsers. \carlos{Prior prevalence deep learning NLP, shown useful syntactic disambiguation certain contexts} %Certainly nigh-on forgotten pre-deep learning era NLP, seemed useful syntactic disambiguation certain contexts . However, neural network implementations, especially utilise character embeddings, POS tags shown much less useful . Others found POS tags still positive impact using character representations given accuracy predicted POS tags used sufficiently high . \citet{smith2018investigation} undertook systematic study impact features Universal Dependency parsing found using universal POS tags still offer marginal improvement transition-based neural parser. The use fine-grained POS tags still seems garner noticeable improvements %even challenging multi-lingual settings . %By far away common use Latterly, POS tags commonly utilised implicitly neural network parsers multi-learning frameworks leveraged without cost error-propagation . Beyond multi-learning systems, \citet{strzyz2019viable} introduced dependency parsing sequence labelling encoding dependencies using relative positions UPOS tags, thus explicitly requiring runtime. %So even coarse POS tags, universal otherwise, prove superfluous graph- transition-based neural parsers direct features, still many uses them.% dependency parsing. We follow work \citet{smith2018investigation} evaluate interplay word embeddings, character embeddings, POS tags features two modern parsers, one graph-based parser, Biaffine, transition-based parser, UUParser . Similar \citet{zhang2020pos}, focus contribution POS tags evaluate UPOS tags. \paragraph{Contribution} We analyse effect UPOS accuracy two dependency parser systems number UD treebanks. Our results suggest order leverage UPOS tags explicit features neural parsers, prohibitively high tagging accuracy needed, gold tag annotation seems possess exceptionality. We also investigate aspects predicted UPOS tags impact parsing accuracy. In paper, propose new evaluation method field text summarization. We found quality summary evaluated two separate dimensions: semantic quality linguistic quality. Since human-authored references used existing metrics costly, investigate automatic evaluation metrics unsupervised reference-free setting. Leveraging powerful representations BERT, methods achieve highest performance two datasets. Although experiments single-document summarization datasets, method also also extended evaluation multi-document summarization slight changes, especially part semantic quality evaluation."," We present an analysis %contributing to the discussion  on the effect UPOS accuracy has on parsing performance. Results suggest that leveraging UPOS tags as features for neural parsers requires a prohibitively high tagging accuracy and that the use of gold tags offers a non-linear increase in performance, suggesting some sort of exceptionality. We also investigate what aspects of predicted UPOS tags impact parsing accuracy the most, highlighting some potentially meaningful linguistic facets of the problem."
"Conversational Machine Reading challenging rule text may contain literal answer, provide procedure derive interactions . In case, machine needs read rule text, interpret user scenario, clarify unknown user's background asking questions, derive final answer. Taking Figure example, answer user whether suitable loan program, machine needs interpret rule text know requirements, understand meets ``American small business'' user scenario, ask follow-up clarification questions ``for-profit business'' ``not get financing resources'', finally concludes answer ``Yes'' user's initial question. Existing approaches decompose problem two sub-tasks. Given rule text, user question, user scenario, dialog history , first sub-task make decision among ``Yes'', ``No'', ``Inquire'' ``Irrelevant''. The ``Yes/No'' directly answers user question ``Irrelevant'' means user question unanswerable rule text. If user-provided information enough determine fulfillment eligibility, ``Inquire'' decision made second sub-task activated. The second sub-task capture underspecified condition rule text generate follow-up question clarify it. \citet{zhong-zettlemoyer-2019-e3} adopt BERT reason decision, propose entailment-driven extracting editing framework extract span rule text edit follow-up question. The current \sota model EMT uses Recurrent Entity Network explicit memory track fulfillment rules dialog turn decision making question generation. In problem, document interpretation requires identification conditions determination logical structures rules appear format bullet points, in-line conditions, conjunctions, disjunctions, etc. Hence, correctly interpreting rules first step towards decision making. Another challenge dialog understanding. The model needs evaluate user's fulfillment conditions, jointly consider fulfillment states logical structure rules decision making. For example, disjunctions conjunctions conditions completely different requirements user's fulfillment states. However, existing methods considered condition-level understanding reasoning. In work, propose \modelnameshortnsp: \modelnamecap. To better understand logical structure rule text extract conditions it, first segment rule text clause-like elementary discourse units using pre-trained discourse segmentation model. Each EDU treated condition rule text, model estimates entailment confidence scores three states: Entailment, Contradiction Neutral reading user scenario description existing dialog. Then map scores entailment vector condition, reason decision based entailment vectors logical structure rules. Compared previous methods little entailment reasoning use multi-task learning , \modelnameshort first method explicitly build dependency entailment states decisions dialog turn. \modelnameshort achieves new \sota results blind, held test set ShARC. In particular, \modelnameshort outperforms previous best model EMT 3.8\% micro-averaged decision accuracy 3.5\% macro-averaged decision accuracy. Specifically, \modelnameshort performs well simple in-line conditions conjunctions rules still needing improvements understanding disjunctions. Finally, conduct comprehensive analyses unveil limitation \modelnameshort current challenges ShARC benchmark. We find one biggest bottlenecks user scenario interpretation, various types reasoning required. % Code models released facilitate research along line. We evaluated impact POS tag accuracy parsing performance leading graph- transition-based parsers across diverse range UD treebanks, highlighting stark difference using predicted POS tags gold POS tags runtime. We observed non-linear increase performance using gold tags, suggesting somehow exceptional\carlos{, i.e., precisely tag patterns even accurate taggers correctly predict seem important parsing}. \carlostwo{This could due parsers implicitly learning POS tag information, way taggers learn nothing new contribute enough avoid loss performance due errors disrupting parsers learnt.} runtime using gold POS tags non-linear increase performance using gold tags suggesting gold tagged annotation somehow exceptional. This corroborated experiment using treebanks could obtain high scoring taggers. Our analysis also shows practitioners evaluate efficacy using predicted tags given system language. rather assuming negative impact. Beyond global conclusions drawn analysis, We also analysed aspects erroneous tagging predictions greatest impact correlation parsing performance. We observed global trends, \carlos{like importance \texttt{CCONJ},} also language-specific issues highlight need evaluate usefulness POS tags per language. The results also suggest using subset POS tags might effective. potentially even per treebank.","  Document interpretation and dialog understanding are the two major challenges for conversational machine reading. In this work, we propose \modelnameshortnsp, a discourse-aware entailment reasoning network to strengthen the connection and enhance the understanding for both document and dialog. Specifically, we split the document into clause-like elementary discourse units  using a pre-trained discourse segmentation model, and we train our model in a weakly-supervised manner to predict whether each EDU is entailed by the user feedback in a conversation. Based on the learned EDU and entailment representations, we either reply to the user our final decision ``yes/no/irrelevant"" of the initial question, or generate a follow-up question to inquiry more information. Our experiments on the ShARC benchmark  show that \modelnameshort achieves \sota results of 78.3\% macro-averaged accuracy on decision making and 64.0 BLEU1 on follow-up question generation. Code and models are released at \url{https://github.com/Yifan-Gao/Discern}."
". % % % final paper: en-us version % % % space normally used marker % This work licensed Creative Commons % Attribution 4.0 International License. % License details: % \url{http://creativecommons.org/licenses/by/4.0/}. } Neural Language Models become central component NLP systems last years, showing outstanding performance improving state-of-the-art many tasks . However, introduction systems come cost interpretability %and explainability and, consequently, cost obtaining meaningful explanations automated decisions take place. % and, specifically, understanding linguistic predictors - common features earlier systems - encoded models. Recent work begun study models order understand whether encode %are able learn linguistic phenomena even without explicitly designed %forse meglio trained? learn properties . Much work focused analysis interpretation attention mechanisms definition probing models trained predict simple linguistic properties unsupervised representations. Probing models trained different contextual representations provided evidences models able capture wide range linguistic phenomena even organize information hierarchical manner . However, way knowledge affects decisions make solving specific downstream tasks less studied. In paper, extended prior work studying linguistic properties encoded one prominent NLM, BERT , properties affect predictions solving specific downstream task. %, using suite 80 probing tasks. % qui vedere se tenere 'several' perch abbiamo 10 task di classificazione dire che  uno solo diviso 10 ""sotto-task"". We defined three research questions aimed understanding: kind linguistic properties already encoded pre-trained version BERT across 12 layers; knowledge properties modified fine-tuning process; whether implicit knowledge %of properties affects ability model solve specific downstream task, i.e. Native Language Identification . %With aim, firstly perform large suite probing tasks using %on %DOMI: SPOSTIAMO QUESTA PARTE %To answer first two questions, firstly perform large suite probing tasks using %on %the sentence representations extracted internal layers BERT. Each tasks makes explicit particular property sentence, shallow features complex aspects morpho--syntactic syntactic structure , thus making particularly suitable assess implicit linguistic knowledge encoded NLM deep level granularity. %with respect wide spectrum phenomena overing lexical, morpho-syntactic syntactic structure. To tackle first two questions, adopted approach inspired `linguistic profiling' methodology put forth , assumes wide counts linguistic features automatically extracted parsed corpora allow modeling specific language variety detecting changes respect varieties, e.g. complex vs simple language, female vs male--authored texts, texts written L2 language authors different L1 languages. Particularly relevant study, multi-level linguistic features shown highly predictive role tracking evolution learners' linguistic competence across time developmental levels, first second language acquisition scenarios . %when leveraged traditional learning models variety text classification problems, successfully tackled using formal, rather content based aspects text: assessment sentence complexity text readability , identification personal sociodemographics traits author, his/her native language, gender, age etc. prediction evolution learners' linguistic competence across time . %From perspective, approach considered particular implementation `linguistic profiling' methodology put forth , assumes wide counts linguistic features automatically extracted parsed corpora allow modeling specific language variety detecting way changes respect varieties, e.g. complex vs simple language, female vs male--authored texts, texts written L2 language authors different L1 languages. Given strong informative power features encode variety language phenomena across stages acquisition, assume also helpful dig issues interpretability NLMs. In particular, would like investigate whether features successfully exploited model evolution language competence similarly helpful profiling implicit linguistic knowledge NLM changes across layers tuning specific downstream task. We chose NLI task, i.e. task automatically classifying L1 writer based his/her language production learned language . %Secondly, investigate type degree variations linguistic information fine-tuning pre-trained model 10 distinct datasets used solve Native Language Identification , i.e. task automatically classifying L1 writer based his/her language production learned language . As shown , linguistic features play important role NLI tackled sentence--classification task rather traditional document--classification task. %NLI addressed exploiting linguistic features extracted sentence--level reaching comparable performance obtained state--of--the--art models based word embeddings . This reason considered sentence-level NLI classification task particularly suitable probing NLM linguistic knowledge. %perch  un task che per essere risolto  necessario che il modello codifichi un'ampia gamma di informazioni linguistiche e anche perch  un task basato sull'info estratta dalla sentence -come dimostrato da Cimino et al nonostante lo stato dell'arte  stato definito soltanto usando word embeddings %vecchia versione: fine-tuning process based Native Language Identification downstream task. %vecchia versione: -base 10 fine-tuned models obtained training BERT many Native Language Identification tasks. Finally, investigated whether linguistic information encoded BERT involved discriminating sentences correctly incorrectly classified fine-tuned models. To end, tried understand linguistic knowledge model sentence affects ability solve specific downstream task involving sentence. %vecchia versione: Adopting suite 80 probing tasks, firstly perform % We perform experiments using suite 80 probing tasks, corresponds specific/distinct sentence-level feature. We find / We show %The remainder paper organized follows. We start presenting related works closely related study Section highlight main novelties approach. We describe details data , probing tasks models used. Experiments results described Section , . To conclude, Section summarize main findings study. \paragraph{Contributions} In paper: carried in-depth linguistic profiling BERT's internal representations %deep analysis implicit linguistic knowledge stored BERT's internal representations changes across layers using wide suite sentence-level probing tasks, corresponding wide spectrum linguistic phenomena different level complexity; % verify implicit linguistic knowledge stored BERT's internal representations using suite 80 probing tasks corresponding wide range linguistic phenomena different level complexity; showed contextualized representations tend lose precision encoding wide range linguistic properties %general-purpose linguistic properties fine-tuning process; % RIVEDERE 'GENERAL-PURPOSE' COME TERMINE PER DESCRIVERE LE NOSTRE FEATURES showed linguistic knowledge stored contextualized representations BERT positively affects ability solve NLI downstream tasks: BERT stores information features% embeddings/internal representations , higher capacity predicting correct label. In paper, present \modelnameshortnsp, system discourse-aware entailment reasoning conversational machine reading. \modelnameshort explicitly builds connection entailment states conditions final decisions. Results ShARC benchmark shows \modelnameshort outperforms existing methods large margin. We also conduct comprehensive analyses unveil limitations \modelnameshort challenges ShARC. In future, plan explore incorporate discourse parsing current decision making model end-to-end learning. One possibility would frame multi-task learning common encoder. Another direction leveraging current methods question generation improve follow-up question generation sub-task since \modelnameshort\ par previous best model EMT."," In this paper we investigate the linguistic knowledge learned by a Neural Language Model  before and after a fine-tuning process and how this knowledge affects its predictions during several classification problems. We use a wide set of probing tasks, each of which corresponds to a distinct sentence-level feature extracted from different levels of linguistic annotation. We show that BERT is able to encode a wide range of linguistic characteristics, but it tends to lose this information when trained on specific downstream tasks. We also find that BERT's capacity to encode different kind of linguistic properties has a positive influence on its predictions: the more it stores readable linguistic information of a sentence, the higher will be its capacity of predicting the expected label assigned to that sentence."
"Recent emergent-communication studies, renewed astonishing success neural networks, often motivated desire develop neural network agents eventually able verbally interact humans . To facilitate interaction, neural networks' emergent language possess many natural-language-like properties. However, shown that, even emergent languages lead successful communication, often bear core properties natural language . In work, focus one basic property natural language resides tendency use messages close informational optimum. This illustrated Zipf's law Abbreviation , empirical law states natural language, frequent word is, shorter tends . Crucially, ZLA considered efficient property language . Besides obvious fact efficient code would easier process us, also argued core property natural language, likely correlated fundamental aspects human communication, regularity compositionality . Encouraging might hence lead emergent languages also likely develop desirable properties. Despite importance property, \citet{chaabouni:etal:2019} showed standard neural network agents, trained play simple signaling game , develop inefficient code, even displays anti-ZLA pattern. That is, counterintuitively, frequent inputs coded longer messages less frequent ones. This inefficiency related neural networks' ``innate preference'' long messages. In work, aim understanding constraints need introduced neural network agents order overcome innate preferences communicate efficiently, showing proper ZLA pattern. To end, %follow \citet{chaabouni:etal:2019} use reconstruction game two neural network agents: speaker listener. For input, speaker outputs sequence symbols sent listener. The latter needs predict speaker's input based given message. Also, similarly previous work, inputs drawn power-law distribution. We first describe experimental optimization framework . In particular, introduce new communication system called `LazImpa', comprising two different constraints Laziness speaker side Impatience listener side. The former constraint inspired least-effort principle attested ubiquitous pressure human communication . However, constraint applied early, system learn efficient system. We show incrementally penalizing long messages cost function enables early exploration message space prevents converging inefficient local minimum. The constraint, listener side, relies prediction mechanism, argued important language comprehension \citep[e.g.,][]{federmeier2007, altmann2009}, achieved allowing listener reconstruct intended input soon possible. We also provide two-level analytical method: first, metrics quantifying efficiency code; second, new protocol measure informativeness . Applying metrics, demonstrate that, contrary standard speaker/listener agents, new communication system `LazImpa' leads emergence efficient code. The latter follows ZLA-like distribution, close natural languages . Besides plausibility introduced constraints, new communication system is, first, task- architecture-agnostic , second allows stable optimization speaker/listener. We also show listener speaker constraints fundamental emergence ZLA-like distribution, efficient natural language . In paper studied kind linguistic properties stored internal representations learned BERT fine-tuning process implicit knowledge correlates model predictions trained specific downstream task. Using suite 68 probing tasks, showed pre-trained version BERT encodes wide range linguistic phenomena across 12 layers, order probing features stored internal representations necessarily reflect traditional division respect linguistic annotation levels. We also found BERT tends lose precision encoding set probing features fine-tuning process, probably storing task--related information solving NLI. QUI NON SERVE : Interestingly, noticed features encoding verbal tense knowledge ones decreases significantly fine-tuned models. We thus think work needs done investigate kind discriminant linguistic properties properties emerge fine-tuning process. This particularly evident models fine-tuned classification language pairs belonging family . Se possibile Scrivere meglio: Finally, showed implicit linguistic knowledge encoded BERT positively affects strongly correlated ability solve tested downstream tasks. In particular, first showed that, regardless layer model taken account, probing features involved discriminating sentences correctly incorrectly classified fine-tuned models. Second, noticed features probing model performance show improvement BERT correctly predicts L1 native speaker, especially true pre--trained model. This suggests capacity encode linguistic information influence predictions. decisions. In future work, would like extend approach NLMs, ELMo XLNet , investigate linguistic information implicitly encoded models affects different downstream tasks. The demonstrated influence linguistic competence NLM classification tasks would allow us develop NLMs able maximize In future work, plan study linguistic information encoded NLM arise training, performing probing tasks several sentence representations extracted pre-training phase. The aim investigation studying new strategies maximize linguistic competence NLM, example adding pre-training process specific linguistic tasks. Moreover, would interesting study linguistic information encoded NLM arise evolve models trained, performing probing tasks several sentence representations extracted pre-training process. DA FELICE: se riusciamo aggiungere anche: Un ulteriore campo di indagine sar quello di generare NLM massimizzando la loro competenza linguistica ad esempio adding pre-training process specific linguistic tasks. include bib file like this:"," Previous work has shown that artificial neural agents naturally develop surprisingly non-efficient codes.  This is illustrated by the fact that in a referential game involving a speaker and a listener neural networks optimizing accurate transmission over a discrete channel, the emergent messages fail to achieve an optimal length. Furthermore, frequent messages tend to be longer than infrequent ones, a pattern contrary to the Zipf Law of Abbreviation  observed in all natural languages. Here, we show that near-optimal and ZLA-compatible messages can emerge, but only if both the speaker and the listener are modified. We hence introduce a new communication system, ``LazImpa'', where the speaker is made increasingly lazy, i.e.,~avoids long messages, and the listener impatient, i.e.,~seeks to guess the intended content as soon as possible."
"% 1 - What problem solving? Entity typing classifies textual mentions entities, according semantic class, within set labels organized inventory. %Multi-label text classification task assigning sample relevant labels label inventory . The task progressed recognizing coarse classes , extremely large inventories, hundreds thousands labels . Therefore, exploiting inter-label correlations become critical improve performance. % 2 - Why interesting/important problem? % es interesante porque son buenos para modelar redes estructuras jerquicas. % Problema: su adopcion en nlp ha sido baja dado que hay una forma muy intuitiva de modelar texto en ellos. Distintos papers muestran como agregar un peque cambio pero una aplicacion real completa Large inventories tend exhibit hierarchical structure, either explicit tree-like arrangement labels , implicitly label distribution dataset . %A natural solution dealing large inventories organize hierarchy ranging general, coarse labels near top, specific, fine classes bottom. Prior work integrated explicit hierarchical information formulating hierarchy-aware loss representing instances labels joint Euclidean embedding space . However, resulting space hard interpret, methods fail capture implicit relations label inventory. Hyperbolic space naturally equipped embedding symbolic data hierarchical structures . Intuitively, amount space grows exponentially points move away origin. This mirrors exponential growth number nodes trees increasing distance root . %Its tree-like properties make efficient learn hierarchical representations low distortion . % Embeddings close origin disk relatively small distance points, rep-resenting root hierarchy. On hand,embeddings close boundary disk relatively large distance points well suited represent leaf nodes % 3 - How going solve it? In work, propose fully hyperbolic neural model fine-grained entity typing. Noticing perfect match hierarchical label inventories linguistic task benefits hyperbolic spaces, endow classification model suitable geometry capture fundamental property data distribution. By virtue hyperbolic representations, proposed approach automatically infers latent hierarchy arising class distribution achieves meaningful interpretable organization label space. This arrangement captures implicit hyponymic relations inventory enables model excel fine-grained classification. To best knowledge, work first apply hyperbolic geometry beginning end perform multi-label classification real NLP datasets. %NICE PHRASE FROM GULCEHRE: The focus work endow neural network representations suitable geometry capture fundamental properties data... given perfect fit label distribution linguistic task entity typing mathematical properties hyperbolic spaces. % esto deberia ser ""hay componentes ya hechos"". Y lo conecto al toque con el parrafo sig. Recent work proposed hyperbolic neural components, word embeddings , recurrent neural networks attention layers . %Advantages hyperbolic representations well-established discrete data networks graphs . In realm Natural Language Processing components exploit hyperbolic geometry developed well, word embeddings , recurrent neural networks attention layers . %or classifiers Me encanta este paper pero hace NLP :. We address issues. Our model encodes textual inputs, applies novel attention mechanism, performs multi-class multi-label classification, executing operations Poincar\'e model hyperbolic space . %By employing leveraging geometric properties hyperbolic space %The lack systems utilize hyperbolic space beginning end due three main difficulties: %First, different analytic models hyperbolic space, previous work operates one, hinders combination. %Second, clear integrate components conventional Euclidean neural models since mapping data one space onto required. Third, optimization hyperbolic models non-trivial. %We bridge gaps among previous work developing missing connections adapting different components employ Poincar\'e model hyperbolic space layers network. % We bridge gaps among previous work developing missing connections adapting different components, order accomplish full hyperbolic neural network. This is, network extracts features text, applies attention layers performs \todo{I one this}{multi-class classification}, executing operations hyperbolic geometry. % able perform multi-label multi-class classification text input %The model proposed generic manner applied classify sequential data . Since hyperbolic geometry naturally equipped model hierarchical structures, hypothesize model excel tasks profit incorporation hierarchical information. % \todo{awful}{systems} operate metric space result superior performance incorporating hierarchical information. %We evaluate model task fine-grained entity type classification , consider suitable testbed due connection textual inputs hierarchical type inventories. % Introduce main results % HNN's phrase: ""On series experiments datasets showcase effectiveness hyperbolic neural network layers compared ""classic"" Euclidean variants on"" % \todo[inline]{Forwarding bit results good idea . %\todo[inline]{Cambiar esta frase la idea de que ""imponer right metric es como imponer right bias""} %We impose inductive bias model means geometry internal representation. This allows us operate low-dimensional spaces thus substantially reducing parameter cost. Instead relying large pre-trained models, impose suitable inductive bias choosing adequate metric space embed data, introduce extra burden parameter footprint. %Phrase xiong2019inductiveBias: ""Instead using explicit graphical model, enforce relational bias model parameters, introduce extra burden label decoding."" % Misma idea pero yo meto el bias en la representacion, lo cual introduce un costo adicional permite operar con MUCHOS menos paretros. %Our components developed modular way allows seamlessly integrated NLP architectures. %\todo{Remove!}{While exist several hyperbolic components, practitioner faced options simple question: How integrate conventional layers? In work, answer question.} By means exponential logarithmic maps able mix hyperbolic Euclidean components one model, aiming exploit strengths different levels representation. We perform thorough ablation allows us understand impact hyperbolic component final performance system , showcases ease integration Euclidean layers. %In summary, make following contributions: %%%%% % %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% We demonstrated standard communication system, standard Speaker Listener LSTMs trained solve simple reconstruction game, leads long messages, close maximal threshold. Surprisingly, messages long, LSTM agents rely small number informative message symbols, located end. We introduce LazImpa, constrained system consists Lazy Speaker Impatient Listener. On one hand, Lazy Speaker obtained introducing cost messages length communication successful. We found early exploration potentially long messages crucial successful convergence . On hand, Impatient Listener aims succeed game soon possible, predicting Speaker's input message's symbol. We show constraints necessary emergence ZLA-like protocol, efficient natural languages. Specifically, Lazy Speaker alone would fail shorten messages. We connect importance Impatience mechanism locate useful information beginning messages. If function mechanism subject standing debate \cite[e.g.,][]{jackendoff2007,anderson2013}, many prior works pointed necessity human language understanding \citep[e.g.,][]{friston:2010,clark:2013}. We augment line works suggest impatience could play emergence ZLA-obeying languages. However, impatience leads ZLA, sufficient human-level efficiency. In words, efficiency needs constraints Speaker Listener sides. Our work highlights importance introducing right pressures communication system. Indeed, construct automated agents would eventually interact humans, need introduce task-agnostic constraints, allowing emergence human-like communication. Moreover, general, LazImpa provides stable optimization compared unconstrained system. Finally, study opens several lines research. One would investigate gap optimality. Indeed, LazImpa emergent languages show human-level efficiency, reach optimal coding. Specifically, emergent languages still non-informative symbols end messages. If additional non-useful symbols drift protocol optimality, encounter similar trend human animal communication . We leave understanding role non-informative symbols reach optimal coding future works. A second line research would apply system games NLP problems study affects properties language regularity compositionality."," Label inventories for fine-grained entity typing have grown in size and complexity. Nonetheless, they exhibit a hierarchical structure. Hyperbolic spaces offer a mathematically appealing approach for learning hierarchical representations of symbolic data. However, it is not clear how to integrate hyperbolic components into downstream tasks. This is the first work that proposes a fully hyperbolic model for multi-class multi-label classification, which performs all operations in hyperbolic space. We evaluate the proposed model on two challenging datasets and compare to different baselines that operate under Euclidean assumptions.  Our hyperbolic model infers the latent hierarchy from the class distribution, captures implicit hyponymic relations in the inventory, and shows performance on par with state-of-the-art methods on fine-grained classification with remarkable reduction of the parameter size. A thorough analysis sheds light on the impact of each component in the final prediction and showcases its ease of integration with Euclidean layers. \footnote{Code available at:\\ \url{https://github.com/nlpAThits/hyfi}}"
"Entity Recognition involves detection classification entities mentioned unstructured text pre-defined categories. It one foundational sub-task several Information Extraction Natural Language Processing pipelines. Hence, errors introduced extraction entities propagate degrade performance complete IE NLP pipeline. In domains experimental biology, growing complexity experiments resulted need automate wet laboratory procedures. Such automation useful avoiding human errors introduced wet lab protocols thereby enhance reproducibility experimental biological research. To achieve reproducibility, previous research works focussed defining machine-readable formats writing wet lab protocols . However, vast majority today protocols written natural language jargon colloquial language constructs emerge byproduct ad-hoc protocol documentation. This motivates need machine reading systems interpret meaning natural language instructions, enhance reproducibility via semantic protocols enable robotic automation mapping natural language instructions executable actions. In order enable research interpreting natural language instructions, practical applications biology life sciences, annotated database wet lab protocols introduced. The first step interpreting natural language lab protocols extract entities, followed identification relations them. To address research focussing entity recognition Wet Lab Protocols shared task introduced EMNLP WNUT-2020 Workshop. The task based annotated database wet lab protocols. We tackle task two phases. In first phase, experiment various contextualised word embeddings BiLSTM-CRF model arrive best-performing architecture. In second phase, create ensemble composed eleven BiLSTM-CRF models. The individual models trained random train-validation splits complete dataset. Here, also experiment different output merging schemes, including Majority Voting SLE. The rest paper structured follows: Section 2 states task definition. Section 3 describes specifics methodology. Section 4 explains experimental setup results, Section 5 concludes paper. 1) X important problem 2) The core challenges that. 3) Previous work X addressed Y, problems Z. 4) In work W . 5) This following appealing properties experiments show that. Incorporating hierarchical information label inventory neural models become critical improve performance. Hyperbolic spaces exciting approach since naturally equipped model hierarchical structures. However, previous work integrated isolated components neural systems. In work propose fully hyperbolic model showcase effectiveness challenging datasets. Our hyperbolic model automatically infers latent hierarchy class distribution, captures implicit hyponymic relations inventory achieves performance comparable state-of-the-art systems fine-grained labels remarkable reduction parameter size. This emphasizes importance choosing metric space suitable data distribution effective inductive bias capture fundamental properties, hierarchical structure. Moreover, illustrate ways integrate different components Euclidean layers, showing strengths drawbacks. An interesting future direction employ hyperbolic representations combination contextualized word embeddings. We release implementation aim ease adoption hyperbolic components neural models, yielding lightweight efficient systems. Add future work! En Gulcehre citan un paper dicen ""future work seria hacer lo mismo que CITE, pero co hyperbolic whatever. Para mi future work podr ser explorar variations de HyperbolicMLR para paliar algunas de las desventajas de Softmax . De Gulcehre: ""Similarly future work, interesting potential future direction use hyperbolic..."", say clearly I use contextualized word embeddings future work: explore hyperbolic representation combination contextualized word embeddings"," In this paper, we describe the approach that we employed to address the task of Entity Recognition over Wet Lab Protocols - a shared task in EMNLP WNUT-2020 Workshop. Our approach is composed of two phases. In the first phase, we experiment with various contextualised word embeddings  and a BiLSTM-CRF model to arrive at the best-performing architecture. In the second phase, we create an ensemble composed of eleven BiLSTM-CRF models. The individual models are trained on random train-validation splits of the complete dataset. Here, we also experiment with different output merging schemes, including Majority Voting and Structured Learning Ensembling . Our final submission achieved a micro F1-score of 0.8175 and 0.7757 for the partial and exact match of the entity spans, respectively. We were ranked first and second, in terms of partial and exact match, respectively."
"We make many decisions interact world. When rewarded , learn modify proximal cause stimulus chain decisions leading it, encourage future similar results. This process naturally paradigm Reinforcement Learning . Policy-based learning seeks find good estimates , function returns expected cumulative reward action chosen state . A desirable property methodologies learn ability generalize appropriate action taken encountering previously unseen state. Recent advances shown strong evidence generalization spatiotemporal modalities robotic manipulation , video games , autonomous navigation . However, modality language, less work applying generalization approaches decision making. Useful applications sequential decision making language models personal assistants proactively anticipate client needs; anti-phishing mediation agents waste would-be thief's time relevant non-helpful responses; investigative journalist assistants determine read, contact, questions ask create revelatory news report. Neural reinforcement learning training approaches, used play action video games , potential applicability language-based decision making due ability learn navigate adversarial exploratory scenarios. Naturally, generalization background knowledge capability afforded large contextualized language models \bert may applicable well. A useful virtual world proxy explore approaches' applicability text adventure game playing. In text adventure game, player immersed environment reading textual descriptions scene issuing natural language commands navigate inside scene. The player discovers interacts entities accomplishes goals, receiving explicit rewards so. Learning play text games useful pursuit convenient proxy real world cases cited above. Unlike these, plentiful data numerous games exist, endless supply games constructed, text games built-in reward functions, making suitable RL. This class problems also useful challenging: exposure family games explore topic similar gameplay , human players perform nearly perfectly additional games, computer models struggle. Why this? Humans quickly understand situation placed make rational decisions based trial-and-error life experience, call commonsense knowledge. Knowing priori that, e.g., door helpful allows players learn faster. Even though games complexity finite-state machines, computer models cannot learn play well. The problem appears due lack generalization caused lack commonsense. To computer model, considering whether using ludicrous considering whether using . Both actions discouraged negative reinforcement, human needs learn latter. Furthermore, computer player learning one may generalize one way, human surely will. There existing work learning play text games RL standard pattern incorporating large language models \bert yet seen current literature. It turns integration trivial. Most models use \bert ilk predominantly apply results supervised learning tasks training data ground truth least, case generation-based tasks like dialogue translation, corpus desirable output mimic . For tasks suited RL exploration interaction world, true target even, initially, corpus, thus learning proceed iteratively via, e.g., exploration-exploitation , requires millions training iterations converge . Integrating process additional overhead fine-tuning large model like \bert leads impractical slowdown: experiments considered work, baseline models use \cnn require little three weeks train Nvidia P100 GPU-equipped machine. Using models tasks run number iterations hardware fine-tuning 12-layer \bert model would take two years. In work, compare different previously used representation models deep RL imitation learning method first trains light-weight teacher using exploration-exploitation, uses trained model train heavy-weight student model. This dramatically decreases amount training time needed learn. Moreover, devise means casting RL problem supervised learning paradigm, allowing better exploitation large contextualized language models. In doing, show agents benefit imitation learning reformulation, converging faster models, exceeding teacher performance 7\% 24\% in- out-of-domain problems, despite limited search space. The novel contributions work are: File main.tex Based style files ACL 2020, Based style files ACL 2018, NAACL 2018/19, Based style files ACL-2015, improvements taken NAACL-2016 style Based style files ACL-2014, were, turn, based ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009, EACL-2009, IJCNLP-2008... Based style files EACL 2006 e.agirre@ehu.es Sergi.Balari@uab.es ACL 08 Joakim Nivre Noah Smith \documentclass[11pt,a4paper]{article} \usepackage[hyperref]{emnlp2020} \usepackage{times} \usepackage{latexsym} \renewcommand{\UrlFont}{\ttfamily\small} This strictly necessary, may commented out, improve layout manuscript, typically save space. \usepackage{microtype} \usepackage[utf8]{inputenc} \usepackage{dirtytalk} \usepackage{natbib} \usepackage{amsmath} \usepackage{amsfonts} \usepackage{bm} \usepackage{adjustbox} \renewcommand{\UrlFont}{\ttfamily\small} \usepackage{xcolor} \usepackage{xspace} \usepackage{comment} \usepackage{graphicx} \usepackage{url} \usepackage{array} \usepackage{multirow} \usepackage{booktabs} \usepackage{caption} \usepackage{subcaption} \usepackage{etoolbox} \AtBeginEnvironment{quote}{\singlespacing \small} \aclfinalcopy Uncomment line final submission \def\aclpaperid{***} Enter acl Paper ID \setlength\titlebox{5cm} You expand titlebox need extra space show authors. Please make titlebox smaller 5cm ; check camera-ready version ask change back. \newcommand{\joy}[1]{{\color{blue}{\bf #1 -joy}}} \newcommand{\cy}[1]{{\color{orange}{\bf #1 -cy}}} \newcommand{\lwku}[1]{{\small\color{red}{\bf\xspace#1 -ku}}} \newcommand{\kenneth}[1]{{\color{blue}{\bf #1 -Kenneth}}} \newcommand{\fitb}{context modeling \xspace} \newcommand{\ent}{entailment modeling \xspace} \newcommand{\entshort}{EMLA\xspace} \newcommand{\fitbshort}{CMLA\xspace} \newcommand\BibTeX{B\TeX} \title{Assessing Helpfulness Learning Materials \\with Inference-Based Learner-Like Agent} \author{Yun-Hsuan Jen, Chieh-Yang Huang, Mei-Hua Chen, \\ Ting-Hao Huang, Lun-Wei Ku \\ Academia Sinica, Taipei, Taiwan. \\ \texttt{yhjen2@gmail.com}, \texttt{lwku@iis.sinica.edu.tw}\\ Pennsylvania State University, University Park, PA, USA. \\ \texttt{\{chiehyang,txh710\}@psu.edu}\\ Tunghai University, Taichung, Taiwan. \texttt{mhchen@thu.edu.tw} } \date{}"," We consider problems of making sequences of decisions to accomplish tasks,  interacting via the medium of language. These problems are often tackled with reinforcement learning approaches. We find that these models do not generalize well when applied to novel task domains. However, the large amount of computation necessary to adequately train and explore the search space of sequential decision making, under a reinforcement learning paradigm, precludes the inclusion of large contextualized language models, which might otherwise enable the desired generalization ability. We introduce a teacher-student imitation learning methodology and a means of converting a reinforcement learning model into a natural language understanding model. Together, these methodologies enable the introduction of contextualized language models into the sequential decision making problem space. We show that models can learn faster and generalize more, leveraging both the imitation learning and the reformulation. Our models exceed teacher performance on various held-out decision problems, by up to 7\% on in-domain problems and 24\% on out-of-domain problems."
"% Reinforcement learning shown great success environments large state spaces. Using neural networks capture state representations allowed end-to-end training agents domains like Atari Go . It natural emulate success text domains, especially given state space language-based tasks combinatorially large. A sentence length allowed vocabulary possible states, tabular methods like learning fail unless coupled powerful function approximators like neural networks.\\ While current state RL multiple challenges, sparse rewards one leads slow, sometimes convergence. Consider agent learning environment large state space, states leading reward . An agent starting far left must take large number actions encountering reward. In turn, sparse feedback results noisy gradient training neural network. In extreme scenario, Figure , agent might take exponential number actions reach single leaf reward. Some early work, reward shaping , attempted solve sparse reward problem introducing dense rewards based heuristics, e.g., close agent goal. However, require complex design choices might result unexpected behavior agents.\\ Sparse rewards common straightforward way specify task needs solved. If robot expected pour water jug glass, simplest way give reward fills glass, otherwise. This type reward design common text-based games, agent rewarded upon reaching goal state, task-oriented dialogue, agent rewarded based successful completion task.\\ For study, examine text-based games find providing dense rewards help sentiment analysis improves performance conditions. We provide recipe integrating large contextualized language models deep reinforcement learning, applying sequential decision making demonstration proxy task text games, showing dramatic improvements standard practice, particularly out-of-domain held-out tests. We expect apply approach various challenging real-world sequential decision scenarios, goal-directed dialogue active information-gathering."," While reinforcement learning  has been successful in natural language processing  domains such as dialogue generation and text-based games, it typically faces the problem of sparse rewards that leads to slow or no convergence. Traditional methods that use text descriptions to extract only a state representation ignore the feedback inherently present in them. In text-based games, for example, descriptions like ``Good Job! You ate the food'' indicate progress, and descriptions like ``You entered a new room'' indicate exploration. Positive and negative cues like these can be converted to rewards through sentiment analysis. This technique converts the sparse reward problem into a dense one, which is easier to solve. Furthermore, this can enable reinforcement learning without rewards, in which the agent learns entirely from these intrinsic sentiment rewards. This framework is similar to intrinsic motivation, where the environment does not necessarily provide the rewards, but the agent analyzes and realizes them by itself. We find that providing dense rewards in text-based games using sentiment analysis improves performance under some conditions."
"Natural language data rich structure, structure visible surface. Machine learning models tackling high-level language tasks would benefit uncovering underlying structures trees, sequence tags, segmentations. Traditionally, practitioners turn pipeline approaches external, pretrained model used predict, \eg, syntactic structure. The benefit approach predicted tree readily available inspection, downside errors easily propagate throughout pipeline require attention . In contrast, deep neural architectures tend eschew preprocessing, instead learn soft hidden representations, easily amenable visualization analysis. The best worlds would model structure latent variable, combining transparency pipeline approach end-to-end unsupervised representation learning makes deep models appealing. Moreover, large-capacity model tend rediscover structure scratch , structured latent variables may reduce required capacity. Learning discrete, combinatorial latent variables is, however, challenging, due intersection large cardinality null gradient issues. For example, learning latent dependency tree, latent parser must choose among exponentially large set possible trees; what's more, parser may learn gradient information downstream task. If highest-scoring tree selected using argmax operation, gradients zero, preventing learning. One strategy dealing null gradient issue use surrogate gradient, explicitly overriding zero gradient chain rule, different computation performed. The commonly known example straight-through estimator \citep[STE;][]{bengio2013estimating}, pretends argmax node instead identity operator. Such methods lead fundamental mismatch objective learning algorithm. The effect mismatch still insufficiently understood, design successful new variants therefore challenging. For example, recently-proposed SPIGOT method found beneficial use projection part surrogate gradient. In paper, study surrogate gradient methods deterministic learning discrete structured latent variables. Our contributions are: While discrete methods outperform relaxed alternatives using building \linebreak blocks, hope interpretation insights would trigger future latent structure research. The code paper available \url{https://github.com/deep-spin/understanding-spigot}. We find adding auxiliary rewards using sentiment analysis help improve RL agents' performance text domains. Our methods take step direction creating agents infers rewards themselves. We expect improvements applicable similar text-based domains, task-oriented dialogue. Given rapid improvements NLP methods, believe better pre-training sentiment analysis models translate better RL agents future."," Latent structure models are a powerful tool for modeling language data: they can mitigate the error propagation and annotation bottleneck in pipeline systems, while simultaneously uncovering linguistic insights about the data. One challenge with end-to-end training of these models is the argmax operation, which has null gradient. In this paper, we focus on surrogate gradients, a popular strategy to deal with this problem. We explore latent structure learning through the angle of pulling back the downstream learning objective. In this paradigm, we discover a principled motivation for both the straight-through estimator  as well as the recently-proposed SPIGOT---a variant of STE for structured models. Our perspective leads to new algorithms in the same family. We empirically compare the known and the novel pulled-back estimators against the popular alternatives, yielding new insight for practitioners and revealing intriguing failure cases."
"%% Paragraph 1: %% * introduce constructions interest %% * give broad impression subtlety grammatical phenomena, %% * emphasize verb bias problem, since one unique contributions When use language, often faced choice several possible ways expressing message. For example, English, express event intended actual transfer two animate entities, one option double-object construction, two noun phrases follow verb. Alternatively, content expressed using prepositional dative construction. \ex. \a. Ava gave something. \hfill DO \b. Ava gave something him. \hfill PO Speakers' preferences one construction depend multiple factors, including length definiteness arguments . % could also cite: Davidse 1996; Givo  1984a; Polinsky 1996; Ransom 1979; Snyder 2003; Thompson 1990, 1995; One particularly subtle factor lexical verb bias. While verbs readily occur either construction, others strong preferences one : \ex. \a. ?Ava said something. \hfill DO \b. Ava said something him. \hfill PO %% Paragraph 2: %% * transition motivation problem interesting NLP %% * briefly mention major previous work problem gaps Decades work linguistics psychology investigated humans learn distinctions . Yet, deep neural networks achieved state-of-the-art performance across many tasks natural language processing, little known extent acquired similarly fine-grained preferences. Although neural language models robustly capture certain types grammatical constraints, e.g., subject-verb agreement long distance dependencies , continue struggle aspects syntax, including argument structure \cite[e.g.][]{warstadt2019neural}. Verb biases provide particularly interesting testbed. Successfully predicting psycholinguistic phenomena requires integration specific lexical information representations higher-level grammatical structures, implications understanding differential performance models tasks. %% Paragraph 3: contribution In current work, take analytic comparative approach. First, introduce DAIS dataset, containing 50K human preference judgments 5K sentence pairs, using 200 unique verbs. These empirical judgments indicate verb bias preferences highly gradient practice , rather belonging binary ``alternating'' ``non-alternating'' classes, commonly assumed. Second, evaluate predictions variety neural models, including recurrent architectures transformers, analyze internal states understand drives differences performance. \change{Finally, evaluate models natural production data Switchboard corpus, finding transformers achieve similar classification accuracy prior work using hand-annotated features \cite[;][]{bresnan2007predicting}.} In work, provide novel motivation straight-through estimator SPIGOT, based pulling back downstream loss. We derive promising new algorithms, novel insight existing ones. Unstructured controlled experiments suggest new algorithms, use cross-entropy loss instead perceptron loss, stable SPIGOT accurately disentangling latent variable. Differentiable relaxation models easiest optimize high downstream accuracy, fail correctly identify latent clusters. On structured NLP experiments, relaxations tend overall perform better stable straight-through variants terms classification accuracy. However, lack gold-truth latent structures makes impossible assess recovery performance. We hope insights, including negative results, may encourage future research learning latent structures."," Languages typically provide more than one grammatical construction to express certain types of messages. A speaker's choice of construction is known to depend on multiple factors, including the choice of main verb -- a phenomenon known as verb bias. Here we introduce DAIS, a large benchmark dataset containing 50K human judgments for 5K distinct sentence pairs in the English dative alternation. This dataset includes 200 unique verbs and systematically varies the definiteness and length of arguments.  We use this dataset, as well as an existing corpus of naturally occurring data, to evaluate how well recent neural language models capture human preferences. Results show that larger models perform better than smaller models, and transformer architectures  tend to out-perform recurrent architectures  even under comparable parameter and training settings.  Additional analyses of internal feature representations suggest that transformers may better integrate specific lexical information with grammatical constructions."
"The core idea behind predominant pretrain fine-tune paradigm transfer learning NLP general language knowledge, gleaned large quantities data using unsupervised objectives, serve foundation specialized endeavors. Current practice involves taking full model amassed general knowledge fine-tuning second objective appropriate new task \citep[see][for overview]{raffelExploringLimitsTransfer2019}. Using methods, pre-trained transformer-based language models \citep[e.g., BERT, ][]{devlin-etal-2019-bert} employed great effect wide variety NLP problems, thanks, part, fine-grained ability capture aspects linguistic context . However, paradigm introduces subtle insidious limitation becomes evident downstream application topic model. A topic model may cast autoencoder , could fine-tune pretrained transformer identical document reconstruction objective. But replacing original topic model, lose property makes desirable: interpretability. The transformer gains contextual power ability exploit huge number parameters, interpretability topic model comes dramatic dimensionality reduction. We combine advantages two approaches---the rich contextual language knowledge pretrained transformers intelligibility topic models---using knowledge distillation . In original formulation, knowledge distillation involves training parameter-rich teacher classifier large swaths data, using high-quality probability estimates outputs guide smaller student model. Since information contained estimates useful---a picture ox yield higher label probabilities buffalo apricot---the student needs less data train generalize better. We show principle apply equally well improve unsupervised topic modeling, knowledge previously attempted. While distillation usually involves two models type, also apply models differing architectures. Our method conceptually quite straightforward: fine-tune pretrained transformer document reconstruction objective, acts capacity autoencoder. When document passed BERT autoencoder, generates distribution words includes unobserved related terms. We incorporate distilled document representation loss function topic model estimation. To connect method standard supervised knowledge distillation, observe unsupervised ``task'' autoencoder topic model reconstruction original document, i.e. prediction distribution vocabulary. The BERT autoencoder, ``teacher'', provides dense prediction richly informed training large corpus. The topic model, ``student'', generating prediction distribution. We use former guide latter, essentially predicting word distributions multi-class labeling problem. \newcommand{\reffig}[1]{\hl{[FIG: #1]}} \newcommand{\reftable}[1]{\hl{[TABLE: #1]}} \newcommand{\refsec}[1]{\hl{[SECTION: #1]}} \newcommand{\ho}[1]{\textcolor{blue}{}} \newcommand{\pg}[1]{\textcolor{red}{}} \newcommand{\psrcomment}[1]{} \newcommand{\ignore}[1]{} \newcommand{\ourmodel}{BAT } \newcommand{\e}[2]{\mathbb{E}_{#1}\left[ #2 \right] } \newcommand{\B}{B} \DeclareMathOperator*{\argmin}{arg\,min} \aclfinalcopy % \newcommand\BibTeX{Bib\TeX} \title{Improving Neural Topic Models using Knowledge Distillation} \author{Alexander Hoyle\thanks{\, Equal contribution.} \\ Computer Science \\ University Maryland \\ College Park, MD \\ \\\And Pranav Goel\footnotemark[1] \\ Computer Science \\ University Maryland \\ College Park, MD \\ \\\And Philip Resnik \\ Linguistics / UMIACS \\ University Maryland \\ College Park, MD \\ \\} \date{} \begin{document} \bibliography{anthology,refs,zotero} \bibliographystyle{acl_natbib} \clearpage \appendix In natural languages, speakers routinely select one alternative others express intended message. These choices sensitive many interacting factors, including choice main verb length definiteness arguments. Our new dataset, DAIS, offers higher-resolution window richness human preferences, also provides newly powerful benchmark evaluating understanding corresponding sensitivity language models. We found transformer architectures corresponded especially well human verb bias judgments. Further work needed precisely determine source architectural differences observed. One possibility transformer's self-attention mechanism layer-wise organization improves ability represent lexically-specific structures. However, also possible differences attributable training data. Another line future research compare incremental predictions neural models finer-grained eye-tracking evidence sentence processing double-object sentences \cite[e.g.][]{filik2004processing}. As neural language models become complex, subtler phenomena like verb bias may yield new insights lexical grammatical representations jointly learned successfully integrated language understanding.","     Topic models are often used to identify human-interpretable topics to help make sense of large document collections. We use knowledge distillation to combine the best attributes of probabilistic topic models and pretrained transformers. Our modular method can be straightforwardly applied with any neural topic model to improve topic quality, which we demonstrate using two models having disparate architectures, obtaining state-of-the-art topic coherence. We show that our adaptable framework not only improves performance in the aggregate over all estimated topics, as is commonly reported, but also in head-to-head comparisons of aligned topics."
"Interactive systems capable understanding natural language responding form natural language text high potentials various applications. In pursuit building evaluating systems, study learning agents Interactive Fiction games. IF games world-simulating software players use text commands control protagonist influence world, illustrated Figure. IF gameplay agents need simultaneously understand game's information text display generate natural language command via text input interface. Without providing explicit game strategy, agents need identify behaviors maximize objective-encoded cumulative rewards. IF games composed human-written texts create superb new opportunities studying evaluating natural language understanding techniques due unique characteristics. Game designers elaborately craft literariness narrative texts attract players creating IF games. The resulted texts IF games linguistically diverse sophisticated template-generated ones synthetic text games. The language contexts IF games versatile various designers contribute enormous domains genres, adventure, fantasy, horror, sci-fi. The text commands control characters less restricted, sizes six orders magnitude larger previous text games. The recently introduced Jericho benchmark provides collection IF games. The complexity IF games demands sophisticated NLU techniques used synthetic text games. Moreover, task designing IF game-play agents, intersecting NLU reinforcement learning , poses several unique challenges NLU techniques. The first challenge difficulty exploration extbf{the huge natural language action space}. To make RL agents learn efficiently %via trial-and-error without prohibitive exhaustive trials, action estimation must generalize learned knowledge tried actions others. To end, previous approaches, starting single embedding vector observation, either predict elements actions independently; embed valid action another vector predict action value based vector-space similarities. These methods consider compositionality role-differences action elements, interactions among observation. Therefore, modeling action values less accurate less data-efficient. The second challenge extbf{partial observability}. At game-playing step, agent receives textual observation describing locations, objects, characters game world. But latest observation often sufficient summary interaction history may provide enough information determine long-term effects actions. Previous approaches address problem building representation past observations . These methods treat historical observations equally summarize information single vector without focusing important contexts related action prediction current observation. Therefore, usages history also bring noise, improvement always significant. We propose novel formulation IF game playing Multi-Passage Reading Comprehension harness MPRC techniques solve huge action space partial observability challenges. The graphical illustration shown Figure. First, action value prediction essentially generating scoring compositional action structure finding supporting evidence observation. We base fact action instantiation template, i.e., verb phrase placeholders object arguments takes~. Then action generation process viewed extracting objects template's placeholders textual observation, based interaction template verb phrase relevant context objects observation. Our approach addresses structured prediction interaction problems idea context-question attention mechanism RC models. Specifically, treat observation passage template verb phrase question. The filling object placeholders template thus becomes extractive QA problem selects objects observation given template. Simultaneously action gets evaluation value predicted RC model. Our formulation approach better capture fine-grained interactions observation texts structural actions, contrast previous approaches represent observation single vector ignore fine-grained dependency among action elements. Second, alleviating partial observability essentially enhancing current observation potentially relevant history predicting actions enhanced observation. Our approach retrieves potentially relevant historical observations object-centric approach , retrieved ones likely connected current observation describe least one shared interactable object. Our attention mechanisms applied across retrieved multiple observation texts focus informative contexts action value prediction. We evaluated approach suite Jericho IF games, compared previous approaches. Our approaches achieved outperformed state-of-the-art performance 25 33 games, trained less one-tenth game interaction data used prior art. We also provided ablation studies models retrieval strategies. %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% To knowledge, first distill ``black-box'' neural network teacher guide probabilistic graphical model. We order combine expressivity probabilistic topic models precision pretrained transformers. Our modular method sits atop neural topic model improve topic quality, demonstrate using two NTMs highly disparate architectures , obtaining state-of-the-art topic coherence across three datasets different domains. Our adaptable framework produce improvements aggregate : effect interpreted specifically identifying space topics generated existing model and, cases, improving coherence individual topics, thus highlighting modular value approach. In future work, also hope explore effects pretraining corpus teachers generated topics. Another intriguing direction exploring connection methods neural network interpretability. The use knowledge distillation facilitate interpretability also previously explored, example, learn interpretable decision trees neural networks. In work, weight BERT autoencoder logits goes one, topic model begins describe less corpus teacher. We believe mining connection open research avenues; instance, investigating differences teacher-topics conditioned pre-training corpus. Finally, although motivated primarily widespread use topic models identifying interpretable topics \cite[][Ch. 3]{boyd2017applications}, plan explore ideas presented context downstream applications like document classification. \looseness=-1"," Interactive Fiction  games with real human-written natural language texts provide a new natural evaluation for language understanding techniques.  In contrast to previous text games with mostly synthetic texts, IF games pose language understanding challenges on the human-written textual descriptions of diverse and sophisticated game worlds and language generation challenges on the action command generation from less restricted combinatorial space. We take a novel perspective of IF game solving and re-formulate it as Multi-Passage Reading Comprehension  tasks. Our approaches utilize the context-query attention mechanisms and the structured prediction in MPRC to efficiently generate and evaluate action outputs and apply an object-centric historical observation retrieval strategy to mitigate the partial observability of the textual observations.  Extensive experiments on the recent IF benchmark  demonstrate clear advantages of our approaches achieving high winning rates and low data requirements compared to all previous approaches.\footnote{Source code is available at: \url{https://github.com/XiaoxiaoGuo/rcdqn}. }"
"Recent advances self-supervised pre-training resulted impressive downstream performance several NLP tasks. However, led development enormous models, often require days training non-commodity hardware . Furthermore, studies shown quite challenging successfully train large Transformer models, requiring complicated learning schemes extensive hyperparameter tuning. Despite expensive training regimes, recent studies found trained, bi-directional language models exhibit simple patterns self-attention without much linguistic backing. For example, 40\% heads pre-trained BERT model simply pay attention delimiters added tokenizer . Since attention patterns independent linguistic phenomena, natural question arises: Transformer models guided towards attention patterns without requiring extensive training? In paper, propose attention guidance mechanism self-attention modules Transformer architectures enable faster, efficient, robust self-supervised learning. Our approach simple agnostic training objective. Specifically, introduce auxiliary loss function guide self-attention heads layer towards set pre-determined patterns . These patterns encourage formation global local structures model. Through several experiments, show approach enables training large Transformer models considerably faster  example, train 16-layer RoBERTa model SOTA performance low-resource domain two days using four GPUs, excluding loss leads slow convergence. Our method also achieves competitive performance BERT three English natural language understanding tasks, outperforms baseline masked language modeling models eleven twelve settings considered. Further, also show initialization agnostic training objective demonstrating gains replaced token detection objective proposed ELECTRA machine translation Transformers. Finally, provide analysis attention heads learned using method. Surprisingly, contrary recent studies, find possible train models perform well language modeling without learning single attention head models coreferences. % . For example, model fails co-reference test still performing well language modeling downstream tasks. To summarize, main contributions are: We formulate general IF game playing MPRC tasks, enabling MPRC-style solution efficiently address key IF game challenges huge combinatorial action space partial observability unified framework. Our approaches achieved significant improvement previous state-of-the-art game scores training data efficiency. Our formulation also bridges broader NLU/RC techniques address critical challenges IF games future work, e.g., common-sense reasoning, novelty-driven exploration, multi-hop inference."," % Despite being successful in downstream language understanding tasks, modern language models contain millions of parameters and require multiple days of training on specialized hardware such as TPUs. Training such models on commodity hardware  often means slow convergence, making it practically intractable for many researchers.  In this paper, we propose a simple and effective technique to allow for efficient self-supervised learning with bi-directional Transformers. Our approach is motivated by recent studies demonstrating that self-attention patterns in trained models contain a majority of non-linguistic regularities. We propose a computationally efficient auxiliary loss function to guide attention heads to conform to such patterns. Our method is agnostic to the actual pre-training objective and results in faster convergence of models as well as better performance on downstream tasks compared to the baselines, achieving state of the art results in low-resource settings. Surprisingly, we also find that linguistic properties of attention heads are not necessarily correlated with language modeling performance.\footnote{Code: \href{https://github.com/ameet-1997/AttentionGuidance}{https://github.com/ameet-1997/AttentionGuidance}}"
"% % Transformer models outperformed previously used RNN based models traditional statistical MT techniques. This improvement, though, comes cost higher computation complexity. The decoder computation sequential becomes bottleneck due autoregressive nature, large depth self-attention structure. % Another recent trend making models larger ensembling multiple models achieve best possible translation quality . Leading solutions common benchmark usually use ensemble Transformer big models, combined 1 billion parameters. % In paper, focus developing architectures faster inference less number parameters, without sacrificing translation quality. % Recent work \citet{ludicrously:kim2019} proposed methods replace self-attention decoder simpler simple recurrent units used knowledge distillation simplify training final architecture. \citet{deepencoder} also proposed make decoder lightweight training deep-encoder, shallow decoder architecture. Another line effort make NMT architectures efficient pruning different components model. \citet{prune_voita-etal-2019-analyzing} \citet{prune_michel:NIPS2019_9551} show attention heads network learn redundant information pruned away. % All works use vanilla Transformer architecture baseline, clear approaches give complimentary results combined together. In work, explore benchmark combining techniques, goal maximizing inference speed without hurting translation quality. % %We adapt approach extend following ideas. First, optimized SSRU make efficient. Second, removed feed-forward network decoder completely. Then, kept 1 layer decoder used deep encoder. Last pruned redundant heads deep encoder. % After carefully stacking approaches, proposed architecture able achieve significant speed improvement 84\% GPU 102\% CPU architectures without degradation translation quality terms BLEU. % %%%%%%%% original Related Work %%%%%%%%% % In section, investigate two questions: 1) We project token-level representations obtained BERT embedders onto 2-dimensional space using t-SNE. \autoref{fig:tsne} presents visualization results CoNLL WNUT test sets . beyond optimal visual effect) Fine-tuning BERT OntoNotes clearly improves task-awareness respect CoNLL WNUT datasets, instances class much closer compared obtained non-fine-tuned BERT model. The separation different entity classes evident CoNLL due greater tag set overlap OntoNotes. Instances labeled \nertag{O} spread across space, regardless fine-tuning. This explains effectiveness \sysname. First, fine-tuning BERT conventional NER setting able learn good entity specific metric space. Second, nearest neighbor classifier emphasizes local distance appropriate assigning \nertag{O} instance. , \nertag{c-work}, \nertag{corp.} correspond \nertag{MEDICAL-RECORD}, \nertag{location}, \nertag{creative-work}, \nertag{corporation} respectively.} \paragraph{Per-class performance analysis} We attempt shed light second question analyzing outputs best five-shot \sysname~systems domain transfer task. The per-class F1 scores shown in, exclude I2B2 classes less 200 instances test set. \sysname~achieves reasonable performance less ambiguous entity classes \nertag{DATE}, \nertag{CITE}, \nertag{person}, \nertag{location}. However, struggles distinguish highly ambiguous classes. For example, \nertag{AGE}, \nertag{MEDICAL-RECORD}, \nertag{PHONE}, \nertag{IDNUM} numbers. It still challenging system differentiate different numerical types without domain specific knowledge. Similarly, \sysname~often predicts \nertag{PATIENT} entity \nertag{DOCTOR} nearly always assigns \nertag{corporation} label entities \nertag{group}. We believe domain specific cues like `Dr.' `MD.' useful resolving ambiguities enable few-shot NER systems generalize better. beyond support examples. We focus typical errors made I2B2 WNUT test sets, system performance CoNLL solid. \section{Experiments} In section, compare \sysname~against existing methods two few-shot NER scenarios: tag set extension domain transfer. We adopt several benchmark NER corpora different domains few-shot experiments. \let\argmin\relax \let\argmax\relax \DeclareMathOperator*{\argmin}{arg\,min} \DeclareMathOperator*{\argmax}{arg\,max} \newcommand{\example}[1]{`#1'} \newcommand{\nertag}[1]{\texttt{#1}} \newcommand{\sysname}{} \newcommand{\arzoo}[1]{[{Arzoo: {#1}}]} -------------------------------------------- \aclfinalcopy Uncomment line final submission \def\aclpaperid{2799} Enter acl Paper ID \setlength\titlebox{5cm} Expanding titlebox \title{Simple Effective Few-Shot Named Entity Recognition\\ Structured Nearest Neighbor Learning} \author{Yi Yang \\ ASAPP Inc.\\ New York, NY 10007\\ yyang@asapp.com \\\And Arzoo Katiyar \thanks{ \hspace{0.15cm}Work done ASAPP Inc.} \\ Pennsylvania State University \\ University Park, PA 16802\\ arzoo@psu.edu } \date{} *********************************************************** Introduction *********************************************************** Problem *********************************************************** Model *********************************************************** Experiment *********************************************************** Discussion *********************************************************** Related Work *********************************************************** Conclusion Future Work *********************************************************** Acknowledgments *********************************************************** *********************************************************** Appendix"," Large Transformer models have achieved state-of-the-art results in neural machine translation and have become standard in the field. In this work, we look for the optimal combination of known techniques to optimize inference speed without sacrificing translation quality. We conduct an empirical study that stacks various approaches and demonstrates that combination of replacing decoder self-attention with simplified recurrent units, adopting a deep encoder and a shallow decoder architecture and multi-head attention pruning can achieve up to $109$\% and $84$\% speedup on CPU and GPU respectively and reduce the number of parameters by $25$\% while maintaining the same translation quality in terms of BLEU. %State-of-the-art neural machine translation has become compute and parameter intensive in the last several years, which puts significant pressure on the latency and hardware resources during inference. In this paper, we change the standard Transformer architecture to reduce the number of parameters and increase inference speed without sacrificing translation quality. We demonstrate that combination of replacing decoder self-attention with the simpler simple recurrent units, adopting a deep encoder and shallow decoder architecture, and multi-head attention pruning, we can achieve up to 102\% speedup and reduce the number of parameters by 13\% while maintaining the same translation quality in terms of BLEU."
"Intent Detection crucial task natural language understanding, whose objective extract underlying intents behind given utterances. The extracted intents could provide contexts downstream Natural Language Processing tasks dialogue state tracking question answering. Unlike traditional text classification, ID challenging two main reasons Utterances usually short diversely expressed, Emerging intents occur continuously, especially across different domains . Despite recent advances, state-of-the-art ID methods require large amount annotated data achieve competitive performance. This requirement inhibits models' capability generalizing newly emerging intents limited annotations inference. Re-training fine-tuning large models samples emerging classes could easily lead overfitting problems. Motivated human capability correctly categorizing new classes examples , few-shot learning paradigms adopted tackle scarcity problems emerging classes. FSL methods take advantage small set labeled examples learn discriminate unlabeled samples classes, even seen training. Recent works FSL focus learning matching information labeled samples unlabeled samples provide additional contextual information instance-level representations, leading effective prototype representation. However, methods extract similarity based fine-grained word semantics, failing capture diverse expressions users' utterances. This problem could lead overfitting either seen intents novel intents, especially challenging Generalized Few-shot Intent Detection setting seen novel intents existent joint label space inference. Instead, matching support query samples coarser-grained semantic components could provide additional informative contexts beyond word levels. For instance, two utterances ""i need get table pub southeastern cuisine"" ``book spot six friends"" share similar intent label ``Book Restaurant"". While word-level semantics might find similar action words ``get"" ``book"", words necessarily contribute correct intent findings. Instead, coarser-grained semantics ``get table"" ``book spot"" could provide hints identify ``Book Restaurant"" intent. As semantic components could effectively extracted multi-head self-attention, matching SC support query enhance query support representations, leading improvements generalization seen training classes unseen testing classes. To enhance dynamics extracted SC across various domains diversely expressed utterances, introduce additional head regularizations. In addition, overcome insufficiency single similarity measure matching sentences diverse semantics, comprehensive matching method explored. Our main contribution summarized follows: In paper explored combination techniques aimed improving inference speed lead discovery efficient architecture. The best architecture deep -layer encoder, shallow decoder one single lightweight recurrent unit layer one encoder-decoder attention mechanism. \ encoder heads pruned giving rise model \ fewer parameters baseline Transformer. In terms inference speed, proposed architecture \ faster GPU, \ faster CPU. In future, plan investigate pruning feed-forward network encoder, explore application lottery ticket hypothesis. In paper, investigated various approaches simplifying Transformer model speed inference successfully combine multiple techniques. To specific, achieve efficient inference architecture, consists one lightweight recurrent unit layer one encoder-decoder attention mechanism decoder. With head pruning method, 18\ attention heads required deep encoder shallow decoder architecture. This model 13\ fewer parameters, inference stage, 84\ 102\ faster baseline GPU CPU, respectively. In future, plan prune feed-forward network encoder explore combination lottery ticket hypothesis. In future, plan investigate different approaches build mroe efficient inference architecture machine translation. plan prune feed-forward neurons apply unstructured pruning techniques remove weights whole model."," Few-shot Intent Detection is challenging due to the scarcity of available annotated utterances. Although recent works demonstrate that multi-level matching plays an important role in transferring learned knowledge from seen training classes to novel testing classes, they rely on a static similarity measure and overly fine-grained matching components. These limitations inhibit generalizing capability towards Generalized Few-shot Learning settings where both seen and novel classes are co-existent. In this paper, we propose a novel Semantic Matching and Aggregation Network where semantic components are distilled from utterances via multi-head self-attention with additional dynamic regularization constraints. These semantic components capture high-level information, resulting in more effective matching between instances. Our multi-perspective matching method provides a comprehensive matching measure to enhance representations of both labeled and unlabeled instances. We also propose a more challenging evaluation setting that considers classification on the joint all-class label space. Extensive experimental results demonstrate the effectiveness of our method. Our code and data are publicly available \footnote{\url{https://github.com/nhhoang96/Semantic\_Matching}} ."
"Neural machine translation data-hungry approach, requires large amount data train well-performing NMT model. However, complex patterns potential noises large-scale data make training NMT models difficult. To relieve problem, several approaches proposed better exploit training data, curriculum learning, data diversification, data denoising. In paper, explore interesting alternative reactivate inactive examples training data NMT models. By definition, inactive examples training examples marginally contribute even inversely harm performance NMT models. Concretely, use sentence-level output probability assigned trained NMT model measure activeness level training examples, regard examples least probabilities inactive examples . Experimental results show removing 10\% inactive examples marginally improve translation performance. In addition, observe high overlapping ratio inactive active examples across random seeds, model capacity, model architectures . These results provide empirical support hypothesis existence inactive examples large-scale datasets, invariant specific NMT models depends data distribution itself. We propose data rejuvenation rejuvenate inactive examples improve performance NMT models. Specifically, train NMT model active examples rejuvenation model re-label inactive examples, resulting rejuvenated examples~. The final NMT model trained combination active examples rejuvenated examples. Experimental results show data rejuvenation approach consistently significantly improves performance SOTA NMT models benchmark WMT14 English-German English-French datasets~. Encouragingly, approach also complementary existing data manipulation methods , combining improve performance. Finally, conduct extensive analyses better understand inactive examples proposed data rejuvenation approach. Quantitative analyses reveal inactive examples difficult learn active ones, rejuvenation reduce learning difficulty~. The rejuvenated examples stabilize accelerate training process NMT models~, resulting final models better generalization capability~. Our contributions work follows: In work, propose multi-task learning framework jointly trains model translation task bitext data, masked language modeling task source-side monolingual data denoising auto-encoding task target-side monolingual data. We explore data noising scheduling approaches demonstrate efficacy proposed approach. We show proposed MTL approach effectively improve performance MNMT high-resource low-resource languages large margin, also significantly improve translation quality zero-shot language pairs without bitext training data. We showed proposed approach effective pre-training followed finetuning NMT. Furthermore, showed effectiveness multitask learning cross-lingual downstream tasks outperforming SOTA larger models trained single task. For future work, interested investigating proposed approach scaled setting languages larger amount monolingual data. Scheduling different tasks different types data would interesting problem. Furthermore, would also like explore sample efficient strategy add new language trained MNMT system."," Large-scale training datasets lie at the core of the recent success of neural machine translation  models. However, the complex patterns and potential noises in the large-scale data make training NMT models difficult. In this work, we explore to identify the inactive training examples which contribute less to the model performance, and show that the existence of inactive examples depends on the data distribution. We further introduce data rejuvenation to improve the training of NMT models on large-scale datasets by exploiting inactive examples. The proposed framework consists of three phases.  First, we train an identification model on the original training data, and use it to distinguish inactive examples and active examples by their sentence-level output probabilities. Then, we train a rejuvenation model on the active examples, which is used to re-label the inactive examples with forward-translation. Finally, the rejuvenated examples and the active examples are combined to train the final NMT model. Experimental results on WMT14 English-German and English-French datasets show that the proposed data rejuvenation consistently and significantly improves performance for several strong NMT models. Extensive analyses reveal that our approach stabilizes and accelerates the training process of NMT models, resulting in final models with better generalization capability.}  %In this work, we propose to improve the training of NMT models on large-scale datasets by exploiting inactive training examples, which contribute less to the model performance. Specifically, the proposed framework consists of three phases. First, we identify the inactive examples with their sentence-level prediction confidence assigned by an identification model trained on the original training data. Then, we train a rejuvenation model on the active examples, which is used to re-label the inactive examples with forward-translation. Finally, the rejuvenated examples and the active examples are combined to train the final NMT model. Experimental results on WMT14 English-German and English-French datasets show that the proposed data rejuvenation consistently and significantly improves performance for several strong NMT models. Extensive analyses reveal that our approach stabilizes and accelerates the training process of NMT models, resulting in final models with better generalization capability."
"The following instructions directed authors papers submitted EMNLP 2020 accepted publication proceedings. All authors required adhere specifications. Authors required provide Portable Document Format version papers. The proceedings designed printing A4 paper. In study, propose data rejuvenation exploit inactive training examples neural machine translation large-scale datasets. The proposed data rejuvenation scheme general framework one freely define, instance, identification rejuvenation models. Experimental results different model architectures language pairs demonstrate effectiveness universality data rejuvenation approach. Future directions include exploring advanced identification rejuvenation models better reflect learning abilities NMT models, well validating NLP tasks dialogue summarization."," This document contains the instructions for preparing a manuscript for the proceedings of EMNLP 2020. The document itself conforms to its own specifications, and is therefore an example of what your manuscript should look like. These instructions should be used for both papers submitted for review and for final versions of accepted papers. Authors are asked to conform to all the directions reported in this document."
"Modern neural machine translation~ models employ sufficient capacity fit massive data well utilizing large number parameters, suffer widely recognized issue, namely, over-parameterization. For example, showed 40\% parameters RNN-based NMT model pruned negligible performance loss. However, low utilization efficiency parameters results waste computational resources , well renders model stuck local optimum. In response over-parameterization issue, network pruning widely investigated computer vision natural language processing tasks . Recent work proven spare parameters reused maximize utilization models CV tasks image classification. The leverage parameter rejuvenation sequence-to-sequence learning, however, received relatively little attention research community. In paper, empirically study efficiency issue NMT models. Specifically, first investigate effects weight pruning advanced Transformer models, showing 20\% parameters directly pruned, continuously training sparse networks, prune 50\% performance loss. Starting observation, exploit whether redundant parameters able re-utilized improving performance NMT models. Experiments systematically conducted different datasets NMT architectures . Results demonstrate rejuvenation approach significantly consistently improve translation quality +0.8 BLEU points. Further analyses reveal rejuvenated parameters reallocated enhance ability model source-side low-level information, lacking leads number problems NMT models. \paragraph{Contributions} Our key contributions are: We introduce GraphGlove~--- graph word embeddings, word node weighted graph distance words shortest path distance corresponding nodes. The graph learned end-to-end unsupervised manner. We show GraphGlove substantially outperforms Euclidean Poincar\'e GloVe word similarity word analogy tasks. Our analysis reveals structure learned graphs hierarchical similar WordNet; geometry highly non-trivial contains subgraphs different local topology. Possible directions future work include using GraphGlove unsupervised hypernymy detection, analyzing undesirable word associations, comparing learned graph topologies different languages, downstream applications sequence classification. Also, given recent success models ELMo BERT, would interesting explore extensions GraphGlove class contextualized embeddings."," Modern neural machine translation  models employ a large number of parameters, which leads to serious over-parameterization and typically causes the underutilization of computational resources. In response to this problem, we empirically investigate whether the redundant parameters can be reused to achieve better performance. Experiments and analyses are systematically conducted on different datasets and NMT architectures. We show that: 1) the pruned parameters can be rejuvenated to improve the baseline model by up to +0.8 BLEU points; 2) the rejuvenated parameters are reallocated to enhance the ability of modeling low-level lexical information."
"Sentiment analysis attracted increasing attention recently. Aspect-based sentiment analysis fine-grained sentiment analysis task includes many subtasks, two aspect category detection detects aspect categories mentioned sentence aspect-category sentiment analysis predicts sentiment polarities respect detected aspect categories. Figure shows example. ACD detects two aspect categories, ambience food, ACSA predicts negative positive sentiment toward respectively. In work, focus ACSA, ACD auxiliary task used find words indicating aspect categories sentences ACSA. Since sentence usually contains one aspect categories, previous studies developed various methods generating aspect category-specific sentence representations detect sentiment toward particular aspect category sentence. To name few, attention-based models allocate appropriate sentiment words given aspect category. \citet{xue2018aspect} proposed generate aspect category-specific representations based convolutional neural networks gating mechanisms. Since aspect-related information may already discarded aspect-irrelevant information may retained aspect independent encoder, existing methods utilized given aspect guide sentence encoding scratch. Recently, BERT based models obtained promising performance ACSA task. However, models ignored sentiment aspect category mentioned sentence aggregation sentiments words indicating aspect category. It leads suboptimal performance models. For example Figure, ``drinks'' ``food'' indicate aspect category food. The sentiment food combination sentiments ``drinks'' ``food''. Note that, words indicating aspect categories contain aspect terms explicitly indicating aspect category also contain words implicitly indicating aspect category . In Figure, ``drinks'' ``food'' aspect terms explicitly indicating aspect category food, ``large'' ``noisy'' aspect terms implicitly indicating aspect category ambience. In paper, propose Multi-Instance Multi-label Learning Network Aspect-Category sentiment analysis . AC-MIMLLN explicitly models fact sentiment aspect category mentioned sentence aggregation sentiments words indicating aspect category. Specifically, AC-MIMLLN treats sentences bags, words instances, words indicating aspect category key instances aspect category. Given bag aspect categories mentioned bag, AC-MIMLLN first predicts instance sentiments, finds key instances aspect categories, finally aggregates sentiments key instances get bag-level sentiments aspect categories. Our main contributions summarized follows: In paper, prove existing NMT systems over-parameterized propose improve utilization efficiency parameters NMT models introducing rejuvenation approach. Empirical results variety language pairs architectures demonstrate effectiveness universality presented method. We also analyze gains perspectives learning dynamics linguistic probing, give insightful research directions future work. Future directions include continuing exploration research topic large sequence-to-sequence pre-training models multi-domain translation models . We employ recent analysis methods better understand behaviors rejuvenated models. \clearpage"," 	Aspect-category sentiment analysis  aims to predict sentiment polarities of sentences with respect to given aspect categories. To detect the sentiment toward a particular aspect category in a sentence, most previous methods first generate an aspect category-specific sentence representation for the aspect category, then predict the sentiment polarity based on the representation. These methods ignore the fact that the sentiment of an aspect category mentioned in a sentence is an aggregation of the sentiments of the words indicating the aspect category in the sentence, which leads to suboptimal performance. In this paper, we propose a Multi-Instance Multi-Label Learning Network for Aspect-Category sentiment analysis , which treats sentences as bags, words as instances, and the words indicating an aspect category as the key instances of the aspect category. Given a sentence and the aspect categories mentioned in the sentence, AC-MIMLLN first predicts the sentiments of the instances, then finds the key instances for the aspect categories, finally obtains the sentiments of the sentence toward the aspect categories by aggregating the key instance sentiments. Experimental results on three public datasets demonstrate the effectiveness of AC-MIMLLN \footnote{Data and code are available at https://github.com/l294265421/AC-MIMLLN}."
"The recent success language model pre-training approaches~, train language models diverse text corpora self-supervised multi-task learning, brought huge performance improvements several natural language understanding tasks~. The key success ability learn generalizable text embeddings achieve near optimal performance diverse tasks additional steps fine-tuning downstream task. Most existing works language model aim obtain universal language model address nearly entire set available natural language tasks heterogeneous domains. Although train-once use-anywhere approach shown helpful various natural language tasks~, considerable needs adapting learned language models domain-specific corpora . Such domains may contain new entities included common text corpora, may contain small amount labeled data obtaining annotation may require expert knowledge. Some recent works~ suggest pre-train language model self-supervised tasks domain-specific text corpus adaptation, show yields improved performance tasks target domain. Masked Language Models objective BERT~ shown effective language model learn knowledge language bi-directional manner~. In general, masks MLMs sampled random~, seems reasonable learning generic language model pre-trained scratch, since needs learn many words vocabulary possible diverse contexts. However, case pre-training already pre-trained language model, conventional selection method may lead domain adaptation inefficient way, since words equally important target task. Repeatedly learning uninformative instances thus wasteful. Instead, done instance selection, effective masks focus important words target domain, specific NLU task hands. How obtain masking strategy train MLMs? Several works~ propose rule-based masking strategies work better random masking ~ applied language model pre-training scratch. Based works, assume adaptation pre-trained language model improved via learned masking policy selects words mask. Yet, existing models inevitably suboptimal since consider target domain task. To overcome limitation, work, propose adaptively generate mask learning optimal masking policy given task, task-adaptive pre-training~ language model. As described Figure , want pre-train language model specific task task-dependent masking policy, directs solution set parameters better adapt target domain, task-agnostic random policy leads model arbitrary solution. To tackle problem, pose given learning problem meta-learning problem learn task-adaptive mask-generating policy, model learned masking strategy obtains high accuracy target task. We refer meta-learner Neural Mask Generator . Specifically, formulate mask learning bi-level problem pre-train fine-tune target language model inner loop, learn NMG outer loop, solve using renforcement learning. We validate method diverse NLU tasks, including question answering text classification. The results show models trained using NMG outperforms models pre-trained using rule-based masking strategies, well finds proper adaptive masking strategy domain task. Our contribution threefold: In paper, propose Multi-Instance Multi-Label Learning Network Aspect-Category sentiment analysis . AC-MIMLLN predicts sentiment aspect category mentioned sentence aggregating sentiments words indicating aspect category sentence. Experimental results demonstrate effectiveness AC-MIMLLN. Since AC-MIMLLN finds key instances given aspect category predicts sentiments key instances, interpretable. In sentences, phrases clauses rather words indicate given aspect category, future work could consider multi-grained instances, including words, phrases clauses. Since directly finding key instances aspect categories ineffective, try first recognize opinion snippets sentence, assign snippets aspect categories mentioned sentence."," We propose a method to automatically generate a domain- and task-adaptive maskings of the given text for self-supervised pre-training, such that we can effectively adapt the language model to a particular target task . Specifically, we present a novel reinforcement learning-based framework which learns the masking policy, such that using the generated masks for further pre-training of the target language model helps improve task performance on unseen texts. We use off-policy actor-critic with entropy regularization and experience replay for reinforcement learning, and propose a Transformer-based policy network that can consider the relative importance of words in a given text. We validate our Neural Mask Generator  on several question answering and text classification datasets using BERT and DistilBERT as the language models, on which it outperforms rule-based masking strategies, by automatically learning optimal adaptive maskings. \footnote{Code is available at \url{github.com/Nardien/NMG}.}"
"Sentiment analysis become increasingly popular natural language processing task academia industry. It provides real-time feedback consumer experience needs, helps producers offer better services. To deal presence multiple categories one document, ACSA tasks, including aspect-category sentiment analysis targeted aspect-category sentiment analysis , introduced. The main purpose ACSA task identify sentiment polarity input sentence upon specific predefined categories . For example, shown Table , giving input sentence ``Food always fresh hot-ready eat, expensive."" predefined categories \{food, service, price, ambience anecdotes/miscellaneous\}, sentiment category food positive, polarity regarding category price negative, none others. In task, models capture explicit expressions implicit expressions. For example, phrase ``too expensive"" indicates negative polarity price category, without direct indication ``price"". In order deal ACSA multiple categories multiple targets, TACSA task introduced analyze sentiment polarity set predefined target-category pairs. An example shown Table , given targets ``restaurant-1"" ``restaurant-2"", case ``I like restaurant-1 cheap, restaurant-2 expansive"", category price target ``restaurant-1"" positive, negative target ``restaurant-2"", none target-category pairs. A mathematical definition ACSA given follows: giving sentence input, predefined set targets predefined set aspect categories , model predicts sentiment polarity target-category pair . For ACSA task, one target categories. In paper, order simplify expression TACSA, use predefined categories, short predefined target-category pairs. } \end{table*} Multi-task learning, shared encoders individual decoders category, approach analyze categories one sample simultaneously ACSA . Compared single-task ways , multi-task approaches utilize category-specific knowledge training signals task get better performance. However, current multi-task models still suffer lack features category name . Models category name features encoded model may improve performance. On hand, predefined categories ACSA task make application new categories inflexible, ACSA applications, number categories maybe varied time. For example, fuel consumption, price level, engine power, space source categories analyzed gasoline automotive domain. For electromotive domain, source categories automotive domain still used, new target category battery duration also analyzed. Incremental learning way solve problem. Therefore, necessary propose incremental learning task incremental learning model concerned new category ACSA tasks. Unfortunately, current multi-task learning ACSA models, encoder shared decoders category individual. This parameter sharing mechanism results shared encoder target-category-related decoders finetuned finetuning process, decoder source categories remains unchanged. The finetuned encoder original decoder source categories may cause catastrophic forgetting problem origin categories. For real applications, high accuracy excepted source categories target categories. Based previous researches decoders different tasks usually modeled mean regularization , idea comes make decoders sharing decoders categories decrease catastrophic forgetting problem. But raises another question, identify category encoder decoder shared network? In approach, solve category discrimination problem input category name feature. In paper, proposed multi-task category name embedding network . The multi-task learning framework makes full use training signals categories. To make feasible incremental learning, encoder decoders category shared. The category names applied another input feature task discrimination. We also present new task ACSA incremental learning. In particular, contribution three-folded: We proposed multi-task CNE-net framework encoder decoder shared weaken catastrophic forgetting problem multi-task learning ACSA model. We achieved state-of-the-art two ACSA datasets, SemEval14-Task4 Sentihood. We proposed new task incremental learning ACSA. By sharing encoder layers decoder layers tasks, achieved better results compared baselines source categories target category. We proposed novel framework automatically generates adaptive masking masked language models based given context, language model adaptation low-resource domains. To end, proposed Neural Mask Generator , trained reinforcement learning mask words helpful domain adaptation. We performed empirical study various rule-based masking strategies multiple datasets question answering text classification tasks, shows optimal masking strategy depends language model domain. We validated NMG rule-based masking strategies, results show either outperforms, obtains comparable performance best heuristic. Further qualitative analysis suggests good performance comes ability adaptively mask meaningful words given task."," ACSA tasks, including aspect-category sentiment analysis  and  targeted  aspect-category sentiment analysis , aims at identifying sentiment  polarity on predefined categories. Incremental learning on new categories is necessary for ACSA real applications. Though current multi-task learning models achieve good performance in ACSA tasks, they suffer from catastrophic forgetting problems in ACSA incremental learning tasks. In this paper, to make multi-task learning feasible for incremental learning, we proposed Category Name  Embedding network  . We set both encoder and decoder shared among all categories to weaken the catastrophic forgetting problem. Besides the origin input sentence, we applied another input feature, i.e., category name, for task discrimination.  Our model achieved state-of-the-art  on two ACSA benchmark datasets. Furthermore, we proposed  a dataset for ACSA incremental learning and achieved the best performance compared with other strong baselines."
"Conditional random fields shown perform well various sequence labeling tasks. Recent work uses rich neural network architectures define ``unary'' potentials, i.e., terms consider single position's label time~. However, ``binary'' potentials, consider pairs adjacent labels, usually quite simple may consist solely parameter parameter vector unique label transition. Models unary binary potentials generally referred ``first order'' models. A major challenge CRFs complexity training inference, quadratic number output labels first order models grow exponentially higher order dependencies considered. This explains common type CRF used practice first order model, also referred ``linear chain'' CRF. One promising alternative CRFs structured prediction energy networks , use deep neural networks parameterize arbitrary potential functions structured prediction. While SPENs also pose challenges learning inference, \citet{tu-18} proposed way train SPENs jointly ``inference networks'', neural networks trained approximate structured inference. In paper, leverage frameworks SPENs inference networks explore high-order energy functions sequence labeling. Naively instantiating high-order energy terms lead large number parameters learn, instead develop concise neural parameterizations high-order terms. In particular, draw vectorized Kronecker products, convolutional networks, recurrent networks, self-attention. We also consider ``skip-chain'' connections~ various skip distances ways reducing total parameter count increased learnability. Our experimental results four sequence labeling tasks show range high-order energy functions yield performance improvements. While optimal energy function varies task, find strong performance skip-chain terms short skip distances, convolutional networks filters consider label trigrams, recurrent networks self-attention networks consider large subsequences labels. We also demonstrate modeling high-order dependencies lead significant performance improvements setting noisy training test sets. Visualizations high-order energies show various methods capture intuitive structured dependencies among output labels. Throughout, use inference networks share architecture unstructured classifiers sequence labeling, test time inference speeds unchanged local models method. Enlarging inference network architecture adding one layer leads consistently better results, rivaling improving BiLSTM-CRF baseline, suggesting training efficient inference networks high-order energy terms make errors arising approximate inference. While focus sequence labeling paper, results show potential developing high-order structured models NLP tasks future. In paper, order make multi-task learning feasible incremental learning, proposed CNE-net different attention mechanisms. The category name features multi-task learning structure help model achieve state-of-the-art ACSA TACSA tasks. Furthermore, shared encoder decoder layers weaken catastrophic forgetting incremental learning task. We proposed task ACSA incremental learning achieved best performance CNE-net compared strong baselines. Further research may concerned zero-shot learning new categories."," Many tasks in natural language processing involve predicting structured outputs, e.g., sequence labeling, semantic role labeling, parsing, and machine translation. Researchers are increasingly applying deep representation learning to these problems, but the structured component of these approaches is usually quite simplistic. In this work, we propose several high-order energy terms to capture complex dependencies among labels in sequence labeling, including several that consider the entire label sequence. We use neural parameterizations for these energy terms, drawing from convolutional, recurrent, and self-attention networks. We use the framework of learning energy-based inference networks for dealing with the difficulties of training and inference with such models. We empirically demonstrate that this approach achieves substantial improvement using a variety of high-order energy terms on four sequence labeling tasks, while having the same decoding speed as simple, local classifiers.  We also find high-order energies to help in noisy data conditions.\footnote{Code  is available at \url{https://github.com/tyliupku/Arbitrary-Order-Infnet}}"
"Long document coreference resolution poses runtime memory challenges. Current best models % coreference resolution large memory requirements quadratic runtime document length~, making impractical long documents. % Recent work revisiting entity-mention paradigm~, seeks maintain explicit representations entities, rather constituent mentions, shown practical benefits memory competitive state-of-the-art models~. In particular, unlike approaches coreference resolution maintain representations mentions corresponding entity clusters~ , entity-mention paradigm stores representations entity clusters, updated incrementally coreference predictions made. While approach requires less memory additionally store mention representations, number entities impractically large processing long documents, making storing entity representations problematic. Is necessary maintain unbounded number mentions entities? Psycholinguistic evidence suggests not, human language processing incremental limited working memory~. In practice, find entities small spread , thus need kept persistently memory. This observation suggests tracking limited, small number entities time resolve computational % issues, albeit potential accuracy tradeoff. Previous work bounded memory models coreference resolution shown potential, tested short documents % . % Moreover, previous work makes token-level predictions standard coreference datasets span-level annotations. % We propose bounded memory model performs quasi-online coreference resolution, We explore arbitrary-order models different neural parameterizations sequence labeling tasks via energy-based inference networks. This approach achieve substantial improvement using high-order energy terms, especially noisy data conditions, decoding speed simple local classifiers. File emnlp2020.tex Based style files ACL 2020, Based style files ACL 2018, NAACL 2018/19, Based style files ACL-2015, improvements taken NAACL-2016 style Based style files ACL-2014, were, turn, based ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009, EACL-2009, IJCNLP-2008... Based style files EACL 2006 e.agirre@ehu.es Sergi.Balari@uab.es ACL 08 Joakim Nivre Noah Smith \documentclass[11pt,a4paper]{article} \usepackage[hyperref]{emnlp2020} \usepackage{times} \usepackage{latexsym} \renewcommand{\UrlFont}{\ttfamily\small} This strictly necessary, may commented out, improve layout manuscript, typically save space. \usepackage{microtype} \usepackage{graphicx} \usepackage{xcolor} \usepackage{color} \usepackage{multirow} \newcommand{\vect}[1]{\mbox{\boldmath }} \newcommand{\theHalgorithm}{\arabic{algorithm}} \DeclareMathOperator*{\LM}{TLM} \DeclareMathOperator*{\BLSTM}{BiLSTM} \DeclareMathOperator*{\g}{G} \DeclareMathOperator*{\d0}{D} \DeclareMathOperator*{\PT}{\#paras_T} \DeclareMathOperator*{\PI}{\#paras_I} \DeclareMathOperator*{\attention}{attention} \usepackage{amssymb} \usepackage{graphicx} \usepackage{subcaption} \usepackage{ctable} \newcommand{\lmreg}{E^{\LM}} \DeclareMathOperator*{\TLM}{TLM} \newcommand{\encseq}{\enc_{\mathrm{NN}}} \DeclareMathOperator*{\LSTM}{LSTM} \newcommand{\Expect}{{\rm I\kern-.3em E}} \DeclareMathOperator*{\LayerNorm}{LayerNorm} \DeclareMathOperator*{\MLP}{MLP} \DeclareMathOperator*{\product}{KP} \DeclareMathOperator*{\product}{VKP} \newcommand{\x}{\boldsymbol{x}} \newcommand{\y}{\boldsymbol{y}} \newcommand{\xspace}{\mathcal{X}} \newcommand{\yspace}{\mathcal{Y}} \newcommand{\relyspace}{\mathcal{Y}_{R}} \newcommand{\cost}{\bigtriangleup} \newcommand{\infnet}{\mathbf{A}_{\Psi}} \newcommand{\canet}{\mathbf{F}_{\Phi}} \newcommand{\mlocal}{\mathit{loc}} \newcommand{\mlabel}{\mathit{lab}} \newcommand{\R}{\mathbb{R}} \newcommand{\ltok}{\ell_{\mathrm{token}}} \newcommand{\pvect}{\mathbf{v}} \newenvironment{itemizesquish}{} This strictly necessary, may commented out, improve layout manuscript, typically save space. \usepackage{microtype} \hyphenation{Gimpel} \hyphenation{Unk-Test} \hyphenation{Unk-Train} \aclfinalcopy Uncomment line final submission \def\aclpaperid{***} Enter acl Paper ID \setlength\titlebox{5cm} You expand titlebox need extra space show authors. Please make titlebox smaller 5cm ; check camera-ready version ask change back. \newcommand\BibTeX{B\TeX} \title{An Exploration Arbitrary-Order Sequence Labeling\\ via Energy-Based Inference Networks} \author{Lifu Tu \Thanks{ Equal contribution.} \ \ \ \ \ \ Tianyu Liu \newcommand{\new}{\marginpar{NEW}}"," Long document coreference resolution remains a challenging task	 due to the large memory and runtime requirements of current models. Recent work doing incremental coreference resolution using just the global representation of entities shows practical benefits but requires keeping all entities in memory, which can be impractical for long documents. % We argue that keeping all entities in memory is unnecessary, and we propose a memory-augmented neural network that tracks only a small bounded number of entities at a time, thus guaranteeing a linear runtime in length of document. We show that  the model remains competitive with models with high memory and computational requirements on OntoNotes and LitBank, and  the model learns an efficient memory management strategy easily outperforming a rule-based strategy."
"Since early days NLP, conversational agents designed interact humans language solve diverse tasks, e.g., remote instructions booking assistants . In goal-oriented dialogue setting, conversational agents often designed compose predefined language utterances. Even approaches efficient, also tend narrow agent's language diversity. To remove restriction, recent work exploring interactive word-based training. In setting, agents generally trained two-stage process: Firstly, agent pretrained human-labeled corpus supervised learning generate grammatically reasonable sentences. Secondly, agent finetuned maximize task-completion score interacting user. Due sample-complexity reproducibility issues, user generally replaced game simulator may evolve conversational agent. Unfortunately, pairing may lead language drift phenomenon, conversational agents gradually co-adapt, drift away pretrained natural language. The model thus becomes unfit interact humans. While domain-specific methods exist counter language drift, simple task-agnostic method consists combining interactive supervised training losses pretraining corpus, later formalized Supervised SelfPlay . Inspired language evolution cultural transmission, recent work proposes Seeded Iterated Learning another task-agnostic method counter language drift. SIL modifies training dynamics iteratively refining pretrained student agent imitating interactive agents, illustrated Figure. At iteration, teacher agent created duplicating student agent, finetuned towards task completion. A new dataset generated greedily sampling teacher, samples used refine student supervised learning. The authors empirically show iterated learning procedure induces inductive learning bias successfully maintains language grounding improving task-completion. \vskip -1em \end{figure*} As first contribution, examine performance two methods setting translation game. We show S2P unable maintain high grounding score experiences late-stage collapse, SIL higher negative likelihood evaluated human corpus. We propose combine SIL S2P applying S2P loss interactive stage SIL. We show resulting Supervised Seeded Iterated Learning algorithm manages get best algorithms translation game. Finally, observe late-stage collapse S2P correlated conflicting gradients showing \algo empirically reduces gradient discrepancy. We propose memory model tracks small, bounded number entities. The proposed model guarantees linear runtime document length, practice significantly reduces peak memory usage training. Empirical results LitBank OntoNotes show model competitive unbounded memory version outperforms strong rule-based baseline. In particular, report state art results LitBank. In future work plan apply model longer, book length documents, plan add structure memory."," Language drift has been one of the major obstacles to train language models through interaction. When word-based conversational agents are trained towards completing a task, they tend to invent their language rather than leveraging natural language. In recent literature, two general methods partially counter this phenomenon: Supervised Selfplay  and Seeded Iterated Learning .  While S2P jointly trains interactive and supervised losses to counter the drift, SIL changes the training dynamics to prevent language drift from occurring. In this paper, we first highlight their respective weaknesses, i.e., late-stage training collapses and higher negative likelihood when evaluated on human corpus. Given these observations, we introduce \longalgo~ to combine both methods to minimize their respective weaknesses.  We then show the effectiveness of \algo in the language-drift translation game."
"Event argument extraction aims identify entities serve arguments event classify specific roles play. As Fig., ``two soldiers'' ``yesterday'' arguments, event triggers ``attacked'' ``injured'' . For trigger ``attacked'', ``two soldiers'' plays argument role Target ``yesterday'' plays argument role Attack\_Time. For event trigger ``injured'', ``two soldiers'' ``yesterday'' play role Victim INJURY\_Time, respectively. There significant work event extraction , EAE task remains challenge become bottleneck improving overall performance EE.\footnote{EAE similarities semantic role labeling. Event triggers comparable predicates SRL roles SRL datasets standard convention interpreting whom. EAE custom taxonomy roles domain. We also use inspiration SRL body work .} Supervised data EAE expensive hence scarce. One possible solution use available resources like unlabeled data. For that, We use BERT model encoder leverages much larger unannotated corpus semantic information captured. Unlike%previous studies ~ added final/prediction layer BERT argument extraction, use BERT token embedder build sequence EAE task-specific components . We use in-domain data adapt BERT model parameters subsequent pretraining step . This makes encoder domain-aware. We perform self-training construct auto-labeled data . A crucial aspect EAE integrate event trigger information learned representations. This important arguments dependent triggers, i.e., argument span plays completely different roles toward different triggers. An example shown Fig., ``two soldiers'' plays role Target event ATTACK role Victim INJURY. Different existing work relies regular sequence encoders, design novel trigger-aware encoder simultaneously learns four different types trigger-informed sequence representations. %for candidate arguments. Capturing long-range dependency another important factor, e.g., connection event trigger distant argument. Syntactic information could useful case, could help bridge gap word another distant highly related word. We modify Transformer explicitly incorporating syntax via attention layer driven dependency parse sequence. % . %Since arguments event entities, entity mentions effective hints. We design role-specific argument decoder seamlessly accommodate settings . We also tackle role overlap problem using set classifiers taggers decoder. Our model achieves new state-of-the-art ACE2005 Events data.% EAE. % % Motivation 1: data scarcity. Proposed used solutions: pretrained model BERT External embedding Self-training BERT MLM MLM encoder decoder joint pre-training. Teacher-Student % We present first systematic study negative interference multilingual models shed light causes. We propose method show improve cross-lingual transferability mitigating negative interference. While prior efforts focus improving sharing cross-lingual alignment, provide new insights different perspective unsharing resolving language conflicts."," Event argument extraction  aims to identify the arguments of an event and classify the roles that those arguments play. Despite great efforts made in prior work, there remain many challenges:  Data scarcity.  Capturing the long-range dependency, specifically, the connection between an event trigger and a distant event argument.  Integrating event trigger information into candidate argument representation. For , we explore using unlabeled data in different ways. For , we propose to use a syntax-attending Transformer that can utilize dependency parses to guide the attention mechanism. For , we propose a trigger-aware sequence encoder with several types of trigger-dependent sequence representations. We also support argument extraction either from text annotated with gold entities or from plain text. Experiments on the English ACE2005 benchmark show that our approach achieves a new state-of-the-art."
"In current NLP tasks, fixed-length vector representations words, word embeddings, used represent form meaning word. In case humans, however, oftentimes use sequence words known definition ---a statement meaning term--- express meanings terms . It mind question ``Can machines define?'' aimed answered task definition modeling . Definition modeling framed task conditional generation, definition word phrase generated given conditioning variable word's associated word embedding representations context. Current approaches task mainly encoder-decoder based, one encodes contextual representation word/phrase using variety features context character composition, uses contextual representation generate definition . % discuss issues approaches including Despite relative success existing approaches definition modelling, discriminative nature ---where distributional-derived information one end model lexical information other--- limits power underlying semantic representations distributional lexical information learned implicit rather direct way. For example, although \citet{ishiwatari-etal-2019-learning} successfully showed local global contexts useful disambiguate meanings phrases certain cases, approach heavily relies attention mechanism identify semantic alignments input phrase output definition, may introduce noise ultimately insufficient capture entire meaning phrase-definition pair. % latent definition space To tackle issue, propose explicitly model underlying semantics phrase-definition pairs introducing continuous latent variable definition space, used conjunction guide generation definition . The introduction latent representation enables us treat global defining signal generation process, complementing existing alignment mechanisms attention. % We specifically incorporate latent variable directly decoder cell, showing addition latent variable way leads increased performance task. Although latent definition variable enables us explicitly model underlying semantics context-definition pairs, incorporation task renders posterior intractable. In paper recur variational inference estimate intractable posterior, effectively making model Conditional Variational Autoencoder evolving generation process . %to serve global decoding signal allows decoder rely attention, attention misleading rely latent variable % issue misleading attentions exacerbated noisy datasets, see improvements well generator learns misleading attention representations. % EDISON % enables us generate definitions previously unknown words phrases, menas example , also obtain semantically meaningful vectors new words means providing definition alongside example usage. Effectively, mode able mapping inputs smooth space/manifold?. We also note existing approaches definition modelling heavily rely word embeddings, due fixed nature capture much semantics, known offer limited capabilities dealing polysemy. Considering success pretrained deep contextualized word representations specifically addressing limitations shown improve performance variety downstream NLP tasks , paper propose mechanism integrate deep contextualized word representations definition modelling task. Specifically, successfully leverage BERT contextual encoder definition encoder produce representations respectively. %While inclusion deep contextual word representations important approach, resuts show essential . %As result, model able allowing meaningful continuous latent space, . [2-3 sentences] Finally, develop two new datasets task, one derived Cambridge Dictionary , derived Le Petit Robert. In summary, contributions are: Datasets pre-trained models publicly released greater NLP community help facilitate advances task upon acceptance paper. We present new model provides best results EAE task. The model generate trigger-aware argument representations, incorporate syntactic information , handle role overlapping problem role-specific argument decoder. We also experiment methods address data scarcity issue. Experimental results show effectiveness proposed approaches. Experimental results demonstrate effectiveness approach. We also addressd learning gap/discrepancy pre-trained newly-trained components.","     %Definition modeling, the task of generating word/phrase definitions, is the task that aims to answer ``Can machines define?'' In this paper, we aim to tackle this problem by introducing a continuous latent variable to explicitly model the underlying relationship between a phrase used within a context and its definition. Additionally, we release 2 new datasets, Cambridge and the first non-English dataset Robert. On most datasets, our Variational Contextual Definition Modeler  achieves a new state-of-the-art, outperforming existing systems as well as a new BERT-based baseline. In this paper we tackle the task of definition modeling, where the goal is to learn to generate definitions of words and phrases. Existing approaches for this task are discriminative, combining distributional and lexical semantics in an implicit rather than direct way. To tackle this issue we propose a generative model for the task, introducing a continuous latent variable to explicitly model the underlying relationship between a phrase used within a context and its definition. We rely on variational inference for estimation and leverage contextualized word embeddings for improved performance. Our approach is evaluated on four existing challenging benchmarks with the addition of two new datasets, Cambridge and the first non-English corpus Robert, which we release to complement our empirical study. Our Variational Contextual Definition Modeler  achieves state-of-the-art performance in terms of automatic and human evaluation metrics, demonstrating the effectiveness of our approach.\footnote{We release the code at: \url{https://github.com/machelreid/vcdm}}"
"Topic segmentation fundamental NLP task received considerable attention recent years . It reveal important aspects document semantic structure splitting document topical-coherent textual units. Taking Wikipedia article Table example, without section marks, reliable topic segmenter able detect correct boundaries within text chunk article topical-coherent units , . The results topic segmentation benefit key downstream NLP tasks document summarization , question answering , machine reading dialogue modeling . } A Wikipedia sample article City Marcus covering three topics: , } \end{table} A wide variety techniques proposed topic segmentation. Early unsupervised models exploit word statistic overlaps , Bayesian contexts %the semantic relatedness graphs measure lexical semantic cohesion sentences paragraphs infer segment boundaries them. More recently, several works framed topic segmentation neural supervised learning, remarkable success achieved models NLP tasks . %While one line research forms topic segmentation sequence labeling problem builds neural models predict segment boundaries directly ; %another line works first trains neural models tasks , uses models' outputs predict boundaries . Despite %the minor architectural differences, neural solutions adopt Recurrent Neural Network variants main framework. On one hand, RNNs appropriate topic segmentation modelled sequence labeling task sentence either end segment not. On hand, choice makes neural models limited model context. Because sophisticated RNNs able preserve long-distance information , largely help language models. But topic segmentation, critical supervise model focus local context. %In fact, RNNs superior many NLP tasks due capability preserving long-distance information . %However, topic segmentation, also critical supervise model learn right information local context. As illustrated Table, prediction segment boundary hardly depends content . Bringing excessive long-distance signals may cause unnecessary noise %further hurt %model's performance. Moreover, text coherence strong relation topic segmentation . For instance, Table, sentence pairs segment %should coherent %to put together sentence pairs across segments . Arguably, proper way modeling coherence adjacent sentences, topic segmenter enhanced. %\textcolor{red}{We hypothesize topic segment prediction rely local contextual information way cannot effectively captured RNNs.} %\textcolor{red}{In essence, RNNs able model long short-distance dependencies implicitly.} %However, restricted self-attention, model pay attention local context neighboring sentences explicitly constrained way . %In essence, local contextual information critical predicting topical boundaries, simple Recurrent Neural Network variants arguably sufficiently powerful represent necessary information. %However, approaches still face challenge insufficient context modeling. Topic segment boundary prediction usually heavily relies local contextual information. Hence, effectively select local contexts model relations contexts becomes important. Neural models like RNN variants represent state timestep memorizing forgetting information previous later contexts. But learned contextual information contribute model's decision straightforward sufficiently transparent. In paper, propose enhance state-of-the-art topic segmenter based hierarchical attention BiLSTM network better model local context sentence two complementary ways. First, add coherence-related auxiliary task make model learn informative hidden states sentences document. %More specifically, refine objective model encourage coherence sentences different segments smaller coherence sentences segment. More specifically, refine objective model encourage smaller coherence sentences different segments larger coherence sentences segment. Secondly, enhance context modeling utilizing restricted self-attention , enables model pay attention local context make better use information closer neighbors sentence . Our empirical results show proposed context modeling strategy significantly improves performance SOTA neural segmenter three datasets, enhanced segmenter robust domain transfer setting applied four challenging real-world test sets, sampled differently training data, context modeling strategy also effective segmenters trained challenging languages , rather English. In paper introduced generative model directly combines distributional lexical semantics via continuous latent variable task definition modeling. Empirical results multiple corpora, including two new datasets released, show model able outperform previous work consistent margin, also successfully able leveraging contextualized word representations. For future work interested exploring definition modeling could adapted multilingual cross-lingual setting. goal learn generate definitions words phrases. Existing approaches task discriminative, To tackle issue propose generative model task, introducing continuous latent variable explicitly model underlying relationship phrase used within context definition. We rely variational inference estimation leverage contextualized word embeddings improved performance. Our approach evaluated four existing challenging benchmarks addition two new datasets, first non-English corpus , release complement empirical study. Our Variational Contextual Definition Modeler achieves state-of-the-art performance terms automatic human evaluation metrics, demonstrating effectiveness approach. Conclude something here.","      Topic segmentation is critical %, the process of splitting a document into topic-coherent pieces,      %plays a vital role      in key NLP tasks and recent works favor highly effective neural supervised  approaches.     %Due to the high effectiveness of neural models, more recent works have favored framing topic segmentation as a neural-based supervised learning problem.     However, current neural solutions are arguably limited in how they model context.     %topic segmenters proposed so far are still limited by the insufficient context modeling.      In this paper, we enhance a segmenter based on a hierarchical attention BiLSTM network to better model context, by adding a coherence-related auxiliary task and restricted self-attention. Our optimized segmenter\footnote{Our code will be publicly available at \url{www.cs.ubc.ca/cs-research/lci/research-groups/natural-language-processing/}} outperforms SOTA approaches when trained and tested on three datasets. We also the robustness of our proposed model in domain transfer setting by training a model on a large-scale dataset and testing it on four challenging real-world benchmarks. Furthermore, we apply our proposed strategy to two other languages , and show its effectiveness in multilingual scenarios."
"Natural Language Understanding evaluation plays key role benchmarking progress natural language processing research. With recent advance language representative learning, results previous benchmarks rapidly saturated. This leads explosion difficult, diverse proposals tasks/datasets NLU evaluation, including Natural Language Inference , Grounded Commonsense Inference, Commonsense QA, Social Interactions Reasoning, Abductive Commonsense Reasoning , etc. One common practice followed recent works simplify evaluation various reasoning abilities classification task. This analogous asking objective questions human educational testing. This simplification facilitates data annotation also gives interpretable evaluation results, based behaviors models studied weaknesses diagnosed. Despite straightforwardness formalization, one assumption behind prior benchmark data sourcing exists single prescriptive ground truth label example. The assumption might true human educational settings prescriptivism preferred descriptivism goal test humans well-defined knowledge norms. However, true many NLP tasks due pragmatic nature meaning sentence might differ depending context background knowledge. Specifically NLI task, \citet{manning2006local} advocate annotation tasks ``natural'' untrained annotators, role NLP model inferences humans make practical settings. Previous work uses graded labeling schema NLI, showed inherent disagreements inference tasks. All discussions challenge commonly used majority ``gold-label'' practice prior data collections evaluations. Intuitively, disagreements among humans allowed different annotators might different subjective views world might think differently encounter reasoning task. Thus, descriptive perspective, evaluating capacity NLP models predicting individual human opinions majority human opinion, also overall distribution human judgments provides representative comparison model capabilities `collective' human intelligence. Therefore, collect ChaosNLI, large set Collective HumAn OpinionS examples several existing NLI datasets, comprehensively examine factor human agreement state-of-the-art model performances. Specifically, contributions are: The ChaosNLI dataset experimental scripts available \url{https://github.com/easonnie/ChaosNLI} This work aims establish better way represent language modality text-based ZSL image classification. Our approach relies semantic information visual features, visual features themselves. Specifically, two orthogonal text-processing methods, employing textual similarity visually-relevant summaries, lead significant improvements across models, splits, datasets, illustrate adequate text-processing essential text-based ZSL tasks. We conjecture text-processing methods essential range vision language-based tasks, hope work assist future research better representing language modality various multi-modal tasks. First, unsupervised clustering algorithms used construct textual similarity vectors seen unseen image-text pairs. Second, visually relevant summaries texts. Each sentence image description assigned VRS-score, determines sentence's level groundedness images."," Despite the subjective nature of many NLP tasks, most NLU evaluations have focused on using the majority label with presumably high agreement as the ground truth. Less attention has been paid to the distribution of human opinions. We collect ChaosNLI, a dataset with a total of 464,500 annotations to study Collective HumAn OpinionS in oft-used NLI evaluation sets. This dataset is created by collecting 100 annotations per example for 3,113 examples in SNLI and MNLI and 1,532 examples in \abdnli. Analysis reveals that:  high human disagreement exists in a noticeable amount of examples in these datasets;  the state-of-the-art models lack the ability to recover the distribution over human labels;   models achieve near-perfect accuracy on the subset of data with a high level of human agreement, whereas they can barely beat a random guess on the data with low levels of human agreement, which compose most of the common errors made by state-of-the-art models on the evaluation sets. This questions the validity of improving model performance on old metrics for the low-agreement part of evaluation datasets. Hence, we argue for a detailed examination of human agreement in future data collection efforts, and evaluating model outputs against the distribution over collective human opinions.\footnote{The ChaosNLI dataset and experimental scripts are available at \url{https://github.com/easonnie/ChaosNLI}}"
"Understanding reasoning natural language plays significant role artificial intelligence tasks Machine Reading Comprehension Question Answering . Several QA tasks proposed recent years evaluate language understanding capabilities machines . These tasks single-hop QA tasks consider answering question given one single paragraph. % The drawback single-hop QA tasks lack evaluating deep reasoning capability. % We observe many existing neural models achieve promising performance without reasoning. Many existing neural models rely learning context type-matching heuristics. Those rarely build reasoning modules achieve promising performance single-hop QA tasks. The main reason single-hop QA tasks lacking realistic evaluation reasoning capabilities require complex reasoning. Recently multi-hop QA tasks, HotpotQA WikiHop, proposed assess multi-hop reasoning ability. HotpotQA task provides annotations evaluate document level question answering finding supporting facts. Providing supervision supporting facts improves explainabilty predicted answer clarify cross paragraph reasoning path. Due requirement multi-hop reasoning multiple documents strong distraction, multi-hop QA tasks challenging. Figure shows example HotpotQA. Given question 10 paragraphs, paragraph paragraph relevant. The second sentence paragraph first sentence paragraph supporting facts. The answer ``Geelong Football Club''. Primary studies HotpotQA task prefer use reading comprehension neural model. First, use neural retriever model find relevant paragraphs question. After that, neural reader model applied selected paragraphs answer prediction. Although approaches obtain promising results, performance evaluating multi-hop reasoning capability unsatisfactory. To solve multi-hop reasoning problem, models tried construct entity graph using Spacy Stanford CoreNLP applied graph model infer entity path question answer. However, models ignore importance semantic structure sentences edge information entity types entity graph. To take in-depth semantic roles semantic edges words account use semantic role labeling graph backbone graph convolutional network. Semantic role labeling provides semantic structure sentence terms argument-predicate relationships. % ``who whom.'' The argument-predicate relationship graph significantly improve multi-hop reasoning results. Our experiments show SRL effective finding cross paragraph reasoning path answering question. Our proposed semantic role labeling graph reasoning network jointly learns find cross paragraph reasoning paths answers questions multi-hop QA. In SRLGRN model, firstly, train paragraph selection module retrieve gold documents minimize distractor. Second, build heterogeneous document-level graph contains sentences nodes , % sentence nodes include SRL sub-graphs including semantic role labeling arguments nodes predicates edges. Third, train graph encoder obtain graph node representations incorporate argument types semantics predicate edges learned representations. Finally, jointly train multi-hop supporting fact prediction module finds cross paragraph reasoning path, answer prediction module obtains final answer. Notice supporting fact prediction answer prediction based contextual semantics graph representations well token-level BERT pre-trained representations. The contributions work follows: {\bf 1)} We propose SRLGRN framework considers semantic structure sentences building reasoning graph network. Not semantics roles nodes also semantics edges exploited model. {\bf 2)} We evaluate analyse reasoning capabilities semantic role labeling graph compared usual entity graphs. %We analyze multi-hop reasoning capacity HotpotQA task. The fine-grained semantics SRL graph help finding answer explainability reasoning path. {\bf 3)} Our proposed model obtains competitive results HotpotQA SQuAD benchmarks. In paper, presented approach detecting categorizing offensive language social media. We proposed multi-lingual learning method detect offensive language knowledge distillation method categorize offensive language. We exploration multilingual offensive language identification future, e.g. validating zero-shot performance model languages. include bib file like this:"," This work deals with the challenge of learning and reasoning over multi-hop question answering . We propose a graph reasoning network based on the semantic structure of the sentences to learn cross paragraph reasoning paths and find the supporting facts and the answer jointly. The proposed graph is a heterogeneous document-level graph that contains nodes of type sentence , and semantic role labeling sub-graphs per sentence that contain arguments as nodes and predicates as edges. Incorporating the argument types, the argument phrases, and the semantics of the edges originated from SRL predicates into the graph encoder helps in finding and also the explainability of the reasoning paths. Our proposed approach shows competitive performance on the HotpotQA distractor setting benchmark compared to the recent state-of-the-art models."
"The organizers 2020 VarDial Evaluation Campaign proposed shared task targeted towards geolocation short texts, e.g.~tweets, namely Social Media Variety Geolocation task. Typically formulated double regression problem, task predicting location, expressed latitude longitude, text received input posted certain social media platform. Twitter Jodel platforms used data collection, divided language area three subtasks, namely: In paper, focus second subtask, SMG-CH, proposing variety handcrafted deep learning models, well ensemble model combines previous models meta-learning. Our first model Support Vector Regression classifier based string kernels, known perform well dialect identification tasks . Our second model character-level convolutional neural network , also known provide good results dialect identification . Due high popularity outstanding results Bidirectional Encoder Representations Transformers solving mainstream NLP tasks, decided try Long Short-Term Memory network based German BERT embeddings third model. Lastly, combine three models ensemble employs Extreme Gradient Boosting meta-learner. We conducted experiments development set provided organizers, order decide models choose three submissions SMG-CH subtask. Our results indicate ensemble model attains best results. Perhaps surprisingly, shallow approach based string kernels outperforms deep learning models. Our observations consistent across development test sets provided organizers. % We experimented Machine Learning algorithms second subtask, namely CH, % Geolocation framed double regression task, sophisticated model architectures proposed . % Jodel mobile chat application lets people anonymously talk users within 10km-radius around them. % All three subtasks use data format evaluation methodology, participants encouraged submit systems subtasks. The rest paper organized follows. We present related work dialect identification geolocation short texts Section. Our approaches described detail Section. We present experiments empirical results Section. Finally, conclusions drawn Section. We proposed novel semantic role labeling graph reasoning network deal multi-hop QA. The model jointly trains detect supporting facts find final answer. The backbone graph proposed graph convolutional network created based semantic structure sentences. In creating edges nodes graph, exploit semantic role labeling sub-graph sentence connect candidate supporting facts. The cross paragraph argument-predicate structure sentences expressed graph provides explicit representation reasoning path helps finding explaining multiple hops reasoning lead final answer. We analyze multi-hop reasoning ability model. SRLGRN exceeds SOTA results HotpotQA benchmark. Moreover, evaluate model reading comprehension benchmarks. Our approach achieves competitive performance SQuAD v v."," In this work, we introduce the methods proposed by the UnibucKernel team in solving the Social Media Variety Geolocation task featured in the 2020 VarDial Evaluation Campaign. We address only the second subtask, which targets a data set composed of nearly 30 thousand Swiss German Jodels. The dialect identification task is about accurately predicting the latitude and longitude of test samples. We frame the task as a double regression problem, employing a variety of machine learning approaches to predict both latitude and longitude. From simple models for regression, such as Support Vector Regression, to deep neural networks, such as Long Short-Term Memory networks and character-level convolutional neural networks, and, finally, to ensemble models based on meta-learners, such as XGBoost, our interest is focused on approaching the problem from a few different perspectives, in an attempt to minimize the prediction error. With the same goal in mind, we also considered many types of features, from high-level features, such as BERT embeddings, to low-level features, such as characters n-grams, which are known to provide good results in dialect identification. Our empirical results indicate that the handcrafted model based on string kernels outperforms the deep learning approaches. Nevertheless, our best performance is given by the ensemble model that combines both handcrafted and deep learning models."
"Comparing contrasting meaning text conveyed different languages fundamental nlp task. It used curate clean parallel corpora downstream tasks machine translation~, cross-lingual transfer learning, semantic modeling~, also useful directly analyze multilingual corpora. For instance, detecting commonalities divergences sentences drawn English French Wikipedia articles topic would help analyze language bias~, mitigate differences coverage usage across languages~. This requires detecting coarse content mismatches, also fine-grained differences sentences overlap content. Consider following English French sentences, sampled WikiMatrix parallel corpus. While share important content, highlighted words convey meaning missing language: We show explicitly considering diverse types semantic divergences bilingual text benefits annotation prediction cross-lingual semantic divergences. We create release Rationalized English-French Semantic Divergences corpus , based novel divergence annotation protocol exploits rationales improve annotator agreement. We introduce \modelname, bert-based model detects fine-grained semantic divergences without supervision learning rank synthetic divergences varying granularity. Experiments \dataset show model distinguishes semantically equivalent divergent examples much better strong sentence similarity baseline unsupervised token-level divergence tagging offers promise refine distinctions among divergent instances. We make code data publicly available.\footnote{Implementations \modelname found at: \url{https://github.com/Elbria/xling-SemDiv}; \dataset dataset hosted at: \url{https://github.com/Elbria/xling-SemDiv/tree/master/REFreSD}.} In current work, tackled SMG-CH shared subtask 2020 VarDial Evaluation Campaign. We addressed challenge shallow perspective, handcrafted models -SVR based string kernels, well deep learning perspective, neural models LSTM based BERT embeddings character-level CNN, respectively. Additionally, combined proposed models ensemble, employing XGBoost meta-learner. We obtained best results XGBoost ensemble, benefits complementary information handcrafted deep models. We therefore brought one proof regarding effectiveness ensemble learning general, XGBoost, particular. Another important conclusion shallow model based string kernels outperforms two deep neural networks. We consider yet another indicator high discriminative power string kernels bring fairly standard learning model, i.e.~the -SVR. In future work, aim explore ways improve performance respect metrics proposed shared task organizers. Currently, seems training models simply minimize MSE MAE values effective, best model significantly outperformed model proposed shared task organizers themselves.","  Detecting fine-grained differences in content conveyed in different languages matters for cross-lingual nlp and multilingual corpora analysis, but it is a challenging machine learning problem since annotation is expensive and hard to scale.~This work improves the prediction and annotation of fine-grained semantic divergences.~We introduce a training strategy for multilingual bert models by learning to rank synthetic divergent examples of varying granularity.~We evaluate our models on the~Rationalized~English-French~Semantic~Divergences, a new dataset released with this work, consisting of English-French sentence-pairs annotated with semantic divergence classes and token-level rationales.~Learning to rank helps detect fine-grained sentence-level divergences more accurately than a strong sentence-level similarity model, while token-level predictions have the potential of further distinguishing between coarse and fine-grained divergences."
"A renewed emphasis must placed sentence fusion context neural abstractive summarization. A majority systems trained end-to-end, abstractive summarizer rewarded generating summaries contain words human abstracts, measured automatic metrics ROUGE. A summarizer, however, rewarded correctly fusing sentences. In fact, examined closely, sentences system abstracts generated fusion. For instance, 6\% summary sentences generated Pointer-Gen fusion, whereas human abstracts contain 32\% fusion sentences. Moreover, sentences generated fusion prone errors. They ungrammatical, nonsensical, otherwise ill-formed. There thus urgent need develop neural abstractive summarizers fuse sentences properly. The importance sentence fusion long recognized community era neural text summarization. The pioneering work Barzilay et al.~\shortcite{barzilay-etal-1999-information} introduces information fusion algorithm combines similar elements across related text generate succinct summary. Later work, as, builds dependency word graph combining syntactic trees similar sentences, employs integer linear programming decode summary sentence graph. Most studies assumed set similar sentences input, fusion necessary reduce repetition. Nonetheless, humans limit combine similar sentences. In paper, pay particular attention fuse disparate sentences contain fundamentally different content remain related make fusion sensible. In Figure, provide example sentence fusion instance. We address challenge fusing disparate sentences enhancing Transformer architecture points correspondence sentences, devices tie two sentences together coherent text. The task sentence fusion involves choosing content sentence weaving content pieces together output sentence linguistically plausible semantically truthful original input. It distinct from~\citet{geva-etal-2019-discofuse} connect two sentences discourse markers. Our contributions follows. We present cascade approach neural abstractive summarization separates content selection surface realization. Importantly, approach makes use text highlights intermediate representation; derived one two sentences using coarse-to-fine content selection strategy, passed neural text generator compose summary sentence. A successful cascade approach expected accurately select sentences highlight appropriate amount text, customized domain-specific tasks.","  The ability to fuse sentences is highly attractive for summarization systems because it is an essential step to produce succinct abstracts. However, to date, summarizers can fail on fusing sentences. They tend to produce few summary sentences by fusion or generate incorrect fusions that lead the summary to fail to retain the original meaning.  In this paper, we explore the ability of Transformers to fuse sentences and propose novel algorithms to enhance their ability to perform sentence fusion by leveraging the knowledge of points of correspondence between sentences. Through extensive experiments, we investigate the effects of different design choices on Transformer's performance. Our findings highlight the importance of modeling points of correspondence between sentences for effective sentence fusion."
"The recent advances neural machine translation provided research community commercial landscape effective translation models times achieve near-human performance. However, usually holds phrase sentence level. When using models larger units text, paragraphs documents, quality translation may drop considerably terms discourse attributes lexical stylistic consistency. In fact, document-level translation still open challenging problem. The sentences make document unrelated pieces text predicted independently; rather, set sequences linked together complex underlying linguistics aspects, also known discourse . The discourse document includes several properties grammatical cohesion , lexical cohesion , document coherence use discourse connectives . Ensuring translation retain linguistic properties expected significantly improve overall readability flow. However, due limitations current decoder technology, NMT models still bound translate sentence level. In order capture discourse properties source document translation, researchers attempted incorporate contextual information surrounding sentences. Most document-level NMT approaches augment model multiple encoders, extra attention layers memory caches encode surrounding sentences, leave model implicitly learn discourse attributes simply minimizing conventional NLL objective. The hope model spontaneously identify retain discourse patterns within source document. Conversely, little work attempted model discourse attributes explicitly. Even evaluation metrics typically used translation BLEU designed assess discourse quality translated documents. For reasons, paper propose training NMT model directly targeting two specific discourse metrics: lexical cohesion coherence . LC measure frequency semantically-similar words co-occurring document . For example, car, vehicle, engine wheels semantically-related terms. There significant empirical evidence ensuring lexical cohesion text eases understanding . At turn, COH measures well adjacent sentences text linked other. In following example Hobbs \shortcite{hobbs1979coherence}: two sentences make little `sense' one another. An incoherent text, even grammatically syntactically perfect, anecdotally difficult understand therefore coherence actively pursued. Relevant translation, Vasconcellos \shortcite{vasconcellos1989cohesion} found high percentage human post-editing changes machine-generated translations involves improvement cohesion coherence. Several LC COH metrics well correlate human judgement proposed literature. However, like BLEU evaluation metrics, discrete, non-differentiable functions model's parameters. Hereafter, propose overcome limitation using well-established policy gradient approach reinforcement learning allows using evaluation metric reward without differentiate it. By combining different types rewards, model trained simultaneously achieve lexically-cohesive coherent document translations, time retaining faithfulness reference translation. %the information contained source document. %The rest paper organized follows. Section discusses related work. Section describes baseline NMT architectures used experiments. Section presents proposed training approach discourse rewards used it. Section presents experiments and, finally, Section concludes paper. We address challenge information fusion context neural abstractive summarization making crucial use points correspondence sentences. We enrich Transformers PoC information report model performance new test bed information fusion. Our findings suggest modeling points correspondence crucial effective sentence fusion, sentence fusion remains challenging direction research. Future work may explore use points correspondence sentence fusion standard setting document summarization. Performing sentence fusion accurately succinctly especially important summarizing long documents book chapters. These domains may contain entities events potentially confuse summarizer, making method explicitly marking entities beneficial.","   Document-level machine translation focuses on the translation of entire documents from a source to a target language. It is widely regarded as a challenging task since the translation of the individual sentences in the document needs to retain aspects of the discourse at document level. However, document-level translation models are usually not trained to explicitly ensure discourse quality. Therefore, in this paper we propose a training approach that explicitly optimizes two established discourse metrics, lexical cohesion  and coherence , by using a reinforcement learning objective. Experiments over four different language pairs and three translation domains have shown that our training approach has been able to achieve more cohesive and coherent document translations than other competitive approaches, yet without compromising the faithfulness to the reference translation. In the case of the Zh-En language pair, our method has achieved an improvement of $2.46$ percentage points  in LC and $1.17$ pp in COH over the runner-up, while at the same time improving $0.63$ pp in BLEU score and $0.47$ pp in $\mathrm{F}_{\mathrm{BERT}}$.      %In fact, in some cases our training approach has even improved translation accuracy metrics such as BLEU and the recently proposed $F_{\text{BERT}}$."
"In recent years, neural models led state-of-the-art results machine translation . Many systems broadly characterized following multi-layer encoder-decoder neural network design: encoder decoder learn representations word sequences stack layers , building interesting line work improving models. The simplest increases model capacity widening network, whereas recent work shows benefits stacking layers encoder side. For example, popular Transformer model , deep systems shown promising BLEU improvements either easing information flow network constraining gradient norm across layers . An improved system even learn 35-layer encoder, deeper vanilla Transformer . Although methods enabled training deep neural MT models, questions remain nature problem. The main question is: deep networks help NMT. Note previous work evaluates systems black-box manner . It thus natural study much deep NMT system able learn different shallow counterpart. Beyond this, training extremely deep model expensive although narrow-and-deep network speed training . For example, takes us longer time train model deepen network 6 layers 48 layers. This might prevent us exploiting deeper models large-scale systems. In paper, explore deep architectures work render learning NMT models effectively. By investigating change hidden states different layers, find new representations learned continually stacking layers top base model. More stacked layers lead stronger model representing sentence. This particularly makes sense deep NMT scenario proven deep models benefit enriched representation . In addition, finding inspires us develop simple yet efficient method train deep NMT encoder: train model parameters shallow deep, rather training entire model scratch. To stabilize training, design sparse linear combination method connecting lower-level layers top. It makes efficient pass information deep network require large memory footprint dense networks. We experiment method state-of-the-art deep Transformer system. Our encoder consists 48-54 layers, almost deepest Transformer model used NMT. On WMT En-De En-Fr tasks, yields speedup training, matching state-of-the-art WMT'16 En-De task. In paper, presented novel training method document-level NMT models uses discourse rewards encourage models generate lexically cohesive coherent translations document level. As training objective used reinforcement learning-style function, named Risk, permits using discrete, non-differentiable terms objective. Our results four different language pairs three translation domains shown models achieved consistent improvement discourse metrics LC COH, retaining comparable values accuracy metrics BLEU . In fact, certain datasets, models even improved metrics. While approach proved effective cases, best combination discourse rewards, accuracy rewards NLL selected validation dataset. In near future plan investigate automate selection, also explore applicability proposed approach natural language generation tasks.","    Deep encoders have been proven to be effective in improving neural machine translation  systems, but training an extremely deep encoder is time consuming. Moreover, why deep models help NMT is an open question. In this paper, we investigate the behavior of a well-tuned deep Transformer system. We find that stacking layers is helpful in improving the representation ability of NMT models and adjacent layers perform similarly. This inspires us to develop a shallow-to-deep training method that learns deep models by stacking shallow models. In this way, we successfully train a Transformer system with a 54-layer encoder. Experimental results on WMT'16 English-German and WMT'14 English-French translation tasks show that it is $1.4$ $\times$ faster than training from scratch, and achieves a BLEU score of $30.33$ and $43.29$ on two tasks. The code is publicly available at \href{https://github.com/libeineu/SDT-Training/}{https://github.com/libeineu/SDT-Training}."
"Task-oriented dialogue systems complete tasks users, making hotel reservation finding train routes, multi-turn conversation . The generated system utterances naturally sound, importantly informative, i.e., proceed dialogue towards task completion. To fulfill requirement, conditioned response generation widely adopted based system actions . The response generation process decoupled two consecutive steps, action first selected utterance generated conditioned action. One optimize step towards goal, i.e., informative naturally sound, without impinging . However, approaches rely action annotations , require domain knowledge extensive efforts obtain. % \end{threeparttable} \end{table} To deal absence action annotations, latent action learning introduced . System utterances represented low-dimensional latent variables auto-encoding task , utterances representations considered convey similar meanings. Such action representations might prone over-dependence training data, restricts model generalization capability, especially multiple domains considered. % This implicit nature latent variables makes unable enforce desired properties latent space, i.e., capture intentions system utterances, without explicit supervision . This because, without explicit supervision, desired property capturing intentions system utterances latent space cannot enforced , turn due implicit nature latent variables. For example, variational auto-encoder , often used latent action learning, tends produce balanced distribution latent variables , true distribution system actions highly imbalanced . The resulting misaligned action representations would confuse model steps degenerate sample efficiency training. % This without explicit supervision desired property capturing intentions system utterances latent space cannot enforced , turn due implicit nature latent variables. To address issues, propose learn natural language actions represent system utterances span words, explicitly reveal underlying intentions. % benefits natural language actions Natural language provides unique compositional structure retaining representation flexibility. These properties promote model generalization thus make natural language xible representation capturing characteristics minimal assumptions . % main rationale obtain actions % In scenarios, aim use language interface Motivated advantages, learn natural language actions identifying salient words system utterances. Salient refers indicative prediction task takes input original utterance. % characteristics utterances. The main rationale principal information task concerns preserved salient words. For example, sentiment sentence ``The movie starts competent turn bland'' revealed word ``bland'' identified salient considering complete context. In scenarios, consider measuring word saliency terms state transitions. This state transitions reflect intentions system utterance influence dialogue progress, action representations capture influences well reveal intentions . By considering salient words state tracking tasks actions, obtain action representations enjoy merits natural language indeed capture characteristics interest, i.e., intentions system utterances. % explainable % technical contributions Obtaining salient words applying existing saliency identification approaches is, however, unable produce unified action representations. Specifically, system utterances intention might share similar wordings, existing attribution approaches identify salient words within utterances. We tackle challenge proposing memory-augmented saliency approach identifies salient words broader vocabulary. The vocabulary consists words could compose natural language actions,~\footnote{We consider content words state annotations task descriptions, specified Sec. } word stored slot memory component. By incorporating memory component dialogue state tracking model, use system utterance query perform memory retrieval, retrieval results considered salient words. The retrieval results might contain words redundant since direct supervision retrieval operations. For example, resulting salient words might ``but turn bland'' example shown earlier, include unnecessary words may lead degenerated action results. To obtain compact action representations, propose auxiliary task based pseudo parallel corpus, i.e., dialogue context state annotation pairs. We observe dialogue states serve good examples compact representation be. Therefore, use encoded dialogue context query ask memory component reconstruct text-based dialogue states. In way, obtained concise actions generalize better easily interpreted. Our contributions summarized follows: We investigated behaviour well-trained deep Transformer models found stacking layers could improve representation ability NMT systems. Higher layers share global information different positions adjacent layers behave similarly. Also, developed shallow-to-deep training strategy employ sparse connections across blocks ease optimization. With help learning rate restart appropriate initialization successfully train 48-layer RPR model progressive stacking achieve speedup WMT'16 English-German WMT'14 English-French tasks. Furthermore, -RPR-24L achieves BLEU score WMT'16 English-German task, speeds training .","   Response generation for task-oriented dialogues implicitly optimizes two objectives at the same time: task completion and language quality. Conditioned response generation serves as an effective approach to separately and better optimize these two objectives.  Such an approach relies on system action annotations which are expensive to obtain. To alleviate the need of action annotations, latent action learning is introduced to map each utterance to a latent representation. However, this approach is prone to over-dependence on the training data, and the generalization capability is thus restricted.   To address this issue, we propose to learn natural language actions that represent utterances as a span of words.  This explicit action representation promotes generalization via the compositional structure of language. It also enables an explainable generation process. Our proposed unsupervised approach learns a memory component to summarize system utterances into a short span of words. To further promote a compact action representation, we propose an auxiliary task that restores state annotations as the summarized dialogue context using the memory component. Our proposed approach outperforms latent action baselines on MultiWOZ, a benchmark multi-domain dataset."
"Consider helping friend prepare dinner unfamiliar house: friend asks clean slice apple appetizer, would approach task? Intuitively, one could reason abstractly: find apple wash apple sink put clean apple cutting board find knife use knife slice apple put slices bowl. Even unfamiliar setting, abstract reasoning help accomplish goal leveraging semantic priors. Priors like locations objects --~apples commonly found kitchen along implements cleaning slicing, object affordances --~a sink useful washing apple unlike refrigerator, pre-conditions --~better wash apple slicing it, rather converse. We hypothesize that, learning solve tasks using abstract language, unconstrained particulars physical world, enables agents complete embodied tasks novel environments leveraging kinds semantic priors exposed abstraction interaction. To test hypothesis, created novel \env framework, first interactive, parallel environment aligns text descriptions commands physically embodied robotic simulation. We build \env extending two prior works: \tw~ - engine interactive text-based games, \alfred~ - large scale dataset vision-language instruction following embodied environments. \env provides two views underlying world two modes interact it: \tw, abstract, text-based environment, generates textual observations world responds high-level text actions; \alfred, embodied simulator, renders world high-dimensional images responds low-level physical actions robot .\footnote{Note: Throughout work, clarity exposition, use \alfred{} refer tasks grounded simulation environment, rendering physics provided \thor{}~.} Unlike prior work instruction following , typically uses static corpus cross-modal expert demonstrations, argue aligned parallel environments like \env offer distinct advantage: allow agents explore, interact, learn abstract environment language encountering complexities embodied environment. While fields robotic control use % simulators like MuJoCo~ provide infinite data interaction, analogous mechanism -- short hiring human around clock -- providing linguistic feedback annotations embodied agent. \tw{} addresses discrepancy providing programmatic aligned linguistic signals agent exploration. This facilitates first work, knowledge, embodied agent learns meaning complex multi-step policies, expressed language, directly interaction. Empowered \env framework, introduce \model , agent first learns perform abstract tasks \tw using Imitation Learning transfers learned policies embodied tasks \alfred. When operating embodied world, \model leverages abstract understanding gained \tw generate text-based actions; serve high-level subgoals facilitate physical action generation low-level controller. Broadly, find \model capable generalizing zero-shot manner \tw unseen embodied tasks settings. Our results show training first abstract text-based environment faster, also yields better performance training scratch embodied world. These results lend credibility hypothesis solving abstract language-based tasks help build priors enable agents generalize unfamiliar embodied environments. Our contributions follows:\\[-15pt] We propose explicit action learning achieve generalizable interpretable dialogue generation. Our proposed model MASP learns unified compact action representations. We propose memory component summarizes system utterances natural language actions, i.e., spans words unified vocabulary. We introduce auxiliary task encourage natural language actions preserve task-relevant information. Experimental results confirm MASP achieves better performance compared state-of-the-art different settings, especially supervision limited. We plan consider structural action representation learning could convey information future work.","  Given a simple request like Put a washed apple in the kitchen fridge, humans can reason in purely abstract terms by imagining action sequences and scoring their likelihood of success, prototypicality, and efficiency, all without moving a muscle.  Once we see the kitchen in question, we can update our abstract plans to fit the scene. Embodied agents require the same abilities, but existing work does not yet provide the infrastructure necessary for both reasoning abstractly and executing concretely.  We address this limitation by introducing \env{}, a simulator that enables agents to learn abstract, text-based policies in \tw and then execute goals from the ALFRED benchmark in a rich visual environment. \env{} enables the creation of a new \model agent whose abstract knowledge, learned in \tw, corresponds directly to concrete, visually grounded actions. In turn, as we demonstrate empirically, this fosters better agent generalization than training only in the visually grounded environment. \model's simple, modular design factors the problem to allow researchers to focus on models for improving every piece of the pipeline ."
"Annual Reports may extend 250 pages long stated above, contains different sections General Corporate Information, financial operating cost, CEOs message, Narrative texts, accounting policies, Financial statement including balance sheet summary financial data documents. In Financial narrative summarisation task, narrative section summarised, explicitly marked dataset, making challenging interesting. In recent years, previous manual small-scale research Accounting Finance literature scaled aid NLP ML methods, example, examine approaches retrieving structured content financial reports, study causes consequences corporate disclosure financial reporting outcomes . \par Companies produce glossy brochures annual reports much looser structure, makes automatic summarisation narratives UK annual reports challenging task . Hence summarize narrative section annual reports, particular narrative sentences spread loosely across document need first identified summarise sentences. The summarisation limit set 1000 words, actual length report may go 250 pages long. Hence summarize long annual reports using combination extractive abstractive summarisation.\par The text summary method classified two paradigms: extractive abstractive. The extractive summarisation method extracts meaningful sentences section text original text combines form summary . Whereas abstractive summarisation generates words sentences similar meaning given text form summary may actual text . When summarizing long documents case 250 pages long, extractive summarisation may produce coherent readable summary, abstractive summarisation cannot cover complete information using encoder-decoder architecture. One problem typical seq2seq frameworks often generate unnatural summaries consisting repeated words phrases . Hence, come combination extractive abstractive summarisation first select important narrative sentences concisely convey them. \par Pointer Networks used various combinatorial optimization problems, Travelling Salesman Problem , Convex hull optimization. We used pointer networks task financial narrative summarization extract relevant narrative sentences particular order logical flow summary. These extracted sentences paraphrased summarise sentences abstractive way using T-5 sequence-to-sequence model. We train complete model optimizing ROUGE-LCS evaluation metric reinforcement learning objective. % % The following footnote without marker nebe fireded camera-ready % version paper. % Comment instructions uncomment 8 lines % ""final paper"" variant English. % . % % % final paper: en-us version % space normally used marker This work licensed Creative Commons Attribution 4.0 International License. License details: \url{http://creativecommons.org/licenses/by/4.0/}. } We introduced \env, first interactive text environment aligned embodied worlds. \env allows agents explore, interact, learn abstract polices textual environment. Pre-training novel \model agent \tw, show zero-shot generalization embodied tasks \alfred dataset. The results indicate reasoning textual space allows better generalization unseen tasks also faster training, compared modalities like vision. \model designed modular components upgraded future work. Examples include template-based state-estimator A* navigator could replaced learned modules, enabling end-to-end training full pipeline. Another avenue future work learn ``textual dynamics models'' environment interactions, akin vision-based world models~. Such models would facilitate construction text-engines new domains, without requiring access symbolic state descriptions like PDDL. Overall, excited challenges posed aligned text embodied environments better cross-modal learning.","   Companies provide annual reports to their shareholders at the end of the financial year that describes their operations and financial conditions. The average length of these reports is 80, and it may extend up to 250 pages long. In this paper, we propose our methodology PoinT-5  algorithms) that we used in the Financial Narrative Summarisation  2020 task. The proposed method uses Pointer networks to extract important narrative sentences from the report, and then T-5 is used to paraphrase extracted sentences into a concise yet informative sentence. We evaluate our method using $\operatorname{ROUGE}$-N , L,and SU4. The proposed method achieves the highest precision scores in all the metrics and highest F1 scores in $\operatorname{ROUGE}$ 1,and LCS and only solution to cross MUSE solution baseline in $\operatorname{ROUGE}$-LCS metrics."
"Neural Architecture Search methods aim automatically discover neural architectures perform well given task dataset. These methods search space possible model architectures, looking ones perform well task generalize unseen data. There substantial prior work define architecture search space, search space, estimate model performance . Recent works, however, cast doubt quality performance NAS-optimized architectures , showing current methods fail find best performing architectures given task perform similarly random architecture search. In work, explore applications SOTA NAS algorithm, ENAS , two sentence-pair tasks, paraphrase detection semantic textual similarity . We conduct large set experiments testing effectiveness ENAS-optimized RNN architectures across multiple models , embeddings datasets . We first, knowledge, apply ENAS PD STS, explore applications across multiple embeddings traditionally LSTM-based NLP models, conduct extensive SOTA HPT across multiple ENAS-RNN architecture candidates. Our experiments suggest baseline LSTM models, appropriate hyperparameter tuning , sometimes match exceed performance models ENAS-RNNs. We also observe random architectures sampled ENAS search space offer strong baseline, sometimes outperform ENAS-RNNs. Given observations, recommend researchers conduct extensive HPT across various candidate architectures fairest comparisons; compare performances ENAS-RNNs standard architectures like LSTMs RNN cells randomly sampled ENAS search space; examine computational requirements ENAS methods alongside gains observed. In work present solution Financial Narrative Summarisation dataset using PoinT-5 method explained . It combination extractive abstractive methods using Pointer Network T-5. With methods able achieve highest precision score every evaluation metric achieve highest F-1 scores ROUGE-LCS ROUGE-1.\par In future work would like address several limitation method factual correctness summaries important financial domain done summarizing radiology reports. To improve precision generated summaries 1000 words would formulate penalty system generates 1000 words training RL algorithm rather restricting algorithm fixed number sentences. include bib file like this:","  Neural Architecture Search  methods, which automatically learn entire neural model or individual neural cell architectures, have recently achieved competitive or state-of-the-art  performance on variety of natural language processing and computer vision tasks, including language modeling, natural language inference, and image classification. In this work, we explore the applicability of a SOTA NAS algorithm, Efficient Neural Architecture Search  \cite{Pham2018EfficientNA} to two sentence pair tasks, paraphrase detection and semantic textual similarity. We use ENAS to perform a micro-level search and learn a task-optimized RNN cell architecture as a drop-in replacement for an LSTM. We explore the effectiveness of ENAS through experiments on three datasets , with two different models , and two sets of embeddings . In contrast to prior work applying ENAS to NLP tasks, our results are mixed -- we find that ENAS architectures sometimes, but not always, outperform LSTMs and perform similarly to random architecture search."
"Constituency parsing well-studied problem natural language processing, state-of-the-art parsers tested written text, e.g.\ standard Penn Treebank Wall Street Journal dataset . These recent neural parsers commonly formulated encoder-decoder systems, encoder learns input sentence representation decoder learns predict parse tree. While input often represented word-level features, representation output trees varies: sequence parse symbols , set spans , syntactic distances , per-word structure-rich labels . A key characteristic many neural parsers recurrent network structure, particularly Long Short-Term Memory networks ; however, Kitaev Klein shown non-recurrent encoder Transformer network introduced also capable encoding timing information self-attention mechanisms, achieving state-of-the-art parse results Treebank WSJ dataset. Further, parsers %seem mainly benefit contextualized information learned larger external text data, ELMo BERT . It clear advances transfer speech data, particularly different styles speech. Even perfect transcripts available, speech poses many challenges parsers learned written text due lack punctuation case, presence disfluencies. On hand, speech signals carry rich information beyond words via variations timing, intonation, loudness, i.e. prosody. Linguistic studies shown prosodic cues align constituent structure , signal disfluencies marking interruption point , help listeners resolve syntactic ambiguities . Empirical evidence, however, mixed regarding utility prosody constituency parsing. Most gains observed sentence boundaries unknown , annotated prosodic labels . Most related current work, Tran et al.\ recently showed benefit using prosody parsing within sequence-to-sequence framework, proposing convolutional neural network mechanism combine discrete word-level features frame-level acoustic-prosodic features. In study, extend work explore utility recent neural advances spontaneous speech data, compare utility prosody read vs.\ spontaneous speech. Specifically, goal current study answer following questions: % TT: may cut space lacking. But I want end intro questions without saying anything %The rest paper organized follows: Section describes models used work; Section reviews datasets metrics constituency parsing; Section presents experiments, results, analyses; Section summarizes findings. %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % Moved data table since oddly arranged %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% \end{table*}\documentclass[a4paper]{article} \usepackage{INTERSPEECH2019} \usepackage{url} \usepackage{multirow} \usepackage{xcolor} \usepackage{subcaption,enumitem} \usepackage{booktabs} \usepackage{comment} \newcommand{\ttcomment}[1]{\textcolor{red}{\bf \small [#1 --TT]}} \newcommand{\jycomment}[1]{\textcolor{blue}{\bf \small [#1 --JY]}} \newcommand{\ylcomment}[1]{\textcolor{cyan}{\bf \small [#1 --YL]}} \newcommand{\mocomment}[1]{\textcolor{green}{\bf \small [#1 --MO]}} \title{On Role Style Parsing Speech Neural Models} \name{Trang Tran, Jiahong Yuan, Yang Liu, Mari Ostendorf} %The maximum number authors author list twenty. If number contributing authors twenty, listed footnote acknowledgement section, appropriate. \address{ Electrical \& Computer Engineering, University Washington\\ LAIX Inc.} \email{\{ttmt001,ostendor\}@uw.edu, \{jiahong.yuan,yang.liu\}@liulishuo.com} Index Terms: constituency parsing, prosody, spontaneous speech, contextualized embeddings %Index Terms: constituency parsing, prosody, spontaneous speech, read speech, switchboard, ELMo, BERT, contextualized embeddings %\ttcomment{take these?} \bibliographystyle{IEEEtran} \bibliography{interspeech19} \end{document} Unlike prior work applying ENAS NLP, find ENAS-RNNs outperform LSTMs random search dataset, embedding, model) configurations. Our findings parallel recent work question effectiveness current NAS methods superiority random architecture search SOTA HPT methods. Given mixed results, recommend researchers: extensively tune hyperparameters standard randomly sampled architectures create strong baselines; benchmark ENAS performance across multiple simple complex model architectures ; present computational requirements alongside gains observed ENAS methods."," The differences in written text and conversational speech are substantial; previous parsers trained on treebanked text have given very poor results on spontaneous speech. For spoken language, the mismatch in style also extends to prosodic cues, though it is less well understood.  This paper re-examines the use of written text in parsing speech in the context of recent advances in neural language processing. We show that neural approaches facilitate using written text to improve  parsing of spontaneous speech, and that prosody further improves over this state-of-the-art result. Further, we find an asymmetric degradation from read vs.\ spontaneous mismatch, with spontaneous speech more generally useful for training parsers.  %  Prosodic information in the speech signal has been shown to correlate with syntactic structure of a sentence; however, the impact of prosody on parsing has been mixed. Recent results show a benefit for conversational speech, particularly in utterances with disfluencies, but there is little recent work on other speaking styles. In this work, we extend recent advances in constituency parsing of spontaneous speech, integrating acoustic-prosodic cues and achieving SOTA results on the Switchboard dataset. We then explore the performance of the parser on mismatched training/testing scenarios. Specifically, we show that training on spontaneous speech results in a small degradation when testing on read speech, while fine-tuning with WSJ read speech substantially degrades the performance on spontaneous speech."
"The recent progress machine translation models led researchers question use n-gram overlap metrics BLEU, focus solely surface-level aspects generated text, thus may correlate poorly human evaluation. This led surge interest flexible metrics use machine learning capture semantic-level information. Popular examples metrics include YiSi-1, ESIM, BERTscore, Sentence Mover's Similarity, \BLEURT{}. These metrics utilize contextual embeddings large models BERT shown capture linguistic information beyond surface-level aspects. The WMT Metrics 2020 Shared Task reference benchmark evaluating metrics context machine translation. It tests evaluation systems to-English languages , requires multilingual approach. An additional challenge learned metrics human ratings available language pairs, therefore, models must use unlabeled data perform zero-shot generalization. We describe several learned metrics based \BLEURT{}~, originally developed English data. We first extend \BLEURT{} multilingual setup, show approach achieves competitive results WMT Metrics 2019 Shared Task.\footnote{We use following languages fine-tuning and/or testing: Chinese, Czech, German, English, Estonian, Finnish, French, Gujarati, Kazakh, Lithuanian, Russian, Turkish. In addition, also pre-train Inuktitut, Japanese, Khmer, Pastho, Polish, Romanian, Tamil.} We also present several simple BERT-based baselines, submit analysis. Finally, focus English German enhance \BLEURT{}'s performance combining predictions YiSi well using alternative references. We show neural architectures, particular contextualized embeddings pretrained large written text , improve constituency parsing conversational speech transcripts. The use prosody results improvements overall, especially longer sentences reducing attachment errors. Assessing utility prosody different speaking styles, found parsers trained spontaneous prosody consistently useful, improving text-only counterparts testing conversational read speech. Fine-tuning parsers read speech improves results testing read style, degrades significantly spontaneous speech. This suggests conversational speech data useful general parser training."," The quality of machine translation systems has dramatically improved over the last decade, and as a result, evaluation has become an increasingly challenging problem. This paper describes our contribution to the WMT 2020 Metrics Shared Task, the main benchmark for automatic evaluation of translation. We make several submissions based on \BLEURT{}, a previously published metric which uses transfer learning. We extend the metric beyond English and evaluate it on 14 language pairs for which fine-tuning data is available, as well as 4 ``zero-shot'' language pairs, for which we have no labelled examples. Additionally, we focus on English to German and demonstrate how to combine \BLEURT{}'s predictions with those of YiSi and use alternative reference translations to enhance the performance. Empirical results show that the models achieve competitive results on the WMT Metrics 2019 Shared Task, indicating their promise for the 2020 edition."
"Although neural machine translation achieved great progress recent years , fed entire document, standard NMT systems translate sentences isolation without considering cross-sentence dependencies. Consequently, document-level neural machine translation methods proposed utilize source-side target-side inter-sentence contextual information improve translation quality sentences document . More recently, researchers DocNMT mainly focus exploring various attention-based networks leverage cross-sentence context efficiently, evaluate special discourse phenomena . However, still issue received less attention: context sentences used translating source sentence? \end{center} \end{table} We conduct experiment verify intuition: translation different source sentences requires different context. As shown Table , train two DocNMT models test using various context settings\footnote{We apply typical DocNMT method train models ZhEn TED, select 1,000 sentences test. The BLEU sentence-level baseline 20.06.}. During test, obtain dynamic context sentences achieve best BLEU scores traversing context combinations source sentence. Compared fixed size context , dynamic context significantly improve translation quality. Although row 2 uses context, redundant information may hurt results. Experiments indicate limited context sentences really useful, change source sentences. Majority existing DocNMT models set context size scope fixed. They utilize previous context sentences , full context entire document . As result, inadequacy redundancy contextual information almost inevitable. From viewpoint, \citet{maruf2019selective} propose selective attention approach uses sparsemax function instead softmax normalize attention weights. The sparsemax assigns low probability softmax zero model focus sentences high probability. However, learning attention weights lacks guidance, cannot handle situation source sentences achieve best translation results without relying context, happens 39.4\% sentences experiment. To address problem, propose effective approach select contextual sentences {\bf dynamically} source sentence document-level translation. Specifically, propose Context Scorer score candidate context sentence according currently translated source sentence. Then, utilize two selection strategies select useful context sentences translation module. The size selected context variable different sentences. A core challenge approach selection process non-differentiable. Therefore, leverage reinforcement learning method train selection DocNMT modules together. We design novel reward encourage model aware different context sentences select appropriate context improve translation quality. In paper, make following contributions: We provide empirical evidence ability self-attention networks learn generalized languages. We compare performance two SA networks, SA SA, differ inclusion starting symbol vocabulary. We demonstrate simple addition starting symbol helps SA generalize sequences longer higher depths. The competitive performance SA LSTMs might seem surprising, considering recognition languages inherently hierarchical task. From experiments, conclude recognizing Dyck languages tied recursion, rather learning right representations look head token. Further, find representations learned SA highly interpretable network performs computations similar stack automaton. Our results suggest formal languages could interesting avenue explore interplay performance interpretability SA. Comparisons SA LSTM reveal interesting contrast two architectures calls investigation. Recent work shows express Transformer RNN linearization attention mechanism, could lay grounds theoretical analysis neural architectures \setcounter{section}{0} \import{}{supp_arxiv}"," 		Document-level neural machine translation has yielded attractive improvements. However, majority of existing methods roughly use all context sentences in a fixed scope. They neglect the fact that different source sentences need different sizes of context. To address this problem, we propose an effective approach to select dynamic context so that the document-level translation model can utilize the more useful selected context sentences to produce better translations. Specifically, we introduce a selection module that is independent of the translation module to score each candidate context sentence. Then, we propose two strategies to explicitly select a variable number of context sentences and feed them into the translation module. We train the two modules end-to-end via reinforcement learning. A novel reward is proposed to encourage the selection and utilization of dynamic context sentences. Experiments demonstrate that our approach can select adaptive context sentences for different source sentences, and significantly improves the performance of document-level translation methods."
"\vsec Automatic text summarization\footnote{We refer abstractive summarization paper.} attractive technique helping humans grasp content documents effortlessly. While supervised neural methods shown good performances, unsupervised approach starting attract interest due advantage requiring costly parallel corpora. However, empirical performance unsupervised methods currently behind state-of-the-art supervised models. Unsupervised text summarization still developing stage various solutions actively explored. One previous unsupervised approach extends neural encoder-decoder modeling zero paired data scenario, model trained paradigm called compression-reconstruction learning. The mechanism similar back-translation: model consists compressor reconstructor, co-trained reconstructor recover original sentence summary generated compressor~. Experimental results showed unsupervised encoder-decoder-based summarizer able learn mapping sentence summary without paired data. % Also, \citealp{zhou-rush-2019-simple} proposes straightforward method mimics reconstruction part means contextual similarity original input sentence top generating summary. % However, performance unsupervised methods still deficient compared latest supervised models. Reinforcement learning also potential solution paired data situation. In related fields, example, unsupervised methods text simplification text compression policy-gradient learning. Recent RL techniques take value-based approach DQN combination policy value-based approaches Asynchronous Advantage Actor-Critic. A critical requirement leverage value-based method value function represents goodness action given state. We naturally define value function utilizing CR-learning paradigm, makes latest value-based approaches available unsupervised text summarization. % , require define value-function. % We leverage values-based approach % A crucial requirement RL value function represents goodness action given state. % We satisfy requirement leveraging definition CR learning paradigm. % One concern is, however, RL large action space generally difficulty training. % In addition, latest techniques improve RL value-based approach DQN combination policy-based value-based approaches Asynchronous Advantage Actor-Critic. In paper, propose new method based Q-learning edit-based summarization~. The edit-based summarization generates summary operating edit action word input sentence. Our method implements editing process two modules: 1) {\bf E}ditorial {\bf A}gent predicts edit actions, 2) {\bf L}anguage {\bf M}model converter deterministically decodes sentence basis action signals, call \ealm. The CR learning defined Q-learning framework train agent predict edit actions instruct LM converter produce good summary. Although vast action space causing sparsity reward, word generation encoder-decoder model, generally difficult learned RL, method mitigates issue thanks fewer edit actions deterministic decoding language model. Moreover, formulation Q-learning enables us incorporate latest techniques RL. The main contribution paper provide new solution form unsupervised edit-based summarization leveraging Q-learning language model. Experimental results show method achieved competitive performance encoder-decoder-based methods even truly paired data , qualitative analysis brings insights current unsupervised models missing. Also, problem formulation Q-learning enables us import latest techniques RL, leads potential improvements future research. % 2) We propose first Q-learning-based method uses pre-trained language model. % , mitigates issues prevalent among previous methods. % Empirically, method shows competitive performance news corpus benchmarks truly paired data . % Also, method requires parallel data even validation; therefore, instantly applicable situation language model. % Our proposed approach brings new insights growing field unsupervised text summarization, pave way future development. % This paper organized follows: Section defines problem statement unsupervised text summarization \algoname\ paradigm. % After reviewing previous methods Section, introduce approach Section . % Then, report experimental results Section . % Discussing insights experiment Section , conclude contribution paper future unsupervised text summarization Section . % Text Summarization task transform input sentence informative summary . % Although supervised summarization models like encoder-decoder shown success years , still issue demand us create massive parallel data. % The question ``how model transformation input sentences?"" attracts research interests, known unsupervised text summarization . % In unsupervised text summarization, input sentences available training model. % Instead, holds hypothesis: summary contain information input sentence extent guess original contents. % And, lgoname approach leverage hypothesis . % In \algoname, prepare two modules, one compression produces summary input sentence, one reconstruction re-produces input sentence generated summary. % These two modules optimized based hypothesis, specifically, minimizing difference input sentence reconstructed sentence compressed sentence satisfying essential properties shortness readability . % In previous studies, use generative models encoder-decoder compression reconstruction, directly train output desired sentences . % We illustrate flow left-hand side Figure . % Our proposed method also top paradigm uses different modules, {\bf Q-learning agent} {\bf fixed-language model} .\footnote{A pretrained language model fine-tuned, i.e., fixed, training.} % As illustrated right-hand side Figure , agent determines action, whether remove, keep, replace word input sentence. % Receiving action signals, fixed-LM deterministically produces compressed reconstructed sentences. % In short, train agent properly control fixed-LM obtain desired sentences results compression reconstruction. % The primary contribution paper provide new option leveraging Q-learning language model growing field unsupervised text summarization. % Introducing Q-learning, open problem sophisticated techniques value-based Reinforcement Learning algorithms , covered policy-based RL algorithms employed far.\footnote{RL algorithms classified value-based policy-based . To best knowledge, text summarization methods RL, supervised unsupervised settings, leverages policy-based RL algorithms . Combining previous policy-based value-based methods sentence compression lead applicability advanced RL algorithms Actor-Critic Asynchronous Advantage Actor-Critic .} % Also, proposing approach fixedly utilize pre-trained language model, benefit powerful performance capturing sentence semantics along mitigating issues generative models inherently hold complexity co-training multiple generators repetition decoding. % Experimentally, approach shows promising results; achieves competitive performance standard datasets outperforms previous generator models out-of-domain circumstances. % This paper brings novel insights unsupervised text summarization contributes flourishing future. % This paper organized follows: Section defines problem statement unsupervised text summarization \algoname\ paradigm. % After reviewing previous methods Section, introduce approach Section . % Then, report experimental results Section . % Discussing insights experiment Section , conclude contribution paper future unsupervised text summarization Section . \vsecu We propose dynamic selection method choose variable sizes context sentences document-level translation. The candidate context sentences scored selected two proposed strategies. We train whole model via reinforcement learning, design novel reward encourage selection useful context sentences. When applied existing DocNMT models, approach improve translation quality significantly. In future, select context sentences larger candidate space, explore effective ways extend approach select target-side context sentences."," % Unsupervised methods for abstractive text summarization are attractive because they do not require parallel corpora. % However, their performance is still somehow lacking, therefore research on promising solutions is ongoing. % In this paper, we propose a new approach based on Q-learning with an edit-based summarization. % Our method combines two key modules to form an {\bf E}ditorial {\bf A}gent and {\bf L}anguage {\bf M}odel converter~. % The agent predicts edit actions, and then the LM converter deterministically generates a summary on the basis of the action signals. % Q-learning is leveraged to train the agent to output proper edit actions. % Experimental results show that \ealm~has a competitive performance compared with the previous encoder-decoder-based methods, even with truly zero paired data .  % Defining the task as Q-learning enables us not only to develop a competitive method but also to make the latest techniques in reinforcement learning available for unsupervised summarization. % We also conduct qualitative analysis and provide insights on future work for the current unsupervised summarizers.\footnote{Our codes are available at \url{https://github.com/kohilin/ealm}} Unsupervised methods are promising for abstractive textsummarization in that the parallel corpora is not required.  However, their performance is still far from being satisfied, therefore research on promising solutions is on-going.   In this paper, we propose a new approach based on Q-learning with an edit-based summarization.  The method combines two key modules to form an Editorial Agent and Language Model converter .  The agent predicts edit actions , and then the LM converter deterministically generates a summary on the basis of the action signals.  Q-learning is leveraged to train the agent to produce proper edit actions.  Experimental results show that \ealm~delivered competitive performance compared with the previous encoder-decoder-based methods, even with truly zero paired data . Defining the task as Q-learning enables us not only to develop a competitive method but also to make the latest techniques in reinforcement learning available for unsupervised summarization. We also conduct qualitative analysis, providing insights into future study on unsupervised summarizers.\footnote{Our codes are available at \url{https://github.com/kohilin/ealm}}"
"Neural machine translation systems data driven models, highly depend training corpus. NMT models tendency towards over-fitting frequent observations neglecting low-frequency observations. Unfortunately, exists token imbalance phenomenon natural languages different tokens appear different frequencies, roughly obey Zipf's Law. Table shows serious imbalance high-frequency tokens low-frequency tokens. NMT models rarely opportunity learn generate ground-truth low-frequency tokens training process. %It harder NMT model generate ground-truth low-frequency tokens even training process. %Compared reference, NMT model tends generate high-frequency tokens less low-frequency tokens, hurts translation quality. Some work tries improve rare word translation maintaining phrase tables back-off vocabulary adding extra components, bring extra training complexity computing expense. Some NMT techniques based smaller translation granularity alleviate issue, hybrid word-character-based model, BPE-based model word-piece-based model. %For example, sub-word model adapted byte pair encoding technique task word segmentation. These effective work alleviate token imbalance phenomenon certain extent become de-facto standard NMT models. Although sub-word based NMT models achieved significant improvements, still face token-level frequency imbalance phenomenon, Table shows. %It obvious always low-frequency tokens matter number merge operations BPE is. %As shown Table, rare word 'slower' split two tokens 'slow' 'er', still exist obvious token-level imbalance 'slow' tokens. \iffalse \end{table} \fi \iffalse \fi \iffalse \end{table} \fi Furthermore, current NMT models generally assign equal training weights target tokens without considering frequencies. It likely NMT models ignore loss produced low-frequency tokens small proportion training sets. The parameters related adequately trained, will, turn, make NMT models tend prioritize output fluency translation adequacy, ignore generation low-frequency tokens decoding, illustrated Table. It shows vanilla NMT model tends generate high-frequency tokens less low-frequency tokens. %This will, turn, make model %tend generate many high-frequency tokens less low-frequency tokens decoding. However, low-frequency tokens may carry critical semantic information may affect translation quality neglected. %It likely NMT models ignore loss produced rare words patterns learned encoder, decoder, attention modules can't adequately updated. What's more, NMT models tend prioritize output fluency translation adequacy ignore translation rare words generation. %In experiments, observed vanilla NMT models usually produce frequent words less rare words real references. Therefore, techniques adopted improve translation rare words. %distribution. %It obvious always rare tokens matter number merge operations BPE problem token distribution imbalance still exists. %One advantages technique reduces number rare words splitting frequent subword tokens , fact %relieve imbalance word %The strength NMT models make use large amounts parallel training sentences learn knowledge features embodied training data. However, one weaknesses NMT models tendency towards over-fitting frequent observations , neglecting rare cases frequently observed. Unfortunately, natural word distribution imbalance corpus. According Zipf's Law, frequency word inversely proportional ranking frequency table, indicates occurrences words far others naturally. %For word-level NMT models, NMT limitation handling larger vocabulary training complexity computing expense. % %In work, first represent word sequence characters iteratively combine frequent pair new symbol. %which achieved better accuracy translation rare words %, seek alleviate token imbalance problem based analysis. For purpose, To address issue, proposed token-level adaptive training objectives based target token frequencies. We aimed meaningful relatively low-frequency tokens could assigned larger loss weights training model learn them. %In objectives, relatively low-frequency valuable tokens assigned larger loss weights training encourage model learn them. To explore suitable adaptive objectives NMT, first applied existing adaptive objectives tasks NMT analyzed performance. We found though could bring modest improvement translation low-frequency tokens, much damage translation high-frequency tokens, led obvious degradation overall performance. This implies objective ensure training high-frequency tokens first. %training high-frequency tokens ensured first. %We ensure training high-frequency tokens enlarge weights low-frequency tokens time. %We firstly tried focal loss, proposed solving token imbalance problem CV task, analyzed performance. Then, based observations, proposed two heuristic criteria designing token-level adaptive objectives based target token frequencies. Last, presented two specific forms different application scenarios according criteria. Our method yields consistent improvements translation quality ZH-EN, EN-RO, EN-DE translation tasks, especially sentences contain low-frequency tokens get 1.68, 1.02, 0.52 BLEU increases compared baseline, respectively. Further analyses show method also improve lexical diversity translation. %We carried experiments ZHEN, ENRO, ENDE translation tasks validate methods. The experimental results show methods achieve significant improvement translation quality, especially sentences contain low-frequency tokens. %Besides, token distribution translations becomes closer references test sets. %Besides, method also improves diversity translations. Our contributions summarized follows: %More specifically, NMT models first trained equal weights fine-tuned well-defined weights introduced scoring functions. In way, hurt translation frequent tokens, also improve translation rare tokens certain degree. To best knowledge, first work trying concern training weights token level solve distribution imbalance problem NMT. The experiments multiple translation tasks show method improve overall translation performance without almost additional computing storage expense. And analysis experiments indicate method improve rare tokens translation significantly tokens distribution translation much closer references baseline translations. \vsec We brought Q-learning framework unsupervised text summarization proposed new method \ealm~that edit-based unsupervised summarizer leveraging Q-learning agent language model. The experments showed \ealm~performed competitively previous encoder-decoder-based methods. However, qualitative analysis, found quality generated summaries unsupervised model sufficient, individual limitations model. These issue must overcome step forward generating practically available summaries without paired data. In particular \ealm, room improvement importing latest techniques RL. Our work paves way research bridging Q-learning unsupervised text summarization."," There exists a token imbalance phenomenon in natural language as different tokens appear with different frequencies, which leads to different learning difficulties for tokens in Neural Machine Translation .  The vanilla NMT model usually adopts trivial equal-weighted objectives for target tokens with different frequencies and tends to generate more high-frequency tokens and less low-frequency tokens compared with the golden token distribution. %%% However, low-frequency tokens may carry critical semantic information that will affect the translation quality once they are neglected.   In this paper, we explored target token-level adaptive objectives based on token frequencies to assign appropriate weights for each target token during training.  We aimed that those meaningful but relatively low-frequency words could be assigned with larger weights in objectives to encourage the model to pay more attention to these tokens. %More specifically, those relatively low-frequency but valuable target tokens will be assigned with larger weights in objectives to encourage the model to pay more attention to these tokens. %%% %We conducted experiments  Our method yields consistent improvements in translation quality on ZH-EN, EN-RO, and EN-DE translation tasks, especially on sentences that contain more low-frequency tokens where we can get 1.68, 1.02, and 0.52 BLEU increases compared with baseline, respectively. Further analyses show that our method can also improve the lexical diversity of translation. %Experiments on multiple translation tasks show that our methods can achieve significant improvement in translation quality, especially on sentences that contain more low-frequency tokens.  %Besides, our method also improves translation diversity. %Besides, the token distribution of our translations becomes closer to the reference of test sets.  %.  %Rare words translation has always been one of the key challenges to Neural Machine Translation ."
"Graph structures play pivotal role NLP able capture particularly rich structural information. For example, Figure shows directed, labeled Abstract Meaning Representation graph, node denotes semantic concept edge denotes relation concepts. Within realm work AMR, focus paper problem AMR-to-text generation, i.e. transducing AMR graphs text conveys information AMR structure. A key challenge task efficiently learn useful representations AMR graphs. Early efforts neglect significant part structural information input graph linearizing it. Recently, Graph Neural Networks explored better encode structural information task . % \tzy{papers 2018??? Gated Graph Neural networks??? Do miss important paper.} One type GNNs Graph Convolutional Networks . GCNs follow local information aggregation scheme, iteratively updating representations nodes based immediate neighbors. Intuitively, stacking convolutional layers GCNs helps capture complex interactions . However, prior efforts shown locality property existing GCNs precludes efficient non-local information propagation. \citet{AbuElHaija2019MixHopHG} proved vanilla GCNs unable capture feature differences among neighbors different orders matter many layers stacked. Therefore, Self-Attention Networks explored alternative capture global dependencies. As shown Figure , SANs associate node nodes model interactions two nodes graph. Still, approach ignores structure original graph. \citet{Zhu2019ModelingGS} \citet{Cai2019GraphTF} propose structured SANs incorporate additional neural components encode structural information input graph. Convolutional operations, however, computationally efficient self-attention operations computation attention weights scales quadratically convolutions scale linearly respect input length . Therefore, worthwhile explore possibility models based graph convolutions. One potential approach considered incorporate information higher order neighbors, helps facilitate non-local information aggregation node classification . However, simple concatenation different order representations may able model complex interactions semantics text generation . We propose better integrate high-order information, introducing novel dynamic fusion mechanism propose Lightweight, Dynamic Graph Convolutional Networks . As shown Figure , nodes LDGCN model able integrate information first third-order neighbors. With help dynamic mechanism, LDGCNs effectively synthesize information different orders model complex interactions AMR graph text generation. Also, LDGCNs require additional computational overhead, contrast vanilla GCN models. We develop two novel weight sharing strategies based group graph convolutions weight tied convolutions. These strategies allow LDGCN model reduce memory usage model complexity. Experiments AMR-to-text generation show LDGCNs outperform best reported GCNs SANs trained LDC2015E86 LDC2017T10 significantly fewer parameters. On large-scale semi-supervised setting, model also consistently better others, showing effectiveness model large training set. We release code pretrained models \url{https://github.com/yanzhang92/LDGCNs}.\footnote{Our implementation based MXNET Sockeye toolkit .} In work, focus token imbalance problem NMT. We show output vanilla NMT contains high-frequency tokens lower lexical diversity. vanilla NMT model tends generate high-frequency words true distribution due to. token imbalance phenomenon natural language vanilla NMT model tends generate high-frequency words true distribution. less low-frequency words This output bias affect translation quality since low-frequency tokens may carry critical semantic information. To alleviate problem, investigated existing adaptive objectives tasks proposed two heuristic criteria based observations. Next, gave two simple effective forms based criteria, assign appropriate training weights target tokens. propose token-level adaptive objectives based token frequencies, aiming assign appropriate training weights target tokens. To achieve this, propose three heuristic criteria put forward two simple effective forms based criteria. The final results show methods achieve significant improvement performance, especially sentences contain low-frequency tokens. Further analyses show method also improve lexical diversity."," 	 	% Camera-Ready 	AMR-to-text generation is used to transduce Abstract Meaning Representation structures  into text. A key challenge in this task is to efficiently learn effective graph representations. Previously, Graph Convolution Networks  were used to encode input AMRs, however, vanilla GCNs are not able to capture non-local information and additionally, they follow a local  information aggregation scheme. To account for these issues, larger and deeper GCN models are required to capture more complex interactions. In this paper, we introduce a dynamic fusion mechanism, proposing Lightweight Dynamic Graph Convolutional Networks  that capture richer non-local interactions by synthesizing higher order information from the input graphs. We further develop two novel parameter saving strategies based on the group graph convolutions and weight tied convolutions to reduce memory usage and model complexity. With the help of these strategies, we are able to train a model with fewer parameters while maintaining the model capacity. Experiments demonstrate that LDGCNs outperform state-of-the-art models on two benchmark datasets for AMR-to-text generation with significantly fewer parameters."
"Neural machine translation achieved promising results use various optimization tricks. In spite that, techniques lead increased training time massive hyper-parameters, making development well-performed system expensive. As alternative mitigation, curriculum learning~\citep[CL,][]{elman1993learning,bengio2009curriculum} shown effectiveness speeding convergence stabilizing NMT model training. CL teaches NMT model easy examples complex ones rather equally considering samples, keys lie definition ``difficulty'' strategy curricula design. Existing studies artificially determine data difficulty according prior linguistic knowledge sentence length word rarity , manually tune learning schedule. However, neither exists clear distinction easy hard examples, human intuitions exactly conform effective model training. Instead, resolve problem introducing self-paced learning, emphasis learning dynamically determined model rather human intuitions. Specifically, model measures level confidence training example, easy sample actually one high confidence current trained model. Then, confidence score served factor weight loss corresponding example. In way, training process dynamically guided model itself, refraining human predefined patterns. We evaluate proposed method IWSLT15 EnVi, WMT14 EnDe, well WMT17 ZhEn translation tasks. Experimental results reveal approach consistently yields better translation quality faster convergence speed Transformer baseline recent models exploit CL. Quantitative analyses confirm intuitive curriculum schedule human fully cope model learning. In paper, presented Multichannel Generative Language Model . MGLM generative joint distribution model marginalizes possible factorizations within across channels. MGLM endows flexible inference, including unconditional, conditional, partially observed generation. We experimented inference modes using Multi30K dataset containing English, French, Czech, German. We provide qualitative samples sampled unconditionally generative joint distribution. We also quantitatively analyze quality-diversity trade-offs find MGLM outperform traditional bilingual discriminative models. Our work focused specific instantiation channels languages. However, MGLM limited languages generalize notions channels. In future work, consider textual channels, paraphrases, premises hypotheses, questions answers, multimodal channels, images. Another direction investigate scaling MGLM dozens/hundreds channels. Fully generative models still often lag behind purely discriminative counterparts performance, hope work motivates future research building generative joint distribution models world. \color{black} File emnlp2020.tex Based style files ACL 2020, Based style files ACL 2018, NAACL 2018/19, Based style files ACL-2015, improvements taken NAACL-2016 style Based style files ACL-2014, were, turn, based ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009, EACL-2009, IJCNLP-2008... Based style files EACL 2006 e.agirre@ehu.es Sergi.Balari@uab.es ACL 08 Joakim Nivre Noah Smith \documentclass[11pt,a4paper]{article} \usepackage[hyperref]{emnlp2020} \usepackage{times} \usepackage{latexsym} \renewcommand{\UrlFont}hchan@cs.toronto.edu This strictly necessary, may commented out, improve layout manuscript, typically save space. \usepackage{microtype} \usepackage{times} \usepackage{latexsym} \usepackage{url} \usepackage{times} \usepackage{latexsym} \usepackage{amsmath} \usepackage{amssymb} \usepackage{amsfonts} \usepackage{booktabs} \usepackage{enumitem} \usepackage{graphicx} \usepackage{hyperref} \usepackage{url} \usepackage{tikz} \usepackage{xcolor} \usepackage{pifont} \usepackage{placeins} \usepackage[english, german, czech, french]{babel} \usepackage[utf8x]{inputenc} \usepackage{todonotes} \usepackage{wrapfig} \usepackage{natbib} \usepackage{subcaption} \usepackage{lipsum} dummy text \usepackage{dblfloatfix} To enable figures bottom page \usepackage{float} \setlength\titlebox{5cm} You expand titlebox need extra space show authors. Please make titlebox smaller 5cm ; check camera-ready version ask change back. \newcommand\BibTeX{B\TeX} \newcommand{\blk}[1]{{#1}} \newcommand{\blu}[1]{{#1}} \newcommand{\ul}[1]{\underline{#1}} \newcommand{\gray}[1]{{\color{gray}#1}} \newcommand{\black}[1]{{\color{black}#1}} \newcommand{\xv}{\mathbf{x}} \newcommand{\yv}{\mathbf{y}} \newcommand{\wv}{\mathbf{w}} \newcommand{\ourmodel}{Multichannel Generative Language Model} \newcommand{\ourtask}{multichannel generative language modeling} \newcommand{\modelabbv}{MGLM} ULM MCM \DeclareMathOperator*{\argmax}{argmax} Optional math commands https://github.com/goodfeli/dlbook_notation. \aclfinalcopy Uncomment line final submission \def\aclpaperid{3148} Enter acl Paper ID \setlength\titlebox{5cm} You expand titlebox need extra space show authors. Please make titlebox smaller 5cm ; check camera-ready version ask change back. \newcommand\BibTeX{B\TeX} \title{Multichannel Generative Language Model: \\ Learning All Possible Factorizations Within Across Channels} \author{Harris Chan\thanks{\;Work done internship Google Brain.} \\ Vector Institute \\ University Toronto \\ kiros@google.com \\\AND Jamie Kiros \\ Google Research, Brain Team \\ williamchan@google.com \\\And William Chan \\ Google Research, Brain Team \\ {\tt williamchan@google.com}} \date{} \begin{document} \maketitle"," Recent studies have proven that the training of neural machine translation  can be facilitated by mimicking the learning process of humans. Nevertheless, achievements of such kind of curriculum learning rely on the quality of artificial schedule drawn up with the hand-crafted features, e.g. sentence length or word rarity. We ameliorate this procedure with a more flexible manner by proposing self-paced learning, where NMT model is allowed to 1) automatically quantify the learning confidence over training examples; and 2) flexibly govern its learning via regulating the loss in each iteration step.  Experimental results over multiple translation tasks demonstrate that the proposed model yields better performance than strong baselines and those models trained with human-designed curricula on both translation quality and convergence speed.\footnote{Our codes:  \href{https://github.com/NLP2CT/SPL_for_NMT}{https://github.com/NLP2CT/SPL\_for\_NMT}.}"
"In recent years, cyberbullying become one pressing online risks among youth raised serious concerns society. Cyberbullying commonly defined electronic transmission insulting embarrassing comments, photos videos, illustrated Figure~ . Harmful bullying behavior include posting rumors, threats, pejorative labels, sexual remarks. Research American Psychological Association White House revealed young people US indicate bullied social media platforms~. Such growing prevalence cyberbullying social media detrimental societal effects, victims may experience lower self-esteem, increased suicidal ideation, variety negative emotional responses~. Therefore, become critically important able detect prevent cyberbullying social media. Research computer science aimed identifying, predicting, ultimately preventing cyberbullying better understanding nature key characteristics online cyberbullying. In literature, existing efforts toward automatically detecting cyberbullying primarily focused textual analysis user comments, including keywords~ sentiments analysis ~. These studies attempt build generic binary classifier taking high-dimensional text features input make predictions accordingly. Despite satisfactory detection performance practice, models largely overlooked temporal information cyberbullying behaviors. They also ignore user interactions social networks. Furthermore, majority methods focus detecting cyberbullying sessions effectively cannot explain ``why'' media session detected cyberbullying. Given sequence comments user attributes, think sequential learning allow us better exploit model evolution correlations among individual comments. Besides, graph-based learning enable us represent learn users interact session. This work aims detect cyberbullying jointly exploring explainable information user comments social media. To end, build explainable cyberbullying detection framework, \underline{HE}terogeneous \underline{N}eural \underline{I}nteraction \underline{N}etworks , coherent process. HENIN consists three main components learn various interactions among heterogeneous information displayed social media sessions. A comment encoder created learn representations user comments hierarchical self-attention neural network semantic syntactic cues cyberbullying captured. We create post-comment co-attention mechanism learn interactions posted text comments. Moreover, two graph convolutional networks leveraged learn latent representations depicting sessions interact one another terms users, posts correlated terms words. Specifically, address several challenges work: perform explainable cyberbullying detection boost detection performance, highlight explainable comments without ground truth, model correlation posted text user comments, model interactions sessions terms users, interactions textual posts terms words. Our solutions challenges result novel framework HENIN. Our contributions summarized follows. % In paper, propose novel self-paced learning model NMT learning schedule determined model rather intuitively predefined humans. Experimental results three translation tasks verify universal effectiveness approach. Quantitative analyses confirm exploiting self-paced strategy presents flexible way facilitate model convergence CL counterparts. It interesting combine techniques improve NMT. Besides, idea limited machine translation, also interesting validate model NLP tasks, low-resource NMT model training neural architecture search."," In the computational detection of cyberbullying, existing work largely focused on building generic classifiers that rely exclusively on text analysis of social media sessions. Despite their empirical success, we argue that a critical missing piece is the model explainability, i.e., why a particular piece of media session is detected as cyberbullying. In this paper, therefore, we propose a novel deep model, HEterogeneous Neural Interaction Networks , for explainable cyberbullying detection. HENIN contains the following components: a comment encoder, a post-comment co-attention sub-network, and session-session and post-post interaction extractors. Extensive experiments conducted on real datasets exhibit not only the promising performance of HENIN, but also highlight evidential comments so that one can understand why a media session is identified as cyberbullying."
"\zc{ Title: need concrete, something like ""Denoising Multi-Source Weak Supervision Neural Text Classification"" probably better Introduction: Paragraph 1: many NLP tasks formulated text classification dnns successful require labeled data, expensive obtain recently, pre-trained language models alleviate problem, still suffers degraded performance labeled data limited. \wendi{BERT still need labeled data} Paragraph 2: weak supervision promising, also challenging apply weak labels inaccurate incomplete. Paragraph 3: study using multiple weak supervision sources learn text classifiers; intuition multiple weak supervision sources provide complementary information eliminate noise; combined unlabeled data, address label incompleteness well. \wendi{key: complementary information; bootstrapping D_U} Paragraph 4: large body works weakly-supervised learning, dealing single-source weak supervision may suffer unreliability single sources error propagation; \wendi{sensitive single source} several works deal multiple sources, XXX , need make sure cite discuss them). Paragraph 5: introduce method, key idea, uniqueness compared existing methods. I feel current method description bit plain, need distill main ideas. I think main ideas are: - source reliability estimation neural classification benefit co-training framework \wendi{regularization} - conditional source reliability - self-training leverage unmatched samples obtain labeled instances. - maybe also mention rely pre-trained language models get good representations, helps denoising \wendi{high level: denoise, enhance} Other Sections: Section 2: make half page Section 3: 2.5 pages Section 4: 3 pages others: 1 page Something better show experiments: - multi-source weak supervision powerful this, already lot results - majority voting work - method works better existing weak supervision methods - happens use subsets multiple weak supervision sources - interpretations source reliability learned - different designs method work - would labeled data help } Text classification, relation extraction, question answering fundamental natural language tasks numerous applications document classification knowledge extraction. \zc{ Many NLP tasks formulated text classification problems, sentiment analysis, topic classification, relation extraction, XXX .} Recently, deep neural nets demonstrated superior performance problem \zc{briefly mention earlier dnns , recent trend BERT-based ones}, largely due capabilities automatically learning distributed features fitting complex functions based large-scale training data. However, many real world scenarios, large-scale labeled data unavailable manually annotating data large scale prohibitively expensive. \zc{merge paragraph 1 2} To address label scarcity bottleneck, study problem using heuristic rules train neural text classifiers. While domain experts \zc{not necessarily domain experts, also KBs.} often cannot afford annotate millions documents carefully, easily provide set heuristic rules weak supervision signals. Using rules automatically induce labeled data model training , meanwhile introduces two major challenges: label noise low label coverage. %The first challenge label noise. The label noise issue arises heuristic rules often simple capture rich contexts complex patterns text classification. For instance, rule `expensive \ negative' restaurant ranking correct times, sometimes wrong delicious food deserves high price. Seed rules limited coverage real-life text corpora often long-tail distributions, many heuristic rules defined frequent keywords, instances containing long-tail keywords cannot covered given rules. \zc{can merge previous paragraph shorten it.} There studies attempt use weak supervision deep text classification. Unfortunately, performance limited two challenges. Ratner \etal proposed data programming method, uses heuristic rules labeling functions trains discriminative models using automatically created labeled data. However, training data annotated data programming come instances directly matched rules, making model limited performance unmatched data. Meng \etal proposed deep self-training method, uses weak supervision learn initial model updates model using model's confident predictions. However, self-training procedure overfit label noise suffer error propagation. \sep Our contributions. We propose new method uses weak supervision train deep text classifiers label-efficient way, addressing label noise label coverage issues. We assume multiple weak supervision sources provide complementary sets heuristic rules. \zc{the previous two sentences merged.} Our idea complementary information multiple sources reduce label noise, also effectively bootstrap unlabeled data improve label coverage, making possible learn accurate deep text classifier weak supervision. Motivated above, propose model two carefully designed components. The first component rule-based classifier \zc{ rule reliability estimators} using conditional soft attention mechanism. Given weak labels annotators document representations, learn reliability scores labeling sources, emphasize weak annotators' opinions informative particular corpus. We use reliability scores aggregate disparate weak labels denoised pseudo label. \zc{need highlight rule reliability conditional input text features} The second component neural classifier learns labels distributed feature representations samples, matched unmatched. This neural classifier supervised denoised labels confident predictions unmatched data, enabling solve rule coverage problem simultaneously enhancing rule denoiser via patterns present unmatched data. The two components integrated end-to-end training framework. \zc{maybe also say use pre-trained BERT feature extractor: representation power help denoiser work better.} We evaluate model four text classification tasks, including sentiment analysis, topic classification, spam classification, information extraction. The results five benchmarks show that: soft-attention module indeed effectively denoise noisy training data induced weak supervision sources, achieving \textasciitilde{}\% accuracy denoising; co-training design improve prediction accuracy unmatched samples, achieving least \% accuracy increase them. In terms overall performance, model consistently outperforms state-of-the-art weakly supervised methods , semi-supervised methods , fine-tuning methods 9.2\% average. Further, show denoised labels fed fully supervised models fine-tune models improve performance. % Our contributions summarized follows: % % =============================================== % Chao: I outline structure intro, fill extend paragraphs! % % Paragraph 1: Text classification one fundamental problems text mining, information retrieval, natural language processing. While deep neural nets % % achieved dominant performance text classification, highly label-hungry, often requiring hundreds thousands labeled samples achieve strong performance. This become key bottleneck applying deep % % text classifiers many real-life applications, large-scale labeled data expensive obtain. % % Paragraph 2: An overview existing methods handling label sparsity. Including: % % self-training methods, % % fine-tuning methods, % % weakly supervised methods. Think hard drawbacks. % % Paragraph 3: An overview model: propose deep neural text classifier, learned excessive labeled data, unlabeled data plus set easy-to-provide heuristic rules. % % Paragraph 4: Two challenges learning rules: Learning model heuristic rules difficult, rules induce noisy training data limited coverage. % % Paragraph 5: How address two challenges: % % First, label denoising module, estimates source reliability denoises rule-induced supervision soft attention mechanism. Second, self-learning module improving label coverage issue, iteratively predicts soft labels unmatched samples aggregating denoised multi-source classifiers. The two modules integrated neural co-training model, learned end-to-end manner. % % Paragraph 6: The results obtain real data % % A bullet list summarizing contributions: Cyberbullying detection social media attracts growing attention recent years. It also crucial understand media session detected cyberbullying. Thus study novel problem explainable cyberbullying detection aims improving detection performance highlighting explainable comments. We propose novel deep learning-based model, HEterogeneous Neural Interaction Networks , learn various feature representations comment encodings, post-comment co-attention, graph-based interactions sessions posts. Experimental results exhibit promising performance evidential explanation HENIN. We also find learning graph-based session-session post-post interactions contributes performance. Such results encourage future studies develop advanced graph neural networks better representing interactions heterogeneous information. In addition, worthwhile model information propagation temporal correlation comments future."," % While deep neural nets have achieved superior performance for % text classification, they highly rely on large-scale labeled data. Obtaining large-scale labeled data, however, is prohibitively % expensive in many applications.  We study the problem of learning neural text classifiers without using any labeled data, but only easy-to-provide rules as multiple weak supervision sources. This problem is challenging because rule-induced weak labels are often noisy and incomplete. To address these two challenges, we design a label denoiser, which estimates the source reliability using a conditional soft attention mechanism and then reduces label noise by aggregating rule-annotated weak labels. The denoised pseudo labels then supervise a neural classifier to predicts soft labels for unmatched samples, which address the rule coverage issue. % To address these challenges, we % propose an end-to-end model with two key components \zc{this sentence is not %   informative enough, need to deliver the key idea of our method in one sentence % here, and then use the remaining sentences to elaborate our idea.}. The first component is a % rule denoiser, which estimates conditional source reliability using a soft % attention mechanism and reduces label noise by aggregating rule-annotated weak % labels. The second is a neural classifier that predicts soft labels for % unmatchable samples to address the rule coverage issue. %The two components are integrated into a co-training framework, which can be trained end-to-end to mutually enhance each other. We evaluate our model on five benchmarks for sentiment, topic, and relation classifications. The results show that our model outperforms state-of-the-art weakly-supervised and semi-supervised methods consistently, and achieves comparable performance with fully-supervised methods even without any labeled data. Our code can be found at \url{https://github.com/weakrules/Denoise-multi-weak-sources}."
"Systematic reviews part field evidence-based analysis, methodology conducting literature surveys, focus comprehensively summarising synthesising existing research purpose answering research questions . The aim process broad coverage avoid unknown bias creeping results via alternative cherry-picking scientific results . %As many relevant documents possible included, process also thoroughly documented aid replicability. Conducting systematic reviews requires trained researchers domain knowledge. The stages process time-consuming, vary much physical mental labour require . As result, systematic reviews suffer three primary challenges : So though systematic reviews shown effective less prone human biases , issues often prove prohibitive. \\ However, challenges well suited Machine Learning solutions, recently increase interest applying NLP process . In paper, investigate feasibility implementing multi-stage human process systematic review Machine Learning pipeline. We construct systematic review pipeline aims assist researchers organisations focusing livestock health various African countries previously performed reviews manually . The pipeline begins scraping articles, classifies whether include review, identifies data extract outputs spreadsheet. We discuss technical options evaluated steps. Pipeline components evaluated intrinsic metrics well pragmatic, extrinsic, considerations time effort saved. While previous work exists surveying applicability various Machine Learning methods toolkits systematic review process apply them, extant studies implement full system analyse trade-offs different methods training data creation, different annotation schemas, human expert hours needed build system, final accuracy. We experiment factors, well different architectures, aim informing planning implementation systematic review automation broadly. To goal, particularly experiment low resource scenarios generalisability. We investigate different thresholds training data document classifier different annotation schemas data extraction. We additionally test ability system generalise documents new countries. % also talk needing deep learning resources Key research questions follows: \paragraph{Extraction} Which techniques best identifying extracting desired information? \paragraph{Data Requirements} How much labelled training data needed? Can existing resources leveraged? \paragraph{Re-usability} How generalisable pipeline new diseases countries? \paragraph{Performance} What trade-off pipeline accuracy human time savings? \paragraph{Architecture \& Pre-training} How important model architecture applied extraction tasks? How important embedding pre-training, important pre-training scientific literature vs. general content ?\\ We find surprisingly little training data necessary get accurate document classifier, generalises well unseen African countries , enables systematic reviews expanded new areas essentially constant time. In text extraction experiments, find sentence phrase level extraction models play role pipeline, %given complementary strengths weaknesses kind data, phrase extraction, previously done task, performed better expected baseline CNN models BERT-based Transformers , Transformers based scientific pre-training performing best. We demonstrate creation labelled training data sped annotation tools, consideration given balance training examples present within data, since may require less data overall still maintaining good performance. Furthermore, besides automatic information extraction, much labour constructing systematic reviews saved simply automating process searching downloading documents. We empirically demonstrate three month pipeline systematic review automated require little human intervention, acceptable accuracy results. We release code, annotation schema, labelled data assist expansion systematic reviews via automation. While demonstrate system one domain, framework domain independent could applied kinds systematic reviews. New training data annotation schemes would necessary switch medical domains, findings time saving processes annotation would apply, confidence thresholds implement adjustable customise different levels accuracy human time trade-offs appropriate different fields. Our exploration necessary amounts training data accuracy generalisability broadly applicable. \item Compositionality provides explanation LSTMs learn long-range connections slowly LSTMs take advantage linguistic structure. \item Long-range connections build predictable short range connections training. \item Familiar patterns attract new significance encouraging interdependence, even cost general predictors. \item Syntactically associated words higher interdependence English. Using proposed tool Decompositional Interdependence, illustrate information exchanged words aligns roughly syntactic structure, indicating LSTMs compose meaning bottom-up. Synthetic experiments illustrate memorized span intervening long distance dependency promotes early learning dependency rule, fails generalize new domains, implying memorized spans used scaffolding bottom-up learning process. This combination behaviors similar syntactic language model, suggesting LSTM's demonstrated inductive bias towards hierarchical structures implicitly aligned understanding language emerges natural learning process."," Systematic reviews, which entail the extraction of data from large numbers of scientific documents, are an ideal avenue for the application of machine learning. They are vital to many fields of science and philanthropy, but are very time-consuming and require experts. Yet the three main stages of a systematic review are easily done automatically: searching for documents can be done via APIs and scrapers, selection of relevant documents can be done via binary classification, and extraction of data can be done via sequence-labelling classification. Despite the promise of automation for this field, little research exists that examines the various ways to automate each of these tasks. We construct a pipeline that automates each of these aspects, and experiment with many human-time vs. system quality trade-offs. We test the ability of classifiers to work well on small amounts of data and to generalise to data from countries not represented in the training data. We test different types of data extraction with varying difficulty in annotation, and five different neural architectures to do the extraction. We find that we can get surprising accuracy and generalisability of the whole pipeline system with only 2 weeks of human-expert annotation, which is only 15\% of the time it takes to do the whole review manually and can be repeated and extended to new data with no additional effort.\footnote{\hspace{0.1cm}Code and links to models available at \url{https://github.com/seraphinatarrant/systematic_reviews}}"
"Although recent neural models language made advances learning syntactic behavior, research continues suggest inductive bias plays key role data efficiency human-like syntactic generalization . Based long-held observation language exhibits hierarchical structure, previous work proposed coupling recurrent neural networks differentiable stack data structures give computational power pushdown automata , class automata recognize context-free languages . However, previously proposed differentiable stack data structures model deterministic stacks, store one version stack contents time, theoretically limiting power stack RNNs deterministic~CFLs. A sentence's syntactic structure often cannot fully resolved conclusion , requiring human listener track multiple possibilities hearing sentence. Past work psycholinguistics suggested models keep multiple candidate parses memory explain human reading times better models assume harsher computational constraints. This ability also plays important role calculating expectations facilitate efficient language processing . Current neural language models track multiple parses, learn syntax generalizations . We propose new differentiable stack data structure explicitly models nondeterministic PDA, adapting algorithm \citet{lang:1974} reformulating terms tensor operations. The algorithm able represent exponential number stack configurations using cubic time quadratic space complexity. As existing stack RNN architectures, combine data structure RNN controller, call resulting model \ourmodel{} . We predict nondeterminism help language processing two ways. First, improve trainability, since possible sequences stack operations contribute objective function, sequence used current model. Second, improve expressivity, able model concurrent parses ways deterministic stack cannot. We demonstrate claims comparing \om{} deterministic stack RNNs formal language modeling tasks varying complexity. To show nondeterminism aids training, show \om{} achieves lower cross-entropy, fewer parameter updates, deterministic CFLs. To show nondeterminism improves expressivity, show \om{} achieves lower cross-entropy nondeterministic CFLs, including ``hardest context-free language"" , language least difficult parse CFL inherently requires nondeterminism. Our code available \url{https://github.com/bdusell/nondeterministic-stack-rnn}. We investigated application automation stages systematic review pipeline veterinary research case study. We found two weeks human expert annotation automate systematic review previously took 3 months, still maintain high levels accuracy. Our classification system generalises well, enabling applied new countries additional systematic reviews additional human annotation cost. Sentence-based phase-based data extraction perform well, creation phrase-based training data still fit within small amount human annotation hours avoids need extensive post-processing. Fine-tuned BERT-based Transformers perform best data extraction, BERT pre-trained scientific data giving largest boost performance, though baseline CNN still performs surprisingly well. In future work, plan test generalisability cross-lingually, expand generalisability tests extraction well classification, study performance improvements continuous training classifiers human corrections low-confidence output."," We present a differentiable stack data structure that simultaneously and tractably encodes an exponential number of stack configurations, based on Lang algorithm for simulating nondeterministic pushdown automata. We call the combination of this data structure with a recurrent neural network  controller a \ourmodel. We compare our model against existing stack RNNs on various formal languages, demonstrating that our model converges more reliably to algorithmic behavior on deterministic tasks, and achieves lower cross-entropy on inherently nondeterministic tasks."
"Cryptography used since antiquity encode important secrets. There many unsolved ciphers historical interest, residing national libraries, private archives, recent corpora collection projects . Solving classical ciphers automatic methods needed step analyzing materials. In work, concerned automatic algorithms solving historically-common type book code, word tokens systematically replaced numerical codes. Encoding decoding done reference dictionary possessed sender recipient. While type code common, automatic decipherment algorithms yet exist. The contributions work are: We implement evaluate techniques pronounce Chinese text Mandarin, without use pronunciation dictionary parallel resource. The EM method achieves test-set accuracy 71\ , vector-based method achieves 81\ . By combining two methods, obtain 89\ accuracy, significantly exceeds prior work. We also demonstrate current methods unsupervised matching vector spaces sensitive structure spaces. In presence one-to-many mappings pinyin characters, mapping accuracy severely downgraded, leaving open opportunity design robust unsupervised vector mapping systems."," We solve difficult word-based substitution codes by constructing a decoding lattice and searching that lattice with a neural language model.  We apply our method to a set of enciphered letters exchanged between US Army General James Wilkinson and agents of the Spanish Crown in the late 1700s and early 1800s, obtained from the US Library of Congress.  We are able to decipher 75.1\% of the cipher-word tokens correctly."
"Neural network language models , pretrained vast amounts raw text, become dominant input downstream tasks . Commonly, tasks involve aspects language comprehension . One explicit example coreference resolution, wherein anaphora linked antecedents requiring knowledge syntax, semantics, world-knowledge match human-like comprehension. Recent work suggested LMs acquire abstract, often human-like, knowledge syntax \cite[e.g.,][]{gulordavaetal18, futrelletal2018, huetal2020-systematic}. Additionally, knowledge grammatical referential aspects linking pronoun antecedent noun demonstrated transformer long short-term memory architectures . Humans able modulate referential syntactic comprehension given abstract linguistic knowledge . Contrary humans, find discourse structure influences LM behavior reference, syntax, despite model representations encode necessary discourse information. The particular discourse structure examined governed implicit causality verbs . Such verbs influence pronoun comprehension: \ex. \a. Sally frightened Mary terrifying. \b. Sally feared Mary terrifying. In , agrees gender Sally Mary, possible antecedents. However, English speakers overwhelmingly interpret referring Sally Mary , despite semantic overlap verbs. Verbs subject preference called subject-biased IC verbs, verbs object preference called object-biased IC verbs. In addition pronoun resolution, IC verbs also interact relative clause attachment: \ex. \a. John babysits children musician who... \a. ...lives La Jolla. \b. ...are students private school. \z. \b. John detests children musician who... \a. ...lives La Jolla. \b. ...are arrogant rude. \z. \z. \citep[from][]{rohdeetal2011} In , sentence fragments possible continuations modifying musician continuations modifying children . We might expect human continuation preferences . However, use object-biased IC verb increases proportion continuations given human participants refer children . Without object-biased IC verb majority continuations refer recent noun . Effects IC received renewed interest field psycholinguistics recent years \cite[e.g.,][]{kehler2008coherence, ferstl2011implicit, hartshorne2013verb, hartshorne2014, williams_IC_2020}. Current accounts IC claim phenomenon inherently linguistic process, rely additional pragmatic inferences comprehenders \cite[e.g.,][]{rohdeetal2011, hartshorne2013verb}. Thus, IC argued contained within linguistic signal, analogous evidence syntactic agreement verb argument structure within corpora. We hypothesize claims correct, current LMs able condition reference syntactic attachment IC verbs language data . We tested hypothesis using unidirectional transformer long short-term memory network \citep[LSTM;][]{hochreiterschmidhuber97} language models. We find LSTM LMs fail acquire subject/object-biased IC distinction influences reference RC attachment. In contrast, transformers learned representational distinction subject-biased object-biased IC verbs interacts reference RC attachment, distinction influenced model output reference. The apparent failure model syntactic behavior exhibit IC contrast present model representations raises questions broader capacity LMs display human-like linguistic knowledge. In work, show possible decipher book-based cipher, using known-plaintext attack neural English language model. We apply method letters written US General James Wilkinson, recover 75.1\ word tokens correctly. We believe word-based neural language models powerful tool decrypting classical codes ciphers. Because much lower perplexities widely-used n-gram models, distinguish candidate plaintexts resemble English distance, versus candidate plaintexts grammatical, sensible, relevant historical context.","  Language models  trained on large quantities of text have been claimed to acquire abstract linguistic representations. Our work tests the robustness of these abstractions by focusing on the ability of LMs to learn interactions between different linguistic representations. In particular, we utilized stimuli from psycholinguistic studies showing that humans can condition reference  and syntactic processing on the same discourse structure . We compared both transformer and long short-term memory LMs to find that, contrary to humans, implicit causality only influences LM behavior for reference, not syntax, despite model representations that encode the necessary discourse information. Our results further suggest that LM behavior can contradict not only learned representations of discourse but also syntactic agreement, pointing to shortcomings of standard language modeling."
"Word ordering often determines meaning sentence; therefore utilize position information word sequence important topic NLP widely investigated recently. A common approach modeling word ordering use recurrent neural networks , long short-term memory gated recurrent unit , use hidden state represent information ordered sequence update model weights backpropagation time ; thus ordering information modeled structure. However, RNN BPTT inefficient modern GPU computation due difficulty parallelization time dependency. To solve problem, recent work, convolutional seq2seq Transformers apply convolutional neural network self-attention respectively, succeed eliminate time dependency take computational advantage GPU. Instead storing information ordered sequences, models utilize position information using feature-level positional encoding. For example, convolutional seq2seq proposed learnable position embeddings represent positions sequence. Recently, various pre-trained Transformer language models keep breaking state-of-the-art results numerous NLP tasks. There many different ways pre-train Transformer language model. For example, using encoder, decoder, whole part Transformer, adapting self-attention masks, training different objectives . However, terms positional encoding, work used learned position embedding originally proposed convolutional seq2seq without analysis, even different objectives may learn completely different position information. Motivated observations, goal investigate position information pre-trained Transformers could learn different settings. We conduct deep analysis learned position embeddings among three iconic pre-trained Transformer language models: BERT , RoBERTa GPT-2 . To examine performance different NLP types, conduct experiments text classification, language modeling, machine translation, empirically analyze explain meaning influence position embeddings different aspects. The contributions paper 3-fold: The present study examined extent discourse structure, determined implicit causality verbs, could acquired transformer LSTM language models . Specifically, evaluated, via comparison human experiments, whether IC verb biases could influence reference syntactic attachment LMs. Analyses conducted two levels granularity: model behavior model representation . Given claims recent literature implicit causality arises without extra pragmatic inference part human comprehenders, hypothesized LMs would able acquire contrasts . We found LSTM LMs unable demonstrate knowledge IC either influencing reference syntax. However, transformer trained exact data LSTM LMs able partially represent IC distinction, model output influenced IC bias resolving reference, syntactic attachment. In evaluating transformer model trained vastly data , found robust, human-like sensitivity IC bias resolving reference: subject-biased IC verbs increased model preference subject pronouns object-biased IC verbs increased model preferences object pronouns. However, mismatch TransformerXL model representation model behavior arose processing syntactic attachment. In contrast results, \citet{davis-van-schijndel-2020b} showed syntactic predictions LSTM LMs influenced aspects discourse structure. A simple explanation conflicting results may LMs examined unable learn syntactic operation attachment, thus influence discourse surface. The erasure number agreement final layers transformer LMs provides compelling evidence towards conclusion.\footnote{Further cross-linguistic evidence bearing inability LSTM LMs, specifically, learn relative clause attachment given \citet{davis-van-schijndel-2020-recurrent}.} From theoretical perspective, present study provides additional support centering implicit causality within linguistic signal proper. That is, IC bias learnable, degree, without pragmatic inference hypothesized Section \cite[see also][]{hartshorne2014}. The mismatches syntactic representations behavior suggest, however, models ignore abstract categories learned, contrary human findings \cite[cf.][]{rohdeetal2011}. We believe solution may lie changing model training objectives . Psycholinguistic studies focusing interaction discourse syntax suggested coherence relations may unit linguistic prediction, contrast next-word prediction used language modeling work \cite[see][]{rohdeetal2011}. We leave future work investigation suggestion well teasing apart exact role training data model architecture play interaction types linguistic representation. \section*{Acknowledgments} Thank members C.Psyd lab Cornell, gave feedback earlier form work. We would also like thank three anonymous reviewers comments suggestions. \section{Stereotypically gendered nouns used referential experiments}"," In recent years, pre-trained Transformers have dominated the majority of NLP benchmark tasks.  Many variants of pre-trained Transformers have kept breaking out, and most focus on designing different pre-training objectives or variants of self-attention.  Embedding the position information in the self-attention mechanism is also an indispensable factor in Transformers however is often discussed at will.  Therefore, this paper carries out an empirical study on position embeddings of mainstream pre-trained Transformers, which mainly focuses on two questions: 1) Do position embeddings really learn the meaning of positions? 2) How do these different learned position embeddings affect Transformers for NLP tasks?  This paper focuses on providing a new insight of pre-trained position embeddings through feature-level analysis and empirical experiments on most of iconic NLP tasks. It is believed that our experimental results can guide the future work to choose the suitable positional encoding function for specific tasks given the application property.\footnote{The source code is available at: \url{https://github.com/MiuLab/PE-Study}} %to make our study more convincing."
"Autoregressive sequence sequence models Transformers trained maximize log-likelihood target sequence, conditioned input sequence. Furthermore, approximate inference typically done using beam search algorithm , allows controlled exploration exponential search space. However, seq2seq models suffer discrepancy token level classification learning sequence level inference search. This discrepancy also manifests form curse sentence length i.e. models' proclivity generate shorter sentences inference, received considerable attention literature . In work, focus better model long-tailed phenomena, i.e. predicting long-tail low-frequency words/tokens , seq2seq models, task Neural Machine Translation . Essentially, two mechanisms tokens low frequency receive lower probabilities prediction: firstly, norms embeddings low frequency tokens smaller, means dot-product based softmax operation generate probability distribution vocabulary, receive less probability. This well known Image Classification Neural Language Models . Since NMT shares dot-product softmax operation, observe phenomenon holds true NMT well. For example, observe Spearman Rank Correlation 0.43 norms token embeddings frequency, standard transformer model trained IWSLT-14 De-En dataset . Secondly, transformer based NMT, embeddings low frequency tokens lie different subregion space semantically similar high frequency tokens, due different rates updates , thereby, making rare words token embeddings ineffective. Since token embeddings match context vector getting next-token probabilities, dot-product similarity score lower low frequency tokens, even semantically similar high frequency tokens. Further, better modeling long-tailed phenomena significant implications several text generation tasks, well compositional generalization . To end, primarily ask seek answers following two fundamental questions context NMT: By exploring questions, arrive conclusion widely used cross-entropy loss limits NMT models' expressivity inference propose new loss function better incorporate inductive biases beam search. This paper investigates implicit meaning pre-trained Transformer position embeddings. Transformer encoders learn local position information effective masked language modeling. On hand, Transformer decoders autoregressive language modeling actually learn absolute positions. The empirical experiments pre-trained position embeddings validate hypothesis. We also show different NLP tasks different model architectures different training objectives may utilize position information different ways. As result, believed study benefit future work choosing suitable positional encoding functions designing modeling methods position information target NLP tasks based properties."," State-of-the-art Neural Machine Translation  models struggle with generating low-frequency tokens, tackling which remains a major challenge. The analysis of long-tailed phenomena in the context of structured prediction tasks is further hindered by the added complexities of search during inference. In this work, we quantitatively characterize such long-tailed phenomena at two levels of abstraction, namely, token classification and sequence generation. We propose a new loss function, the Anti-Focal loss, to better adapt model training to the structural dependencies of conditional text generation by incorporating the inductive biases of beam search in the training process. We show the efficacy of the proposed technique on a number of Machine Translation  datasets, demonstrating that it leads to significant gains over cross-entropy across different language pairs, especially on the generation of low-frequency words. We have released the code to reproduce our results.\blfootnote{The first author is now a researcher at Microsoft, USA.}\footnote{\url{https://github.com/vyraun/long-tailed}} %"
"Grammar induction task learning grammar target corpus without exposure parsing ground truth expert-labeled tree structures . Recently emerging latent tree learning models provide new approach problem . They learn syntactic parsing indirect supervision main training tasks language modelling natural language inference. In study, analyze ON-LSTM , new latent tree learning model set state art unsupervised constituency parsing WSJ test published ICLR 2019. The model trained language modelling generate binary constituency parsing trees input sentences like one Figure . As far know, though excellent theoretical analysis paper ON-LSTM model focuses model's architecture parsing algorithm, systematic analysis parses model generates. There in-depth investigations whether model's parsing behavior consistent among different restarts parses produces different PTB gold standards. Answering questions crucial better understanding capability model may bring insights build advanced latent tree learning models future. Therefore, replicate model 5 random restarts look parses generates. We find ON-LSTM fairly consistent parsing behaviors across different restarts, achieving self F1 65.7 WSJ test. The model struggles correctly parse internal structures complex noun phrases. The model consistent tendency overestimate height split points right verbs auxiliary verbs, leading major difference parses Penn Treebank gold-standard parses. We speculate problems explained training task, unidirectional language modelling, thus hypothesize training bidirectional model syntax-related task like acceptability judgement might good choice future latent tree learning models. In work, characterized long-tailed phenomena NMT demonstrated NMT models able effectively generate low-frequency tokens output. We proposed new loss function, Anti-Focal loss, incorporate inductive biases beam search NMT training process. We conducted comprehensive evaluations 9 language pairs different amounts training data IWSLT TED corpora. Our proposed technique leads gains across range metrics, improving long-tailed NMT token well sequence level. In future, wish explore connections entropy regularization model calibration whether fully encode inductive biases label smoothing loss function itself."," Recent latent tree learning models can learn constituency parsing without any exposure to human-annotated tree structures. One such model is ON-LSTM \citep{ONLSTMShen}, which is trained on language modelling and has near-state-of-the-art performance on unsupervised parsing. In order to better understand the  performance and consistency of the model as well as how the parses it generates are different from gold-standard PTB parses, we replicate the model with different restarts and examine their parses. We find that  the model has reasonably consistent parsing behaviors across different restarts,  the model struggles with the internal structures of complex noun phrases,  the model has a tendency to overestimate the height of the split points right before verbs. We speculate that both problems could potentially be solved by adopting a different training task other than unidirectional language modelling."
"Deep learning become dominant approach address Natural Language Processing tasks, including text classification. With sufficient high-quality training data, deep learning models perform incredibly well . However, real-world cases, ideal datasets scarce. Often times, available datasets small, full regular irrelevant words, contain unintended biases . These lead suboptimal models undesirable properties. For example, models may biases sub-populations may work effectively wild overfit imperfect training data. To improve models, previous work looked different techniques beyond standard model fitting. If weaknesses training datasets models anticipated, strategies tailored mitigate weaknesses. For example, augmenting training data gender-swapped input texts helps reduce gender bias models . Adversarial training prevent models exploiting irrelevant and/or protected features . With limited number training examples, using human rationales prior knowledge together training labels help models perform better . Nonetheless, side-effects sub-optimal datasets cannot predicted found training thanks post-hoc error analysis. To rectify problems, attempts enable humans fix trained models . Since models usually complex understand, manually modifying model parameters possible. Existing techniques, therefore, allow humans provide feedback individual predictions instead. Then, additional training examples created based feedback retrain models. However, local improvements individual predictions could add inferior overall performance . Furthermore, existing techniques allow us rectify errors related examples hand provide way fix problems kept hidden model parameters. In paper, propose framework allows humans debug improve deep text classifiers disabling hidden features irrelevant classification task. We name framework FIND . FIND exploits explanation method, namely layer-wise relevance propagation , understand behavior classifier predicts training instance. Then aggregates information using word clouds create global visual picture model. This enables humans comprehend features automatically learned deep classifier decide disable features could undermine prediction accuracy testing. The main differences work existing work are: first, FIND leverages human feedback model components, individual predictions, perform debugging; second, FIND targets deep text classifiers convoluted traditional classifiers used existing work . We conducted three human experiments demonstrate usefulness FIND. For experiments, used classifiers convolutional neural networks , popular, well-performing architecture many text classification tasks including tasks experimented . The overall results show FIND human-in-the-loop improve text classifiers mitigate said problems datasets. After experiments, discuss generalization proposed framework tasks models. Overall, {\bf main contributions} paper are: The rest paper organized follows. Section explains related work analyzing, explaining, human-debugging text classifiers. Section proposes FIND, debugging framework. Section explains experimental setup followed three human experiments Section . Finally, Section discusses generalization framework concludes paper. Code datasets paper available \url{https://github.com/plkumjorn/FIND}. In summary, model shows basic self-consistency task constituency parsing, consistently able correctly identify certain constituents . All results show unique design model brings us closer developing consistently powerful unsupervised parsing models. However, experiments show struggles internal structures complex NPs, often overestimates height split points right verbs. Based analysis, hypothesize failures least partially attributed use unidirectional language modelling training task. There two potential problems training task. First, motivation language modelling generally perfectly match target task constituency parsing, since cross-constituent hints sometimes helpful, revealed . Second, hard unidirectional model correctly identify high-level constituents, revealed . Therefore, believe promising research direction build latent tree learning models based bidirectional model architectures like transformer task acceptability judgement dataset like CoLA , syntax-related sentence-level task requires model predict whether input sentence grammatically acceptable. Another option consider masked language modelling also bidirectional task much easier scale compared acceptability judgement since self-supervised task."," Since obtaining a perfect training dataset  is hardly possible, many real-world text classifiers are trained on the available, yet imperfect, datasets.  These classifiers are thus likely to have undesirable properties. For instance, they may have biases against some sub-populations or may not work effectively in the wild due to overfitting.  In this paper, we propose FIND -- a framework which enables humans to debug deep learning text classifiers by disabling irrelevant hidden features. Experiments show that by using FIND, humans can improve CNN text classifiers which were trained under different types of imperfect datasets ."
"% Neural dependency parsers predicts relations interactions words equipped nerual networks. Graph-based dependency parsing popular approach dependency parsing scores parse components sentence finds highest scoring tree inference. First-order graph-based dependency parsing takes individual dependency edges components parse tree, higher-order dependency parsing considers complex components consisting multiple edges. There exist exact inference algorithms approximate inference algorithms find best parse tree. %Neural network based dependency parsers become popular due high efficiency accuracy. Transition-based dependency parsing builds dependency trees making series decisions sequence words, graph-based dependency parser first encodes words sentence using bi-directional LSTM score components parse tree find highest scoring tree inference. Recent work focused neural network based graph dependency parsers . \citet{dozat2016deep} proposed first-order graph-based neural dependency parsing approach simple head-selection training objective. It uses biaffine function score dependency edges high efficiency good performance. Subsequent work introduced second-order inference parser. \citet{ji-etal-2019-graph} proposed graph neural network captures second-order information token representations, used first-order parsing. Very recently, \citet{zhang2020efficient} proposed efficient second-order tree CRF model dependency parsing achieved state-of-the-art performance. %Higher-order dependency parsing takes complex higher-order components like siblings grandparents consideration decoding phase uses algorithms like dynamic programming exact inference. Such kind higher-order components increases global information inference results improvements parsing accuracy, also make inference slower complicated. Recent work graph-based higher-order dependency parsing semantic dependency parsing focused approximate inference graph, , much faster minor performance reduction compared exact inference algorithm. % \citet{falenska-kuhn-2019-non} also showed adding second-order inference BiLSTM-based parser leads small improvements. \citet{wang-etal-2019-second} proposed second-order approach semantic dependency parsing , tree constraint syntactic dependency parsing. They employed end-to-end neural network derived message-passing algorithms approximate second-order parsing achieved state-of-the-art accuracies SDP. In paper, first show previously proposed second-order semantic dependency parser applied syntactic dependency parsing simple modifications. The parser end-to-end neural network derived message passing inference conditional random field encodes second-order parsing problem. We propose alternative conditional random field incorporates head-selection constraint syntactic dependency parsing, derive novel second-order dependency parser. We empirically compare two second-order approaches first-order baselines English Penn Tree Bank 3.0 , Chinese Penn Tree Bank 5.1 datasets 12 languages Universal Dependencies . We show approaches achieve state-of-the-art performance PTB CTB approaches significantly faster recently proposed second-order parsers. We also make two interesting observations empirical study. First, common belief contextual word embeddings ELMo BERT already conveys sufficient high-order information renders high-order parsing less useful, find second-order decoding still helpful even strong contextual embeddings like BERT. Second, \citet{zhang-etal-2019-empirical} previously found incoperating head-selection constraint helpful first-order parsing, find better loss function design hyper-parameter tuning first- second-order parsers without head-selection constraint match accuracy parsers head-selection constraint even outperform latter using BERT embedding. Our approaches closely related work \citet{gormley-etal-2015-approximation}, proposed non-neural second-order parser based Loopy Belief Propagation . Our work differs that: 1) use Mean Field Variational Inference instead LBP, \citet{wang-etal-2019-second} found faster equally accurate practice; 2) add head-selection constraint include global tree constraint shown produce slight improvement would complicate neural network design implementation; 3) employ modern neural encoders achieve much better parsing accuracy. Our approaches also closely related recent work \citet{turbo2020}. The main difference use MFVI use dual decomposition algorithm approximate inference. % In recent work semantic dependency parsing , \citet{wang-etal-2019-second} proposed second-order parser following first-order parser \citet{dozat-manning-2018-simpler}. % %encodes first-order score following \citet{dozat-manning-2018-simpler} biaffine functions second-order score trilinear functions. % They used Mean Field Variational Inference Loopy Belief Propagation algorithm pass messages Conditional Random Field trained end-to-end manner. The approach SDP sees existence every single edge binary classification problem also applied tree-based dependency parsing. \citet{zhang-etal-2019-empirical} compared different structured outputs dependency parsing showed first-order dependency parsing, head constraint \citet{dozat2016deep} stronger binary classification structure \citet{dozat-manning-2018-simpler} dependency parsing. % In paper, adopt message passing method MFVI dependency parsing additionally add Local head constraint second-order inference procedure, views problem head-selection classification problem. We investigate advantage Single Local structured output second-order parsers show second-order parsers achieve state-of-the-art performance PTB CTB. Both second-order parsers Local Single structured outputs outperform first-order parser \citet{dozat2016deep} improvement BERT embeddings. Compared approach \citet{ji-etal-2019-graph} decodes second-order information passing token features graph neural network, second-order parsers message passing interpretive follow previous higher-order approaches assigns scores components. \citet{gormley-etal-2015-approximation} proposed second-order parser Single structured output tree constraint dependency parsing using LBP. Compared approach, consider Local head constraint. We use MFVI algorithm faster practice, need time-consuming Inside-Outside algorithm keep tree structure training. Furthermore, compare structured output tree constraint second-order parser empirical investigation \citet{zhang-etal-2019-empirical}, tree constraint gives modest improvement compared first-order Local approach. We believe advantage tree constraint diminished second-order parser considers tree components. % In paper, propose novel two-stage pipeline approach Loire learn commonsense images. In first stage, text representation model ViBERT trained bi-modal sequence-to-sequence approach scene layout generation COCO. Therefore, visual commonsense knowledge like spatial relations encoded ViBERT supervision caption image layout. After that, ViBERT concatenated pre-trained language model perform knowledge-augmented reasoning process. Experimental results show Loire outperforms current state-of-the-art language models BERT RoBERTa two NLP commonsense reasoning tasks, i.e.~commonsense question answering data CommonsenseQA pronoun resolution data WinoGrande. The ablation case study show improvements truly owing learned visual commonsense knowledge, knowledge helps NLP reasoning process. The current approach preliminary study proposed direction using images automatically learn commonsense knowledge facilitate NLP reasoning tasks, could modified following aspects improve empirical performances. Firstly, larger bi-modal data could employed learn commonsense required reasoning task. Secondly, bi-modal methods instead training ViBERT supervision scene layout generation may investigated. Thirdly, design intrinsic evaluation help understand learned Lorie still challenging considered future."," In this paper, we propose second-order graph-based neural dependency parsing using message passing and end-to-end neural networks. We empirically show that our approaches match the accuracy of very recent state-of-the-art second-order graph-based neural dependency parsers and have significantly faster speed in both training and testing. We also empirically show the advantage of second-order parsing over first-order parsing and observe that the usefulness of the head-selection structured constraint vanishes when using BERT embedding. %We adapt a previous approach that predicts dependency edges independently and we also propose a new approach that incorporates the head-selection structural constraint."
"Our SJTU-NICT team participated WMT20 shared task, including supervised track, unsupervised, low-resource track. During participation, placed attention Polish English English Chinese supervised track, unsupervised low-resource track, German Upper Sorbian directions focused. Our baseline system supervised track based Transformer big architecture proposed \citet{vaswani2017attention}, open-source implementation version Fairseq adopted. In unsupervised low-resource track, draw successful experience XLM framework , used two-stage training mode masked language modeling pre-training + back-translation finetune obtain strong baseline performance. Marian toolkit utilized training decoder reranking using machine translation targets instead common GPT-style language modeling targets. In order better play role WMT evaluation polishing methods proposed improved team , divided three language pairs participated three categories: % In supervised PLEN translation direction, based XLM framework pre-train Polish language model using common crawl news crawl monolingual data, proposed XLM enhanced NMT model inspired idea incorporating BERT NMT . Besides, trained bidirectional translation model EN-PL based parallel corpus finetuned PLEN direction. In supervised ENZH translation document information, propose document enhanced NMT model based Longformer . The training proposed document enhanced NMT model split three stages. In first stage, pre-train Longformer document encoder MLM target document text Wikipedia dumps, UN News, News Commentary monolingual corpus. A conventional Transformer-big NMT model trained second stage. In final stage, Longformer encoder conventional Transformer big NMT model used initialize full document-enhanced NMT model parameters, Longformer encoder adopted extract representations document input sequence, document representations fused layer encoder decoder NMT model attention mechanisms. In unsupervised machine translation track DE-HSB, experimented reference language based UNMT framework proposed recently. Under framework, choose English reference language, use Europarl parallel corpus EN-DE enhance unsupervised machine translation DE HSB. Specifically, adopted reference language translation , reference language back-translation , cross-lingual back-translation three training targets help cross-lingual agreement provided EN-DE parallel corpus enhance unsupervised translation performance. Due introduction explicit supervision signals brought parallel corpus low-resource machine translation track DE-HSB, discarded use weaker agreement provided reference language, conducted joint training unsupervised back-translation supervised translation directly, introduced BT-BLEU based collaborative filtering technology self-training. In addition, inspired previous work , also use MLM translation language modeling continue pre-training model machine translation training. In addition, basic NMT models, empower training process proposed data-dependent gaussian prior objective , model maintain diversity output. When main model training finished, TF-IDF algorithm employed filter training set according input test set, training subset whose domain similar test set obtained, used finetune model reducing performance degradation caused domain inconsistency. For final submission, ensemble several different trained models outputs -best predictions, used decoder trained Marian toolkit performs reranking get final system output. We propose second-order graph-based dependency parsing based message passing end-to-end neural networks. We modify previous approach predicts dependency edges independently also design new approach incorporates head-selection structured constraint. Our experiments show second-order approaches better overall performance first-order baselines; achieve competitive accuracy recent start-of-the-art second-order graph-based parsers significantly faster. Our empirical comparisons also show second-order decoders still outperform first-order decoders even BERT embeddings, usefulness head-selection constraint limited, especially using BERT embeddings. Our code publicly avilable .","  In this paper, we introduced our joint team SJTU-NICT 's participation in the WMT 2020 machine translation shared task. In this shared task, we participated in four translation directions of three language pairs: English-Chinese, English-Polish on supervised machine translation track, German-Upper Sorbian on low-resource and unsupervised machine translation tracks. Based on different conditions of language pairs, we have experimented with diverse neural machine translation  techniques: document-enhanced NMT, XLM pre-trained language model enhanced NMT, bidirectional translation as a pre-training, reference language based UNMT,  data-dependent gaussian prior objective, and BT-BLEU collaborative filtering self-training. We also used the TF-IDF algorithm to filter the training set to obtain a domain more similar set with the test set for finetuning. In our submissions, the primary systems won the first place on English to Chinese, Polish to English, and German to Upper Sorbian translation directions."
"Neural summarizers achieved impressive performance evaluated ROUGE ~ in-domain setting, recent success pre-trained models drives state-of-the-art results benchmarks new level ~. However, superior performance guarantee perfect system since exsiting models tend show defects evaluated aspects. For example, \citet{zhang-etal-2018-abstractiveness} observes many abstractive systems tend near-extractive practice. \citet{cao2018faithful,wang2020asking,kryscinski2019evaluating,maynez2020faithfulness,durmus2020feqa} reveal generated summaries factually incorrect. These non-mainstream evaluation methods make easier identify model's weaknesses. Orthogonal two evaluation aspects, aim diagnose limitation existing systems cross-dataset evaluation, summarization system trained one corpus would evaluated range out-of-dataset corpora. Instead evaluating quality summarizers solely based one dataset multiple datasets individually, cross-dataset evaluation enables us evaluate model performance different angle. For example, Fig. shows ranking summarization systems studied paper different evaluation metrics, ranking list `` in-dataset R2'' obtained traditional ranking criteria two based designed cross-dataset measures. Intuitively, observe 1) different definitions ``good'' system various evaluation aspects; 2) abstractive extractive systems exhibit diverse behaviors evaluated cross-dataset setting. The example recaps general motivation work, encouraging us rethink generalization ability current top-scoring summarization systems perspective cross-dataset evaluation. Specifically, ask two questions follows: Q1: {How different neural architectures summarizers influence cross-dataset generalization performances?} When designing summarization systems, plethora neural components adopted ~. For example, copy coverage mechanisms improve cross-dataset generalization ability summarizers? Is risk BERT-based summarizers perform worse adapted new areas compared ones without BERT? So far, generalization ability current summarization systems transferring new datasets still remains unclear, poses significant challenge design reliable system realistic scenarios. Thus, work, take closer look effect model architectures cross-dataset generalization setting. Q2: {Do different generation ways summarizers influence cross-dataset generalization ability?} Extractive abstractive models, two typical ways summarize texts, usually follow diverse learning frameworks favor different datasets. It would absorbing know discrepancy perspective cross-dataset generalization. To answer questions above, conducted comprehensive experimental analysis, involves eleven summarization systems , five benchmark datasets different domains, two evaluation aspects. Tab. illustrates overall analysis framework. We explore effect different architectures generation ways model generalization ability order answer Q1 Q2. Semantic equivalency factuality adopted characterize different aspects cross-dataset generalization ability. Additionally, strengthen analysis presenting two views evaluation: holistic fine-grained views . }% % \end{table}% Our contributions summarized as: 1) Cross-dataset evaluation orthogonal evaluation aspects , used re-evaluate current summarization systems, accelerating creation robust summarization systems. 2) We design two measures Stiffness Stableness, could help us characterize generalization ability different views, encouraging us diagnose weaknesses state-of-the-art systems. 3) We conduct dataset bias-aided analysis suggest better understanding datasets helpful us interpret systems' behaviours. This paper describes SJTU-NICT's submission WMT20 news translation task. For three typical scenarios, adopt different strategies. In work, study pre-trained language model enhance MT, also consider impact document information translation. We considered way converting document alignment sentence alignment use BERT's NSP recover structure documents. In addition, transfer learning supervision taken account unsupervised translation, various means used enhance low-resource translation. Our systems performed strongly among constrained submissions: ranked 1st PLEN, ENZH, DEHSB respectively, stayed Top-3 HSBDE."," Neural network-based models augmented with unsupervised pre-trained knowledge have achieved impressive performance on text summarization. However, most existing evaluation methods are limited to an in-domain setting, where summarizers are trained and evaluated on the same dataset. We argue that this approach can narrow our understanding of the generalization ability for different summarization systems. In this paper, we perform an in-depth analysis of characteristics of different datasets and investigate the performance of different summarization models under a cross-dataset setting, in which a summarizer trained on one corpus will be evaluated on a range of out-of-domain corpora. A comprehensive study of 11 representative summarization systems on 5 datasets from different domains reveals the effect of model architectures and generation ways  on model generalization ability. Further, experimental results shed light on the limitations of existing summarizers. Brief introduction and supplementary code can be found in \url{https://github.com/zide05/CDEvalSumm}."
"As robots deployed collaborative applications like healthcare household assistance , growing need reliable human-robot communication. One communication modality user-friendly versatile natural language; end, focus robust natural language interfaces map utterances executable behavior . Most existing work NLIs falls static train-then-deploy paradigm: models first trained large datasets pairs deployed, hope reliably generalize new utterances. Yet, happens models make mistakes faced types utterances unseen training --- example, providing household robot novel utterance like ``wash coffee mug?'' Such static systems fail way recover, burdening user find alternate utterances accomplish task . Instead, argue NLIs need dynamic adaptive, learning interactively user feedback index perform complicated behaviors. In work, explore building NLIs simulated robotics learn real humans. Inspired \citet{wang2017naturalizing}, leverage idea learning decomposition learn new abstractions. Just like human interactively teaches new task friend breaking down, users interactively teach system simplifying utterances system cannot understand lower-level utterances . To map language executable behavior, \citet{wang2017naturalizing} \citet{thomason2019improving} built adaptive NLIs leverage grammar-based parsers allow reliable one-shot generalization lack lexical flexibility. For example, grammar-based system understands ``wash coffee mug'' may generalize ``clean mug.'' Meanwhile, recent semantic parsers based primarily neural sequence-to-sequence models . While models excel lexical flexibility perspective, lack ability perform reliable one-shot generalization: difficult train generalize individual examples . In paper propose new interactive NLI lexically flexible reliably efficiently perform one-shot generalization. We introduce novel exemplar-based neural network semantic parser first abstracts away entities , allowing generalization previously taught utterances novel object combinations. Our parser retrieves corresponding ``lifted'' utterance respective program training examples based learned metric , giving us lexical flexibility sequence-to-sequence models. We demonstrate efficacy learning decomposition framework set human-in-the-loop experiments crowdworkers use NLI solve suite simulated robotics tasks household environments. Crucially, completing task, update semantic parser users immediately reuse taught. We show time, users able complete complex tasks efficiently exemplar-based method compared neural sequence-to-sequence baseline. However, straightforward tasks completed fewer steps, see similar performance baseline. We end error analysis discussion user trust incentives context building interactive semantic parsing systems, paving way future work better realizes potential interactive paradigm. By performing comprehensive evaluation eleven summarization systems five mainstream datasets, summarize observations below: 1) Abstractive summarizers extremely brittle compared extractive approaches, maximum gap reaches 37 terms measure stableness defined paper. 2) BART superior abstractive models even comparable extractive models terms stiffness . On hand, robust transferring datasets possesses high stableness . 3) BERT performs excellently terms stiffness, still lacks stableness transferred \texttt{Bigpatent B} datasets. 4) The robustness models improved either equipped model ability copy span source document make use well trained sequence sequence pre-trained model . 5) Simply adding BERT encoder could improve stiffness model cause larger cross-dataset in-dataset performance gap, better way found merge BERT abstractive model, better training strategy applied offset negative influence brings. 6) Existing factuality checker limited predictive power positive samples . 7) Out-of-domain systems even surpass in-domain systems terms factuality.","  Our goal is to create an interactive natural language interface that efficiently and reliably learns from users to complete tasks in simulated robotics settings. We introduce a neural semantic parsing system that learns new high-level abstractions through decomposition: users interactively teach the system by breaking down high-level utterances describing novel behavior into low-level steps that it can understand. Unfortunately, existing methods either rely on grammars which parse sentences with limited flexibility, or neural sequence-to-sequence models that do not learn efficiently or reliably from individual examples. Our approach bridges this gap, demonstrating the flexibility of modern neural systems, as well as the one-shot reliable generalization of grammar-based methods. Our crowdsourced interactive experiments suggest that over time, users complete complex tasks more efficiently while using our system by leveraging what they just taught. At the same time, getting users to trust the system enough to be incentivized to teach high-level utterances is still an ongoing challenge. We end with a discussion of some of the obstacles we need to overcome to fully realize the potential of the interactive paradigm."
"% ============== version 5.0 ================= Intent detection, fundamental component task-oriented dialogue system , increasingly raising attention Multi-Label Classification problem , since single utterance often carries multiple user intents . In real-world scenarios, intent detection often suffers lack training data, dialogue tasks/domains change rapidly new domains usually contain data examples. Recent success Few-Shot Learning presents promising solution data scarcity challenges. It provides human-like learning paradigm generalizes learning examples exploiting prior experience. % old domains. %For multi-label intent detection, state-of-the-art works adopt ``one-vs-rest'' strategy convert multi-class classification binary-class classifications . State-of-the-art works multi-label intent detection focus threshold-based strategy, common practice estimating label-instance relevance scores picking intent labels score higher threshold value . Usually, coordination respective quality two modules, i.e. thresholding relevance scoring, crucial performance MLC models. However, few-shot scenarios, multi-label setting poses unique challenges threshold estimation label-instance relevance scoring. For thresholding, previous works explore tune fixed threshold learn thresholds data . But, thresholds work well learning examples sufficient. In few-shot scenarios, pretty hard determine appropriate thresholds examples. %In few-shot scenarios, pretty hard determine appropriate thresholds %with examples. %without overfitting limited examples. % limited examples. %For few-shot scenarios, pretty hard determine appropriate thresholds examples. Besides, also difficult directly transfer pre-learned thresholds due domain differences, differences label number per instance, score density scale. Estimation label-instance relevance scores also challenging. %It also challenging compute label-instance relevance scores. Few-shot learning achieved impressive progress similarity-based methods , relevance scores modeled label-instance similarities. And label representations obtained corresponding support examples. Unfortunately, despite huge success previous single-label tasks, similarity-based methods become impractical multi-label problems. When instances multiple labels, representations different labels may obtained support examples become confused other. For example Fig , intents query\_time query\_loc share support example thus label representation, %Such confused label representations makes impossible predict correct labels similarity scores. %In situations, vanilla similarities assign query x equal score query\_time query\_loc In paper, study few-shot learning problem multi-label intent detection propose novel framework tackle challenges thresholding label-instance relevance scoring. To solve thresholding difficulties prior-knowledge transferring domain adaption limited examples, propose Meta Calibrated Threshold mechanism first learns universal thresholding experience data-rich domains, adapts thresholds certain few-shot domains Kernel Regression based calibration. Such combination universal training domain-specific calibration allows estimate threshold using prior domain experience new domain knowledge. %Here, non-parametric learning method, Kernel Regression allows alleviate overfitting calibrating thresholds without finetuning. To tackle challenge confused label representation relevance scoring, propose Anchored Label Representation obtain well-separated label representations. Inspired idea embedding label name anchor points refine representation space , ALR uses embeddings label names additional anchors represents label support examples corresponding anchors. Different previous single-label intent detection uses label embedding additional features , label embeddings unique effects separating different labels metric space. Finally, encourage better coordination thresholding label-instance relevance scoring, introduce Logit-adapting mechanism MCT automatically adapts thresholds different score densities. Experiments two datasets show methods significantly outperform strong baselines. Our contributions summarized follows: We explore few-shot multi-label problem intent detection task-oriented dialogue, also early attempt few-shot multi-label classification. We propose Meta Calibrated Threshold mechanism Kernel Regression Logits Adapting estimates threshold using prior domain experience new domain knowledge. We introduce Anchored Label Representation obtain well-separated label representation better label-instance relevance scoring. %% ============== version 4.0 ================= %Intent detection, fundamental component task-oriented dialogue system , increasingly raising attention Multi-Label Classification problem , since single utterance often carries multiple user intents . %In real-world scenarios, intent detection often suffers lack training data, dialogue tasks/domains change rapidly new domains usually contain data examples. %Recent success Few-Shot Learning presents promising solution data scarcity challenges. %It provides human-like learning paradigm generalizes learning examples exploiting prior experience. %% old domains. % %%For multi-label intent detection, state-of-the-art works adopt ``one-vs-rest'' strategy convert multi-class classification binary-class classifications . %State-of-the-art works multi-label intent detection focus threshold-based strategy, common practice estimating label-instance relevance scores picking intent labels score higher threshold value . %Usually, coordination respective quality two modules, i.e. thresholding relevance scoring, crucial performance MLC models. %However, few-shot scenarios, multi-label setting poses unique challenges threshold estimation label-instance relevance scoring. % %For thresholding, previous works explore tune fixed threshold learn thresholds data . %But, thresholds work well learning examples sufficient. %In few-shot scenarios, pretty hard determine appropriate thresholds without overfitting. %% limited examples. %%For few-shot scenarios, pretty hard determine appropriate thresholds examples. %Besides, also difficult directly transfer pre-learned thresholds due domain differences, differences label number per instance, score density scale. % % % %It also challenging compute label-instance relevance scores. %Few-shot learning achieved impressive progress similarity-based methods , relevance scores modeled label-instance similarities. %And label representations obtained corresponding support examples. %Unfortunately, despite huge success previous single-label tasks, similarity-based methods become impractical multi-label problems. %When instances multiple labels, representations different labels may obtained support examples become confused other. %For example Fig , intents query\_time query\_loc share support example thus label representation, %%Such confused label representations %which makes impossible predict correct labels similarity scores. %%In situations, vanilla similarities assign query x equal score query\_time query\_loc % %In paper, study few-shot learning problem multi-label intent detection propose novel framework tackle challenges thresholding label-instance relevance scoring. % %To solve thresholding difficulties prior-knowledge transferring overfitting, propose Meta Calibrated Threshold mechanism first learns universal thresholding experience data-rich domains, adapts thresholds certain few-shot domains Kernel Regression based calibration. %Here, non-parametric learning method, Kernel Regression allows avoid overfitting calibrating thresholds without finetuning. % %To tackle challenge confused label representation relevance scoring, propose Anchored Label Representation obtain well-separated label representations. %Inspired idea embedding label name anchor points refine representation space , ALR uses embeddings label names additional anchors represents label support examples corresponding anchors. %Different previous single-label intent detection uses label embedding additional features , label embeddings unique effects separating different labels metric space. % %Finally, encourage better coordination thresholding label-instance relevance scoring, introduce logit-adapting mechanism MCT automatically adapts thresholds different score densities. % %Experiments two datasets show methods significantly outperform strong baselines. %Our contributions summarized follows: % We explore few-shot multi-label problem intent detection task-oriented dialogue, also early attempt few-shot multi-label classification. % We propose Meta Calibrated Threshold mechanism Kernel Regression Logits Adapting estimates threshold using prior domain experience new domain knowledge. % We introduce Anchored Label Representation obtain well-separated label representation better label-instance relevance scoring. %% ============== version 3.0 EMNLP version ================= % %Intent detection fundamental component task-oriented dialogue system . %In real-word scenarios, intent detection often suffers rapid changing domains, new domains usually lacking data may contain data examples. %Few-Shot Learning promising solution problem. %It provides human-like learning paradigm generalizes learning examples exploiting prior experience old domains. % %In addition data scarcity problem, intent detection also faces problem multi-label prediction. %As shown Fig , single utterance may carry multiple user intents. %For consideration, intent detection needs formulated Multi-Label Classification problem , common practice estimating label-instance relevance scores picking labels score higher threshold value . % %Usually, threshold crucial performance MLC models. %For multi-label intent detection, previous works explore tune fixed threshold learn thresholds data . %However, thresholds work well learning examples sufficient. %For few-shot scenarios, pretty hard determine appropriate thresholds examples. %Also, difficult directly transfer threshold learned data-rich domains due domain differences, differences label number per instance, score density scale. % % % %It also challenging compute label-instance relevance scores few-shot MLC. %Previous few-shot research mainly focuses single label classification achieved impressive progress similarity-based methods . %Generally, methods first obtain per class representations examples , classify instance according similarity representation class. %However, similarity scores rely well-separated class representations, poses unique challenges multi-label settings. %When instances multiple labels, representations different labels may obtained support examples become confused other. %For example Fig , intents query\_time query\_loc share support example thus label representation. % %In paper, study few-shot learning problem multi-label intent detection . %As mentioned above, difficult estimate transfer thresholds few-shot MLC. %To solve this, first learn universal thresholding experience data-rich domains, exploit experience estimate appropriate thresholds unseen few-shot domains. %Specifically, propose Meta Calibrated Threshold , first learns domain-general meta threshold, learns calibrate fit specific domains Kernel-Regression. %To encourage threshold generalization, introduce logit-adapting mechanism automatically adapts meta thresholds different score densities. % %For computing label-instance score few-shot MLC, propose Anchored Label Representation obtain well-separated label representations. %Inspired idea embedding label name anchor points refine representation space , ALR uses embeddings label names additional anchors represent label support examples corresponding anchors. % %Experiments two datasets show methods significantly outperform strong baselines. %Our contributions summarized follows: % We explore few-shot multi-label problem intent detection task-oriented dialogue, %which also early attempt few-shot multi-label classification. % We propose Meta Calibrated Threshold mechanism estimate threshold using prior domain experience new domain knowledge. % We introduce Anchored Label Representation obtain well-separated label representation better label-instance relevance score calculation. We employed copy mechanism address lexical cohesion problem document-level NMT. Our model computes copy probability weights words copy referring preceding source sentences translation outputs. Experiments Japanese English translation indicated model effective improve lexical cohesion, compared strong context-aware NMT models. As future work, intend evaluate effectiveness model various language pairs domains, English-French English-Russian; news novels. Also, improve weighting method copy words avoid copying inappropriate words."," % ========== Version 6.0 ============= In this paper, we study the few-shot multi-label classification for user intent detection.  For multi-label intent detection, state-of-the-art work estimates label-instance relevance scores and uses a threshold to select multiple associated intent labels.  To determine appropriate thresholds with only a few examples, we first learn universal thresholding experience on data-rich domains, and then adapt the thresholds to certain few-shot domains with a calibration based on non-parametric learning. %on metric learning. %, that does not require fine tuning to avoid overfitting. %Kernel Regression here allows to avoid overfitting by calibrating threshold without finetuning. For better calculation of label-instance relevance score, we introduce label name embedding as anchor points in representation space, which refines representations of different classes to be well-separated from each other. Experiments on two datasets show that the proposed model significantly outperforms strong baselines in both one-shot and five-shot settings.\footnote{Data and code are available at \url{https://github.com/AtmaHou/FewShotMultiLabel}}   %% ========== Version 5.0 ============= %In this paper, we study the few-shot multi-label classification for user intent detection.  %For multi-label intent detection, state-of-the-art work estimates label-instance relevance scores and uses a threshold to select multiple associated intent labels.  %To determine appropriate thresholds with only a few examples, we first learn universal thresholding experience on data-rich domains, and then adapt the thresholds to certain few-shot domains with a calibration based on Kernel Regression, that does not require fine tuning to avoid overfitting. %%Kernel Regression here allows to avoid overfitting by calibrating threshold without finetuning. %For better calculation of label-instance relevance score, we introduce label name embedding as anchor points in representation space, which refines representations of different classes to be well-separated from each other. %Experiments on two datasets show that our model significantly outperforms strong baselines in both one-shot and five-shot settings.\footnote{Data and code are available at \url{https://anonymous.com}}  %% ========== Version 5.0 ============= %In this paper, we study the few-shot multi-label classification for user intent detection.  %For multi-intent detection, state-of-the-art work estimates label-instance relevance scores and uses a threshold to select multiple associated intent labels.  %To determine appropriate thresholds with only a few examples, we first learn universal thresholding experience on data-rich domains, and then adapt the thresholds to certain few-shot domains with a Kernel Regression based calibration.  %Kernel Regression here allows to avoid overfitting by calibrating threshold without finetuning. %For better calculation of label-instance relevance score, we introduce label name embedding as anchor points in representation space, which refine representations of different classes to be well-separated from each other. %Experiments on two datasets show that our model significantly outperforms strong baselines in both one-shot and five-shot settings.\footnote{Code is available at \url{https://anonymous.com}}  %% ========= version 4.0 EMNLP version ========= %In this paper, we study the few-shot multi-label classification for user intent detection.  %Multi-label classification usually estimates label-instance relevance scores and uses a threshold to select multiple associated labels.  %To determine appropriate thresholds with only a few examples, we first learn universal thresholding experience on data-rich domains, and then calibrate the learned universal thresholds to fit certain few-shot domains. %For better calculation of label-instance relevance score, we introduce label name embedding as anchor points in representation space, which refine representations of different classes to be well-separated from each other. %Experiments on both open and in-house datasets show that our model significantly outperforms strong baselines in both one-shot and five-shot settings.\footnote{Code is available at: \url{https://anonymous.com}}"
"% % The following footnote without marker needed camera-ready % version paper. % Comment instructions uncomment 8 lines % ""final paper"" variant English. % . % % final paper: en-us version % % % space normally used marker % This work licensed Creative Commons % Attribution 4.0 International License. % License details: % \url{http://creativecommons.org/licenses/by/4.0/}. } Translation languages grammatical gender involves correctly inferring grammatical gender entities sentence. In languages grammatical gender dependent social gender human referents. For example, Spanish translation sentence `This doctor', `the doctor' would either `el mico', masculine, `la mica', feminine. Since noun refers person grammatical gender inflection correct given referent. In practice many NMT models struggle generating inflections correctly , often instead defaulting gender-based social stereotypes masculine language . For example, NMT model might always translate `This doctor' sentence masculine inflected noun: `Este es el mico'. Such behaviour viewed translations exhibiting gender bias. By `bias' follow definition behaviour `systematically unfairly discriminate[s] certain individuals groups individuals favor others.' Specifically, translation performance favors referents fitting groups corresponding social stereotypes, male doctors. Such systems propagate representational harm erasure referents -- example, non-male doctor would incorrectly gendered example translation. Systems may also cause allocational harms incorrect translations used inputs systems . System users also experience representational harms via reinforcement stereotypes associating occupations particular gender . Even referent, user may wish words translated way appear endorse social stereotypes. Users also experience lower quality service receiving grammatically incorrect translations. A common approach broad problem NMT use gender features, implicit explicit. The gender one words test sentence determined external context reliance `gender signals' words source sentence gendered pronouns. That information used translating. Such approaches combine two distinct tasks: identifying gender inflection feature, applying translate words source sentence. These feature-based approaches make unstated assumption could correctly identify that, e.g., doctor example female, could inflect entities sentence correctly, reducing effect gender bias. Our contribution exploration assumption. We propose scheme incorporating explicit gender inflection tag NMT, particularly translating coreference sentences reference gender label known. Experimenting translation English Spanish English German, find simple existing approaches overgeneralize gender signal, incorrectly using inflection every entity sentence. We show tagged-coreference adaptation approach effective combatting behaviour. Although work English source sentences extend prior work, note approach extended source languages without inherent gender signals like gendered pronouns, unlike approaches rely signals. Intuitively, gender tagging perform well use label determined human coreference resolution, even less useful gender label must automatically inferred. Conversely, gender tagging effective scenario may beneficial user specify gendered language use referent, Google Translate's translation inflection selection , translations grammatical gender use human referents known. We also find approach works well RoBERTa-based gender tagging English test sentences. Existing work NMT gender bias focused translation sentences based binary gender signals, exclusively male female personal pronouns. This excludes erases use binary gendered language, including limited non-binary individuals . As part work therefore explore applying tagging indicate gender-neutral referents, produce WinoMT set assess translation coreference sentences gender-neutral entities. \subsection{Related work} Variations gender tag signal machine translation proposed several forms. incorporate `speaker gender' tag training data, allowing gender conveyed sentence level. However, allow fine-grained control, example one referent sentence. Similar approaches infer use gender information discourse context. also incorporate single explicit gender feature sentence inference. integrate coreference links machine translation reranking improve pronoun translation cross-sentence context. propose NMT gender bias reduction `mixing signals' addition pro-stereotypical adjectives. Also related work recent approach , train NMT models scratch source language words annotated target language grammatical gender. In treat gender bias domain adaptation problem adapting small set synthetic sentences equal numbers entities using masculine feminine inflections. We also interpret gender `tagging' approach, since gendered terms synthetic dataset give strong signal model. In work extend synthetic datasets work explore effect further. Other approaches reducing gender bias effects involve adjusting word embeddings either directly training counterfactual data augmentation . We view approaches orthogonal proposed scheme: similar goals directly control inference-time gender inflection word sentence level. In paper, explore few-shot learning problem multi-label intent detection. To estimate reasonable threshold support examples, propose Meta Calibrated Threshold adaptively combines prior experience domain-specific knowledge. To obtain label-instance relevance score few-shot setting, introduce metric learning based method Anchored Label Representation. It provides well-separated label representations label-instance similarity calculation. Experiment results validate Meta Calibrated Threshold Anchored Label Representation improve few-shot multi-label intent detection."," Neural Machine Translation  has been shown to struggle with grammatical gender that is dependent on the gender of human referents, which can cause gender bias effects. Many existing approaches to this problem seek to control gender inflection in the target language by explicitly or implicitly adding a gender feature to the source sentence, usually at the sentence level.    In this paper we propose schemes for incorporating explicit word-level gender inflection tags into NMT. We explore the potential of this gender-inflection controlled translation when the gender feature can be determined from a human reference, or when a test sentence can be automatically gender-tagged, assessing on English-to-Spanish and English-to-German translation.  We find that simple existing approaches can over-generalize a gender-feature to multiple entities in a sentence, and suggest effective alternatives in the form of tagged coreference adaptation data. We also propose an extension to assess translations of gender-neutral entities from English given a corresponding linguistic convention, such as a non-binary inflection, in the target language."
"Self-supervised pretraining language modeling massive datasets revolutionized NLP. One reason method works pretraining shapes model's hypothesis space, giving inductive biases help learn linguistic tasks . Numerous probing studies provided support idea showing language models learn representations encode linguistic features . However, feature learning first step acquiring helpful inductive biases. Models must also able learn features matter. The NLU datasets models often fine-tuned ambiguous contain artifacts, often support multiple possible generalizations. Neural networks mind readers: Models shown represent linguistic features sometimes fail use fine-tuning NLU tasks, instead adopting shallow surface generalizations . To end, recent work probing pretrained models advocates shifting focus study away whether represent linguistic features favor whether learn useful representations features . % } \end{table*} We investigate RoBERTa acquires language-specific inductive biases self-supervised pretraining. We track separately RoBERTa's representation linguistic features preferences linguistic generalizations surface generalizations change amount pretraining data increases. We pretrain RoBERTa scratch datasets ranging 1M 1B words evaluate models alongside RoBERTa series experiments probe inductive biases pretrained model time fine-tuning downstream task. We probe models three kinds experiments: First, conduct control experiments fine-tune models unambiguous binary classification tasks test whether learn represent simple linguistic surface features. Second, conduct ambiguous experiments following poverty stimulus design , illustrated Figure . In experiments, fine-tune pretrained model ambiguous binary classification task training set consistent linguistic generalization surface one. We test classifier disambiguating data reveal generalization model adopted, extension preference among two features. Third, conduct inoculation experiments \citep[following][]{liu2019inoculation} test hard sway model surface bias adopt linguistic generalization. We introducing small amounts disambiguating data otherwise ambiguous training set. We automatically generate data tasks, call resulting dataset \dataset\ , pronounced ``messages''. The results show RoBERTa acquires stronger linguistic bias pretraining increases. RoBERTa strongest linguistic bias, requires little inoculating data reliably make linguistic generalization. In general, models pretraining data generally induced adopt linguistic generalizations less inoculating data. We also find large gap amount pretraining data RoBERTa needs learn linguistic features necessary generalize out-of-domain amount needs learns prefer features generalizing. The control experiments unambiguous data reveal models little pretraining actually represent linguistic features, nonetheless show strong surface bias. In words, main contribution pretraining linguistic bias learning devoted extracting features, learning features matter. We conclude helpful inductive biases learned pretraining, current models require abundant data so. The implications conclusion point two directions: First, probably continue pretrain increasingly massive training sets improve generalization few-shot learning abilities models like T5 GPT-3 . Second, since models learn useful features early, hope future advances could accelerate reducing amount data needed learn features matter. To aid effort, release MSGS dataset, pretrained RoBERTas, code: \href{https://github.com/nyu-mll/msgs}{\url{https://github.com/nyu-mll/msgs}}. Tagging words target language gender inflection powerful way improve accuracy translated inflections. This could applied cases correct grammatical gender use given referent known, monolingual coreference resolution tools improve sufficiently used automatic tagging. It also potential application new inflections defined gender-neutral language. However, risk gender features used over-general way. Providing strong gender signal one entity potential harm users referents erasing entities sentence, unless model specifically trained translate sentences multiple entities. In particular find V3 system, trained multiple-entity translation examples, allows good performance minimizing peripheral effects. We conclude emphasising work gender coreference translation requires care ensure effects interventions intended, well testing scenarios capture full complexity problem, work impact gender bias.","   One reason pretraining on self-supervised linguistic tasks is effective is that it teaches models features that are helpful for language understanding. However, we want pretrained models to learn not only to represent linguistic features, but also to use those features preferentially during fine-turning. With this goal in mind, we introduce a new English-language diagnostic set called MSGS , which consists of 20 ambiguous binary classification tasks that we use to test whether a pretrained model prefers linguistic or surface generalizations during fine-tuning. We pretrain RoBERTa models from scratch on quantities of data ranging from 1M to 1B words and compare their performance on \dataset\ to the publicly available RoBERTa$\subtxt{BASE}$. We find that models can learn to represent linguistic features with little pretraining data, but require far more data to learn to prefer linguistic generalizations over surface ones. Eventually, with about 30B words of pretraining data, RoBERTa$\subtxt{BASE}$ does demonstrate a linguistic bias with some regularity. We conclude that while self-supervised pretraining is an effective way to learn helpful inductive biases, there is likely room to improve the rate at which models learn which features matter."
". % % % final paper: en-us version % % % space normally used marker % This work licensed Creative Commons % Attribution 4.0 International License. % License details: % \url{http://creativecommons.org/licenses/by/4.0/}. } \reza{ Neural models revolutionising machine translation , achieved state-of-the-art many high-resource language pairs . However, scarcity bilingual parallel corpora still major challenge training high-quality NMT models % especially broad range languages available translation training resources small used existing NMT systems . % Transfer learning fine-tuning, model trained high-resource language-pair, % \wray{Wray: Having trouble this: ""high-resource language-pair"" mean source target high resource relate do?} standard approach tackle scarcity data target low-resource language-pair . % However, one-to-one approach, able exploit models trained multiple high-resource language-pairs target language-pair interest. % Furthermore, models transferred different high-resource language-pairs may complementary syntactic and/or semantic strengths, hence using single model may sub-optimal. } %Transfer learning one widely used solutions addressing data scarcity problem low-resource scenarios . % However, applying original transfer learning LR models neither able make full use highly related multiple high-resource languages receive different parameters effective high-resource NMT models simultaneously. %However, transfer learning high-resource low-resource NMT models generally one-to-many approach able exploit multiple high-resource languages high-resource NMT models' parameters simultaneously. Contrariwise, \reza{ Another appealing approach multilingual NMT, whereby single NMT model trained combining data multiple high-resource low-resource language-pairs . % %is appealing approach low-resource languages utilizing training examples multiple languages . %In practice, training multilingual NMT, multilingual vocabulary set language pairs used training single NMT model among languages enable sharing resources high-resource low-resource languages. % improves regularization model avoiding over-fitting limited data low-resource languages. However, performance multilingual NMT model highly dependent types languages used train model. Indeed, languages distant language families, lead negative transfer, causing low translation quality multilingual system compared counterparts trained individual language-pairs . % To address problem, proposed knowledge distillation approach effectively train multilingual model, % selectively distilling knowledge individual teacher models multilingual student model. However, still language pairs trained single model blind contribution training. %during training process accuracy individual models surpasses multilingual one. % distilling knowledge individual NMT models. To avoid distilling knowledge effective teachers, selectively apply distillation training process accuracy individual models surpasses multilingual one. } \reza{ In paper, propose many-to-one transfer learning approach effectively transfer models multiple high-resource language-pairs target low-resource language-pair interest. % As fine-tuned models different high-resource language pairs complementary syntactic and/or semantic strengths target language-pair, idea distill knowledge single student model make best use teacher models. % We propose effective adaptive knowledge distillation approach dynamically adjust contribution teacher models distillation process, enabling making best use teachers ensemble. % Each teacher model provides dense supervision student via dark knowledge using mechanism similar label smoothing , amount smoothing regulated teacher. % In AKD approach, label smoothing coming different teachers combined regulated, based loss incurred teacher models distillation process. % %\wray{Wray: This next sentence could deleted need space.} %Although focus application method NMT, applied generally NLP tasks suffering scarcity training data, e.g. summarisation {CITE} question answering \todo{CITE}. } %Experimental results various teacher-student language pairs show 0.9 BLEU score improvement compare strong baselines. Experiments transferring collection six language pairs IWSLT five low-resource language-pairs TED Talks demonstrate effectiveness approach, achieving +0.9 BLEU score improvements compared strong baselines. %\todo{talk experiments?} %In paper, introduce new distil-based approach make full use high-resource languages % NMT models simultaneously effectively. To so, firstly apply transfer learning high-resource low-resource languages generate strong teachers. Then, adaptively distil knowledge multiple teachers based effectiveness %to improve accuracy low-resource NMT model. % What distinguishes approach previous distil-based method choosing best teachers statistically rather deterministically. Our approach weights teachers based context mini-batch ability teacher improve prediction student specific mini-batch training. % Our experiments show proposed approach outperforms vanilla transformer, original transfer learning, multilingual NMT, selective knowledge distillation translation five low-resource languages English. %Our main contributions follows: % a) We % propose new approach % transfer knowledge high-resource low-resource language pairs assumes availability translation models high-resource bilingual data low-resource languages leads best usage computational resources via exploiting computational work already done high-resource side. % , particularly interesting limitation available computational resources. % b) We % propose new method % dynamically distil knowledge existing teacher models student model. What distinguishes approach previous distillation-based methods choosing best teachers statistically based data knowledge gap student model, rather deterministically done previous work . % c) Experimental results various teacher-student language pairs show 0.9 BLEU score improvement compare strong baselines. % In paper, propose task Video-based Multimodal Summarization Multimodal Output chooses proper video cover generates appropriate textual summary video-attached article. We propose model named Dual-Interaction-based Multimodal Summarizer including local conditional self-attention mechanism global-attention mechanism jointly model summarize multimodal input. Our model achieves state-of-the-art results terms autometrics outperforms human evaluations large margin. In near future, aim incorporate video script information multimodal summarization process."," \reza{ Scarcity of parallel sentence-pairs poses a significant hurdle for training high-quality Neural Machine Translation  models in bilingually low-resource scenarios.  % A standard approach is transfer learning, which involves taking a model trained on a high-resource language-pair and fine-tuning it on the data of the low-resource MT condition of interest.  % However, it is not clear generally which high-resource language-pair offers the best transfer learning for the target MT setting. Furthermore, different transferred models may have complementary semantic and/or syntactic strengths, hence using only one model may be sub-optimal.      % In this paper, we tackle this problem using knowledge distillation, where we propose to distill the knowledge of ensemble of teacher models to a single student model.  % As the quality of these teacher models varies, we propose an effective adaptive knowledge distillation approach to dynamically adjust the contribution of the teacher models during the distillation process.  % Experiments on transferring from a collection of six language pairs from IWSLT to five low-resource language-pairs from TED Talks demonstrate the effectiveness of our approach, achieving up to +0.9 BLEU score improvement compared to strong baselines.  } %In this paper, we propose a two-phase method  to tackle this challenge. The first phase involves transfer learning, where models trained on high-resource languages-pairs are fine-tuned on the data of the low-resource MT condition of interest.  % %The second phase involves disstilling the knowledge from this collection of teachers to a single student model.  % %As the quality of these teacher models vary, we propose an adaptive knowledge distillation approach to adaptively adjust the contribution of the teacher models during the training process of the student.  % %NMT models, where a pretrained modeld on high-resource data is fine tuned on .  The transferred models are treated as teachers which produce soft targets for each low-resource language. In the second phase, we adaptively distil knowledge from all teachers based on their capability to improve the accuracy of the low-resource NMT model . By optimizing the student to fit the teachers' distribution over smoothed labels, we expect the student generalisation affected by teachers' probability calibration. Moreover, we propose to control the teachers' contributions when computing the soft targets for knowledge distillation, such that better teachers contribute more. This contribution is adaptively changing based on how good a teacher captures the context of an incoming mini-batches during training. Experiments on IWSLT and TED dataset demonstrate the effectiveness of our model which outperforms strong baselines on the translation of five low-resource languages to English."
"Natural language processing deception detection focus preprocessing text computational data required features propose. As deception detection understanding meaning text text viewed people, sequence text always considered one primary source context. For example, N-gram, representative method natural language processing, contains data word subsequent word statistical probabilities. The attribute subsequent contains continuous context text, linguist describes linearity. In contrast, feature extractions without considering language's linearity seems nonsense. However, data processed non-linear feature extractions shows notable accuracy detecting deceptions, possible suggest preprocessing methods could used one possible natural language processing certain situations. \ In paper, discuss effectiveness APV, simple natural language processing method using alphabet frequency, context application fake news detection. By using deep learning algorithm fake news dataset Kaggle, findings suggest simple deep learning algorithms using APV pre-processing method could show prominent accuracy predicting deception text. \ In section 2, investigate conventional natural language processing used machine learning deep learning algorithms. In section 3, define APV mathematical structure. We also discuss hypothesis might improve feature extraction APV. In section 4, basic experiment protocol set including structure deep learning algorithms performance metrics used experiment. In section 5, present result algorithms performance. Finally, section 6, conclude study. In paper, present adaptive knowledge distillation approach improve NMT low-resource languages. We address inefficiency original transfer learning multilingual learning making wiser use high-resource languages models effective collaborative learning manner. Our approach shows effectiveness translation low-resource languages especially complementary knowledge multiple high-resource languages linguistic family explicitly clear language impact every mini-batch low-resource training data. Experiments translation five extremely low-resource languages English show improvements compared strong baselines.","     Feature extraction is an important process of machine learning and deep learning, as the process make algorithms function more efficiently, and also accurate. In natural language processing used in deception detection such as fake news detection, several ways of feature extraction in statistical aspect had been introduced . In this research, it will be shown that by using  deep learning algorithms and alphabet frequencies of the original text of a news without any information about the sequence of the alphabet can actually be used to classify fake news and trustworthy ones in high accuracy . As this pre-processing method makes the data notably compact but also include the feature that is needed for the classifier, it seems that alphabet frequencies contains some useful features for understanding complex context or meaning of the original text.\\\\  keywords: {[FEATURE EXTRACTION], [DEEP LEARNING]}  % Received, Accepted    ."
"Sentence matching fundamental technology natural language processing. Over past years, deep learning data-driven technique yielded state-of-the-art results sentence matching . However, data-driven technique typically requires large amounts manual annotation brings much cost. If large labeled data can't obtained, advantages deep learning significantly diminish. To alleviate problem, active learning proposed achieve better performance fewer labeled training instances . Instead randomly selecting instances, active learning measure whole candidate instances according criteria, select efficient instances annotation . However, previous active learning approaches natural language processing mainly depend entropy-based uncertainty criterion , ignore characteristics natural language. To specific, ignore linguistic similarity, may select redundant instances waste many annotation resources. Thus, devise linguistic criteria measure candidate instances important challenge. Recently, pre-trained language models shown powerful learning language representation. Accordingly, pre-trained language models may provide reliable way help capture language characteristics. In paper, devise linguistic criteria pre-trained language model capture language characteristics, utilize extra linguistic criteria enhance active learning. It shown Figure . Experiments English Chinese sentence matching datasets demonstrate pre-trained language model enhance active learning. As current natural language processing suggests, ideal natural language considered strict linear order. However, paper, suggest even natural sequence data excluded feature extraction text, possible use data classify text high accuracy. And consider accuracy fair trade-off accuracy pre-processing effort, APV shrinks data approximately size still obtained 92\ accuracy deep learning algorithm reported . It sure whether APV capable summarizing text, however seems possible use APV supervised learning regarding natural language. We planning use proposed method classify 2 classes, also hopefully find mathematical explanations method works improve feature extraction results listed paper."," Active learning is able to significantly reduce the annotation cost for data-driven techniques. However, previous active learning approaches for natural language processing mainly depend on the entropy-based uncertainty criterion, and ignore the characteristics of natural language. In this paper, we propose a pre-trained language model based active learning approach for sentence matching. Differing from previous active learning, it can provide linguistic criteria to measure instances and help select more efficient instances for annotation. Experiments demonstrate our approach can achieve greater accuracy with fewer labeled training instances."
"%text matchingep learning The neural networks represent two sentences individually dense vector embedding space, define different functions calculate matching degree two-sentence vectors. However, getting extremely time-consuming networks becoming sophisticated introducing parameters. Even worse, still black box researchers practitioners, urgent need interpretability. We can't figure what's specific meaning representation obtained neural networks, unaccountable challenging comprehend lead untrusty irresponsible result. %deep learningtric learnetric learningtric learning To tackle these, aim find fast interpretable approach sentence matching. There several studies focused learning low-dimensional representations data, called metric learning even combine similarity metrics ranking tasks . Moreover, researchers apply metric learning principles design loss function information retrieval question-answering tasks. But deep metric learning utilized, neural network part still demands lot time. It hardly runs memory-limited device, together high energy consumption. %ext matchingpply It considering unexplainable implications brought neural networks, fairness transparency, challenge time-consuming. In paper, apply metric learning approaches address problems mentioned above. Because metric learning advantage time memory usage large-scale high-dimensional datasets compared methods above. Here, metric learning finds representation data preserves constraints placed human-provided labels. Building success learning ``label constraint preserving'' representations, low-distortion embeddings, explore two \textsf{F}ast, \textsf{I}nterpretable, \textsf{L}ow-rank \textsf{M}etric learning approaches, called \textsf{FILM}. %metric learning Notably, explore \textsf{FILM} methods text matching tasks, also known semantic equivalence problem IR community~. To specific, one based interpretable low-rank manifold optimization method. To solve optimization problem, apply Cayley transformation method Barzilai-Borwein step size. After trained task, added kNN index prediction efficient retrieval. The input question encoded used query index, returning top k similar questions. We test approaches data Quora Challenge SemEval-2017 Semantic Textual Similarity Task, provide pairwise sentence similarity labels. %\footnote{} %Our motivation investigate whether \textsf{FILM} approaches perform well as, better than, ``black box'' approaches popular days. The rest paper organized follows. In Section , provide quick overview metric learning. In Section present interpretable \textsf{FILM} method. In Section , summarize Quora dataset task, explain \textsf{FILM} applied task, summarize deep neural network approach. In Section report results. \end{comment} In paper, combine active learning pre-trained language model. We devise extra linguistic criteria pre-trained language model, capture language characteristics enhance active learning. Experiments show proposed active learning approach obtains better performance."," Detection of semantic similarity plays a vital role in sentence matching. It requires to learn discriminative representations of natural language. Recently, owing to more and more sophisticated model architecture, impressive progress has been made, along with a time-consuming training process and not-interpretable inference. % In sentence matching and semantic analysis, detecting semantic similarity is a challenge that requires learning discriminative representations of natural language. Recent advances in the deep neural network enable us to learn semantic representation, but are getting time-consuming and fail in interpretation. To alleviate this problem, we explore a metric learning approach, named \textsf{FILM}  to efficiently find a high discriminative projection of the high-dimensional data. We construct this metric learning problem as a manifold optimization problem, and solve it with the Cayley transformation method with Barzilai-Borwein step size. % To alleviate this problem, in this paper we construct sentence matching as a manifold optimization problem that learns a distance function between sentences. % % and obtain the semantic representation by learning a similarity or distance function. % We explore a metric learning approach, named \textsf{FILM}  to efficiently find a high discriminative projection of the high-dimensional data. % that still preserves high discriminative power. % To this end, our manifold optimization method is solved by the Cayley transformation method with Barzilai-Borwein step size.  In experiments, we apply \textsf{FILM} with triplet loss minimization objective to the Quora Challenge and Semantic Textual Similarity  Task. The results demonstrate that the \textsf{FILM} method achieves a superior performance as well as the fastest computation speed, which is consistent with our theoretical analysis of time complexity."
"%A common situation language learners encounter unrecognized words. %In case, looking dictionary may preferred solution many people. %However, capacity dictionaries limited, may contain new words new meanings words. %What's more, language pairs dictionaries, especially low resources. %Therefore, may good idea directly generate definitions words. The definition modeling task proposed \citet{Noraset2017DefinitionML} generate dictionary definition specific word. This task prove useful language learners, provide reading help giving definitions words text. However, definition modeling work specific language, puts high demands users requires read definitions written language. Besides, many low-resource languages lack large-scale dictionary data, making difficult train definition generation models languages. %This task prove useful language learners, provide reading help giving definitions words text. %However, definition modeling work specific language, puts high demands users requires read definitions written language. Therefore, emphasize necessity generating definitions cross-lingually, generate definitions various language inputs, illustrated figure . Since English widely used around world, English dictionary resources relatively easy obtain, choose generate definitions English. In way, cross-lingual model trained English directly applied languages. The challenging issue effectively transfer knowledge definition generation learned English languages. To solve problem, propose employ cross-lingual pretrained language models encoders. These models shown able encode sequences various languages, enables ability cross-lingual transfer . %In work, emphasize necessity generating definitions cross-lingually, requires model generate definitions one language words various languages illustrated figure . %Considering English widely used around world, English dictionary resources relatively easy obtain, choose use English generate definitions languages work. %Recently, cross-lingual pretrained language models shown capable encoding sequences different languages vector space, enables ability cross-lingual transfer. %Therefore, propose employ encoders cross-lingual definition generation. %After training fine-tuning model English dataset, directly apply obtained model generate definitions languages. To verify proposed method, build English dataset model training Chinese dataset zero-shot cross-lingual evaluation. %We collected English words, example sentences definitions OALD English dataset, collected Chinese words, example sentences English definitions Chinese WordNet Chinese dataset. Experiments manual analyses constructed datasets show proposed models good cross-lingual transfer ability. Compared reference definitions CWN dataset, although generated definitions still insufficient accuracy, fluency already good enough. Furthermore, considering generated definitions provided language learners, many non-English native speakers, argue difficulty definitions control. We control lexical complexity generated definitions limiting definitions training set Oxford 3000 vocabulary, list important useful words carefully selected language experts experienced teachers . %These words used write definitions Oxford Advanced Learner's Dictionary , order make easy understand. %We compute Type/Token Ratio measure lexical complexity. %The TTR generated definitions much lower reference definitions , indicates lower lexical complexity. We compute four different metrics measure lexical complexity. Definitions generated models outperform reference definitions four metrics large margin. The result shows method generate simpler definitions, suitable language learners. We investigated text matching, core task information retrieval semantic analysis. We introduced notation definition metric learning, applied text matching. Then, explored \textsf{FILM} , aim reduces time cost memory usage, also save energy consumption. In order solve task efficiently, \textsf{FILM} combined fast approximate k nearest neighbour search index. Compare neural models, method also advantage time memory usage large-scale high-dimensional datasets."," Generating dictionary definitions automatically can prove useful for language learners. However, it's still a challenging task of cross-lingual definition generation. In this work, we propose to generate definitions in English for words in various languages. To achieve this, we present a simple yet effective approach based on publicly available pretrained language models. In this approach, models can be directly applied to other languages after trained on the English dataset. We demonstrate the effectiveness of this approach on zero-shot definition generation. Experiments and manual analyses on newly constructed datasets show that our models have a strong cross-lingual transfer ability and can generate fluent English definitions for Chinese words. We further measure the lexical complexity of generated and reference definitions. The results show that the generated definitions are much simpler, which is more suitable for language learners. %We further conduct a manual analysis of the generated Chinese definitions and find that although these definitions are insufficient on the accuracy, they are already good enough on fluency and lexical complexity."
"The CoNLL 2020 MRP Shared Task combines five frameworks graph-based meaning representation: EDS, PTG, UCCA, AMR DRG. It includes evaluations English, Czech, German Chinese. While EDS, UCCA AMR participated 2019 MRP shared task , focused English, PTG DRG newly-added frameworks MRP uniform format. For shared task, extended TUPA , adapted baseline system 2019 MRP shared task , support two new frameworks different languages. In order add support, minimal changes needed, demonstrating TUPA's strength parsing wide array representations. TUPA general transition-based parser directed acyclic graphs , originally designed parsing UCCA . It previously used baseline system SemEval 2019 Task 1 , generalized support frameworks . We also experimented HIT-SCIR parser . This parser highest average score across frameworks 2019 MRP shared task, also since applied frameworks . \end{adjustbox} \end{figure*} In work, employ pretrained language models, namely mBERT XLM cross-lingual definition generation. In addition, propose use Oxford 3000 vocabulary limit lexical complexity generated definitions. We build OALD dataset monolingual training CWN dataset cross-lingual evaluation. Experiments indicate strong cross-lingual transfer ability proposed method. Furthermore, results lexical complexity shows definitions generated using method simpler reference, suitable language learners. Experiments conducted datasets show effectiveness proposed method. Furthermore, manual analysis performed CWN test set shows although generated definitions insufficient accuracy, already good enough fluency lexical complexity. \clearpage \clearpage \caption{Generated samples} \caption{Hyperparameters used Experiments}","   This paper describes the HUJI-KU system submission to the shared task   on Cross-Framework Meaning Representation Parsing  at the 2020   Conference for Computational Language Learning ,   employing TUPA and the HIT-SCIR parser, which were, respectively,   the baseline system and winning system in the 2019 MRP shared task.   Both are transition-based parsers using BERT contextualized embeddings.   We generalized TUPA to support the newly-added MRP frameworks and languages,   and experimented with multitask learning with the HIT-SCIR parser.   We reached 4th place in both the cross-framework and cross-lingual tracks."
"\renewcommand{\thefootnote}{} Recurrent Neural Network language models shown learn many aspects natural language syntax including number long-distance dependencies representations incremental syntactic state . However, previous studies investigated relationship token's frequency training corpus syntactic properties models learn it. In work, assess neural models' ability make robust syntactic generalizations token's nominal number verbal argument structure based minimal exposure token training. Because Zipfian distribution words corpus, vast majority word types seen handful times training . Therefore, few-shot learning capabilities neural LMs critical robustness NLP system cognitive model. However, human learning goes beyond simply learning syntactic properties particular constructions. People apply properties across different constructions, meaning representations syntactic features word sense invariant grammatical context word. For example, speakers listeners sensitive verb's argument structure relationships easily recognize verb cannot take direct object active, declarative sentences cannot passivized The relationship active sentence passive sentence termed transformation linguistic literature . Many semantic-syntactic rules govern word co-occurrence one form, verb's argument structure relationships, hold uniformly across transformations. It remains open question whether models learn grammatical rules invariant surface realization, property call syntactic invariance. We combine assessment few-shot learning syntactic invariance two grammatical features English: whether noun singular plural whether verb transitive intransitive . We assess whether model able make different predictions based number argument structure simple active voice base context. We assess whether models able make similar distinctions transformed context---passive voice verbs polar questions nouns. In transformed contexts, test models tokens occur base context training. For models succeed transformed contexts must represent syntactic features way invariant specific realization features terms word co-occurrences different constructions. For grammatical feature, introduce suite novel targeted test sentences, similar presented \citet{marvin2018targeted}. We find neural models tested able induce proper syntactic generalizations base transformed contexts two three exposures, whereas baseline -gram model fails learn relevant generalizations. For constructions tested two neural models enhanced explicit structural supervision outperform purely sequence model. Assessing invariance properties, find neural models demonstrate proper behavior transformed contexts, even tokens seen base contexts training. This behavior indicates models able deploy generalizations learned one syntactic context different syntactic environments, key component human linguistic capabilities far untested neural setting. \subsection{Related Work} Bayesian models word learning shown successes acquiring proper syntactic generalizations minimal exposure , however clear well neural network models would exhibit rapid generalizations. Comparing neural network architectures, recent work shown models enhanced explicit structural supervision training produce humanlike syntactic generalizations , remains untested whether supervision helps learn properties tokens occur rarely training. Previous studies found Artificial Neural Networks capable learning argument structure paradigms make correct predictions across multiple frames , however capabilities remain untested incremental language models. Much written ability ANNs learn number agreement , including ability maintain dependency across different types intervening material coordinated noun phrases . \citet{hu2020systematic} find model architecture, rather training data size, may contribute performance number agreement related tasks. Focusing RNN models, \citet{lakretz2019emergence} find evidence number agreement tracked specific ``number"" units work concert units carry general syntactic information like tree depth. \citet{jumelet2019analysing} argue learning dependencies RNNs acquire default form , predicting non-default form requires explicit contrary evidence. Our results support hypothesis. Models accurate singular nouns transitive verbs seen times training, behavior indicates forms expected evidence sparse. We presented TUPA-MRP modified HIT-SCIR parser, constitute HUJI-KU submission CoNLL 2020 shared task Cross-Framework Meaning Representation. TUPA general transition-based DAG parser uniform transition system, easily adaptable multiple frameworks. We used parsing cross-framework cross-lingual tracks, adapting newly introduced frameworks, PTG DRG. HIT-SCIR transition-based parser framework-specific transition systems, adapted year's shared task used English EDS UCCA parsing cross-framework track. The HIT-SCIR parser additionally used experimenting multitask learning, negative results approach. Future work tackle MRP task modern transition-based-like parser architectures, pointer networks , far applied bilexical framworks, i.e., flavor-0 SDP ."," Humans can learn structural properties about a word from minimal experience, and deploy their learned syntactic representations uniformly in different grammatical contexts. We assess the ability of modern neural language models to reproduce this behavior in English and evaluate the effect of structural supervision on learning outcomes. First, we assess few-shot learning capabilities by developing controlled experiments that probe models' syntactic nominal number and verbal argument structure generalizations for tokens seen as few as two times during training. Second, we assess invariance properties of learned representation: the ability of a model to transfer syntactic generalizations from a base context  to a transformed context . We test four models trained on the same dataset: an $n$-gram baseline, an LSTM, and two LSTM-variants trained with explicit structural supervision \citep{dyer2016rnng, charniak2016parsing}. We find that in most cases, the neural models are able to induce the proper syntactic generalizations after minimal exposure, often from just two examples during training, and that the two structurally supervised models generalize more accurately than the LSTM model. All neural models are able to leverage information learned in base contexts to drive expectations in transformed contexts, indicating that they have learned some invariance properties of syntax.\blfootnote{Miguel conducted this work while at IBM Research}"
"Despite \bert{'s} popularity effectiveness, little known inner workings. Several attempts made demystify certain aspects \bert , often leading contradicting conclusions. For instance, \citet{clark-etal-2019-bert} argue attention measures importance particular word computing next level representation word. However, \citet{kovaleva-etal-2019-revealing} showed attention heads contain trivial linguistic information follow vertical pattern , could related under-utilization over-parameterization issues. Other studies attempted link specific \bert heads linguistically interpretable functions , agreeing single head densely encodes enough relevant information instead different linguistic features learnt different attention heads. We hypothesize aforementioned largely contributes lack attention-based explainability \bert. Another open topic knowledge distributed across \bert layers. Most studies agree syntactic knowledge gathered middle layers , final layers task-specific. Most importantly, seems semantic knowledge spread across model, explaining non-trivial tasks better solved higher layers . Driven discussion, propose novel fine-tuning approach different parts \bert guided directly solve increasingly challenging classification tasks following underlying label hierarchy. Specifically, focus Large Scale Multilabel Text Classification documents assigned one labels large predefined set. The labels organized hierarchy general specific concepts. Our approach attempts tie specific \bert layers specific hierarchy levels. In effect, layers responsible predicting labels corresponding level. We experiment two \lmtc datasets several variations structured \bert training. Our contributions are: We propose novel structured approach fine-tune \bert specific layers tied specific hierarchy levels; We show structured training yields better results baseline across levels hierarchy, also leading better parameter utilization. In paper, tested few-shot learning capabilities neural language models, well whether models learn grammatical representations invariant syntactic transformation. First, addressed neural models' ability learn nominal number, introducing novel testing paradigm leveraged polar questions assess subject/verb number agreement learning syntactically transformed settings. Second, turned neural models' ability represent verbal argument structure, developing two novel suites tests assessed preference themes---either realized direct objects passive subjects---in active contexts passive contexts. In experiment assessed effect syntactic supervision learning outcomes comparing two supervised models one purely sequence model. A summary results seen Table, few-shot learning outcomes colored cells left, effect structural supervision right. The results experiments assess syntactic invariance bottom, line break. This table makes clear neural models capable making syntactic generalizations token minimal exposure training. Although model accuracy reduced tests assess syntactic invariance, neural models show least moderate ability generalize across syntactic transformations. Furthermore, Table shows syntactic invariance enhanced structurally supervised models. Interestingly, ActionLSTM RNNG access syntactic information, comparison Table indicates RNNG leverage information effectively produce syntactic invariance. Therefore suggest RNNG's improved performance come mere presence syntactic information training test data, rather fact uses syntactic information structure computation non-sequential way. Models performed better singular nouns transitive verbs, especially token occurred minimally training. This behavioral pattern consistent hypothesis outlined \citet{jumelet2019analysing}, suggest models acquire default syntactic categories, require supporting evidence make non-default predictions. Because experiments require careful robust syntactic analysis training data, evaluated models trained relatively small, human-annotated corpus. While small training data poses limitations interpreting results, makes relevant low-resource NLP applications suggests using structurally supervised models lead better generalization sparse data environment. While sub-word tokenization schemes Byte-Pair Encoding helped reduce number individual lexical items need learned, completely eliminate long tail sub-word units. Thus, robust few-shot generalization still important problem environments. It may larger amounts training data support even better few-shot learning syntactic invariance outcomes. Scaling carefully-controlled methods larger data setting important next step. However, even relatively small models tested here, results support growing body evidence incremental statistical models language able induce many key features human linguistic competence. \section*{Acknowledgements} The authors thank anonymous reviewers feedback. This work supported MIT-IBM Watson AI Lab. \section{Effect Exposure Model Accuracy} In section report result statistical tests assessing effect token's frequency training model accuracy token. We derive significance general linear model \# exposures sole predictor, random by-item intercepts ))}) \paragraph{Nominal Number} For base context, modifier condition find positive effect increased exposure models . For PP modifier test find effect exposure ActionLSTM RNNG , negative, insignificant effect -gram LSTM. For RC Modifier experiment find effect increased exposure three neural models , effect -gram. For inverted contexts: modifier tests find effect increased exposure, except LSTM, effect negative . For modifier tests, find significant effect ActionLSTM RNNG . \paragraph{Argument Structure} For base context : In infinitival tests, find significant effect exposure accuracy ActionLSTM RNNG negative effect -gram model . In past-tense, find significant effect RNNG ActionLSTM, negative effect -gram LSTM models . In transformed contexts , no-modifier tests find significant effect exposure models . For short-modifier tests find effect ActionLSTM RNNG . And long-modifier test find marginally significant effect three neural models . \section{Learning Outcomes Grammatical Condition} In section, test reported paper, break model performance grammatical category, either singular vs plural nouns transitive vs. intransitive verbs . Charts follow presentational paradigm: -axis shows accuracy -axis number times word appears training, log-10 scale. Smooth lines results logistic regression model fits raw data, shaded regions indicating standard error. Dark blue lines show model performance averaged two conditions . The data presented consistent hypothesis . When models receive scant evidence token's syntactic properties training, assume belongs ``base"" category, singular nouns transitive verbs. Thus, models accurate singular nouns transitive verbs seen rarely training. As model receives evidence token base category, predictions flip. Hence, gains overall-accuracy tend come models learning proper agreement non-base tokens . Generally, effects stronger nominal number learning, stronger structurally supervised models LSTM, consistent findings presented main body text. The nominal number breakdown base contexts seen Figure , accuracy scores singular nouns red plural nouns \texttt{NNS}) teal. Over all, models tended show higher accuracy scores singular nouns, indicates presence singular bias. Interestingly, ActionLSTM RNNG capable overcoming singular bias presented sufficient data, however LSTM remains equally biased tokens seen 2 100 times training. The nominal number breakdown transformed seen Figure . The empirical picture complicated here, however anything models show higher performance plural nouns. This behavior suggests sets weaker expectations singular nouns plural nouns. Such pattern consistent hypothesis models learn singular base form, case would set weaker expectations singular nouns. These results compliment \citet{an2019representation} , also test inverted settings find models tend surprised coordinated NPs following singular verb, ungrammatical sentence *What pig cat eating? The breakdown argument structure learning base contexts seen Figure , accuracy scores intransitive verbs red transitive verbs teal. Here, see strong transitive bias two structurally supervised models, obvious bias LSTM intransitive bias -gram. The breakdown argument structure learning transformed contexts seen Figure transformation tests top invariance tests bottom. In case, performance different two conditions models display higher accuracy scores transitive verbs.","     Although \bert is widely used by the \nlp community, little is known about its inner workings. Several attempts have been made to shed light on certain aspects of \bert, often with contradicting conclusions. A much raised concern focuses on \bert's over-parameterization and under-utilization issues. To this end, we propose o novel approach to fine-tune \bert in a structured manner. Specifically, we focus on Large Scale Multilabel Text Classification  where documents are assigned with one or more labels from a large predefined set of hierarchically organized labels. Our approach guides specific \bert layers to predict labels from specific hierarchy levels. Experimenting with two \lmtc datasets we show that this structured fine-tuning approach not only yields better classification results but also leads to better parameter utilization."
"Deep neural network-based models demonstrated remarkable performance multitude text-to-text \cite[inter alia]{bahdanau-attention,bert-to-bert,narayan-etal-2018-dont,rush-etal-2015-neural} well data-to-text generation tasks \cite[inter alia]{wiseman-etal-2017-challenges,puduppully-etal-2019-data}. % To reach high performance, DNN models require large training corpus normally readily available. Indeed, rare sufficiently large human-curated corpus parallel data , researchers come heuristic rules mine input-output pairs large scale . No matter powerful, DNN models known sensitive data artifacts pick noise training data. While hallucinations defined formally, term standardly used refer generated content either unfaithful input, nonsensical . In work concerned former hallucination kind primarily caused imperfect quality training data. % If data noisy, one reduce chances hallucinating? % One may try improve quality dataset clean phrases clear support input missing, augment input information found output. The former path risky easily results ungrammatical targets. The latter approach enforcing stronger alignment inputs outputs tried previously assumes moderate amount noise data . % Alternatively, one leave data try put pressure decoder pay attention input every generation step . This requires significant modifications model may make harder decoder generate fluent diverse text found targets. In contrast described approaches, proposal train model data without modifying decoding architecture instead introduce handle input side control degree hallucination . With ""hallucination knob"" one minimize amount unsupported information output generation . The hallucination noise degree every training instance estimated separately converted categorical value becomes part input, like controlled generation setting . We introduce simple technique measure amount noise every training example based intuition whenever language model smaller loss conditional generator forced-path decoding, good signal next token cannot explained input. % . We consider particularly noisy dataset, WikiBio , found extra information 62\% references 1:1 correspondence input output never holds \citet{perez-beltrachini-gardent-2017-analysing}. Our models demonstrate superior performance model reports SoTA BLEU results WikiBio. % In sum, contributions novel idea controlling hallucinations requires modification model, data- task-independent technique implementing idea three-way evaluation human raters confirms faithfulness need traded coverage. In work, present novel techniques enable successful offline reinforcement learning base language model real human conversations. This allows dialog systems practitioner train models learn language structure vast, readily-available corpora, fine-tune specific desirable behaviors post-hoc RL rewards. We observe new offline RL method successfully optimizes generated bot rewards elicited human responses. We show presents better option using regularization training specific bot behavior. Further, RL currently remains option maximizing user feedback course conversation. Compared prior work offline RL, novel WOP offline RL algorithm achieves higher performance traditional RL tasks, elicits positive feedback conversations novel humans test time, earns overall higher human ratings. A limitation study question optimize RL improve overall qualitative ratings remains open. We shown manual ratings sparse optimize effectively, instead suggest using implicit rewards. However, reward set proved insufficient achieve higher human quality ratings, least limited offline training data able collect. It unlikely rewards proposed fully cover means high quality open-ended conversation. Future work investigate rewards training open-domain dialog model long term conversation rewards may need computed many conversation turns. Our work computes conversational rewards based dialog data annotations online task workers United States. Considering broader impacts work, representative diverse set conversations annotations collected real world systems trained deployed using algorithms. We shown proposed techniques useful shaping dialog model behavior towards desired objective. For many practical applications, may specific requirements language generated model---for example, appropriate, positive, polite---even leads lower perception conversation quality users. We shown Way Off-Policy algorithm provides effective way teach language model specific behaviors offline data previously proposed RL regularization techniques. \section*{Acknowledgments} We would like thank Scott Fujimoto insightful email correspondence topic, approval DBCQ algorithm, suggestion apply model averaging. We would like thank Sudha Rao Yonatan Bisk helpful guidance feedback re-framing re-writting process work. We also thank Max Kleiman-Weiner, Ardavan Saeedi, Sebastian Zepf, Sara Taylor, Oliver Saunders Wilder, Kyle Kastner, Marissa Zhang, Kristy Johnson helpful discussions project, many others helping test-drive bots. We thank MIT Quest Intelligence, MIT Stephen A. Schwarzman College Computing, Machine Learning Across Disciplines Challenge providing computing resources, MIT Media Lab Consortium support research. This work partially supported RTI2018-095232-B-C22 grant Spanish Ministry Science. \section{Reproducibility} \subsubsection*{Baseline Models} The underlying architecture baseline language models employed work Variational Hierarchical Recurrent Encoder Decoder . We also conduct second set experiments enhanced version model additional knowledge distillation improve model's ability track sentiment semantics conversation, proposed \citet{ghandeharioun2019approximating}. The language models originally trained two datasets: movie dialogs dataset scraped . The underlying parameters VHRED model follows: Context RNN hidden size , decoder hidden size , encoder hidden size , embedding size , gradient clip , dropout . The maximum conversation length fixed 5 utterances , maximum sentence length 30 tokens. The VHRED model million parameters. We also added layers Context RNN regularized able predict semantic content input utterance using form knowledge distillation state-of-the-art sentence-embedding model . There 2 additional feedforward semantic prediction prediction layers size 128, used ReLu activation. The VHRED model sentiment infersent regularization million parameters. Each RL model trained NVIDIA GeForce GTX 1080 GPU. \subsubsection*{RL Models} The RL models, main focus work, trained using human conversation data collected via online interactive platform batch size fixed 32. Each model trained epochs. The RL models initialized weights best model trained Reddit dataset. Early stopping used determine number training iterations best checkpoint. For bot, 3 different stopping epochs tested best selected. The checkpoint selected using manual tuning based interactive chat chatbots. For best performing bots, KL-Control KL-Control , 1600 1800 epoch checkpoints selected respectively. The reward weights also tuned determine weighting rewards produced desired bot behavior. We tried uniform weights slightly increased weights repetition rewards human bot interaction rewards. The best weights found assigning repetition human bot interaction rewards rewards. Reward weights also determined using manual tuning conversational interaction. The reward weights shared RL models trained. Only 3 sets weights tried reward weights hyperparameter optimization process. All hyperparameters shared RL models, follows: discount , weight placed RL reward vs. KL-divergence term , number Monte Carlo samples Target -network , target network update rate , learning rate . We used smooth loss function approximate -values, clipped gradients value . The RL models total parameters . } \\ \hline VHRED-EI Baseline & 3.11 .41 & 4.34 .44 & 4.66 .49 & 3.02 .47 & 3.45 .47 & 18.59 1.76 & 0.19 & -0.05\\ \hline DBCQ & 1.64 .48 & 1.87 .34 & 3.13 .58 & 1.84 .34 & 2.09 .38 & 10.58 1.55 & -0.23 & -0.02 \\ Batch & 1.87 .30 & 2.36 .42 & 2.20 .41 & 1.91 .32 & 2.58 .47 & 11.91 1.58 & -0.16 & 0.00 \\ Batch + MC & 1.85 .39 & 2.46 .44 & 2.46 .52 & 1.98 .39 & 2.34 .49 & 11.07 1.82 & -0.07 & 0.03 \\ KL-control & 2.38 .39 & 3.24 .47 & 3.42 .54 & 2.38 .45 & 2.56 .43 & 13.98 1.81 & 0.02 & 0.01 \\ KL-control & 2.33 .41 & 3.73 .53 & 2.82 .50 & 2.31 .44 & 3.47 .50 & 14.67 1.82 & 0.13 & 0.03 \\ \hline } } & Quality & Fluent & Diverse & Related & Empathy & Total & Votes & \\ \hline Conv. len. & 2.20 .40 & 3.61 .53 & 3.02 .52 & 2.25 .46 & 2.48 .45 & 13.57 1.84 & -0.04 & -0.01 \\ Infersent Coher. & 1.93 .34 & 3.50 .45 & 2.37 .45 & 2.11 .45 & 2.52 .48 & 12.43 1.75 & -0.02 & -0.01 \\ User laughter & 1.96 .38 & 3.56 .48 & 2.33 .51 & 1.93 .42 & 3.20 .55 & 12.98 1.60 & -0.15 & -0.01 \\ User Word Len & 2.11 .32 & 3.96 .44 & 3.04 .45 & 2.04 .35 & 2.55 .46 & 13.70 1.44 & 0.06 & 0.04 \\ Manual votes & 2.14 .38 & 3.47 .45 & 2.91 .47 & 2.07 .39 & 2.42 .46 & 13.00 1.65 & -0.03 & 0.01 \\ Sent. trans. & 2.02 .31 & 3.71 .49 & 2.98 .50 & 2.04 .42 & 2.84 .48 & 13.60 1.63 & 0.03 & 0.01 \\ Bot Question & 2.29 .37 & 4.31 .50 & 3.31 .52 & 2.20 .40 & 2.60 .41 & 14.71 1.63 & 0.06 & 0.04 \\ User Sentiment & 2.47 .32 & 4.05 .45 & 3.23 .46 & 2.42 .39 & 3.23 .55 & 15.40 1.49 & 0.09 & 0.04 \\ \hline } Each RL model trained NVIDIA GeForce GTX 1080 GPU. Training models 2000 epochs took approximately 30 minutes model. The runtime training VHRED baseline models around 6 hours. The speediness training RL models illustrates scalability RL training improving dialog models specific features. We use interactive human evaluation online chat interface. Human participants recruited using Amazon Mechanical Turk rate either 7 8 bots each. Participants instructed continue conversation least 6 human responses. After conversation, participants asked rate bot terms Quality, Fluency, Diversity, Contingency, Empathy 7-point Likert scale. A detailed example chat interaction platform found Section . Since models evaluated using interactive chat, also validate models interactive chat rate models tuning hyperparameters. The authors interacted rated bots validate bots. \section{Offline-RL VHRED Emotion Infersent Regularization} We also conducted experiments using offline RL algorithm Sentiment Infersent regularized VHRED Model. As described Section , adding 20 million extra parameters VHRED model order better achieve semantic coherence sentiment contingency, VHRED-EI model better performing baseline terms human ratings . We conducted human experiments recruited participants Amazon Mechanical Turk chat rate dialog model. We found similar results presented main paper. While KL-control models achieved higher qualitative ratings offline RL algorithms, none RL models received higher qualitative ratings VHRED-EI Model . We also replicated training KL-Control model single rewards found training User Sentiment elicited highest human qualitative ratings . This consistent results VHRED model. \section{Traditional RL experiments} To demonstrate effectiveness techniques, tested traditional RL tasks using OpenAI gym , focusing CartPole-v0 Acrobot-v1 experiments. We first train online -learning Behavior policy, store experience samples replay buffer. We use buffer train prior model using Variational Auto-encoder. The VAE trained reconstruct next state given current state, , using mean-squared error loss. The next action predicted latent embedding , meaning model learned three functions: , , . For Cartpole, encoder decoder made two linear layers 750 neurons each. The latent dimension VAE size 256. For Acrobot, encoder decoder one layer size 256 each, latent dimension 64. This VAE used part DBCQ WOP algorithms. We also use imitation learning, sampling actions directly obtain Behavioral Cloning . We benchmark techniques vanilla -learning batch data . All -networks shared underlying architecture: three fully-connected layers size [256, 128, 64], ReLU activation between. All models trained Adam optimizer . For experiment, ran 50 trials model different random seed time. The Behavior policy trained total 20,000 steps environment, Full buffer condition offline agents saw 20,000 experience samples. The Behavior policy typically converged 10,000 steps, Expert demonstrator condition offline agents received last 10,000 experience samples trained agent. In Concurrent condition, offline agents saw moving window 1000 samples, since online learner used recent 1000 samples buffer learning. The learning rate .001, , decayed linearly 1.0 .01 2000 steps. The KL-constraint computed , . DBCQ sampled actions selecting best action based maximum -value; note environment 2 actions. For Cartpole used -learning loss, Acrobot used traditional -learning loss. We experiment four different conditions vary quality Behavior policy replay buffer data: a) Full buffer: experience samples experienced online training used offline learning; b) Concurrent: offline learning algorithms see sliding window experience samples order online learner experienced them; c) Expert demonstrator: buffer contains experience generated fully trained online learner; d) Noisy demonstrator: online learner high probability acting randomly thus bad model optimal policy. Figure shows results. Across conditions, see WOP able outperform Batch , imitation learning , DBCQ, original behavior policy. As expected, Imitation learning underperforms techniques batch contains noisy inexpert experience samples. However, batch contains expert trajectories, Batch fails learn, batch cover full state-action space well, increasing extrapolation error. DBCQ matches outperforms BC Batch scenarios. However, DBCQ acts sampling learned BC model, performance suffers batch data noisy imperfect. In contrast, WOP able learn trade-off staying close prior obtaining higher reward, consistently outperforms algorithms environment. \section{Additional results} Figure shows KL-divergence RL policies prior language model throughout offline RL training. Without KL-regularization, baseline RL models diverge quickly continuously prior, losing information realistic sequences. This figure also helps explain poor performance DBCQ Table . The underlying -network DBCQ directly integrate prior. As -learning causes model diverge prior, -estimates language generated according prior become unrealistic, selects unrealistic actions. This results highly `diverse' generated utterances. Note since operate discrete action space, could include perturbation model originally proposed , may critical achieving good performance BCQ. \section{Implicit Rewards Details} The total reward used train bots combination rewards described Table . These rewards selected based average z-score rewards utterances upvoted downvoted. Figure shows user rewards User Laughter User Sentiment reward scores correlate upvotes downvotes. Figure shows bot rewards Bot Sentiment, Bot Laughter, Bot Convo. Repetition, Bot Utterance Repetition rewards correlate manual votes. Figure shows bot-user combined rewards, Word Similarity USE Similarity rewards correlate manual downvotes. Based prior work , use number turns conversation indicator quality bot's performance. To distribute reward every utterance conversation, take total conversation length , compute discounted reward utterance . We also reward utterance number words characters user's response, refer User Ans. Word Len User Ans. Char Len. We also examine long bot responses Bot Response Length reward. Laughter shown important human affiliation solidarity . Therefore, detect number occurrences strings indicating laughter user's response, use reward. Interestingly, find bots trained maximize user laughter learn extremely supportive cheerful compared bots . Language style matching shown strong predictor relationship initiation stability . While would ideal chatbots could intelligently adapt conversation style new user, reality baseline dialog models struggle maintain topic coherence, even utterances . Therefore reward semantic similarity user's input bot's response, encourage bot stay topic produce reasonable answers. The Infersent Cornell Coherence Infersent Reddit Coherence rewards computed using sentence embedding model trained Reddit Cornell corpora respectively . We use Universal Sentence Encoder compute USE Similarity reward. We also directly compute word overlap reward Word Similarity. Asking questions important listening skill, linked conversation management, attentiveness, responsiveness . Therefore, give bot reward 0.5 utterance contains question word , additional 0.5 contains question mark. We refer reward Bot Question. After training bots rewards, noticed shift distribution language towards polite, cheerful, supportive speech. Therefore, designed post-hoc metrics measure qualities, based counting whether subset phrases present utterance. Compliment phrases: beautiful, beautiful, beautiful, beautiful, best, best, like you, good, good, love way Politeness phrases: I may; may I; please; thanks; worries; mind; great day; I'm sorry. Supportive phrases: right; right; alone; alone; congrats; that's good idea; good idea; fine; fine; okay; okay; get better; sorry going through; sorry going through; makes feel better; makes feel better; keep head up; keep up; I'm similar situation; I similar situation; get it; get it; happy you; I'm boat; I boat; feel like need vent. Cheerful phrases: nice hear; happy; excited; really nice; glad; best; great; good time; looking forward; beautiful. We also want discourage bot malicious offensive language. \citet{saleh2019hierarchical} incorporate Toxicity Classifier trained data Toxic Comment Classification Challenge\footnote{} reward training hierarchical RL dialog models. We compute Toxicity reward scores using classifier Bot Toxicity . Specificity within conversation valuable avoid exchanging vacuous phrases back forth. However building chit-chat bot without knowledge graph back-end limits level substance incorporated conversation. We use approach computing normalize IDF create specificity conversation. We compute NIDF user bot text. While minimizing repetition common implicit goal dialog systems, explicitly optimize reducing repetition repetition rewards. We compute utterance repetition number non-unique words utterance Bot Utterance Repetition Reward. We compute conversation repetition number non-unique words conversation Bot Convo. Repetition Reward. These rewards negated since want higher reward score less repetition. We also remove stop words computation non-unique words. \section{Interactive bot platform details} To collect data humans interacting bots, built platform hosting deep neural network dialog models online GPU fast, real-time inference. Figure shows example interface, users able rate bots talking least three turns. Note chat, annotators optionally click arrows beside chatbot response give feedback specific utterance. Once 6 turns conversation taken place, participants may click ``Close Chat Rate"" get rating screen. We train RL models based chat data collected platform. Currently, conversations contain Personally Identifiable Information user name, age, location, etc. We obtained IRB approval study cannot release conversations time current form. The server hosted Google Cloud Platform virtual instance 64GB RAM NVIDIA Tesla P100 graphics card. The backend Django program served NGINX uWSGI. For simplicity, opted Django process import chatbots Python process Django, rather two connect via means sockets. This configuration decreased development time increased reliability, would need revisited server needed scale several orders magnitude past required study. The current configuration still able support hundreds simultaneous users host 30 bots concurrently. The chatbots kept separate project Django project maintained separately server code. Each chatbot extended abstract class defined key methods Django program use, registered globally accessible dictionary via decorator. The Django project provided path Chatbots project PYTHONPATH, could import dictionary chatbot objects registered use dynamically determine chatbots available access views. It important note chatbots used PyCUDA, PyCUDA work multiprocessing environment. Because this, uWSGI needed configured one python process disable attempt multiprocessing. Furthermore, chatbots required substantial startup times, chatbots kept memory times Django process. In order keep chatbots memory concurrently, needed high amount RAM server opted 64GB virtual instance, GPU 16GB RAM. This combination CUDA run chatbots GPU high amount RAM keep bots memory time resulted incredibly fast server response times, effectively increase response time using bots requests compared requests not. For information instructions server configuration, please read server documentation available . We hope platform allow others host bots evaluate interactive setting."," Neural text generation  demonstrates remarkable performance when training data is abundant which for many applications is not the case.  To collect a large corpus of parallel data, heuristic rules are often used but they inevitably let noise into the data, such as phrases in the output which cannot be explained by the input.  Consequently, models pick up on the noise and may hallucinate--generate fluent but unsupported text.  Our contribution is a simple but powerful technique to treat such hallucinations as a controllable aspect of the generated text, without dismissing any input and without modifying the model architecture. On the WikiBio corpus \cite{lebret-etal-2016-neural}, a particularly noisy dataset, we demonstrate the efficacy of the technique both in an automatic and in a human evaluation."
"% %Added value \atomicTT{}: 1) diversity terms vocab, style, concepts, 2) higher quality %\ronan{Cite publications used ATOMIC downstream application} Commonsense understanding % knowledge modeling reasoning remain long-standing challenges general artificial intelligence. % However, subfield natural language processing, last years brought tremendous progress AI applications. However, large-scale language models brought tremendous progress sub-field natural language processing. Such large-scale language models trained extreme-scale data shown effectively adapt diverse downstream tasks, achieving significant performance gains across natural language benchmarks . %%%%%%%OLD %%%%%% Despite successes, models shown learn brittle representations, often simple surface word associations , routinely lead make nonsensical predictions detached common sense . Interestingly, models grown larger , benchmark performance continued improve despite limited conceptual improvements, %leading many researchers conjecture leaving open questions regarding source remarkable generalization properties. Recent work hypothesized many performance gains could result language models able memorize facts parameters training leveraged evaluation time. As result, new paradigm language models knowledge bases emerged . In setting, language models prompted natural language prefixes questions, express knowledge language generation. The initial success paradigm representing commonsense knowledge %, combined limited examples LMs successfully integrated structured commonsense knowledge resources downstream application, led optimistic claim language models comprehensively encode commonsense knowledge, remove need structured knowledge resources. %\antoine{run-on sentence, need shorten} We take skeptical view capacity language models -- Does scaling language models actually endow commonsense knowledge? While language models successfully express certain types knowledge, best results observed narrowly specific conditions -- show perform better evaluated knowledge bases prioritize ontological relations whose examples resemble language-like assertions .\footnote{An observation supported \citet{brown2020language}'s \gpttt{} model, whose best few-shot performance commonsense knowledge benchmarks comes PhysicalIQA HellaSwag datasets.} Consequently, types knowledge directly accessed language model's interface remains limited. %Consequently, methods encouraging, also demonstrate limited interface language models precludes expressing diversity commonsense knowledge must accessible robust commonsense reasoning. %\chandra{i sure last line paragraph flows logically rest paragraph. maybe missing something?} However, prior work also shown training language models knowledge graph tuples leads learn express implicit knowledge directly , allowing provide commonsense knowledge on-demand. These adapted knowledge models exhibited promising results commonsense benchmarks compared methods require linking entities knowledge graphs . Inspired successes, propose dual use commonsense knowledge bases going forward: static graphs linked discrete knowledge access, resources adapting language models hypothesize commonsense knowledge un-annotated entities events. %%%%%%% OLD %%%%%%%% As result, recent work investigated augmenting language models retrieval mechanisms query commonsense knowledge graphs related facts entities mentioned text. The idea behind approaches access facts potential compose learned reasoning functions would allow models robustly leverage commonsense knowledge make predictions. Despite premise approaches, unfortunately limited coverage resources used provide commonsense knowledge facts , motivating need new, high coverage resources short-term. % Option 1 % With second purpose mind, shift design goals commonsense knowledge resources toward prioritizing pieces knowledge readily accessible pretrained language models. % Option 2 With second purpose mind, propose evaluating commonsense knowledge resources based complementary information bring pretrained language models. We construct \atomicTT{}, new, high-quality knowledge graph M commonsense knowledge tuples across commonsense relations. We compare \atomicTT{} respect coverage accuracy competition highly used CSKGs, \conceptnet~. Our results show \atomicTT{} able cover correct facts diverse types commonsense knowledge existing, publicly-available commonsense knowledge resource. However, results also indicate remains large amount exclusivity KGs, highlighting challenge creating resources cover scale diversity general commonsense knowledge. %%%%%%% OLD %%%%%%Meanwhile, new paradigm emerged proposes large-scale language models implicitly learn represent large amounts factual commonsense knowledge . While methods promising, also show limited interface language models precludes producing commonsense knowledge robustly. However, using knowledge graph tuples additional training signal allows model better adapted representing knowledge . Furthermore, use knowledge models provide commonsense knowledge on-demand shown promising results static knowledge graphs . Consequently, work, propose evaluating commonsense knowledge resources new, second purpose: whether used repurpose language models commonsense modeling. Furthermore, formalize \comet framework \citet{Bosselut2019COMETCT} across different seed language models training knowledge graphs, evaluate commonsense knowledge hypothesized adapted knowledge models. %Our results indicate purpose promising evaluation commonsense resources, \comet models successfully hypothesize plausible knowledge new, unseen entities. Our empirical study yields two promising conclusions. First, confirms KG-adapted language models learn express knowledge precisely naive language models trained language. And second, show \atomicTT{} transfer resource leads \comet models achieve largest increase seed language model commonsense knowledge types covers, validating importance constructing knowledge resources examples knowledge readily found language models. %allows language models learn representations commonsense knowledge types less covered naive language models. % Furthermore, comparison \comet models across different commonsense knowledge graphs shows \atomicTT{} transfer resource allows language models learn richer commonsense knowledge representation training resources. % Key Contributions: In summary, make three key contributions paper. We present \atomicTT{}---a new commonsense knowledge graph covering social, physical, eventive aspects everyday inferential knowledge . Next, compare \atomicTT{} prominent CSKBs head-to-head show new symbolic knowledge graph accurate current CSKB . Finally, show new neural knowledge model \comet{}-\atomicTT{} successfully transfers \atomicTT{}'s declarative knowledge beat \gpttt{}, largest pre-trained language model, spite using ~400x fewer parameters . This demonstrates utility importance high-quality symbolic knowledge provided \atomicTT{} generalize commonsense information LMs cannot expressively capture . % * Our new symbolic knowledge graph ATOMICTT superior accuracy coverage currently existing large-scale knowledge graphs . % * neural knowledge model COMET-ATOMICTT successfully transfers ATOMICTT's declarative knowledge beat even impressively large pretrained model, GPT-3 . This demonstrates LMs, matter size, benefit symbolic knowledge provided high quality KB like ATOMICTT. Comparing two methods estimating amount hallucinations target, applications input output use vocabulary comparable term distribution overlap method may better clear foundation. The LM-based method proposed important advantage makes assumptions data. In WikiBio experiment also produced better results human evaluation, presumably allowed paraphrasing straightforward inferences. For example, target ozren nedoklan yugoslav footballer manager. high score source table occupation field mention yugoslav. The score example zero footballer manager inferred names clubs manageryears fields source. \paragraph{Possible extensions} It emphasized alternative methods detecting noise explored may perform better controlled-hallucination framework. For example, possible measuring target-source similarity embedded space use word alignment tools find unsupported information. While focused eliminating hallucinations, one think applications one interested generating adversarial sentences sound fluent guaranteed include unsupported information. Figure shows amount hallucinations output increases following value hallucination knob. \paragraph{Why BLUE different?} It striking models tested outperform \citet{liu-2018-structure-aware} terms PARENT human evaluation scores, none could approach BLEU performance. We explanation note results line review \citet{reiter-2018-structured} concludes BLEU inappropriate metric generation tasks MT. \paragraph{Can measure length instead noise?} One may wonder whether even simpler approach controlling length would deliver similar reduction hallucinations. Indeed, hallucinations length expected correlate, shorter length result fewer hallucinations. However, pointed Sec.\ , drastically reducing hallucinations may possible without control mechanism achieved, least WikiBio, templates. The main challenge lies without big drop informativeness, is, coverage input fields. Comparing outputs , \citet{tian-sticking}, note ranking terms average sentence length coincides ranking terms coverage : 17.2, 17.8, 18.7. While may associate special hal\_0 token shortest 20\ training data, token apparently associated different selection 20\ data points. \section{Conclusions} We presented simple powerful idea controlling hallucinations caused noise training data proposed two ways detecting noise. We demonstrated possible reduce amount hallucinations coverage cost informing model noisy every source-target example without changing model architecture. Importantly, done without making assumptions data. In evaluation humans showed faithfulness generated sentences significantly improved loss fluency coverage. The results reported noisy WikiBio dataset improve upon prior work."," % Check out this new knowledge graph! % Storyline: % \begin{enumerate} % \item We introduce \atomicTT. % \item We provide the first side-by-side comparison of commonsense knowledge bases and comprehensive ways to capture precision and coverage. % \item We show how commonsense KGs provide a clear vehicle to access knowledge in LMs.  ).  % \end{enumerate}  Recent years have brought about a renewed interest in commonsense representation and reasoning in the field of natural language understanding. The development of new commonsense knowledge graphs  has been central to these advances as their diverse facts can be used and referenced by machine learning models for tackling new and challenging tasks. At the same time, there remain questions about the quality and coverage of these resources due to the massive scale required to comprehensively encompass general commonsense knowledge.  In this work, we posit that manually constructed CSKGs will never achieve the coverage necessary to be applicable in all situations encountered by NLP agents. Therefore, we propose a new evaluation framework for testing the utility of KGs based on how effectively implicit knowledge representations can be learned from them.   With this new goal, we propose \atomicTT{}, a new CSKG of general-purpose commonsense knowledge containing knowledge that is not readily available in pretrained language models. We evaluate its properties in comparison with other leading CSKGs, performing the first large-scale pairwise study of commonsense knowledge resources. Next, we show that \atomicTT{} is better suited for training knowledge models that can generate accurate, representative knowledge for new, unseen entities and events. Finally, through human evaluation, we show that the few-shot performance of GPT-3 , while impressive, remains $\sim$12 absolute points lower than a BART-based knowledge model trained on \atomicTT{} despite using  over 430x fewer parameters.  % useful they are for training knowledge models that can generate relevant representative knowledge for new, unseen entities.  % In this work, we propose \atomicTT{}, a new knowledge graph of general-purpose commonsense knowledge facts. To evaluate its utility in comparison to existing resources, we perform the first large-scale pairwise study of commonsense knowledge graphs on coverage and precision. Finally, we posit that a new use for commonsense knowledge graphs is their ability to allow large-scale language models to learn to represent knowledge implicitly. We propose a new evaluation for testing knowledge graphs on how useful they are for training knowledge models that can generate relevant representative knowledge for new, unseen entities."
"Despite successes, neural machine translation still unresolved problems. Among problem rare words, paradoxically common Zipf's Law. In part, problem intrinsic data-driven machine translation system inevitably encounter words seen training data. In part, however, NMT systems seem particularly challenged rare words, compared older statistical models. One reason NMT systems fixed-size vocabulary, typically 10k--100k words; words outside vocabulary represented using special symbol like \unk{}. Byte pair encoding breaks rare words smaller, frequent subwords, least allowing NMT see instead \unk{} . But means solves problem; even subwords, NMT seems difficulty learning translations rare words, possibly instance catastrophic forgetting . Humans deal rare words looking dictionary, idea using dictionaries assist machine translation extremely old. From statistical perspective, dictionaries useful complement running text uniform distribution dictionary headwords smooth long-tailed distribution running text. In pre-neural statistical machine translation systems, typical way incorporate bilingual dictionaries simply include parallel sentences training data. But , work well NMT systems. We aware previous attempts find better ways incorporate bilingual dictionaries NMT. Some methods use dictionaries synthesize new training examples . \citet{arthur-etal-2016-incorporating} extend model encourage generate translations dictionary. \citet{post+vilar:naacl2018} constrain decoder generate translations dictionary. What approaches common treat dictionary definitions target-language text, when, fact, often properties different ordinary text. For example, CEDICT defines \zh{ ``'' cannot used translation. In case monolingual source-language dictionary, definitions are, course, written target language all. In paper, present extension Transformer ``attaches'' dictionary definitions rare words occurrences source sentences. We introduce new position encodings represent nonlinear structure source sentence attachments. Then unmodified translation model learn make use attached information. We show additional information yields improvements translation accuracy 3.1 BLEU. Because method force dictionary definitions treated target-language text, generalizable kinds information, monolingual source-language dictionaries, yield smaller improvements, still much 0.7 BLEU. }} \centering \scalebox{0.8}{% \textrm{WE}[f]f\textrm{PE}[p]p\textrm{DPE}[q]q$ within dictionary definition. The rare word \zh{ replaced \unk{} defined Dead Sea. The words definition encoded position defined word positions within definition.} \end{figure*} \paragraph{Do pretrained language models already encode commonsense knowledge?} Our conclusions subject mixed hinge ambiguous meaning means encode knowledge. Despite conclusions prior work , results Table clear language models fail express large varieties knowledge prompted zero-shot manner. When converted \comet models training knowledge graph, performance hypothesizing knowledge tuples skyrockets -- 47.9\ absolute difference \cometbart{} \gptxl{} \atomicTT. However, evaluation tuples adversarially selected include head entities training set. The model must generalize learned representations relations entities observed relationships point fine-tuning, meaning representation entities solely formulated learning language. As result, language models may still encode knowledge parameters, even capable expressing directly. With framing mind, COMET training paradigm proposed \citet{Bosselut2019COMETCT} perhaps viewed less means learning knowledge KGs, method learning interface language models hypothesize encoded knowledge language generation. We look forward future work space attempts disentangle two ideas. What considerations made designing commonsense knowledge resources? Commonsense knowledge graphs uniquitous tools natural language processing agents must perform commonsense reasoning. Based results Section, outline desiderata design development future commonsense knowledge graphs. Because certain types knowledge already encoded expressible pretrained language models, CSKG designers focus collecting examples categories knowledge less likely known language models. For example, 378 test tuples evaluated \gptxl{} zero-shot model contained \HinderedBy{} relation, 1.3\ deemed plausible human raters -- jumping 85\ plausibility \cometbart{} -- pointing advantage constructing \atomicTT{} relationship mind . Second, commonsense knowledge resources designed goal accuracy relationship coverage. Because language models exhibit powerful adaptation , generalize many commonsense relationships long examples train. Consequently, construct commonsense resources encapsulate larger numbers relations knowledge pretrained language models grounded variety relationships. However, language models also benefit learning precise examples. Being able train large collection examples \transomcs allow \comet models generalize unseen entities examples sufficient quality . Resources carefully validated quality facts, example set \citet{speer2017conceptnet} \citet{sap2018atomic}. \section{Conclusion} In work, formalize use commonsense knowledge graphs transfer learning tools pretrained language models. With new purpose, hypothesize commonsense knowledge graphs designed contain knowledge already expressible language models without difficulty . Consequently, propose \atomicTT, novel commonsense knowledge graph containing tuples whose relations specifically selected challenging pretrained language models express. Our empirical studies demonstrate \atomicTT contains high-accuracy knowledge tuples across multiple novel relations found existing CSKGs expressible LMs. Furthermore, show \atomicTT effectively used training set adapting language models knowledge models generate high quality tuples on-demand. \clearpage"," Despite advances in neural machine translation  quality, rare words continue to be problematic. For humans, the solution to the rare-word problem has long been dictionaries, but dictionaries cannot be straightforwardly incorporated into NMT. In this paper, we describe a new method for ``attaching'' dictionary definitions to rare words so that the network can learn the best way to use them. We demonstrate improvements of up to 3.1 BLEU using bilingual dictionaries and up to 0.7 BLEU using monolingual source-language dictionaries."
"% % The following footnote without marker needed camera-ready % version paper. % Comment instructions uncomment 8 lines % ""final paper"" variant English. % % % . % % % % % final paper: en-us version % % % % % space normally used marker % % This work licensed Creative Commons % % Attribution 4.0 International License. % % License details: % % \url{http://creativecommons.org/licenses/by/4.0/}. % } % 1.  CCG CCG  % 2. CCG parsing  supertagging contextual information  encode  powerful encoder contextual feature  % 3. n-gram  contextual feature supertagging  combination  % 4.  model %  CCGupertag Combinatory categorial grammar lexicalized grammatical formalism, lexical categories words sentence provide informative syntactic semantic knowledge text understanding. %  ccg supertagging  Therefore, CCG parse often provides useful information many downstream natural language processing tasks logical reasoning semantic parsing . To perform CCG parsing different languages, %  ccg parsing upertagging  studies conducted supertagging-parsing pipline , main focus first step, generated CCG parse trees directly supertags rules afterwards. % known ``almost parsing'' % essential CCG information sentence one generate parse directly supertags rules. % supertagging  contextual information % Building accurate supertagger sequence labeling process requires good modeling contextual information. % Recent neural approaches supertagging mainly focused leveraging powerful encoders recurrent models , limited attention paid modeling extra contextual features word pairs strong relations. % Graph convolutional networks demonstrated effective approach model contextual information words many NLP tasks ; thus want determine whether approach also help CCG supertagging. However, cannot directly apply conventional GCN models CCG supertagging previous studies GCN models built edges dependency tree input sentence. As high-quality dependency parsers always available, want CCG supertaggers rely existence dependency parsers. % Thus, need another way extract useful word pairs build GCN models. For that, propose obtain word pairs frequent chunks corpus, chunks easy identify co-occurrence counts. % % % % Such features, may come n-grams dependency parsing results, demonstrated helpful many NLP tasks , expected enhance CCG supertagging well. % Among features, ones n-grams attractive since n-grams easy obtain also provide word relation cues, dependency parsing results exactly goal CCG thus conflicts problem setting. % % As model encode features, graph convolutional networks one promising choices although often built dependency semantic parse input text. %However, GCN suffers limitation obtaining parsing results, exactly goal CCG thus conflicts problem setting. % %So one expected enhance CCG supertagging. %especially n-gram ones easy obtain provide cues word-word combination appropriately modeled. % %\textcolor{blue}{ %To leverage contextual features, graph convolutional networks one privileging approaches so, graph often built dependency semantic parsing results input text. %However, GCN suffers limitation obtaining parsing results, exactly goal CCG thus conflicts problem setting. %} % \textcolor{red}{ % Consider graph convolutional networks , effective solution learn contextual information demonstrated useful many NLP tasks , potentially useful CCG supertagging.} % , semantic role labeling , sentiment classification , question answering . %input words based results dependency semantic parsing input texts, may appropriate way construct graph CCG, %since task parsing. % \textcolor{blue}{ % Therefore, appropriate way construct graph required CCG n-grams could potentially helpful since carry contextual information provide group words % containing words % may strong relationship respect word-word combination n-grams appropriately selected. % % } % Previous studies using GCN often build graph dependency semantic parsing results input text, suffering limitation obtaining parsing results, exactly goal CCG thus conflicts problem setting. % To appropriately learn n-grams, one requires GCN able distinguish different word pairs information n-grams explicitly structured dependency parses. %In addition, Because existing GCN models limited treating word pairs equally, %while identifying learning essential units important syntactic tasks, propose adaptation conventional GCN CCG supertagging. %especially graph constructed dependencies. % % Inspired n-grams carry contextual information provide span containing words may strong relationships n-grams appropriately selected, build graph upon well selected n-grams. % , especially ones containing words strong relationships other, % % n-gram  contexutal feature % Consider n-grams conventionally used simple yet effective method represent contextual features many NLP tasks %in powerful encoders used % , % -gram  supertagging  n-gram supertagging % also expected serve effective contextual features CCG supertagging, they, \textcolor{blue}{especially ones containing words strong relationships other,} % valid phrases, % provide plausible cues potential combinations among words. %  n-gram  n-gram  supertagger %\textcolor{blue}{ % However, trivial appropriately learn n-grams syntactic tasks, % one needs identify informative n-grams possible combinations words task. %since unimportant ones carrying misleading cues combination may hurt performance supertagger. %} % % ramGCNgram %  channeled attention  model  n-gram %To address problems, In paper, propose attentive GCN CCG supertagging, input graph built based chunks extracted unsupervised methods. % In paper, propose attentive GCN CCG supertagging, input graph built upon word groups suggested high confident n-grams extracted unsupervised methods. % , graph constructed word groups. %which follows sequence labeling paradigm. %  n-gram  % Inspired n-grams carry contextual information provide span containing words may strong relationships n-grams appropriately selected, build graph upon n-grams sentence, edge added pair words n-gram. In detail, two types edges graph introduced model word relations within across chunks %for word groups model word-word relation within cross groups. % build graph words upon n-grams input sentence, edge added pair words span suggested n-gram. % % For edges within group, feed-forward attention applied attention mechanism applied GCN weight edges. %and discriminately learn edges. %In addition, word, attention mechanism used % weight contextual information carried associated words according contribution tagging process. % In so, different contextual information discriminatively learned facilitate CCG supertagging without requiring external resources. % , \textcolor{blue}{within cross chunk relations} % local global word relations % weighted in-chunk cross-chunk edges, respectively. %Moreover, way building graph requires external resources %suggested high confident n-grams learned A-GCN in-group edges; long distance relations among groups also leveraged cross-group edges. %Therefore, hierarchical structure word relations built %Besides, approach proposes novel self-supervised method build graph GCN, extra parsing results required extra input. % , also attentive GCN able discriminately learn contextual information carried different words.} % In proposed attention, n-grams associated word input texts firstly categorized different groups according length, %  n-gram  % fed specific channel attentions according groups, n-grams weighted separately group according contributions supertagging process. %  n-gram n-gram  context information % In so, important n-grams distinguished, also approach discriminatively learn n-grams different length, infrequent long n-grams carrying important long range contextual information appropriately modeled without influenced frequent short ones. % %  The validity approach demonstrated experimental results CCGbank , state-of-the-art performance obtained tagging parsing. In paper, presented simple yet effective way incorporate dictionaries Transformer NMT system, attaching definitions source sentences form nonlinear structure Transformer learn use. We showed method beat baselines significantly, 3.1 BLEU. We also analyzed system's outputs found model learning select adapt parts definition, learn dictionary simply appended training data. We also found method potential work monolingual dictionaries.","  % supertagging  CCG parsing  Supertagging is conventionally regarded as an important task for combinatory categorial grammar  parsing, where effective modeling of contextual information is highly important to this task. %  encoder biLSTM supertagging  task  context feature n-gram However, existing studies have made limited efforts to leverage contextual features except for applying powerful encoders . %  channeled n-gram attention  In this paper, we propose attentive graph convolutional networks to enhance neural CCG supertagging through a novel solution of leveraging contextual information. %  Specifically, we build the graph from chunks  extracted from a lexicon and apply attention over the graph, so that different  % word relations  word pairs from the contexts within and across chunks are weighted in the model and facilitate the supertagging accordingly. %  The experiments performed on the CCGbank demonstrate that our approach outperforms all previous studies % , as well as strong baselines from existing toolkits,  in terms of both supertagging and parsing. %  Further analyses illustrate the effectiveness of each component in our approach to discriminatively learn from word pairs to enhance CCG supertagging.\footnote{Our code and models for CCG supertagging are released at \url{https://github.com/cuhksz-nlp/NeST-CCG}.}"
"Pre-trained Transformers lead state-of-the-art results wide range NLP tasks, example, named entity recognition, relation extraction question answering, often approaching human inter-rater agreement . These models also demonstrated learn effective cross-lingual representations, even without access parallel text bilingual lexicons . Multilingual pre-trained Transformers, mBERT XLM-RoBERTa , support surprisingly effective zero-shot cross-lingual transfer, training development data assumed high resource source language , performance evaluated another target language. Because target language annotations assumed setting, source language data typically used select among models fine-tuned different hyperparameters random seeds. However, recent work shown English dev accuracy always correlate well target language performance . In paper, propose alternative strategy model selection zero-shot setting. Our approach, dubbed Learned Model Selection , learns function scores compatibility fine-tuned multilingual transformer, target language. The compatibility score calculated based features multilingual model's learned representations target language. A model's features based internal representations; done aggregating representations unlabeled target language text corpus. These model-specific features capture information cross-lingual representations transfer target language fine-tuning source language data. In addition model-specific representations, also make use learned language embeddings lang2vec package , shown encode typological information, example, whether language prepositions postpositions. To measure compatibility multilingual model's fine-tuned representations target language, model- language- specific representations combined bilinear layer. Parameters scoring function optimized minimize pairwise ranking loss set held-out models, gold ranking calculated using standard performance metrics, accuracy F, set pivot languages . LMS rely annotated data target language meta-learning hyperparameter tuning, yet effective learning predict whether multilingual model's representations good match specific target language. In experiments five well-studied NLP tasks , find LMS consistently selects models better target-language performance chosen using English dev data. Appendix demonstrates framework supports multi-task learning, helpful settings target-language annotations available, desired task. Finally, show LMS generalizes mBERT XLM-RoBERTa Appendix . In paper, propose A-GCN CCG supertagging, graph built chunks extracted lexicon. We use two types edges graph, namely, in-chunk cross-chunk edges word pairs within across chunks, respectively, propose attention mechanism attention mechanism used enhance model. Specifically, construct graph based word groups suggested high confident n-grams in-group cross-group edges used A-GCN able learn word groups edges. attention mechanism proposed distinguish important word pairs according contribution CCG supertagging.  n-gram n-gram  context information Therefore, important n-grams distinguished, also approach discriminatively learn n-grams different length, especially long infrequent ones carry important long distance contextual information could influenced majority voting effect. Therefore, context features appropriately modeled GCN discriminatively learn them. The effectiveness approach CCG supertagging well parsing demonstrated experimental results ablation study English CCGbank, state-of-the-art performance obtained. Experimental results ablation study English CCGbank demonstrate effectiveness approach CCG supertagging, state-of-the-art performance obtained CCG supertagging parsing. Further analysis performed investigate using different types edges, reveals quality confirms necessity introducing attention GCN CCG supertagging. For future studies, plan explore approaches building graph well performing end-to-end analyze effect CCG supertagging parsing."," Transformers that are pre-trained on multilingual text corpora, such as, mBERT and XLM-RoBERTa, have achieved impressive cross-lingual transfer learning results.  In the zero-shot cross-lingual transfer setting, only English training data is assumed, and the fine-tuned model is evaluated on another target language.  No target-language validation data is assumed in this setting, however substantial variance has been observed in target language performance between different fine-tuning runs.  Prior work has relied on English validation/development data to select among models that are fine-tuned with different learning rates, number of steps and other hyperparameters, often resulting in suboptimal choices.  To address this challenge, we propose a meta-learning approach to model selection that uses the fine-tuned model's own internal representations to predict its cross-lingual capabilities.  In extensive experiments we find that our approach consistently selects better models than English validation data across five languages and five well-studied NLP tasks, achieving results that are comparable to small amounts of target language development data.\footnote{We will make our code and data available on publication.}  %We further demonstrate that our method can benefit from pooling data across tasks when auxiliary annotations are available in the target language."
"% Summarization process identifying important information pieces document. For humans, process heavily guided background knowledge, encompasses preconceptions task priors kind information important . % % % Understanding background knowledge would yield insights what, average, humans consider known, interesting important. % Furthermore, accurate models human background knowledge would greatly valuable improve selection methods information selection systems. % Despite fundamental role, background knowledge received little attention summarization community. Existing approaches largely focus relevance aspect, enforces similarity generated summaries source documents . % , without consideration background knowledge. In previous work, background knowledge usually modeled simple aggregation large background corpora. % A prominent example \cpt{TFIDF} , practical solution problem identifying content words based document frequencies within background corpora. For instance, using \cpt{TFIDF} , one may operationalize background knowledge set words large document frequency background corpora. %While approach useful stopword problem significant development summarization systems, cannot easily extended model background knowledge. However, assumption frequently discussed topics reflect is, average, known necessarily hold. For example, common-sense information often even discussed . Also, information present background texts already gone importance filter humans, e.g., writers publishers. In general, particular difficulty preventing development proper background knowledge models latent nature. We hope infer proxy signals. Besides, is, present, principled way compare evaluate background knowledge models. % In work, put background knowledge foreground propose infer summarization data. Indeed, choices made human summarizers human annotators provide implicit information background knowledge. We build upon recent theoretical model information selection , postulates information selected summary results 3 desiderata: low redundancy , high relevance , high informativeness . The tension 3 elements encoded summary scoring function explicitly depends background knowledge . % explicitly depends background knowledge . As illustrated \Figref{fig:overall}, latent inferred residual differences information selection explained relevance redundancy. For example, black information unit \Figref{fig:overall} selected summary despite prominent source document. Intuitively, explained unit already known receiver. % human summarizer regarded important. To leverage implicit signal, view latent parameter learned best fit observed summarization data. % \xhdr{Contributions} We develop algorithms inferring two settings: pairs documents reference summaries pairs observed pairs document summaries enriched human judgments . % The framework also provides evaluation methodology , measuring well resulting correlates human judgments. In \Secref{sec:comparison} evaluate inferred respect well induced scoring function correlates human judgments. Our proposed algorithms significantly surpass previous baselines large margins. In \Secref{sec:geometry}, give geometrical perpespective framework show clear geometrical structure emerges real summarization data. % The framework simple, constrained interpretable hinder ability fit data. In fact, proposed algorithms significantly largely surpass previous baselines terms correlation human judgments. % The framework general inferring human prior information importance broad use. We explore several applications briefly discuss potential future work. The ability infer interpretable importance priors data-driven way many applications, explore \Secref{sec:applications}. % We explore later discuss possibilities future work. \Secref{sec:qualitative_analysis} qualitatively reveals topics emerge known unkown fitted priors. % First, possible investigate qualitatively fitted priors understand topics emerge known unkown. % We word level topic-model level. Moreover, infer based different subsets data. By training data one annotator, get prior specific annotator. Similarly, one find domain-specific 's training different datasets. This explored \Secref{sec:annotator_specific}, analyze annotators different summarization datasets, yielding interesting insights, e.g., averaging several, potentially biased, annotator-specific domain-specific 's results systematic generalization gains. % Adding inferred 's summarization systems produce improvements quality extracted summaries . Finally, discuss future work potential applications beyond summarization \Secref{sec:ccl}. Our code available \url{https://github.com/epfl-dlab/KLearn} %that averaging various annotator specific 's gives large generalization improvements single annotators compared previous baselines. Furthermore, average annotators performs almost good optimal . Similarly, averaging many domain-specific 's gives significant improvements baselines TAC datasets. %Finally, qualitative analysis best 's reveals capture stopwords properties IDFs even without exposed background corpora. %Background knowledge important summarization often left out. %When left out, requires design choices collection large background corpora. %Previous work defined simple models summarization involves background knowledge first principles %We show formulation allows us infer background knowledge simply observing human preferences. %In fact, probabilistic model developed infer background knowledge pairs document summaries. In paper, presented meta-learning approach model selection zero-shot cross-lingual transfer. We showed approach improves standard practice model selection using source language development data. Experiments five well-studied NLP tasks show inspecting internal representations, method consistently selects better models. LMS also achieves comparable results slower expensive alternative annotating small amounts target-language development data. \subsubsection*{Acknowledgments} We thank Wei Xu helpful feedback. Use unnumbered third level headings acknowledgments. All acknowledgments, including funding agencies, go end paper."," The goal of text summarization is to compress documents to the relevant information while excluding background information already known to the receiver. So far, summarization researchers have given considerably more attention to relevance than to background knowledge. In contrast, this work puts background knowledge in the foreground. Building on the realization that the choices made by human summarizers and annotators contain implicit information about their background knowledge, we develop and compare techniques for inferring background knowledge from summarization data. Based on this framework, we define summary scoring functions that explicitly model background knowledge, and show that these scoring functions fit human judgments significantly better than baselines. We illustrate some of the many potential applications of our framework. First, we provide insights into human information importance priors. Second, we demonstrate that averaging the background knowledge of multiple, potentially biased annotators or corpora greatly improves summary\hyp scoring performance. Finally, we discuss potential applications of our framework beyond summarization. % Finally, we apply our models in a simple yet effective summarization system."
". } Definition Extraction refers task Natural Language Processing detecting extracting term definition different types text. A common use automatic definition extraction help building dictionaries , employed many applications. For example, ontology building benefit methods extract definitions , whilst fields definition extraction information extraction employ similar methodologies. It therefore normal growing interest task definition extraction. This paper describes system participated two three subtasks Task 6 SemEval 2020 , shared task focused definition extraction specialised corpus. Our method employs state-of-the-art neural architectures combination automatic methods extend clean provided dataset. %Task 6 SemEval 2020 shared task definition extraction specialised corpus, tailoured specifically needs definition extraction. This paper describes RGCL team system works three subtasks shared task. We employ state-of-the-art neural architectures combine simple automatic methods extend clean provided dataset appropriate. The remaining parts paper structured follows. First, present related work area definition extraction related field relation extraction . The three subtasks dataset provided task organisers described Section . Next, describe system , followed results evaluation final conclusion . We focus often-ignored background knowledge summarization infer implicit signals human summarizers annotators. We introduced evaluated different approaches, observing strong abilities fit data. We also provide geometrical insights framework inferred background knowledge. The newly-gained ability infer interpretable priors importance data-driven way many potential applications. For example, describe topics extracted frequently systems improve agreement humans. Using pretrained priors also helps systems reduce overfitting frequency signal within source documents illustrated initial results \Appref{sec:summarization}. An important application made possible framework infer meaningful subset data. In particular, learned annotator-specific 's, yielded interesting insights: annotators exhibit large differences others, averaging several, potentially biased 's results generalization improvements. We also inferred 's different summarization datasets also found increased performance news domain averaging 's diverse domains. For future work, different choices semantic units explored, e.g., learning directly embedding space. Also, fixed get comparable results across methods, including learnable parameters could provide performance boosts. Investigating infuse fitted priors summarization systems another promising direction. More generally, inferring common-sense task like summarization provide insights general human importance priors. Inferring priors applications beyond summarization, framework model information selection task. Finally, inferring unobserved importance priors general problem applications beyond summarization. The proposed framework benefit information selection task. method proposed bene information selection task. Put focus background knowledge Data-driven way infer implicit signal summarization data Proposed several approaches work different kind data They work well The general framework inferring priors several potential applications. Some investigated work. For example, found topics extracted more/less summarization systems improve agreement human judgments. Also, use priors help systems overfit frequency signal original documents An interesting application aggregate different subsets data. In particular, obtained annotator specific domain specific priors could compare quantitatively annotators domains. Interestingly, find consistent improvements resulting averaging several, potentially biased, priors. The framework also application beyond summarization methodology easily extended general information selection tasks . Within summarization, one also explore use different semantic units, particular learning directly semantic sapce embeddings could interesting. Also, fix parameters 1, learning parameters alongside would give better ability fit data. Finally, investigating infuse fitted priors summarization systems promising direction improviment systems. In work, leveraged summarization data infer background knowledge. We inferred annotator domain-specific priors found large benefits resulting averaging different background knowledge. For future work, human priors used improve summarization systems also automatic evaluation metrics. Another promising direction could study different semantic unit representations, e.g., distributional representations. In general, better understanding human priors background knowledge benefit wide range applications like information retrieval dialog systems. Introduction Include file LaTeX papers write dlab adding line """" right ""\documentclass"" command. Some standard packages \usepackage[utf8]{inputenc} \usepackage[T1]{fontenc} \usepackage{hyphenat} \usepackage{xspace} \usepackage{amsmath} \usepackage{amsfonts} \usepackage{hyperref} \usepackage{url} \usepackage{booktabs} \usepackage{multirow} \usepackage{subfig} \usepackage{makecell} \usepackage{caption} \usepackage{minibox} \usepackage{bbm} \usepackage{graphicx} \usepackage{balance} \usepackage{mathtools} \usepackage{color} \usepackage{marvosym} \usepackage{ifthen} \usepackage{textcomp} \usepackage{enumitem} \usepackage{verbatim} \usepackage{algorithm} \usepackage{algorithmic} \usepackage{numprint} \usepackage{balance} \usepackage{amsthm} \theoremstyle{plain} \newtheorem{theorem}{Theorem} \newtheorem{definition}[theorem]{Definition} \newtheorem{lemma}[theorem]{Lemma} \newtheorem{proposition}[theorem]{Proposition} \newtheorem{example}[theorem]{Example} How include TODOs notes Adapted widely circulating chato-notes.sty -- thanks, ChaTo! \newcommand{\chatoDisplayMode}[1]{#1} If quickly want hide notes, e.g., check long paper would without them, add following line preamble uncomment here. \renewcommand{\chatoDisplayMode}[1]{} Usage: \todo[Your name]{What needs done} \note[Your name]{A note include box} \inote{An inline note} \citemissing{} \definecolor{MyRed}{rgb}{0.6,0.0,0.0} \definecolor{MyBlack}{rgb}{0.1,0.1,0.1} \newcommand{\inred}[1]{{\color{MyRed}\sf}} \newcommand{\frameit}[2]{ }\\ } } \newcommand{\note}[2][]{\chatoDisplayMode{\def\@tmpsig{#1}\frameit{{\Pointinghand} Note}{#2\ifx \@tmpsig \@empty \else \mbox{ --\em #1}\fi}}} \newcommand{\todo}[2][]{\chatoDisplayMode{\def\@tmpsig{#1}\frameit{{\Writinghand} To-do}{#2\ifx \@tmpsig \@empty \else \mbox{ --\em #1}\fi}}} \newcommand{\inote}[1]{\chatoDisplayMode{\inred{{{\Pointinghand} }} {\sf #1} \inred{}}} \newcommand{\citemissing}[0]{\chatoDisplayMode{\inred{[citation]}}} How make edits conspicuous In final stages editing, often useful mark edits color, everyone easily see changed. To so, define command name use favorite color. \newcommand{\bob}[1]{{#1}} \newcommand{\yourname}[1]{{#1}} Latin abbreviations Don't use plain text Latin abbreviations ""e.g."", ""i.e."", etc. Use macros instead. Advantage: consistently change style, e.g., want typeset italics point. Latin abbreviations normal font. \newcommand{\abbrevStyle}[1]{#1} Latin abbreviations italics. \newcommand{\abbrevStyle}[1]{#1} \newcommand{\ie}{\abbrevStyle{i.e.}\xspace} \newcommand{\eg}{\abbrevStyle{e.g.}\xspace} \newcommand{\cf}{\abbrevStyle{cf.}\xspace} \newcommand{\etal}{\abbrevStyle{et al.}\xspace} \newcommand{\vs}{\abbrevStyle{vs.}\xspace} \newcommand{\etc}{\abbrevStyle{etc.}\xspace} \newcommand{\viz}{\abbrevStyle{viz.}\xspace} Referring sections, figures, tables, etc. To refer sections, figures, tables, etc., use following macros. Don't type ""Section~1"", ""Fig.~1"", etc., manually. This way, easily consistently switch styles, e.g., want use ""Sec."" instead ""Section"" point. \newcommand{\Secref}[1]{Sec.} \newcommand{\Eqnref}[1]{Eq.} \newcommand{\Dashsecref}[2]{Sec.--} \newcommand{\Dblsecref}[2]{Sec. } \newcommand{\Tabref}[1]{Table} \newcommand{\Figref}[1]{Fig.} \newcommand{\Dashfigref}[2]{Fig.--} \newcommand{\Appref}[1]{Appendix} \newcommand{\Thmref}[1]{Thm.} \newcommand{\Lemmaref}[1]{Lemma} \newcommand{\Defref}[1]{Def.} Paragraph headings Academic text often much legible give important paragraphs concise name describes paragraph about. Use \xhdr command this. \newcommand{\xhdr}[1]{{{\bf #1.}}} Same \xhdr, without period heading. Use version heading directly integrated first sentence paragraph; e.g., ""\xhdrNoPeriod{Results} shown \Figref{fig}."" \newcommand{\xhdrNoPeriod}[1]{{{\bf #1}}} More compact lists In styles, list items widely spaced. To condense save space, may use command. \newcommand{\denselist}{ \itemsep -2pt\topsep-10pt\partopsep-10pt } Same, slightly different spacing. \newcommand{\denselistRefs}{ \itemsep -2pt\topsep-5pt\partopsep-7pt } Miscellaneous useful macros Some bibliography styles make hard typeset references like ""Einstein et al. "". This command provides convenient way so. \newcommand{\textcite}[1]{\citeauthor{#1} \shortcite{#1}} When frequently refer Wikipedia articles, Wikidata entities, etc., may useful typeset particular font. Use \cpt command purpose. \newcommand{\cpt}[1]{}} To exclude large portion text PDF, wrap \hide. \newcommand{\hide}[1]{} Wrap matrix variables \mtx. Don't make bold etc. manually. By using macro, consistently change rendering style point. \newcommand{\mtx}[1]{\mathbf{#1}} Transpose matrix, e.g., . \newcommand{\trans}{^\top} \argmin \argmax. \DeclareMathOperator*{\argmax}{arg\,max} \DeclareMathOperator*{\argmin}{arg\,min} Hyphenation Some words ill-hyphenated default. Here define correct hyphenation once, used consistently. \hyphenation{ Wi-ki-pe-dia Wi-ki-me-dia Wi-ki-da-ta De-ter-mine Page-Rank web-page web-pages da-ta-set } Avoid widows! The term ""widow"" refers first line paragraph last line page, last line paragraph first line page. Widows considered cardinal typesetting sin, avoid cost, via following commands. \widowpenalty=10000 \clubpenalty=10000 Enable section numbering AAAI style In AAAI style, enables section numbering. \setcounter{secnumdepth}{2} Listing authors space-economic way ACM style By default, using ""\documentclass[sigconf]{acmart}"" list authors rows 2, take lot space. To get authors one row, use something like this: \author{ \authorbox{Author 1}{Affiliation 1}{Email 1} \authorbox{Author 2}{Affiliation 2}{Email 2} ... } If use \authorbox, also suppress standard reference block, pasting following row somewhere "" BEFORE BOB'S EDITS: Despite essential aspect information selection process, background knowledge received little attention summarization field. In contrast, work puts focus neglected component. We emphasize choices made human summarizers annotators contain implicit information priors. Thus, develop compare several approaches leveraging signals. This produces data-driven interpretable information importance priors fit human judgment data significantly better baselines. We illustrate many potential applications. First, investigate topics received low high weight inferred priors. By using different aggregation data, obtain annotator\hyp specific domain\hyp specific priors. A simple analysis yields interesting insights, e.g., averaging many, potentially biased, priors systematically greatly improves performance. Finally, resulting priors used guide summarization systems.","   This paper presents the RGCL team submission to SemEval 2020 Task 6: DeftEval, subtasks 1 and 2. The system classifies definitions at the sentence and token levels. It utilises state-of-the-art neural network architectures, which have some task-specific adaptations, including an automatically extended training set. Overall, the approach achieves acceptable evaluation scores, while maintaining flexibility in architecture selection."
"Event extraction process extract named entities, event triggers relationships real-world corpora. The named entities refer texts predefined classes event triggers words express types events texts . In literature, named entities triggers connected named entities corresponding roles called arguments given trigger specific event. %Named entities refer text mentions predefined classes person names, company names locations, etc. An event trigger word mostly expresses event types text. Named entities link triggers different roles, named entities corresponding roles called arguments given trigger specific event. Currently, existing works divide event extraction two independent sub-tasks: named entity recognition trigger labeling. These two sub-tasks always formulated multi-class classification problems, many works apply sequence-to-sequence based labeling method aims translate sentence sequential tags. From investigation, one problem sequence-to-sequence methods ignore orders output tags, therefore, difficult precisely annotate different parts entity. To address issue, methods propose incorporate conditional random field module aware order-constraints annotated tags. Since entities triggers naturally connected around events, recent works try extract jointly corpora. Early methods apply pipeline frameworks predefined lexical features lack generality different applications. Recent works leverage structural dependency entities triggers improve performances entity trigger identification sub-tasks. %The prevalent methods divided two categories: a) parallel framework obtain entities triggers simultaneously b) pipeline framework get triggers first perform sub-tasks extract entities. Takanobu et al. propose hierarchical reinforcement learning model extract triggers first evoke sub-process get related entities referring obtained triggers sentences. Nguyen et al. design attention mechanism augment accuracy trigger extraction multilingual environments. Fu el al. employ graph convolutional network capture local contextual information sentences use two-stage method extract entities triggers text together. % The main challenges improve performance jointly extract entities triggers two-fold: Although existing works achieved comparable performance jointly extracting entities triggers, approaches still suffer major limitation losing co-occurrence relationships entities triggers. Many existing methods determine trigger entities separately match entities triggers. % In way, co-occurrence relationships entities triggers ignored, therefore, methods might require pre-trained features prior data order achieve better performance. In way, co-occurrence relationships entities triggers ignored, although pre-trained features prior data introduced achieve better performance. It also challenging capture effective co-occurrence relationships entities triggers. We observed experiments entities triggers co-occurred sparsely throughout corpus. This issue exacerbates problem losing co-occurrence relationships mentioned before. %However, existing methods suffer performance degradation extracting entities triggers jointly. The reason entities triggers sparsely co-occurred throughout corpus previous approaches well handle sparse co-occurred relationship. %In addition, challenging establish effective interaction mechanism sub-tasks joint-event-extraction, traditional joint learning may lead error-propagation issue lowers accuracy joint tasks. %% label entire figure \end{figure*} To address aforementioned challenge, core insight paper joint-event-extraction task, ground-truth annotations triggers could leveraged supervise extraction entities, vice versa. Based insight, paper proposes novel method extract structural information corpora utilizing co-occurrence relationships triggers entities. Furthermore, order fully address aforementioned sparsely co-occurrence relationships, model entity-trigger co-occurrence pairs heterogeneous information network supervise trigger extraction inferring entity distribution given triggers based indirect co-occurrence relationships collected along meta-paths heterogeneous information network . Figure illustrates process proposed method collect indirect co-occurrence relationships entities triggers. Figure sub-graph ``entity-trigger'' HIN ACE 2005 corpus. Figure compares entity distributions inferred given triggers based direct adjacency matrix inferred meta-path adjacency matrix. From figure, observe trigger necessarily connect entities directly direct-adjacency-based distribution concentrated entities, meta-path-based distribution spread larger number entities. This shows model could collect indirect co-occurrence patterns entities triggers based meta-path adjacency matrix ``entity-trigger'' HIN. Moreover, obtained indirect patterns could applied improve performance extract entities triggers. Based aforementioned example analysis, propose neural network extract event entities triggers. Our model built top sequence-to-sequence labeling framework inner parameters supervised ground-truth annotations sentences ``entity-trigger'' co-occurrence relationships. Furthermore, fully address indirect ``entity-trigger'' co-occurrence relationships, propose \underline{C}ross-\underline{S}upervised \underline{M}echanism based HIN. The CSM alternatively supervises entity trigger extraction indirect co-occurrence patterns mined corpus. CSM builds bridge triggers entities collecting latent co-occurrence patterns along meta-paths corresponding heterogeneous information network corpus. Then obtained patterns applied boost performances entity triggers extractions alternatively. We define process ``cross-supervise'' mechanism. The experimental results show method achieves higher precisions recalls several state-of-the-art methods. In summary, main contributions paper follows: The remainder paper organized follows. In Section, first introduce preliminary knowledge event extraction HIN, also formulate problem. Section presents proposed model detail. Section verifies effectiveness model compares state-of-the-art methods real-world datasets. Finally, conclude paper Section. We presented system RGCL team prepared SemEval-2020 Task 12. The design system allows easy switching different architectures accommodate needs task hand. For task, shown Transformer architecture using XLNet successful working limited resources. It also shown data augmentation techniques experimented, detrimental overall performance, necessarily improve performance. In shared task setting, effect extended data Wikipedia useful, however, wider approach higher recall, could helpful. We also tried participate final subtask, Relation Classification. However, due time constraints, able achieve valid submission subtask. We approached sequence pair classification task employed Siamese Neural Network shown perform well sequence pair classification tasks . The architecture employed similar architecture presented . When two sequences relation, extracted sequences provided input Siamese transformer architecture. Then used objective function suggested classification objective function optimised cross-entropy loss. Due complexity task, managed run baseline proposed architecture achieved low evaluation scores development data. Therefore, submission task present results here. In future, hope carry experiments Siamese transformer architectures relation classification tasks. Going forth, also wish use system tasks across languages. While may achieve best performance, system utilises realistic system resources therefore versatile. This particularly regard first subtask, difference best team around 0.09, whereas subtask two best team 0.36 ahead us, indicating system competitive. It possible extend experiments different domain easily using pretrained transformer model domain given corpus similar deft corpus available domain. For example, system easily adoptable biology domain using BioBERT pretrained transformer model deft corpus like corpus biology domain. include bib file like this:"," Joint-event-extraction, which extracts structural information  from unstructured real-world corpora, has attracted more and more research attention in natural language processing. Most existing works do not fully address the sparse co-occurrence relationships between entities and triggers, which loses this important information and thus deteriorates the extraction performance. To mitigate this issue, we first define the joint-event-extraction as a sequence-to-sequence labeling task with a tag set composed of tags of triggers and entities. Then, to incorporate the missing information in the aforementioned co-occurrence relationships, we propose a \underline{C}ross-\underline{S}upervised \underline{M}echanism  to alternately supervise the extraction of either triggers or entities based on the type distribution of each other. Moreover, since the connected entities and triggers naturally form a heterogeneous information network , we leverage the latent pattern along meta-paths for a given corpus to further improve the performance of our proposed method. To verify the effectiveness of our proposed method, we conduct extensive experiments on four real-world datasets as well as compare our method with state-of-the-art methods. Empirical results and analysis show that our approach outperforms the state-of-the-art methods in both entity and trigger extraction."
"Recently, pre-trained self-supervised models BERT attracted increasing amount attention natural language processing vision-language processing. Benefiting common knowledge contained massive unlabeled data, pretraining-finetuning framework become representative paradigm advancing various language-related downstream tasks. Most endeavors pre-trained representation models rely elaborately designed self-supervised tasks, typically corrupt given sequence certain types noise , train model recover original sequence. As consequence, learned representations tend covariant input noise pre-training paradigm. However, transferred downstream tasks, pre-trained model responsible encoding original sequence without noise, expected obtain noise invariant representations. Such pretrain-finetune discrepancy impedes fast fine-tuning, also may result suboptimal sequence representations, thus affecting performance downstream tasks. %%%%%%%%%%%% % % \vskip -0.1in % \end{table} %%%%%%%%%%%% %%%%%%%%%%%% % %%%%%%%%%%%% To remedy this, present ContrAstive Pre-Training learn noise invariant sequence representations. %, inspired Noise Contrastive Estimation. The core idea CAPT enhance consistency semantic representations original sequence corresponding corrupted version via unsupervised instance-wise training signals. %can fully utilized via elaborately designed semantic contrastive loss. %As shown Figure, approach In detail, strives pull representation corrupted sequence towards original instance semantic space, pushing away representations instances. % Such training objectives formulated multi-class classification task, aims classifying original sequence class corrupted version vice versa, classifying different instances different classes. % For implementation feasibility, two effective model extension proposed enhance capability model extract noise-concentrated instance-diffused features. Moreover, order enable model learn ``difficult'' ``diverse'' instances, two effective methods proposed enhance capability model extract noise-concentrated instance-diffused features. With training objective, pre-trained model encouraged learn noise invariant representations, thereby alleviating pretrain-finetune discrepancy extent. As additional benefit, CAPT also assists pre-trained model effectively capture global semantics input. Most prior work focuses token-level pre-training tasks , lacks modeling global semantics input. Some efforts alleviate problem introducing sentence-level pre-training tasks rely relative position segments document. However, semantic connection segments tends excessively loose, may result confusing gradient signals. By contrast, CAPT offers incentives representations inputs sharing semantics similar, representations inputs expressing different semantics penalized distinguished other. Such reasonable sentence-level supervision enables approach look beyond local structures input sequences become aware global semantics. %With reasonable sentence-level supervision, approach achieves better modeling global semantics input. We perform evaluation comprehensive suite benchmark, covering 8 natural language understanding 3 cross-modal tasks. Extensive empirical evidence demonstrates approach achieve consistent improvements baselines language vision-language domains. To specific, CAPT raises performance RoBERTa 88.9\% 89.5\% GLUE dev set, also surpasses LXMERT 0.5\%, 0.6\% 0.8\% VQA, GQA , respectively. In paper, proposed novel cross-supervised mechanism allows models extract entities triggers jointly. Our mechanism alternately supervises extraction process either triggers entities, based information type distribution other. In way, incorporate co-occurrence relationships entities triggers joint-event-extraction process model. Moreover, address problem caused sparse co-occurrence relationships, method also resorts heterogeneous information network technology collect indirect co-occurrence relationships. The empirical results show method improves extraction performances entities triggers simultaneously. This verifies incorporated co-occurrence relationships useful joint-event-extraction task method effective existing methods utilizing training samples. Our future works include: investigating impact length sampled meta-paths, paper limited meta-path fixed length; connecting extracted entities triggers corpus facilitate automatic knowledge graph construction."," Pre-trained self-supervised models such as BERT have achieved striking success in learning sequence representations, especially for natural language processing. These models typically corrupt the given sequences with certain types of noise, such as masking, shuffling, or substitution, and then try to recover the original input. However, such pre-training approaches are prone to learning representations that are covariant with the noise, leading to the discrepancy between the pre-training and fine-tuning stage. To remedy this, we present ContrAstive Pre-Training  to learn noise invariant sequence representations. The proposed CAPT encourages the consistency between representations of the original sequence and its corrupted version via unsupervised instance-wise training signals. In this way, it not only alleviates the pretrain-finetune discrepancy induced by the noise of pre-training, but also aids the pre-trained model in better capturing global semantics of the input via more effective sentence-level supervision. Different from most prior work that focuses on a particular modality, comprehensive empirical evidence on 11 natural language understanding and cross-modal tasks illustrates that CAPT is applicable for both language and vision-language tasks, and obtains surprisingly consistent improvement, including 0.6\% absolute gain on GLUE benchmarks and 0.8\% absolute increment on $\text{NLVR}^2$."
"\subsection{Natural Language Processing} Ang Natural Language Processing ay isang subfield ng linguistics, computer science, artificial intelligence na nauukol sa pag proseso pag-unawa ng natural na wika . Ang ilan sa mga aplikasyon ng NLP ay ang email spam filters , pag-unawa ng nais sabihin tulad ng mga smart assistants , pagsasalin ng isang wika sa iba pang wika , mag predict ng susunod na salita base sa mga naunang salita , marami pang iba. Dahil sa kaunlaran sa kasaganahan sa datos pagiging accessible ng malakas na compute power, nabuhay muli ang machine learning approach. Sa maikling salita, ang machine learning approach ay gumagamit ng malaking datos na ginagamit ng isang computer algorithm upang matutunan ang mga patterns ng datos na ito. Dahil dito, naging epektibo siyang approach sa mga komplikadong problema dahil hindi na kailangan direktang i-program ang mga rules para malutas ang isang problema. \subsection{Transfer Learning} Notorious ang machine learning approach sa pangangailangan nito ng sobrang laking datos para mapakinabangan. Ang Transfer Learning ay isang area ng research na concerned sa problemang ito . Sa maikling salita, ang TL ay ang pag retain pagpapanatili ng mga natutunan ng isang model sa isang gawain paggamit ""transfer"" ng mga natutunan nito sa iba pero may kaugnayan na gawain. Halimbawa, ang mga natutunan ng isang model sa pag detect ng muka ng tao ay maaring gamitin bilang tuntungan para sa pag-aaral ng model na matutunan kung ang muka ng tao ay galit, masaya, iba pang facial expressions . This work presents contrastive pre-training learning denoised sequence representations self-supervised manner. By enhancing consistency representations original sequence corresponding corrupted version, pre-trained model encouraged learn noise invariant sequence representations. On account, proposed approach alleviates pretrain-finetune discrepancy induced noise pre-training, also better captures global semantics input via effective sentence-level supervision. Extensive experiments demonstrate effectiveness versatility approach, achieve consistent improvements baselines language vision-language domains."," Ang mga low-resource languages tulad ng Filipino ay gipit sa accessible na datos kaya't mahirap gumawa ng mga applications sa wikang ito. Ang mga Transfer Learning  techniques ay malaking tulong para sa low-resource setting o mga pagkakataong gipit sa datos. Sa mga nagdaang taon, nanaig ang mga transformer-based TL techniques pagdating sa low-resource tasks ngunit ito ay mataas na compute and memory requirements kaya nangangailangan ng mas mura pero epektibong alternatibo. Ang papel na ito ay may tatlong kontribusyon. Una, maglabas ng pre-trained AWD-LSTM language model sa wikang Filipino upang maging tuntungan sa pagbuo ng mga NLP applications sa wikang Filipino. Pangalawa, mag benchmark ng AWD-LSTM sa Hate Speech classification task at ipakita na kayang nitong makipagsabayan sa mga transformer-based models. Pangatlo, suriin ang performance ng AWD-LSTM sa low-resource setting gamit ang degradation test at ikumpara ito sa mga transformer-based models."
"\iffalse \dr{%If want reposition abstract, start considering event Fig. 1: Natural language text typically written tell reader events. But events expressed single predicate mentions, rather structures multiple predicates arguments. Consider description impact Typhoon Fig..... It mentioned typhoon killed people , flights canceled affected many people. It also clear temporal order among predicates, recognizing important understanding composite event. Then continue saying goal.} \fi % typically, single predicate mention constitute typically think events; typically think event something consists multiple primitive structures %{\fontsize{10.5}{11} \selectfont Text} %\fontsize{11pt}{13pt}\selectfont Human languages evolve communicate %always involve description real-world events. Therefore, understanding events plays critical role natural language understanding . A key challenge mission lies fact events simple, standalone predicates. Rather, often described different granularities may form complex structures. %topologies. Consider example Figure, description storm involves fine-grained event mentions people killed , flights canceled passengers affected . Some mentions also follow strict temporal order . Our goal induce event complex recognizes %organizes membership multi-granular events described text, well temporal order. This core text understanding, also beneficial various applications question answering , narrative prediction , timeline construction summarization . %\dr{The choice references good revealing; I suggest replace summarization ``classical"" summarization paper . %such question answering , narrative prediction , coreference resolution , summarization . Since events standalone objects, understanding event essentially involves comprehending relations, %cite{wities-etal-2017-consolidated, wadden-etal-2019-entity}, relations , well internal structures processes . inasmuch necessarily provide actionable knowledge support question answering , narrative prediction , timeline construction summarization . \muhao{TODO: forming call ``event complex''} Human languages always involve description real-world events. Therefore, understanding events plays critical role natural language understanding , supports tasks question answering , narrative prediction , timeline construction summarization . Typically, events standalone predicate mentions, rather structures multiple predicates. Consider example Figure. The description impact storm also involves mentions killed people , canceled flights affected passengers . Some mentions thereof also follow temporal order. To support comprehension complex events, important recognize multifaceted relations predicate mentions text. \fi % second paragraph \iffalse Recently, much research effort put extracting specific aspects relations events. \citet{ning-etal-2018-improving} studied event temporal relation extraction statistical common sense resource \citet{ning-etal-2019-improved} \citet{han-etal-2019-joint} adopted data-driven methods TempRel extraction; parent-child relations among events studied \citealp[]{liu-etal-2018-graph} \citealp[]{aldawsari-finlayson-2019-detecting}. Though previous work ensured consistency via adding constraints inference phase, essentially improving local predictions inconsistent results models might corrected inference stage. Besides, approaches suffered limited learning resources tasks studied separately. \fi Recently, significant %much research effort devoted several event-event relation extraction tasks, event temporal relation extraction subevent relation extraction . Addressing challenging tasks requires model recognize inherent connection event %\dr{should predicate mentions, ease ambiguity?} mentions well contexts documents. Accordingly, previous methods apply statistical learning methods characterize grounded events documents . Such methods often require designing various features characterize structural, discourse narrative aspects events, costly produce often specific certain task dataset. More recent works attempted use data-driven methods based neural relation extraction models refrain feature engineering offer competent performances. \iffalse \dr{The next two paragraphs shortened, right paragrpahs include here.} While data-driven methods provide general tractable way capture specific event-event relations, still remains challenging methods precisely infer correct relations. One challenge almost every task event-event relation extraction comes limited available annotated resources. Specifically, tasks annotate hundred articles . Even largest one literature, i.e., MATRES TempRel extraction, contains annotation merely 275 articles. The lack supervision hinders feature learning events well inference relations, %Therefore, effectively tackling tasks inevitably calls therefore calling upon plausible auxiliary supervision resources external tasks. On hand, event-event relations often constrained %\drc{logical \dr{}change everywhere} %logic %\muhao{done.} properties, transitivity TempRels Before After , well %the relation parent child events subevent relations . In favor constraints, literature employed global inference inference phase comply logical properties particularly TempRels . However, lacks effective way ensure global logical consistency training phase, key making data-driven machine learning model consistent beliefs training data various relation types . Moreover, logical constraints may apply different categories %event-event relations, form complex conjunctive rules. Consider example Figure : given e2:died Before e3:canceled e3:canceled parent event e4:affecting, learning process enforce e2:died Before e4:affecting. %\todo{Add example conjunctive rule containing temporal subevent relations.} Accordingly, ensuring logical constraints across task-specific relations another challenge overlooked literature, resolve provides natural way bridge learning processes multiple tasks. %\magenta{HW:TCR?} \fi While data-driven methods provide general tractable way event-event relation extraction, performance restricted limited annotated resources available. For example, largest temporal relation extraction dataset MATRES 275 articles, far enough training well-performing supervised model. The observation relations and, particular, event-event relations constrained logical properties , led employing global inference comply transitivity symmetry consistency, specifically TempRel . However, event complex, logical constraints may globally apply different task-specific relations, form complex conjunctive constraints. Consider example Figure : given e2:died Before e3:canceled e3:canceled Parent event e4:affecting, learning process enforce e2:died Before e4:affecting considering conjunctive constraints TempRel subevent relations. While previous works focus preserving logical consistency inference structured learning , %lacks effective way endow neural models sense global logical consistency training. %\dr{Notice previous statement correct; I change limit neural models, since structure learning it} %ensure global logical consistency training phase. This key bridging %bridge learning processes %on TempRel subevent relations, research focus paper. %Event-relation extraction non-trivial task following challenges: %1) Almost every event relation extraction task comes limited learning resources annotations. %2) Event relations often volatile given different scenarios, determination parent-child relation especially difficult since less explicit lexical expressions compared cases time causation. %3) Event relations often endowed logical properties: % temporal relations parent-child relations comply transitivity; % logical consistency also ensured across different categories event relations. The first contribution work proposing %to propose joint constrained learning model multifaceted event-event relation extraction. The joint constrained learning framework seeks regularize model towards consistency logical constraints across temporal subevent relations, three types consistency requirements considered: annotation consistency, symmetry consistency conjunction consistency. Such consistency requirements comprehensively define interdependencies among relations, essentially unifying ordered nature time topological nature multi-granular subevents based set declarative logic rules. Motivated logic-driven framework proposed \citet{li-etal-2019-logic}, declarative logical constraints converted differentiable functions incorporated learning objective relation extraction tasks. Enforcing logical constraints across temporal subevent relations also natural way combine %two event-event relation extraction tasks shared learning objective. supervision signals coming two different datasets, one relation extraction tasks shared learning objective. %\dr{You said first contribution, second; want claim second contribution? Note I modified emphasize two datasets} %Besides, consistency final prediction enforced global inference via ILP solver. Despite scarce annotation tasks, proposed method surpasses SOTA TempRel extraction method MATRES relatively 3.27\% ; %\dr{I understand -- relative F1? Also, Tab. 2 shows 2.5\%} also offers promising performance HiEve dataset subevent relation extraction, relatively surpassing previous methods least 3.12\% . %\dr{which table from?} %by 3.12\% 21.4\%. %We provide ablation studies show importance component framework. %This fact illustrated ablation studies. From NLU perspective, %the acquired knowledge method able simultaneously models internal membership structure complex event, well temporal relations among simple complex events. second contribution work lies providing general method inducing event complex comprehensively represents relational structure several related event %\drc{predicate} % mentions. %in two directions. This supported memberships vertically identified multi-granular events, well horizontal temporal reasoning within event complex. As far know, %essentially different %many previous works formulated relations along single axis. Our model demonstrates potent capability inducing event complexes %with promising performance evaluated %based RED dataset . In paper explore problem seed-guided topical taxonomy construction. Our proposed framework \corel completes taxonomy structure relation transferring module enriches semantics concept nodes concept learning module. The relation transferring module learns user-interested relation preserved seed parent-child pairs, transfers along multiple paths expand taxonomy width depth. The concept learning module finds discriminative topical clusters concept process jointly embedding concepts words. Extensive experiments show modules work effectively generating high-quality topical taxonomy based user-given seeds. For future work, interesting study generate multi-faceted taxonomy automatically, concept node described terms different aspects . Though terms captured concept learning module, recognize organize meaningful clusters remains challenging worth exploring.\clearpage \onecolumn","     %\dr{I think that the current version  is too detailed and does not position the work at all, it just says what is being done. Here is a suggestion:}    Understanding natural language involves recognizing how multiple event mentions structurally and temporally interact with each other.     In this process, one can induce event complexes that organize multi-granular events with temporal order and membership relations interweaving among them.    Due to the lack of jointly labeled data for these relational phenomena and the restriction on the structures they articulate, we propose a joint constrained learning framework for modeling event-event relations.    Specifically, the framework enforces logical constraints within and across multiple temporal and subevent relations     %of events     by converting these constraints into differentiable learning objectives. We show that our joint constrained learning approach effectively compensates for the lack of jointly labeled data, and outperforms SOTA methods on benchmarks for both temporal relation extraction and event hierarchy construction, replacing a commonly used but more expensive global inference process.    We also present a promising case study showing the effectiveness of our approach in inducing event complexes on an external corpus.\footnote{Our code is publicly available at \url{https://cogcomp.seas.upenn.edu/page/publication_view/914}.} %\dr{Doesn't this contradict the statement above regarding the lack of joint data? Do we need to address it somehow}    %\dr{do we need the next clause? really, you show that you don't need it, but it reads like you just don't use it. If you really want to keep it, maybe better to say ""replacing a commonly used, more expensive, global inference process""} even without global inference that is widely used in previous methods.     \iffalse     \drc{Understanding events described in natural language text requires a reader to identify how they interact, structurally and temporally, to form an event complex.      Nevertheless, most of the work in NLP has focused on predicate mentions and not on the event complex they form together.      In this paper we study the induction of larger event units from text -- identifying a set of predicate mentions that together -- via temporal, co-reference, and subevent relations, form event complexes.     The scarcity of jointly labeled data for these relational phenomena presents a significant technical challenge. However, these phenomena interact with each other, thus restricting the structures they articulate. To make this explicit, we propose a joint learning framework that enforces logical constraints among the relations to be identified, by converting these into differentiable learning objectives.      We show that not only does our joint training approach address the lack of jointly labeled data, but it also outperforms SOTA results on both the temporal benchmark data set and the event hierarchy benchmark data set. %We also present a promising case study on RED, a small-scale dataset with fully annotated relations.     }     \fi     \ignore{     We study within-document temporal and hierarchical relations of events using a joint constrained learning framework.      %We first obtain the event representation  via an encoder, and then jointly train a multi-layer perceptron to predict confidence scores for temporal and hierarchical relations before we make structured prediction via integer linear programming .      The framework first incorporates a contextualized encoder to characterize the events in the document, and then predicts the confidence scores for temporal and hierarchical relations among them.     In the training phase, our framework learns to enforce logic consistency among various types of event relations in both categories,     by converting declarative rules into differentiable learning objective functions. %Furthermore, the consistency of final prediction is enforced by global inference .      %The inference phase performs structured prediction based on integer linear programming  to respect the corresponding logic constraints of relations.     %We utilize the benchmark dataset for the extraction task of each category of relations for training and evaluation. %By experimental results, we prove the feasibility of joint constrained learning of different tasks using datasets that have partial annotations for each task, %avoiding the labor for creating another dataset that has full annotation.     The experimental results show that the proposed framework outperforms the state-of-the-art method on the benchmark dataset, MATRES, of event temporal relation extraction task by 2.8\%; and it improves over the model of training jointly without constraints by 5\% F1-score on HiEve dataset, a benchmark for event hierarchy construction.     Therefore, the joint constrained learning effectively bridges the tasks with limited annotated learning resources, and promisingly leverages domain rules to support the precise learning and inference of various event relations.     }"
"Word embeddings capture semantic similarities extensively explored wide spectrum Natural Language Processing applications recent years. Word2Vec , FastText , Glove examples. Even though distributional word embeddings produce high quality representations, representing longer pieces text sentences paragraphs still open research problem. A sentence embedding contextual representation sentence often created transformation word embeddings composition function. There large body work literature propose different approaches represent sentences word embeddings. SkipThought , InferSent , Universal Sentence Encoder well-known examples. % Other proposed methods learning sentence representations include, limited . There growing interest understanding linguistic knowledge encoded deep contextual representation language. For purpose, several probing tasks proposed understand representations capturing . One interesting findings despite existence explicit syntactic annotations, learned deep representations encode syntax extent . Hewitt et. al. provide evidence entire syntax tree embedded implicitly deep model's vector geometry. Kuncoro et. al. show LSTMs trained language modeling objectives capture syntax-sensitive dependencies. Even though deep contextual language models implicitly capture syntactic information sentences, explicit modeling syntactic structure sentences shown improve results different NLP tasks including neural language modeling \cite {shen2017neural, havrylov2019cooperative}, machine comprehension , summarization , text generation , machine translation , authorship attribution , etc. Furthermore, Kuncoro et. al. provide evidence models explicit syntactic information result better performance . Of particular interest, one areas syntactic structure sentences plays important role style-based text classification tasks, including authorship attribution. The syntactic structure sentences captures syntactic patterns sentences adopted specific author reveal author structures sentences document. Inspired observations, initial work demonstrates explicit syntactic information sentences improves performance recurrent neural network classifier domain authorship attribution . We continue work paper investigating structural representation sentences learned explicitly. In words, similar pre-trained word embeddings mainly capture semantics, pre-trained embeddings mainly capture syntactic information words. Such pre-trained word embeddings used conjunction semantics embeddings different domains including authorship attribution. For purpose, propose self-supervised framework using Siamese network explicitly learn structural representation sentences. The Siamese network comprised two identical components; lexical sub-network syntactic sub-network; take sequence words sentence corresponding linearized syntax parse tree inputs, respectively. This model trained based contrastive loss objective pair vectors close embedding space belong identical sentence , far belong two different sentences . As result, word sentence embedded vector representation mainly carries structural information. Due -to- mapping word types structural labels, word representation deduced structural representations. In words, semantically different words mapped similar structural labels ; hence, semantically different words may similar structural representations. These pre-trained structural word representations used complimentary information pre-trained semantic embeddings . We use probing tasks proposed Conneau et al. investigate linguistic features learned training. The results indicate structural embeddings show competitive results compared semantic embeddings, concatenation structural embeddings semantic embeddings achieves improvement. Finally, investigate efficiency learned structural embeddings words domain authorship attribution across four datasets. Our experimental results demonstrate classification improvements structural embeddings concatenated pre-trained word embeddings. The remainder paper organized follows: elaborate proposed self-supervised framework Section . The details datasets experimental configuration provided experimental results reported Section ; We review related work Section . Finally, conclude paper Section . Event-event relation extraction challenging task beneficial understanding event complex composed multi-granular events temporal orders. Despite existence previous attempts addressing TempRel subevent relation extraction, first work We propose joint constrained learning framework extracting event complexes documents. combines two tasks addresses constrained learning shared objectives. The proposed framework bridges TempRel subevent relation extraction tasks comprehensive set logical constraints, enforced learning converting differentiable objective functions. On two benchmark datasets, proposed method outperforms SOTA statistical learning methods data-driven methods task, without using data jointly annotated two classes relations. It also presents promising event complex extraction results RED external training. Thus, work shows global consistency event complex significantly helps understanding temporal order event membership. For future work, plan extend framework towards end-to-end system event extraction. We also seek extend conjunctive constraints along event argument relations. , demonstating effectiveness joint constrained learning framework machine-learning NLU view .","   Syntactic structure of sentences in a document substantially informs about its authorial writing style. Sentence representation learning has been widely explored in recent years and it has been shown that it improves the generalization of different downstream tasks across many domains. Even though utilizing probing methods in several studies suggests that these learned contextual representations implicitly encode some amount of syntax, explicit syntactic information further improves the performance of deep neural models in the domain of authorship attribution. These observations have motivated us to investigate the explicit representation learning of syntactic structure of sentences.  In this paper, we propose a self-supervised framework for learning structural representations of sentences. The self-supervised network contains two components; a lexical sub-network and a syntactic sub-network which take the sequence of words and their corresponding structural labels as the input, respectively. Due to the $n$-to-$1$ mapping of words to their structural labels, each word will be embedded into a vector representation which mainly carries structural information. We evaluate the learned structural representations of sentences using different probing tasks, and subsequently utilize them in the authorship attribution task. Our experimental results indicate that the structural embeddings significantly improve the classification tasks when concatenated with the existing pre-trained word embeddings."
"Since end twentieth century spread mobile communication technologies Arab world, youth, particular, developed new chat alphabet communicate efficiently informal Arabic. Because media applications initially enable chatting Arabic, Arab speakers resorted commonly known ""Arabizi"". In, Arabizi defined newly-emerged Arabic variant written using Arabic numeral system Roman script characters. With widespread use social media worldwide recent years, Arabizi emerged established Arabic writing system mobile communication social media Arab world. Compared increasing studies sentiment analysis Indo-European languages, similar research Arabic dialects still limited.\ This mainly attributed lack needed good quality Modern Standard Arabic publicly-available sentiment analysis resources general, specifically dialectical Arabic publicly-available resources.\ Building resources involves several difficulties terms data collection annotation, especially underrepresented Arabic dialects Tunisian dialect. Nevertheless, existing Tunisian annotated datasets focused code-switching datasets written using Arabic Romanized Alphabet. The studies datasets applied off-the-shelf models built MSA dataset Tunisian Arabic. An intuitive solution translate Tunisian Romanized Alphabet Arabic Script. This approach suffers need parallel Tunisian-Arabic text corpus, low average precision performances achieved irregularity words written. Using model trained Modern Standard Arabic sentiment analysis data applying model dialectal sentiment analysis data, produce good performances shown in. This suggests MSA models cannot effective applied dialectical Arabic. There is, thus, growing need creation computational resources, MSA also dialectical Arabic. The situation holds one tries use computational resources used specific dialect Arabic another one. To best knowledge, first study sentiment analysis TUNIZI Romanized Alphabet. \ This could deduced next sections present TUNIZI state-of-the-art Tunisian sentiment analysis followed proposed approach, results discussion conclusion future work. In paper, proposed self-supervised framework learning structural representation sentences domain authorship attribution. The result training self-supervised framework pre-trained structural embeddings capture information regarding syntactic structure sentences. Subsequently, structural embeddings concatenated existing pre-trained word embeddings create style-aware embedding carries semantic syntactic information well-suited domain authorship attribution. Moreover, structural embeddings eliminate necessity syntactic parsing training syntactic neural networks; therefore, training neural model using pre-trained structural embeddings computationally efficient. According experimental results four benchmark datasets authorship attribution, using structural embedding improves performances proposed neural model. The next two lines define bibliography style used, bibliography file."," Tunisians on social media tend to express themselves in their local dialect using Latin script . This raises an additional challenge to the process of exploring and recognizing online opinions. To date, very little work has addressed TUNIZI sentiment analysis due to scarce resources for training an automated system. In this paper, we focus on the Tunisian dialect sentiment analysis used on social media. Most of the previous work used machine learning techniques combined with handcrafted features. More recently, Deep Neural Networks were widely used for this task, especially for the English language. In this paper, we explore the importance of various unsupervised word representations  and we investigate the use of Convolutional Neural Networks and Bidirectional Long Short-Term Memory. Without using any kind of handcrafted features, our experimental results on two publicly available datasets showed  comparable performances to other languages.    \keywords{Tunisian Dialect  \and TUNIZI \and Sentiment Analysis \and Deep Learning \and Neural networks \and Natural language analysis.}"
"In recent years, neural networks shown impressive performance gains long-standing AI problems, natural language understanding, speech recognition, computer vision. Based successes, researchers considered application neural nets data management problems, including learning indices, query optimization entity matching. In applying neural nets data management, research far assumed data modeled database schema. The success neural networks processing unstructured data natural language images raises question whether use extended point relax fundamental assumption database management, data process represented fields pre-defined schema. What if, instead, data queries represented short natural language sentences, queries answered sentences? This paper presents first step answering question. We describe \systemname, database system updates queries given natural language. The query processor \ndb\ builds primitives offered state art Natural Language Processing~ techniques. Figure shows example facts queries \ndb\ answer. %\ms{In Figure 1, queries 4&5 really joins, need language understanding/paraphrasing} Realizing vision \systemname\ offer several benefits database systems struggled support decades. The first, important benefit \ndb, definition, pre-defined schema. Therefore, scope database need defined advance data becomes relevant application used stored queried. The second benefit updates queries posed variety natural language forms, convenient user. In contrast, traditional database query needs based database schema. A third benefit comes fact \ndb\ based pre-trained language model already contains lot knowledge. For example, fact London UK already encoded language model. Hence, query asking lives UK retrieve people known live London without explicitly specify additional join. Furthermore, using paradigm, endow \ndb\ domain knowledge extending pre-training corpus domain. By nature, \ndb\ meant provide correctness guarantees traditional database system, i.e., answers returned query satisfy precise binary semantics query language. Hence, \ndb considered alternative traditional databases applications guarantees required. Given benefits, \neuraldatabases\ well suited emerging applications schema data cannot determined advance data stated wide range linguistic patterns. A family applications arise area storing knowledge personal assistants currently available home use future accompany Augmented Reality glasses. In applications, users store data habits experiences, friends preferences, designing schema application impractical. Another class applications modeling querying political claims . Here too, claims huge variety topics expressed many ways. Our first contribution show state art transformer models adapted answer simple natural language queries. Specifically, models process facts relevant query independent specific linguistic form, combine multiple facts yield correct answers, effectively performing join. However, identify two major limitations models: perform well aggregation queries , since input size transformer bounded complexity transformer quadratic size input, work relatively small collection facts. Our second contribution propose architecture neural databases uses power transformers core, puts place several components order address scalability aggregation issues. Our architecture runs multiple instances Neural SPJ operator parallel. The results operator either answer query input aggregation operator, done traditional fashion. Underlying architecture novel algorithm generating small sets database sentences fed Neural SPJ operator. Finally, describe experimental study validates different components \systemname s, namely ability Neural SPJ answer queries create results subsequent aggregation operator even minimal supervision, ability produce support sets fed Neural SPJ operators. Putting components together, final result shows accurately answer queries thousands sentences high accuracy. To run experiments create experimental dataset training data \ndb s, make available future research. % capable generating intermediate results accurately predicting aggregation operation execute intermediate results. In work, tackled Tunisian Romanized alphabet sentiment analysis task. We experimented two different word-level representations two deep neural networks , without use pre-processing step. Results showed CNN trained M-BERT achieved best results compared word2vec, frWac Bi-LSTM. This model could improve performance baselines. Experiments promising results achieved TUNIZI TSAC-TUNIZI datasets helped us better understand nature Tunisian dialect specificities. This help Tunisian NLP community research activities limited sentiment analysis task, also complex NLP tasks. A natural future step would involve releasing TunaBERT, Tunisian version Bi-directional Encoders Transformers learned large heterogeneous Tunisia dataset. The Tunisian language model applied complex NLP tasks . To demonstrate value building dedicated version BERT Tunisian, also plan compare TunaBERT multilingual cased version BERT."," \jt{TODO Before final submission remove page numbers} In recent years, neural networks have shown impressive performance gains on long-standing AI problems, and in particular, answering queries from natural language text. These advances raise the question of whether they can be extended to a point where we can relax the fundamental assumption of database management, namely, that our data is represented as fields of a pre-defined schema.   This paper presents a first step in answering that question.  We describe \ndb, a database system with no pre-defined schema, in which updates and queries are given in natural language. We develop query processing techniques that build on the  primitives offered by the state of the art Natural Language Processing methods.   We begin by demonstrating that at the core, recent NLP transformers, powered by pre-trained language models, can answer select-project-join queries if they are given the exact set of relevant facts. However, they cannot scale to non-trivial databases and cannot perform aggregation queries. Based on these findings, we describe a \ndb\ architecture that runs multiple Neural SPJ operators in parallel, each with a set of database sentences that can produce one of the answers to the query. The result of these operators is fed to an aggregation operator if needed. We describe an algorithm that learns how to create the appropriate sets of facts to be fed into each of the Neural SPJ operators. Importantly, this algorithm can be trained by the Neural SPJ operator itself. We experimentally validate the accuracy of \systemname\ and its components, showing that we can answer queries over thousands of sentences with very high accuracy."
"% Enabling chatbots indulge engaging conversations requires massive datasets human-human conversations . Training dialog agents requires substantial time effort expended collection adequate number high quality conversation samples. \citet{hancock2019learning} alleviate problem introducing self-feeding chatbot directly learn user interactions. This chatbot requests users provide natural language feedback users dissatisfied response. \citet{hancock2019learning} treat feedback gold response wrong turn use additional training sample improve chatbot. Although natural language feedback cheap collect chatbot's end-users, often, feedback cannot used directly training sample since feedback usually answer itself, simply contains hints answer. \Cref{tab:response_samples} shows feedback text samples. Naive modification feedback using heuristics like regular expressions would lead generic responses ineffective improving dialog ability chatbots . Additionally, writing exhaustive set regular expression rules time consuming requires extensive analysis data. Annotating data convert feedback text natural response also expensive defeats purpose learning feedback text. \end{table} In work, propose generative adversarial setup converting noisy feedback instances natural, human-like responses provide better training signals dialog agents. \Cref{fig:interface} gives bird's-eye view problem. We frame problem variant text style transfer generator tasked making feedback resemble optimal response user's previous utterance discriminator classifier distinguishes whether given response feedback natural. Our main contributions following: % Medical code assignment clinical notes fundamental task healthcare information systems diagnosis decision support. This paper proposes novel framework gated convolutional neural networks note-code message passing mechanism automated medical code assignment. Our solution learn meaningful features lengthy clinical documents effectively control deep propagation information flow. Moreover, message passing mechanism enhance ICD code space's semantics model note-code interaction improve medical code prediction. Experiments show effectiveness proposed method.","  The ubiquitous nature of chatbots and their interaction with users generate an enormous amount of data. Can we improve chatbots using this data? A self-feeding chatbot improves itself by asking natural language feedback when a user is dissatisfied with its response and uses this feedback as an additional training sample. However, user feedback in most cases contains extraneous sequences hindering their usefulness as a training sample. In this work, we propose a generative adversarial model that converts noisy feedback into a plausible natural response in a conversation. The generator's goal is to convert the feedback into a response that answers the user's previous utterance and to fool the discriminator which distinguishes feedback from  natural responses. We show that augmenting original training data with these modified feedback responses improves the original chatbot performance from 69.94\% to 75.96\% in ranking correct responses on the \personachat dataset, a large improvement given that the original model is already trained on 131k samples.\footnote{Our code is released at \url{https://github.com/ekunnii/adversarial-feedback-chatbot/}}"
"Text Generation task producing written spoken narrative structured unstructured data. The overarching goal seamless human-machine communication presenting wealth data way comprehend. With respect modeling approaches, three main paradigms generating text based schema input output: Text-to-Text Data-to-Text None-to-Text. Table presents categorization different tasks based paradigm. These several tasks deserve undivided attention accordingly heavily dissected, studied surveyed recent past. For instance, independent exclusive surveys periodically conducted summarization , knowledge text generation {DBLP:conf/inlg/GardentSNP17, DBLP:conf/naacl/Koncel-Kedziorski19}, machine translation , dialog response generation , storytelling, narrative generation , image captioning etc., dig deeper task specific approaches foundational well bleeding edge research. While extremely necessary, often focus techniques beneficial tightly coupled tasks overlooked. The goal survey focus key components task agnostic improve ensemble tasks neural text generation. %The rest survey organized follows: Section describes modeling approaches text generation including learning paradigms, pre-training decoding strategies. This followed Section describing key challenges solutions text generation fluency, length, content selection, speed etc.,. Section describes evaluation finally Section presents conclusions prospective future directions. } \end{table} %https://www.sciencedirect.com/science/article/pii/S1319157820303360 There several studies conducted surveying text generation. \citet{DBLP:journals/cai/PereraN17} present detailed overview information theory based approaches. \citet{iqbal2020survey} primarily focus core modeling approaches, especially VAEs GANs . \citet{DBLP:journals/jair/GattK18} elaborated tasks captioning, style trasfer etc., primary focus data-to-text tasks. Controllability aspect explored \citet{prabhumoye2020exploring}. The workclosest \citet{DBLP:journals/corr/abs-1803-07133} perform empirical study core modeling approaches only. In contrast these, paper focuses task agnostic components factors capable pushing ensemble tasks forward. Figure presents various components factors important study neural text generation elaborated paper. %Text generation overarching set tasks underlying factors cut across tasks critical pushing field forward paper dedicated one stop destination learn several fundamental factors. In work, show chatbots improved using natural language feedback, converting feedback natural responses fit conversation outperform naive usage feedback. We presented \feedresp, generative adversarial model, converts feedback natural responses without requiring manually annotated parallel data. Our results show \feedresp results 6~point improvement \polyencoder chatbot, already powerful dialog ranking agent. This strong result HITS@1/20 tough metric improve upon . Our work joins class models use natural language feedback improve different tasks, e.g., image captioning , classification . While methods use feedback reward shaping feature extraction, use feedback produce correct response using adversarial learning. We pose problem style transfer problem inspired style transfer literature . While focus studying stylistic attributes sentences, e.g, sentiment, explore problem context improving chatbots.","   Neural text generation metamorphosed into several critical natural language applications ranging from text completion to free form narrative generation. Generating natural language has fundamentally been a human attribute and the advent of ubiquitous NLP applications and virtual agents marks the need to impart this skill to machines. There has been a colossal research effort in various frontiers of neural text generation including machine translation, summarization, image captioning, storytelling etc., We believe that this is an excellent juncture to retrospect on the directions of the field. Specifically, this paper surveys the fundamental factors and components relaying task agnostic impacts across various generation tasks such as storytelling, summarization, translation etc., In specific, we present an abstraction of the imperative techniques with respect to learning paradigms, pretraining, modeling approaches, decoding and the key challenges. Thereby, we hope to deliver a one-stop destination for researchers in the field to facilitate a perspective on where to situate their work and how it impacts other closely related tasks. %scope it : current neural techniques %for single and multi-sentence"
"The following instructions directed authors papers submitted EACL 2021 accepted publication proceedings. All authors required adhere specifications. Authors required provide Portable Document Format version papers. The proceedings designed printing A4 paper. The past decade witnessed text generation dribbling niche scenarios several mainstream NLP applications. This urges need snapshot retrospect progress varied text generation tasks unison. This paper written goal presenting one-stop destination task agnostic components factors text generation researchers foraging situate work guage impact vast field. Moving forward, envision crucial directions focus impactful innovation text generation. These include generation real time non-autoregressive decoding consistency situated contexts real virtual environments games consistency personality opinions especially virtual agents conditioning multiple modalities together text data investigation still ongoing finding better metrics evaluate NLG better correlated human judgements creative text generation. We believe right time extend advancements particular task tightly coupled tasks revamp improvements text generation holistic task."," This document contains the instructions for preparing a manuscript for the proceedings of EACL 2021. The document itself conforms to its own specifications, and is therefore an example of what your manuscript should look like. These instructions should be used for both papers submitted for review and for final versions of accepted papers. Authors are asked to conform to all the directions reported in this document."
"Cross-lingual abstractive summarization task generate summary given document different target language. This task provides overview article foreign language thus helps readers understand text written unfamiliar language quickly. Early work cross-lingual abstractive summarization adopted pipeline approach: either translation given document target language followed summarization translated document summarization given document followed translation summary target language. On hand, recent studies applied neural encoder-decoder model, widely used natural language generation tasks including machine translation monolingual abstractive summarization, generate summary target language given document directly. %Such direct generation approaches prevent error propagation problems pipeline methods. Such direct generation approaches prevent error propagation pipeline methods. Training neural encoder-decoder models requires numerous sentence pairs. In fact, provided 3.8M sentence-summary pairs train neural encoder-decoder model English abstractive summarization, following studies used training data. However, constructing large-scale cross-lingual abstractive summarization dataset much difficult collecting monolingual summarization datasets require sentence-summary pairs different languages. To address issue, recent studies applied machine translation model monolingual sentence-summary pairs. They used constructed pseudo dataset train neural encoder-decoder models. Meanwhile, possibility whether existing genuine parallel corpora translation pairs monolingual abstractive summarization datasets utilized needs explored. In machine translation, indicated using translation pairs multiple languages improved performance neural machine translation model. Similarly, consider existing genuine parallel corpora positive influence cross-lingual abstractive summarization task since task combination machine translation summarization. In study, propose multi-task learning framework, Transum, includes machine translation, monolingual abstractive summarization, cross-lingual abstractive summarization, neural encoder-decoder models. The proposed method controls target task special token inspired Google's multilingual neural machine translation system. For example, attach special token beginning source-side input sentence translation. The proposed Transum quite simple require additional architecture contrast effective cross-lingual abstractive summarization. Experimental results show Transum improves performance cross-lingual abstractive summarization outperforms previous methods Chinese-English Arabic-English summarization. In addition, Transum significantly improves machine translation performance compared obtained using genuine parallel corpus machine translation. Furthermore, construct new test set simulate realistic situations: cross-lingual summarization several length constraints. In summarization process, important generate summary desired length. However, existing test sets cross-lingual abstractive summarization cannot evaluate whether model controls output lengths test sets contain summaries multiple lengths. Thus, translate existing monolingual abstractive summarization contains summaries multiple lengths construct new test set. The contributions study follows: Our results suggest GPT-2 generally outputs better narratives recent non-GPT-based neural model. Additionally, find larger models better. While GPT-2 Large may infeasible long sequence generation, possible use GPT-2 Medium narrative lengths generated here. Once GPT-3 released public use, likely model outperform GPT-2 based trends. We encourage future work investigate similar hyperparameters see whether trends observed stable across model sizes. We recommend keeping hyperparameter within range . This aligns findings \citet{ippolito2020}, suggest values well needed generate text closely approximates human text. Diverse decoding increased narrative quality metrics small . This could used qualitatively induce intense vivid stories higher values, though finding seen preliminary tested domains. Using higher values also seemed induce vivid stories, less consistent fluency coherence; thus, diverse decoding objective could promising way increase narrative interestingness without significantly decreasing performance particular metric. While relatively low dist- may correlate consistently poor quality stories relatively high dist- may correlate variable-quality stories, find metric correlate well metrics general . Sent-BERT correlate metrics narrative quality. Thus, recommend optimizing either automatic quantities. Surprisingly, find strong diversity-quality trade-off narrative generation, perhaps due creative long-form nature task. Indeed, diversity quality correlate well general: diverse decoding higher values often coincide better performance human metrics domain point. This could due creative long-form nature narrative generation compared tasks chatbot response generation. We thus encourage future work investigate methods inducing diverse output, certain methods increase human perceptions narrative quality. Our findings aim inform future efforts narrative generation domain establishing future baselines given recommended hyperparameters, facilitating investigation decoding objectives better narrative generation. Additionally, hope investigation highlights issues addressed future work evaluating narratives automatically, since metrics aside perplexity seem correlate well human judgments quality. \clearpage"," We present a multi-task learning framework for cross-lingual abstractive summarization to augment training data. Recent studies constructed pseudo cross-lingual abstractive summarization data to train their neural encoder-decoders. Meanwhile, we introduce existing genuine data such as translation pairs and monolingual abstractive summarization data into training. Our proposed method, Transum, attaches a special token to the beginning of the input sentence to indicate the target task. The special token enables us to incorporate the genuine data into the training data easily. The experimental results show that Transum achieves better performance than the model trained with only pseudo cross-lingual summarization data. In addition, we achieve the top ROUGE score on Chinese-English and Arabic-English abstractive summarization. Moreover, Transum also has a positive effect on machine translation. Experimental results indicate that Transum improves the performance from the strong baseline, Transformer, in Chinese-English, Arabic-English, and English-Japanese translation datasets."
"Table-to-text generation important task text generation structured data. It aims automatically producing descriptive natural language text covers salient information table help people get salient information tables. Practical applications found domains weather forecasts, biography generation, NBA news generation, etc. Over pass several years, neural text generation methods made significant progress task. \citeauthor{lebret-etal-2016-neural,wiseman-etal-2017-challenges,bao2018table} model machine translation task view input table record sequence. To generate text contains salient well-organized facts, \citeauthor{sha2018order,puduppully-etal-2019-data,moryossef-etal-2019-step,trisedya2020sentence,ijcai2020-522} explicitly model content selection planning. %Some works also introduce extra knowledge pre-executed symbolic operations table improve result. To learning better representation tables, \citeauthor{liu2018table,bao2018table,nema-etal-2018-generating,jain-etal-2018-mixed,gong-etal-2019-table} explicitly model structure table multiple levels different dimensions. In addition, \citeauthor{liu2019hierarchical} propose three auxiliary supervision tasks capture accurate semantic representation table. However, issues overlooked. First, many tables ) contain large number numerical records. For instance, records almost column types numeric ROTOWIR , benchmark NBA basketball games. Current methods treat records words natural language text ignore characteristics number play important role table representation, size attribute. In addition, noises human-written summaries dataset. These noises include redundant information records exist input tables ). These noises may cause incorrect alignments input tables target text wrong supervision signals. And affect performance models based content selection planning auxiliary supervision. %In addition, human writing summary describe given table, may consider salient records. For example, describing table Figure , may pay attention K. Leonard, top scorer. To solve problems, explore use information contained tables introduce two self-supervised tasks learn better representation tables. We argue better representation tables help model capture organize important facts, even without explicitly modeling content selection planning. Specially, improve ~\citeauthor{gong-etal-2019-table}'s method employ hierarchical table encoder model table structure record level row level. The record-level encoder utilizes two cascaded self-attention models encode table column row dimension, respectively. And then, introduce row-level fusion gate obtain row-level representation row. To learn number-aware record representation, introduce Number Ordering task. This task utilizes pointer network generate descending record sequence column table, according content. Figure shows number ordering example column PTS. To best knowledge, first work neural table-to-text generation via focusing learning representation number table. Another self-supervised task, Significance Ordering , proposed learn significance-aware representation record. The significance denotes relative relation records row. This inspired intuition humans describe performance player, tend focus salient records. For example, Figure , K. Thompson's scores likely described other's records. The SO task executes descending sort operation row according significance scores records. We use position index record measure importance smaller significance score, important record is. The position index record obtained results Number Ordering. For example, Figure , K. Thompson scores points largest PTS, significance score record 1. The proposed two tasks trained together table2text generation model share encoder parameters. Obviously, two proposed tasks self-supervised training labels easily obtained input tables. Therefore, errors caused noises training set avoided. %For record row, includes another size information:significance. It denotes relative relation records row. To learn significance-aware representation table, propose Significance Ordering task executes ascending sort operation row according significance records. We use position index record measure importance smaller significance score, important record is. The position index record obtained results Number Ordering. For example, Figure , K. Leonard score 45 points largest PTS, significance score record 1). Obviously, two proposed tasks self-supervised training labels easily obtained input tables. Therefore, errors caused noise training set avoided. We conducted experiments ROTOWIRE verify effectiveness proposed approach. The experimental results demonstrate that, even without explicitly modeling content selection introducing extra knowledge, method help generate text contains salient well-organized facts. And achieve state-of-the-art performance automatic metrics. %Content Selection , Content Ordering BLEU. This paper presents multi-task learning framework cross-lingual abstractive summarization augment training data. The proposed method, Transum, attaches special token beginning input sentence indicate target task. The special token enables us use genuine translation pairs monolingual abstractive summarization dataset addition pseudo cross-lingual abstractive summarization data training. The experimental results show Transum achieved better performance pipeline approach model trained pseudo data only. We achieved top ROUGE scores Chinese-English Arabic-English abstractive summarization. Moreover, Transum also improved performance machine translation outperformed previous top score JIJI English-Japanese translation."," Table-to-text generation aims at automatically generating natural text to help people to conveniently obtain the important information in tables. Although neural models for table-to-text have achieved remarkable progress, some problems still overlooked. The first is that the values recorded in many tables are mostly numbers in practice. The existing approaches do not do special treatment for these, and still regard these as words in natural language text.  Secondly, the target texts in training dataset may contain redundant information or facts do not exist in the input tables. These may give wrong supervision signals to some methods based on content selection and planning and auxiliary supervision. To solve these problems, we propose two self-supervised tasks, Number Ordering and Significance Ordering,  to help to learn better table representation. The former works on the column dimension to help to incorporate the size property of numbers into table representation. The latter acts on row dimension and help to learn a significance-aware table representation. We test our methods on the widely used dataset ROTOWIRE which consists of NBA game statistic and related news. The experimental results demonstrate that the model trained together with these two self-supervised tasks can generate text that contains more salient and well-organized facts, even without modeling context selection and planning. And we achieve the state-of-the-art performance on automatic metrics. % Content Selection , Content Ordering  and BLEU."
"In healthcare, real-world data refers patient data routinely collected clinic visits, hospitalization, well patient-reported results. In recent years, RWD's volume become enormous, invaluable insights real-world evidence generated datasets using latest data processing analytical techniques. However, RWD's quality remains one main challenges prevent novel machine learning methods readily adopted healthcare. Therefore, creating data quality tools great importance health care health data sciences. Erroneous data healthcare systems could jeopardize patient's clinical outcomes affect care provider's ability optimize performance. Common data quality issues include missing critical information medical history, wrong coding condition, inconsistency documentation across different care sites. Manual review domain experts gold standard achieving highest data quality unattainable regular care practices. Recent developments field Natural Language Processing attracted great interest healthcare community since algorithms identifying variables interest classification algorithm diseases recently developed . In paper, presented novel model extraction queries corpus dialogue data entry clinicians expert reviewers multi-site dialysis environment. %The work's ultimate goal identify data elements caused uncertainty errors documentation process. The main contributions work are: Finally, addition evaluating model's performance medical context, also experimented section general-domain dataset show model's generalizability. The rest paper organized follows. Related work presented section . The different question detection methods examined, described section . Section details characteristics proposed multi-channel CNN model. Finally, results experiments reported section conclusion plan future work given section . In work, first point shortcomings MLE based training keyphrase generation. We specifically address lack output diversity issue via use unlikelihood training objective. We adopt target level unlikelihood loss propose novel copy token unlikelihood loss, combination provides large diversity gains. In addition, -step ahead MLE UL objective incorporated training. Through extensive experiments datasets three different domains, demonstrate effectiveness model diverse keyphrase generation. For future work, plan explore directions would enable us simultaneously optimize quality diversity metrics."," In most clinical practice settings, there is no rigorous reviewing of the clinical documentation, resulting in inaccurate information captured in the patient medical records. The gold standard in clinical data capturing is achieved via ``expert-review"", where clinicians can have a dialogue with a domain expert  and ask them questions about data entry rules. Automatically identifying ``real questions"" in these dialogues could uncover ambiguities or common problems in data capturing in a given clinical setting.  In this study, we proposed a novel multi-channel deep convolutional neural network architecture, namely Quest-CNN, for the purpose of separating real questions that expect  an answer  about an issue from sentences that are not questions, as well as from questions referring to an issue mentioned in a nearby sentence , which we will refer as ``c-questions"". We conducted a comprehensive performance comparison analysis of the proposed multi-channel deep convolutional neural network against other deep  neural networks. Furthermore, we evaluated the performance of traditional rule-based and learning-based methods for detecting question sentences. The proposed Quest-CNN achieved the best F1 score both on a dataset of data entry-review dialogue in a dialysis care setting, and on a general domain dataset."
"Semantic parsing task mapping natural language query formal language, extensively used goal-oriented dialogue systems. For given query, model identify requested action associated values specifying parameters action . For example, query Call Mary action call value slot contact Mary. The number different intents slots publicly available datasets close hundred may orders magnitude larger real-world systems. Such big number classes usually causes long tail class frequency distribution . These tail classes significantly improved small quantities additional labeled data. However, training neural semantic parsing model scratch take hours even relatively small public dataset . The real-world datasets contain millions examples change time scale weeks. % Need describe problem motivation production settings more. In work, propose fine-tune model already trained old dataset instead training new model significantly speed incorporation new portion data. We call setting Incremental training, new portions data added incrementally. We focus semantic parsing % seq2seq networks case studies following reasons. Semantic parsing complex NLP task compared classification NER hope lessons learned would widely applicable. Task-oriented semantic parsing tend large output vocabulary frequently updated, thus, benefit Incremental setting. % We choose seq2seq networks work due two reasons: first, seq2seq networks % general easily adapted simpler tasks like NER; % second, seq2seq models perform really well popular natural language understanding datasets like TOP SNIPS. % Exploring space possible solutions, compare effectiveness approaches come set guidelines useful incremental training tasks well. % To emulate ""data-patch"" scenario, split datasets focusing classes. We show naive fine-tuning leads catastrophic forgetting come approaches remedy this. We observe possible fine-tune models new classes minutes compared hours retraining scratch. We also compare effect pre-trained representations like BERT fine-tuning. Using observations come fine-tuning guidelines scenarios label space change. We verify approaches work 2 popular semantic parsing datasets: TOP SNIPS different data splits. The main contributions work are: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % Related work %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% In paper, provided analysis performance existing methods question extraction real-world misclassification examples showed weak point method. Furthermore, proposed novel approach automatic identification real questions c-questions. We also shown empirically proposed architecture unifying syntactic, semantic statistical features achieved state-of-the-art F1 score particular task. Finally, presented relevance exploiting domain knowledge overall performance model. We process obtaining access datasets different application contexts order examine generalizability model. As future work, plan extend work calculating similarity questions order create groups questions represent impactful ``problems'' given application environment. Finally, plan compare model recent language representation models like BERT model task question identification task creating mentioned ``problem'' groups."," A semantic parsing model is crucial to natural language processing applications such as goal-oriented dialogue systems. Such models can have hundreds of classes with a highly non-uniform distribution. In this work, we show how to efficiently  improve model performance given a new portion of labeled data for a specific low-resource class or a set of classes. We demonstrate that a simple approach with a specific fine-tuning procedure for the old model can reduce the computational costs by ~90\% compared to the training of a new model. The resulting performance is on-par with a model trained from scratch on a full dataset. We showcase the efficacy of our approach on two popular semantic parsing datasets, Facebook TOP, and SNIPS."
"Recent progress abstractive summarization fueled advent large-scale Transformers pre-trained autoregressive language modeling objectives . Despite strong performance automatic metrics like ROUGE , abstractive models straightforward interpretable extractive counterparts. Free-form generation models also leads serious downstream errors, factual inconsistencies input document . Although interpretability NLU models extensively studied , summarization models specifically received similar attention, analysis efforts often focused datasets evaluation . %Generic explanation methods language models neural machine translation models entirely applicable, summarization models typically different interactions input document. In work, focus interpreting understanding abstractive summarization models lens decoder uncertainty, entropy decisions generation. While uncertainty generation studied perspective data , sampling , training , underutilized technique analysis inspection generation systems. We study two prominent summarization models, PEGASUS BART , fine-tuned two English summarization datasets, CNN/Daily Mail XSum , understand model behavior setting. %We analyze model using blackbox whitebox perspectives. First, comparing -grams input document generated summaries, establish two coarse types decoded tokens, copy generate . We find entropy generation decision correlates whether model copying generating, well sentence token is. This paints picture certain contexts restrictive standpoint generation, particularly early sentences model ``decided'' copy yet, illustrates interaction content selection lexical choice. %Furthermore, illustrates interaction content selection lexical choice: new bigrams higher entropy, beginnings sentences also high entropy, indicating model uncertainty sentence discuss, even going copy. Second, extend analysis looking uncertainty relates syntax generated sentence: whether uncertainty connects syntactic notions surprisal entropy varies across certain syntactic productions. % Finally, derive way quantify decoder attention aggregating self-attention heads, investigating correspondence prediction entropy fraction decoded tokens aggregated attention.\todo{change sent refer entropy more} Finally, derive way quantify decoder attention aggregating distinct self-attention heads, revealing correlation attention entropy prediction entropy, investigating correspondence prediction entropy fraction past future decoded tokens. % highly attentive positions decoded not-yet-decoded tokens respect specific Transformer layers decoder. Taking analysis together, find abstractiveness reference summaries fundamentally changes model behavior: extractive nature CNN/DM makes decisions low entropy copy-oriented model maintains higher uncertainty XSum, yielding abstractive summaries. More broadly, show uncertainty simple effective tool characterize decoder behavior text generation. % By analyzing decoder self-attention layers, find attention focuses tokens, prediction entropy fairly low focused tokens likely predicted. In work, consider practical side CL previously overlooked NLP researchers - ability quickly update existing model new data. Nowadays, performance models scaling superlinearly size, training time becomes challenging issue every year. We anticipate near future billion-parameter-sized models, incremental continual learning settings lead significant advantage terms resource efficiency also become necessity. Our experimental results show simple incremental setup reduce computational costs 90\ . It beneficial terms increasing speed development cycle terms environmental impact becoming significant field . We also want notice negative results discovered. Training top layer surprisingly bad way include data. Possibly, feature-engineering happen lower layers model. Also, even though model quickly fits fine-tuning data, increasing regularization seem improve final performance. And finally, na\""{\i}ve combination successful methods dynamic sampling, freezing, move norm seem help either. This paper evaluates simple efficient methods Incremental training, The Continual Learning community made incredible progress using sophisticated methods EWC LAMOL . Many approaches applicable real-world scenario tested practical applications. In future work, want consider models evaluate terms performance computational costs. Another important direction study model changes multiple iterative updates. References TODO: REMOVE THIS BEFORE THE FINAL VERSION!!! \section{Hyperparameters training setup} Table contains hyperparameters used pretraining. We used Noam schedule learning rate. Note involves learning rate scaling. For fine-tuning, parameters used unless otherwise stated experiment description. The exception rule batch size set 128 fine-tuning experiments. Model, optimizer, learning rate scheduler states restored checkpoint best EM."," % An advantage of seq2seq abstractive summarization models is that they generate text in a free-form manner, but this inherent flexibility makes it difficult to interpret and understand model behavior. In this work, we adopt a data-driven methodology to unpack decoder behavior in both a blackbox and whitebox way. We fine-tune and analyze a GPT-2 \cite{radford-2019-gpt2} model on two benchmark datasets featuring different levels of abstraction. Our experiments yield three key results. First, by analyzing the entropy of model predictions and its corresponding test-time behavior, we find a strong correlation between low entropy and where the model copies document spans rather than generating novel text. Second, this entropy analysis can allow us to understand what sentence positions and even what syntactic configurations are associated with copying existing content. Finally, by analyzing decoder self-attention patterns, we can trace this copying behavior to a particular pattern of attending to immediate decoder context and finding the next token to generate in the source document. An advantage of seq2seq abstractive summarization models is that they generate text in a free-form manner, but this flexibility makes it difficult to interpret model behavior.  In this work, we analyze summarization decoders in both blackbox and whitebox ways by studying on the entropy, or uncertainty, of the model's token-level predictions. For two strong pre-trained models, PEGASUS \cite{pegasus} and BART \cite{lewis-2019-bart} on two summarization datasets, we find a strong correlation between low prediction entropy and where the model copies tokens rather than generating novel text. The decoder's uncertainty also connects to factors like sentence position and syntactic distance between adjacent pairs of tokens, giving a sense of what factors make a context particularly selective for the model's next output token. Finally, we study the relationship of decoder uncertainty and attention behavior to understand how attention gives rise to these observed effects in the model. We show that uncertainty is a useful perspective for analyzing summarization and text generation models more broadly.\footnote{Code is available at \url{https://github.com/jiacheng-xu/text-sum-uncertainty}} % can trace this copying behavior to a particular pattern of attending to immediate decoder context and finding the next token to generate in the source document."
"Neural attention mechanisms widely applied computer vision shown enable neural networks focus aspects input important given task. While neural networks able learn meaningful attention mechanisms using supervision received target task, addition human gaze information shown beneficial many cases. An especially interesting way leveraging gaze information demonstrated works incorporating human gaze neural attention mechanisms, example image video captioning visual question answering. While attention least important reading text viewing images, integration human gaze neural attention mechanisms natural language processing tasks remains under-explored. A major obstacle studying integration data scarcity: Existing corpora human gaze reading consist samples provide effective supervision modern data-intensive architectures human gaze data available small number NLP tasks. For paraphrase generation sentence compression, play important role tasks reading comprehension systems, human gaze data available. We address data scarcity two novel ways: First, overcome low number human gaze samples reading, propose novel hybrid text saliency model combine cognitive model reading behavior human gaze supervision single machine learning framework. More specifically, use E-Z Reader model attention allocation reading obtain large number synthetic training examples. We use examples pre-train BiLSTM network Transformer whose weights subsequently refine training small amount human gaze data. We demonstrate model yields predictions well-correlated human gaze out-of-domain data. Second, propose novel joint modeling approach attention comprehension allows human gaze predictions flexibly adapted different NLP tasks integrating TSM predictions attention layer. By jointly training TSM task-specific network, saliency predictions adapted upstream task without need explicit supervision using real gaze data. Using approach, outperform state art paraphrase generation Quora Question Pairs corpus 10\% BLEU-4 achieve state art performance Google Sentence Compression corpus. As such, work demonstrates significant potential combining cognitive data-driven models establishes general principle flexible gaze integration NLP potential also benefit tasks beyond paraphrase generation sentence compression. This work analyzes pre-trained summarization models via uncertainty, entropy decoding decisions. We pursue several lines inquiry: uncertainty help us understand copying document spans vs.~generating novel text, behavior models different syntactic environments, coarse properties model's attention distribution. All give insight conditions heavily restrict model's generation: generating observed bigram , low syntactic distance, attention easily identify decoder context source document. We believe approach power future analyses pre-trained text generation systems."," A lack of corpora has so far limited advances in integrating human gaze data as a supervisory signal in neural attention mechanisms for natural language processing . We propose a novel hybrid text saliency model  that, for the first time, combines a cognitive model of reading with explicit human gaze supervision in a single machine learning framework. On four different corpora we demonstrate that our hybrid TSM duration predictions are highly correlated with human gaze ground truth. We further propose a novel joint modeling approach to integrate TSM predictions into the attention layer of a network designed for a specific upstream NLP task without the need for any task-specific human gaze data. We demonstrate that our joint model outperforms the state of the art in paraphrase generation on the Quora Question Pairs corpus by more than 10\% in BLEU-4 and achieves state of the art performance for sentence compression on the challenging Google Sentence Compression corpus. As such, our work introduces a practical approach for bridging between data-driven and cognitive models and demonstrates a new way to integrate human gaze-guided neural attention into NLP tasks."
"%\hh{check fuzziness: pre-trained pretrained decide one use .} Modern techniques text summarization generally categorized either extractive methods, identify suitable %\pfliu{How ``which identify suitable semantic units ''} words sentences input document concatenate form summary, abstractive methods, generate summaries freely able produce novel words sentences. Compared extractive algorithms, abstractive algorithms flexible, making likely produce fluent coherent summaries. %\pfliu{better adding references here} %and generation process human-like \gn{Re ``more human-like''. First, I'm sure actually true: humans copy-paste text well. Second, seem really important here. Maybe could expand ``more flexible'' part mention practical advantages this.}. However, unconstrained nature abstractive summarization also result problems. First, result unfaithful summaries, containing factual errors well hallucinated content. Second, difficult control content summaries; hard pick advance aspects original content abstractive system may touch upon. %\pfliu{I'm thinking suitable place following paragraph . Will better exchange ``There ...'' paragraph make corresponding modification.} To address issues, propose methods guided neural abstractive summarization: methods provide various types guidance signals 1) constrain summary output content deviate less source document; 2) allow controllability provision user-specified inputs. % Table generated Excel2LaTeX sheet 'Sheet1' \iffalse % % \end{table*}% \fi \iffalse % '' ``{cover.}'' represent copy coverage mechanism respectively. Guidance represents different guided information Guiding Method denotes introduce guided information. ``ourGuidance'' contains sentences, relations keywords retrieved summaries. ``Marker Embedding'' suggests guided information introduced embedding feature vector.}% \gn{add completeness. Make sure chronological order. I think BART needs included, might also include methods provide guidance on, example, style output .}} %\zj{Is particular reason make ``copy'' ``cover.'' italic?}.} % \end{table*}% \fi % %'' ``{cover.}'' represent copy coverage mechanism respectively. Guidance represents different guided information Guiding Method denotes introduce guided information. ``ourGuidance'' contains sentences, relations keywords retrieved summaries. ``Marker Embedding'' suggests guided information introduced embedding feature vector.}% \gn{add completeness. Make sure chronological order. I think BART needs included, might also include methods provide guidance on, example, style output .}} %\zj{Is particular reason make ``copy'' ``cover.'' italic?}.} % \end{table*}% %\gn{The term ``hybrid summarization models'' sudden, follow clearly last sentence previous paragraph. I think point paragraph ``we first propose guided neural summarization models, previous methods limited particular type guidance''. If so, say ``we first'' part beginning paragraph, ``limited'' part final part paragraph.} There previous methods guiding neural abstractive summarization models. For example,~\citet{kikuchi-etal-2016-controlling} specify length abstractive summaries,~\citet{li2018guiding} provide models keywords prevent model missing key information, ~\citet{cao2018retrieve} propose models retrieve reference relevant summaries training set. %, and~\citet{gehrmann2018bottom} propose train model identify salient words encourage final model faithfully copy source. While methods demonstrated improvements summarization quality controllability, focuses one particular type guidance -- remains unclear better whether complementary other. %In addition, previous work whether compatible pre-trained language models BERT. %Previously, order address issues abstractive summarization models, researchers proposed hybrid summarization models combine merits extractive abstractive methods. %\gn{In following three sentences, explicitly stated clear methods address issues abstractive summarization models.} %For example,~\citet{gu2016incorporating} propose methods copy words source document.~\citet{gehrmann2018bottom} utilize bottom-up attention constrain decoder attend salient parts inputs. %Similarly, %While approaches achieve good performance terms ROUGE, cannot guarantee models learn identify salient segments correctly control summaries due lack explicit supervision signals %\gn{can model guarantee this? putting downside seems something apply model.} \zd{I think model try learn identify salient part. Instead, explicitly provide salient part model model learns rely input.} \gn{But extractive summarization model may fail test time, right?} \zd{right, think that's problem extractive summarization, goal model learn depend input, matter whether input signal correct not. } \gn{See comment below. I think there's problem disconnect presenting method , we're actually experiments. It'd best write story way encompasses things experiments . Could think way reframe intro little bit direction? I think one thing definitely say method use wide variety different types guidance, including automatic up-stream systems, perhaps user-specified keywords etc. You using method encourage model pay close attention guidance . This empirically effective. I'll take look thought bit modified intro accordingly. Additionally, might want add sentence end first paragraph describing attempt achieve paper jumping previous work. This help make contrasts clear paragraph.} \zd{Thanks lot! I'll think change paper accordingly!}. %To improve controllability summarization models, previous works attempted provide models keywords length information, choices guidance limited thus controllability output summaries hindered \gn{Again, super-clear proposed method better aspects}. %\gn{I think OK, could really benefit figure top-right page 1 demonstrating behavior.} %To obtain abstractive summarization models good performance well flexible controllability, In paper, propose general extensible guided summarization framework take different kinds external guidance input. %\gn{Maybe one sentence framework works.} Like recent summarization models, model based neural encoder-decoders, instantiated contextualized pretrained language models, including BERT BART. With strong starting point, make modifications allowing model attend source documents guidance signals generating outputs. %\gn{A little concreteness could help, even saying ``attends sequences representing source document guidance signal''.} %\gn{I would put next two sentences method description above, discuss specific types guidance provide.} As shown Figure, provide automatically extracted user-specified guidance model test time constrain model output. At training time, encourage model pay close attention guidance, %\pfliu{Since oracle-based training method contribution work, would better express explicitly. For example: ``we propose use ...instead ..''} propose use oracle select informative guidance signals -- simple modification nonetheless proved essential effective learning guided summarization models. %\gn{How different ? This sentence seems say thing second-to-last sentence previous paragraph. I understand ``extensible'' may attempting make contrast, clear.}. Using framework, investigate four types guidance signals: highlighted sentences source document, keywords, salient relational triples form , retrieved summaries. %\zj{Just minor point. Maybe better make orders consistent experiment section .} We evaluate methods 6 popular summarization benchmarks. Our best model, using highlighted sentences guidance, achieve state-of-the-art performance 4 6 datasets, including 1.28/0.79/1.13 ROUGE-1/2/L improvements previous state-of-the-art model widely-used CNN/DM dataset. In addition, perform in-depth analyses different guidance signals demonstrate complementary aggregate outputs together obtain improvements. An analysis results also reveals guided models generate faithful summaries novel words. Finally, demonstrate control output providing user-specified guidance signals, different provided signals resulting qualitatively different summaries. %\pfliu{Do need highlight contributions?} %We first evaluate methods widely-used CNN/DailyMail benchmark perform in-depth analysis different guidance signals. Experimental results demonstrate best method achieve 1.13 ROUGE-L improvements state-of-the-art model. We pick best guidance signal evaluate models five popular summarization benchmarks. Extensive experiments demonstrate effectiveness model extractive datasets analyses reveal methods generate novel words faithful summaries. In addition, control output providing user-specified guidance signals. In work made two novel contributions towards improving natural language processing tasks using human gaze predictions supervisory signal. First, introduced novel hybrid text saliency model that, first time, integrates cognitive reading model data-driven approach address scarcity human gaze data text. Second, proposed novel joint modeling approach allows TSM flexibly adapted different NLP tasks without need task-specific ground truth human gaze data. We showed advances result significant performance improvements state art paraphrase generation well competitive performance sentence compression much less complex model state art. We demonstrated approach effective yielding task-specific attention predictions. Taken together, findings demonstrate feasibility significant potential combining cognitive data-driven models NLP tasks -- potentially beyond -- also saliency predictions effectively integrated attention layer task-specific neural network architectures improve performance."," Neural abstractive summarization models are flexible and can produce coherent summaries, but they are sometimes unfaithful and can be difficult to control. While previous studies attempt to provide different types of guidance to control the output and increase faithfulness, it is not clear how these strategies compare and contrast to each other. In this paper, we propose a  general and extensible guided summarization framework  that can effectively take different kinds of external guidance as input, and we perform experiments across several different varieties. Experiments demonstrate that this model is effective, achieving state-of-the-art performance according to ROUGE on 4 popular summarization datasets when using highlighted sentences as guidance. In addition, we show that our guided model can generate more faithful summaries and demonstrate how different types of guidance generate qualitatively different summaries, lending a degree of controllability to the learned models.\footnote{Code is available at \url{https://github.com/neulab/guided_summarization}.}%, generating more novel words, and generating more faithful summaries on 4 popular summarization datasets \gn{``when using XXX as guidance''}. In addition, we demonstrate how different types of guidance generate qualitatively different summaries, lending a degree of controllability to the learned models."
"In recent years, abstractive summarization made impressive progress development sequence-to-sequence framework . This framework composed encoder decoder. The encoder processes source text extracts necessary information decoder, predicts word summary. Thanks generative nature, abstractive summaries include novel expressions never seen source text, time, abstractive summaries difficult produce compared extractive summaries formed directly selecting subset source text. It also found seq2seq-based abstractive methods usually struggle generate out-of-vocabulary words rare words, even words found source text. Copy mechanism alleviate problem meanwhile maintain expressive power seq2seq framework. The idea allow decoder generate summary scratch also copy words source text. Though effective English text summarization, copy mechanism remains relatively undeveloped summarization East Asian languages e.g. Chinese. Generally speaking, abstractive methods Chinese text summarization comes two varieties, word-based character-based. Since explicit delimiter Chinese sentence indicate word boundary, first step word-based methods perform word segmentation . Actually, order avoid segmentation error reduce size vocabulary, existing methods character-based . When trying combine character-based methods Chinese copy mechanism, original ``word copy'' degrades ``character copy'' guarantee multi-character word copied verbatim source text . Unfortunately, copying multi-character words quite common Chinese summarization tasks. Take Large Scale Chinese Social Media Text Summarization Dataset example, according Table I, 37\% words summaries copied source texts consist multiple characters. } \end{center} \end{table} Selective read proposed handle problem. It calculates weighted sum encoder states corresponding last generated character adds result input next decoding step. Selective read provide location information source text decoder help perform consecutive copy. A disadvantage approach, however, increases reliance present computation partial results current step makes model vulnerable errors accumulation leads exposure bias inference. Another way make copied content consecutive directly copying text spans. Zhou et al. implement span copy operation equipping decoder module predicts start end positions span. Because longer span decomposed shorter ones, actually many different paths generate summary inference, model optimized longest common span time step training, exacerbates discrepancy two phases. In work, propose novel lexicon-constrained copying network . The decoder LCN copy either single character text span time, constrain text span match potential multi-character word. Specifically, given text several off-the-shell word segmentators, text span included segmentation result text, consider potential word. By so, number available spans significantly reduced, making viable marginalize possible paths training. Furthermore, inference, aggregate partial paths fly producing output using word-enhanced beam search algorithm, encourages model copy multi-character words facilitates parallel computation. To line aforementioned decoder, encoder revised learn representations characters also multi-character words. In context neural machine translation, Su et al. first organized characters multi-character words directed graph named word-lattice. Following Xiao et al. , adopt encoder based Transformer take word-lattice input allow character word hidden representation. By taking account relative positional information calculating self-attention, encoder capture global local dependencies among tokens, providing informative representation source text decoder make copy decisions. Although model character-based , directly utilize word-level prior knowledge, keywords. In setting, keywords refer words source text high probability inclusion summary. Inspired Gehrmann et al. , adopt separate word selector based large pre-trained language model, e.g. BERT extract keywords. When decoder intends copy words source text, selected keywords treated candidates, words masked out. Experimental results show model achieve better performance incorporating word selector. We propose general framework guided neural summarization, using investigate four types guidance signals achieve state-of-the-art performance various popular datasets. We demonstrate complementarity four guidance signals, find models generate novel words faithful summaries. We also show control output providing user-specified guidance signals. Given generality framework, opens possibility several future research directions including 1) developing strategies ensemble models different guidance signals; 2) incorporating sophisticated techniques copy coverage source document, guidance signal, both; 3) experimenting kinds guidance signals salient elementary discourse units."," Copy mechanism allows sequence-to-sequence models to choose words from the input and put them directly into the output, which is finding increasing use in abstractive summarization. However, since there is no explicit delimiter in Chinese sentences, most existing models for Chinese abstractive summarization can only perform character copy, resulting in inefficient. To solve this problem, we propose a lexicon-constrained copying network that models multi-granularity in both encoder and decoder. On the source side, words and characters are aggregated into the same input memory using a Transformer-based encoder. On the target side, the decoder can copy either a character or a multi-character word at each time step, and the decoding process is guided by a word-enhanced search algorithm which facilitates the parallel computation and encourages the model to copy more words. Moreover, we adopt a word selector to integrate keyword information. Experiments results on a Chinese social media dataset show that our model can work standalone or with the word selector. Both forms can outperform previous character-based models and achieve competitive performances."
"Humans supervised natural language inference . Supervision necessary applications human-defined domains. For example, humans need supervision noun POS tagging, tiger Wordnet classify image tiger ImageNet. However, NLI, people able entail \textcircled{a} A man plays piano contradicts \textcircled{b} A man plays clarinet family without supervision NLI labels. In paper, define inference general process establishing associations inferences texts, rather strictly classifying whether two sentences entail contradict other. Inspired this, raise core problem paper: Given pair natural language sentences, machines entail relationship without supervision inference labels? In highly acclaimed paper, neuroscientist Moshe Bar claims ``predictions rely existing scripts memory, result real well previously imagined experiences''. The exemplar theory argues humans use {\bf similarity} recognize different objects make decisions. Analogy helps humans understand novel object linking similar representation existing memory. Such linking facilitated object context. Context information widely applied self-supervision learning . Adapting context NLI even straightforward. A simple idea {\bf constant conjunction} A causes B constantly conjoined. Although constant conjunction contradicts ``correlation causation'', modern neuroscience confirmed humans use reasoning mental world. For example, found increase synaptic efficacy arises presynaptic cell's repeated persistent stimulation postsynaptic cell Hebbian theory. As natural language, object context naturally used determine inference. For example, \textcircled{a} contradicts \textcircled{b} cannot happen simultaneously {\bf context}. The context representation learned SSL already achieved big success NLP. From perspective context, models learn sentence level contextual information word level contextual information . Besides linguistic contexts, humans also link modalities novel inputs. Even goal reason plain texts, modalities still help . For example, textual information used, difficult entail contradiction \textcircled{a} \textcircled{b}. We need commonsense man two arms, cannot play piano clarinet simultaneously. This commonsense hard obtain text. However, link sentences visual scenes, contradiction much clearer two scenes cannot happen visual context. We think necessary incorporate modalities unsupervised natural language inference. The idea adapting multimodal SSL new. According to, briefly divide previous multimodal SSL approaches two categories based encoder infrastructures. As shown Fig., first category uses one joint encoder represent multimodal inputs. Obviously, downstream task plain text, cannot extract representation text separately joint encoder. So first category infeasible natural language inference. The second category first encodes text image separately two encoders. Then represents multimodal information via joint encoder lower layer encoders. This shown Fig.. Although textual representation extracted text encoder lower layer, representation go joint learning module contains little visual knowledge. In summary, encoders previous multimodal SSL approaches coupled. If textual inputs given, cannot effectively incorporate visual knowledge representations. Thus help entailing contradiction \textcircled{a} \textcircled{b} limited. In order benefit multimodal data plain text inference, propose \underline{M}ultimodal \underline{A}ligned \underline{C}ontrastive \underline{D}ecoupled learning network. This shown Fig.. Its text encoder decoupled, takes plain text inputs. Thus directly adapted downstream NLI tasks. Besides, use multimodal contrastive loss text encoder image encoder, thereby forcing text representation align corresponding image. Therefore even text encoder MACD takes plain text input, still represents visual knowledge. In downstream plain text inference tasks, without taking images input, text encoder MACD still implicitly incorporating visual knowledge learned multimodal contrastive loss. Note need decoupled image encoder SSL. So image encoder Fig. MACD takes texts inputs provides precise image encoder. We elaborate section. In paper, propose novel lexicon-constrained copying network Chinese summarization. Querying multigranularity representation learned encoder, decoder copy either character multi-character word time step. Experiments LCSTS dataset show model superior Transformer baselines quite competitive latest models. With help keyword information provide word selector, even achieve state-of-the-art performance. In future, plan apply model tasks, comment generation, languages, English. \hfill mds \hfill August 26, 2015 An example floating figure using graphicx package. Note \label must occur AFTER \caption. For figures, \caption occur \includegraphics. Note IEEEtran v1.7 later special internal code designed preserve operation \label within \caption even captionsoff option effect. However, issues like this, may safest practice put \label \caption rather within \caption{}. Reminder: ""draftcls"" ""draftclsnofoot"", ""draft"", class option used desired figures displayed draft mode. Note IEEE typically puts floats top, even results large percentage column occupied floats. An example double column floating figure using two subfigures. The subfigure \label commands set within subfloat command, \label overall figure must come \caption. \hfil used separator get equal spacing. Watch combined width subfigures line exceed text width line break occur. Note often IEEE papers subfigures employ subfigure captions , instead reference/describe , , etc., within main caption. Be aware subfig.sty generate , , etc., subfigure labels, optional argument \subfloat must present. If subcaption desired, leave contents blank, e.g., \subfloat[]. An example floating table. Note that, IEEE style tables, \caption command come BEFORE table and, given table captions serve much like titles, usually capitalized except words a, an, and, as, at, but, by, for, in, nor, of, on, or, the, up, usually capitalized unless first last word caption. Table text default \footnotesize IEEE normally uses smaller font tables. The \label must come \caption always. Note IEEE put floats first column - typically anywhere first page matter. Also, in-text middle positioning typically used, allowed encouraged Computer Society conferences . Most IEEE journals/conferences use top floats exclusively. Note that, LaTeX2e, unlike IEEE journals/conferences, places footnotes bottom floats. This corrected via \fnbelowfloat command stfloats package.","   We propose to solve the natural language inference problem without any supervision from the inference labels via task-agnostic multimodal pretraining. Although recent studies of multimodal self-supervised learning also represent the linguistic and visual context, their encoders for different modalities are coupled. Thus they cannot incorporate visual information when encoding plain text alone. In this paper, we propose \underline{M}ultimodal \underline{A}ligned \underline{C}ontrastive \underline{D}ecoupled learning  network. MACD forces the decoupled text encoder to represent the visual information via contrastive learning. Therefore, it embeds visual knowledge even for plain text inference. We conducted comprehensive experiments over plain text inference datasets . The unsupervised MACD even outperforms the fully-supervised BiLSTM and BiLSTM+ELMO on STS-B."
"%... netune... As neural machine translation models become heavier heavier , resort model compress techniques deploy smaller models devices limited resources, mobile phones. However, practical challenge hardware conditions different devices vary greatly. To ensure calculation latency, customizing distinct model sizes different devices necessary, leads huge model training maintenance costs . For example, need distill pre-trained large model N individual small models. %Then model post-processing steps, model pruning quantization , also performed independently small model. The situation becomes worse industry considering translation directions frequent model iterations. An ideal solution train single model run different model sizes. Such attempts explored SlimNet LayerDrop . SlimNet allows running four width configurations joint training width networks, LayerDrop decode depth configuration applying Dropout layers training. In work, take step along line flexible depth network like LayerDrop. As shown Figure, first demonstrate large gap predefined layer dropout training actual pruning ratio inference, LayerDrop's performance poor. %We attribute huge sub-network training space mismatch random sampling training deterministic inference. To solve problem, propose use multi-task learning train flexible depth model treating supported depth configuration task. We reduce supported depth space aggressive model compression rate propose effective deterministic sub-network assignment method eliminate mismatch training inference LayerDrop. %Specifically, design two metrics determine sub-network assignment good. Experimental results deep Transformer show approach simultaneously support decoding 24 depth configurations superior individual training LayerDrop. In paper, study multimodal self-supervised learning unsupervised NLI. The major flaw previous multimodal SSL methods use joint encoder representing cross-modal correlations. This prevents us integrating visual knowledge text encoder. We propose multimodal aligned contrastive decoupled learning , learns represent visual knowledge using texts inputs. In experiments, proposed approach steadily surpassed methods large margin."," The standard neural machine translation model can only decode with the same depth configuration as training. Restricted by this feature, we have to deploy models of various sizes to maintain the same translation latency, because the hardware conditions on different terminal devices  may vary greatly. Such individual training leads to increased model maintenance costs and slower model iterations, especially for the industry. In this work, we propose to use multi-task learning to train a flexible depth model that can adapt to different depth configurations during inference. Experimental results show that our approach can simultaneously support decoding in 24 depth configurations and is superior to the individual training and another flexible depth model training methoderDrop."
"Targeted sentiment analysis involves jointly predicting entities targets opinion, well polarity expressed towards . The TSA task, part larger set fine-grained sentiment analysis tasks, enable companies provide better recommendations , well give digital humanities scholars quantitative approach identifying sentiment emotions develop literature . Although many improvements modelling TSA since original CRF models , utilising Recurrent Neural Networks , treating task span prediction rather sequence labelling task , concentrated making best use data annotated specifically task. However, annotation fine-grained sentiment taxing tends lower inter-annotator agreement document sentence classification tasks . This leads lack available high-quality training data, even highly resourced languages prevents TSA models learning complex, compositional phenomena necessary correctly predict targeted sentiment end-to-end fashion. We believe lack data fine-grained sentiment analysis leads TSA models cannot learn effectively complex compositional phenomena exists language, thus making TSA models fragile highly compositional language. It also shown incorporating compositional information negation speculation detection improves sentence-level sentiment classification . Other supervised tasks, semantic role labelling , document level sentiment analysis shown promise improving fine-grained sentiment analysis. Further transfer learning self-supervised language-modelling task, commonly referred contextualised word representations , also shown greatly benefit fine-grained sentiment analysis . Based this, paper, wish explore two research questions: To end, propose multi-task learning approach incorporate sources negation speculation information neural targeted sentiment classifier. We additionally compare approach MTL models use part-of-speech tagging, dependency relation prediction, lexical analysis auxiliary tasks, following previous work . Furthermore, order overcome lack evaluative resources investigate effects negation speculation, annotate two new challenge datasets contain difficult negated speculative examples. We find MTL models robust single task learning , performing competitively majority standard datasets significantly outperforming STL models negation challenge datasets, average better STL models speculation challenge datasets. Moreover, show transfer learning applied, using CWR, MTL STL models, MTL models longer significantly better, still better average negation challenge dataset one speculation challenge datasets. This result suggests transfer learning incorporate compositional information required negated speculative samples. However results challenge datasets considerably lower standard dataset, showing work needed make models robust compositional language. The contributions paper following: We demonstrated LayerDrop suitable FDM training huge sub-network space training mismatch training inference. Then proposed use multi-task learning mitigate it. Experimental results show approach decode 24 depth configurations obtain comparable better performance individual training LayerDrop. In future, plan explore effective FDM training methods, combining flexible depth width also one attractive directions.","   The majority of work in targeted sentiment analysis has concentrated on finding better methods to improve the overall results. Within this paper we show that these models are not robust to linguistic phenomena, specifically negation and speculation. In this paper, we propose a multi-task learning method to incorporate information from syntactic and semantic auxiliary tasks, including negation and speculation scope detection, to create models that are more robust to these phenomena. Further we create two challenge datasets to evaluate model performance on negated and speculative samples. We find that multi-task models and transfer learning from a language model can improve performance on these challenge datasets. However the results indicate that there is still much room for improvement in making our models more robust to linguistic phenomena such as negation and speculation."
"% making new tools useless noone uses efficiently The consensus human activity caused climate crisis led development many tools possible policy interventions, designed minimize greenhouse gas emissions mitigate negative impacts climate change. However, even promising tools counter climate crisis futile, used. All important research efforts mitigate climate crisis lost without efficient international adaptation tools policies. %promises politicans lead action Strategies adopted national level, following international cooperative guidelines Sustainable Development Goals Kyoto protocol . However, scientists, non-state actors\footnote{https://www.euronews.com/living/2018/12/21/ngos-sue-french-government-over-insufficient-climate-change-action}, voters increasingly critique government insufficient action mitigating climate change . This suggests gap promises made politicians actual action taken: somewhere along way ambitious promises climate change mitigation turned careless discourse insufficient measures taken. %accountability shown prevent mismanagement Holding politicians accountable actions shown major factor preventing mismanagement, political corruption misalignment politician opinions public representing . %so working improving accountability use existing tools Our work aims provide general public metric assess candidate party using platform discuss topics related climate change. % overall system In Section introduce Multi-Source Topic Aggregation System increases transparency providing overview topics discussed politicians. The large amount publicly available documents made transparent MuSTAS topic overview, would otherwise unattainable general public due amount data. Through transparency, decision-makers held accountable promises claims general public, accelerating policies societal changes needed mitigate adapt climate change. %Using large amount publicly available data processed asses politician uses influence across channels timelines. % The research multi-source LDA builds scientific foundation In Section describe novel multi-source hybrid latent Dirichlet allocation model builds scientific foundation MuSTAS forms core research proposal. In Section outline MuSTAS impacts climate change. % climate change impact % Here short paragraph structure proposal roles: MuSTAS larger scope, topic modelling multi-source hybrid LDA focus research. %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% In paper, compared effects MTL using various auxiliary tasks TSA created negation speculation annotated challenge dataset\footnote{Dataset found } TSA order isolate effects MTL. We show TSA methods drastically affected negation speculation effects data. These speculation effects reduced incorporating speculation information model MTL. The negation speculation effects also reduced transfer learning via CWR. Additionally, MTL negation still improve CWR models significantly. These findings answer two original research questions. Whereby found general MTL using negation auxiliary task make TSA models robust negated samples. Additionally transfer learning must extent learnt compositional knowledge useful classifying negated speculative samples, leading robust TSA models. Lastly using general syntactic information auxiliary task within MTL creates models robust negation speculation. However results presented show sensitive current models linguistic phenomena suggest still much room improvement. As results standard datasets found using MTL always improve performance results \Fi Laptop Table showed transfer learning harm performance\footnote{Compare performance MTL using GloVe uses CWR .}. Additionally results challenge datasets showed different auxiliary tasks improved performance different subtasks TSA extraction sentiment classification metrics. This may suggest target extraction sentiment classification tasks treated collapsed labelling task, sentiment extraction tasks similar enough . Future work consider using pipeline approach, subtask paired beneficial auxiliary tasks, whereby approach could lead MTL transfer learning better complimenting other. Finally, release code\footnote{}, dataset, trained models associated paper, hyperparameter search details compute infrastructure , number parameters runtime details , detailed dev test results , line result checklist .","     Decades of research on climate have provided a consensus that human activity has changed the climate and we are currently heading into a climate crisis.     Many tools and methods, some of which utilize machine learning, have been developed to monitor, evaluate, and predict the changing climate and its effects on societies.      However, the mere existence of tools and increased awareness have not led to swift action to reduce emissions and mitigate climate change.     Politicians and other policy makers lack the initiative to move from talking about the climate to concrete climate action. % in an appropriate schedule allowing for mitigation of the potentially catastrophic changes.      In this work, we contribute to the efforts of holding decision makers accountable by describing a system which digests politicians' speeches and statements into a topic summary.     We propose a multi-source hybrid latent Dirichlet allocation model which can process the large number of publicly available reports, social media posts, speeches, and other documents of Finnish politicians, providing transparency and accountability towards the general public."
"In recent years, effectiveness utilizing image data tandem text corpus improve quality machine translation source extensive investigation. Several proposals made incorporate visual data, using doubly-attentive decoder image text data , initializing encoder decoder hidden state image features , using deliberation network approach refine translations using image data . However, common difficulty lack publicly available multimodal corpora, particularly English-Japanese translation tasks. Currently, two available English-Japanese multimodal datasets Japanese extension Pascal sentences Flickr30k Entities JP , Japanese translation Flickr30k Entities dataset . In order contribute current list English-Japanese multimodal corpora, propose new multimodal English-Japanese corpus comparable sentences. Comparable sentences sentences contain bilingual terms parallel phrases describe similar topic, direct translations . This data particular interest due natural prevalence across various areas media. For example, e-commerce sites different countries may product descriptions similar products different languages, social media users may comment images several different languages. In study, created large comparable training corpus compiling existing image captions MS-COCO STAIR captioning datasets. %By compiling existing image captions MS-COCO STAIR captioning datasets, able create large comparable training corpus require translation. Furthermore, validation testing purposes, translated small subset MS-COCO captions contain ambiguous verbs. The advantage comparable sentences relation available quantity clearly seen Table , proposed corpus containing almost twice many sentence pairs Flickr30k Entities JP, current largest parallel multimodal English-Japanese corpus. As benchmark current multimodal NMT models corpus, performed English-Japanese translation experiment using several baseline models, confirmed current NMT models well suited comparable translation task. %o evaluate proposed corpus, performed English-Japanese translation experiment several baseline models, confirmed current NMT models well suited comparable translation task. However, believe corpus used facilitate research creating multimodal NMT models better utilize comparable sentences. % % The following footnote without marker needed camera-ready % version paper. % Comment instructions uncomment 8 lines % ""final paper"" variant English. % . % % % final paper: en-uk version % % % space normally used marker % This work licensed Creative Commons % Attribution 4.0 International Licence. % Licence details: % \url{http://creativecommons.org/licenses/by/4.0/}. % % % final paper: en-us version % % % space normally used marker % This work licensed Creative Commons % Attribution 4.0 International License. % License details: % \url{http://creativecommons.org/licenses/by/4.0/}. } \newcommand\T{\rule{0pt}{2.6ex}} \newcommand\B{\rule[-1.5ex]{0pt}{0pt}} \newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash}m{#1}} \newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash}m{#1}} \newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash}m{#1}} \end{table} In paper, propose multi-adversarial learning method cross-lingual word embeddings. Our system learns different linear mappings different source subspaces instead learning single one whole source space. The results experiments bilingual lexicon induction close languages difficult case typologically-distant languages prove learning cross-lingual word embeddings multi-mapping improves performance single mapping."," Multimodal neural machine translation  has become an increasingly important area of research over the years because additional modalities, such as image data, can provide more context to textual data. Furthermore, the viability of training multimodal NMT models without a large parallel corpus continues to be investigated due to low availability of parallel sentences with images, particularly for English-Japanese data. However, this void can be filled with comparable sentences that contain bilingual terms and parallel phrases, which are naturally created through media such as social network posts and e-commerce product descriptions. In this paper, we propose a new multimodal English-Japanese corpus with comparable sentences that are compiled from existing image captioning datasets. In addition, we supplement our comparable sentences with a smaller parallel corpus for validation and test purposes. To test the performance of this comparable sentence translation scenario, we train several baseline NMT models with our comparable corpus and evaluate their English-Japanese translation performance. Due to low translation scores in our baseline experiments, we believe that current multimodal NMT models are not designed to effectively utilize comparable sentence data. Despite this, we hope for our corpus to be used to further research into multimodal NMT with comparable sentences."
"%\todo[inline]{Why predicting hate-speech important general?} %\todo[inline]{why detecting hate-speech important Yahoo news Yahoo finance?} % %What problem? %Why interesting important? %Why hard? %Why solved before? %What key components approach results? Also include specific limitations. %Hatespeech speech ``intended insult, offend, intimidate person trait "". The occurrence hatespeech increasing. It become easier reach large audience quickly via social media, causing increase temptation inappropriate behaviors hatespeech, potential damage social systems. In particular, hatespeech interferes civil discourse turns good people away. Furthermore, hatespeech virtual world lead physical violence certain groups real world\footnote{https://www.nytimes.com/2018/10/31/opinion/caravan-hate-speech-bowers-sayoc.html}\footnote{https://www.washingtonpost.com/nation/2018/11/30/how-online-hate-speech-is-fueling-real-life-violence}, ignored ground freedom speech. To detect hatespeech, researchers developed human-crafted feature-based classifiers , proposed deep neural network architectures . %Online service providers also strive combat hatespeech ranking algorithms, filtering, suspending deactivating user accounts. \textcolor{red}{However, blah blah blah}. However, might explore possible important features hatespeech detection, ignored pre-trained language model understanding, proposed uni-directional language models reading left right right left. %--> 2. Other deep model hatespeech detection: either understand fully hateful context , ignore pretrained language model understanding and/or uni-directionally understanding language models reading left right right left . Recently, BERT model achieved tremendous success Natural Language Processing % . The key innovation BERT applying transformer language modeling tasks. %It proposed language modeling two tasks: predicting masked words predicting next sentence. A BERT model pre-trained language modeling tasks forms good basis fine-tuning supervised tasks machine translation question answering, etc. Recent work hatespeech detection applied BERT model shown prominent results previous hatespeech classifiers. However, point two limitations hatespeech detection domain. First, previous studies shown hateful corpus owns distinguished linguistic/semantic characteristics compared non-hateful corpus. For instance, hatespeech sequences often informal even intentionally mis-spelled, words hateful sequences sit long tail ranking uniqueness, comment hateful non-hateful using words . %For example, ``n1gger'' sentence ``i `n1gger' indicated'' non-hateful, ``n1gger'' ``you n1gger!'' hateful. For example, ``dick'' sentence ``Nobody knew dick meant'' non-hateful, ``d1ck'' ``You weak small-d1cked keyboard warrior'' hateful \footnote{It important note paper contains hate speech examples, may offensive readers. They represent views authors. We tried make balance showing less number hate speech examples illustrating challenges real-world applications.}. Thus, better understand hateful vocabularies contexts, better pre-train mixture hateful non-hateful corpora. Doing helps overcome limitation using BERT models pre-trained non-hateful corpora like English Wikipedia BookCorpus. Second, even smallest pre-trained BERT ``base'' model contains 110M parameters. It takes lot computational resources pre-train, fine-tune, serve. %There recent efforts reducing Some recent efforts aim reduce complexity BERT model knowledge distillation technique DistillBert TinyBert . In methods, pre-trained BERT-alike model used teacher model, student model trained produce similar output teacher model. Unfortunately, complexity reduced, performance also degraded NLP tasks compared BERT. Another direction use cross-layer parameter sharing, ALBERT . However, ALBERT's computational time similar BERT, since number layers remains BERT; likewise, inference equally expensive. Based observation analysis, aim investigate whether possible achieve better hatespeech prediction performance state-of-the-art machine learning classifiers, including classifiers based publicly available BERT model, significantly reducing number parameters compared BERT model. By so, believe performing pre-training tasks ground hatespeech-related corpus would allow model understand hatespeech patterns better enhance predictive results. However, language model pretraining tasks require large scale corpus size, available hatespeech datasets normally small: 16K115K annotated comments . Thus, introduce large annotated hatespeech dataset 1.4M comments extracted Yahoo News Yahoo Finance. To reduce complexity, reduce number layers hidden size, propose Quaternion-based Factorization mechanisms BERT architecture. To improve model effectiveness robustness, introduce multi-source ensemble-head fine-tuning architecture, well target-based adversarial training. %Internet platforms moderate user-generated content interest majority users, business needs. Through ranking algorithms, filtering, suspending deactivating user accounts, many Internet companies strive combat hatespeech. %Twitter, example, ""the twitter rules"", states ``Violence, harassment similar types behavior discourage people expressing themselves, ultimately diminish value global public conversation."". %To ensure users positive experience properties, Verizon Media also clear rules hatespeech. %, state ``Don't use hatespeech. Hatespeech directly attacks person group basis race, ethnicity, national origin, religion, disability, disease, age, sexual orientation, gender, gender identity. As noted above, we're diverse global community many types people, different beliefs, opinions, sensitivities, comfort levels. If feel abide Community Guidelines outlined below, maybe participating Oath community you."" %At Verizon Media, Standard Moderation Platform runs platform service moderate text, URL, images videos. The hatespeech classifiers SMP based number past research work, including. % The purpose work described paper improve performance current state art hatespeech classifiers. In previous study, % used pretrained BERT model starting point fine tuning. % %, investigated range different machine learning models text classification. % %We show combination linear model, % We found BERT architecture gives better performance baseline models, including. %, well Google's Prospective API. In study, pretrained BERT model used starting point fine tuning. %Recently, BERT model become state-of-the-art language model achieved tremendous success Natural Language Processing . %\todo[inline]{talking BERT success variety nlp tasks cited works} %BERT modified transformer network architecture. Traditionally, many language tasks translation question answering, handled using recurrent neural networks, combined attention mechanism. This %reflects fact tend read sentence left right. However, human also read words within context words, %some could quite far apart, %instead left right right left mechanical way. Furthermore, recurrent network memory problem handle long text, due problems vanishing exploding gradient. In addition, intrinsically sequential, making training process slow. Transformer network proposed solve problems. In setup, word input text visibility words, use multi-headed attention. %It used %in variety NLP tasks well area image processing. %One motivation paper investigate whether possible achieve performance similar to, better publicly available BERT models, smaller models. In so, want realize considerable saving training serving time. Another motivation see possible improve BERT model further, introducing changes model architecture. The third motivation following. The pretrained BERT models based % BooksCorpus English Wikipedia. They different characteristics dataset interest us, consists users-generated comments Yahoo News Yahoo Finance. Consequently believe retraining language model scratch % give us model understands % language dataset better. %\todo[inline]{talking limitation BERT, like complicated heavy many parameters. Then question build better model, less number parameters?} The major contributions work are: \squishlist % \squishend % We organize paper followed. % We give related work Section, % define problem solving formerly Section. We present approach Section, show experimental results Section. We conclude paper Section discussions future work. % In paper, proposed new multimodal English-Japanese corpus comparable sentences. Based baseline performance data, believe current multimodal NMT models well suited type task, research required order better leverage comparable sentences images together order improve translation performance. In future, hope see corpus used encourage research multimodal machine translation tasks comparable sentences instead parallel sentences."," We present our {HABERTOR} model for detecting hatespeech in large scale user-generated content. Inspired by the recent success of the BERT model, we propose several modifications to BERT to enhance the performance on the downstream hatespeech classification task. {HABERTOR} inherits BERT's architecture, but is different in four aspects:  it generates its own vocabularies and is pre-trained from the scratch using the largest scale hatespeech dataset;  it consists of Quaternion-based factorized components, resulting in a much smaller number of parameters, faster training and inferencing, as well as less memory usage;  it uses our proposed multi-source ensemble heads with a pooling layer for separate input sources, to further enhance its effectiveness; and  it uses a regularized adversarial training with our proposed fine-grained and adaptive noise magnitude to enhance its robustness. Through experiments on the large-scale real-world hatespeech dataset with 1.4M annotated comments, we show that {HABERTOR} works better than 15 state-of-the-art hatespeech detection methods, including fine-tuning Language Models. In particular, comparing with BERT, our {HABERTOR} is 4$\sim$5 times faster in the training/inferencing phase, uses less than 1/3 of the memory, and has better performance, even though we pre-train it by using less than 1\% of the number of words. Our generalizability analysis shows that {HABERTOR} transfers well to other unseen hatespeech datasets and is a more efficient and effective alternative to BERT for the hatespeech classification.  %The code and the pretrained models are available at anonymized."
"%------------------Previous version------------------ %Since UNMT low-resource domains yet actively explored field, one may naively approach problem training model multiple domains expect generalize unseen, low-resource domains, e.g., training model news sports domains evaluating biomedical domain. %However, due domain mismatch, studied supervised NMT, model show inferior performance. %------------------------------------------------------- Unsupervised neural machine translation leverages unpaired monolingual corpora training, without requiring already labeled, parallel corpus. Recently, state art UNMT achieved comparable performances supervised machine translation approaches. However, case translation domain-specific documents, monolingual data scarce, collecting involves high cost, still suffering low NMT performance. For instance, model trained monolingual data low-resource domain, say, medical domain, experience degraded translation quality due overfitting. %------------------Previous version------------------ %Another reasonable approach transfer learning, frequently used domain adaption literature supervised NMT often showed improvements target domain. The model pretrained multiple domains finetuned new domain. However, approach may suffer overfitting catastrophic forgetting given small number training data large domain gap downstream task. %-------------------------------------------------- Yet, UNMT low-resource domains actively explored field. One naive approach train model high-resource domains hoping generalize unseen low-resource domain well. However, shown recent studies supervised NMT nontrivial domain mismatch significantly cause low translation accuracy. Another reasonable approach transfer learning, particular domain adaptation, shown performance improvements literature supervised NMT. In approach, model first pretrained using existing domains finetuned using data new domain. However, approach may suffer overfitting catastrophic forgetting due small number training data large domain gap. As effective method handling small number training data, meta-learning shown superiority various NLP tasks, dialog generation, translation, natural language understanding. However, best knowledge, applied tackle UNMT tasks small number training data, i.e., low-resource UNMT. In response, paper extends meta-learning approach low-resource UNMT, called \toolnameMeta. The objective \toolnameMeta find optimal initialization model parameters quickly adapt new domain even small amount monolingual data. To specific, assuming data multiple source domains available, makes meta-learning applicable, first pretrain UNMT model source domains based \toolnameMeta finetune model using target domain. Moreover, propose improved meta-learning approach called \ourtoolname low-resource UNMT explicitly promoting common knowledge across multiple domains well generalizable knowledge particular domain another. In particular, proposed approach prevents model overfitting due small amount training data new domain. In summary, contributions include following. %\item \ourtoolname shows domain-general knowledge faster convergence methods. %\item We empirically demonstrate enhanced algorithm, \ourtoolname, consequently boosts performance low-resource UNMT baseline models including \toolnameMeta. %\item We extend meta-learning algorithm incorporating domain mixing loss, outperforms methods. % %We show zero-shot performance evaluate generalization ability \ourtoolname, \ourtoolname outperforms methods. % To best knowledge, work first apply meta-learning approach UNMT tasks. Our proposed algorithms quickly adapt in-domain iteration steps. Both \toolnameMeta \ourtoolname consistently outperform baseline models 3 BLEU scores. Especially, \ourtoolname achieves promising results among others including \toolnameMeta. Besides, show zero-shot performance evaluate generalization ability \ourtoolname, \ourtoolname outperforms methods. %--------------------------------- %       general  feature . % Although domain distance others domain adaptation, share linguistic features, grammar basic words. % Azam: To alleviate aforementioned challenge, % Azam: To overcome issue, many % % \item % \item  contribtuion bullet point summary : % \item 1. formulate new task % \item 2. New frame work proposed % \item 3. evaluate various domain. show fast adaptation quality. %On hand, since unsupervised machine translation attained comparable performance supervised machine translation, fully unsupervised domain adaptation, uses monolingual data in-domain out-of-domain, suitable handle data-scarce languages. %yet challenge data-scarce domains. In words, unsupervised domain adaptation task handle data-scarce languages; however, cannot resolve challenge low-resource domains. % %such alleviates aforementioned challenge, building parallel corpus. %Therefore, fully unsupervised domain adaptation task, consisted unpaired language corpus in-domain out-of-domain, realistic setting supervised domain adaptation task.%and substantial effort collect domain specific data. %A meta-learning algorithm superior low-resource data. Unlike domain adaptation, meta-learning algorithm require in-domain data learn initial parameters. It asks training samples meta-train model. Collaborating meta-learning algorithm unsupervised machine translation %Since leverage cross-lingual language model pretraining allows model learn cross-lingual representations, gradient updates divided two objective functions, back-translation language modeling. % Several approaches proposed resolve scarcity problem. For instance, data mixing one approach aggregates high-resource low-resource data train model adequately translate low-resource target language % To overcome data scarcity problem, one simple approach data mixing aggregates high-resource low-resource data train model adequately translate low-resource target language. The approach transfer learning first pretrains high-resource data fine-tunes low-resource data. Although aforementioned approaches explicitly tackle low-resource challenge, scarcity problem still remains NMT building parallel corpus specialized expertise costly expensive. % In paper, leverage recent success unsupervised NMT uses monolingual corpus. Inspired , propose new task called low-resource UNMT. To best knowledge, first attempt % To overcome issue, unsupervised learning NMT proposed resolve parallel data scarcity problem. However, approach constraint abundant monolingual corpus always available. % In reality, monolingual corpus also scarce domains languages often used. %    % Although various approaches proposed address low-resource challenge , none works consider low-resource unsupervised task NMT. To best knowledge, first attempt explicitly tackles low-resource UNMT task. % In paper, % When translate word different language, semantic meaning changed. For instance, meaning word ""CNN"" different domain deep learning news % To overcome issue, abundant parallel data required easy obtain. Recently, unsupervised NMT studies show reasonable performance comparison supervised NMT. % Data mixing high-resource low-resource one approach handle following issue. The approach transferring learning method first trains high-resource datasets fine-tunes low-resource datatset. However, problem still remain parallel data scarce domains languages. To overcome parallel data scarcity problem % To overcome issue, unsupervised learning NMT proposed resolve problem insufficient parallel data. However, approach assumes obtaining monolingual corpus always easier acquiring parallel corpus. Since languages vary domains , either monolingual parallel data scarce. % utilize monolingual corpus assume monolingual corpus always available. However, % In NMT, data scarcity problem divided two different training data scenarios, insufficient training parallel data training data . To overcome scarce parallel data issue, recent studies proposed utilize monolingual corpus. % parallel data essential train NMT model % several learning methods, unsupervised learning transfer learning NMT, proposed overcome data scarcity problem. However, works consider languages still remains problem domains . Moreover, best knowledge, none works attempt %Learning inevitable phase adapt new task. However, various learning experiences reduce exertion learning new task. % %Domains %To overcome issue, one simple approach domain mixing aggregates high-resource low-resource domains train model adequately translate low-resource domain. The approach transfer learning first pretrains high-resource domains fine-tunes low-resource domain. %Despite remarkable success neural machine translation ~, performance NMT drops substantially traditional statistical machine translation training data scarce %To overcome scarcity training data languages, variants multilingual translation approaches proposed. These approaches basically exploit high-resource knowledge aggregating high-resource low-resource data train one single model. The approach utilizing transfer learning model first pretrains high-resource data later fine-tunes low-resource data. The similar manner follows domains well. %Moreover, few-shot learning meta-learning arise machine learning attempt handle data scarcity problem. In NMT, ~\citet{gu2018meta} re-formulates model-agnostic meta-learning algorithm resolve low-resource challenge NMT. %Although aforementioned approaches tackle low-resource challenge, data scarcity problem still remain following approaches require parallel data, building low-resource language pair specialized expertise costly expensive. Hence, recent research suggests rely monolingual corpus instead using parallel corpus. The various unsupervised NMT ~ studies show reasonable performance comparison supervised NMT. %  monolingual corpus     -  ..? %sufficient in-domain data train model; however, real-world, collecting domain specific data requires substantial effort. %building low-resource language pair specialized expertise costly expensive %   MT      . % Machine learning  Data scarcity   % Domain translation   % unsuperivsed machine translation % data mixing, transfer learning % knowledge gets partially vanished %   %  transfer learning mixing data    %  parallel setting   % parallel    % monolingual corpus  UNMT work  % unsupervised     %  monolingual corpus     -  ..? %   low-resource UNMT meta-learning algorithm    %  meta-nmt     unsupervised % multi doamin  % Though important issue, long sequence translation used regarded hard problem sequence-to-sequence methods neural style handle. However, paper, prove feasibility end end training field. Firstly, basic experiments show direct document-level translation large scale dataset comparable performance compared merged sentences generated sentence unit model. One step forward, widely available large scale sentence-level parallel data almost infinite document-level monolingual data used back-translation, potential document-level translation activated. In words, propose training criteria document translation break length bottleneck sentence-level translation model. The observation may shed important light extremely long sentence generation make us rethink routine long sequence machine translation. Finally, dataset proposed paper contributes greatly boost field. In paper, review main challenges unsolved document-level neural machine translation, including datasets, metrics, context usage, restricted training. After pointing under-explored status field, attempt refine groundwork. A package datasets, along new training paradigm DNMT proposed push limitation field. We hope work advance research works inspire correlative long-range sequence generation. In paper, review recent studies document-level NMT find sort misguided. Most works focus appending model modules model regularization, turns oversold. Instead, suggest heading back original concise way Doc2Doc deal DNMT. With multi-resolutional training strategy, Doc2Doc yields best results show significant superiority Doc2Sent. We also make step reveal bottleneck field lies datasets metrics. Correspondingly, propose package datasets along metrics boost development DNMT. We hope analytical review contributive datasets thought-provoking inspire works. In paper, propose literal document-to-document translation successfully activate multi-resolutional training. Different traditional methods modifying model architectures, approach introduces extra parameters. A comprehensive set experiments various metrics show advantage MR Doc2Doc. In addition, contribute new document-level dataset well three new metrics community."," Unsupervised machine translation, which utilizes unpaired monolingual corpora as training data, has achieved comparable performance against supervised machine translation. However, it still suffers from data-scarce domains. To address this issue, this paper presents a meta-learning algorithm for unsupervised neural machine translation  that trains the model to adapt to another domain by utilizing only a small amount of training data. We assume that domain-general knowledge is a significant factor in handling data-scarce domains. Hence, we extend the meta-learning algorithm, which utilizes knowledge learned from high-resource domains to boost the performance of low-resource UNMT. Our model surpasses a transfer learning-based approach by up to 2-4 BLEU scores. Extensive experimental results show that our proposed algorithm is pertinent for fast adaptation and consistently outperforms other baseline models."
"Numerous entities emerging everyday. The attributes entities often noisy incomplete, even missing. In field electronic commerce, target attributes new products often missing . In medical analysis, attributes like transmission, genetics origins novel virus often unknown people. Even DBpedia, well-constructed large-scale knowledge base extracted Wikipedia, half entities contain less 5 relationships . %In KG construction area, KGs often suffer incompleteness. %For example, DBpedia, well-constructed large-scale knowledge base extracted Wikipedia, half entities contain less 5 relationships . %Therefore, A method capable supplementing reliable attribute values emerging entities highly useful many applications. %With method automatically extract attribute values emerging entities, eCommerce retailers able better serve customers updated information; extracted medical attribute information novel virus organized assist understanding virus; KG able provide complete information users. Although information extraction methods extensively studied, task open attribute value extraction remains challenging. First, emerging entities may new attribute values absent existing KG. Under circumstances, prediction methods closed-world assumption methods cannot utilize external information well suited due limited recalls. Second, web corpus used good resource provide relatively updated relevant articles large varieties emerging entities, %that relatively complete updated timely manner, %considering large variety emerging entities, web corpus, relatively complete updated timely manner, able provide rich collection relevant articles. %However, articles retrieved web corpus noisy and/or irrelevant, turn leads limited precision. Finally, even articles relevant, extracted answers might still inaccurate due error-prone information extraction model. To effectively filter noisy answers obtained either due irreverent articles errors incurred information extraction system, %need answer pose following two questions: First, many articles collect enormous web corpus? Second, select reliable value pool possible answers extracted articles? There common answer first question works triplets inconsistent degrees difficulties finding correct attribute values. The decision stop querying external articles needs made successive evaluations candidate answers. Thus decision making process inherently sequential. %Thus, inherently sequential decision making problem. Reinforcement learning commonly adopted method deal sequential decision problems widely studied field robotic game . But many researches open attribute value extraction RL. One existing literature RL-based method value extraction proposed . In work, RL framework designed improve accuracy event-related value extraction acquiring incorporating external evidences. However, approach requires great amount context information specific event interest training process. It trivial extend framework open attribute value extraction, would need collect context words train new model annotated data emerging attribute. Therefore, framework cannot generalized open attribute value extraction task various entities attributes involved. While using context words construct states RL suitable task, solution leverage rich, well-organized information KG, informative also generalizable. %The knowledge KG Such information leveraged answer comparisons, addresses second question. For example, fill incomplete triplet iPhone 11, display resolution, ?, KG may find attribute values ``display resolutions"" entity category ``Phone"" commonly expressed format ``xxx xxxx Pixels"", x stands digit. The typical instances attribute values entities category provide valuable background information. In paper, propose knowledge-guided RL framework perform open attribute value extraction. The RL agent trained make good actions answer selection stopping time decision. Our experiments show proposed framework significantly boosts extraction performance. To best knowledge, first integrate KG RL framework perform open attribute value extraction %use KG guide RL-based sequential decision open attribute value extraction. %The experiment results demonstrate approach improves extraction performances substantially. In summary, contribution three folds: This paper proposed novel meta-learning approaches low-resource UNMT. \toolnameMeta leverages multiple source domains quickly effectively adapt model target domain. Moreover, introduce improved method called \ourtoolname, enhances aggregate-domain cross-domain generalization model incorporates knowledge learned across multiple domains. Eventually, method prevents model overfitting due small amount training data new domain, thereby leading improved performance low-resource UNMT. We empirically show proposed approaches consistently outperform baseline models nontrivial margin. In paper, propose novel meta-learning algorithms low-resource UNMT. These algorithms leverages multiple source domains learn common knowledge finetune target domain. By comparing various baseline models, empirically show proposed algorithms significantly surpass others. Moreover, introduce enhanced algorithm, \ourtoolname, utilizes aggregate-domain cross-domain losses model incorporates learned knowledge across multiple domains. Owing losses, \ourtoolname quickly adapt new domain iterations, improve performance low-resource UNMT . In experiment section, demonstrate \ourtoolname quickly pretrain source domains finetunes new domain iteration. Moreover, \ourtoolname shows superiority low resource doamain importance proposed loss effectiveness proposed algorithms varying size new domain. Moreover, \ourtoolname shows superiority Future work, apply extended algorithms computer vision domain tasks, whic suffer data scarcity problem. In paper, propose novel meta-learning algorithm low-resource UNMT. The algorithm leverages multiple source domains learn domain-general information finetune target domain. Moreover, introduce enhanced algorithm, \ourtoolname, utilizes aggregate-domain cross-domain losses model incorporates learned knowledge across multiple domains. Eventually, algorithm prevents model over-fitted due small amount training data new domain improves performance low-resource UNMT. We empirically show proposed algorithms effectively leverage domain-general knowledge outperform baseline models considerable margin. \begin{comment}"," Open attribute value extraction for emerging entities is an important but challenging task.  A lot of previous works formulate the problem as a question-answering  task.  While the collections of articles from web corpus provide updated information about the emerging entities, the retrieved texts can be noisy, irrelevant, thus leading to inaccurate answers. Effectively filtering out noisy articles as well as bad answers is the key to improving extraction accuracy. Knowledge graph , which contains rich, well organized information about entities, provides a good resource to address the challenge. In this work, we propose a knowledge-guided reinforcement learning  framework for open attribute value extraction.  Informed by relevant knowledge in KG, we trained a deep Q-network  to sequentially compare extracted answers to improve extraction accuracy. The proposed framework is applicable to different information extraction system. Our experimental results show that our method outperforms the baselines by 16.5 - 27.8\%."
"% NMT good needs lots parallel data + exploit mono data Neural machine translation using sequence sequence architectures become dominant approach automatic machine translation. While able approach human-level performance , still requires huge amount parallel data, otherwise easily overfit. Such data, however, might always available. At time, generally much easier gather large amounts monolingual data, therefore, interesting find ways making use data. The simplest strategy use backtranslation , %but rather costly since requires training another model opposite translation direction creating source-side synthetic sentences translating target-side monolingual corpus. rather costly since requires training model opposite translation direction translating monolingual corpus. % We introduce compositionality It suggested \citet{lake2017machines} development general human-like AI system, one desired characteristics system ability learn continuous manner using previously learned tasks building blocks mastering new, complex tasks. %by combining knowledge learned previously learned simpler tasks. Until recently, continuous learning neural networks problematic, among others, due catastrophic forgetting . Several methods proposed , however, %they mostly focused preserving knowledge task learned whole network. mainly focus adapting whole network new tasks maintaining good performance previously learned tasks. % Summary method using EWC %\XXX{toto mozna posunout za nasledujici odstavec + jak resime jejich nedostatky} In work, present unsupervised pretraining method NMT models using Elastic Weight Consolidation . First, initialize encoder decoder source target language models respectively. Then, fine-tune NMT model using parallel data. To prevent encoder decoder forgetting original language modeling task, regularize weights individually using Elastic Weight Consolidation based importance task. Our hypothesis following: forcing network remember original LM tasks reduce overfitting NMT model limited parallel data. %\XXX{Ukazujeme, ze metoda funguje, je rychlejis + mame odvozeno, ze mela fungovat pro podsite} %\XXX{Zminit rovnou strucne prinosy?} % Summary method used comparison We also provide comparison approach method proposed \citet{ramachandran2017pretraining}. They also suggest initialization encoder decoder language model. However, fine-tuning phase use original language modeling objectives additional training loss place model regularization. Their approach two main drawbacks: first, fine-tuning phase, still require original monolingual data might available anymore life-long learning scenario. Second, need compute machine translation language modeling losses increases number operations performed update slowing fine-tuning process. Our proposed method addresses problems: requires small held-out set estimate EWC regularization term converges 2-3 times faster previous method.\footnote{The speedup regard wall-clock time. In experiments EWC LM-objective methods require similar number training examples converge.} %Intro compositionality %Compositional learning + using previosly learned elementary knowledge learn complex model %Avoiding catastrophic forgetting key continual learning compositionality -> choice EWC %Benefits compositionality greater scope + NMT + LM pretrain? %It first step ongoing reseach %The paper structured following... Multi-hop QG task challenging worthy exploration compared conventional single-hop QG. To address additional challenges multi-hop QG, propose MulQG, multi-hop context encoding Graph Convolutional Network encoding fusion via Gated Reasoning module. To best knowledge, first tackle challenge multi-hop reasoning paragraphs without sentence-level information. The model performance HotpotQA dataset demonstrates effectiveness aggregating scattered pieces evidence across paragraphs fusing information effectively generate multi-hop questions. The strong reasoning ability Multi-hop Encoder MulQA model potentially leveraged complex generation tasks future work. In addition, human evaluation, proposed model likely generate fluent complete questions outperform baseline 20.8\ percentage questions assessed multi-hop type.","   This work presents our ongoing research of unsupervised pretraining in neural machine translation . In our method, we initialize the weights of the encoder and decoder with two language models that are trained with monolingual data and then fine-tune the model on parallel data using Elastic Weight Consolidation  to avoid forgetting of the original language modeling tasks.   We compare the regularization by EWC with the previous work that focuses on regularization by language modeling objectives.   %We compare the EWC regularization with the previous work that uses language modeling objectives from the original task for model regularization.      The positive result is that using EWC with the decoder achieves BLEU scores similar to the previous work. However,   the model converges 2-3 times faster and does not require the original unlabeled training data during the fine-tuning stage.      In contrast, the regularization using EWC is less effective if the original and new tasks are not closely related. We show that initializing the bidirectional NMT encoder with a left-to-right language model and forcing the model to remember the original left-to-right language modeling task limits the learning capacity of the encoder for the whole bidirectional context.      %%% POZNAMKY %%%   % Analyza Fisher Information   % - Self-attention projekce are more important than the Feedforward layers   % - Self-attention:   %     - output and value projections are more important at the higher layers   %     - key and query projections are more important at lower layers   % ...      % Previous work    % - requires orig. unlabeled data for MT training -> EWC can estimate Empirical Fisher on small  heldout data   % - is effective even when the orig. and new tasks differ   %  and then using this pretrained encoder in MT  -> EWC is bad at this      % Our work :   % - has nice mathematical definition    % - faster convergence in time    % - works only with decoder  -> little worse than LM obj.   % - method works when task are similar in nature    % - how deep should the unlabeled-data-pretrained enc-dec should be?   %       % why only left-context? previous work shows that with transformer, the drop in performance is not that big + it is much easier to implement       % Future work :   % - Investigate complementarity of EWC and LM obj.    % - Investigate the learning rate schemes    % - Investigate the method in the multimodal/multisource scenario   % - Investigate the method"
"Even though machine translation greatly improved emergence neural machine translation recently Transformer architecture , remain challenges solved using sentence-level NMT systems. Among issues, includes problem inter-sentential anaphora resolution consistent translation across document , system inevitably needs document-level context information. In recent years, many works focused changing existing NMT architectures incorporate context information translation process . However, often times results reported specific tasks , making difficult assess potential different methods general setting. This, together fact big improvements typically reported low resource tasks, gives impression document-level NMT mostly improves due regularization rather leveraging additional context information. In work want give complete overview current state document-level NMT comparing various approaches variety different tasks including application-oriented E-commerce setting. We discuss both, widely used performance metrics, well highly task-specific observations. Another important aspect talking document-level NMT applicability ``real life"" settings. There, faced low resource data scenario, back-translation established way greatly improving system performance . However, best knowledge, effect back-translation data obtained used context-aware models never explored before. The main contributions paper summarized below: We introduced work progress, exploration model regularization NMT encoder decoder parameters based importance previously learned tasks application unsupervised pretraining scenario. used unsupervised pretraining scenarios based importance language modeling tasks. We documented method slightly improves NMT performance combined pretrained target language model. We achieve improvement reduced training time. reducing training time. We also showed method less effective original language modeling task used pretrain NMT encoder different task learned fine-tuning. We plan investigate whether gain improvements using different pretraining method encoder much task mismatch relates learning capacity encoder.","  Context-aware neural machine translation  is a promising direction to improve the translation quality by making use of the additional context, e.g., document-level translation, or having meta-information. Although there exist various architectures and analyses, the effectiveness of different context-aware NMT models is not well explored yet. This paper analyzes the performance of document-level NMT models on four diverse domains with a varied amount of parallel document-level bilingual data. We conduct a comprehensive set of experiments to investigate the impact of document-level NMT.  We find that there is no single best approach to document-level NMT, but rather that different architectures come out on top on different tasks. Looking at task-specific problems, such as pronoun resolution or headline translation, we find improvements in the context-aware systems, even in cases where the corpus-level metrics like BLEU show no significant improvement. We also show that document-level back-translation significantly helps to compensate for the lack of document-level bi-texts.   \includecomment{ Context-aware neural machine translation  is a promising direction for improving the translation quality having more context, e.g., document-level translation, or having meta-information. The goal is to enhance the translation of discourse phenomena and polysemous words. This paper analyzes the performance of document-level NMT models with a varied amount of parallel document-level bilingual data. Including a diverse set of tasks, e.g., movie subtitles and e-commerce data, we conduct a comprehensive set of experiments to analyze and to learn the impact of document-level NMT. We show the document-level back-translation significantly helps to compensate for the lack of document-level bi-texts.  }"
"Automatic summarization fundamental task Natural Language Processing, aims condense original input shorter version covering salient information continuously studied decades . Recently, online multi-speaker dialogue/meeting become one important ways people communicate daily works. Especially due spread COVID-19 worldwide, people dependent online communication. In paper, focus dialogue summarization, help people quickly grasp core content dialogue without reviewing complex dialogue context. Recent works incorporate additional commonsense knowledge dialogue generation dialogue context representation learning show even though neural models strong learning capabilities, explicit knowledge still improve response generation quality. It dialog system understand conversations better thus respond properly access make full use large-scale commonsense knowledge. However, current dialogue summarization systems ignore exploration commonsense knowledge, may limit performance. In work, examine benefit incorporating commonsense knowledge dialogue summarization task also address question best incorporate information. Figure shows positive example illustrate effectiveness commonsense knowledge dialogue summarization task. Bob asks Tom help car broken down. On one hand, introducing commonsense knowledge according pick car broke down, know Bob expects Tom give lift. On hand, commonsense knowledge serve bridge non-adjacent utterances help model better understanding dialogue. In paper, follow previous setting also use ConceptNet large-scale commonsense knowledge base, difference regard knowledge text heterogeneous data real multi-speaker dialogue. We propose model named Dialogue Heterogeneous Graph Network incorporating commonsense knowledge constructing graph including utterance knowledge nodes. Besides, heterogeneous graph also contains speaker nodes time, proved useful feature dialogue modeling. In particular, equip heterogeneous graph network two additional designed modules. One called message fusion, specially designed utterance nodes better aggregate information speakers knowledge. The one called node embedding, help utterance nodes aware position information. Compared homogeneous graph network related works , claim heterogeneous graph network effectively fuse information contain rich semantics nodes links, thus accurately encode dialogue representation. We conduct experiments SAMSum corpus , large-scale chat summarization corpus. We analyze effectiveness integration knowledge heterogeneity modeling. The human evaluation also shows approach generate abstractive correct summaries. To evaluate whether commonsense knowledge help model better generalize new domain, also perform zero-shot setting experiments Argumentative Dialogue Summary Corpus , debate summarization corpus. In end, give brief summary contributions: We first incorporate commonsense knowledge dialogue summarization task. We propose D-HGN model encode dialogue viewing utterances, knowledge speakers heterogeneous data. Our model outperform various methods. This paper proposes adaptive attentional network few-shot KG completion, termed FAAN. Previous studies solve problem learning static representations entities references, ignoring dynamic properties. FAAN proposes encode entity pairs adaptively, predict facts adaptively matching references queries. Experiments two public datasets demonstrate model outperforms current state-of-art methods different few-shot sizes. Our future work might consider advanced methods model few-shot relations, exploiting contextual information like textual description enhance entity embeddings."," Abstractive dialogue summarization is the task of capturing the highlights of a dialogue and rewriting them into a concise version. In this paper, we present a novel multi-speaker dialogue summarizer to demonstrate how large-scale commonsense knowledge can facilitate dialogue understanding and summary generation. In detail, we consider utterance and commonsense knowledge as two different types of data and design a Dialogue Heterogeneous Graph Network  for modeling both information. Meanwhile, we also add speakers as heterogeneous nodes to facilitate information flow. Experimental results on the SAMSum dataset show that our model can outperform various methods. We also conduct zero-shot setting experiments on the Argumentative Dialogue Summary Corpus, the results show that our model can better generalized to the new domain."
"%\yy{para 1: problem important, para 2: temporal graph, existing systems, para 3: neural networks, para 4: difficult: lack training data, para 5: do} %\yy{this comment} %\yyc{before correction}{after correction} %The flow time used chain narratives, reason causes effects events, form deeper understanding past, postulate future. Temporal reasoning crucial analyzing interactions among complex events producing coherent interpretations text data . There rich body research use temporal information variety important application domains, including topic detection tracking, information extraction, parsing clinical records , discourse analysis, question answering. %\yy{Aman: Please update cites based quick Google search temporal reasoning/expressions IE/TDT/medical .} %Motivated ubiquity text understanding, undertake task extracting temporal graphs documents. %and rich understanding temporal aspects document helps humans reading comprehension. %Temporal reasoning also plays critical role downstream natural language processing tasks like Graphs natural choice representing temporal ordering among events, nodes individual events, edges capture temporal relationships ``before'', ``after'' ``simultaneous''. Representative work automated extraction graphs textual documents includes early work by~\citet{chambers2009unsupervised}, focus construction event chains collection documents, recent \caevo \cct, extract graph input document instead. These methods focus rule-based statistical sub-modules extract verb-centered events temporal relations among them. %Specifically, given document, system extracts temporal event graph, nodes graph events, edges capture temporal order~ them. %Classical temporal information extraction systems focus one two broad themes relation identification temporal relation classification. %Relation identification task identifying events connected temporal relation. %The task temporal relation involves identifying temporal relationship exists given two events. % For example, sentence I coffee I getting haircut, phrase I expresses fact events drinking coffee getting haircut took place time. %For example, given sentence I coffee I getting haircut, relation identification system would identify events coffee getting haircut. %A temporal relation classification system would determine events happened simultaneously. %Our goal create system perform tasks together end-to-end fashion multiple sentences. %The idea extracting temporal graphs given document new. %Tempeval-3 introduced task specifically end. %The idea extracting events temporal links graph proposed Tempeval-3. %However, evaluation still relied set pre-identified events TimeBank corpus, leading teams focus relation classification. %Despite importance, task received limited attention. %Representative temporal graph extraction systems like \caevo \cct break problem sub-tasks, like event identification relation extraction, employ rule-based statistical systems solve sub-task. %Additionally, use small amounts hand-labeled corpora development, limiting generalizability scalability. As emerging area nlp, large scale pre-trained language models made strides addressing challenging tasks like commonsense knowledge graph completion task-oriented dialog generation. %Besides relying intricate arrangement sub-systems, common shortcomings: i) They either admit lot noisy events ignore events secondary narrative , ii) generate one-word verbs events, without adding context, iii) limited generalization capabilities way relying rules small training corpora. % These systems typically fine-tune large language models like gpt \gptz corpus task-specific dataset. These systems typically fine-tune large language models corpus task-specific dataset. %However, advances benefited temporal graph extraction. However, techniques investigated temporal graph extraction. This paper focuses problem generation event-level temporal graph document, refer task contextualized graph generation. We address open challenge proposing novel reformulation task sequence-to-sequence mapping problem, enables us leverage large pre-trained models task. Further, proposed approach completely end-to-end eliminates need pipeline sub-systems commonly used traditional methods. %This helps approach end-to-end, easier implement, approach prevents error propagation across stages minimizes effort required feature engineering. We also address related open challenge, prerequisite main goal: difficulty obtaining large quantity training graphs human-annotated events temporal relations. %We address second challenge unsupervised approach, i.e., % To end, automatically produce large collection document-graph pairs applying existing information extraction \nlp tools textual documents, followed rule-based post-processing steps pruning noise reduction. Specifically, using \caevo tools, generate large collection 89,000 document/graph pairs. To end, automatically produce large collection document-graph pairs using \caevo, followed rule-based post-processing steps pruning noise reduction. %Specifically, using \caevo tools, generate large collection 89,000 document/graph pairs. %. %which facilitates large-scale fine-tuning well large-scale evaluation new approach comparison competing methods.} %The primary block union remains data-hungry nature large language models; typically require sizeable datasets effective training, popular temporal corpora usually offer tens hundreds hand-labeled documents. %Given large scale pre-trained language models %Despite importance, temporal graph extraction benefited recent advances large scale pre-trained language models, effective %The limitation lie representative capabilities. %Rather, lack training data forms %The lack training data forms biggest bottleneck unification: large scale language models typically require large datasets effective training. Simultaneously, popular temporal corpora usually tens hundreds documents. % bridge gap generating large corpus 89k document-graph pairs task. %We achieve first using \caevo cheap supervision mechanism creating large corpus dense temporal graphs. %Admittedly, data generated \caevo considerable amounts error noise. %We alleviate issues injecting human knowledge \caevo generated data applying several post-processing strategies. % Specifically, remove noisy events relations extracted low confidence, use event clusters map graph correct context. We encode graph training pair string graph representation format \dotlang, transforming text-to-graph mapping sequence-to-sequence mapping. %task. We fine-tune \gptz dataset document-graph pairs, yields large performance gains strong baselines system generated test set outperforms \caevo TimeBank-Dense multiple metrics. Figure 1 shows example input document generated graph system. %While automatic labeling cannot rival human-curation quality, strong experimental results show dataset prepared method provides competitive signal noise ratio virtually zero cost. %allowing strong learners generalize unseen data. %and use masked-language modeling \gptz estimating conditional distribution temporal graphs given document. %Our experiments \gptz show large gains strong baselines dataset outperforms \caevo TimeBank-Dense range metrics. %In process, answer several practical questions selecting salient events, identifying context temporal graph, graph representation, evaluation. %The system trained strong results data outperforming \caevo TimeBank-Dense range metrics. %TODO first %Qualitative analysis nodes generated method shows approach successfully use large training corpus learning generalized patterns temporal relations, error analysis held-out set revealing \caevo fixes labels 10\% cases. % We use \caevo label large corpus documents apply novel pruning techniques top graphs generated \caevo. % These pruning techniques retain high confidence annotations \caevo removing noisy events relations. % Further, context graph automatically discovered using notion event communities, obviating need hardcoded cutoffs typically adopted temporal systems. In summary, main contributions are: %\am{write three contributions: i) annotation pipeline, ii) encoding strings, thus allowing use gpt, iii) strong results data, good result \tbden, dramatic improvements off-the-shelf \gptz} % % File acl2020.tex % %% Based style files ACL 2020, %% Based style files ACL 2018, NAACL 2018/19, %% Based style files ACL-2015, improvements %% taken NAACL-2016 style %% Based style files ACL-2014, were, turn, %% based ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009, %% EACL-2009, IJCNLP-2008... %% Based style files EACL 2006 %%e.agirre@ehu.es Sergi.Balari@uab.es %% ACL 08 Joakim Nivre Noah Smith \documentclass[11pt,a4paper]{article} \usepackage[hyperref]{acl2020} \usepackage{times} \usepackage{latexsym} \renewcommand{\UrlFont}{\ttfamily\small} \usepackage{color} \usepackage{xspace} \usepackage{amsmath} \usepackage{amsfonts} \usepackage{multirow} \usepackage[multiple]{footmisc} \usepackage{array, booktabs, makecell} \usepackage{graphicx} \usepackage{colortbl} \usepackage{xcolor} \setlength{\textfloatsep}{0.1cm} % This strictly necessary, may commented out, % improve layout manuscript, % typically save space. \usepackage{microtype} %\aclfinalcopy % Uncomment line final submission %\def\aclpaperid{***} % Enter acl Paper ID %\setlength\titlebox{5cm} % You expand titlebox need extra space % show authors. Please make titlebox % smaller 5cm ; check % camera-ready version ask change back. \newcommand\BibTeX{Bib\TeX} \title{Neural Language Modeling Contextualized Temporal Graph Generation} \aclfinalcopy \author{Aman Madaan, Yiming Yang \\ Language Technologies Institute, Carnegie Mellon University \\ Pittsburgh, PA, USA \\ \\} \date{} In paper, improve abstractive dialogue summarization incorporating commonsense knowledge. We first construct heterogeneous dialogue graph introducing knowledge large-scale commonsense knowledge base. Then present Dialogue Heterogeneous Graph Network task viewing utterances, knowledge speakers graph heterogeneous nodes. We additionally design two modules named message fusion node embedding facilitate information flow. Experiments SAMSum dataset show effectiveness model outperform various methods. Zero-shot setting experiments Argumentative Dialogue Summary Corpus show model better generalized new domain."," This paper presents the first study on using large-scale pre-trained language models for automated generation of an event-level temporal graph for a document. Despite the huge success of neural pre-training methods in NLP tasks, its potential for temporal reasoning over event graphs has not been sufficiently explored. Part of the reason is the difficulty in obtaining large training corpora with human-annotated events and temporal links. We address this challenge by using existing IE/NLP tools to automatically generate a large quantity  of system-produced document-graph pairs, and propose a novel formulation of the contextualized graph generation problem as a sequence-to-sequence mapping task. These strategies enable us to leverage and fine-tune pre-trained language models on the system-induced training data for the graph generation task. Our experiments show that our approach is highly effective in generating structurally and semantically valid graphs. Further, evaluation on a challenging hand-labeled, out-domain corpus shows that our method outperforms the closest existing method by a large margin on several metrics.\footnote{Code and pre-trained models available at}"
"Building dialog systems typically requires large collection conversation logs model use training data. Crowd-sourcing popular method generating data-sets depending aspect dialog modeling studied, crowd-sourced workers may asked annotate existing chat logs intents dialog acts, create dialog summaries, converse based script converse accomplish tasks goals etc. For instance, create datasets task oriented dialogs, crowd-sourced workers may provided goal describes task needs accomplished; workers play roles user agent generate conversations . % plays role user. The user worker begins conversation stating requirement agent worker provides information user querying knowledge base , required. Together, two workers interact via natural language generate conversations involve booking restaurant tables, making train reservations, calling taxi etc. However, creating large crowd-sourced datasets time consuming expensive. Current methods generating event-level temporal graphs focused use existing Information Extraction trained relatively small amounts hand-labeled data. Current methods generating event-level temporal graphs developed relatively small amounts hand-labeled data. On hand, possibility using pre-trained neural sequence-to-sequence models task received sufficient attention, primarily due difficulty obtaining large corpora human-annotated graphs temporal reasoning events. On hand, possibility using pre-trained language models task received sufficient attention, primarily due difficulty obtaining large corpora human-annotated graphs temporal reasoning events. The possibility using pre-trained language models generating event-level temporal graphs received sufficient attention. This primarily due difficulty obtaining large corpora human-annotated graphs temporal reasoning events. This paper addresses open challenge first developing data generation pipeline uses existing //clustering techniques automated acquisition large corpus document-graph pairs, proposing new formulation graph generation task sequence-to-sequence mapping task, allowing us leverage fine-tune pre-trained language models goal. \am{note later: break two sentences/points} Our experiments strongly support effectiveness proposed approach, significantly outperforms strong baselines, represents transitional IE approaches. Our experiments strongly support effectiveness proposed approach, significantly outperforms strong baselines, including traditional techniques. We experiment \gptz show achieves large gains strong baselines system created test set outperforms \caevo several metrics hand-labeled, out-of-domain dataset. We plan explore techniques adapting large-scale language models unseen domains multiple granularity levels future. As exciting extension work, plan measure efficacy temporal graphs downstream tasks like narrative extraction generation temporal graphs different granularity levels."," Popular task-oriented dialog data sets such as MultiWOZ \cite{Multiwoz} are created by providing crowd-sourced workers a goal instruction, expressed in natural language, that describes the task to be accomplished. Crowd-sourced workers play the role of a user and an agent to generate dialogs to accomplish tasks involving booking restaurant tables, making train reservations, calling a taxi etc. However, creating large crowd-sourced datasets can be time consuming and expensive. To reduce the cost associated with generating such dialog datasets, recent work has explored methods to automatically create larger datasets from small samples.  %Popular dialog data sets such as MultiWoz \cite{Multiwoz} are created by providing crowd-sourced workers a goal instruction, expressed in natural language, which described the task that needed to be accomplished. Crowd-sourced workers played the role of a user and an agent to generate dialogs that can involve booking restaurant tables, making train reservations, calling a taxi etc.  In this paper, we present a data creation strategy that uses the pre-trained language model, GPT2 \cite{GPT2}, to simulate the interaction between crowd-sourced workers by creating a user bot and an agent bot.  We train the simulators using a smaller percentage of actual crowd-generated conversations and their corresponding goal instructions. We demonstrate that by using the simulated data, we achieve significant improvements in both low-resource setting as well as in overall task performance. To the best of our knowledge we are the first to present a model %this is the first model proposed  for generating entire conversations by simulating the crowd-sourced data collection process. %To the best of our knowledge we are the first to use inter-bot conversation logs to improve the performance of task oriented dialog systems."
"Multilingual machine translation , serve multiple language pairs single model, attracted much attention. In contrast bilingual MT systems serve one single language pair, multilingual models serve language pairs . The amount available training data differ lot across language pairs majority available MT training data English-centric practice means non-English language pairs see single training example training multilingual models . As consequence, actual performance language pairs include English source target side lags behind ones large amounts training data. Further, increasing number languages, gets impractical gather training data language pair challenging find right mix training. Which models tasked direct translation non-English pairs either resort bridging pivot language , make use synthetic parallel data study problem zero-shot settings . In study, make use potential pre-existing multi-way property training corpora generate many direct training examples pre-existing English-centric training data. If find training examples language pair multilingual mix, call model complete Multilingual Neural Machine Translation . cMNMT trained bilingual pairs source target languages utilizing multi-way aligned training examples consist translations sentence multiple languages. We resurface multi-way aligned training examples aligning training examples different language pairs either source target sides identical . To make use data, model samples source target language set multi-way aligned corpus training, allows model see language pairs originally training data existed . As experiments support, method enables us get access training data tested language pairs ). We show possible generate complete graph least 6-language WMT setup. Some WMT training data multi-way parallel construction. Nevertheless, show also find many training examples source target origin different sources. We show 112 languages internal dataset, find sufficient training data 12,000 language pairs providing 111 English-centric training corpora. This result indicates possible generate direct training data many language pairs without need crawling new training examples. Our experiments suggest falling back methods like zero-shot translation, investigate structure pre-existing training data. To address problem finding right mix examples different language pairs training, introduce hierarchical sampling strategy language-specific . In addition fixing chronic issues MNMT , proposed sampling strategy efficiently ensures source-target pairs covered. Experiments demonstrate train cMNMT model 30-language-pair WMT setup outperforms bilingual multilingual baselines well bridging non-English language pairs. We show performance English language pairs stay stable suffer changes training data new training data sampling strategy. Furthermore, share experiments scale demonstrating train cMNMT model serve 12,432 language pairs. Our contribution three-fold: In paper, demonstrated dialog generation framework mimics data creation process employed crowd-sourced workers. We find method able generate meaningful conversations aids training end-task dialog models both, low resource full data settings. The use additional simulated data train end-task dialog models result performance improvement 18-25\ low resource settings, combined full training data, find performance simple GPT2 based end-task model becomes comparable current state-of-the-art models. The simulation-framework make strict assumptions domain dataset would interesting explore use dialogue tasks Persona-Chat future work. wish explore future work. We include qualitiatve results demonstrating se"," Multilingual Neural Machine Translation  models are commonly trained on a joint set of bilingual corpora which is acutely English-centric . While direct data between two languages that are non-English is explicitly available at times, its use is not common. In this paper, we first take a step back and look at the commonly used bilingual corpora , and resurface the existence and importance of implicit structure that existed in it: multi-way alignment across examples . We set out to study the use of multi-way aligned examples to enrich the original English-centric parallel corpora. We reintroduce this direct parallel data from multi-way aligned corpora between all source and target languages. By doing so, the English-centric graph expands into a complete graph, every language pair being connected. We call MNMT with such connectivity pattern complete Multilingual Neural Machine Translation  and demonstrate its utility and efficacy with a series of experiments and analysis. In combination with a novel training data sampling strategy that is conditioned on the target language only, cMNMT yields competitive translation quality for all language pairs. We further study the size effect of multi-way aligned data, its transfer learning capabilities and how it eases adding a new language in MNMT. Finally, we stress test cMNMT at scale and demonstrate that we can train a cMNMT model with up to 111$*$112=12,432 language pairs that provides competitive translation quality for all language pairs."
"Machine Translation shown impressive progress recent years. Neural architectures greatly contributed improvement, especially languages abundant training data. This progress creates novel challenges evaluation machine translation, human automated evaluation protocols. Both types evaluation play important role machine translation. While human evaluations provide gold standard evaluation, involve fair amount careful hence expensive work human assessors. Cost therefore limits scale application. On hand, automated evaluations much less expensive. They typically involve human labor collecting human reference translations hence run scale compare wide range systems validate design decisions. The value automatic evaluations therefore resides capacity used proxy human evaluations large scale comparisons system development. The recent progress MT raised concerns whether automated evaluation methodologies reliably reflect human ratings high accuracy ranges. In particular, observed best systems according humans might fare less well automated metrics. Most metrics \BLEU TER measure overlap system output human reference translation. More refined ways compute overlap consequently proposed. Orthogonal work building improved metrics, hypothesized human references also important factor reliability automated evaluations. In particular, observed standard references exhibit simple, monotonic language due human `translationese` effects. These standard references might favor systems excel reproducing effects, independent underlying translation quality. They showed better correlation human automated evaluations could obtained replacing standard references paraphrased references, even still using surface overlap metrics BLEU~. The novel references, collected asking linguists paraphrase standard references, shown steer evaluation away rewarding translation artifacts. This improves assessment alternative, equally good translations. Our work builds success paraphrased translations evaluating existing systems, asks different design choices could made designing system evaluation protocol mind. This examination several potential benefits: help identify choices improve BLEU standard references limited impact final human evaluations; result better translations human reader, worse terms standard reference BLEU. Conversely, might turn paraphrased references robust enough support system development due presence `metric honeypots': settings produce poor translations, nevertheless assigned high BLEU scores. To address points, revisit major design choices best EnglishGerman system WMT2019 step-by-step, measure impact standard reference BLEU well paraphrased BLEU. This allows us measure extent steps data cleaning, back-translation, fine-tuning, ensemble decoding reranking benefit standard reference BLEU paraphrase BLEU. Revisiting development choices two metrics results two systems quite different behaviors. We conduct human evaluation adequacy fluency assess overall impact designing system using paraphrased BLEU. Our main findings show optimizing paraphrased BLEU advantageous human evaluation compared identical system optimized standard BLEU. The system optimized paraphrased BLEU significantly improves WMT newstest19 adequacy ratings fluency ratings despite scoring 5 BLEU points lower standard references. In work, introduced complete Multilingual Neural Machine Translation exploits multi-way alignment information underlying training data improve translation quality language pairs training data scared available. Standard MNMT models trained joint set different training corpora variety language pairs. cMNMT combines different corpora constructs multi-way aligned training examples consist translations sentence multiple languages. In combination novel temperature-based sampling approach conditioned target language only, show cMNMT superior standard MNMT model even better-performing bridging approach. Experimental results public WMT 30 language pairs dataset in-house 12,432 language pairs dataset demonstrated average BLEU increase 10 BLEU points non-English language pairs. This approach leads single NMT model serve 12,432k language pairs reasonable quality also surpasses translation quality bridging approach, nowadays used modern MT services.","  Automatic evaluation comparing candidate translations to human-generated paraphrases of reference translations has recently been proposed by \newcite{freitag2020bleu}. When used in place of original references, the paraphrased versions produce metric scores that correlate better with human judgment. This effect holds for a variety of different automatic metrics, and tends to favor natural formulations over more literal  ones. In this paper we compare the results of performing end-to-end system development using standard and paraphrased references. With state-of-the-art English-German NMT components, we show that tuning to paraphrased references produces a system that is significantly better according to human judgment, but 5 BLEU points worse when tested on standard references. Our work confirms the finding that paraphrased references yield metric scores that correlate better with human judgment, and demonstrates for the first time that using these scores for system development can lead to significant improvements."
"% Demonstrating intelligent behavior complex environments requires agents reason entities relationships, identify regularities structured data help predict properties-of relationships-between entities. % Understanding natural language realistic settings requires models reason interactions content context, model dependencies different textual elements leverage information authors interpreting content. For example, analyzing interactions social network, leveraging information users' social behavior help identify similarities contents posts author. Dealing type relational data requires making predictions multiple, often inter-dependent, variables. Understanding natural language interactions realistic settings requires models deal noisy textual inputs, reason dependencies different textual elements leverage dependencies textual content context emerges. Work linguistics anthropology defined context frame surrounds focal communicative event provides resources interpretation . %\citealt{contextualization-92} introduced term contextualization cues signalling mechanisms communication add shared understanding participants, relationships, situation, environment conversation %Say something debate networks add references. As motivating example, consider interactions debate network described Fig.. Given debate claim , two consecutive posts debating , define textual inference task, determining whether pair text elements hold stance debate }). This task similar textual inference tasks successfully approached using complex neural representations. In addition, leverage dependencies decisions. For example, assuming one post agrees debate claim }}), one }}), disagreement two posts inferred: {\small \PRED{\neg Agree\wedge Agree \rightarrow \neg Agree}}. Finally, consider social context text. The disagreement posts reflect difference perspectives authors hold issue. While information might directly observed, inferred using authors' social interactions behavior. % Given principle social homophily, stating people strong social ties likely hold similar views authors' perspectives captured representing social interactions. Exploiting information requires models align social representation linguistic one. Motivated challenges, introduce \DRAIL, Deep Relational Learning framework, uses combined neuro-symbolic representation modeling interaction multiple decisions relational domains. Similar neuro-symbolic approaches goal exploit complementary strengths two modeling paradigms. Symbolic representations, used logic-based systems probabilistic graphical models, interpretable, allow domain experts directly inject knowledge constrain learning problem. Neural models capture dependencies using network architecture better equipped deal noisy data, text. However, often difficult interpret constrain according domain knowledge. Our main design goal \DRAIL provide generalized tool, specifically designed NLP tasks. Existing approaches designed classic relational learning tasks, knowledge graph completion, equipped deal complex linguistic input. While others designed specific NLP settings word-based quantitative reasoning problems aligning images text. We discuss differences \DRAIL approaches Section. % While examples paper focus modelings various argumentation mining tasks social political context, principles applied wide array NLP tasks different contextualizing information, images appear next text, prosody analyzing transcribed speech, name examples. %TODO: explain DRAIL specifically useful NLP compared languages. We type evaluation interested working raw entities. % Entities \DRAIL either human-interpretable discrete entities , refer symbols, raw entities complex internal structure cannot easily represented symbol . This view allows us define two conceptual learning tasks: relations connecting raw symbolic entities }}), relations connecting raw inputs other, define inference tasks }}). % \DRAIL uses declarative language defining deep relational models. Similar declarative languages, allows users inject knowledge specifying dependencies decisions using first-order logic rules, later compiled factor graph neural potentials. % In addition probabilistic inference, \DRAIL also models dependencies using distributed knowledge representation, denoted \relnets, provides shared representation space entities relations, trained using relational multi-task learning approach. This provides mechanism explaining symbols, aligning representations different modalities. %Introduce s-s, r-r, s-r, distinction way support classification, textual inference, probabilistic inference. Following running example, ideological standpoints, \PRED{Liberal} \PRED{Conservative}, discrete entities embedded space textual entities social entities. These entities initially associated users, however using \relnets information propagate texts reflecting ideologies, exploiting relations bridge social linguistic information . % In resulting shared embedding space, explain ideological standpoints terms users holding them, texts express them.%}). %TODO: research questions %TODO - explain difference task DRAIL's perspective - argument relations inside single text, analyzing discussions - simple case, discussed literature, predict symbol , debate.org setup combine textual inference soclia linfo To demonstrate \DRAIL's modeling approach, introduce task open-domain stance prediction social context, combines social networks analysis textual inference complex opinionated texts, shown Fig. . %Unlike traditional stance prediction tasks, prediction problem defined fixed set issues ~ , go beyond coarse-grained definitions, delve specific arguments questions discussion, shown Fig. . We follow intuition debates part broader online conversation, involving multiple people contribute express support different views, explicitly model interactions. % AugensteinD16-1084,P18-2123,C18-1316} %TODO: add discussion qualitative evaluation % We complement evaluation \DRAIL two additional tasks, issue-specific stance prediction, identify views expressed debate forums respect set fixed issues, argumentation mining, document-level discourse analysis task. %We demonstrate \DRAIL's modeling approach three challenging problems. Argumentation mining, document-level discourse analysis task. Debate stance prediction, identifying views expressed-in, interactions-between, debate forum posts. Finally, introduce new problem, open-domain stance prediction social context, combines social networks analysis textual inference complex opinionated texts. In three tasks evaluate different modeling choices, obtaining competitive results. %TODO: contributions %Our contributions summarized follows: % % %Unrealted TODO: add discussion globally normalized RELNETs- constraints multiple objectives shape them. %homophily, %, This phenomenon previously used help overcome language variation issues % political-social representations %network embedding:we learn graph embedding, different way define social context %graphical models way In paper, present Bi-directional Cognitive Thinking Network corresponding Bi-directional Cognitive Knowledge Framework perspective psychology. The BCTN answers question bi-directional knowledge simulating inertial thinking reverse thinking. And decouple two parts knowledge rather couple module. bi-directional way thinking stemmed cognitive psychology. To determine stimulus intensity reverse thinking memory, consider decoded tokens calculate score based gate mechanism. We show proposed BCTN effective, competitiveness previous methods literature DuReader single model. Our future work consider use different datasets design various models simulate behavior brain capture human-level language understanding intelligence. Finally, believe framework generalize generative tasks, summarization image caption."," Building models for realistic natural language tasks requires dealing with long texts and accounting for complicated structural dependencies. Neural-symbolic representations have emerged as a way to combine the reasoning capabilities of symbolic methods, with the expressiveness of neural networks. However, most of the existing frameworks for combining neural and symbolic representations have been designed for classic relational learning tasks that work over a universe of symbolic entities and relations. In this paper, we present \DRAIL, an open-source declarative framework for specifying deep relational models, designed to support a variety of NLP scenarios. Our framework supports easy integration with expressive language encoders, and provides an interface to study the interactions between representation, inference and learning."
"End-to-end neural models emerged recent years dominant approach wide variety sequence generation tasks natural language processing, including speech recognition, machine translation, dialog generation, among many others. While highly accurate, models typically operate outputting tokens predetermined symbolic vocabulary, require integration larger pipelines use user-facing applications voice assistants neither input output modality text. In speech domain, neural methods recently successfully applied end-to-end speech translation , goal translate directly speech one language speech another language. We propose study analogous problem in-image machine translation. Specifically, image containing text one language transformed image containing text another language, removing dependency predetermined symbolic vocabulary processing. \paragraph{Why In-Image Neural Machine Translation ?} In-image neural machine translation compelling test-bed research engineering communities variety reasons. Although existing commercial products address problem image translation feature Google Translate underlying technical solutions unknown. By leveraging large amounts data compute, end-to-end neural system could potentially improve overall quality pipelined approaches image translation. \iffalse First, existing commercial products address problem image translation feature Google Translate employ traditional pipelined approach consisting separate optical character recognition, translation, image rendering steps.\todo{orhanf check mobile team. Elman: commented suggested mobile wordlens team. technical solution wordlens publicly available hence sentence bit speculative} Combining components single end-to-end neural system could help reduce cascading errors improve overall translation quality, leveraging large amounts data compute. \fi Second, arguably importantly, working directly pixels potential sidestep issues related vocabularies, segmentation, tokenization, allowing possibility universal approaches neural machine translation, unifying input output spaces via pixels. Text preprocessing vocabulary construction active research area leading work investigating neural machine translation systems operating subword units , characters even bytes highlighted one major challenges dealing many languages simultaneously multilingual machine translation , cross-lingual natural language understanding . Pixels serve straightforward way share vocabulary among languages expense significantly harder learning task underlying models. In work, propose end-to-end neural approach in-image machine translation combines elements recent neural approaches relevant sub-tasks end-to-end differentiable manner. We provide initial problem definition demonstrate promising first qualitative results using pixel-level supervision target side. We analyze errors made models, process uncover common deficiency suggests path forward future work. In paper, motivate need declarative neural-symbolic approach applied NLP tasks involving long texts contextualizing information. We introduce general framework support this, demonstrate flexibility modeling problems diverse relations rich representations, obtain models easy interpret expand. Going forward, would like study distant supervision, support learning latent predicates, reason relations properties directly observed. The code, data documentation \DRAIL application examples paper released community, help promote modeling approach applications. We discuss evaluate different modeling choices deep relational learning. We characterize strengths weaknesses symbolic neural approaches, suggest framework combining them. We demonstrate framework's flexibility modeling problems diverse relations rich representations, allowing us focus abstractions needed understand relevant dependencies task. \DRAIL designed streamline process help alleviate reproducibility challenges. Going forward, continue explore ways leverage representation, inference learning, gain insights challenges opportunities hybrid approaches present us."," In this paper, we offer a preliminary investigation into the task of in-image machine translation: transforming an image containing text in one language into an image containing the same text in another language. We propose an end-to-end neural model for this task inspired by recent approaches to neural machine translation, and demonstrate promising initial results based purely on pixel-level supervision. We then offer a quantitative and qualitative evaluation of our system outputs and discuss some common failure modes. Finally, we conclude with directions for future work."
"Transformer based models proven effective building state-of-the-art Neural Machine Translation systems via neural networks attention mechanism . Following standard Sequence-to-Sequence architecture, Transformer models consist two essential components, namely encoder decoder, rely stacking several identical layers, i.e., multihead attentions position-wise feed-forward network. Multihead attentions position-wise feed-forward network, together basic unit, plays essential role success Transformer models. Some researchers propose improve model capacity stacking basic unit many times, i.e., deep Transformers, achieve promising results. Nevertheless, orthogonal direction, investigation multiple parallel units draws little attention. Compared single unit models, multiple parallel unit layout expressive capture complex information flow two aspects. First, multiple-unit layout boosts model varied feature space composition different attentions inputs. With diversity, multi-unit models advance expressiveness. Second, multi-unit setting, one unit could mitigate deficiency units compose expressive network, complementary way. In paper, propose Multi-Unit TransformErs , aim promote expressiveness transformer models introducing diverse complementary parallel units. Merely combining multiple identical units parallel improves model capability diversity varied feature compositions. Furthermore, inspired well-studied bagging gradient boosting algorithms machine learning field, design biased units sequential dependency boost model performance. Specifically, help module named bias module, apply different kinds noises form biased inputs corresponding units. By so, explicitly establish information gaps among units guide learn other. Moreover, better leverage power complementariness, introduce sequential ordering multi-unit setting, % learning permutaion matrix automatically shuffle outputs multiple units, force unit learn residual preceding accumulation. We evaluate methods three widely used Neural Machine Translation datasets, NIST Chinese-English, WMT'14 English-German WMT'18 Chinese-English. Experimental results show multi-unit model yields improvement +1.52, +1.90 +1.10 BLEU points, baseline model three tasks different sizes, respectively. Our model even outperforms Transformer-Big WMT'14 English-German 0.7 BLEU points 54\% parameters. Moreover, interesting side effect, model introduces mild inference speed decrease compared Transformer-Base model, faster Transformer-Big model. % proves practicability methods. The contributions paper threefold: In work\footnotemark\footnotetext{ .}, explored different ways trained models applied improve AMR parsing performance via self-learning. Despite recent strong improvements performance novel architectures, show proposed techniques improve performance further, achieving new state-of-the-art AMR 1.0 AMR 2.0 tasks without need extra human annotations. uncomment redo bbl","     Transformer models \cite{vaswani2017attention} achieve remarkable success in Neural Machine Translation.      Many efforts have been devoted to deepening the Transformer by stacking several units  in a cascade,      while the investigation over multiple parallel units draws little attention.     In this paper, we propose the Multi-Unit TransformErs , which aim to promote the expressiveness of the Transformer by introducing diverse and complementary units.     Specifically, we use several parallel units and show that modeling with multiple units improves model performance and introduces diversity.      Further, to better leverage the advantage of the multi-unit setting, we design biased module and sequential dependency that guide and encourage complementariness among different units.      % need more results and exciting data.     Experimental results on three machine translation tasks, the NIST Chinese-to-English, WMT'14 English-to-German and WMT'18 Chinese-to-English, show that the MUTE models significantly outperform the Transformer-Base, by up to +1.52, +1.90 and +1.10 BLEU points, with only a mild drop in inference speed .      In addition, our methods also surpass the Transformer-Big model, with only 54\% of its parameters. These results demonstrate the effectiveness of the MUTE, as well as its efficiency in both the inference process and parameter usage. \footnote{Code is available at \url{https://github.com/ElliottYan/Multi\_Unit\_Transformer}}"
"% % Prior work primarily focused exploiting visual patterns using carefully crafted features . These rendering-based methods two major drawbacks: 1) expensive since require downloading external files including CSS, javascript, images render page compute visual features; 2) require carefully crafted heuristics around visual proximity work well expensive features. In paper, propose novel two-stage neural architecture, named FreeDOM, trained small number seed websites generalize well unseen websites without requiring hand-engineered visual features. %we want employ neural networks learning transferable visual features eliminate need rendering human engagement crafting textual patterns. %We propose novel two-stage neural architecture directly learn annotated websites based raw HTML content transfer models unseen websites without using human labels . %We parse HTML documents DOM Trees page classifies one target fields. This node-level module combines neighboring character sequences, token sequences, well markup learn combined representation node. We propose combination CNNs LSTMs show effectively encode useful features DOM nodes. These node representations encoded individually inevitably lose global information useful extraction task. In particular, relying local node features cause failure value nodes obvious patterns local features similar non-value nodes. To mimic signal may available visual features used rendering-based methods, use relational neural network second module . This allows us model relationship pair elements using distance-based semantic features. The rationale behind learn global representations node pairs jointly predict node labels instead relying local features. Extensive experimental results large-scale public dataset, Structured Web Data Extraction corpus, show model consistently outperforms competitive baseline methods large margin. The proposed FreeDOM able generalize unseen sites training small number seed sites. In fact, show training data three seed sites, approach out-performs techniques use explicit visual rendering features 3.7 F1 points average. To best knowledge, framework among first neural architectures efficiently obtains high-quality representations web documents structured information extraction. \eat{Our framework utilizes minimal human efforts feature engineering require rendering results, thus making large-scale information extraction web documents much easier effort-light. We believe proposed model promising applications require neural representations web documents.} %The node-level module predict node labels identifying values interested fields, encoded local features cannot capture long-range dependencies values thus degenerate unlabeled target websites. %To address problem, propose relational neural network. %As second-stage module, explicitly models relations DOM nodes effectively learns page-level constraints producing structured predictions. % models relational features reflected node pairs, finally conducts structured data extraction structured prediction problem. %Our contributions paper three-fold: %% %} %Our contribution propose novel neural model, FreeDom, structured data extraction web documents using less information hand-crafted features. Extensive experiments large-scale public data set show proposed FreeDom outperforms strong baseline methods using raw features. %%ying{The last sentence looks complete. \yuchen{Done.}} %\tata{Don't say 'less information' emphasize requiring visual rendering cheaper requiring hand-crafted features means generalize new tasks better. Need make claim focused contributions clear. We also need spell two stages clearly 'First stage blah', 'Second stage blah'} %\tata{Might worth adding entity resolution scope work -- ie, might extract duplicate entries across websites car. There many papers dealing we're focused paper.} % In paper, propose Multi-Unit Transformers NMT improve expressiveness introducing diverse complementary units. In addition, propose two novel techniques, namely bias module sequential dependency improve diversity complementariness among units. We show merely integrate several identical units improve model performance diversity. Furthermore, introduce Biased Multi-Unit Sequentially Biased Multi-Unit towards explicit guidance interaction units. We evaluate methods two widely used NMT datasets. Experimental results show methods significantly outperform baseline methods achieve comparable / better performance compared existing strong NMT systems. In meantime, methods use much fewer parameters introduce mild inference speed degradation, proves efficiency models. \clearpage"," % tata: Jan 26 rewrite of Abstract.  Extracting structured data from HTML documents is a long-studied problem with a broad range of applications like augmenting knowledge bases, supporting faceted search, and providing domain-specific experiences for key verticals like shopping and movies. Previous approaches have either required a small number of examples for each target site or relied on carefully handcrafted heuristics built over visual renderings of websites. In this paper, we present a novel two-stage neural approach, named FreeDOM, which overcomes both these limitations.  The first stage learns a representation for each DOM node in the page by combining both the text and markup information. The second stage captures longer range distance  and semantic relatedness using a relational neural network. By combining these stages, FreeDOM is able to generalize to unseen sites after training on a small number of seed sites from that vertical without requiring expensive hand-crafted features over visual renderings of the page. Through experiments on a public dataset with 8 different verticals, we show that FreeDOM beats the previous state of the art by nearly 3.7 F1 points on average without requiring features over rendered pages or expensive hand-crafted features. % 3.7 is from Table 2 k=3 .  % tata: Previous version of abstract follows:  %"
"Data-to-Text aims generating natural language descriptions structured data ; fostered recent advances neural approaches %for data-to-text made possible emergence large scale datasets made pairs . Figure illustrates example WikiBIO dataset . These datasets either hand-crafted via crowdworkers automatically built aligning sources found Internet. As such, %training examples imperfect reference texts might include divergences two types, limiting ability generation models produce realistic descriptions. First, reference texts might contain information grounded source data; especially automatically constructed datasets, references written source-data description task mind. For instance, phrase ``who served lieutenant [...]'' Figure basis associated infobox. Second, reference texts always cover entirety table . In settings, second point referred content selection inherent data-to-text tasks. % part normal subtask flow data-to-text. %; see example Figure information wars. %However, hand-crafted datasets designed annotators asked transcribe every fields, systems also expected same. In case, incomplete references lead models fail learn transcribe information, partially cover data-sources inference. However, hand-crafted datasets designed annotators asked transcribe every fields, models also expected same. In case, incomplete references lead models failing learn transcribe information, partially cover data-sources inference. Divergence training examples leads hallucinated/omitted content model output; well-known problem neural approaches text generation . This problem arises training procedure , testing protocols. Indeed, current standard metrics measure similarity ground truth reference texts fully capture relevance source data. %Indeed, evaluation metrics work computing precision n-grams contained generated sentence w.r.t ground truth description. Thus, distinction mismatch caused paraphrase, poor lexicalization content, made-up/incorrect statement, leading imperfect model selection. While number work argue need novel automatic evaluation method , best knowledge \citet{wiseman2017} \citet{dhingra2019} propose metrics based reference source data. %Additionally, \citet{dhingra2019} show proposed metric PARENT correlates strongly human evaluation metric, easier use box. Recently, different regularization methods also proposed mitigate negative influence divergences reference texts. These approaches either dataset level , authors propose techniques clean/standardize instances; training level , authors propose novel neural modules designed limit hallucinations/omissions. However, approaches severely limited: e.g., require significant annotation labor, model-specific tricks and/or manual tuning. Furthermore, virtually proposed neural approaches still suffer 1)~exposure bias 2)~inconsistency train/test measurement. Indeed, current neural models trained via mechanism called teacher forcing , decoder fed previous correct token, matter actual prediction~, order maximize log-likelihood target sentence , evaluated previously discussed n-gram metrics~. See Section detailed discussion subject.\\ %On one hand, controllable approaches proposed: example, \citet{Liu2019hier} train hierarchical encoder-decoder three auxiliary tasks meant guide decoding process, order achieve descriptions higher fidelity respect conditioning input. To best knowledge, approaches focused training procedure. %We cite , \citet{liu2019} train hierarchical encoder-decoder three auxiliary tasks meant guide decoding process. %, order achieve descriptions higher fidelity respect conditioning input. Closest work, \citet{Liu2019b} propose novel neural module constrained attention, along reinforcement learning training procedure based BLEU TFIDF. In work, remedy shortcomings building upon work \citet{Liu2019b}, show novel neural module necessary handle hallucinations omissions. We propose model-agnostic RL framework, called PARENTing, pretrained models trained self-critical policy gradient algorithm limit impact divergences training examples text generation. Specifically, use PARENT metric exhibits strong correlation human evaluation, easier use box. We provide extensive automatic evaluations two data-to-text model families two widely used benchmarks , well focused human evaluation %to high-light differences several training procedures WikiBIO. We report new state art PARENT scores datasets BLEU scores par previous SOTA approaches, shows framework efficiently reduces pathological behaviors keeping generation fluent. %To remedy shortcomings, propose model-agnostic reinforcement learning framework, called PARENTing, pretrained models trained self-critical policy gradient algorithm limit impact divergences training examples text generation. % inspired recent advancements text generation fields. %Specifically, %we fine-tune pretrained models self-critical policy gradient algorithm based %we use PARENT metric exhibits strong correlation human evaluation, easier use box. We provide extensive evaluations two data-to-text model families two widely used benchmarks . We report new state art PARENT scores datasets BLEU scores par previous approaches, shows framework efficiently reduces pathological behaviors keeping generation fluent. %In following, first review Section data-to-text approaches well recent attempts controlling hallucinations/omissions. We introduce Section model-agnostic framework limiting hallucinations/omissions generation. The evaluation protocol presented Section, followed obtained results . Section concludes paper presents perspectives. %In following, first present state-of-the art attempts reduce hallucinations address exposure bias inconsistencies train/test measurement data-to-text literature . revoir la structure We describe details PARENT metric \citet{dhingra2019} Section proposed RL training framework Section. The evaluation protocol presented Section, followed results . Section concludes paper presents perspectives. In paper, propose neural architecture extracting structured data web documents. It uses training data seed sites generalizes well unseen websites vertical. We show approach, , beats previous state-of-the-art performance large-scale public dataset consisting 8 different verticals nearly 3.7 F1 points. In particular, without using expensive rendering-based visual features. We also discovered typical sequence labeling techniques NLP work well task presented hypotheses case. We believe work opens multiple avenues future research web data extraction. What structured prediction techniques might work better incorporating information farther away work well large DOM trees sparse labels? An even interesting question transfer information across verticals? That is, able well one vertical, leverage information somehow train model next vertical? Will large pre-trained neural encoding model HTML documents, like BERT plain texts? We believe work also useful future research needs learn site-general neural representations semi-structured documents including web pages, pdf files on. The next two lines define bibliography style used, bibliography file. \clearpage \clearpage \endinput End file `sample-authordraft.tex'.","  %The effectiveness of language generation models conditioned by structured data is inherently due to the quality of reference texts and the training protocol. First, these reference texts often diverge from the information contained in the associated source data . Second,  In language generation models conditioned by structured data, the classical training  via maximum likelihood almost always leads  models to pick up on dataset divergence , and to incorporate them erroneously in their own generations at inference.  %In this work, we propose a model-agnostic reinforcement learning framework in order to reduce hallucinations and omissions. To do so, we rely on the recently introduced PARENT metric assessing the adequacy of a candidate generation with both the human reference and the source data.  In this work, we build ontop of previous Reinforcement Learning based approaches and show that a model-agnostic framework relying on the recently introduced PARENT metric is efficient at reducing both hallucinations and omissions. Evaluations on the widely used WikiBIO and WebNLG benchmarks demonstrate the effectiveness of this framework compared to state-of-the-art models."
"Relation classification aims identify relation two specified entities sentence. Previous supervised approaches task heavily depend human-annotated data, limit performance classifying relations insufficient instances. Therefore, making RC models capable identifying relations training instances becomes crucial challenge. Inspired success few-shot learning methods computer vision community , first introduce few-shot learning RC task propose FewRel % dataset. % dataset benchmark. Recently, many works focus task achieve remarkable performance . %distant supervision proposed automatically construct training instances RC. %However, dataset extracted distant supervision, long-tail relations instances suffer data sparsity problem. %Inspired success few-shot learning methods computer vision community, e.g., Matching Network , Relation Network Memory-augmented network , first introduce FSL RC tackle long tail problem. They use Prototypical Network , achieves state-of-the-art performance several FSL benchmarks. Recently, many works followed framework achieved remarkable performance Few-shot RC dataset FewRel . %The prototypical network learns representation relation based sampled instances, classifies queries set pre-defined relations. %\CheckedBox % Even though existing works perform well, assume one relation sentence. Previous few-shot relation classifiers perform well sentences one relation single entity pair. However, real natural language, sentence usually jointly describes multiple relations different entity pairs. Since relations usually keep high co-occurrence context, previous few-shot RC models struggle distinguish annotated instances. For example, Table shows three instances FewRel dataset, sentence describes multiple relations corresponding keyphrases highlighted evidence. When specified two entities sentence, great opportunity instance incorrectly categorized {\color{red}{confusing relation}} instead {\color{blue}{true relation}} . % % Specifically, %is different entity pairs usually described input sentence, relation classification entity pairs often interferes other. %This results entity pairs relations often misclassified confusing relations models without ability explicitly decoupling easily-confused relations. %Table shows three instances FewRel dataset , contains sentence two given entities right side, positive confusing relations left side. %Previous few-shot methods tend misclassify sentences confusing relations. first instance categorized true relation `parents-child' based given entity pair natural language expression `a daughter of'. However, since also includes NL expression `his wife', %which describes confusing relation `husband-wife', probably misclassified confusing relation `husband-wife'. In paper, name relation confusion problem. %=============================================================================================== % \verb|\checkmark|: \checkmark \par % \verb|\cmark|: \cmark \par % \verb|\xmark|: \xmark {blue}} {\color{red}{red}} words respectively correspond true confusing relations.} \end{table} %============================================================================================== To address relation confusion problem, crucial model % effectively select information high relevance given entity pair aware NL expressions cause confusion learn avoid mapping instance easily-confused relation. % To address relation confusion problem, crucial model aware NL expressions cause confusion explicitly distinguish easily-confused relations. From perspectives, propose two assumptions. Firstly, sentence, words keep high relevance given entities important expressing true relation. Intuitively, specified entity information crucial identify true relations. Secondly, explicitly learning mapping instance confusing relation augmented data turn boosts few-shot RC model identifying true relation. % allowing model explicitly learn confusing relations help identify true relations. %Intuitively, specific entities information helpful identify positive relation. Based assumptions, propose CTEG, few-shot RC model two novel mechanisms: An Entity-Guided Attention encoder, leverages syntactic relations relative positions word specified entity pair softly select important information words expressing true relation filter information causing confusion. A Confusion-Aware Training method, explicitly learns distinguish relations playing pushing-away game classifying sentence true relation confusing relation. %has ability explicitly learning distinguish easily-confused relations. In addition, inspired success pre-trained language models, approaches based BERT , proved effective especially few-shot learning tasks. %=========================================================================== % Specifically, encoding sentence attention mechanism, EGA guides calculation attention score multiply entity-aware gate. %we adopt transformer incorporating self-attention mechanism encoding input instance, Specifically, backbone encoder model transformer equipped proposed EGA guides calculation self-attention distributions weighting attention logits entity-guided gates. % The gate matrix relevance scores, used measure importance word according relevance entities. % The gates used measure importance word according relevance entities. The gates used measure relevance word given two entities. % Two types position information words used calculate scores. One relative position , relative distance word entity sentence squence. Two types information word used calculate gate. % One relative position , relative distance word entity sentence squence. One relative position information, relative distance word entity input sequence. The syntactic relation proposed paper, defined dependency relations word entities. % Besides, propose syntax position, defined dependency relations word entities. Based information, entity-guided gates EGA able select important words control contribution word self-attention. % Based information, entity-aware gate EGA able select important words control contribution word self-attention. % For proposed CAT, allows model asynchronously learn confusing relations sentence. After training step, CAT first selects misclassified sentences, regards relations misclassified confusing relations. After that, The CAT uses misclassified sentences confusing relations conduct additional training process, aimes learn confusing relations explicitly. We also propose CAT explicitly force model asynchronously learn classification instance true relation confusing relation. After training step, CAT first selects misclassified sentences, regards relations misclassified confusing relations. After that, The CAT uses misclassified instances confusing relations augmented data conduct additional training process, aims learn mapping instances confusing relations. % After that, The CAT uses misclassified sentences confusing relations conduct additional training process, aims learn confusing relations explicitly. Afterwards, CAT adopts KL divergence teach model distinguish difference true confusing relations, benefits true relation classification confusing relation identification. % Extensive experiments conducted FewRel dataset, results show proposed model achieves comparable even better results strong baselines terms accuracy. % Furthermore, ablation test case study verify effectiveness proposed EGA CAT, especially addressing relation confusion problem. The contributions paper summarized follows: We propose Entity-Guided Attention encoder, select crucial words filter NL expressions causing confusion based relevance specified entities. We propose Confusion-Aware Training process enhance model ability distinguishing true confusing relations. We conduct extensive experiments few-shot RC dataset FewRel, ans results show model achieves comparable even much better results strong baselines. Furthermore, ablation case studies verify effectiveness proposed EGA CAT, especially addressing relation confusion problem. In work, proposed model-agnostic reinforcement learning framework data-to-text aimed reducing hallucinations improving recall/coverage relevant information. We shaped reward based PARENT , recently proposed metric high correlation human judgement. number datasets. This training protocol allows flexible training, model learns depend less reference source data. Framework effectiveness assessed via thorough experiments two model family two benchmarks . In particular, showed training using proposed framework led models learn non-trivial behavior, like shortening generation pretrained models would still output content, contrary adding relevant information table missed pretrained models. Furthermore, quantitative qualitative evaluations show PARENTing framework help models reduce hallucinated omitted content. This approach obtains better results dedicated attention module less source-relying reward. . However, framework relies quality metric employed training, crafting effective metric still open problem. In particular, PARENT metric designed specifically datasets like WikiBIO WebNLG, values linguistic sequences associated single entity explicit semantic fields. This avoids possible confusion value associated field. However, metric reliable complex datasets \citet{wiseman2017,puduppully2019}. For instance, RotoWire , tables report statistics basketball games regroup several entities nature. In setting, sentence ``James Harden scored 20 points'' could achieve high PARENT score player scored points game. Therefore, information-retrieval metrics introduced \citet{wiseman2017} still metric able capture precision generated texts. However, based manually tuned pattern-matching computed using ensemble six deep neural network: specific RotoWire practice usable realistic training protocol. For future work, plan propose evaluation metric robust dataset peculiarities final objective evaluate model-agnostic framework complex challenging datasets. Once metric devised, would interesting apply framework challenging settings ask in-depth reasoning. However, approach relies metric employed crafting effective metric still open problem. In particular, PARENT designed single-entity datasets, like WikiBIO WebNLG, However, framework relies quality metric employed training, crafting effective metric still open problem. In particular, PARENT metric designed specifically datasets relating single entity explicit semantic fields, like WikiBIO WebNLG. values linguistic sequences associated single entity explicit semantic fields. This avoids possible confusion value associated field. reliable complex datasets reporting heterogeneous data and/or containing multiple entities , seen basketball games . In setting, sentence ``James Harden scored 20 points.'' could achieve high PARENT score player scored points game. Therefore, information-retrieval metrics introduced \citet{wiseman2017} still metric able capture precision generated texts. However, based manually tuned pattern-matching computed using ensemble six deep neural network: specific RotoWire practice usable realistic training protocol. An interesting future work would design evaluation metric robust dataset peculiarities. final objective evaluate model-agnostic framework complex challenging datasets. Once metric devised, would interesting apply framework challenging settings ask in-depth reasoning."," This paper aims to enhance the few-shot relation classification especially for sentences that jointly describe multiple relations. Due to the fact that some relations usually keep high co-occurrence in the same context, previous few-shot relation classifiers struggle to distinguish them with few annotated instances. To alleviate the above relation confusion problem, we propose CTEG, a model equipped with two mechanisms to learn to decouple these easily-confused relations. On the one hand, an Entity-Guided Attention  mechanism, which leverages the syntactic relations and relative positions between each word and the specified entity pair, is introduced to guide the attention to filter out information causing confusion. On the other hand, a Confusion-Aware Training  method is proposed to explicitly learn to distinguish relations by playing a pushing-away game between classifying a sentence into a true relation and its confusing relation. Extensive experiments are conducted on the FewRel dataset, and the results show that our proposed model achieves comparable and even much better results to strong baselines in terms of accuracy. Furthermore, the ablation test and case study verify the effectiveness of our proposed EGA and CAT, especially in addressing the relation confusion problem."
"% The following footnote without marker needed camera-ready % version paper. % Comment instructions uncomment 8 lines % ""final paper"" variant English. . % final paper: en-us version % % space normally used marker % This work licensed Creative Commons % Attribution 4.0 International License. % License details: % \url{http://creativecommons.org/licenses/by/4.0/}. } Complaining basic speech act, usually triggered discrepancy reality expectations towards entity event. Social media become popular platform expressing complaints online customers directly address companies regarding issues services products. Complaint detection aims identify breach expectations given text snippet. However, use implicit ironic expressions accompaniment speech acts suggestions, criticism, warnings threats make challenging task. Identifying classifying complaints automatically important for: improving customer service chatbots; linguists analyze complaint characteristics large scale ; psychologists understand behavior humans express complaints. Previous work focused binary classification complaints non-complaints various domains. Furthermore, studies performed fine-grained complaint classification. For instance, complaints directed public authorities categorized based topics responsible departments. Other categorizations based possible hazards risks well escalation likelihood. Most previous studies used supervised machine learning models features extracted text task-specific neural models trained scratch. Adapting state-of-the-art pre-trained neural language models based transformer networks BERT XLNet yet explored. In paper, focus binary classification Twitter posts complaints \shortcite{preotiuc2019automatically}. We adapt evaluate battery pre-trained transformers subsequently combine external linguistic information topics emotions. \paragraph{Contributions} New state-of-the-art results complaint identification Twitter, improving macro FI 8.0\% previous work Preotiuc-Pietro et al. \shortcite{preotiuc2019automatically}; A qualitative analysis limitations transformers predicting accurately whether given text complaint not. We show use LRP evaluate relative contributions source target NMT predictions. We illustrate potential approach analyzing changes contributions conditioning different types prefixes , varying training objectives amount training data, training process. Some findings are: ~models trained data rely source information sharp token contributions; ~the training process non-monotonic several distinct stages. These stages agree ones found previous work focused validating lottery ticket hypothesis, suggests future investigation connection. Additionally, show models suffering exposure bias prone over-relying target history ones exposure bias mitigated. In future work, methodology used measure effects different novel training regimes balance source target contributions.","    Complaining is a speech act extensively used by humans to communicate a negative inconsistency between reality and expectations. Previous work on automatically identifying complaints in social media has focused on using feature-based and task-specific neural network models. Adapting state-of-the-art pre-trained neural language models and their combinations with other linguistic information from topics or sentiment for complaint prediction has yet to be explored. In this paper, we evaluate a battery of neural models underpinned by transformer networks which we subsequently combine with linguistic information. Experiments on a publicly available data set of complaints demonstrate that our models outperform previous state-of-the-art methods by a large margin achieving a macro F1 up to 87."
"\com{ Remember recheck: intro section place clear reiterate paragraph structure: taxonomy explained validations taxonomy+classification comparison taxonomies /classifications proof usefulness various kinds analysis allows qualitative results discussion related work conclusion } Taxonomies grammatical errors important linguistic computational analysis learner language, well Grammatical Error Correction systems.\footnote{Code found \href{https://github.com/borgr/GEC_UD_divergences}{in github repo GEC\_UD\_divergences}. Matrices directly mentioned included appendix.} Such taxonomies divide complex space errors meaningful categories enable characterizing distribution learner productions. This information beneficial GEC: support development systems focus specific error types, serve form inductive bias , guide data augmentation data filtering controlling distribution error types. Error taxonomies also improve interpretability system outputs error analysis learner feedback. % % % \end{small} % \end{table} A number annotation efforts learner language developed error taxonomies , statistical classifiers taxonomies, notably ERRANT . Taking error types consideration learning also shown improve GEC performance \citep[][{cf. \S}]{kantor2019learning}. However, existing taxonomies fairly coarse-grained language specific, produce meaningful types large proportion errors. For example, 25\% errors standard NUCLE corpus mapped residual category OTHER . We propose \secl, taxonomy Syntactic Errors automatic Classification. Inspired longstanding tradition Machine Translation analyses divergences source translated texts based syntactic structure , \secl\ based divergences ungrammatical sentences corrections. We define SEs errors whose correction involves changing morphological features, POS labels syntactic structure labels. \secl\ takes input edits, i.e., grammatically incorrect text spans corrections, compares labels. For example, error Fig. adjective replaced adverb POS terms, \ra edge-label terms. Thus, SEs defined changes form, rather principles governing choice correct form. \secl\ first taxonomy derived syntactic representation framework, uses Universal Dependencies formalism \citep[UD;][]{nivre2016universal}. This approach provides three major advantages prior learner error taxonomies. First, \secl\ taxonomy derived automatically UD annotations, circumventing need constructing ad-hoc manually defined error categories. Second, using UD formalism makes method applicable across languages, allowing consistent analyses comparisons learner errors across different languages within one unified framework. Third, \secl\ compatible standard representations tools NLP. Further, UD based approach error classification yield finer distinctions compared existing schemes. For example, divides commonly used class adposition errors errors use prepositions nominal modifiers , use prepositions prepositional objects adjuncts . %prepositions involving verbal arguments errors involving spatial/temporal relations.\oa{how exactly? maybe say distinguish NP-internal PPs clause-level PPs?} \lc{One would obj something . Isn't object subject main thing syntax allows?, write another example. Note example involve type usually split . } POS tags alone cannot distinguish them, UD trees expose distinction straightforwardly. UD also help classify agreement case-assignment errors thanks morphological-feature layer containing information case, number, gender, features relevant inflection. We validate \secl's reliability showing SEs based automatic parses similar ones based manual parses. ; \secl\ types map well NUCLE's manually curated taxonomy ; \secl\ complementary standard type classifier ERRANT: 60\% errors classified ERRANT classified \secl. We demonstrate \secl's unique features, notably cross-linguistic applicability, analyzing SE distributions available corpora learner English learner Russian . Finally, find GEC systems certain SEs harder correct SEs harder non-SEs granular types help devising rules improve products . %\lc{yb, I gave try, better? Am I general?} %We validate accuracy relying parsing technology compare \secl\ manual automatic taxonomies , finding \secl\ classifies 60\% errors covered leading error classifier English ERRANT . We examine characteristics using UD features applying Russian . All findings suggest \secl\ reliable, fine-grained annotation current taxonomy classifier language specific. To show wide applicability, use \secl\ provide detailed picture distribution SEs various learner English corpora . We proceed use \secl\ detect trends error type distribution across learner levels . We conclude analyzing system outputs .\yb{many people skip paper summary paragraphs. I would instead list contributions bullet points .} % %While classifying grammatical error types . In paper focus syntactic errors, i.e., errors require changing tree structure fix, %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% In paper, proposed Token Drop mechanism neural machine translation task. Inspired self-supervised learning, introduced Replaced Token Detection Dropped Token Prediction training objective. We found NMT model trained Token Drop gains larger generalization capacity reduction overfitting. Even without prior knowledge additional parameters, proposed approach reports convincing results neural machine translation. In future work, plan investigate impact dropping different words, e.g. word importance word type."," 		We present a method for classifying syntactic errors in learner language, namely errors whose correction alters the morphosyntactic structure of a sentence. 		The methodology builds on the established Universal Dependencies syntactic representation scheme, and provides complementary information to other error-classification systems.  		Unlike existing error classification methods, our method is applicable across languages, which we showcase by producing a detailed picture of syntactic errors in learner English and learner Russian. We further demonstrate the utility of the methodology for analyzing the outputs of leading Grammatical Error Correction  systems."
"% Second, NMT model's ability handle streaming ASR output; ASR system provides best greedy recognition live speech's segmented audio. % really talk ^ %deleted simultaneous With advance Automatic Speech Recognition Neural Machine Translation systems, speech translation become increasingly feasible received considerable attention. However, researchers encountered many challenging problems within standard cascaded framework ASR system outputs passed NMT systems. First, since NMT models often trained clean, well-structured text, disfluency spoken utterances recognition errors ASR systems modeled NMT systems. Second, people speak differently write, results changes sentence structure meaning. Third, automatically predicting sentence boundaries challenging. Taken whole, poorly segmented sentences incorrect word recognition leads poor translations. These problems pose unique challenges ASR NMT robustness readily addressed current methods. % %\subsection{Related Work} % Current approaches robust NMT noisy inputs typically focus improving word transcription data augmentation techniques. Such methods include disfluency removal redundant unnecessary words removed translating transcript, domain adaptation NMT models augmented in-domain training data, synthetic noise, random edits made training data. % % \footnotetext{When compared Table , sum System/System transcript segmentation degradation surpasses Gold/Gold evaluation 0.57 BLEU points. This modifications evaluation data directly additive, ie .} % %\subsection{Contributions} % % Although data domain segmentation issues often tackled separately, find compounded effects, namely erroneous transcript sentence boundaries, neglected substantially detrimental final translation quality. % As such, contributions two fold, analyze impact noisy ASR segmentations translation propose easily adaptable simple data augmentation strategy increase NMT robustness. % sentence optional In experimentation, found ASR system punctuation often imperfect. It may omit insert sentence-final punctuation, resulting sentences erroneously compounded fragmented. While corroborated similar works, note degradation translation caused poor system sentence boundary prediction, specifically evaluate, quantify, address issue. % mention much sentence boundaries degrade accuracy % Find example? To tackle sentence boundary problem, propose simple scheme augment NMT training data, yields +1 BLEU point average. % Similar , first ascertain NMT model implicitly learn target punctuation unpunctuated source text, even noisy imperfect sentence boundaries. % sentence necessary?^ %We show simple data augmentation scheme general in-domain NMT training data, achieve improvement BLEU tst2015 tst2018 respectively. This procedure agnostic ASR systems applied NMT model training easily. % In research, shown use stacking model using neural network xgboost makes able get high score Ukara 1.0 challenge dataset. We also propose use SMOTE TPE handle problems automatic short answer scoring also improve model performance. The model produced received combined F1 score 0.821, better previously published methods. From results obtained, seen still big difference F1 Score question A question B. This could due different characteristics questions. In future research, explored use different features methods question. Furthermore, also explored use multi-task learning methods train model questions time."," Neural Machine Translation  models have demonstrated strong state of the art performance on translation tasks where well-formed training and evaluation data are provided, but they remain sensitive to inputs that include errors of various types. Specifically, in the context of long-form speech translation systems, where the input transcripts come from Automatic Speech Recognition , the NMT models have to handle errors including phoneme substitutions, grammatical structure, and sentence boundaries, all of which pose challenges to NMT robustness.  %This paper makes two main contributions via an in-depth error analysis and a proposed solution.  Through in-depth error analysis, we show that sentence boundary segmentation has the largest impact on quality, and we develop a simple data augmentation strategy to improve segmentation robustness."
"Automatic summarization automated process reducing size input text preserving relevant information content core semantics. Techniques summarization often characterized either: Extractive Abstractive. Extractive methods construct summaries combining salient passages source text; process similar human's way identifying right information. One way achieve extractive summarization define problem sentence classification task, using form representation sentences document . To avoid content overlap issues, previous work used sentence reranking sentence ordering extracting sentences recurrently . Abstractive methods generate summaries generating new sentence constructs ``from scratch'', representation document content, process conceptually similar notion paraphrasing. Abstractive text summarization attracted interest since capable generating novel formulations summaries using language generation models conditioned source text. Several attention-based Recurrent Neural Network encoder-decoders introduced tackle varying text generation issues standalone abstractive sequence-to-sequence models. Copy pointer mechanisms , example, enabled decoders better generate unseen words, out-of-vocabulary words named entities. Most recently, hybrid extractive abstractive architectures proposed shown promising results quantitative performance measures human evaluations. In set-ups, extractive model first selects salient sentences source article, abstractive model paraphrases extracted sentences final summary. The majority current state-of-the-art abstractive summarization models\footnote{Excluding summarization models using large scale pre-trained language models BERT } based hybrid approach . Nonetheless, hybrid models limited three disadvantages. First, since ground-truth labels extractive summarization usually provided, extractive labels must generated potentially suboptimal algorithm . The performance models trained labels therefore bounded quality performance extractive heuristics. Second, since ground-truth binary labels recurrently extracted sentences typically teacher forced \citet{chen-bansal-2018-fast}, ``exposure bias'' may negatively affect content selection performance inference. Finally, given hard extraction step differentiable, existing hybrid models typically require multi-step training reinforcement learning train whole model. In paper, introduce novel abstractive summarization model incorporates intermediate extractive step require labels type extractive content selection, fully end-to-end trainable. To achieve this, propose new memory augmented encoder-decoder architecture called Mem2Mem. Mem2Mem 2 memorization modes: absorb key information encoded source sequence via compression mechanism, sequentially update external memory target summary generation. Without using extractive ground-truth labels, find analysis Mem2Mem's compression mechanism behaves implicit sentence extractor stores sentence representations salient content. The choice sentence representations guided memory regularization conditional language modeling loss decoder, thus avoiding exposure bias maximizing likelihood sequential binary extraction labels. Finally, encoded memory transferred decoder memory, iteratively refined decoding process. To knowledge, Mem2MeM first abstractive summarization model uses memory compression sentence extraction directly employs memorized representations summary generation. We empirically demonstrate merits approach setting new state-of-the-art long text abstractive summarization tasks Pubmed, arXiv Newsroom datasets . Our contributions three fold: % fig_architecture We propose sentence boundary errors neglected area study NMT robustness, especially context speech translation. We quantitatively demonstrate poor sentence segmentation degrades performance almost twice much transcript level-errors. To address this, developed simple method data augmentation immediate gains serve baseline future work segmentation NMT robustness. Given simplicity ease adaptation existing systems, hope integrate approach production systems. \vfill\pagebreak References produced using bibtex program suitable BiBTeX files . The IEEEbib.bst bibliography style file IEEE produces unsorted bibliography list. -------------------------------------------------------------------------"," We introduce Mem2Mem, a memory-to-memory mechanism for hierarchical recurrent neural network based encoder decoder architectures and we explore its use for abstractive document summarization. Mem2Mem transfers ``memories"" via readable/writable external memory modules that augment both the encoder and decoder. Our memory regularization compresses an encoded input article into a more compact set of sentence representations. Most importantly, the memory compression step performs implicit extraction without labels, sidestepping issues with suboptimal ground-truth data and exposure bias of hybrid extractive-abstractive summarization techniques. By allowing the decoder to read/write over the encoded input memory, the model learns to read salient information about the input article while keeping track of what has been generated.  Our Mem2Mem approach yields results that are competitive with state of the art transformer based summarization methods, but with 16 times fewer parameters. %On abstractive long text summarization, Mem2Mem surpasses, with full end-to-end training, the current state-of-the-art by 3.98 and 3.08 average ROUGE scores on the Pubmed and arXiv datasets while using $16$ times less parameters.  % Our code and trained models are available at \url{https://github.com/anonymously999/mem2mem}."
"Attention-based encoder-decoder modeling natural powerful paradigm speech text tasks, automatic speech recognition speech translation , led significant progress . However, relies large amounts supervised speech data, expensive transcribe translate. In addition, amount speech transcripts speech translation labels dwarfed amount text data available language model machine translation training. For example, number text tokens used LM modeling two orders magnitude larger number tokens corresponding speech corpus Librispeech data corpus, shown Table. Attention-based encoder-decoder models designed incorporate heterogeneous inputs cannot benefit large amounts low cost text data directly speech applications. As expected, performance gaps still observed attention based encoder-decoder systems conventional systems multiple components. %Short description previous work In order alleviate data scarcity issue, different approaches studied, including acoustic linguistic aspects. %In study, focus leveraging text data improve linguistic modeling ability speech text systems. LM commonly used method integrate linguistic information ASR. Prior work focuses building LM monolingual text data, integrate LM transfer knowledge decoder. generate synthetic data text augment speech training corpus. Another direction leverage text data directly training multitask learning. use common representation space learn correspondences different modalities spoken language understanding. propose multi-modal data augmentation jointly train text speech ASR. %is reminiscent work done multimodal learning spoken language understanding also uses common representation space learn correspondences different modalities. focused ST tasks trained ASR system together, ASR used auxiliary task. Hence, methods cannot applied back ASR systems. %What proposed, describe main idea %We follow second direction propose using auxiliary text tasks enhance speech text tasks. In study, focus leveraging text data improve linguistic modeling ability speech text tasks. We propose general framework leverage text data ASR ST tasks. %Two encoders take text speech input respectively, decoder shared tasks. During inference, speech encoder decoder used. A denoising autoencoder task introduced jointly trained ASR task monolingual data, machine translation task co-trained ST task parallel data. Text input represented spoken form using phoneme sequence effectively reduces difference speech input text input. We also carefully study different design choices joint training system, including strategies share text speech encoders comparing joint training system models initialized pre-trained components. Our experiments show proposed joint training systems effectively reduce word error rate ASR task 10\% 15\% improve BLEU score 3.69.2 ST tasks. %Compared previous methods, method emphasizes reducing difference two encoders eases knowledge transfer text text speech text tasks. %The method includes three parts: first, representation difference text speech input minimized phoneme sequence representation additional speech end sentence token. Second, novel cross attentive loss proposed increase similarity sequences different lengths. It acts auxiliary loss regularize outputs two encoders. Third, %masking applied input text tokens simulate adverse conditions speech, noise incomplete pronunciation. It also encourages decoder learn better language context representation fill gap due masking. %Instead focusing one particular task previous work, method applied ASR ST tasks. Experiments conducted two popular ASR ST benchmark tasks. The results show proposed method brings substantial gains baseline ASR ST tasks. This work proposes Mem2Mem, novel MAED based mechanism long text abstractive summarization. Mem2Mem involves two memory types: A static encoder memory compressing input texts dynamic decoder memory refines generation process. Memory transfer links two memories maximizes benefit content extraction aimed summarization. Different existing hybrid extractive abstractive approaches, Mem2Mem incorporates extraction step without ground truth sentence labels multi-step training. We demonstrate effectiveness Mem2Mem showing promising results PubMed, arXiv, Newsroom summarization datasets order magnitude less parameters competing transformer-based models. The Mem2Mem's memory compression generalized domains require text generation guided content selection. In future work, extend validate strength approach variety language learning tasks."," Attention-based sequence-to-sequence modeling provides a powerful and elegant solution for applications that need to map one sequence to a different sequence.  Its success heavily relies on the availability of large amounts of training data.  This presents a challenge for speech applications where labelled speech data is very expensive to obtain, such as automatic speech recognition  and speech translation .  In this study, we propose a general multi-task learning framework to leverage text data for ASR and ST tasks. Two auxiliary tasks, a denoising autoencoder task and machine translation task, are proposed to be co-trained with ASR and ST tasks respectively.  We demonstrate that representing text input as phoneme sequences can reduce the difference between speech and text inputs, and enhance the knowledge transfer from text corpora to the speech to text tasks.  Our experiments show that the proposed method achieves a relative 10$\sim$15\% word error rate reduction on the English Librispeech task compared with our baseline, and improves the speech translation quality on the MuST-C tasks by 3.6$\sim$9.2 BLEU."
"Motivated process human inquiry learning, field question generation requires model generate natural language questions context. QG wide applicability automated dialog systems, language assessment, data augmentation, development annotated data sets question answering research. Most prior research QG focused generating relatively simple factoid-based questions, answering question simply requires extracting span text single reference document. However, motivated desire build NLP systems capable sophisticated forms reasoning understanding, increasing interest developing systems multi-hop question answering generation , answering questions requires reasoning content multiple text documents . Unlike standard QG, generating multi-hop questions requires model understand relationship disjoint pieces information multiple context documents. Compared standard QG, multi-hop questions tend substantially longer, contain higher density named entities, and---perhaps importantly---high-quality multi-hop questions involve complex chains predicates connecting mentioned entities To address challenges, existing research multi-hop QG primarily relies graph-to-sequence methods. These approaches extract graph inputs augmenting original text structural information apply graph neural networks learn graph embeddings fed sequence-based decoder. However, necessity complex G2S approaches---which require designing hand-crafted graph extractors---is entirely clear, especially standard transformer-based sequence-to-sequence models already induce strong relational inductive bias. Since transformers inherent ability reason relationships entities text, one might imagine models alone would suffice relational reasoning requirements multi-hop QG. \xhdr{Present work} In work, show that, fact, standard transformer architecture sufficient outperform prior state-of-the-art multi-hop QG. We also propose analyze graph-augmented transformer ---which integrates explicit graph structure information transformer model. GATE sets new state-of-the-art outperforms best previous method 5 BLEU points HotpotQA dataset. However, show gains induced graph augmentations relatively small compared improvements vanilla transformer architecture, auxiliary contrastive objective data filtering approach, improve model 7.9 BLEU points ablation studies. Overall, results suggest diminishing returns incorporating hand-crafted graph structures multi-hop reasoning provides foundation stronger multi-hop reasoning systems based transformer architectures. Our key contributions summarized follows: We hope work provides strong foundation future research multi-hop QG guiding field towards promising avenues future model improvements.\documentclass[11pt,a4paper]{article} \usepackage[hyperref]{emnlp2020} \usepackage{times} \usepackage{latexsym} \renewcommand{\UrlFont}sachande@mila.quebec, wuli@us.ibm.com \usepackage{microtype} \aclfinalcopy % Uncomment line final submission \usepackage[utf8]{inputenc} % allow utf-8 input \usepackage[T1]{fontenc} % use 8-bit T1 fonts \usepackage{url} % simple URL typesetting \usepackage{booktabs} % professional-quality tables \usepackage{amsfonts} % blackboard math symbols \usepackage{amsmath} \usepackage{nicefrac} % compact symbols 1/2, etc. \usepackage{graphicx} \usepackage{microtype} % microtypography \usepackage{tabularx} \usepackage{xcolor} \usepackage{bbm} \usepackage{array} \usepackage{arydshln} \usepackage{amsfonts} \usepackage{amsmath} \DeclareMathOperator*{\argmax}{arg\,max} \DeclareMathOperator*{\argmin}{arg\,min} \usepackage{bbm} \usepackage{boldline} \usepackage{bigstrut} \usepackage{blindtext} \usepackage{booktabs, siunitx} \usepackage[labelfont=bf, format=plain, justification=justified, singlelinecheck=false]{caption} \usepackage{color} \usepackage{cprotect} \usepackage{ctable} \usepackage{dirtytalk} \usepackage{enumitem} \usepackage[export]{adjustbox} \usepackage{float} \usepackage{graphicx} \usepackage{hhline} \usepackage{latexsym} \usepackage{mathrsfs} \usepackage{microtype} \usepackage{moresize} \usepackage{multicol} \usepackage{multirow} \usepackage{nccmath} \usepackage{nicefrac} \usepackage{pifont} \usepackage{placeins} \setlength\bigstrutjot{3pt} \usepackage{soul} \usepackage{subcaption} \usepackage{times} \usepackage[utf8]{inputenc} \usepackage{url} \usepackage{verbatim} \usepackage{wrapfig, lipsum} \usepackage{textcomp} \usepackage{enumitem} %\hypersetup{draft} \newcommand\sL{\ensuremath{\mathcal{L}}} \newcommand\sD{\ensuremath{\mathcal{D}}} % colors \definecolor{lblue}{HTML}{A6CEE3} \definecolor{lgreen}{HTML}{B2DF8A} \definecolor{lred}{HTML}{FB9A99} \definecolor{lorange}{HTML}{FDBF6F} \definecolor{mblue}{HTML}{80B1D3} \definecolor{mgreen}{HTML}{B3DE69} \definecolor{mred}{HTML}{FB8072} \definecolor{morange}{HTML}{FDB462} \definecolor{blue}{HTML}{1F78B4} \definecolor{green}{HTML}{33A02C} \definecolor{red}{HTML}{E31A1C} \definecolor{orange}{HTML}{FF7F00} \definecolor{dblue}{HTML}{0050EF} \definecolor{dgreen}{HTML}{006D2C} \definecolor{dorange}{HTML}{EC7014} \newcommand{\blue}[1]{{\color{blue} #1}} \newcommand{\green}[1]{{\color{green} #1}} \newcommand{\red}[1]{{\color{red} #1}} \newcommand{\orange}[1]{{\color{orange} #1}} \newcommand{\dblue}[1]{{\color{dblue} #1}} \newcommand{\dgreen}[1]{{\color{dgreen} #1}} \newcommand{\dorange}[1]{{\color{dorange} #1}} \newcommand{\cut}[1]{} \newcommand{\xhdr}[1]{{\bfseries #1}.} \interfootnotelinepenalty=1000 \title{Stronger Transformers Neural Multi-Hop Question Generation} \author{Devendra Singh Sachan, Lingfei Wu, Mrinmaya Sachan, William Hamilton \\ Mila - Quebec AI Institute\\ School Computer Science, McGill University\\ IBM Thomas J. Watson Research Center, Yorktown Heights\\ ETH Zurich\\ mrinmaya.sachan@inf.ethz.ch, wlh@cs.mcgill.ca\\ {\tt mrinmaya.sachan@inf.ethz.ch, wlh@cs.mcgill.ca} } \date{} % !TeX root = main.tex In study, propose general multi-task learning framework leverage text data ASR ST tasks. The ASR task co-trained denoising autoencoder task using monolingual text, MT task jointly trained ST task parallel data. Text input represented phoneme sequences reduce difference speech input text input. We examined different factors impact performance jointly trained system. Our experimental results show substantial WER reduction achieved dataset large BLEU score gain obtained datasets. It proves effectiveness proposed method. \vfill\pagebreak References produced using bibtex program suitable BiBTeX files . The IEEEbib.bst bibliography style file IEEE produces unsorted bibliography list. -------------------------------------------------------------------------"," Prior work on automated question generation has almost exclusively focused on generating simple questions whose answers can be extracted from a single document. However, there is an increasing interest in developing systems that are capable of more complex multi-hop question generation, where answering the questions requires reasoning over multiple documents. In this work, we introduce a series of strong transformer models for multi-hop question generation, including a graph-augmented transformer that leverages relations between entities in the text.  While prior work has emphasized the importance of graph-based models, we show that we can substantially outperform the state-of-the-art by {5 BLEU points}  using a standard transformer architecture. We further demonstrate that graph-based augmentations can provide complimentary improvements on top of this foundation. Interestingly, we find that several important factors---such as the inclusion of an auxiliary contrastive objective and data filtering could have larger impacts on performance.  We hope that our stronger baselines and analysis provide a constructive foundation for future work in this area."
"Variational Autoencoders allow design complex generative models data. % since inference process VAE-based approaches advantage independent model architecture providing high flexibility designing new neural components. In wake renewed interest VAEs, traditional probabilistic topic models revised giving rise several Neural Topic Model variants, NVDM , ProdLDA , NTM-R , etc. % GSM , W-LDA However, existing topic models applied user reviews may extract topics associated writers' subjective opinions mixed related factual descriptions plot summaries movies books . Although approaches achieved significant results via neural inference process, surprisingly little work done disentangle inferred topic representations. % Despite lack general consensus formal definition disentangled representations , Disentangled representations defined representations individual latent units sensitive variations single generative factor, relatively invariant changes factors . Inducing representations shown significantly beneficial generalization interpretability . For example, image viewed results several generative factors mutually interacting, one many sources light, material reflective properties various surfaces shape objects depicted . % In context topic modeling, documents result generative process mixtures latent topics, therefore, propose consider latent topics generative factors disentangled improve interpretability discriminative power. Disentangled topics topics invariant factors variation text, instance, context book movie reviews could author's opinion , salient parts plot auxiliary information reported. An illustration shown Fig. opinion topics separated plot topics. % leads separating topics based ``factor variation"" revealing. % For example, generating book review, factors variation involved could depend author's expertise identifying salient features book, %his knowledge book's genre, % ability summarize plot feelings evoked book. % % [Let's break in/the atom] % % Figure reports examples polarity-disentangled topics generated IMDB movie reviews ""The Hobbit"". The topics left right summarize positive negative aspects described users, neutral topics middle report main elements movie's plot. % An effective approach disentangling features latent space VAEs adopt adversarial training . However, despite successful applications computer vision , applications text analysis rather limited far , narrowed lack proper tasks evaluate generated disentangled representations limited availability suitable datasets. % For example, book movie reviews, want disentangle topics related opinions expressed text topics relating book/movie plots. An illustration shown Figure opinion topics separated plot topics. However, models relying solely sentiment information easily misled suitable disentangle opinion plots, since even plot descriptions frequently make large use sentiment expressions . Consider example following sentence: ``The ring holds dark power, soon begins exert evil influence Bilbo"", excerpt strong positive Amazon's review. % This overcomes difficulty separating opinions plot auxiliary information yet containing polarised descriptions easily mislead models merely relying sentiment lexicon; analogously issue mixed topics generated traditional topic models applied review documents, pointed \citet{Blei08}. % Despite successful employment computer vision , adversarial approach rather limited application text analysis far , narrowed lack proper tasks evaluate generated disentangled representations limited availability suitable datasets. Therefore, propose distinguish opinion-bearing topics plot/neutral ones combining neural topic model architecture adversarial training. In study, present DIsentangled Adversarial TOpic Model \footnote{Source code dataset omitted anonymous submission.}, aiming disentangling information related target labels , distinct aspects yet possibly still polarised . We also introduce new dataset, namely MOBO dataset\footnotemark[\value{footnote}], made movie book reviews, paired related plots. The reviews come different publicly available datasets: IMDB , GoodReads Amazon reviews , %, encompass wide spectrum domains styles. We conduct extensive experimental assessment model. First, assess topic quality terms topic coherence diversity compare DIATOM supervised topic models sentiment classification task; then, analyse disentangling rate topics quantitatively assess degree separation actual opinion plot/neutral topics. Our contributions summarized below: The rest paper organized follows. We review related literature sentiment-topic models, neural topic models studies disentangled representations . Then, present details proposed DIATOM model , followed experimental setup results . Finally, conclude summary results suggestions future works . %%%%%%%%%%%%%%%%%%%%%%%%%%% In work, propose series strong transformer models multi-hop QG. To effectively encode context documents answer, introduce answer type embeddings new sublayer incorporate extracted entity-centric graph. We also propose auxiliary contrastive objective identify supporting facts data filtering approach balance training-test distribution mismatch. Experiments HotpotQA dataset show models outperform current best approaches substantial margin 5 BLEU points. Our analysis reveals graph-based components may critical improving performance, render complementary strengths transformer. !TeX root = main.tex"," The flexibility of the inference process in Variational Autoencoders  has recently led to revising traditional probabilistic topic models giving rise to Neural Topic Models . Although these approaches have achieved significant results, surprisingly very little work has been done on how to disentangle the latent topics. Existing topic models when applied to reviews may extract topics associated with writers' subjective opinions mixed with those related to factual descriptions such as plot summaries in movie and book reviews. It is thus desirable to automatically separate opinion topics from plot/neutral ones enabling a better interpretability. %Since in the topic modeling framework documents result from a generative process over mixtures of latent topics, we propose to interpret these latent topics as generative factors to be disentangled to improve their interpretability and discriminative power. In this paper, we propose a neural topic model combined with adversarial training to disentangle opinion topics from plot and neutral ones. We conduct an extensive experimental assessment introducing a new collection of movie and book reviews paired with their plots, namely MOBO dataset, showing an improved coherence and variety of topics, a consistent disentanglement rate, and sentiment classification performance superior to other supervised topic models."
"\subsection{Dialogue act recognition} Mutual understanding interactive situations, either several people engaged dialogue interacting modern computer system natural language, may achieved without considering semantic information speakers utterances pragmatic interaction level, especially relative dialogue acts. Dialogue Acts represent meaning utterance context dialogue, or, words, function utterance dialogue. For example, function a~question request information, answer shall provide information. Dialogue acts thus commonly represented phrase-level labels statements, yes-no questions, open questions, acknowledgements, on. Automatic recognition dialogue acts fundamental component many human-machine interacting systems support natural language inputs. For instance, dialogue acts typically used input dialogue manager help deciding next action system: giving information user asking question, eventually keeping quiet user acknowledging, giving comment, even asking delaying interaction. In latter case, system reaction may perceived intrusive. Beyond human-machine interaction, task also important applications rely analysis human-human interactions, either oral, e.g., recordings meetings, % lada - added reference according rev 1 written, e.g., reply mention-at structures Twitter conversations. It also essential large range applications, example talking head animation, machine translation, automatic speech recognition topic tracking. The knowledge user dialogue act useful render facial expressions avatar relevant current state discourse. In machine translation domain, recognizing dialogue acts may bring relevant cues choose alternative translations, adequate syntactic structure may depend user intention. Automatic recognition dialogue acts may also used improve word recognition accuracy automatic speech recognition systems, different language model applied recognition depending dialogue act. %lada - added reference according rev 1, To conclude, dialogue act recognition important building block many understanding interacting systems. %pav --I've commented rest sentence, clear 2 reviewers ) -- typically completes semantic role labelling dialogue management. \subsection {Motivation objectives} Researches dialogue act recognition carried long time, detailed Section. The majority works exploit supervised learning lexical, syntactic, prosodic and/or dialogue history features. However, approaches consider semantic features, may bring additional information prove useful improve accuracy dialogue act recognition system. For instance, a~frequent cause recognition errors ``unknown'' words testing corpus never occur training sentences. Replacing specific named entities text category proposed literature remedy issue. We investigate general solution exploits lexical similarity word vectors. These word vectors may computed various ways, typically include mostly lexical semantic information word well syntactic information, e.g., related relative position degree proximity pairs words within sentence. This additional information may used improve dialogue act recognition, particular training test conditions differ, size training corpus relatively small. %goal In work, propose new Deep Neural Network based Long Short-Term Memory task dialogue act recognition, compare performance standard Maximum Entropy model. Our first objective leverage modelling capacity DNN order achieve dialogue act recognition raw observed word forms, i.e., without additional expert-designed feature. This model described Section. The second objective validate model standard English DA corpus, well two languages, without changing anything model, order assess genericity robustness approach. These experiments summarized Section. Finally, third objective study impact word embeddings, shown provide extremely valuable information numerous Natural Language Processing tasks, never used far~\footnote{To best knowledge time submission} dialogue act recognition. This study summarized Section. %The following Section presents review related works domain. We described DIATOM, new neural topic model generate disentangled topics combination VAE adversarial learning. We reported results experimental study based novel dataset highlighting benefit approach leading topics higher interpretability terms topic coherence topic uniqueness discriminative power reflected better sentiment classification results compared supervised topic models. We discussed model capability consistently disentangle opinion-bearing topics plot/neutral ones measuring introduced disentangling rate. Finally, identified current limitations viable solutions explored future."," Dialogue act recognition is an important component of a large number of natural language processing pipelines. Many research works have been carried out in this area, but relatively few investigate deep neural networks and word embeddings. This is surprising, given that both of these techniques have proven exceptionally good in most other language-related domains. We propose in this work a new deep neural network that explores recurrent models to capture word sequences within sentences, and further study the impact of pretrained word embeddings. We validate this model on three languages: English, French and Czech. The performance of the proposed approach is consistent across these languages and it is comparable to the state-of-the-art results in English. More importantly, we confirm that deep neural networks indeed outperform a Maximum Entropy classifier, which was expected. However, and this is more surprising, we also found that standard word2vec embeddings do not seem to bring valuable information for this task and the proposed model, whatever the size of the training corpus is. We thus further analyse the resulting embeddings and conclude that a possible explanation may be related to the mismatch between the type of lexical-semantic information captured by the word2vec embeddings, and the kind of relations between words that is the most useful for the dialogue act recognition task."
"As important task Natural Language Generation , dialogue generation empowers wide spectrum applications, chatbot customer service automation. In past years, breakthroughs dialogue generation technology focused series sequence-to-sequence models . More recently, external knowledge employed enhance model performance. % , instance, propose Mem2Seq using structured knowledge task-oriented dialogue generation. assist dialogue generation using knowledge triples. Similarly, explore document knowledge discovery dialogue generation, utilize unstructured knowledge explore open-domain dialogue generation. However, unaffordable knowledge construction defective domain adaptation restrict utilization. Copy-based generation models widely adopted content generation tasks show better results compared sequence-to-sequence models faced out-of-vocabulary problem. Thanks nature leveraging vocabulary context distributions content copy, enables copy aforementioned named entities appeared context) upper context improve specificity generated text. In task dialogue generation, often observe phrases/utterance patterns across different ""similar dialogue"" instances. For example, customer service, similar inquiries customers get similar responses staff. It motivates us build model copy content within upper context target dialogue instance, also learn similar patterns across different similar cases target instance. Such external copy critical scenarios. %Fi Judge's questions, target court debate case, copied internal external sources, `cross-copy' enhance dialougue generation essentially. Figure paper, aware possibility copying adjacent Unfortunately, methods enable internal copy, e.g., copy content within target dialogue instance. External copy, e.g., copy content across different dialougue instances, incapable. However, Figure. depicted, %is another effective network structure. It solved problem traditional sequence-to-sequence model cannot solve problem vocabulary output sequence change length input sequence. %Copynet proposed humans tend repeat entity names even long phrases conversation.And generate entity appeared previous article copied. %Recently, Pointer networks Copynet's variants played important role NLG. Among them, Pointer-Generator Networks proposed. %In order copy key information context well cope Out-Of-Vocabulary problem. It relies vocabulary distribution context distribution, extended vocabulary obtained. % GLMP proposed global memory encoder local memory decoder share external knowledge Pointer networks. %}As general domain network structure, pointer network Copynet shows fine effect general text generation tasks. It solves problem domain adaptability poor dialog generation, introduce external knowledge, also address Out-Of-Vocabulary problem enable content copy. % Pointer networks Copynet provided effective approach address Out-Of-Vocabulary problem enable content copy. %The recent effort, Pointer-Generator Networks , inherited advantages leveraging vocabulary context distributions content copy. As shown Figure., propose two different kinds copy mechanisms study: vertical copy context-dependent information within target dialogue instance, horizontal copy logic-dependent content across different 'Similar Cases' . This framework labeled Cross-Copy Networks . As exemplar dialogue depicted, judges may repeat words, phrases utterances historical dialogues SCs sharing similar content, e.g., `A sue B X Y'. %In study, 'Similar Cases' refers similar dialogue dialogue. When generating next sentence based historical dialogue, refer similar dialogue dialogue obtain it. In paper, propose new network: Cross-Copy Networks, copy previous entity, also learn logic dialogue generation copied specific words, phrases utterance similar cases deal out-of-vocabulary words. % The CCN two pointers, one copy specific entity sentence context another copy process discourse complete sentence SC. % As shown Figure 1, two similar cases target case. Our copy methods divided two types, internal copy external copy. internal copy: directly copy specific entities words appear context words generated. external copy: copy related sentences phrases similar cases directly generated sentences. % As shown Fig., There three samples CCN: Selective copy: copy specific words phrases SC sentences generated, sample 1. Cross copy: copy specific entities context, copy process-frame nature sentences SC, sample 2. Deep copy: copy process discourse directly generated sentence, usually sentence appears frequently full text, sample 3. In order validate proposed model, employ two different dialogue datasets two orthogonal domains - court debate customer service. We apply proposed CCN datasets dialogue generation. Experiments show model achieves best results. To sum up, contributions follows: % We propose work LSTM-based deep neural network dialogue act recognition. We show model performs good state-of-the-art, even though uses raw word forms inputs, without additional information, particular neither part-of-speech tags information speaker. We applied exactly model hyper-parameters three different languages: English, French Czech. The proposed model performs well three languages, suggesting performance generalizes nicely various types corpora dependent specific tuning hyper-parameters experimental conditions. This confirms interesting modelling potential deep recurrent networks NLP general, supports conclusions recent works domain, demonstrate good performance end-to-end training deep neural networks dialogue act recognition. A surprising conclusion work concerns actual impact pretrained word embeddings, shown great importance several NLP tasks literature. We show work standard pretrained embeddings help dialogue act recognition task three tested languages. We thus study embeddings result training proposed model end-to-end manner, show seem differ vanilla word2vec embeddings, may explain perform well tasks. Of course, single type word embeddings tested work, word2vec, additional preliminary experiments suggest LDA COALS-based embeddings help either. More experiments various embeddings made confirm infirm conclusion, would convincing realized another deep network implementation variable experimental conditions. To best knowledge, first work exploits pretrained word embeddings dialogue act recognition, one rare published work shows analyzes weakness word2vec embeddings. We compare proposed deep neural network standard Maximum Entropy classifier, show DNN consistenly outperforms Maximum Entropy classifier French English. This case Czech, likely due already high level accuracy reached corpus, leaves little gained improving model. A interesting conclusion comparison DNN Maximum Entropy pretrained word embeddings improve Maximum Entropy model DNN. This likely results limited modelling capacity Maximum Entropy model, still benefits information brought pretrained embeddings. But information precise enough DNN, shown qualitative analysis word2vec."," In the past few years, audiences from different fields witness the achievements of sequence-to-sequence models  to enhance dialogue content generation. While content fluency and accuracy often serve as the major indicators for model training, dialogue logics, carrying critical information for some particular domains, are often ignored. Take customer service and court debate dialogue as examples, compatible logics can be observed across different dialogue instances, and this information can provide vital evidence for utterance generation.  In this paper, we propose a novel network architecture - Cross Copy Networks  to explore the current dialog context and similar dialogue instances logical structure simultaneously. Experiments with two tasks, court debate and customer service content generation, proved that the proposed algorithm is superior to existing state-of-art content generation models. % The traditional sequence-to-sequence model  has achieved good results in Natural Language Generation tasks. % For dialogue generation task in specific areas , some of the utterances by judge and customer service personnel to be saied usually contain specific logic and this utterances are highly similar. % Therefore, when generating the current utterance, we need to refer to not only the current context but also similar cases. % In this paper, we proposed a new neural network architecture named Cross Copy Networks , It locates entity in the context and the logical expression of similar cases by learning two conditional probability pointers. % We apply CCN to the legal dialogue data and customer service dialogue data for dialogue generation task. % Experiments show that our model achieves the best results."
"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% A long-standing challenge computer science develop algorithms interact human users via dialog natural language~. Of particular interest task-oriented dialog, wherein user interacts system achieve goal . The system understand user's requests assist taking appropriate actions . In recent years, supervised learning approaches problem become particularly popular, potentially learn complex patterns without relying hand-crafted rules. While data-driven methods already demonstrate impressive performance open-domain dialog , task-oriented dialog models face additional difficulty transferring skills tasks domains present training data. To address issue, present Schema-guided Dialog Dataset Transfer Learning dataset, collection realistic, task-oriented dialogs, especially designed test facilitate transfer learned patterns tasks. Unlike open-domain dialogs, task-oriented dialogs accompanied set steps necessary complete task. These steps typically known priori thus learned data. In fact, practical applications desirable could make modifications logic without discard large parts dataset. The ideal sequences steps dialog would follow complete task arranged graph . Together utterances actions associated nodes graph, hence call task schema, simply schema. Note, call `schema' similar `task specification' , distinct `schemas' define slots intents task used \citet{rastogi2019towards}. %or \citet{kimEighthDialogSystem2019}. In typical supervised model trained to, say, predict next system action task-oriented dialog, schema training tasks implicitly captured learned model parameters. This makes generalizing new task difficult, implicitly memorized schema longer appropriate . With \DATASETNAME\ provide explicit schema representations task thereby enable models condition schema . To collect \DATASETNAME\ use Wizard Oz setup , system's role played human `wizard'. Based pilot studies, found quality crowd-sourced dialogs depends strongly We refined approach extensive internal testing four rounds pilot studies. % All code instructions available open source \anonymous{\DATASETURL}. Our aim create ecologically valid dataset following four attributes, believe crucial dataset high quality: % The progression difficulty allows better assessment dialog models potential transfer learning across levels difficulty. \item Consistency system side. % The behavior task-oriented dialog system largely deterministic subject whims personality wizard. % In particular, encourage wizards follow given task schema closely possible. \item Explicit knowledge base queries. % A large part developing dialog system implementation application programming interface calls, knowledge base queries. % In \DATASETNAME\ represent dialogs three-party interaction wherein system acts intermediary user knowledge base . % Thus, models learn query knowledge base, query be, explain returned knowledge base item user. \end{enumerate} % With properties, create ecologically valid, described by. With paper, contribute The code latter setup, collected data, modeling code freely available \anonymous{\DATASETURL}. We studied feasibility training fully bilingual deep neural language model, i.e.\ model approaches matches performance monolingual models language-specific tasks. We trained bilingual Finnish-English \bertbase{} model expanding vocabulary size sum size two individual vocabularies, compared model performance monolingual models. We found that, range NLU tasks, bilingual model performs comparably nearly comparably monolingual models. We conclude that, \bertbase{} architecture, possible train fully bilingual deep contextual model two remotely related languages. We release newly introduced \bbert{} model tools introduced create model open licenses ."," We present \DATASETNAME, a schema-guided task-oriented dialog dataset consisting of 127,833 utterances and knowledge base queries across 5,820 task-oriented dialogs in 13 domains that is especially designed to facilitate task and domain transfer learning in task-oriented dialog. Furthermore, we propose a scalable crowd-sourcing paradigm to collect arbitrarily large datasets of the same quality as \DATASETNAME. Moreover, we introduce novel schema-guided dialog models that use an explicit description of the task to generalize from known to unknown tasks.  We demonstrate the effectiveness of these models, particularly for zero-shot generalization across tasks and domains."
"% -------------------------------------------------------------- % % The following footnote without marker needed camera-ready % version paper. % Comment instructions uncomment 8 lines % ""final paper"" variant English. % . % % final paper: en-us version % space normally used marker This work licensed Creative Commons Attribution 4.0 International License. License details: \url{http://creativecommons.org/licenses/by/4.0/}. } The relationship group human languages characterized across several dimensions variation , including temporal dimension, wherein languages diverged common historical ancestor case Romance languages; spatial dimension, wherein speaker communities geographically adjacent case Indo-Aryan Dravidian languages India; socio-political dimension, wherein languages evolved shared political and/or religious forces case Arabic Swahili. Languages, language varieties, related across dimensions, often results dialect continuum. Speakers languages constitute dialect continuum usually communicate efficiently using mother tongue. The degree intercomprehensibility speakers different language varieties within continuum mainly determined linguistic similarities. A notable case phenomenon mutual intelligibility among Slavic languages, study paper. One goals linguistics study categorize languages based objective measures linguistic distance. The degrees similarity different levels linguistic structural organization seen preconditions for, well predictors of, successful oral intercomprehension. For closely-related languages, similarities pre-lexical, acoustic-phonetic phonological, level found better predictors cross-lingual speech intelligibility lexical similarities . In different, yet relevant research direction, investigated non-linguists' perception language variation using data popular spoken language guessing game, Great Language Game . By analyzing confusion patterns GLG's human participants, authors shown factors predicting players' confusion game correspond objective measures similarity established linguists. For example, phylogenetic relatedness overlap phoneme inventories identified factors perceptual confusability languages GLG. The development automatic systems determine identity language speech segment received attention speech recognition community . State-of-the-art approaches automatic spoken language identification, henceforth LID, based multilayer deep neural networks . DNN-based LID systems parametric models learn mapping spectral acoustic features speech high-level feature representations geometric space languages linearly separable. These models shown tremendous success discriminating distant languages also closely-related language varieties . Nevertheless, none previous works spoken language recognition analyzed emerging representations neural LID models related languages. Thus, still unknown whether distances representation spaces correspond objective measurements linguistic similarity and/or non-linguists' perception language variation. In paper, aim fill gap consider family Slavic languages case study. Our key contribution two-fold: In paper, attempt bridge different lines research far remained unconnected. On one hand, employ neural architectures field spoken language recognition build robust model identify languages contemporary acoustic realizations Slavic speech. On hand, analyze emerging language representations using techniques established previous research multilingual natural language processing . We consequently shed light speech modality show speech signals complement research done computational studies linguistic typology language variation. % best knowledge % The recognition spoken language % LID speech technology % untranscribed speech % NN made possible end-to-end systems developed, traditional approaches feature many components % closely-related languages similar phonotactics, differ acoustic realizations segments suprasegmental features % language identity objective linguistic measures similarity % The GLG % similarity representation deep neural networks % -------------------------------------------------------------- With work, make multiple contributions field task-oriented dialog research. First, presented \DATASETNAME, novel dialog dataset specifically designed facilitate transfer learning experiments. Second, introduced new, scalable crowd sourcing paradigm collect data similar quality \DATASETNAME. In future work, setup could used expand \DATASETNAME\ collecting data additional tasks, domains, languages English. Finally, established baseline scores next action prediction, response generation, zero-shot transfer learning former two tasks. With demonstrated task schemas used improve transfer learning capabilities. We also outlined variety experiments \DATASETNAME\ would suitable for, look forward seeing experiments, well improvements upon baseline scores, implemented future publications. \clearpage"," Deep neural networks have been employed for various spoken language recognition tasks, including tasks that are multilingual by definition such as spoken language identification. In this paper, we present a neural model for Slavic language identification in speech signals and analyze its emergent representations to investigate whether they reflect objective measures of language relatedness and/or non-linguists' perception of language similarity. While our analysis shows that the language representation space indeed captures language relatedness to a great extent, we find perceptual confusability between languages in our study to be the best predictor of the language representation similarity."
"For conversational AI digital assistant system , Natural Language Understanding established component produces semantic interpretations user request, typically involves analysis terms domain, intent, slot . For instance, request ``Play song Taylor Swift"" interpreted falling within scope Music domain Play Song intent Taylor Swift identified Artist slot. Improving accuracy NLU component important satisfactory end-to-end user experience. Without accurate semantic understanding user request, conversational AI system cannot fulfill request satisfactory response action. As one upstream components runtime workflow , NLU's errors also wider blast radius propagates subsequent downstream components, dialog management, routing logic back-end applications, language generation. A straight-forward way improve NLU human annotations. For example, mine user requests resulted unsatisfactory user experience make ground-truth annotations requests produced incorrect NLU outputs, used additional supervision data improving models rule engines within NLU. However, approach labor-intensive expensive. It requires least multiple tiers annotations , hard consider underlying contextual conditions. It also limited existing annotation guidelines may accurately reflect user expectations. Due limitations, leveraging user feedback, implicit explicit, real production systems emerging new area research. In paper, propose scalable automatic approach improving NLU leveraging implicit user feedback, insight user interaction data dialog context rich information embedded user satisfaction intention inferred. For instance, interacting conversational AI system, dissatisfied users might often choose intervene stopping system response middle rephrasing previous request make clearer less room ambiguous interpretation . Our work makes three main contributions. First, knowledge, work first literature introduce scalable automatic approach leveraging domain-agnostic implicit user feedback continuously improve NLU component large-scale conversational AI system production. Second, propose general framework curating supervision data improving NLU live traffic leveraged various subtasks within NLU - e.g., supervision data applied improve individual semantic interpretation models ranking/classification model across interpretations . Last, show extensive set experiments live traffic performance proposed framework impact improving NLU production system across 10 widely used domains. \def\year{2021}\relax %File: formatting-instructions-latex-2021.tex %release 2021.1 \documentclass[letterpaper]{article} % DO NOT CHANGE THIS \usepackage{aaai21} % DO NOT CHANGE THIS \usepackage{times} % DO NOT CHANGE THIS \usepackage{helvet} % DO NOT CHANGE THIS \usepackage{courier} % DO NOT CHANGE THIS \usepackage[hyphens]{url} % DO NOT CHANGE THIS \usepackage{graphicx} % DO NOT CHANGE THIS \urlstyle{rm} % DO NOT CHANGE THIS \def\UrlFont{\rm} % DO NOT CHANGE THIS \usepackage{natbib} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT \usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT \frenchspacing % DO NOT CHANGE THIS \setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS \setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS \usepackage{amsfonts} \usepackage{amsmath} \usepackage{algorithm} \usepackage{xcolor} \usepackage[noend]{algpseudocode} % \nocopyright %PDF Info Is REQUIRED. % For /Author, add authors within parentheses, separated commas. No accents commands. % For /Title, add Title Mixed Case. No accents commands. Retain parentheses. % \pdfinfo{ % /Title % /Author % /TemplateVersion %} %Leave % /Title % Put actual complete title within parentheses mixed case % Leave space \Title beginning parenthesis alone % /Author % Put actual complete list authors within parentheses mixed case. % Each author comma. If name contains accents, remove them. If LaTeX commands, % remove them. % DISALLOWED PACKAGES % \usepackage{authblk} -- This package specifically forbidden % \usepackage{balance} -- This package specifically forbidden % \usepackage{color % \usepackage{CJK} -- This package specifically forbidden % \usepackage{float} -- This package specifically forbidden % \usepackage{flushend} -- This package specifically forbidden % \usepackage{fontenc} -- This package specifically forbidden % \usepackage{fullpage} -- This package specifically forbidden % \usepackage{geometry} -- This package specifically forbidden % \usepackage{grffile} -- This package specifically forbidden % \usepackage{hyperref} -- This package specifically forbidden % \usepackage{navigator} -- This package specifically forbidden % % \indentfirst} -- This package specifically forbidden % \layout} -- This package specifically forbidden % \multicol} -- This package specifically forbidden % \nameref} -- This package specifically forbidden % \usepackage{savetrees} -- This package specifically forbidden % \usepackage{setspace} -- This package specifically forbidden % \usepackage{stfloats} -- This package specifically forbidden % \usepackage{tabu} -- This package specifically forbidden % \usepackage{titlesec} -- This package specifically forbidden % \usepackage{tocbibind} -- This package specifically forbidden % \usepackage{ulem} -- This package specifically forbidden % \usepackage{wrapfig} -- This package specifically forbidden % DISALLOWED COMMANDS % \nocopyright -- Your paper published use command % \addtolength -- This command may used % \balance -- This command may used % \baselinestretch -- Your paper published use command % \clearpage -- No page breaks kind may used final version paper % \columnsep -- This command may used % \newpage -- No page breaks kind may used final version paper % \pagebreak -- No page breaks kind may used final version paperr % \pagestyle -- This command may used % \tiny -- This acceptable font size. % {2} %May changed 1 2 section numbers desired. % The file aaai21.sty style file AAAI Press % proceedings, working notes, technical reports. % % Title % Your title must mixed case, sentence case. % That means verbs , % nouns, adverbs, adjectives capitalized, including words hyphenated terms, % articles, conjunctions, prepositions lower case unless % directly follow colon long dash % \title{AAAI Press Formatting Instructions \\for Authors Using \LaTeX{} --- A Guide } % \author{ % %Authors % % All authors must font size format. % Written AAAI Press Staff\textsuperscript{\rm 1}\thanks{With help AAAI Publications Committee.}\\ % AAAI Style Contributions Pater Patel Schneider, % Sunil Issar, \\ % J. Scott Penberthy, % George Ferguson, % Hans Guesgen, % Francisco Cruz, % Marc Pujol-Gonzalez % \\ % } % \affiliations{ % %Afiliations % \textsuperscript{\rm 1}Association Advancement Artificial Intelligence\\ % %If multiple authors multiple affiliations % % use superscripts text roman font identify them. % %For example, % % Sunil Issar, \textsuperscript{\rm 2} % % J. Scott Penberthy, \textsuperscript{\rm 3} % % George Ferguson,\textsuperscript{\rm 4} % % Hans Guesgen, \textsuperscript{\rm 5}. % % Note comma placed BEFORE superscript optimum readability % 2275 East Bayshore Road, Suite 160\\ % Palo Alto, California 94303\\ % % email address must roman text type, monospace sans serif % publications21@aaai.org % % See examples next % } %\iffalse % %Example, Single Author, ->> remove \iffalse,\fi place surrounding AAAI title use \title{A Scalable Framework Learning From Implicit User Feedback Improve Natural Language Understanding Large-Scale Conversational AI Systems} \author { Sunghyun Park\thanks{Equal contribution.}, Han Li\textsuperscript{\rm *}, Ameen Patel, Sidharth Mudgal, Sungjin Lee, Young-Bum Kim, Spyros Matsoukas, Ruhi Sarikaya \\ } \affiliations{ % Affiliations Amazon Alexa AI \\ \{sunghyu, lahl, paameen, sidmsk, sungjinl, youngbum, matsouka, rsarikay\}@amazon.com } %\fi \iffalse %Example, Multiple Authors, ->> remove \iffalse,\fi place surrounding AAAI title use \title{My Publication Title --- Multiple Authors} \author { Authors First Author Name,\textsuperscript{\rm 1} Second Author Name, \textsuperscript{\rm 2} Third Author Name \textsuperscript{\rm 1} \\ } \affiliations { % Affiliations \textsuperscript{\rm 1} Affiliation 1 \\ \textsuperscript{\rm 2} Affiliation 2 \\ firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com } \fi \newcommand{\red}[1]{{\color{red} #1}} \newcommand{\vecb}[1]{\mathbf{#1}} \newtheorem{definition}{Definition} \newpage \bibliography{citation} % \bibliographystyle{aaai21} \end{document} Insufficient labeled data limits effectiveness attention-based models ASC task. In paper, propose novel attention transfer framework, two different attention transfer methods designed exploit attention knowledge resource-rich document-level sentiment classification corpus enhance attention process resource-poor aspect-level sentiment classification, finally achieving goal improving performance ASC. Experimental results indicate approaches outperform state-of-the-art works. Further analysis validates effectiveness benefits transferring attention knowledge DSC data ASC task."," Natural Language Understanding  is an established component within a conversational AI or digital assistant system, and it is responsible for producing semantic understanding of a user request. We propose a scalable and automatic approach for improving NLU in a large-scale conversational AI system by leveraging implicit user feedback, with an insight that user interaction data and dialog context have rich information embedded from which user satisfaction and intention can be inferred. In particular, we propose a general domain-agnostic framework for curating new supervision data for improving NLU from live production traffic. With an extensive set of experiments, we show the results of applying the framework and improving NLU for a large-scale production system and show its impact across 10 domains."
"Chinese Word Segmentation fundamental task Chinese natural language processing , aims identifying word boundaries sentence composed continuous Chinese characters. It provides basic component NLP tasks like named entity recognition, dependency parsing, semantic role labeling, etc. Generally, previous studies model CWS task character-based sequence labeling task . Recently, pre-trained models BERT introduced CWS tasks, could provide prior semantic knowledge boost performance CWS systems. directly fine-tunes BERT several CWS benchmark datasets. fine-tunes BERT multi-criteria learning framework, criterion shares common BERT-based feature extraction layer owns private projection layer. combines Chinese character glyph features pre-trained BERT representations. % builds unified BERT-based model multi-criteria CWS tasks fine-tunes eight CWS criteria jointly. proposes neural CWS framework WMSeg, utilizes memory networks incorporate wordhood information pre-trained model ZEN. PTMs proved quite effective fine-tuning downstream CWS tasks. However, PTMs used previous works usually adopt language modeling pre-training tasks. Thus, usually lack task-specific prior knowledge CWS ignore discrepancy pre-training tasks downstream CWS tasks. \end{table} To deal aforementioned problems PTMs, consider introducing CWS-specific pre-trained model based existing CWS corpora, leverage prior segmentation knowledge. However, multiple inconsistent segmentation criteria CWS, criterion represents unique style segmenting Chinese sentence words, shown Table. Meanwhile, easily observe different segmentation criteria could share large proportion word boundaries them, boundaries word units ``'', ``'' ``'', segmentation criteria. It shows common prior segmentation knowledge shared different criteria. In paper, propose CWS-specific pre-trained model MetaSeg. To leverage shared segmentation knowledge different criteria, MetaSeg utilizes unified architecture introduces multi-criteria pre-training task. Moreover, alleviate discrepancy pre-trained models downstream unseen criteria, meta learning algorithm incorporated multi-criteria pre-training task MetaSeg. Experiments show MetaSeg could outperform previous works significantly, achieve new state-of-the-art results twelve CWS datasets. Further experiments show MetaSeg better generalization performance downstream unseen CWS tasks low-resource settings, improve Out-Of-Vocabulary recalls. To best knowledge, MetaSeg first task-specific pre-trained model especially designed CWS. In paper, proposed domain-agnostic scalable framework leveraging implicit user feedback, particularly user dissatisfaction rephrase behavior, automatically curate high-quality supervision data continuously improve NLU large-scale conversational AI digital assistant system. We showed extensive set experiments live traffic framework applied improve NLU analyzed performance across 10 popular domains traffic volume real production system. We showed component-level analysis framework in-depth validation performance.","     Recent researches show that pre-trained models  are beneficial to Chinese Word Segmentation .     However, PTMs used in previous works usually adopt language modeling as pre-training tasks, lacking task-specific prior segmentation knowledge and ignoring the discrepancy between pre-training tasks and downstream CWS tasks.     % However, existing approaches usually fine-tune general-purpose pre-trained models directly on separate downstream CWS corpora.     % These general-purpose pre-trained models usually adopt language modeling objectives, lack task-specific prior segmentation knowledge, and ignore the discrepancy between pre-training tasks and downstream CWS tasks.     In this paper, we propose a CWS-specific pre-trained model MetaSeg, which employs a unified architecture and incorporates meta learning algorithm into a multi-criteria pre-training task.     Empirical results show that MetaSeg could utilize common prior segmentation knowledge from different existing criteria and alleviate the discrepancy between pre-trained models and downstream CWS tasks.     Besides, MetaSeg can achieve new state-of-the-art performance on twelve widely-used CWS datasets and significantly improve model performance in low-resource settings."
"Automatic question answering active area research within natural language processing. %Open-domain question answering looks methods re-utilize systems across multiple domains. One possible way approach task look answers text passages collection documents. Recent research shown promising results developing neural models passage retrieval tasks, including Retrieval Question Answering, Open Domain Question Answering, MS MARCO. The models systems often trained using dual encoder framework questions passages encoded separately. Training effective neural retrieval model usually requires large amount high-quality data. To alleviate need high-quality data, training approached two-stages: pre-training noise data fine tuning smaller amount high-quality data, also regarded ``gold"" data. % One significant advantage dual encoder framework that, question passage embeddings available, efficient nearest neighbour search used retrieve passages contain answers questions. When used question answering, one advantage dual encoder training batches allows use, question, passages answer questions batch negatives. Given training batches randomly sampled question-passage pairs, negatives batch random nature. While effective many retrieval tasks, random negatives limitation targeted challenging enough clearly separate passage answers given question passage. How sample negatives way widens separation improves contrast correct incorrect passages remains open question. % A viable approach negative sampling use ``hard"" negatives specific question answer pair. In paper systematically explore use ``hard'' negatives neural passage retrieval models train using two-stage approach. Using hard negatives part dual encoder framework shown advantageous different tasks . %Using hard negatives part dual encoder framework shown advantageous cross-lingual tasks. %For example, \citet{guo-etal-2018-effective} show training hard negatives generated retrieving ``coarse"" negatives low-resolution model improves quality translation pairs retrieved dual encoder model. %Similarly, \citet{dpr} showed improvement using hard negatives retrieved BM25 model passage retrieval part Open Domain Question Answering task. %In contrast previous works, We explore different types negatives, experiment using pre-training fine-tuning stages. The types negatives tried are: We first use hard negatives data use pre-train models. We leverage question generator model described generate new questions passages use pre-training stage . %The new questions paired original passages. %The augmented set question-passage pairs used train first stage neural retrieval model. % It shown effective approach improve passage retrieval models. During pre-training use negatives generated strategy 4\footnote{Or strategy 1, strategy 4 feasible} improve retrieval model, strategies could introduce false negatives data. %Our initial experiments showed using retrieval models find hard negatives point, often generated noisy question-passage pairs, especially pre-training data includes synthetic pairs. %As generated question passage pairs sometimes noise, retrieval-based approaches may create better question-passage pairs synthetic pairs. %We apply heuristic based context negatives pre-training task. Next, continue fine tuning stage using small amount gold training data. At stage, explore four types negative sampling. To best knowledge, first work explores effectiveness hard negatives passage retrieval systematic way, integrates retrieval models pre-training stage. Our overall experimental architecture outlined Figure. %For question-passage pair training set, collect negatives using strategies listed augment training. We conduct experiments approach two passage retrieval tasks: Open Domain QA SQuAD) MS MARCO. %Open Domain QA Natural Questions~, Open Domain QA SQuAD, MS MARCO. Our results show four kinds hard negatives improve dual encoder models significantly consistent performance gains across tasks. However, depending types questions domain, one kind hard negative may perform better others particular task. For example, context negatives work best NQ semantic retrieval-based negatives work best SQuAD. We ensemble models trained different types hard negatives. The final models achieve state-of-the-art performance Open Domain QA task improvement prior works 0.8--2.9 points accuracy rates. %\hl{highlight numbers here}. The main contribution paper are: In work, pursued new research problem M\&A prediction. Our transformer-based classifier leveraged regularization benefits adversarial training enhance model robustness. More importantly, built upon previous techniques quantify importance words help guarantee generation plausible counterfactual explanations masked language model financial text classification. The results demonstrate superior accuracy explanatory performance compared state-of-the-art techniques. An obvious extension would include canceled deals classifier, predict novel M\&A events based market descriptions companies . Moreover, additional financial events yet another related task considered research."," %In this paper we explore the discriminate training for neural passage retrieval models with hard negatives. %Four different hard negative sampling strategies are experimented, including one BM25 based hard negative, two semantic based hard negatives, and one heuristic hard negative. %For training the model, we employ a two stage dual encoder model with pre-training using synthetic data followed by a fine-tuning using the gold training data. %Discriminate training is applied on both stages. %The trained models are evaluated on 3 passage retrieval tasks from Open Domain QA NQ, Open Domain QA SQuAD, and MS MARCO. %Results show that all of them can improve the naive dual encoder models significantly with consistent performance gain over all three tasks. %However, there is no single type of hard negative perform best on all tasks. %Further analysis show that the synthetic question pre-training with discriminate training is an effective approach to improve the passage retrieval performance. %The best trained models establish the new state-of-the-art on retrieval tasks of Open Domain QA NQ and SQuAD. %"
"Neural machine translation explored typically sentence-level translation settings. Such sentence-level nmt models inevitably suffer ambiguities multiple %% semantically-different translations accepted interpretations possible source sentence. To address issue, context-aware nmt models recently presented %to address issue incorporate document-level information translation. Most existing context-aware nmt models end-to-end models take input current source sentence translated context sentences, output translation. These models trained document-level parallel data, namely, sentence pairs surrounding, usually preceding, sentences source target language. However, practical scenarios, document-level bilingual data limited language pairs domains, % posing challenge building context-aware nmt systems . In study, propose simple yet effective approach context-aware nmt % consisting using two primitive components, sentence-level nmt model document-level language model . This approach allows us independently train two components bilingual data monolingual data, respectively, without resorting expensive document-level bilingual data. % thereby document-level bilingual data needed. To give probabilistic foundation combination two independent models, exploit % take advantage probabilistic nature nmt decoding. When generating sequence, left-to-right decoder outputs categorical probability distribution vocabulary every time step. % . The decoder assigns higher probability tokens would suitable step. Therefore, % assume multiple valid translations possible source sentence, % , ambiguities sentence-level nmt confused by, decoder gives higher % sequence probability translation plausible without considering contexts. % wrong ones. Our idea adjust probability distributions context-aware manner using document-level lm target language % capable modeling models inter-sentential dependencies target side document. % Since network structure nmt models evolves quickly, model-agnostic approach like preferable model-tweaking approach . We evaluate methods English French, Russian Japanese translations OpenSubtitles2018 corpus terms bleu scores contrastive discourse test sets. Experimental results confirmed method achieved comparable performance existing context-aware nmt models. The contributions paper follows: In work, presented simple unified representation learning framework, \modelname, event entity coreference. \modelname~learns mention-pair representation forwarding concatenated sentences RoBERTa, sentences provide context mentions. This algorithm applied event entity coreference benchmarks obtains state art performance. In addition, augmented pairwise representation structured argument features improve performance event coreference."," % There exist inevitable ambiguities in translating a single sentence, and we resort to context beyond the target sentence for resolving such ambiguities. Although many context-aware neural machine translation models have been proposed to incorporate contexts in translation,  most of those models are trained end-to-end on parallel documents aligned in sentence-level.  Because only a few domains  have such document-level parallel data, we cannot perform accurate context-aware translation in most domains. We therefore present a simple method to turn a sentence-level translation model into a context-aware model by incorporating a document-level language model into the decoder. Our context-aware decoder is built upon only a sentence-level parallel corpora and monolingual corpora; thus no document-level parallel data is needed. In a theoretical viewpoint, the core part of this work is the novel representation of contextual information using point-wise mutual information between context and the current sentence. We show the effectiveness of our approach in three language pairs, English to French, English to Russian, and Japanese to English, by evaluation in bleu and contrastive tests for context-aware translation."
"A keyphrase multi-word text representing highly abstractive information long document. Keyphrase extraction task aims generate appropriate keyphrase set given document, thus helping identify salient contents concepts document. Recently, KE task attracted much research interest since serves important component many downstream applications text summarization, document classification, information retrieval question generation. Early KE systems commonly operate extractive manner, usually consists two steps: 1) selecting candidates source document using heuristic rules, 2) ranking candidates list determine correct. However, two-step ranking approaches usually based feature engineering, labor-intensive. Motivated progress sequence-to-sequence applications neural networks, KE research's focus gradually shifted deep learning methods. \citet{DBLP:conf/acl/MengZHHBC17} first formulate KE sequence generation problem introduce attentive Seq2Seq framework generate keyphrase sequence conditioned input document. Compared traditional methods, Seq2Seq based method achieves superior performance. Seq2Seq based KE exposed two major challenges: 1) Document-level representation learning. For Seq2Seq generative framework, latent hidden representation important factor, quality directly affect decoder's performance. In KE task, input commonly long document instead sentence, poses greater challenge latent representation learning. 2) Modeling compositionality keyphrases set. The elements keyphrase set dependent correlated. That is, better modeling inherent composition embodied keyphrase set learning process effectively boost diversity quality final results. Recently, various approaches proposed optimize Seq2Seq generation framework KE task. To learn better latent representation, previous studies try introduce different encoding structures address two issues simultaneously. We explore incorporate dependency tree document representation learning encoder part. The syntactic dependency tree help locate key information document. In practice, document graph constructed depending syntactic dependency tree, convolution process operated . On hand, rethink implication compositionality keyphrase set. In training process generative models, whether candidate keyphrase generated hinges document itself, also depends keyphrases already generated. Therefore, dynamic graph updating mechanism introduced explicitly modeling inter-dependency among keyphrases. In method, graph structure encoder part dynamically updated according keyphrases generated decoder part. Concretely, one keyphrase decoded, information transferred modify edge weights document graph score function, latent hidden representation also updated. In approach, could dynamically ensure information exchange encoder decoder parts directions. The contribution work three-fold: 1) A novel generative framework, Div-DGCN, proposed leverages dynamic syntactic graph encoder diversified inference process KE. 2) A dynamic computation mechanism adopted model compositionality keyphrase set explicitly enhancing information interchange encoder decoder parts Seq2Seq architecture. 3) Extensive experiments conducted five benchmarks show proposed method effective competitive baselines several metrics. We present approach context-aware based context current sentence. We first provide formulation objective, , computation process using sentence-level translation model document-level language model. We investigate two search methods, reranking beam search, evaluate methods English-French, English-Russian, Japanese-English translation. We also provide analysis visualization better understand nature context current sentence. We plan design context-aware using context-aware models. We extend method non-autoregressive . We release code promote reproducibility results."," Keyphrase extraction  aims to summarize a set of phrases that accurately express a concept or a topic covered in a given document. Recently, Sequence-to-Sequence  based generative framework is widely used in KE task, and it has obtained competitive performance on various benchmarks. The main challenges of Seq2Seq methods lie in acquiring informative latent document representation and better modeling the compositionality of the target keyphrases set, which will directly affect the quality of generated keyphrases. In this paper, we propose to adopt the Dynamic Graph Convolutional Networks  to solve the above two problems simultaneously. Concretely, we explore to integrate dependency trees with GCN for latent representation learning. Moreover, the graph structure in our model is dynamically modified during the learning process according to the generated keyphrases. To this end, our approach is able to explicitly learn the relations within the keyphrases collection and guarantee the information interchange between encoder and decoder in both directions. Extensive experiments on various KE benchmark datasets demonstrate the effectiveness of our approach."
"Sanskrit one oldest Indo-Aryan languages. The oldest known Sanskrit texts estimated dated around 1500 BCE. It one oldest surviving languages world. A large corpus religious, philosophical, socio-political scientific texts multi cultural Indian Subcontinent Sanskrit. Sanskrit, multiple variants dialects, Lingua Franca ancient India . Therefore, Sanskrit texts important resource knowledge ancient India people. Earliest known Sanskrit documents available form called Vedic Sanskrit. Rigveda, oldest four Vedas, principal religious texts ancient India, written Vedic Sanskrit. In sometime around 5\textsuperscript{th} century BCE, Sanskrit scholar named pARini wrote treatise Sanskrit grammar named azwADyAyI, pARini formalized rules linguistics, syntax grammar Sanskrit. azwDyAyI oldest surviving text comprehensive source grammar Sanskrit today. azwADyAyI literally means eight chapters eight chapters contain around 4000 sutras rules total. These rules completely define Sanskrit language known today. azwADyAyI remarkable conciseness contains highly systematic approach grammar. Because well defined syntax extensively well codified rules, many researchers made attempts codify pARini sutras computer programs analyze Sanskrit texts. \subsection{Introduction Sandhi Sandhi Split Sanskrit} Sandhi refers phonetic transformation word boundaries, two words combined form new word. Sandhi literally means 'placing together' principle sounds coming together naturally according certain rules codified grammarian pARini azwADyAyI. There 3 different types Sandhi defined azwADyAyI. An example type Sandhi shown below: \end{quote} Sandhi Split hand, resolves Sanskrit compounds honetically merged words constituent morphemes. Sandhi Split comes additional challenge splitting compound word correctly, also predicting split. Since Sanskrit compound word split multiple ways based multiple split locations possible, split words may syntactically correct semantically may meaningful. \end{quote} \subsection{Existing Work Sandhi} The current resources available Sandhi open domain accurate. Three popular publicly available set Sandhi tools viz. JNU, UoH \& INRIA tools mentioned table . \end{table*} An analysis description tools present paper Sandhikosh . The paper introduced dataset Sandhi Sandhi Split verification compared performance tools table dataset. Neural networks used Sandhi Split many researchers, example , . The task Sandhi mainly addressed rule based algorithm e.g. . There research Sandhi using neural networks public domain far. This paper describes experiments Sandhi operation using neural networks compares results suggested approach results achieved using existing Sandhi tools . \subsection{Existing Work Sandhi Split} Many researchers like tried codify pARini rules achieving Sandhi Split along lexical resource. proposed statistical method based Dirichlet process. Finite state methods also used . A graph query method proposed . Lately, Deep Learning based approaches increasingly tried Sandhi Split. used one-layer bidirectional LSTM two parallel character based representations string. proposed deep learning models Sandhi Split sentence level. uses double decoder model compound word split. The method proposed paper describes RNN based, two stage deep learning method Sandhi Split isolated compound words without using lexical resource sentence information. In addition above, exist multiple Sandhi Splitters open domain. The prominent ones JNU Sandhi Splitter , UoH Sandhi Splitter INRIA Sanskrit reader companion. The paper compares performance 3 tools results. This attempt create benchmark area Sanskrit Computational Linguistics. In research work, propose Pratyaya-Kosh, benchmark corpus help researchers new Sanskrit building AI based Morphological Analyzer Sanskrit derivative nouns. Also propose neural approach learning derivative noun formation without use external resources language models, morphological phonetic analyzers still manage outperform existing approaches. In future intend extend current work verb derivative indeclinable derivative using machine learning methods. Proposed models refined using additional training data. Benchmark corpus made available git hub."," This paper describes neural network based approaches to the process of the formation and splitting of word-compounding, respectively known as the Sandhi  and Vichchhed, in Sanskrit language. Sandhi is an important idea essential to morphological analysis of Sanskrit texts. Sandhi leads to word transformations at word boundaries. The rules of Sandhi formation are well defined but complex, sometimes optional and in some cases, require knowledge about the nature of the words being compounded. Sandhi split or Vichchhed is an even more difficult task given its non uniqueness and context dependence. In this work, we propose the route of formulating the problem as a sequence to sequence prediction task, using modern deep learning techniques. Being the first fully data driven technique, we demonstrate that our model has an accuracy  better than the existing methods on multiple standard datasets, despite not using any additional lexical or morphological resources. The code is being made available at https://github.com/IITD-DataScience/Sandhi\_Prakarana"
"% Unsupervised representation learning allows models learn high-level latent representations unlabeled data. % Models pretrained unsupervised data fine-tuned small amount labeled data. % % Deep probabilistic generative models presents powerful approach learn representations modeling data generation process. % Variational AutoEncoders one popular approaches representation learning modeling latent features unit Gaussian space. % Vector-Quantized VAE method learn discrete representations data. Speech waveforms complex, high-dimensional form data influenced number underlying factors, broadly categorized linguistic contents speaking styles. % Learning disentangled latent representations speech wide set applications generative tasks, including speech synthesis, data augmentation, voice transfer, speech compression. Downstream tasks speech recognition speaker classification also benefit learned representations. % A pre-trained model also fine-tuned classification tasks speech recognition speaker classification. % \ngyuzh{what rephrase like: Downsteam tasks speech recognition speaker classification also benefit learned representations.} Because cost, complexity, privacy concerns around collecting labeled speech data, lot interest unsupervised representation learning speech. Of particular interest learn representations speech styles unsupervised data due difficulty describing prosody human labels. Some previous works aim learn global representations entire speech sequences. % Global style tokens learn dictionary embeddings speech without prosody labels. % As another example, Hsu et al. model disentangled speech styles hierarchy variational autoencoder . % Hu et al. proposed content style separation model pre-training single-speaker dataset text transcription minimizing mutual information content style representation. Other works try learn fine-grain localized representations speech. % apply self-supervised learning unlabeled speech data extract localized latent representations fine-tuned speech recognition. % FHVAE learns sequence high-level features applying VAE every frame. % leverages vector-quantized VAE learn discrete sequence representation speech. We propose framework learn global localized representation speech. In order disentangle content style representations, apply local encoder VQ layer learn discrete per-timestep representation speech captures linguistic contents global VAE extraction per-utterance representations reflect speech styles. We disentangle local global representations mutual information loss. We evaluate quality linguistic style representations running speech speaker recognition models reconstructed speech. We also show global representation captures speaker information well enough obtain speaker classification model training linear projection layer top global representation one example per speaker. In research work, propose novel algorithms Sandhi word formation Sandhi Split trained without use external resources language models, morphological phonetic analyzers, still manage match outperform existing approaches. Due simplicity models, computationally inexpensive train execute. In future intend extend current work internal Sandhi internal Sandhi-split using machine learning methods. Proposed models refined using additional training data well investigating techniques reduce errors current training data. The next two lines define bibliography style used, bibliography file."," We present an approach for unsupervised learning of speech representation disentangling contents and styles. Our model consists of:  a local encoder that captures per-frame information;  a global encoder that captures per-utterance information; and  a conditional decoder that reconstructs speech given local and global latent variables. Our experiments show that  the local latent variables encode speech contents, as reconstructed speech can be recognized by ASR with low word error rates , even with a different global encoding;  the global latent variables encode speaker style, as reconstructed speech shares speaker identity with the source utterance of the global encoding. Additionally, we demonstrate an useful application from our pre-trained model, where we can train a speaker recognition model from the global latent variables and achieve high accuracy by fine-tuning with as few data as one label per speaker. % % \rpang{How about: Our deep generative model consists of:  a local encoder that captures per-frame information;  a global encoder that captures per-utterance information; and  a conditional decoder that reconstruct speech given local and global latent variables, potentially extracted from different utterances. Our experiments show that  the local latent variables encode speech contents, since reconstructed speech can be recognized by ASR with low word error rates , even with a different global encodings;  the global latent variables encode speaker style, as reconstructed speech shares speaker identity with the source utterance of the global encoding and a speaker recognition model can be trained from the global latent variables with as few as one supervised example per speaker.  % }"
"%Sentiment analysis one fundamental tasks natural language processing aims find attitude author expressed his/her sentence. One important sub-tasks SA aspect based sentiment analysis goal find sentiment polarity toward specific aspect mentioned sentence. Due importance ABSA, several sub-tasks proposed studied problem, including aspect category extraction, aspect term extraction, opinion word extraction opinion summarization . Among sub-tasks, Targeted Opinion Word Extraction important sub-task might provide useful information explain prediction sentiment polarity ABSA system. In particular, goal TOWE find words express attitude author toward specific target mentioned sentence. For instance, sentence ``The food good, especially basic dishes, drinks delicious"", word ``good"" opinion word target ``food"" delicious opinion word target word ``drinks"". Among different applications, TOWE used target-oriented sentiment analysis pair-wise opinion summarization . %Sentiment analysis one fundamental tasks natural language processing aims find attitude author expressed his/her sentence. One important sub-tasks SA aspect based sentiment analysis goal find sentiment polarity toward specific aspect mentioned sentence. Due importance ABSA, several sub-tasks proposed studied problem, including aspect category extraction, aspect term extraction, opinion word extraction opinion summarization . Among topics, Targeted Opinion Word Extraction important task might provide useful information explain and/or improve sentiment polarity prediction ABSA systems. In particular, given target word input sentence, goal TOWE find words sentence help express attitude author toward aspect represented target word. For instance, sentence ``The food good, especially basic dishes, drinks delicious"", ``good"" opinion word target word ``food"" opinion words target word ``drinks"" would involve ``delicious''. Among different applications, TOWE finds application target-oriented sentiment analysis pair-wise opinion summarization . %Targeted Opinion Word Extraction important task aspect based sentiment analysis sentiment analysis . Given target word input sentence, goal TOWE find words sentence help express attitude author toward aspect represented target word. For instance, sentence ``The food good, especially basic dishes, drinks delicious"", ``good"" opinion word target word ``food"" opinion words target word ``drinks"" would involve ``delicious''. As opinion words might provide useful information explain and/or improve sentiment prediction ABSA systems, TOWE applied different problems, including target-oriented sentiment analysis pair-wise opinion summarization . Targeted Opinion Word Extraction important task aspect based sentiment analysis sentiment analysis . Given target word input sentence, goal TOWE identify words sentence help express attitude author toward aspect represented target word. For instance, running example, sentence ``All warranties honored XYZ disappointing."", ``disappointing"" opinion word target word ``warranties"" opinion words target word ``company"" would involve ``reputable''. Among others, TOWE finds applications target-oriented sentiment analysis opinion summarization . %As opinion words might provide useful information explain and/or improve sentiment prediction ABSA systems, TOWE applied different problems, including target-oriented sentiment analysis pair-wise opinion summarization . %A notable problem although related tasks TOWE extensively explored past, work explicitly consider TOWE problem literature . In particular, related task TOWE opinion word extraction aims locate terms used express attitude explicitly sentence . A key difference OWE TOWE OWE require opinion words tie target words sentence opinion words TOWE explicitly paired given target word. Note previous works also attempted jointly predict target opinion words ; however, target words still paired corresponding opinion words studies . %Among previous works TOWE, The early approach TOWE involved rule-based lexicon-based methods recent work focused deep learning models problem . One insights rule-based methods syntactic structures sentences provide useful information improve performance TOWE . However, syntactic structures exploited current deep learning models TOWE . Consequently, work, seek fill gap extracting useful knowledge syntactic structures help deep learning models learn better representations TOWE. In particular, based dependency parsing trees, envision two major syntactic information complementarily beneficial deep learning models TOWE, i.e., syntax-based opinion possibility scores syntactic word connections representation learning. First, syntax-based possibility scores, intuition closer words target word dependency tree input sentence tend better chance opinion words target TOWE. For instance, running example, opinion word ``disappointing"" sequentially far target word ``warranties"". However, dependency tree shown Figure , ``disappointing"" directly connected ``warranties"", promoting distance ``disappointing"" ``warranties"" dependency tree useful feature TOWE. Consequently, work, propose use distances words target word dependency trees obtain score represent likely word opinion word TOWE . These possibility scores would introduced deep learning models improve representation learning TOWE. In order achieve possibility score incorporation, propose employ representation vectors words deep learning models compute model-based possibility score word sentence. The model-based possibility scores also aim quantify likelihood opinion word word sentence; however, based internal representation learning mechanism deep learning models TOWE. To end, propose inject information syntax-based possibility scores models TOWE enforcing similarity/consistency syntax-based model-based possibility scores words sentence. The rationale leverage possibility score consistency guide representation learning process deep learning models generate effective representations TOWE. In work, employ Ordered-Neuron Long Short-Term Memory Networks obtain model-based possibility scores words sentences TOWE. ON-LSTM introduces two additional gates original Long Short-Term Memory Network cells facilitate computation model-based possibility scores via numbers active neurons hidden vectors word. %The second type syntactic information employed TOWE work considers dependency connections words sentence. %As deep learning models need compute representation vector word perform opinion word prediction TOWE, %While possibility scores aim improve representation vectors TOWE via syntax-based possibility features, second type syntactic information work seeks leveraging dependency connections words infer effective context words encoded representation vector word sentence. In particular, motivated running example, argue effective context words representation vector current word TOWE involve neighboring words current word target word dependency tree. For instance, consider running example ``warranties"" target word ``reputable"" word need compute representation vector. One one hand, important include information neighboring words ``reputable"" representation models know context current word . On hand, information target word also encoded representation vector ``reputable"" models aware context target word make appropriate comparison representation decide label ``reputable"" case. Note syntactic connection mechanism allows models de-emphasize context information ``I'' representation ``reputable"" improve representation quality. Consequently, work, propose formulate intuitions importance score matrix whose cells quantify contextual importance word would contribute representation vector another word given target word TOWE. These importance scores conditioned distances target word words dependency tree. Afterward, score matrix consumed Graph Convolutional Neural Network model produce final representation vectors opinion word prediction. For second type syntactic information work, main motivation improve representation vector computation word leveraging dependency connections words infer effective context words word sentence. In particular, motivated running example, argue effective context words representation vector current word TOWE involve neighboring words current word target word dependency tree. For instance, consider running example ``warranties"" target word ``reputable"" word need compute representation vector. On one hand, important include information neighboring words ``reputable"" representation models know context current word . On hand, information target word also encoded representation vector ``reputable"" models aware context target word make appropriate comparison representation decide label ``reputable"" case. Note syntactic connection mechanism allows models de-emphasize context information ``I'' representation ``reputable"" improve representation quality. Consequently, work, propose formulate intuitions importance score matrix whose cells quantify contextual importance word would contribute representation vector another word, given target word TOWE. These importance scores conditioned distances target word words dependency tree. Afterward, score matrix consumed Graph Convolutional Neural Network model produce final representation vectors opinion word prediction. Finally, order improve induced representation vectors TOWE, introduce novel inductive bias seeks explicitly distinguish representation vectors target-oriented opinion words words sentence. We conduct extensive experiments demonstrate benefits proposed model, leading state-of-the-art performance TOWE several benchmark datasets. %Finally, order improve induced representation vectors TOWE, introduce novel inductive bias seeks explicitly distinguish representation vectors target-oriented opinion words opinion words sentence . Extensive experiments conducted demonstrate benefits proposed model, leading state-of-the-art performance TOWE several datasets. %Finally, order improve induced representation vectors TOWE, introduce novel inductive bias seeks explicitly distinguish representation vectors target-related opinion words opinion words sentence . As target-related non-target opinion words used express opinion author , expect explicit representation distinction would help better separate two types opinion words based target word, eventually improving performance TOWE work. We conduct extensive experiments demonstrate benefits proposed model, leading state-of-the-art performance TOWE several datasets. %the close distance ``disappointing"" target word suggest models include information ``warranties"" representation vector ``disappointing"" long distance ``warranties"" ``reputable"" help prevent/mitigate representation vector ``reputable"". The presence information target word representation vectors help models successfully accept ``disappointing"" opinion word reject ``reputable"" case. %the close words target word would provide effective information induce representation vectors word sentence TOWE farther ones. %we argue syntactic neighboring words dependency tree would provide effective information induce representation vector word opinion word prediction. For instance, running example target word ``warranties"", close distance ``disappointing"" target word suggest models include information ``warranties"" representation vector ``disappointing"" long distance ``warranties"" ``reputable"" help prevent/mitigate representation vector ``reputable"". The presence information target word representation vectors help models successfully accept ``disappointing"" opinion word reject ``reputable"" case. %employ dependency connections words infer effective context words . %employ syntactic neighboring words compute representation vectors word sentence TOWE. %extends popular Long Short-Term Memory Networks introducing two additional gates hidden vector computation. These new gates controls long neuron hidden vectors activated across different time steps sentence . Based controlled neurons, model-based importance score word determined number active neurons word possesses operation ON-LSTM. To knowledge, first time ON-LSTM applied RE literature. %How encode syntax-based importance scores words deep model? In paper, propose employ syntax-based importance scores retain update information encoded representations word. In particular, words syntactically important retain information computation graph deep model information less important words discarded frequently. In order impose information update policy model, use new proposed architecture Ordered-Neuron Long Short-Term Memory . ON-LSTM extension well-known Long Short-Term Memory two additional gates . These new gates employed control frequency updating neuron across different time steps sentence. Concretely, values master forget input gates determine much information hidden vector LSTM cell retained updated based word current time step. Thereby, one infer importance scores inferred model using values master forget input gates. So, based characteristics ON-LSTM, encode syntax-based importance scores model, propose exploit syntax-based importance scores regulate model-based importance scores. Specifically, training time, encourage model-based scores consistent syntax-based importance scores. %the two words ``disappointing"" ``warranties"" directly connected other. %Early feature-based models shown syntactical structure sentence useful TWOE. More specifically, application dependency tree TOWE two fold: Pairwise Word Importance: Dependency tree useful infer relative importance word toward another word sentence. This relative importance could helpful TOWE attend important words target word. To infer pair-wise importance two words using dependency tree, one computes distance two words dependency tree. For instance, running example, sentence ``All warranties honored HP disappointing"", opinion word ``disappointing"" sequentially far target word ``warranties"". However, dependency tree shown Figure , two words ``disappointing"" ``warranties"" directly connected other. The short distance two words could helpful infer importance word ``disappointing"" target ``warranties"". Word Connection: Dependency tree could provide better contextual information word via connections word head dependants, thus helps improve word representations. Thereby, dependency tree could benefit TOWE. For instance, running example, head ``reputable"" ``company"" head ``warranties"" ``disappointing"". Therefore, would easier infer opinion word ``disappointing"" related target word ``warranties"" ``reputable"" irrelevant. %Besides difference rule-based deep learning models TOWE regarding representation learning methods, rule-based methods exploited syntactic structures sentences improve performance TOWE %In particular, related tasks TOWE involves target word extraction/aspect term exaction , opinion word extraction . A key difference OWE TOWE opinion words OWE general need tie target words sentence TOWE explicitly %Despite potential benefits, TOWE studied works past, characterizing early rule-based lexicon-based approaches recently deep learning models . %In literature, feature-based models deep learning model proposed target word extraction opinion word extraction . While joint models predict opinion target words, cannot pair them, thus unable solve task TOWE. In literature, works studied task TOWE, including early attempts rule-based lexicon-based approaches recent works deep learning models TOWE . We present framework learn disentangled representations speech unlabeled data. The framework includes local VQ encoder extract discrete sequence representation speech contents global VAE encoder learn continuous representation speech styles. Our evaluation shows discrete sequence representation effectively captures linguistic contents continuous global representation encapsulates speaker style. Additionally, also show application pre-trained model, successfully train speaker recognition system high accuracy one sample per speaker. \vfill\pagebreak References produced using bibtex program suitable BiBTeX files . The IEEEbib.bst bibliography style file IEEE produces unsorted bibliography list. -------------------------------------------------------------------------"," Targeted opinion word extraction  is a sub-task of aspect based sentiment analysis  which aims to find the opinion words for a given aspect-term in a sentence. Despite their success for TOWE, the current deep learning models fail to exploit the syntactic information of the sentences that have been proved to be useful for TOWE in the prior research. In this work, we propose to incorporate the syntactic structures of the sentences into the deep learning models for TOWE, leveraging the syntax-based opinion possibility scores and the syntactic connections between the words. We also introduce a novel regularization technique to improve the performance of the deep learning models based on the representation distinctions between the words in TOWE. The proposed model is extensively analyzed and achieves the state-of-the-art performance on four benchmark datasets.  %Deep learning models have been shown to achieve the state-of-the-art performance for TOWE in the recent studies.  %While previous feature-based models have shown syntactical structure  is useful for this task, recent deep neural nets ignore this information in their model. To address this limitation, in this paper, we propose a new approach which incorporates syntactical structure  into deep neural nets. More specifically, our model employs the dependency tree to capture the relative importance of the words to the aspect-term and to encode the connections between words. Our extensive experiments on four benchmark datasets prove the superiority of the proposed model, leading to new state-of-the-art results on all datasets. Moreover, detailed analysis shows the effectiveness of the components of the proposed model."
"Aspect-based Sentiment Analysis fine-grained version sentiment analysis aims find sentiment polarity input sentences toward given aspect. We focus term-based aspects ABSA aspects correspond terms input sentence. For instance, ABSA system able return negative sentiment input sentence ``The staff polite, quality food terrible.'' assuming ``food'' aspect term. %Aspect based sentiment analysis fine-grained version sentiment analysis . In ABSA, goal find sentiment polarity sentence toward given aspect. In literature, two versions aspect proposed: Aspect-category: Aspect categories set pre-defined categories given sentence contains opinion author toward one them. Aspect categories may explicitly appear sentence. Aspect-term: Aspect term subsequent sentence given sentence express sentiment toward it. For instance example The staff polite, quality food terrible., author positive sentiment toward aspect-category service negative sentiment toward aspect-term food. In paper, introduce novel model sentiment analysis toward aspect-term. %The early attempts ABSA performed feature engineering produce useful features statistical models problem . One limitation feature-based models require significant human effort linguistic background design effective features. In order overcome limitation, %The typical network architectures ABSA literature involve convolutional neural networks , recurrent neural networks , memory networks , attention gating mechanisms . %automatically induce effective features ABSA Due important applications , ABSA studied extensively literature. In studies, deep learning employed produce state-of-the-art performance problem . Recently, order improve performance, syntactic dependency trees integrated deep learning models ABSA . Among others, dependency trees help directly link aspect term syntactically related words sentence, thus facilitating graph convolutional neural networks enrich representation vectors aspect terms. %Although graph-based models achieved decent performance ABSA, models However, least two major issues graph-based models addressed boost performance. First, representation vectors words different layers current graph-based models ABSA customized aspect terms. This might lead suboptimal representation vectors irrelevant information ABSA might retained affect model's performance. Ideally, expect representation vectors deep learning models ABSA mainly involve related information aspect terms, important words sentences. Consequently, work, propose regulate hidden vectors graph-based models ABSA using information aspect terms, thereby filtering irrelevant information terms customizing representation vectors ABSA. In particular, compute gate vector layer graph-based model ABSA leveraging representation vectors aspect terms. This layer-wise gate vector would applied hidden vectors current layer produce customized hidden vectors ABSA. In addition, propose novel mechanism explicitly increase contextual distinction among gates improve representation vectors. %as hidden vectors different layers graph-based models tend capture different levels contextual information, gate vectors different layers also maintain level contextual distinction. To end, propose novel mechanism explicitly increase contextual distinction among gate vectors improve quality representation vectors. The second limitation current graph-based deep learning models failure explicitly exploit overall importance words sentences estimated dependency trees ABSA problem. In particular, motivation graph-based models ABSA neighbor words aspect terms dependency trees would important sentiment terms words sentence. The current graph-based models would focus syntactic neighbor words induce representations aspect terms. However, based idea important words, also assign score word sentences explicitly quantify importance/contribution sentiment prediction aspect terms. In work, hypothesize overall importance scores dependency trees might also provide useful knowledge improve representation vectors graph-based models ABSA. Consequently, propose inject knowledge syntax-based importance scores graph-based models ABSA via consistency model-based importance scores. In particular, using representation vectors graph-based models, compute second score word sentences reflect model's perspective importance word sentiment aspect terms. The syntax-based importance scores employed supervise model-based importance scores, serving method introduce syntactic information model. In order compute model-based importance scores, exploit intuition word would important ABSA similar overall representation vector predict sentiment sentence final step model. In experiments, demonstrate effectiveness proposed model state-of-the-art performance three benchmark datasets ABSA. In summary, contributions include: %, propose obtain another important score word sentence based representation vectors models. These model-based importance scores %In particular, ABSA, words might introduce useful information predict sentiment aspect terms words sentence %In particular, words given sentence might involve useful information relation prediction RE words, dependency tree sentence help better identify important words assign higher importance scores . We expect introducing importance information words deep learning models might lead improved performance RE. Consequently, work, propose obtain importance score word sentences dependency trees . These serve general tree representation incorporate syntactic information deep learning models RE. %In particular, aspect terms important words sentences ABSA, %Syntactical structure, e.g., dependency tree, could shorten distance syntactically related words thus improve contextualized representation words. In order incorporate syntactical tree deep models, recent work mainly employs graph convolutional network model interaction words based syntactic tree. In order emphasize given aspect term, current models use representation aspect term generated GCN either directly final classification gate filter features sequential model. However, methods cannot benefit information given aspect term control information flow graph based model. Moreover, expected words syntactically related given aspect term convey information sentiment toward it. Unfortunately, non existing work considers relative importance final representation sentence. In order address issue, propose new graph based model employs semantic given aspect term control interaction nodes/words GCN model emphasize syntactically important words final representation sentence. %Due application ABSA downstream applications, e.g., opinion mining, gained lot attention natural language processing community several methods proposed task. Early attempts employed feature engineering extract useful features statistical models like SVM . These methods require extensive human effort strong linguistic knowledge. They also suffer low generalization ability. Due limitations, neural networks deep models superseded feature based models obtain promising results ABSA . Early deep models ABSA exploited sequential models , convolutional neural nets even memory networks . In order improve performance, attention gating mechanism also widely adopted deep models. Recently, shown syntactical information could also improve performance deep models . Syntactical structure, e.g., dependency tree, could shorten distance syntactically related words thus improve contextualized representation words. In order incorporate syntactical tree deep models, recent work mainly employs graph convolutional network model interaction words based syntactic tree. In order emphasize given aspect term, current models use representation aspect term generated GCN either directly final classification gate filter features sequential model. However, methods cannot benefit information given aspect term control information flow graph based model. Moreover, expected words syntactically related given aspect term convey information sentiment toward it. Unfortunately, non existing work considers relative importance final representation sentence. In order address issue, propose new graph based model employs semantic given aspect term control interaction nodes/words GCN model emphasize syntactically important words final representation sentence. %In particular, paper, propose novel model employs representation given aspect term compute gate. This gate applied output one layer GCN. By so, information represented aspect term would erase non-relevant information node/word obtained interaction neighbors one aggregation step GCN. As different layers GCN capture different substructure graph, e.g., 1-hop vicinity vs 2-hop vicinity, propose exploit different gates different layers. To ensure gates different layers same, propose novel method encourage diversity among gates different layers GCN. Moreover, addition exploiting semantic aspect term control interactions nodes/words GCN, paper, propose encourage model emphasize words syntactically important aspect term. In particular, use distance word aspect term dependency tree indication syntactic importance word aspect term. This importance employed supervision signal encourage model emphasize words syntactically important aspect term. This obtained final layer model sentiment prediction performed. More specifically, first estimate semantic importance word employing final representation word input classifier predict label distribution compute KL-Divergence label distribution predicted word representation label distribution predicted sentence representation. If two label distribution similar, shows word representation contains information model consumes perform final classification. Finally, order ensure words syntactically important aspect term semantically important model too, decrease divergence distribution syntactic score semantic score word via KL-Divergence two distributions. %Our extensive experiments three benchmark datasets, empirically prove effectiveness proposed model leading new state-of-the-art results three benchmark datasets. A novel method regulate GCN-based representation vectors words using given aspect term ABSA. A novel method encourage consistency syntax-based model-based importance scores words based given aspect term. Extensive experiments three benchmark datasets ABSA, resulting new state-of-the-art performance datasets. We propose novel deep learning model TOWE seeks incorporate syntactic structures sentences model computation. Two types syntactic information introduced work, i.e., syntax-based possibility scores words syntactic connections words . We also present novel inductive bias improve model, leveraging representation distinction words TOWE. Comprehensive analysis done demonstrate effectiveness proposed model four datasets. Our comprehensive analysis model architecture together extensive experiments four benchmark datasets demonstrate effectiveness proposed model. More specifically, syntactic structure employed infer target-related importance words incorporated model using newly proposed architecture Ordered-Neuron LSTM . Moreover, employed GCN encode word connections dependency tree. To overcome noisy non-relevant connections dependency tree, propose learn dense graph dependency tree customized given target word. Finally, introduce syntax-based regularization preserve target-related information model. Our comprehensive analysis model architecture together extensive experiments four benchmark datasets show effectiveness proposed model, establishing new state-of-the-art results datasets. In work, study target-oriented opinion word extraction problem , one sub-tasks aspect-based sentiment analysis . We propose deep learning model incorporate syntactic structure model computations. More specifically, syntactic structure employed infer target-related importance words incorporated model using newly proposed architecture Ordered-Neuron LSTM . Moreover, employed GCN encode word connections dependency tree. To overcome noisy non-relevant connections dependency tree, propose learn dense graph dependency tree customized given target word. Finally, introduce syntax-based regularization preserve target-related information model. Our comprehensive analysis model architecture together extensive experiments four benchmark datasets show effectiveness proposed model, establishing new state-of-the-art results datasets."," Aspect-based Sentiment Analysis  seeks to predict the sentiment polarity of a sentence toward a specific aspect. Recently, it has been shown that dependency trees can be integrated into deep learning models to produce the state-of-the-art performance for ABSA. However, these models tend to compute the hidden/representation vectors without considering the aspect terms and fail to benefit from the overall contextual importance scores of the words that can be obtained from the dependency tree for ABSA. In this work, we propose a novel graph-based deep learning model to overcome these two issues of the prior work on ABSA. In our model, gate vectors are generated from the representation vectors of the aspect terms to customize the hidden vectors of the graph-based models toward the aspect terms. In addition, we propose a mechanism to obtain the importance scores for each word in the sentences based on the dependency trees that are then injected into the model to improve the representation vectors for ABSA. The proposed model achieves the state-of-the-art performance on three benchmark datasets.  %These models employ graph based neural nets to incorporate syntactical structure into the model. However, they ignore the aspect term information to control the interaction between words in the syntax tree which is modeled by graph neural net.  % Moreover, they neglect the consistency between the syntactic and semantic importance of the words toward the given aspect. %Moreover, the relative importance of the words to the given aspect term based on their syntactical role is neglected in the final representation produced by the existing syntax-aware models. To address these two issues, in this paper, we introduce a new syntax-aware model which incorporates gating mechanism to control information flow in the graph based model using the given aspect term. It also ensures the words that are syntactically important to the aspect term are more pronounced in the final representation of the sentence. Our extensive experiments on three benchmark datasets empirically prove the effectiveness of the proposed model leading to new state-of-the-art results on all three benchmark datasets."
"Entity normalization variant generation fundamental variety tasks semantic search relation extraction . Given entity name , goal entity normalization convert canonical form , goal entity variant generation convert set different textual representations refer entity E . \looseness=-1 Typically, entity normalization variant generation done first performing entity linking , i.e., matching entity names appearing context named entities curated knowledge bases , use canonical form variations residing KBs complete tasks. Unfortunately, scenarios, search , entity names surrounded context. Furthermore, specialized domain-specific applications, may knowledge base govern names relevant entities. Thus, entity linking always applicable. In paper, take view %the problem differently, particular, argue entity normalization variant generation done without contextual information external KBs understand internal structures entity names. % Fundamental success entity linking availability contextual information ontological information . % external KBs use master datasets match input entity names. \todo{One may argue DBpedia Wikipedia good proxy. It may useful talk related work taking view here.} % For example, searching \example{General Electric Company}, need also consider variations like \example{GE Co.}, \example{G.E.}, \example{General Electric}, etc. without relying contextual information external KBs. % Performing entity normalization variant generation contextless fashion extremely challenging surface forms entity names. % Several attempts made parse structured representation entity names. As observed , entity names often %structured representation implicit structures exploited solve entity normalization variant generation. Table shows manipulate structured representations entity names generate different variations without help context external knowledge. % For example, know \example{Michael} \component{first} \example{Jordan} \component{last} \example{Michael Jordan}, generate two variations: \example{M. Jordan} \example{Jordan, Michael} . \end{table*} Declarative frameworks proposed allow %high-skill developers manually specify rules parse entity names %the structured representation. %of enity names. To avoid low-level manual effort, used fully supervised methods identifying nested entities embedded flat named entities. Unfortunately, labeled data rarely available leverage methods real-world. To mitigate need training data, proposed active learning %-based system, LUSTRE, semi-automatically learn rules mapping entity names structured representations. By %making use using regex-based extractors list comprehensive dictionaries capture crucial domain vocabularies, LUSTRE generate rules achieve SoTA results. However, complex realistic scenarios, dictionaries may available regex-based extractors alone expressive enough. Moreover, shown Section, LUSTRE cannot handle long entities machine logs. In paper, present %learning framework learns high-quality BERT-CRF models parsing entity names structured representations %of entity names low-resource settings, namely, labeled data available. The proposed framework essentially active learning-based approach learns human interactions. We believe comprehensible user interfaces essential active learning-based approaches, especially labeling tasks require non-trivial human labels . Therefore, developed system named PARTNER implements framework. We designed interface PARTNER similar LUSTRE, also made major modifications user friendly. Interested readers find video demo PARTNER \url{http://ibm.biz/PARTNER}. Our main contributions include: % \todo{Low-resource setting mean different things. It would helpful clearly describe mean here.} % { % \squishlist % \item We developed full-fledged system built upon effective framework learn high-quality BERT-CRF models parsing structured representation entity names without contextual information . % \item To minimize human effort, framework combines active learning weak supervision, usually applied isolation. % \item Both datasets system made publicly available. % \squishend % } { \squishlist \item %We propose A hybrid framework combining active learning weak supervision effectively learn BERT-CRF-based models low human effort. \item %We developed A full-fledged system, intuitive UI, implements framework. \item Comprehensive experimental results showing framework learns high-quality models merely dozen labeled examples. \squishend } \medskip Related work. Our problem related flat nested named entity recognition . However, discussed , NER focuses identifying outermost flat entities completely ignores internal structured representations. identify nested entities within context using fully supervised methods require large amounts labeled data, whereas goal learn labels contextless fashion. Active learning weak supervision widely adopted solving many entity-centric problems, entity resolution , NER , entity linking . While power combination two techniques demonstrated domains , best knowledge, two approaches usually applied isolation prior entity-related work. Recently, data programming approaches use labeling functions/rules generate weak labels train machine learning models low-resource scenarios. Data programming approaches like Snorkel usually assume labeling functions manually provided users, indicating target users must programming skills order provide labeling functions. In contrast, goal minimize human effort lower human skills . % Named entity recognition % Our problem similar NER fine-grained version nested NER several key differences: labeled data available settings; focus scenarios entity names . Recently, proposed active learning based approaches NER low-resource settings. Following direction, enhance active learning weak supervision reduce labeling efforts. % Understanding entity names important task many entity-centric tasks entity disambiguation information retrieval. Computing textual similarity two entity names one widely used methods tell whether name . However, entity names highly ambiguous text-based similarity functions robust enough resolve complex cases . Consider following date entities: % \smallskip % { % {\small % \squishlist % \item [] ``December first, nineteen nineteen"", % \item [] ``December first, nineteen ninety"", % \item [] ``1919-12-01"". % \squishend % } % } % \smallskip % Two different entities may textually similar ), entity may textually dissimilar ). % Entity names merely sequences characters, usually internal semantic structures useful understanding different name variations. For example, identify \example{December} \component{Month}, \example{first} \component{Day}, \example{nineteen nineteen} \component{Year}, transform components separately assemble transformed components according standardized format, \component{Year}-\component{Month}-\component{Day}, translate . % Currently, named entity recognition subsequent entity disambiguation task either treated separately , treated one joint task looking unstructured text entities extracted linking reference knowledge base . However, tasks , entities given without context . % Enriching entities normalized form variations obtained manipulating semantic structures helpful tasks. % Several attempts made understand entity name structures. proposed declarative frameworks allow high-skill developers manually specify rules translate entity names semantic structures. To avoid labor-intensive clearly scalable manual process, proposed active learning-based framework named LUSTRE semi-automatically learns parsing rules. However, availability list comprehensive, accurate, complete dictionaries crucial success LUSTRE. % \looseness=-1 Understanding entity name structures viewed sequence labeling problem. Recently, deep learning-based approaches shown achieve state-of-the-art performance sequence labeling problems . One foundations approaches use pre-trained character word embeddings carry semantic information learned large text corpus. However, deep learning-based approaches data hungry. This work validates supports existing literature curriculum learning. Our results confirm curriculum learning methods supervised learning lead faster convergence better local minima, measured test set performance . We shown replacing heuristic difficulty theoretically-based, learned difficulty value training examples, static curriculum learning strategies improved. We also proposed \modelabbr, first curriculum learning method dynamically probe model training estimate model ability point time. Knowing model's ability allows data selected training appropriate model rigidly tied heuristic schedule. \modelabbr~trains effective models cases considered, particularly randomly initialized LSTM models. Based experiments, report mixed results stated hypotheses. Replacing heuristics learned difficulty values leads improved performance training models curriculum learning, supporting H1. \modelabbr~does outperform training setups used train LSTM models. Results mixed used fine-tune model. Therefore H2 partially supported. We see similarly mixed results evaluating efficiency. With fine-tuning, fully supervised fine-tuning usually efficient, number fine-tuning epochs needed already low. For LSTM, \modelabbr~is efficient curriculum learning strategies, efficient overall two six tasks. H3 partially supported results. There limitations work inform future work. Strategies learning difficulties artificial crowds identifying necessary number examples estimate ability need study identify optimal strategies. For example, recent work shown 85\ accuracy optimal learning rate target . \modelabbr~can used dynamically select data maintain learning rate 85\ . Even though dynamic, \modelabbr~employs simple schedule: include examples . However, able estimate ability fly \modelabbr~opens exciting new research direction: best way build curriculum, knowing example difficulty model ability ? Our results also showed ``best'' heuristic competency-based baselines across data sets. In four data sets , using learned difficulty IRT outperformed using sentence length difficulty heuristic, new result. A key contribution DDaCLAE selecting rate schedule difficulty heuristic longer needed. Gathering response patterns estimate difficulty expensive, one-time cost. Learned difficulty estimates shared data set features. Identifying best way generate artificial response patterns, including parallelization, open question. By releasing learned difficulty values GLUE data sets hope encourage use non-heuristic curriculum learning also future IRT evaluations. It may case data difficulty within range ability better, training set shifts model improves. There many directions future work, exciting area work moving forward. Re-add camera ready"," \looseness=-1  %Entity names usually have structured representation that is useful for many entity-related tasks such as entity normalization and variant generation. Learning the structured representation of entity names in low-resource settings without context and external knowledge bases is challenging. In this paper, we present a novel learning framework that combines active learning and weak supervision to solve this problem, and we experimentally show that our method can learn high-quality BERT-CRF models in low-resource settings. A video demo of a system that implements this framework is included in supplementary materials. Structured representations of entity names are useful for many entity-related tasks such as entity normalization and variant generation. Learning the implicit structured representations of entity names without context and external knowledge  is particularly challenging. In this paper, we present a novel learning framework that combines active learning and weak supervision to solve this problem. Our experimental evaluation show that this framework enables the learning of high-quality models from merely a dozen or so labeled examples."
"Coreference resolution task grouping mentions text refer real-world entity clusters . %The task important prerequisite variety natural language processing systems, textual entailment information extraction . Coreference resolution difficult task %since requires reasoning, context understanding, background knowledge real-world entities, %Therefore, task driven research natural language processing machine learning, particularly since release ontonotes multilingual corpus providing annotated coreference data Arabic, Chinese English used 2011 2012 {\CONLL} shared tasks . Since then, substantial research English coreference, recently using neural coreference approaches , leading significant increase %that significantly increased performance coreference resolvers English. % % The general objective %the research described % paper close evident gap recent literature coreference. By contrast, almost research Arabic coreference; performance Arabic coreference resolution improved much since {\CONLL} 2012 shared task, particular neural architectures proposed--the current state-of-the-art system remains model proposed %feature-based . In paper close obvious gap proposing knowledge first neural coreference resolver Arabic. One explanation lack research might simply lack training data large enough task. Another explanation might Arabic problematic English %more complex English rich morphology, %rich proprieties, %has many dialects, and/or %contains high degree ambiguity. We explore first possibilities. %Our proposal address aspects. %one aspect problem. % Another explanation might lack large-size training data task. % We explore Coreference resolution divided two subtasks--mention detection mention clustering--as illustrated %an example two steps Figure . % % Coreference resolution difficult task % %since % requires reasoning, context understanding, background knowledge real-world entities, % % %Therefore, task % driven research natural language processing machine learning, particularly since release ontonotes multilingual corpus providing annotated coreference data Arabic, Chinese English . % %and various approaches applied. In early work, coreference's two subtasks usually carried pipeline fashion , candidate mentions selected prior mention clustering step. Since introduced end-to-end neural coreference architecture achieved state art carrying two tasks jointly, first proposed , state-of-the-art systems followed approach. % first end-to-end coreference system solves two subtasks jointly. % This leads number subsequent systems significantly increased coreference resolution performance English. % By contrast, little developments Arabic coreference resolution, performance Arabic coreference resolution improve much since CoNLL 2012 shared task current state-of-the-art system remain feature-based . However, end-to-end solution attempted Arabic. We intend explore whether end-to-end solution would practicable corpus limited size. % One explanation might Arabic complex English morphologically rich proprieties, many dialects, contains high degree ambiguity. % Another explanation might lack large-size training data task. The approach followed adapt %In work, introduce recipe show state-of-the-art English coreference resolution architecture Arabic %can adapted Arabic language follows. We started strong baseline system , enhanced contextual {\BERT} embeddings . We explored three methods improving model's performance Arabic. %In total evaluated three options, The first method pre-process Arabic words heuristic rules. We follow normalize letters different forms, removing diacritics. This results substantial improvement 7 percentage points baseline. The second route replace multilingual {\BERT} {\BERT} model trained Arabic texts . Multilingual {\BERT} trained 100+ languages; result, optimized them. %needs balance tread languages. As shown , monolingual {\BERT} trained Arabic texts better performance various {\NLP} tasks. We found holds coreference: %resolution task, using embeddings monolingual {\BERT}, model improved {\CONLL} F1 4.8 percentage points. Our third step leverage end-to-end system separately trained mention detector . We show better mention detection performance achieved using separately trained mention detector. And using hybrid training strategy end-to-end pipeline approaches system gains additional 0.8 percentage points. Our final system achieved {\CONLL} F1 score 63.9\%, 15\% previous state-of-the-art system Arabic coreference {\CONLL} dataset. Overall, show state-of-the-art English coreference model adapted Arabic coreference leading substantial improvement performance compared previous feature-based systems. Building trust confidence agents conversational interfaces requires smooth dialogue avoids breakdown. Detecting dialogue breakdown, either happens, essential part ensuring users satisfactory experiences agents. We investigate two semi-supervised learning methods leverage unlabelled data improve breakdown detection, including continued pre-training Reddit dataset SSMBA data augmentation. We utilize methods submission 5th Dialogue Breakdown Detection Challenge, beating baselines submissions large margin. In ablations previous test sets, show addition semi-supervised methods improves baseline models 2\ accuracy reduces JS divergence 0.003. These methods simple applicable dialogue task. In future continue investigate applying methods intent prediction, slot filling, state tracking language generation.","  No neural coreference resolver for Arabic exists, in fact we are not aware of any learning-based coreference resolver for Arabic since \cite{anders:2014}.  In this paper, we introduce a coreference resolution system for Arabic based on Lee et al's end-to-end architecture combined with the Arabic version of {\BERT} and an external mention detector.  As far as we know, this is the first neural  coreference resolution system aimed specifically to Arabic, and it substantially outperforms the existing state-of-the-art on  OntoNotes 5.0 with a gain of 15.2 points {\CONLL} F1.   We also discuss the current limitations of the task for Arabic and possible approaches that can tackle these challenges.  %\footnote{Our code is available at \url{https://github.com/juntaoy/aracoref}.}  \blfootnote{     % % final paper: en-us version     \hspace{-0.65cm}  % space normally used by the marker     This work is licensed under a Creative Commons     Attribution 4.0 International License.     License details:     \url{http://creativecommons.org/licenses/by/4.0/}. } \let\thefootnote\relax\footnotetext{* Equal contribution. Listed by alphabetical order.}"
"Deep architectures emotion recognition speech growing research field . Using short time signal analysis, speech utterance represented matrix size time dimension size spectral dimension. The sequence sequence layers % model spectral phenomena keep size time dimension without modification. A sequence vector layer used convert sequence fixed dimension vector fed feed forward dense layers. The global average pooling, global max pooling attention common choices type layer. %Feed forward layers used improve modeling power classifier. Fully-connected dense layers used apply nonlinear compression input features better representation improves modeling power classifier. A multiclass classifier implemented using softmax layer. Typically, model trained using cross-entropy objective function. Convolutional neural networks recently used many emotion recognition tasks. For example, CNNs designed visual recognition directly adapted emotion recognition speech. Moreover, study , conducted extensive experiments using attentive CNN multi-view learning objective function using Interactive Emotional Dyadic Motion Capture database . They concluded CNN architecture, particular choice features important model architecture, amount kind training data. %CNNs example sequence sequence layers extremely fast training classification phases. CNNs excellent feature extraction fast training compared standard sequence modeling. Long short-term memory networks sequence sequence layers excellent capturing sequential phenomena speech signal various style speaking. In study , propose solution problem ontext-aware emotional relevant feature extraction, combining CNNs LSTM networks, order automatically learn best representation speech signal directly raw time representation. They use commonly hand-engineered features, mel-Frequency cepstral coefficients perceptual linear prediction coefficients. Their end-to-end system targeted learn intermediate representation raw input signal automatically better suits task hand hence leads better performance. Both CNN LSTM networks shown significant improvements fully-connected neural network across wide variety tasks. In recent work , took advantage CNNs, LSTMs DNNs combining one unified architecture speech recognition task. CNNs good reducing frequency variations, LSTMs good temporal modeling, finally DNNs map features separable space. Their CLDNN provided 4-6\% relative improvement WER. In similar work emotion recognition speech , combination CNNs LSTMs led improvements classification accuracy. The last state LSTM used sequence vector conversion. Recently, end-to-end multimodal emotion gender recognition model dynamic joint loss weights developed . The proposed model need pre-trained features audio visual data. In addition, system trained using multitask objective function weights assigned using dynamic approach. In paper, build contributions develop emotion recognition system Arabic data using recently introduced KSU emotions corpus\footnote{https://catalog.ldc.upenn.edu/LDC97S45}. Our contributions are: Introducing novel approach emotion recognition using attention based CNN-LSTM-DNN architecture; Studying deep CNN models task; Comparing results published state-of-the art results IEMOCAP database Providing scripts code research community usage potential future contributions\footnote{http://github.com/qcri/deepemotion} %In paper, build previous contributions develop system first time using attention based CNN-LSTM-DNN architecture emotion recognition. In addition, second architecture based deep CNN models developed. In study, benchmark results using recently introduced KSU Emotions%\footnote{https://catalog.ldc.upenn.edu/LDC97S45} , comprised approximately five hours emotional Modern Standard Arabic speech 23 speakers, see section details. The results Arabic speech emotion recognition task shows two approaches led similar accuracy results deep CNN models significantly faster attention based CNN-LSTM-DNN models training classification phases. The rest paper organized follows: In section 2, describe attention-based CNN-LSTM-DNN proposed deep CNN architectures. Data explained section 3. Experimental setup illustrated section 4. This followed results section 5. Finally section 6 concludes paper discusses future work. %Speech emotion recognition active area research improve man-machine interface. The task aims classify utterance discrete emotion labels . It may challenging task since individuals express emotions differently due lack large datasets train machine learning models. %Deep learning specially convolutional neural network became dominant approach classify detect speech emotions . The CNN layers provide efficient method extract features speech. With help fully connected dense layers, possible contract powerful emotion classifiers. Recently, attention layers used CNN improve classification accuracy . In paper, modernize Arabic coreference resolution task adapting state-of-the-art English coreference system Arabic language. We start strong baseline system introduce three methods effectively enhance performance Arabic coreference resolution. Our final system enhanced three methods achieved {\CONLL} F1 score 63.9\ improved state-of-the-art result Arabic coreference resolution task 15 percentage points."," Emotion recognition from speech signal based on deep learning is an active research area. Convolutional neural networks  may be the dominant method in this area. In this paper, we implement two neural architectures to address this problem.  The first architecture is an attention-based CNN-LSTM-DNN model. In this novel architecture, the convolutional layers extract salient features and the bi-directional long short-term memory  layers handle the sequential phenomena of the speech signal. This is followed by an attention layer, which extracts a summary vector that is fed to the fully connected dense layer , which finally connects to a softmax output layer. The second architecture is based on a deep CNN model. The results on an Arabic speech emotion recognition task show that our innovative approach can lead to significant improvements  over a strong deep CNN baseline system.  On the other hand, the deep CNN models are significantly faster than the attention based CNN-LSTM-DNN models in training and classification.% phases.  %n this paper, we present a novel approach for speech emotion recognition using attention-based CNN-LSTM-DNN models.  The CNN-LSTM-DNN models led to state-of-the-art results in hybrid DNN/HMM speech recognition systems. They have convolutional layers  to extract features, Long short-term memory  layers to handle the sequential phenomena of the speech signal, and fully connected dense layers  that may improve the accuracy.  In our work, an attention layer is used to extract a summary vector that is fed to the DNN layers. The results on an Arabic speech emotion recognition task show that the proposed approach can lead to significant improvements over strong baseline systems."
"The following instructions directed authors papers submitted EMNLP 2020 accepted publication proceedings. All authors required adhere specifications. Authors required provide Portable Document Format version papers. The proceedings designed printing A4 paper. In paper, designed end-to-end attention-based CNN-LSTM-DNN emotion classifier. In classifier, convolutional layers extract salient features, bi-directional long short-term memory layers handle sequential phenomena speech signal. This followed attention layer, extracts summary vector fed fully connected dense layer , finally connects softmax layer. The results Arabic speech emotion recognition task show innovative approach lead significant improvements strong deep CNN baseline system. However, deep CNN models significantly faster attention-based CNN-LSTM-DNN models training classification phases. Future work focus training ensemble classifier interpolating predictions improve classification accuracy. We plan use large Arabic emotion databases using powerful attention-based CNN-LSTM-DNN models. In addition, joint estimation emotion, dialect, language, accent using multitask learning investigated. In addition, separate label per frame methods developed compared single label per utterance methods commonly used field unified framework. \pagebreak"," Text classification is a critical research topic with broad applications in natural language processing. Recently, graph neural networks  have received increasing attention in the research community and demonstrated their promising results on this canonical task. Despite the success, their performance could be largely jeopardized in practice since they are:  unable to capture high-order interaction between words;  inefficient to handle large datasets and new documents. To address those issues, in this paper, we propose a principled model -- hypergraph attention networks , which can obtain more expressive power with less computational consumption for text representation learning. Extensive experiments on various benchmark datasets demonstrate the efficacy of the proposed approach on the text classification task."
"Publicly available biomedical articles keep increasing rapidly. Automated systems utilize biomedical text mining methods necessary able handle large amount data minimal manual effort. An important first step biomedical text mining method detection classification biomedical entities disease, drug chemical mentions biomedical articles. This task referred biomedical named entity recognition . BioNER seen remarkable progress advents machine learning deep learning methods. These methods require labeled datasets benefit increasing amount labeled examples. Artificial neural networks form core almost state-of-the-art bioNER systems. The main drawback methods networks must trained scratch dataset, separately. Even though recent progress BioNER promising, overall performance significantly lower general domain NER. This mainly due scarcity sub-optimal utilization labeled datasets biomedical domain. Transfer learning training paradigm mitigates mentioned issues current approaches. It attempts utilize information obtained source task improve performance target task. Transfer learning shown especially useful size labeled data limited target task, making BioNER suitable target task. Multi-task learning special case transfer learning multiple tasks learned simultaneously. In context, corresponds learning multiple biomedical named entity datasets using single neural network. Recently, seminal work Devlin et al., i.e. BERT model, enabled progress various NLP tasks, including NER. BERT uses self-supervised learning relieves need labeled examples train neural network. Lee et al. proposed BioBERT, BERT model pretrained large unlabeled biomedical dataset. They finetuned BioBERT model labeled datasets using supervised learning obtained improvements several downstream biomedical NLP tasks. Yet, BioBERT applied context multi-task learning, best knowledge. This motivated us use BioBERT shared network across biomedical datasets. We claim sharing information across datasets help improve overall performance representations obtained one biomedical dataset relevant others, even though annotated entities different. Multi-task learning also used recently improve performance BioNER datasets. Yet, analysis improvements come limited effect transfer learning unclear. Thus, lack theoretical understanding transfer learning multi-task learning bring improvements. In study, analyze effect multi-task learning biomedical named entity recognition. To end, experimented seven BioNER benchmark datasets analyzed effect multi-task learning using ten different measures. We evaluate usefulness measures three different metrics. Besides, propose combining transfer learning multi-task learning BioNER employed best knowledge. The main contributions study follows: This paper presents comprehensive review text style transfer deep learning methods. We surveyed recent research efforts TST developed schemes categorize distill existing literature. This survey covered task formulation, evaluation metrics, methods parallel non-parallel data. We also discussed several important topics TST, connection NLP tasks, important future directions. This survey provide reference future researchers working TST."," Developing high-performing systems for detecting biomedical named entities has major implications. State-of-the-art deep-learning based solutions for entity recognition often require large annotated datasets, which is not available in the biomedical domain. Transfer learning and multi-task learning have been shown to improve performance for low-resource domains. However, the applications of these methods are relatively scarce in the biomedical domain, and a theoretical understanding of why these methods improve the performance is lacking. In this study, we performed an extensive analysis to understand the transferability between different biomedical entity datasets. We found useful measures to predict transferability between these datasets. Besides, we propose combining transfer learning and multi-task learning to improve the performance of biomedical named entity recognition systems, which is not applied before to the best of our knowledge."
"Aspect-based sentiment analysis fine-grained sentiment analysis task, analyzes sentiment opinions toward given aspect sentence. The task consists set subtasks, including aspect category detection, aspect term extraction, aspect-level sentiment classification , aspect-oriented opinion words extraction , etc. Most existing researches perform certain subtask ABSA training machine learning algorithms labeled data. However, public corpora ABSA small-scale due expensive labor-intensive manual annotation. Scarce training data limits performance data-driven approaches ABSA. Therefore, interesting valuable research question mine exploit internal connections ABSA subtasks achieve goal facilitating simultaneously. In work, focus two subtasks ALSC AOWE highly mutually indicative. We first introduce briefly presenting motivations. Aspect-level sentiment classification aims predict sentiment polarity towards given aspect sentence. As Figure shows, two aspects mentioned sentence ``waiters unfriendly pasta world.'', namely ``waiters'' ``pasta''. The sentiments expressed towards aspect negative positive respectively. Different ALSC, aspect-oriented opinion words extraction recently proposed ABSA subtask. The objective task extract corresponding opinion words towards given aspect sentence. Opinion words refer word/phrase sentence used express attitudes opinions explicitly. In example above, ``unfriendly'' opinion word towards aspect ``waiters'', ``out world'' opinion words towards aspect ``pasta''. It common sense positive opinion words imply positive sentiment polarity, negative opinion words correspond negative sentiment polarity. Inspired common sense, find corresponding opinion words toward given aspect help infer corresponding sentiment . Correspondingly, sentiment determined ALSC also provide clues help extract polarity-related opinion words AOWE task. Therefore, goals AOWE ALSC mutually indicative benefit other. To exploit relation mutual indication, propose novel model, Opinion Transmission Network , jointly model two tasks ALSC AOWE finally improve simultaneously. Overall, OTN contains two base modules, namely attention-based ALSC module CNN-based AOWE module, two tailor-made opinion transmission mechanisms, respectively AOWE ALSC ALSC AOWE. Specifically, utilize extracted results AOWE complementary opinions information inject ALSC module form additional attention. To successfully transmit implicit opinions ALSC AOWE, unearth features attention layer ALSC module keep abundant useful aspect-related opinions, utilized facilitate AOWE. It worth noting proposed model works without requiring simultaneous annotations AOWE ALSC data, thus applied practical scenarios. The main contributions work summarized follows: In work, proposed combining transfer learning multi-task learning BioNER, done best knowledge. The proposed method achieved state-of-the-art results several biomedical named entity datasets. The main purpose study analyze understand conditions transferring information auxiliary dataset helps improve performance target dataset. To end, used various dataset measures evaluated ability predict MTL gains using three different evaluation metrics. The analysis showed dataset measures contain strong signals benefits multi-task learning."," Aspect-level sentiment classification  and aspect oriented opinion words extraction  are two highly relevant aspect-based sentiment analysis  subtasks. They respectively aim to detect the sentiment polarity and extract the corresponding opinion words toward a given aspect in a sentence. Previous works separate them and focus on one of them by training neural models on small-scale labeled data, while neglecting the connections between them. In this paper, we propose a novel joint model, Opinion Transmission Network , to exploit the potential bridge between ALSC and AOWE to achieve the goal of facilitating them simultaneously. Specifically, we design two tailor-made opinion transmission mechanisms to control opinion clues flow bidirectionally, respectively from ALSC to AOWE and AOWE to ALSC. Experiment results on two benchmark datasets show that our joint model outperforms strong baselines on the two tasks. Further analysis also validates the effectiveness of opinion transmission mechanisms.  \keywords{Aspect-level sentiment classification  \and Aspect-oriented opinion words extraction \and Opinion transmission network.}"
"With development large-scale pre-trained Language Models BERT , XLNet , T5 , tremendous progress made Question Answering . Fine tuning pre-trained LMs task-specific data surpassed human performance QA datasets SQuAD NewsQA . Nevertheless, existing QA systems largely deal factoid questions assume simplified setup multiple-choice questions, retrieving spans text given documents, filling blanks. However, many realistic situations online communities, people tend ask emph{descriptive} questions . Answering questions requires identification, linking, integration relevant information scattered long-form multiple documents generation free-form answers. We particularly interested developing QA system questions e-shopping communities using customer reviews. Compared factoid QA systems, building review QA system faces following challenges: opposed extractive QA answers directly extracted documents multiple-choice QA systems need make selection set pre-defined answers, review QA needs gather evidence across multiple documents generate answers free-form text; factoid QA mostly centres `entities' needs deal limited types questions, review QA systems often presented wide variety emph{descriptive} questions; customer reviews may contain contradictory opinions. Review QA systems need automatically identify prominent opinion given question answer generation. In work here, focus AmazonQA dataset , contains total 923k questions questions associated 10 reviews one answers. We propose novel Cross-passage Hierarchical Memory Network named Chime address aforementioned challenges. Regular neural QA models search answers interactively comparing question supporting text, line human cognition solving factoid questions . While opinion questions, cognition process deeper: reading larger scale complex texts, building cross-text comprehension, continually refine opinions, finally form answers . Therefore, Chime designed maintain hierarchical dual memories closely simulates cognition process. In model, context memory dynamically collect cross-passage evidences, answer memory stores continually refines answers generated Chime reads supporting text sequential manner. Figure illustrates setup task example output generated Chime. The top box shows question extracted test set left panel right upper panel show related 10 reviews paired 4 actual answers. We observe question decomposed complex sub-questions reviews answers contain contradictory information. However, Chime deal information effectively generate appropriate answers shown right-bottom box. In summary, made following contributions: %%%%%%%%%%%%%%%% % Related Work % %%%%%%%%%%%%%%%% In ABSA research, Aspect-level sentiment classification aspect-oriented opinion words extraction two highly relevant tasks. Previous works usually focus one two tasks neglect mutual indication them. In paper, propose novel joint model, Opinion Transmission Network , exploit potential connection ALSC AOWE benefit simultaneously. In OTN, two tailor-made opinion transmission mechanisms designed control opinion clues flow respectively ALSC AOWE AOWE ALSC. Experiment results two tasks validate effectiveness method. \subsubsection{Acknowledgements.} This work supported NSFC National Key R\&D Program China . ---- Bibliography ---- BibTeX users specify bibliography style 'splncs04'. References sorted formatted correct style.","   We introduce Chime, a cross-passage hierarchical memory network for question answering  via text generation. It extends XLNet \cite{yang2019xlnet} introducing an auxiliary memory module consisting of two components: the context memory collecting cross-passage evidence, and the answer memory working as a buffer continually refining the generated answers. Empirically, we show the efficacy of the proposed architecture in the multi-passage generative QA, outperforming the state-of-the-art baselines with better syntactically well-formed answers and increased precision in addressing the questions of the AmazonQA review dataset. An additional qualitative analysis revealed the interpretability introduced by the memory module\blfootnote{This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: \url{http://creativecommons.org/licenses/by/4.0/}.}."
". } The ability understand user's requests essential develop effective task-oriented dialogue systems. For example, utterance ""I want listen Hey Jude The Beatles"", dialogue system correctly identify user's intention give command play song, Hey Jude The Beatles are, respectively, song's title artist name user would like listen. In dialogue system information typically represented semantic-frame structure , %as shown Table . extracting representation involves two tasks: identifying correct frame }) filling correct value slots frame }). In recent years, neural-network based models achieved state art wide range natural language processing tasks, including SF IC. Various neural architectures experimented SF IC, including RNN-based attention-based approaches, till recent transformers models . Input representations also evolved static word embeddings contextualized word embeddings . Such progress allows better address dialogue phenomena involving SF IC, including context modeling, handling out-of-vocabulary words, long-distance dependency words, better exploit synergy SF IC joint models. In addition rapid progresses research community, demand commercial conversational AI also growing fast, shown variety available solutions, Microsoft LUIS, Google Dialogflow, RASA, Amazon Alexa. These solutions also use various kinds semantic frame representations part framework. Motivated rapid explosion scientific progress, unprecedented market attention, think guided map approaches SF IC useful large spectrum researchers practitioners interested dialogue systems. The primary goal survey give broad overview recent neural models applied SF IC, compare performance context task-oriented dialogue systems. We also highlight discuss open issues still need addressed future. The paper structured follows: Section describes SF IC tasks, commonly used datasets evaluation metrics. Section , , elaborate progress state art independent, joint, transfer learning models tasks. Section discusses performance existing models open challenges. % \footnote{https://www.luis.ai/home} % \footnote{https://dialogflow.com/} % \footnote{https://rasa.com/docs/rasa/} % \footnote{https://developer.amazon.com/en-US/docs/alexa/custom-skills/create-intents-utterances-and-slots.html} \end{table*} % \todo[inline]{Explain structure paper} In paper, proposed , cross-passage hierarchical memory network multi-passage generative review QA. It built XLNet generator adding memory module consisting context answer memory guarantees accurate refining process cross-passage evidence collection answer generation. The sequential process adopted makes possible elaborate longer text passages straightforward interpretability. We assessed experimentally significant quality improvement using different state-of-the-art metrics measure lexical semantic coherence generated text. We plan extend model multiple ground truth simultaneously leverage available product attributes."," % pertama harus ngomongin perkembangan yang menarik di area dialgoue systems terus  % SLU itu penting % terus paper ini ngapain % harapannya apa dengan paper ini In recent years, fostered by deep learning technologies and by the high demand for conversational AI, various approaches have been proposed  that address the capacity to elicit and understand user needs in task-oriented dialogue systems. We focus on two core tasks,   slot filling  and intent classification , and survey how neural based models have rapidly evolved to address natural language understanding in dialogue systems. We introduce three neural architectures: independent models, which model SF and IC separately, joint models,  which exploit the mutual benefit of the two tasks simultaneously, and transfer learning models,  that scale the model to new domains.  We  discuss the current state of the research in SF and IC, and highlight challenges that still require attention."
"%Conversational systems usually built using manual rules, supervised machine learning combination both. Supervised systems developed trained carefully curated hand-collected datasets, tested datasets. In Conversational Question Answering systems, user makes set interrelated questions system, extracts answers reference text . These systems trained datasets human-human dialogues collected using Wizard-of-Oz techniques, two crowdsourcers paired random emulate questioner answerer. Several projects shown possible train effective systems using datasets. For instance, QuAC includes question answers popular people Wikipedia , DoQA includes question-answer conversations cooking, movies travel FAQs . Building datasets comes cost, limits widespread use conversational systems built using supervised learning. The fact conversational systems interact naturally users poses exciting opportunity improve deployment. Given enough training data, company deploy basic conversational system, enough accepted used users. Once system deployed, interaction users feedback used improve system. %\todo{add brief summary related work here: requirement user providing correct answer , lack comparison supervised systems, chit-chat conversations} %\todo{User telling right answer: This stronger assumption ours, case, require teacher recognizes correct incorrect answers. } In work focus case CQA system trained off-line deployed receives explicit binary feedback users. An example task seen Figure point conversation two different users give binary feedback system according correctness received answer. Assuming large number interactions, safely ignore examples feedback received. We propose feedback-weighted learning based importance sampling technique improve initial supervised system using binary feedback users. In experiments user feedback simulated, correct/incorrect feedback extracted gold standard. That is, system output matches gold standard output deemed correct, otherwise taken incorrect. In order develop test feedback-weighted learning perform initial experiments document classification. The results show model improved proposed algorithm performs comparably fully supervised model fine-tuned true labels rather binary feedback. Those experiments also used check impact hyperparameters like weight feedback balance exploitation exploration, shows method particularly sensitive values hyperparameters. Regarding CQA, use best hyperparameters earlier experiment document classification, conduct experiments using several domains CQA including datasets like QuAC DoQA. Our method always improves initial supervised system. In in-domain experiments method close fully supervised model fine-tuned true labels rather binary feedback, out-of-domain experiments method matches it. The out-of-domain results particularly exciting, related case CQA system trained off-line one domain could deployed another domain, letting users improve via partial feedback interacting system. Our experiments reveal proposed approach robust choice system architecture, experimented multi-layer perceptron pre-trained transformer. %Regarding supervised architectures, feedback-weighted learning shown effective two deep learning architectures, including multi-layer feed forward network high-performing pre-trained transformer fine-tuned task. %Our work following contributions: % The main contribution work novel method based importance sampling, feedback-weighted learning, improves results two widely used deep learning architectures using partial feedback only. Experimental results document classification show feedback-weighted learning improves initial supervised system, matching performance fully supervised system uses true labels. In-domain out-of-domain CQA experiments show proposed method improves initial supervised system cases, matching fully supervised system out-of-domain experiments. This work opens prospect exploit interactions real users improve conversational systems deployment. All code dataset splits made publicly available . %\item Motivation: enpresak S0, deployment, nola hobetu S0 erabiltzaileei erantzun zuzenak eskatu gabe ? Aukeratzen dugu S0rako arkitektura neuronal superbisatu standard batzuk , eta hori hobetzen saiatzen gara. %\item Google Award-etik recuperatu daiteke zerbait? % %Given specific task, overarching objective work design system able continue learning deployment adapting changes input data distribution. %Our main motivation comes dialogue domain following usual workflow train initial system using available training data offline supervised manner deploy interaction real users. Once system deployed expect great amount interactions containing feedback system's performance. This feedback could explicit instructing users provide binary feedback could also implicit conversational way containing positive negative sentences reacting initial system answers. In experiments analyze case explicit feedback could use improve initially deployed system. We surveyed recent neural-based models applied SF IC context task-oriented dialogue systems. We examined three approaches, i.e. independent, joint, transfer learning based models. Joint models exploiting relation SF IC simultaneously shown relatively better performance independent models. Empirical results shown joint models nearly ""solve"" widely used datasets, ATIS SNIPS, given sufficient in-domain training data. Nevertheless, still several challenges related SF IC, especially improving scalability model new domains languages limited labeled data available."," %Feedback weighted learning for ConvQA in LLL The interaction of conversational systems with users poses an exciting opportunity for improving them after deployment, but little evidence has been provided of its feasibility. In most applications, users are not able to provide the correct answer to the system, but they are able to provide binary  feedback. In this paper we propose feedback-weighted learning based on importance sampling to improve upon an initial supervised system using binary user feedback. We perform simulated experiments on document classification  and Conversational Question Answering datasets like QuAC and DoQA, where binary user feedback is derived from gold annotations. The results show that our method is able to improve over the initial supervised system, getting close to a fully-supervised system that has access to the same labeled examples in in-domain experiments , and even matching in out-of-domain experiments .  Our work opens the prospect to exploit interactions with real users and improve conversational systems after deployment."
". % % % final paper: en-us version % % space normally used marker This work licensed Creative Commons Attribution 4.0 International License. License details: \url{http://creativecommons.org/licenses/by/4.0/}. } %1Yang MTTAMTMT1LEU Neural machine translation models achieved state-of-the-art results widely used many fields. Due numerous parameters, NMT models play advantages based large-scale training data. However, practical applications, NMT models often need perform translation specific domain small quantity in-domain data available. In situation, continual training, also referred fine-tuning, often employed improve in-domain translation performance. In method, model first trained large-scale general-domain training data continually trained in-domain data. With method, in-domain performance improved greatly, unfortunately, general-domain performance decline significantly, since NMT models tend overfit frequent observations in-domain data forget previously learned knowledge. This phenomenon called catastrophic forgetting. Figure shows performance trends in-domain general-domain. %As size training corpus grows, NMT model trained manner continual learning stream data. %Unfortunately, usually exists distribution bias large data set especially data collected different domains. In situation, NMT model tendency towards over-fitting frequent observations newly added data, forgetting previously learned patterns old data, leading poor performance old data. In example domain adaptation shown Figure, training goes, performance surges in-domain slides fast general-domain. This phenomenon catastrophic forgetting neural network. %when large amounts parallel training sentences available. However, similar many successful neural network-based methods, also limited continual learning ability learn stream training data, could different distributions . It NMT system suffers catastrophic forgetting refers model tendency towards over-fitting frequent observations newly added training data, forgetting previously learned features old data. %Figure denotes phenomenon NMT. %1Yang  %Improving continual learning ability NMT system significant importance theory practice. From artificial intelligence perspective, seen another step towards grand goal creating real intelligent translation system learn continuously new translation skills without forgetting old knowledge human does. From practical perspective, enables model update model recent new data improve model's overall performance. We need retrain model scratch time-consuming. Moreover, considering well-trained model maybe already deployed application, original training data may available time. Therefore necessary improve continual learning ability NMT system. %1Yang  Many methods proposed address catastrophic forgetting problem scheme fine-tuning. ensembles general-domain model fine-tuned model together integrated model consider domains. introduces domain-specific output layers domains thus domain-specific features two domains well preserved. , , propose regularization-based methods introduce additional loss original objective help model trade general-domain in-domain. All methods show effectiveness mitigated performance decline general-domain, still know happened inside model continual training methods alleviate catastrophic forgetting problem. The study help understand working mechanism continual training inspire effective solutions problem return. %Catastrophic forgetting long-known problem training neural networks. Some researchers managed alleviate problem different strategies, changing model structure, adding extra regularization term, employing complementary learning systems theory-based strategies on. However, best knowledge, methods mainly focus solve problem, causes problem. %Understanding cause problem inspire effective solutions. %, still work trying figure inner reason catastrophic phenomenon direct evidence show change model parameters NMT. We believe attempt understand phenomenon help us adopt appropriate measures solve problem. %it still clear happens continual learning process causes catastrophic forgetting indeed. %1Yang arametersarametersAbsolute valueIMn-domainin-domain %Given this, seek understand relationship catastrophic forgetting phenomenon model parameters task domain adaptation. More specifically, aim figure trend model parameters catastrophic forgetting. To fulfill goal, propose two methods evaluate importance model parameters. The first use absolute value model parameters second use empirical Fisher Information Matrix . To verify effectiveness correctness proposed methods, parameter erasure experiments. According experimental results, find parameters important general-domain in-domain. Based findings, try alleviate catastrophic forgetting designing learning strategies based importance parameters. We put constrains important parameters make change conservatively encourage less important parameters change aggressively continual learning process. The experiments multiple translation tasks show methods improve translation quality new domain without degrading performance old domain much. Given above, paper, focus catastrophic forgetting phenomenon investigate roles different model parts continual training. To end, explore model granularities modules parameters . In module analyzing experiments, operate model two different ways, freezing one particular module freezing whole model except module. We find different modules preserve knowledge different domains. In parameter analyzing experiments, erase parameters according importance evaluated Taylor expansion-based method . According experimental results, find parameters important general-domain in-domain meanwhile change greatly domain adaptation may result catastrophic forgetting. To ensure validity reliability findings, conduct experiments different language pairs domains. \iffalse Given this, step catastrophic forgetting phenomenon investigating influence different model parts different granularities, depicting different roles played continual training. Inspired work , conducted two kinds analyzing experiments. The first, focusing macro parts model, module analyzing experiment, freeze target module model freeze whole model except target module continual training study influence module translation performance. We found modules higher capacity preserve general-domain knowledge modules essential adapting in-domain. The second, focusing micro parts model parameter analyzing experiment based parameter importance, Taylor expansion-based method adopted importance evaluation criterion. According experimental results, found parameters important general-domain in-domain meanwhile fluctuate greatly domain adaptation may result performance slipping. To ensure validity reliability conclusions, conducted experiments across different language pairs domains. \fi Our main contributions summarized follows: \iffalse To answer questions, put forward two ways evaluating importance model parameters. The first use absolute value model parameters larger absolute value stands important model. Inspired work of, use diagonal Fisher information matrix model parameters evaluate importance. To verify effectiveness correctness proposed methods, parameter erasure experiments effective analysis approach. The results show model parameters important others much impact final translation quality. phenomenon analyzing change model parameters continual learning process. We focus domain adaptation task NMT continual learning scenario means first make model well-trained using large amounts general-domain data, model trained using limited amounts in-domain data another domain. It noted general-domain data available trained process common practice continual learning. We aim investigate following questions: Based findings parameter importance above, investigate changes continual learning process. We find important parameters general-domain translation still play major roles in-domain translation another parameter erasure experiments. What's more, substantial decline general-domain translation quality rise in-domain translation quality also due change parameters. Finally, based findings, propose practical methods overcome catastrophic forgetting phenomenon parameter regularization method learning rate adjustment method based importance model. We change important parameters slightly changing less important parameters aggressively. The results show approach alleviate catastrophic forgetting significantly. Our work indicates parameters important others change parameters influence translation results lot. Therefore, try alleviate catastrophic forgetting designing different learning strategies based importance parameters. As far know, first work trying analyze catastrophic forgetting phenomenon NMT. Moreover, analyzing methods put forward work task-independent applied neural network-based methods tasks. \fi \iffalse extra space store old training data even retrain scratch without storing old training data even retraining This work focuses domain adaptation problem NMT special case continual learning scenario neural network. They share training task distribution training data different. Domain adaptation deals problem improving performance model trained general domain data test instances new domain. In scenario, usually large amounts general-domain training data welled trained model based it. In contrast, limited number in-domain training data lead NMT system overfit soon perform poorly trained data. Some researchers solve problem combining training data general-domain in-domain together train new system scratch. They usually make use domain information improve translating performance adding domain labels training data using domain discriminator find domain invariant features. On one hand, methods time consuming need extra space store training data efficient real-life applications. On hand, due relatively small size in-domain data, lead model overfit general-domain data observed results. Fine-tuning fast efficient method continual learning neural networks already applied NMT. NMT system first trained general-domain data trained in-domain data. Domain adaptation common application scenario continual learning NMT drawn much attention recently. Under scenario, The translation quality drops quickly distribution training data changes. It suffers catastrophic forgetting continual training process. \fi We presented first empirical study practical concerns targeted attacks black-box NMT system driven parallel data poisoning. We evaluated scenarios poisoning from-scratch training, pre-training, fine-tuning NMT systems trained parallel data. We show small poisoning budgets , systems severely compromised, even trained tens millions clean samples. We hope raise awareness risk training NMT systems malicious inputs untrusted sources. As end goal effective defence, one next steps look developing countermeasures attack, designing algorithms robust parallel data filtering, well detecting protecting named entities attack. \paragraph{\bf Ethical Considerations} Our aim work identify mitigate potential threats NMT systems, adopting established threat modelling machine learning systems, identify prioritise need devise effective defences develop robust systems. Our results help answer security review question NMT system development: ``What impact training data poisoned tampered recover adversarial contamination?'' As attack shown straightforward enact implementation requires minimal knowledge attacker, believe attacks expose crucial blind spot machine translation vendors, needs addressed promptly.","  %Neural machine translation  always suffers catastrophic forgetting during the continual learning process which means the model tends to forget all its previously learned knowledge when further trained with new data with different distributions, like from different domains or languages. However, it is not clear what happens during this process and what causes this phenomenon. More specifically, it is not clear whether this is due to the overall change of the model or the impact of certain parameters. In this work, we focus on the domain adaptation task of NMT under the continual learning scenario. First, we put forward two ways for evaluating the importance of the parameters and show that the translation quality mainly dependents on the most important parameters of the model. Then we analyze the behavior of the parameters according to their importance for the model during the continual learning process and it shows that the important parameters for the general-domain translation still play major roles for the in-domain translation after the continual learning process. What's more, the catastrophic forgetting phenomenon, shown as the substantial decline of general-domain translation quality with the rise of in-domain translation quality,  is mainly due to the change of these important parameters.  Finally, we propose some practical methods to overcome the catastrophic forgetting by controlling the updates of parameters differently based on their importance.    Neural machine translation  models usually suffer from catastrophic forgetting during continual training where the models tend to gradually forget previously learned knowledge and swing to fit the newly added data which may have a different distribution, e.g. a different domain. Although many methods have been proposed to solve this problem, we cannot get to know what causes this phenomenon yet. Under the background of domain adaptation, we investigate the cause of catastrophic forgetting from the perspectives of modules and parameters . The investigation on the modules of the NMT model shows that some modules have tight relation with the general-domain knowledge while some other modules are more essential in the domain adaptation. And the investigation on the parameters shows that some parameters are important for both the general-domain and in-domain translation and the great change of them during continual training brings about the performance decline in general-domain. We conduct experiments across different language pairs and domains to ensure the validity and reliability of our findings.    %by tracing parameter variation in this progress and depict the influence of different model modules.  %and depict the relationship between them so that we can work out solutions to the catastrophic forgetting problem based on these findings.  %Under the background of domain adaptation for machine translation, we found that some parameters play an essential role in both general domain and in-domain translation and the change of them brings about the performance decline in general-domain. Based on these findings, we propose a solution to detect these important parameters and accordingly suppress their fluctuation during domain adaptation. Experimental results prove  %that our method can greatly improve the translation quality in in-domain and meanwhile minimize the negative influences on general-domain translation."
"Recurrent neural network architectures demonstrated remarkable success natural language processing, achieving state art performance across impressive range tasks ranging machine translation semantic parsing question answering . These tasks demand use wide variety computational processes information sources , evaluated coarse-grained quantitative ways. As result, easy matter identify specific strengths weaknesses network's solution task. In paper, take different tack, exploring degree neural networks successfully master one specific aspect linguistic knowledge: interpretation sentences containing reflexive anaphora. We address problem context task semantic parsing, instantiate mapping sequence words predicate calculus logical form representation sentence's meaning. \pex<ex:transform> \a Mary runs \a John sees Bob \xe Even simple sentences like in~, represent smallest representations object reflexives English, network must learn lexical semantic correspondences mode composition . %Such simple disentangled representations meaning highly successful words learned. Of course, natural language adheres simple formulas. Reflexives, words like himself, interpretation assigned independently meaning surrounding context. \pex<ex:transform-refl> \a Mary sees \a Alice sees \xe In sentences, interpretation reflexive constant combined meaning surrounding elements. Rather, reflexive object must interpreted identical meaning verb's subject. Of course, network could learn context-sensitive interpretation reflexive, sentence \lex{Mary} subject, reflexive interpreted , \lex{Alice} subject interpreted . However, piecemeal learning reflexive meaning support generalization sentences involving subject encountered antecedent reflexive training, even interpretation subject occurred elsewhere. What needed instead interpretation reflexive characterized specific output token, rather abstract instruction duplicate interpretation subject. Such abstraction requires ``jigsaw puzzle"" approach meaning simpler sentences afford. \citet{Marcus98} argues kind abstraction, takes require use algebraic variables assert identity, beyond capacity recurrent neural networks. \citeauthor{Marcus98}'s demonstration involves simple recurrent network language model trained predict next word corpus sentences following form: \pex \a A rose rose. \a A mountain mountain. % \a A car car \xe All sentences training set identical subject object nouns. \citeauthor{Marcus98} shows, however, resulting trained network correctly predict subject noun tested novel preamble `\lex{A book }'. Though intriguing, demonstration entirely convincing: since noun occurring novel preamble, \lex{book} example, occur training data, way network could possibly known output correspond reflexive sentence containing novel subject noun, even network successfully encode identity relation subject object. \citet{frank2013} explore related task context SRN interpretation reflexives. In experiments, SRNs trained map input words corresponding semantic symbols output time step word presented. For words vocabulary, simple task: desired output constant function input . For reflexives however, target output depends subject occurs earlier sentence. \citeauthor{frank2013}\ tested network's ability interpret reflexive sentences containing subject occurred reflexive's antecedent training. However, unlike Marcus' task, subject corresponding semantic symbol occur contexts training data, therefore realm possible inputs outputs network. Nonetheless, none SRNs trained succeeded task even single test example. Since experiments conducted, substantial advances made recurrent neural network architectures, crucial success practical NLP systems. These innovations open possibility modern network architectures may well able solve variable identity problem necessary mapping reflexive sentences logical form. In experiments describe below, explore whether case. In work, focus catastrophic forgetting phenomenon NMT aim find inner reasons this. Under background domain adaptation, propose two analyzing methods perspectives modules parameters conduct experiments across different language pairs domains. We find modules tend maintain general-domain knowledge modules tend adapt in-domain; also find parameters important general-domain in-domain translation change brings performance decline general-domain. Based findings, proposed several ideas may help improve vanilla continual training method. We prove effectiveness ideas future work. The investigation different modules NMT model showed modules higher capacity preserve general-domain knowledge modules essential adapting in-domain; investigation parameters showed parameters important general domain in-domain translation change brings performance decline general-domain. We put forward two methods evaluating importance parameters, find parameters play essential role general-domain in-domain. Then find change important parameters brings performance decline general-domain series analyzing experiments. Finally, based analysis, propose importance evaluation based method alleviate catastrophic forgetting experimental results different languages domains prove effectiveness method."," Reflexive anaphora present a challenge for semantic interpretation: their meaning varies depending on context in a way that appears to require abstract variables. Past work has raised doubts about the ability of recurrent networks to meet this challenge. In this paper, we explore this question in the context of a fragment of English that incorporates the relevant sort of contextual variability. We consider sequence-to-sequence architectures with recurrent units and show that such networks are capable of learning semantic interpretations for reflexive anaphora which generalize to novel antecedents. We explore the effect of attention mechanisms and different recurrent unit types on the type of training data that is needed for success as measured in two ways: how much lexical support is needed  to induce an abstract reflexive meaning  and what contexts must a noun phrase occur in to support generalization of reflexive interpretation to this noun phrase?"
"Pre-trained contextualized language models BERT state-of-the-art wide variety natural language processing tasks. Similarly, Information Retrieval , models brought large improvements task ad-hoc retrieval---ranking documents relevance textual query, models increasingly dominate leaderboards ad-hoc retrieval competitions. Despite success, little understood pretrained language models effective ad-hoc ranking. What new aspects task neural models solve previous approaches not? Previous work shown traditional IR axioms, e.g. increased term frequency correspond higher relevance, explain behavior recent neural models . Outside IR, others examined characteristics contextualized language models learn general , remains unclear qualities valuable ad-hoc ranking task specifically. Thus, new approaches necessary characterize models. We propose new framework aimed Analyzing Behavior Neural IR ModeLs based three testing strategies: ``measure match'', ``textual manipulation'', ``dataset transfer''. The ``measure match'' strategy, akin diagnostic tests proposed by~\citet{Rennings2019AnAA}, constructs test samples controlling one measurement varying another using samples existing IR collection. The ``textual manipulation'' strategy tests effect altering document text ranking. The ``dataset transfer'' strategy constructs tests non-IR datasets. The new tests allow us isolate model characteristics---such sensitivity word order, preference summarized rather full documents---that imperceptible using approaches. We also release open-source implementation framework makes easy define new diagnostics replicate analysis new models. Using new framework, perform first large-scale analysis neural IR models. We compare today's leading ranking techniques, including using BERT T5, well methods focused efficiency like DocT5Query EPIC. We find evidence showing neural models able make effective use textual signals reflected classical term matching methods like BM25. For example, controlling term frequency match, neural models detect document relevance much accurately BM25 baseline, effect pronounced larger neural models. Further, unlike prior approaches, rankers based BERT T5 heavily influenced word order: shuffling words document consistently lowers document's score relative unmodified version. We also find significant differences different neural models: e.g., models treat queries navigationally , BERT-based EPIC model T5 exhibit behaviors. Finally, models exhibit unexpected : adding additional relevant text end document frequently reduce ranking score, adding non-relevant content increase it---despite document length limited effect ranking scores. % In summary, present new framework performing analysis ad-hoc ranking models. We demonstrate framework provide insights ranking model characteristics providing comprehensive analysis neural ranking models date. Our released software framework facilitates conducting analyses future work. Because abstract meaning, reflexive anaphora present distinctive challenge semantic parsing thought beyond capabilities recurrent networks. The experiments described demonstrate incorrect. Sequence-to-sequence networks range recurrent unit types fact capable learning interpretation reflexive pronouns generalizes novel antecedents. Our results also show generalization nonetheless contingent appearance held-out antecedent variety syntactic positions well diversity antecedents providing support reflexive generalization. Additionally successful generalization depends network architecture ways fully understand. It present unknown whether demands architecture impose learning environment successful learning reflexives consistent children experience, could explored corpus experimental work. Future work also necessary elucidate nature networks' representations reflexive interpretation understand support lexical generalization . The question explored related to, distinct from, issue systematicity , according pieces representations learned distinct contexts freely recombine. This issue addressed using sequence-to-sequence architectures recent work synthetic SCAN robot command interpretation dataset language modeling , cases limited success. One aspect SCAN domain particularly relevant reflexive interpretation commands involving adverbial modifiers \lex{twice}. Commands like \lex{jump twice} must interpreted duplicating meaning verb, i.e., , similar require interpretation reflexive object, though way require sensitivity syntactic structure explored here. Recently, \citet{lake2019compositional}, \citet{li-etal-2019-compositional} \citet{Gordon2020Permutation} proposed novel architectures increase systematic behavior, look forward exploring degree impact performance reflexive interpretation. Our current work focused exclusively recurrent networks, ranging SRNs GRUs LSTMs. Recent work \citet{transformer} shows Transformer networks attain superior performance variety sequence-to-sequence tasks dispensing recurrent units altogether. Examining performance training characteristics Transformers allow us compare effects attention recurrence anaphora interpretation task. This especially interesting given impact attention performance experiments. Finally, current experiments revealing capacity recurrent networks learn generalizations context-sensitive interpretation, nonetheless limited number respects simplifications English fragment use create synthetic data. Reflexives famously impose structural requirement antecedents . In following example, reflexive's antecedent must cannot . \ex The student near teacher sees \xe We know whether architectures succeed experiments would similarly well relevant generalization required reference structure. Past work explored sensitivity recurrent networks hierarchical structure, mixed results . In ongoing work, exploring question studying complex synthetic domains kind recurrent sequence-to-sequence network used well networks explicitly encode decode sentences hierarchical manner. A second simplification concerns distribution reflexives themselves. English reflexives appear broader range syntactic environments apart transitive objects . It would considerable interest explore reflexive interpretation naturalistic setting incorporate broader set distributions."," Numerous studies have demonstrated the  effectiveness of pretrained contextualized language models such as BERT and T5 for ad-hoc search. However, it is not well-understood why these methods are so effective, what makes some variants more effective than others, and what pitfalls they may have.  We present a new comprehensive framework for Analyzing the Behavior of Neural IR ModeLs , which includes new types of diagnostic tests that allow us to probe several characteristics---such as sensitivity to word order---that are not addressed by previous techniques.  To demonstrate the value of the framework, we conduct an extensive empirical study that yields insights into the factors that contribute to the neural model's gains, and identify potential unintended biases the models exhibit.  We find evidence that recent neural ranking models have fundamentally different characteristics from prior ranking models. For instance, these models can be highly influenced by altered document word order, sentence order and inflectional endings. They can also exhibit unexpected behaviors when additional content is added to documents, or when documents are expressed with different levels of fluency or formality. We find that these differences can depend on the architecture and not just the underlying language model.\footnote{\url{https://github.com/allenai/abnriml}}"
". % % % final paper: en-uk version % % % space normally used marker % This work licensed Creative Commons % Attribution 4.0 International Licence. % Licence details: % \url{http://creativecommons.org/licenses/by/4.0/}. % % % final paper: en-us version % % % space normally used marker % This work licensed Creative Commons % Attribution 4.0 International License. % License details: % \url{http://creativecommons.org/licenses/by/4.0/}. } Commonsense knowledge shared majority people society acquired naturally everyday life. Commonsense reasoning process logical inference using commonsense information. Commonsense answer questions ``'' Figure depicted as: ``'', ``'', ``.'' An enormous amount pre-defined commonsense knowledge available people make inferences using commonsense following example: ``'' ``'' ``'' ``'' This chain commonsense reasoning naturally deduced humans without substantial difficulty. Whereas people acquire commonsense lives, machines cannot learn knowledge without assistance. A large amount external knowledge several reasoning steps required machines learn commonsense. In recent years, various datasets constructed enable machines reason commonsense. one widely researched datasets presented Figure \subref{subfig:examplea}. The studies commonsense reasoning based dataset categorized two mainstream approaches. The first approach uses pre-trained language models distributed representations, exhibit high performances Natural Language Processing tasks. However, despite high performance, models must trained excessive number parameters cannot explain process commonsense reasoning. The second approach reasoning commonsense knowledge graph. The generally used commonsense knowledge graph ConceptNet 5.5 , includes parsed representation Open Mind Commonsense different language sources WordNet DBPedia . In approach, subgraph ConceptNet corresponding questions transformed node embeddings graph encoder. The candidate highest attention score selected answer computed node embeddings word vectors language models. To learn commonsense knowledge observed understood language models, relations ConceptNet serve critical role method. The performance improved utilizing relations represented text; however, interpretation question still enough. Unlike , commonly used method solving problem Knowledge-Based Question-Answering employing semantic representations. As method infers answer logical structure question using knowledge base, question-answering process explained logical form. In work, Abstract Meaning Representation , one logical structure, used understand overall reasoning process, question answer. AMR graph meaning representation symbolizes meaning sentences. AMR illustrates ``who whom'' implied sentence graph. The components graphs words, rather concepts relations. Each concept denotes event entity, relation represents semantic role concepts. In paper, enable language models exploit AMR graph understand logical structure sentences. However, difficult infer commonsense information AMR graph, owing deficiency commonsense knowledge given sentence. For example, Figure \subref{subfig:exampleb}, AMR graph indicates path logical structure sentence ``'' ; words, paths single AMR graph lack proficient information predict right answer. Therefore, commonsense reasoning, dynamic interactions AMR graph ConceptNet inevitable reach correct answer. Thus, propose new compact AMR graph expanded ConceptNet's commonsense relations pruning, called ACP graph. The proposed method interpret path question answer performing commonsense reasoning within connected graph, ``'' . The contributions study follows. The remainder paper organized follows. In Section 2, present entire process method detail. The experimental setup results explained Section 3. A discussion proposed model provided Section 4, Section 5 presents conclusions. Appendix A provides related works including ConceptNet, previous works commonsense reasoning, AMR. We presented new framework analyzing ranking models based three testing strategies: Measure Match Tests , Textual Manipulation Tests , Dataset Transfer Tests . By using tests, demonstrated variety insights gained behaviors recently-proposed ranking models, based BERT T5. Our analysis is, date, extensive analysis behaviors neural ranking models, sheds light several unexpected model behaviors. For instance, adding non-relevant text increase document's ranking score, even though models largely biased towards longer documents. We also see base language model used different ranking architecture yield different behaviors, higher sensitivity shuffling document's text. Meanwhile, different language models sensitive different characteristics, importance prepositions. \documentclass[11pt,a4paper]{article} \usepackage[table]{xcolor} \usepackage{times,latexsym} \usepackage{url} \usepackage[T1]{fontenc} \usepackage{amsfonts} \usepackage{amsmath} \usepackage{todonotes} \usepackage{booktabs} \usepackage{adjustbox} \usepackage{rotating} \usepackage{layouts} \usepackage{enumitem} \usepackage{afterpage} \usepackage{float} \usepackage{lipsum} \DeclareMathOperator{\sgn}{sgn} \definecolor{pos}{RGB}{76,144,186} \definecolor{neg}{RGB}{222,102,62} \definecolor{art}{RGB}{200,200,200} \setlist[itemize]{noitemsep, topsep=0pt} \hyphenation{Conv-KNRM} \usepackage[acceptedWithA]{tacl2018v2} \usepackage{xspace,mfirstuc,tabulary} \newcommand{\dateOfLastUpdate}{Sept. 20, 2018} \newcommand{\styleFileVersion}{tacl2018v2} \newcommand{\ex}[1]{{\sf #1}} \newcommand{\sys}{ABNIRML} \newif\iftaclinstructions \taclinstructionsfalse \iftaclinstructions \renewcommand{\confidential}{} \renewcommand{\anonsubtext}{} \newcommand{\instr} \fi \iftaclpubformat \newcommand{\taclpaper}{final version\xspace} \newcommand{\taclpapers}{final versions\xspace} \newcommand{\Taclpaper}{Final version\xspace} \newcommand{\Taclpapers}{Final versions\xspace} \newcommand{\TaclPapers}{Final Versions\xspace} \else \newcommand{\taclpaper}{submission\xspace} \newcommand{\taclpapers}{{\taclpaper}s\xspace} \newcommand{\Taclpaper}{Submission\xspace} \newcommand{\Taclpapers}{{\Taclpaper}s\xspace} \newcommand{\TaclPapers}{Submissions\xspace} \fi \newcommand\ac[1]{{\color{brown}\{#1\}}} \newcommand\sergey[1]{{\color{blue}\{#1\}}} \newcommand\doug[1]{{\color{orange}\{#1\}}} \newcommand\sm[1]{{\color{purple}\{#1\}}} \title{\sys: Analyzing Behavior Neural IR Models} \author{ {\bf Sean MacAvaney}\thanks{\xspace \xspace Work done internship AI2} \qquad Sergey Feldman \qquad {\bf Nazli Goharian} \\ {\bf Doug Downey} \qquad {\bf Arman Cohan} \\ IR Lab, Georegetown University, Washington, DC \\ Allen Institute AI, Seattle, WA \\ {\tt\small \{sean,nazli\}@ir.cs.georgetown.edu} \\ {\tt\small \{sergey,dougd,armanc\}@allenai.org} } \date{}"," \texttt{CommonsenseQA} is a task in which a correct answer is predicted through commonsense reasoning with pre-defined knowledge. Most previous works have aimed to improve the performance with distributed representation without considering the process of predicting the answer from the semantic representation of the question. To shed light upon the semantic interpretation of the question, we propose an AMR-ConceptNet-Pruned  graph. The ACP graph is pruned from a full integrated graph encompassing Abstract Meaning Representation  graph generated from input questions and an external commonsense knowledge graph, ConceptNet . Then the ACP graph is exploited to interpret the reasoning path as well as to predict the correct answer on the \texttt{CommonsenseQA} task. This paper presents the manner in which the commonsense reasoning process can be interpreted with the relations and concepts provided by the ACP graph. Moreover, ACP-based models are shown to outperform the baselines."
"Part-Of-Speech tagging crucial step language understanding, used automatic language understanding applications named entity recognition question answering , also used manual language understanding linguists attempting answer linguistic questions document less-resourced languages . Much prior work developing high-quality POS taggers uses neural network methods rely availability large amounts labelled data. However, resources readily available majority world's 7000 languages . Furthermore, manually annotating large amounts text trained experts expensive time-consuming task, even linguists/annotators might native speakers language. Active Learning \cite[AL]{lewis1995evaluating,settles2009active} family methods aim train effective models less human effort cost selecting subset data maximizes end model performance. While many methods proposed AL sequence labeling , empirical study across six typologically diverse languages show within task setup methods perform inconsistently. Furthermore, even oracle scenario % access true labels data selection, existing methods far optimal. We posit primary reason inconsistent performance existing methods consider uncertainty predictions, consider direction uncertainty respect output labels. For instance, Figure consider German token ``die,'' may either pronoun determiner . According initial model , ``die'' labeled PRO majority time, significant amount probability mass also assigned output tags many examples. Based this, existing AL algorithms select uncertain tokens likely select ``die'' frequent predictions certain, may select instance ``die'' either gold label PRO DET. Intuitively, would like correct errors tokens true labels DET mis-labeled model PRO, asking human annotator tag instance true label PRO, even uncertain, likely much benefit. Inspired observation, pose problem AL part-of-speech tagging selecting tokens maximally reduce confusion output tags. For instance, example would attempt pick token-tag pair ``die/DET'' reduce potential errors model over-predicting PRO despite belief DET also plausible option. We demonstrate features model oracle setting know true model confusions , also describe approximate strategy know true confusions. We evaluate proposed AL method running simulation experiments six typologically diverse languages namely German, Swedish, Galician, North Sami, Persian, Ukrainian, improving upon models seeded cross-lingual transfer related languages . In addition, conduct human annotation experiments Griko, endangered language truly lacks significant resources. Our contributions follows: % File tacl2018v2.tex % Sep 20, 2018 % The English content file modified various *ACL instructions % Lillian Lee Kristina Toutanova % % LaTeXery mostly adapted acl2018.sty. \documentclass[11pt,a4paper]{article} \usepackage{times,latexsym} \usepackage{url} \usepackage[T1]{fontenc} \usepackage{amsmath} \usepackage{amssymb} \usepackage{tabularx} \usepackage{mathtools} \usepackage{booktabs} \usepackage{url} \usepackage{longtable} \usepackage{tabu} \usepackage{multirow} \usepackage{amsfonts} \usepackage{tabu} \usepackage{algorithm} \usepackage{bbm} \usepackage{subfigure} \usepackage[noend]{algpseudocode} \usepackage[normalem]{ulem} \usepackage{enumitem} \makeatletter \def\BState{\State\hskip-\ALG@thistlm} \usepackage{bbm} \usepackage{xcolor} \DeclareMathOperator*{\argmax}{arg\,max} \DeclareMathOperator*{\b-argmax}{ b\text{-}arg\,max} \DeclareMathOperator*{\argmin}{arg\,min} %% Package options: %% Short version: ""hyperref"" ""submission"" defaults. %% More verbose version: %% Most compact command produce submission version hyperref enabled %% \usepackage[]{tacl2018v2} %% Most compact command produce ""camera-ready"" version \usepackage[acceptedWithA]{tacl2018v2} %% Most compact command produce double-spaced copy-editor's version %\usepackage[acceptedWithA]{tacl2018v2} % %% If need disable hyperref settings TACL instructions), add "",nohyperref"" square %% brackets. %\usepackage[nohyperref]{tacl2018v2} %%%% Material block specific generating TACL instructions \usepackage{xspace,mfirstuc,tabulary} \newcommand{\dateOfLastUpdate}{Sept. 20, 2018} \newcommand{\styleFileVersion}{tacl2018v2} \newcommand{\gn}[1]{\textcolor{magenta}{\small [#1 --GN]}} \newcommand{\an}[1]{\textcolor{blue}{\small [#1 --AA]}} \newcommand{\ex}[1]{{\sf #1}} \newif\iftaclinstructions \taclinstructionsfalse % AUTHORS: NOT set true \iftaclinstructions \renewcommand{\confidential}{} \renewcommand{\anonsubtext}{} \newcommand{\instr} \fi % \iftaclpubformat % ""if"" set choice options \newcommand{\taclpaper}{final version\xspace} \newcommand{\taclpapers}{final versions\xspace} \newcommand{\Taclpaper}{Final version\xspace} \newcommand{\Taclpapers}{Final versions\xspace} \newcommand{\TaclPapers}{Final Versions\xspace} \else \newcommand{\taclpaper}{submission\xspace} \newcommand{\taclpapers}{{\taclpaper}s\xspace} \newcommand{\Taclpaper}{Submission\xspace} \newcommand{\Taclpapers}{{\Taclpaper}s\xspace} \newcommand{\TaclPapers}{Submissions\xspace} \fi %%%% End TACL-instructions-specific macro block %%%% \title{Reducing Confusion Active Learning Part-Of-Speech Tagging} \author{Aditi Chaudhary\textsuperscript{1}, Antonios Anastasopoulos\textsuperscript{2,\Thanks{ Work done Carnegie Mellon University.}}, Zaid Sheikh\textsuperscript{1}, Graham Neubig\textsuperscript{1} \\ \textsuperscript{1}Language Technologies Institute, Carnegie Mellon University\\ \textsuperscript{2}Department Computer Science, George Mason University\\ { @cs.cmu.edu}} { } } \date{} % In cases failure, model exhibits two problems follows: As concept node \texttt{illness} disappeared generating graph, model may enough information extracting subgraph ConceptNet. The red edges Figure present paths high attention weight question ``\texttt{What home entertainment equipment requires cable?}'' In Figure \subref{subfig:case-heatmap}, top four paths high attention weights described. As opposed predicting answers simply ConceptNet graph connected question, allow model learn relevant paths inherent ACP graph. That is, graph path learning module ACP graph capable commonsense reasoning exploring paths. \section{Conclusions Future Works} We introduce new commonsense reasoning method, using proposed ACP graph. This method outperformed model simply learns ConceptNet graph. Furthermore, method explain answer-inference process interpreting logical structure sentences within commonsense reasoning process. Models applied method exhibit higher performance compared previous models. However, certain problems still remain. Though relations \texttt{ARG0} \texttt{ARG1} occupy core roles AMR graph, still arguable choice relations may lead better results. Therefore, show experimental results according different pruning rules \texttt{CommonsenseQA} task future. Also, plan develop end-to-end learning model incorporates AMR generation model question-answering model reduce error propagation AMR generation. \section{Acknowledgement} This work supported Institute Information \& communications Technology Planning \& Evaluation grant funded Korea government . Also, research supported MSIT, Korea, ITRC support program supervised IITP \section*{Acknowledgements} The acknowledgements go immediately references. Do number acknowledgements section. Do include section submitting paper review. include bib file like this: ConceptNet. In ConceptNet , real-world assertions represented two nodes directed edges, denote certain concepts relations, respectively. The nodes represent words phrases natural language sentences. The edges represent relations nodes, contain lexical well commonsense relation information. As ConceptNet created collecting data various types knowledge bases, nodes different types also exist. Each node represents slightly different meaning considering role sentence. For example, word ``\texttt{person}'' found concept ``\texttt{person/n},'' analyzed noun POS tagger, detailed semantic information, identified ``\texttt{person/n/wn/body}.'' This information makes possible detailed extraction knowledge considers purpose sentence. Meanwhile, one edges may defined two nodes. For example, edge nodes ``\texttt{person}'' ``\texttt{eat}'' defined independently ``\texttt{CapableOf}'' ``\texttt{Desires}.'' Various concepts relations defined nodes edges ConceptNet, considering ambiguity sentences. Commonsense reasoning. Commonsense reasoning process logical inference using commonsense information. In \texttt{CommonsenseQA}\footnote[1]{https://www.tau-nlp.org/csqa-leaderboard} task, fine-tuning approach pre-trained language representations makes use external commonsense knowledge. There two means exploiting external knowledge. The first\footnote[7]{ttps://drive.google.com/file/d/1sGJBV38aG706EAR75F7LYwCqci9ocG9i/view,\\ https://gist.github.com/commonsensepretraining/507aefddcd00f891c83ebf6936df15e8} method post-trained commonsense sentence corpus. It performs fine-tuning evidence derived questions answers. The second method encode commonsense knowledge graphs train language models. The language models exhibited high performance method BERT , RoBERTa , use bidirectional transformer encoders. They also include XLNet , based autoregressive language modeling, ALBERT , adopts cross-layer parameter sharing factorized embedding parameterization ELECTRA pre-trained Replaced Token Detection task. AMR. AMR represents relations concept nodes using PropBank frameset vocabularies sentences. The edges two concept nodes argument nodes relations. AMR represents semantic roles core numbered roles, uses 100 semantic relations, including negation, conjunction, command, wikification. In PropBank , semantic roles labeled form \texttt{ARG0}\texttt{4} \texttt{ARGM}. In general, \texttt{ARG0} denotes agent verb, \texttt{ARG1} patient, \texttt{ARG2} means instrument, benefactive, attribute, \texttt{ARG3} interpreted starting point, benefactive, attribute, \texttt{ARG4} represents ending point. The root node serves central point representation called frame node. Thereafter, concept nodes sequentially combined according semantic relations. AMR consists concept nodes single graph traversable nodes, similar parse tree. However, unlike parse tree, represents explicit structure sentences, AMR aims describe conceptual semantic structure. That is, semantic meanings explicitly different sentences same, represented AMR graph. For example, two sentences ``\texttt{The boy hard worker}'' ``\texttt{The boy works hard}'' represented PENMAN graph, namely \texttt{ :manner )}. The data constructed generate evaluate representations AMR 2.0 AMR 1.0 . The model highest performance data presented Zhang et al. \shortcite{zhang2019amr:2019}, using BERT. Various NLP fields exploited AMR, sentence generation , summarization , question answering , dialogue systems , paraphrase detection , biomedical text mining ."," Active learning  uses a data selection algorithm to select useful training samples to minimize annotation cost. This is now an essential tool for building low-resource syntactic analyzers such as part-of-speech  taggers. Existing AL heuristics are generally designed on the principle of selecting uncertain yet representative training instances, where annotating these instances may reduce a large number of errors. However, in an empirical study across six typologically diverse languages , we found the surprising result that even in an oracle scenario where we know the true uncertainty of predictions, these current heuristics are far from optimal. Based on this analysis, we pose the problem of AL as selecting instances which maximally reduce the confusion between particular pairs of output tags. Extensive experimentation on the aforementioned languages shows that our proposed AL strategy outperforms other AL strategies by a significant margin.  We also present auxiliary results demonstrating the importance of proper calibration of models, which we ensure through cross-view training, and analysis demonstrating how our proposed strategy selects examples that more closely follow the oracle data distribution. The  code is publicly released here.\footnote{\url{https://github.com/Aditi138/CRAL}}"
"With increasing submission academic papers recent years, task making final decisions manually incurs significant overheads program chairs, desirable automate process. In study, aim utilizing document-level semantic analysis paper review rating prediction recommendation. Given reviews paper several reviewers input, goal infer final acceptance decision paper reviewers' evaluation respect numeric rating . Paper review rating prediction recommendation practical important task AI applications help improve efficiency paper review process. It also intended enhance consistency assessment procedures outcomes, diversify paper review process comparing human recommended rating machine recommended rating. In literature, existing studies cast review rating prediction multi-class classification/regression task . They build predictor using supervised machine learning models review texts corresponding ratings. Due importance features, researches focus extracting effective features context-level features user features boost prediction performance. However, feature engineering time-consuming labor-intensive. Recently, development neural networks wide applications, various deep learning-based models proposed automatically learning features text data . Existing deep learning models usually learn continuous representations different grains text corpus . Although deep learning models automatically learn extensive feature representation, cannot efficiently capture hierarchical relationship inherent review data. To address problem, studied hierarchical architecture implemented deep learning framework learn better document-level representation. Also, success attention mechanism many tasks machine translation, question answering , designed directional self-attention network gain context-aware embeddings words sentences. Despite great progress made models, focus task paper review rating recommendation effective enough directly used task following reasons: First, review data hierarchical nature. There exists three-level hierarchical structure review data: word level, intra-review level inter-review level, previous models capture two-levels hierarchy. Second, paper reviews usually much longer reviews , models working shorter reviews stated leverage date representation techniques BERT SciBERT . In paper, propose novel neural network framework paper review rating recommendation taking word, intra-review inter-review information account. Specifically, inspired HAN DiSAN , introduce Hierarchical Bi-directional self-Attention Network framework effectively incorporate different levels hierarchical information. The proposed framework consists three main modules end-to-end relationship: sentence encoder, intra-review encoder inter-review encoder, consider hierarchical structures review data comprehensive possible. The outputs inter-review encoder leveraged features build rating predictor without feature engineering. We release code data collected us enable replication application new tasks, available https://github.com/RingBDStack/HabNet. The contributions work follows: We presented novel active learning method low-resource POS tagging works reducing confusion output tags. Using simulation experiments across six typologically diverse languages, show confusion-reducing strategy achieves higher accuracy existing methods. Further, test approach true setting active learning ask linguists document POS information endangered language, Griko. Despite unfamiliar language, proposed method achieves performance gains methods iterations. For next steps, plan explore possibility adapting proposed method complete morphological analysis, poses even harder challenge AL data selection due complexity task."," Review rating prediction of text reviews is a rapidly growing technology with a wide range of applications in natural language processing.  However, most existing methods either use hand-crafted features or learn features using deep learning with simple text corpus as input for review rating prediction, ignoring the hierarchies among data.  In this paper, we propose a Hierarchical bi-directional self-attention Network framework  for paper review rating prediction and recommendation, which can serve as an effective decision-making tool for the academic paper review process. Specifically, we leverage the hierarchical structure of the paper reviews with three levels of encoders: sentence encoder , intra-review encoder  and inter-review encoder .  Each encoder first derives contextual representation of each level, then generates a higher-level representation, and after the learning process, we are able to identify useful predictors to make the final acceptance decision, as well as to help discover the inconsistency between numerical review ratings and text sentiment conveyed by reviewers.  Furthermore, we introduce two new metrics to evaluate models in data imbalance situations.  Extensive experiments on a publicly available dataset  and our own collected dataset  demonstrate the superiority of the proposed approach compared with state-of-the-art methods."
"% What QG Why important Question Generation aims endow machines ability ask relevant to-the-point questions document. QG important practical applications, generating assessments course materials education, prompting user interaction dialog systems, enabling machines ask clarification questions FAQs, automatically building large-scale QA datasets research community. % How tranditional works it? Recent QG approaches used Seq2Seq models attention, feeds input document encoder, generates question document decoder. % Why needs RL? The training objective maximize log likelihood ground-truth question paired input document using teacher forcing. However, ground-truth questions insufficient account many equivalent ways asking question, likelihood-based training suffers problem exposure bias, i.e., model learn distribute probability mass sequences valid different ground truth. % How RL addresses problem? % To address issue, previous QG works proposed optimize model directly question-specific rewards via Reinforcement Learning . This process decouples training procedure ground truth data, space possible questions better explored. Moreover, allows training target specific properties want question exhibit, relevant specific topic answerable document. % What problem RL-based method? Although various rewards employed QG --- BLEU, answerability reward, word movers distance --- optimizing reward scores always lead higher question quality practice, observed Hosking Riedel~\shortcite{Hosking2019EvaluatingRF}. How define robust effective QG-specific rewards still requires investigation. % What want do? We aim analyze effectiveness question-specific rewards QG. Instead using general natural language generation metrics BLEU, target three QG-related metrics commonly cited human evaluations question quality: Fluency indicates whether question follows grammar accords correct logic; Relevance indicates whether question relevant document; Answerability indicates whether question answerable given document. We design specific RL reward metric: language model based reward fluency, discriminator-based reward relevance, QA-based reward answerability. After optimizing reward via RL, conduct comprehensive analysis, including automatic human evaluation, arrive following conclusions: individual joint optimization rewards lead performance gain automated metrics, guarantee improvement real question quality; reward relevance substantially helps improve question quality, reward answerability reduces quality due bias brought QA model; reward likely improve question quality reward score correlates well human judgement. In paper, scientific paper review dataset called OpenReview collected ICLR openreview website released. We observe three-level hierarchical structure dataset -- information relationships reviews one paper may affect final decision, may relationships words sentences review. Based observations, hierarchical bi-directional self-attention network framework proposed paper review rating prediction recommendation model interactions among words, sentences, intra- inter-reviews end-to-end manner. Moreover, considering imbalanced distribution different classes review rating prediction task, design two new metrics better evaluate models. It seen experimental results predicting final decisions submitted papers identifying ratings reviews two datasets demonstrate proposed framework sufficient ability capture hierarchical structures words, sentences reviews datasets outperforms models. In future, plan investigate multi-task learning paper review rating recommendation.","     Recent question generation  approaches often utilize the sequence-to-sequence framework  to optimize the log likelihood of ground-truth questions using teacher forcing. However, this training objective is inconsistent with actual question quality, which is often reflected by certain global properties such as whether the question can be answered by the document. As such, we directly optimize for QG-specific objectives via reinforcement learning to improve question quality. We design three different rewards that target to improve the fluency, relevance, and answerability of generated questions. We conduct both automatic and human evaluations in addition to thorough analysis to explore the effect of each QG-specific reward.      We find that optimizing on question-specific rewards generally leads to better performance in automatic evaluation metrics. However, only the rewards  that correlate well with human judgement  lead to real improvement in question quality. Optimizing for the others, especially answerability, introduces incorrect bias to the model, resulting in poor question quality. Our code is publicly available at \href{https://github.com/YuxiXie/RL-for-Question-Generation}{https://github.com/YuxiXie/RL-for-Question-Generation}."
"% In daily bases plethora opinion data published different topics response different stimuli using Social Media. % Aiming analyse gain insights opinions posted social media, research stance detection become increasingly popular recent years. Framed classification task, stance detection consists determining textual utterance expresses supportive, opposing neutral viewpoint respect target topic . Research stance detection largely limited analysis single utterances social media. Furthering research, SardiStance 2020 shared task focuses incorporating contextual knowledge around utterances, including metadata author profiles network interactions. The task included two subtasks, one solely focused textual content social media posts automatically determining stance, whereas allowed incorporating additional features available profiles interactions. This paper describes analyses participation SardiStance 2020 shared task, held part EVALITA campaign focused detecting stance expressed tweets associated Sardines movement. % % % For network interaction graph, generate user embeddings, using variations graph neural network embedding methods, concatenate author's vector corresponding utterance features stance. We also extract two types text embedding representations utterance, embedding-based features, namely word embedding vectors cosine similarity vectors, using different models including variations CNN bidirectional LSTM models. Further, results two feature extraction methods concatenated final classification step. We also consider standard methods extract frequency-based representations author profiles stance utterances including unigrams Tfidf vectors. All four features combined fed drop dense layers, finally generate final label using softmax activation function. Though, deactivate four sources features alter frequency-based vector excluding features, changing embedding source reducing dimensionality highly dimensional vectors using PCA.} In paper designed developed pipeline representing knowledge scientific publication structured graph called scientific knowledge graph. We employed various state-of-the-art NLP tools machine learning, provided workflow merge results. Moreover, integrated knowledge coming many scientific publications single knowledge graph purpose represent detailed knowledge scientific literature Semantic Web domain. The evaluation proved solution able automatically produce good quality scientific knowledge graphs integration different tools yields better overall performance. There number limitations need still addressed future work. In first instance, current version take full advantage semantic characterization research entities verify resulting triples. For instance, currently possible entity kind Material include entity kind Task, may semantically incorrect. For reason, plan develop robust semantic framework could drive extraction process discard triples follow specific constraints. For example, could state material could include another material, task method. These requirements could enforced verified use specific semantic technologies expressing constraints SHACL\footnote{https://www.w3.org/TR/shacl/}. A second limitation current prototype extract one relationship two entities. This completely realistic since two entities linked many kinds relationships. This could also lead higher number relationships could suggest different applications uses entities, increasing probability finding unconsidered issues solutions within research field. We intend explore possibility future work. Additionally, thoroughly investigate conjunction construct might hide rich knowledge relationship frequently occurs two research entities . We also plan improve knowledge graph considering cross document relations link entities, order better support tools scientific inquiry. A third limitation regards ability recognize synonyms defined existent knowledge bases, CSO. For instance, current version may still fail recognize two quite different strings actually refer entity. We intend address issue computing semantic similarity word graph embeddings representing entities order detect merge synonyms effectively. A fourth limitation regards scalability pipeline. The current implementation presents bottlenecks could make difficult apply large-scale datasets. First, Extractor Framework requires lot hard disk space. This entails data must sampled processed. Second, current pipeline adopts Stanford Core NLP server one thread, requires long time mine textual resources sentence-by-sentence. However, big issue since would possible run Stanford Core NLP server multi-thread mode, speeding extraction process. An important next step also perform extrinsic evaluation proposed knowledge base within different tasks. In particular, would like assess AI tasks tackled recommender systems graph embeddings creation strategies benefit it."," This paper presents our submission to the SardiStance 2020 shared task, describing the architecture used for Task A and Task B. While our submission for Task A did not exceed the baseline, retraining our model using all the training tweets, showed promising results leading to  using bidirectional LSTM with BERT multilingual embedding for Task A. For our submission for Task B, we ranked 6th . With further investigation, our best experimented settings increased performance from  to  with same architecture and parameter settings and after only incorporating social interaction features- highlighting the impact of social interaction on the model's performance."
"State-of-the-art existing natural language processing classification tasks currently achieved systems first pre-trained auxiliary language modeling tasks fine-tuned task interest cross-entropy loss . Although commonly used, cross-entropy loss -- KL-divergence one-hot vectors labels distribution model's output logits -- several shortcomings. Cross entropy loss leads poor generalization performance due poor margins , lacks robustness noisy labels adversarial examples . Effective alternatives proposed change reference label distributions label smoothing , Mixup , CutMix , knowledge distillation self-training~. Additionally, recently demonstrated NLP fine-tuning using cross entropy loss tends unstable , especially supervised data limited, scenario pre-training particularly helpful. To tackle issue unstable fine-tuning, recent work proposes local smoothness-inducing regularizers regularization methods inspired trust region theory prevent representation collapse lead poor generalization performance. Empirical analysis suggests fine-tuning longer, reinitializing top layers~, using debiased Adam optimizer fine-tuning~ make fine-tuning procedure stable. We inspired learning strategy humans deploy given examples -- try find commonalities examples class contrast examples classes. We hypothesize similarity-based loss able hone important dimensions multidimensional hidden representations lead better few-shot learning results stable fine-tuning pre-trained models. We propose novel objective fine-tuning pre-trained language models includes supervised contrastive learning term pushes examples class close examples different classes apart. The new term similar contrastive objective used self-supervised representation learning various domains image, speech, video domains. . In constrast methods, however, use contrastive objective supervised learning final task, instead contrasting different augmented views examples. Adding supervised contrastive learning term fine-tuning objective improves performance several natural language understanding tasks GLUE benchmark , including SST-2, CoLA, MRPC, RTE, QNLI state-of-the-art models fine-tuned cross entropy loss. The improvements particularly strong few-shot learning settings , models trained SCL robust noise training data, also better generalization ability related tasks limited labeled data. Our approach require specialized architectures , memory banks , data augmentation kind, additional unsupervised data. To best knowledge, work first successfully integrate supervised contrastive learning objective fine-tuning pre-trained language models. % \ves{end alternative} % State-of-the-art models existing natural language processing tasks currently learned fine-tuning pre-trained large language models shown capture semantic, syntactic, world knowledge. Recent attempts improving pre-training stage masked language modeling~ led improvements natural language understanding tasks, fine-tuning stage stayed downstream NLP classification tasks: add task-specific output layer pre-trained language model continue training labeled task data using cross-entropy loss. % Cross-entropy loss widely adopted objective supervised classification models, defined KL-divergence one-hot vectors labels distribution model's output logits. Although commonly used state-of-the-art models across many fields including NLP, several works demonstrating shortcomings cross-entropy loss, showing leads poor generalization performance due poor margins , lack robustness noisy labels adversarial examples . Among alternative objective functions proposed, effective approaches practice ones change reference label distributions label smoothing , Mixup , CutMix , knowledge distillation self-training~. % Several recent studies show fine-tuning procedure unstable , especially case supervised data limited, scenario pre-training particularly helpful. To tackle issue unstable fine-tuning, local smoothness-inducing regularizers regularization methods inspired trust region theory proposed prevent representation collapse leads poor generalization performance task models. There also empirical analysis suggests fine-tuning longer, reinitializing top layers~, using debiased Adam optimizer fine-tuning~ make fine-tuning procedure stable. % On hand, contrastive learning methods seen remarkable success self-supervised representation learning various downstream tasks, particularly image, speech, video domains. % These self-supervised contrastive learning methods primarily try reduce distance representations positive pairs increasing distance representations negative pairs. Positive pairs constructed different augmented views labeled example, negative pairs simply augmented views examples. Augmented views examples often constructed state-of-the-art data augmentation methods RandAugment AutoAugment computer vision domain, distance metric often chosen inner product Euclidean distance representations pairs low-dimensional embedding space. % Recently, \citet{Khosla2020SupervisedCL} extended contrastive learning fully supervised setting using label information constructing positive negative pairs, showed improved performance cross-entropy loss baseline ImageNet image classification accuracy robustness benchmarks, demonstrated supervised contrastive learning less sensitive hyperparameter changes. Similarly, \citet{Liu2020HybridDT} propose hybrid discriminative-generative training energy-based models, approximate generative term contrastive objective demonstrate improved image classification accuracy CIFAR-10 CIFAR-100, along improved performance robustness, out-of-distribution detection, calibration. % In paper, propose supervised contrastive learning regularization fine-tuning large pre-trained language models helps model leverage label information effectively across different labeled data regimes. Our approach require specialized architectures , memory banks , large batch sizes , still outperforms strong baseline fine-tuning RoBERTa-Large labeled task data cross-entropy loss, unlike previous works. To best knowledge, work first successfully integrate supervised contrastive learning objective fine-tuning pre-trained language models. % sho results few-shot learning, robustness, generalization ability. % We summarize key contributions following: % In work, described state-of-the-art stance detection system leveraging different features including author profiling, word meaning context social interactions. Using different random runs, best model achieved leveraging deepwalk-based knowledge graphs embeddings, FastText similarity feature vectors extracted two multi-headed convolutional neural networks auther's utterance. This motivates future, aiming reduce model complexity automate feature selection process."," State-of-the-art natural language understanding classification models follow two-stages: pre-training a large language model on an auxiliary task, and then fine-tuning the model on a task-specific labeled dataset using cross-entropy loss. Cross-entropy loss has several shortcomings that can lead to sub-optimal generalization and instability.  Driven by the intuition that good generalization requires capturing the similarity between examples in one class and contrasting them with examples in other classes, we propose a supervised contrastive learning  objective for the fine-tuning stage. Combined with cross-entropy, the SCL loss we propose obtains improvements over a strong RoBERTa-Large baseline on multiple datasets of the GLUE benchmark in both the high-data and low-data regimes, and it does not require any specialized architecture, data augmentation of any kind, memory banks, or additional unsupervised data. % In all of our experiments, we use a very competitive baseline of fine-tuning RoBERTa Large using cross entropy loss on the labeled task data.  %including SST-2, CoLA, MRPC, RTE and QNLI. %Our method outperforms the baseline on multiple datasets in the GLUE benchmark including SST-2, CoLA, MRPC, RTE and QNLI for the full dataset regime.  % We also show the effectiveness of our regularization for few-shot learning and demonstrate  % We also demonstrate the robustness of the learned representations by using noisy datasets, and show that the learned representations are more transferable to related tasks.  We also demonstrate that the new objective leads to models that are more robust to different levels of noise in the training data, and can generalize better to related tasks with limited labeled task data."
"With rapid growth textual documents internet, accessing information web become challenging issue . Often users want summary topic various sources fulfill information needs . The QF-MDS task deals problems goal summarize set documents answer given query. In QF-MDS task, summaries generated summarizer either extractive abstractive. An extractive summarizer extracts relevant text spans source document, whereas abstractive summarizer generates summary natural language may contain words appear source document . With rising popularity virtual assistants recent years, growing interest integrate abstractive summarization capabilities systems natural response generation . One major challenge QF-MDS task datasets used tasks contain labeled training data. Therefore, neural summarization models leverage supervised training cannot used datasets. Note related tasks , reduce demands labeling data leverage unlabeled data also identified major challenge. While using datasets similar target dataset training data QF-MDS task, find datasets contain multi-document gold summaries. However, state-of-the-art transformer-based summarization models cannot used long documents due computational complexities . To tackle issues, propose novel weakly supervised approach utilizing distant supervision generate weak reference summary single-document multi-document gold reference summaries. We train model document weak supervision find proposed approach generates abstractive summaries effective QF-MDS task. More concretely, make following contributions: We propose supervised contrastive learning objective fine-tuning pre-trained language models demonstrate improvements strong RoBERTa-Large baseline multiple datasets GLUE benchmark high-data low-data regimes. We also show proposed objective leads models robust different levels noise training data generalize better related tasks limited labeled task data. Currently, data augmentation methods NLP effects downstream tasks neither effective well understood counterparts computer vision domain. In future work, plan study principled automated data augmentation techniques NLP would allow extending supervised contrastive learning objective semi-supervised self-supervised learning settings."," In the Query Focused Multi-Document Summarization  task, a set of documents and a query are given where the goal is to generate a summary from these documents based on the given query. However, one major challenge for this task is the lack of availability of labeled training datasets. To overcome this issue, in this paper, we propose a novel weakly supervised learning approach via utilizing distant supervision. In particular, we use datasets similar to the target dataset as the training data where we leverage pre-trained sentence similarity models to generate the weak reference summary of each individual document in a document set from the multi-document gold reference summaries. Then, we iteratively train our summarization model on each single-document to alleviate the computational complexity issue that occurs while training neural summarization models in multiple documents  at once. Experimental results in Document Understanding Conferences\footnote{https://duc.nist.gov/}  datasets show that our proposed approach sets a new state-of-the-art result in terms of various evaluation metrics."
"One ultimate goal language modelling construct model like human, grasp general, flexible robust meaning language. One reflection obtaining model able master new tasks domains task quickly. However, NLU models building specific task given data domain fail dealing out-of-domain data performing new task. To combat issue, several research areas transfer learning including domain adaptation, cross lingual learning, multi-task learning sequential transfer learning developed extend model handling multiple tasks. However, transfer learning tends favor high-resources tasks trained carefully, also computationally expensive . Meta learning algorithm tries solve problem training model variety tasks equip model ability adapt new tasks samples. In case, adopt idea model-agnostic meta learning optimization method meta learning directly optimized model constructing useful initial representation could efficiently trained perform well various tasks . However, continual learning data comes model sequentially, still potential problem catastrophic forgetting model trained new tasks would start perform worse previous tasks. The two objectives designing continual learning architecture accelerate future learning exploits existing knowledge task quickly together general knowledge previous tasks learn prediction new samples avoid interference previous tasks updates new tasks. . % new In paper, utilize algorithm derived Jave White \shortcite{MLRCL:19} applies Meta-Learning continual learning. Our objective apply framework NLP field, specifically NLU tasks. By taking advantage model-agnostic approach, Meta-Learning continual learning applicable language model optimized gradient-based methods. We compare results Duo et al \shortcite{dou:19} applies meta-learning Glue tasks, MAML-Rep shows comparable results. We hope bring new research direction NLP fields focusing method. The implementation code found \url{https://github.com/lexili24/NLUProject}. % old % This paper aims develop framework incorporate meta learning continual learning framework. Hypothetically, approach efficient training relying low-resources various tasks adapted meta learning characteristics. By training meta learner continual learning framework, model consistent results various tasks little catastrophic forgetting learning general representation tasks. Finally, approach model agnostic, could essentially apply existing language models long model optimized gradient descent. Moreover, method put framework continual learning techniques like GEM. The implementation code found \url{https://github.com/lexili24/NLUProject}. In paper, propose novel weakly supervised approach Query Focused Multi-Document Abstractive Summarization task tackle issue available labeled training data tasks. We also propose iterative approach address computational problem occurs training neural models long documents . Experimental results three datasets show proposed approach sets new state-of-the-art result various evaluation metrics. In future, apply models tasks, information retrieval applications , sentiment analysis , learning imbalanced unlabeled datasets , automatic chart question answering ."," Neural network has been recognized with its accomplishments on tackling various natural language understanding  tasks. Methods have been developed to train a robust model to handle multiple tasks to gain a general representation of text. In this paper, we implement the model-agnostic meta-learning  and Online aware Meta-learning  meta-objective under the continual framework for NLU tasks proposed by Javed and White\shortcite{MLRCL:19}. We validate our methods on selected SuperGLUE \shortcite{superglue:19}  and GLUE benchmark \shortcite{glue:19}."
"% % % final paper: en-us version % % space normally used marker This work licensed Creative Commons Attribution 4.0 International License. License details: \url{http://creativecommons.org/licenses/by/4.0/} } Neural Machine Translation adopts encoder-decoder paradigm model entire translation process . Specifically, encoder finds multi-layer representation source sentence, decoder queries topmost encoding representation produce target sentence cross-attention mechanism . However, over-reliance topmost encoding layer problematic two aspects: Prone over-fitting, especially encoder under-trained, low-resource tasks ; It cannot make full use representations extracted lower encoder layers, syntactically semantically complementary higher layers . Researchers proposed many methods make model aware various encoder layers besides topmost mitigate issue. Almost resort adjustment network structure, divided two categories. The first merge feature representations extracted distinct encoder layers fed decoder . The differences lie design merge function: self-attention , recurrent neural network , tree-like hierarchical merge . Moreover, second makes decoder layer explicitly align parallel encoder layer encoder layers . However, methods either complicate original model limit model's flexibility, requiring number encoder layers decoder layers . Instead, work, propose layer-wise multi-view learning address problem perspective model training, without changing model structure. Our method's highlight training process concerned, inference speed guaranteed standard model. The core idea regard off-the-shelf output encoding layer view input sentence. Therefore, straightforward cheap construct multiple views standard layer-by-layer encoding process. Further, addition output topmost encoder layer used standard models , also incorporate intermediate encoder layer auxiliary view. We feed two views partially shared decoder independent predictions. An additional regularization loss based prediction consistency views used encourage auxiliary view mimic primary view. Thanks co-training two views, gradients back-propagation simultaneously flow two views, implicitly realizes knowledge transfer. Extensive experimental results five translation tasks show method stably outperform multiple baseline models . In particular, achieved new state-of-the-art results 10.8 BLEU KoEn 36.23 BLEU IWSLT'14 DeEn. Further analysis shows method's success lies robustness encoding representations dark knowledge provided consistency regularization. \iffalse Our contributions threefold: \fi In work, able extend Meta-Learning continual learning framework learn general presentation robust set continual tasks efficiency. We replicate \shortcite{MLRCL:19} method implement NLU tasks. Results show less datapoints, could derive MAML like model robust testing tasks, however extending continual setting training phrase, performance drastically worsen. Future direction would extending approach language models, wells experiment combination high low resources Glue SuperGlue benchmark evaluate model performance.","   Traditional neural machine translation is limited to the topmost encoder layer's context representation and cannot directly perceive the lower encoder layers. Existing solutions usually rely on the adjustment of network architecture, making the calculation more complicated or introducing additional structural restrictions. In this work, we propose layer-wise multi-view learning to solve this problem, circumventing the necessity to change the model structure.    We regard each encoder layer's off-the-shelf output, a by-product in layer-by-layer encoding, as the redundant view for the input sentence.   In this way, in addition to the topmost encoder layer , we also incorporate an intermediate encoder layer as the auxiliary view.    We feed the two views to a partially shared decoder to maintain independent prediction.     Consistency regularization based on KL divergence is used to encourage the two views to learn from each other.   Extensive experimental results on five translation tasks show that our approach yields stable improvements over multiple strong baselines. As another bonus, our method is agnostic to network architectures and can maintain the same inference speed as the original model."
"% . } % Emotion analysis established research area finds application variety different fields, including social media analysis \cite[i.a.]{Purver2012,Wang2012b,Mohammad2017,Ying2019}, opinion mining \cite[i.a.]{Choi2006}, computational literary studies \cite[i.a.]{Ovesdotter2005,Kimfanfic2019,Haider2020,Zehe2020}. The prominent task emotion analysis emotion categorization, text receives assignments predefined emotion inventory, fundamental emotions \fear, \anger, \joy, \anticipation, \trust, \surprise, \disgust, \sadness follow theories . Other tasks include recognition affect values, namely valence arousal analyses event appraisal . More recently, categorization tasks complemented fine-grained analyses, namely emotion stimulus detection role labeling, detect words denote experiencer emotion, emotion cue description, target emotion. These efforts lead computational approaches detecting stimulus clauses emotion role labeling sequence labeling , different advantages disadvantages discuss . Further, work led rich set corpora annotations different subsets roles. An example sentence annotated semantic role labels emotion ``\experiencer{John} \cue{hates} \target{cars} \stimulus{pollute environment}.'' A number English-language resources available: manually construct dataset following FrameNet's emotion predicate annotate stimulus core argument. annotate Tweets emotion cue phrases, emotion targets, emotion stimulus. In previous work publish news headlines annotated roles emotion experiencer, cue, target, stimulus. annotate sentence triples taken literature roles. A popular benchmark emotion stimulus detection Mandarin corpus . annotate English Mandarin texts comparable way clause level . In paper, utilize role annotations understand influence emotion classification. We evaluate roles' contents enable emotion classifier infer emotions. It reasonable assume roles' content carries different kinds information regarding emotion: One particular experiencer present corpus might always feel emotion; hence, prone bias model could pick on. The target stimulus might independent experiencer sufficient infer emotion. The presence target might limit set emotions triggered. Finally, corpora contain cue annotations, assume helpful decide expressed emotion, typically explicit references towards concrete emotion names. We studied incorporate different encoder layers multi-view learning neural machine translation. In addition primary view topmost layer, proposed model introduces auxiliary view intermediate encoder layer encourages transfer knowledge two views. Our method agnostic network architecture maintain inference speed original model. We tested method five translation tasks multiple strong baselines: Transformer, deep Transformer, DynamicConv. Experimental results show multi-view learning method stably outperform baseline models. Our models achieved new state-of-the-art results KoEn IWSLT'14 DeEn tasks.","   Emotion recognition is predominantly formulated as text classification in   which textual units are assigned to an emotion from a predefined inventory   .   More recently, semantic role labeling approaches have been developed to   extract structures from the text to answer questions like: ``who is   described to feel the emotion?'' , ``what causes this   emotion?'' , and at   which entity is it directed?'' . Though it has been shown that   jointly modeling stimulus and emotion category   prediction is beneficial for both subtasks, it remains unclear which of   these semantic roles enables a classifier to infer the emotion. Is it the   experiencer, because the identity of a person is biased towards a   particular emotion ? Is it a particular target    or a stimulus ? We   answer these questions by training emotion classification models on five   available datasets annotated with at least one semantic role by masking the   fillers of these roles in the text in a controlled manner and find that   across multiple corpora, stimuli and targets carry emotion information,   while the experiencer might be considered a confounder.  Further, we   analyze if informing the model about the position of the role improves the   classification decision. Particularly on literature corpora we find that   the role information improves the emotion classification."
"In recent years, best results coreference resolution English obtained end-to-end neural models~. However Dutch, existing systems still using either rule-based~ machine learning approach~. The rule-based system dutchcoref~ outperformed previous systems two existing datasets also presented corpus evaluation literary novels . In paper compare rule-based system end-to-end neural coreference resolution system: e2e-Dutch. This system variant \citet{lee2018higher} BERT token representations. We evaluate compare performance e2e-Dutch dutchcoref two different datasets: SoNaR-1 corpus , genre-balanced corpus 1 million words, RiddleCoref corpus contemporary novels . This provides insights relative strengths neural system versus rule-based system Dutch coreference, effect domain differences . The two datasets consider vary greatly terms overall size length individual documents; training subset RiddleCoref contains 23 documents compared 581 documents SoNaR-1. However, average number sentences per document higher RiddleCoref SoNaR-1 . We also conduct error analysis systems examine types errors systems make. Our experiments show importance semantic roles emotion classification differs datasets roles: The stimulus cue critical classification, correspond direct report feeling description triggered emotion. This result shown drop performance removing roles. This information redundantly available outside arguments. It particularly beneficial model's performance access position cues stimuli. This suggests classifier learns tackle problem differently information available, especially ECA ES -- cases literature annotated instances comparably long. The bi-LSTM model indicates experiencer role confounder GNE. The performance increased model access content. Similar results observed ET, target role confounder. However, results taken grain salt given confirmed switching transformer-based model. The differences results bi-LSTM transformer also motivate research, suggest contextualized representation might compensate missing information, is, therefore, robust. Finally, results across models multiple datasets indicate emotion classification approaches indeed benefit semantic roles' information adding positional information. Similarly targeted aspect-based sentiment analysis, motivates future work, emotion classification role labeling modelled jointly. In case, also interesting investigate happens positional indicators added roles jointly.","     We evaluate a rule-based \citep{lee2013deterministic}     and neural \citep{lee2018higher} coreference system on Dutch datasets of     two domains: literary novels and news/Wikipedia text.     The results provide insight into the relative strengths of data-driven and     knowledge-driven systems, as well as the influence of domain, document     length, and annotation schemes.     The neural system performs best on news/Wikipedia text,     while the rule-based system performs best on literature.     The neural system shows weaknesses with limited training data and long     documents, while the rule-based system is affected by annotation     differences. The code and models used in this paper are available at     \url{https://github.com/andreasvc/crac2020}"
"A relational triple consists two entities connected semantic relation, form . The extraction relational triples unstructured raw texts key technology automatic knowledge graph construction, received growing interest recent years. There several studies addressing technical solutions relational triple extraction. Early researches, \citet{zelenko2003kernel,chan2011exploiting}, employ pipeline manner extract entities relations, entities recognized first relation extracted entities predicted. Such pipeline approach ignores relevance entity identification relation prediction tends suffer error propagation problem. % To model cross-task dependencies explicitly prevent error propagation pipeline approach, subsequent studies propose joint entity relation extraction. These studies roughly categorized three main paradigms. The first stream work, \citet{miwa2016end,gupta2016table,zhang2017end}, treats joint entity relation extraction task end-to-end table filling problem. Although methods represent entities relations shared parameters single model, extract entities relations separately produce redundant information . The second stream work, \citet{zheng2017joint,dai2019joint,wei-etal-2020-novel}, transforms joint entity relation extraction sequence labeling. To this, human experts need design complex tagging schema. The last stream work, including \citet{zeng2018extracting,zeng2019learning,nayak2019ptrnetdecoding,zeng2020copymtl}, driven sequence-to-sequence model generate relational triples directly, flexible framework handle overlapping triples require substantial effort human experts. We follow seq2seq based models joint entity relation extraction. Despite success existing seq2seq based models, still limited autoregressive decoder cross-entropy loss. The reasons follows: relational triples contained sentence intrinsic order essence. However, order adapt autoregressive decoder, whose output sequence, unordered target triples must sorted certain order training phase. Meanwhile, cross-entropy permutation-sensitive loss function, penalty incurred every triple predicted position. Consequently, current seq2seq base models need learn generate triples, also required consider extraction order multiple triples. % consists three parts featured transformers non-autoregressive parallel decoding bipartite matching loss. In detail, three parts proposed set prediction networks : avoid introducing order triplets % restoring original form task without considering order multiple triples In work, formulate joint entity relation extraction task set prediction problem, avoiding considering order multiple triples. In order solve set prediction problem, propose end-to-end network featured transformers non-autoregressive parallel decoding bipartite matching loss. In detail, three parts proposed set prediction networks : sentence encoder, set generator, set based loss function. First all, adopt BERT model encoder represent sentence. Then, since autoregressive decoder must generate items one one order, decoder suitable generating unordered sets. In contrast, leverage transformer-based non-autoregressive decoder set generator, predict triples avoid sorting triples. Finally, order assign predicted triple unique ground truth triple, propose bipartite matching loss function inspired assigning problem operation research . Compared cross-entropy loss highly penalizes small shifts triple order, proposed loss function invariant permutation predictions; thus suitable evaluating difference ground truth set prediction set. % To summarize, contributions follows: In nutshell, main contributions are: % main contributions work follows: % conjunction bipartite matching loss transformers % parallel decoding % Our work build prior work several domains:relation extraction, non-autoregressive model, andbipartite matching losses set prediction. % Relation Extraction. Non-autoregressive Model. We found large gaps performance two systems across two domains, result conclusive due several reasons, follows. 1, 2 The neural system shows weakness long documents novel corpus, also needs training data reach full potential. 3, 4 The rule-based system better adapted SoNaR-1 annotation scheme, neural system's capacity adapt arbitrary annotation conventions necessarily imply better linguistic performance. 5 To maximize comparability usefulness corpora, annotations harmonized, involves manual mention annotation. In future work want improve neural system using genre metadata finetuning BERT, rule-based system extended hybrid system adding supervised classifiers."," The joint entity and relation extraction task aims to extract all relational triples from a sentence. In essence, the relational triples contained in a sentence are unordered. However, previous seq2seq based models require to convert the set of triples into a sequence in the training phase. To break this bottleneck, we treat joint entity and relation extraction as a direct set prediction problem, so that the extraction model can get rid of the burden of predicting the order of multiple triples. To solve this set prediction problem, we propose networks featured by transformers with non-autoregressive parallel decoding. Unlike autoregressive approaches that generate triples one by one in a certain order, the proposed networks directly output the final set of triples in one shot. Furthermore, we also design a set-based loss that forces unique predictions via bipartite matching. Compared with cross-entropy loss that highly penalizes small shifts in triple order, the proposed bipartite matching loss is invariant to any permutation of predictions; thus, it can provide the proposed networks with a more accurate training signal by ignoring triple order and focusing on relation types and entities. Experiments on two benchmark datasets show that our proposed model significantly outperforms current state-of-the-art methods. Training code and trained models will be available at \url{http://github.com/DianboWork/SPN4RE}."
"Zero-shot translation first introduced \citet{firat-etal-2016-zero} refers ability multilingual NMT model translate source target languages, even pairs parallel data seen training. In simplest setting, parameters network shared different languages translation guided special tags indicate desired output language . While capability attractive alternative building dedicated translation systems serve languages, performance zero-shot pairs tends lag behind pivot translation. Recent papers, \citet{Arivazhagan2019}, \citet{Gu2019} \citet{Zhang2020}, suggested training techniques improve generalization unseen language pairs, performance varies considerably across settings. In paper, examine detail behavior multilingual model proposed \citet{Johnson2017} zero-shot translation directions. Our experiments show following: Overall, observe improvements 8.1 BLEU 6 zero-shot directions simple changes multilingual training setup. In paper, introduce set prediction networks joint entity relation extraction. Compared previous seq2seq based models, We formulate joint entity relation extraction task set prediction problem. In way, extraction model relieved predicting extraction order multiple triples. To solve set prediction problem, We combine non-autoregressive parallel decoding bipartite matching loss function. We conduct extensive experiments two widely used datasets validate effectiveness proposed set prediction networks. Experimental results show proposed networks outperforms state-of-the-art baselines different scenarios. This challenging task far solved. We find relation types exhibit imbalanced long-tailed distribution NYT dataset WebNLG dataset. Our future work concentrate combine cost-sensitive learning proposed set prediction networks."," Zero-shot neural machine translation is an attractive goal because of the high cost of obtaining data and building translation systems for new translation directions. However, previous papers have reported mixed success in zero-shot translation. It is hard to predict in which settings it will be effective, and what limits performance compared to a fully supervised system. In this paper, we investigate zero-shot performance of a multilingual EN$\leftrightarrow$\{FR,CS,DE,FI\} system trained on WMT data. We find that zero-shot performance is highly unstable and can vary by more than 6 BLEU between training runs, making it difficult to reliably track improvements. We observe a bias towards copying the source in zero-shot translation, and investigate how the choice of subword segmentation affects this bias. We find that language-specific subword segmentation results in less subword copying at training time, and leads to better zero-shot performance compared to jointly trained segmentation. A recent trend in multilingual models is to not train on parallel data between all language pairs, but have a single bridge language, e.g.\ English. We find that this negatively affects zero-shot translation and leads to a failure mode where the model ignores the language tag and instead produces English output in zero-shot directions.  We show that this bias towards English can be effectively reduced with even a small amount of parallel data in some of the non-English pairs."
"Entrainment well-known psycholinguistic phenomenon causing people adapt conversation partners become similar. It affects many linguistic features including phonetics , lexical choice , syntax , prosody . Importantly, correlates interesting aspects conversation task success, liking, even rapport robot . The researchers cited employed various means measure entrainment, correlations, models conditional probabilities, comparisons distributions, perceived similarity. Recently, \citet{Nasir2018} proposed first neural entrainment measure. Our work builds addressing challenge critical measuring entrainment: accounting consistency. Entrainment defined active, though unconscious, adaptation speaker towards partner. In practice, however, static similarity correlation two speakers often measured. Thus, even two speakers whose vocal characteristics initially similar perceived entrained, although adaptation taken place. Alternatively, Speaker B entrains Speaker A, speakers perceived entrained, without adaptation Speaker A. We apply neural methods proposed \citet{Pryzant2018} explicitly deconfound consistency, tendency adhere one's vocal style, entrainment, tendency adapt one's partner. We argue entrainment measures control consistency overestimate degree entrainment conversation. Section explains data features use train networks, described Section . Section introduces two experiments validate methods whose results discussed, lastly, Section . We analyze importance shared subwords multilingual models find language-specific BPE segmentation helps reduce amount untranslated segments zero-shot directions. Furthermore, explore whether tendency produce wrong output language attributed using English bridge language, show even small amount additional training data non-English language pairs, generalization unseen translation directions improves model less likely produce output wrong language. Compared previous work, methods propose easier implement, since concern data collection pre-processing, result higher gains zero-shot directions. They also compatible principle approaches introduce new training objectives model modifications, report best results fine-tuning multi-bridge model back-translation zero-resource translation directions. For future work, interested testing effects subword regularization zero-shot translation performance, scaling multi-bridge setups massively multilingual settings.","   Human interlocutors tend to engage in adaptive behavior known as entrainment to become more similar to each other. Isolating the effect of consistency, i.e., speakers adhering to their individual styles, is a critical part of the analysis of entrainment. We propose to treat speakers' initial vocal features as confounds for the prediction of subsequent outputs. Using two existing neural approaches to deconfounding, we define new measures of entrainment that control for consistency. These successfully discriminate real interactions from fake ones. Interestingly, our stricter methods correlate with social variables in opposite direction from previous measures that do not account for consistency. These results demonstrate the advantages of using neural networks to model entrainment, and raise questions regarding how to interpret prior associations of conversation quality with entrainment measures that do not account for consistency."
"The proliferation online hate speech become prevalent recent times. Numerous social media outlets computational social science community looking various automated techniques detect classify hate speech. However, models, nascent nature, significant limitations due complexity problem. Primarily, lack reliable baseline coupled evolving vocabulary hateful content makes particularly challenging issue. For instance, many studies classified problem binary classification task, fails address subtleties hate speech, direct vs. indirect hate speech. These binary classification models also fail identify different types hate speech like racism, sexism, antisemitism, etc. varying degrees. Another key obstacle plagues binary models inability distinguish general offensive language hate speech. A third issue arises designing automated approaches class imbalance---hate speech usually small percentage overall data---and need adequately upsample hate observations without model overfitting. In work, inspired recent successes developing multi-class hate speech models separate hate speech offensive content, propose DeL-haTE, ensemble tunable deep learning models leverages CNN GRU layers. The CNN layer extracts higher-order features word embedding matrix inform GRU layer, extracts informative features sequence words. These features utilized automatic detection hate speech social media. Our novelty lies using tuning procedure adapt model individual dataset characteristics. %Issues particular developing hate speech detection models % - Class imbalance issue % - Hate speech minute portion overall content social media generally published datasets % - How adequately upsample hate observations training without leading model overfitting? % % - We, like others, utilize downsampling approach training ensure class-balanced dataset passes model epoch % - We combine early stopping procedure utilizes validation dataset saves model state epoch minimal validation loss % % - These procedures, factors, lead variability resultant models %To maintain necessity downsampling training mitigating problems overfitting variability, develop ensemble approach hate speech classification, extending CNN-RNN-FC model topology shown successful hate speech classification. Our major contributions summarized answering following questions. \end{enumerate} Summary Results: Our best ensemble HON dataset achieves 65\% F1 Macro 83\% hate recall, surpassing performance HON dataset current state art models 33\%. We show ensemble models outperform individual models average 5\% hate recall 8\% F1 macro across datasets. When applied unlabeled Gab data, tuning improved pretrained models average 12\%, best tuned ensemble models achieving 57\% hate recall. Our model trained using weak supervision achieved 67\% hate recall posts Gab. %\sidd{We show ensemble models outperform individual components average 5\% hate recall 8\% F1 macro. % %We examine generalizability model framework novel data Gab, experimenting transfer learning weak supervision % - Transfer learning using small manually labeled set posts improved hate recall ensembles pre-trained HON OLID datasets 10\% Gab data. % % - We hypothesized integrating labeling HON OLID datasets combining would lead better generalizability model framework increasing size diversity training examples % This confirmed experiments transfer learning combined ensembles outperformed single dataset models Gab data average 8\% Hate recall HON models 5\% F1 Macro.} We propose two neural measures entrainment control consistency. We empirically validate measures demonstrating ability discriminate real fake sessions. Although measures perform slightly worse one reported \citet{Nasir2018}, believe measure captures entrainment consistency therefore better describes expected similarity two turns, overly broad measure entrainment. Most intriguingly, strict separation consistency entrainment leads correlations different entrainment measures account consistency, even corpus. This resembles results \citet{Perez2016}, found correlations differ based disentrainment treated. Our findings cast previous links conversation quality entrainment measures account consistency new light. It worth revisiting new ability distinguish consistency entrainment. In future work, intend expand network inputs prediction entire prior conversation context using RNNs attention. We also conduct analysis entrainment measures, e.g., feature, speaker sex, role, dialogue act. \section{Multiple testing correlations social variables} The correlations social variables entrainment measures vary greatly across retrainings underlying networks. This especially true , -values correlations dom ranging 7.5e-13 almost 1. To address this, retrained networks 100 times, recomputing Pearson correlations time. To control false discovery rate resulting multiple testing, use procedure \citet{Benjamini1995}. Each run consists three tests per measure. We sort group three tests values determine smallest value least one value, position sorting. Finally, determine largest -th smallest value run respective measure, level least one three correlations significant run measure. Using method, find 65 100 times correlation dom significant well 36 times lik. None correlations reach level significance, even terms ``raw'' values. For three 65 runs significant correlations dom, correlation valence. The three opposite valence among weakest, significant one 47th smallest value. All 36 significant correlations lik valence. Considering clear overall trends, conclude correlates positively dom lesser degree lik."," %This document is a model and instructions for \LaTeX. %This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes,  %or Math in Paper Title or Abstract. Online hate speech on social media has become a fast-growing problem in recent times. Nefarious groups have developed large content delivery networks across several mainstream  and fringe outlets  to deliver cascades of hate messages directed both at individuals and communities. Thus addressing these issues has become a top priority for large-scale social media outlets. Three key challenges in automated detection and classification of hateful content are the lack of clearly labeled data, evolving vocabulary and lexicon - hashtags, emojis, etc - and the lack of baseline models for fringe outlets such as Gab. In this work, we propose a novel framework with three major contributions.  We engineer an ensemble of deep learning models that combines the strengths of state-of-the-art approaches,  we incorporate a tuning factor into this framework that leverages transfer learning to conduct automated hate speech classification on unlabeled datasets, like Gab, and  we develop a weak supervised learning methodology that allows our framework to train on unlabeled data. Our ensemble models achieve an 83\% hate recall on the HON dataset, surpassing the performance of the state of the art deep models. We demonstrate that weak supervised training in combination with classifier tuning significantly increases model performance on unlabeled data from Gab, achieving a hate recall of 67\%."
"% % The following footnote without marker needed camera-ready % version paper. % Comment instructions uncomment 8 lines % ""final paper"" variant English. % . % % % final paper: en-us version % % % space normally used marker % This work licensed Creative Commons % Attribution 4.0 International License. % License details: % \url{http://creativecommons.org/licenses/by/4.0/}. } The following instructions directed authors papers submitted COLING-2020 accepted publication proceedings. All authors required adhere specifications. Authors required provide Portable Document Format version papers. The proceedings designed printing A4 paper. Authors countries access word-processing systems limited contact publication co-chairs Fei Liu Liang Huang soon possible. We may make additional instructions available \url{http://coling2020.org/}. Please check website regularly. We constructed character-level AT-ISR framework trained original architecture attention-based sequence-to-sequence ASR model. The main difference consists shorter sequences standard architecture. No new redesign needed ISR, hyperparameters used without changes. Transfer learning treats non-incremental ASR model teacher ISR student model. Student ISR learns attention alignment teacher model's, allowing simple mechanism incremental recognition. utilize attention alignment teacher model's, allowing simple mechanism incremental recognition. Various types models explored. The optimum performance achieved including ahead blocks, setting last character last set decoder input, keeping recurrent states across steps, utilizing attention transfer.","   This document contains the instructions for preparing a paper submitted   to COLING-2020 or accepted for publication in its proceedings. The document itself   conforms to its own specifications, and is therefore an example of   what your manuscript should look like. These instructions should be   used for both papers submitted for review and for final versions of   accepted papers. Authors are asked to conform to all the directions   reported in this document."
"When Natural Language Processing systems deployed production, interact users , many potential ways collecting feedback data rich interaction logs. For example, one ask explicit user ratings, collect user clicks, elicit user revisions get estimate well deployed system doing. However, user interaction logs primarily used one-off assessment system, e.g., spotting critical errors, detecting domain shifts, identifying successful use cases system production. This assessment used support decision keeping replacing system production. From machine learning perspective, using interaction logs evaluation purposes lost opportunities offline reinforcement learning . Logs user interactions gold mines off-policy learning, put use, rather forgotten one-off evaluation purpose. To move towards goal using user interaction logs learning, discuss challenges hindered RL employed real-world interaction users NLP systems far. Concretely, focus sequence-to-sequence learning NLP applications , machine translation, summarization, semantic parsing dialogue generation chatbots, since applications provide richest interaction users. For example, many machine translation services provide option users give feedback quality translation, e.g. collecting post-edits. Similarly, industrial chatbots easily collect vast amounts interaction logs, utilized offline RL methods. Recent work recognized poorly defined realities real-world systems hampering progress RL production environments. They address, amongst others, issues off-line learning, limited exploration, high-dimensional action spaces, unspecified reward functions. These challenges important RL control systems robots grounded physical world. However, severely underestimate human factor collecting feedback systems interacting humans, e.g. natural language. In following, thus present challenges encountered user-interactive RL NLP systems. With discussion, aim encourage NLP practitioners leverage interaction logs offline RL, inspire RL researchers steel algorithms challenging applications NLP. We used novel word learning paradigm, inspired classic studies psycholinguistics, assess BERT's syntactic generalization behavior two novel phenomena: English verb class alternations verb/object selectional restrictions. In cases address issue single few-shot learning fine-tuning model one two positive examples, finding BERT makes generalizations novel token based minimal experience, generalizations drive robust behavior test time. This novel word learning paradigm continue explored later work use large databases VerbNet , builds Levin's verb documentations providing larger database verb alternations sectional restrictions turned train test sentences BERT without hand-crafting. For verbal/object selectional restrictions, find BERT leverages indirect evidence expect unattested plausible verb/noun pairings unattested implausible pairings. These results provide evidence view model able attend patterns overtly realized data also implicit relationships tokens . The ability use indirect evidence, specifically indirect negative evidence, hallmark human language learning, results indicate models capable similar behavior simple novel word learning paradigm. For verbal alternations, find fine-tuned single frame, BERT routinely expects verb occur sister frame higher likelihood unrelated verbal frames. Interestingly, behavior consistently blocked model asked generalize frame involves object frame object lacking. This behavior consistent general bias towards transitivity model, suggests exciting direction study. Whether general bias exists, whether restricted settings limited evidence, whether changes verbs appear frequently fine-tuning training data question future research. Another question future research whether multilingual BERT would success alternation tests languages, would exhibit biases see English."," Large volumes of interaction logs can be collected from NLP systems that are deployed in the real world. How can this wealth of information be leveraged? Using such interaction logs in an offline reinforcement learning  setting is a promising approach. However, due to the nature of NLP tasks and the constraints of production systems, a series of challenges arise. We present a concise overview of these challenges and discuss possible solutions."
"In addition challenges multiword expression processing addressed previous work, non-compositionality , discontinuity , syntactic variability , The PARSEME shared task edition 1.2 focused another prominent challenge detecting MWEs, namely detection unseen MWEs. The problem unseen data common many NLP tasks. While rule-based unsupervised ML approaches less affected unseen data, supervised ML techniques often found prone overfitting. In respect, introduction language modelling objectives added different NLP tasks effect generalisation shown promising results. Further improvements brought pre-trained language models made popular approach multitude NLP tasks. One particular advantage models facilitate generalisation beyond task-specific annotations . MWEs inherent natural languages distinguishable syntactic semantic idiosyncracies . Since language models good capturing syntactic semantic features, believe suitable approach modelling MWEs. In particular, system relies BERT pre-trained language models . Additionally, render system semi-supervised means multi-task learning. The promising feature jointly learned MWEs dependency parse information . Accordingly, fine-tune BERT two different objectives: MWE detection dependency parsing. MWE learning done via token classification using linear layer top BERT, dependency parse trees learned using dependency tree CRF network . Our experiments confirm joint learning architecture effective capturing MWEs languages represented shared task.~ There large potential NLP leverage user interaction logs system improvement. We discussed algorithms offline RL offer promising solutions type learning problem. However, specific challenges offline RL arise due particular nature NLP systems collect human feedback real-world applications. We presented cases challenges found offered solutions helped. Furthermore, related identified challenges Challenges Real-World Reinforcement Learning . This overview may serve guide NLP researchers explore solutions offline RL, RL researchers test equip algorithms real-world challenges NLP applications."," This paper describes a semi-supervised system that jointly learns verbal multiword expressions  and dependency parse trees as an auxiliary task. The model benefits from pre-trained multilingual BERT.  BERT hidden layers are shared among the two tasks and we introduce an additional linear layer to retrieve VMWE tags. The dependency parse tree prediction is modelled by a linear layer and a bilinear one plus a tree CRF on top of BERT. The system has participated in the open track of the PARSEME shared task 2020 and ranked first in terms of F1-score in identifying unseen VMWEs as well as VMWEs in general, averaged across all $14$ languages."
"% \gn{Title candidate: ``Detecting Hallucinated Content ...'' . I wonder could also run methods extractive summarization outputs true references see many hallucinations detect? Just idea.} % However, recent studies abstractive text summarization % neural machine translation~ shown conditional neural sequence models prone hallucinate content faithful input text. This risk generating unfaithful content impedes safe deployment neural sequence generation models~. The first step building models suffer failures assessment identification hallucinated outputs. Prior work shown standard metrics used sequence evaluation, BLEU scores , ROUGE BERTScores , correlate well faithfulness model outputs~. They also require reference output text, limiting applicability detecting halluciations deployed system run-time. Very recent efforts~ started develop automatic metrics measure faithfulness output sequences. These methods use external semantic models, e.g. question-generation question-answering systems~ textual entailment inference models, score faithfulness tailored abstract text summarization. However, scores directly measure number hallucinated tokens %In addition, metrics often tailored evaluation summaries abstract text summarization correlate weakly human judgements. % \gn{Big question: difference word-level quality estimation, around long time, since least: \citet{bach-etal-2011-goodness} covered many WMT quality estimation shared tasks . This seems related works cited below, describing we'd need something new works would probably big question minds anyone familiar MT field. Also, would proposed methods detecting hallucination better SOTA word-level QE models?} % \gn{Similar motivation: Moreover, distinguish types errors terms fluency adequacy: substitution error referring simple morphological variation % considered way content word substitution changing meaning sentence.~.} We propose new task faithfulness assessment - hallucination detection token level, aims predict token machine output hallucinated faithful source input. This task use reference output assess faithfulness, offers us ability apply online generation scenario references available. Similar spirit proposed task, word-level quality estimation~ machine translation community predicts tokens correctly translated based human post-editing. However, distinguish errors terms fluency adequacy~. % A substitution error referring simple morphological variation considered content word substitution changing meaning sentence.~. In contrast estimating amount human post-editing work required fix errors, specifically focus hallucination errors. We measure hallucination two conditional sequence generation tasks -- abstractive summarization machine translation . For former, produce benchmark dataset recently released annotations ~. For MT, carefully design human assessment guideline create high-quality annotations. We also release human annotated data future research. To learn token-level hallucination prediction general conditional sequence generations tasks, propose novel method creates synthetic ``hallucinated"" data finetunes pretrained language model~ it. Without human annotated supervised training data, achieve average F1 around 0.6 across benchmark datasets, setting initial performance levels new task. % \cz{\st{We also computed sentence-level aggregated predictions achieve significantly higher correlations human scores previous methods. Finally, use new data study effect pretraining MT hallucination show actually produce faithful translations, }} We also show pretraining MT actually produce faithful translations, confirming recent findings abstractive summarization~. Predicting hallucination labels token-level provides tool diagnosing interpreting model outputs, allows us flag potential risks inference time previously unseen inputs. On hand, token-level labels also allow fine-grained controls target sequence learning full translation models. We show use token-level hallucination labels two case studies improve self-training learning noisy mined bitext low-resource MT. In cases, noise target text, either produced self-training teacher mining errors. However, outputs partially hallucinated rest output still useful training, show introducing different token-level loss truncation schemes. %To benefit self-training, filter noisy part also glean useful part model predictions applying token-level loss truncation control information flows target sequence training time. Our best methods outperform strong baselines large margin translation quality hallucination reduction. % We described MTLB-STRUCT, semi-supervised system based pre-trained BERT masked language modelling jointly learns VMWE tags dependency parse trees. The system ranked first open track PARSEME shared task - edition 1.2 shows overall state-of-the-art performance detecting unseen VMWEs. In future, plan augment dependency parsing architecture train dependency relation categories well dependency arcs. We also plan improve system making efficient order train dependency parsing module extra available unannotated datasets."," Neural sequence models can generate highly fluent sentences but recent studies have also shown that they are also prone to hallucinate additional content not supported by the input, which can cause a lack of trust in the model. To better assess the faithfulness of the machine outputs, we propose a new task to predict whether each token in the output sequence is hallucinated conditioned on the source input, and collect new manually annotated evaluation sets for this task. We also introduce a novel method for learning to model hallucination detection, based on pretrained language models fine tuned on synthetic data that includes automatically inserted hallucinations.   Experiments on machine translation and abstract text summarization demonstrate the effectiveness of our proposed approach -- we obtain an average F1 of around 60 across all the benchmark datasets. Furthermore, we demonstrate how to use the token-level hallucination labels to define a fine-grained loss over the target sequence in the low-resource machine translation and achieve significant improvements over strong baseline methods. We will release our annotated data and code to support future research."
"With rise social media e-commerce websites, huge interest analyzing networks tasks like link prediction, recommendation, community detection, etc. Traditionally, done learning finite-dimensional vector embeddings/representations nodes networks used downstream tasks. One challenges quality learned representation decreases network many missing links. This affects performance downstream tasks. This addressed using attribute similarity nodes connected usually similar attributes. For example, citation networks, papers related works cite other, social media, people similar interest follow other. In real-world graphs, nodes networks contain rich textual information attributes. So, need techniques exploit textual information learning node embeddings. The representation learning textual networks deals problem. \iffalse While networks sources relational information, many practical scenarios, nodes networks contain rich information attributes. When data form text networks referred textual networks, representation learning networks several applications diverse fields analyzing social media profiles biomedical networks. One challenges problem quality learned representation decreases network many missing links. This addressed using attribute similarity nodes connected usually similar attributes. For example, citation networks, papers related works cite other, social media, people similar interest follow other. So, exploiting this, one predict edges network. The main aim representation learning network learn embeddings, finite-dimensional vector representations nodes graph. %Representation learning networks uses edge/link weights labels objective function learn embeddings. These finite-dimensional vector representations node graph. In paper study problem textual networks, nodes networks equipped attributes content form textual information . These learned embeddings used problems like link prediction, community detection, social network analysis, on. One challenges problem quality learned representation decreases network many missing links. This addressed using attribute similarity nodes connected usually similar attributes. For example, citation networks, papers related works cite other, social media, people similar interest follow other. So, exploiting this, one predict edges network. %For achieving representation learning textual networks, propose adversarial framework using textual similarity discriminator structural similarity generator. \fi Recent methods representation learning textual networks involves learning two embeddings, one structure information , textual information . The embeddings learned similar nodes connected edge. The challenging task learn combined text structure embeddings, previous approaches use joint learning framework defining loss function models inter-modal similarities structure textual information nodes connected edge, addition intra-modal similarities. For example, consider nodes embeddings . The similarity embeddings used modelling intra-model similarity structure information, hand similarity used intra-model similarity text information. For inter-model similarity, similarity used modelling similarity structure text, vice versa. All similarities modelled using skip-gram loss function . The main disadvantage models dependent edge labels embedding learning. This make unable learn embeddings nodes present training stage. The way modelled learn unseen nodes embeddings mapper function textual information structure embeddings seen nodes apply unseen nodes getting structure embeddings. This result poor performance downstream tasks involving unseen nodes mapping function cannot fully capture structural information nodes. Recenlty, issue addressed using variational autoencoder framework structure text embeddings. Although achieved better performance mapper function-based models, disadvantage autoencoder framework limits information learned structure embeddings used predicting text features decoder. In paper, propose adversarial model generator learns structure embeddings text embedding based discriminator structure embeddings based generator. For generator, use supervision edge-connectivity text embedding similarity learn structure embeddings. For discriminator model, text embeddings made dissimilar node pair generated generator similar node pairs graph. This training make text similarity discriminator approximate actual similarity network. Through framework establish model efficiently amalgamate fuse information text graph text structure embeddings use information modality learning. In addition this, proposed adversarial approach extended embedding learning unseen nodes training dataset. This achieved directly using discriminator based text-similarity supervision post-training stage. This help efficiently learning unseen structure embeddings restrict embedding learning using predict text features like VHE . The performance model depends upon well exploit unstructured textual information, need powerful discriminator. To achieve this, use context-aware embeddings, node different text embedding edges. We address problem proposing novel technique combining two context-aware attention mechanism. The first based mutual attention word embeddings text across pair nodes. The topological attention mechanism. This uses structure embeddings node pairs attend text learn topology-aware text embedding. It reduce adverse effects trying make text embeddings similar textual information connected nodes need match. Because, model better representation capacity learns similarity topological mutual attention. The following main contributions paper. An adversarial technique attributed network representation learning. Here, addition supervision training data, discriminator using text embeddings used give supervision structure embeddings. A novel text embedding learning technique uses mutual topological attention. Extensive comparative study downstream tasks link prediction node classification. Experiments link prediction unseen nodes. \iffalse We evaluated proposed method three datasets Cora, Zhihu, Hepth link prediction. We observed model performs better state-of-the-art methods almost settings three datasets. The performance model especially high low data regime. In Zhihu dataset, model show performance improvement previous state-of-the-art lowest supervision setting. A similar observation made node classification task Cora dataset, adversarial technique achieve state-of-the-art performance. As mentioned earlier, main advantage model ability care representation learning unseen nodes. We evaluated quality embeddings link prediction task edges involving unseen nodes, ACNE achieves state-of-the-art performance settings three datasets. On Zhihu dataset, gave impressive improvement improvement previous methods low-data regime. \fi \iffalse \fi %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% In work, proposed new evaluation task hallucination detection conditional sequence generation created human-annotated benchmark datasets. We also proposed novel general-purpose method learn task, showed models used define fine grained losses improve low resource models machine translation. In future, hope create large-scale pretrained evaluation model datasets models evaluated, also would extend method data-to-text generation scenarios. We also interested investigating leverage detection methods mitigate hallucination problems conditional sequence generation."," \label{section:abstract}  Representation learning of textual networks poses a significant challenge as it involves capturing amalgamated information from two modalities:  underlying network structure, and  node textual attributes. For this, most existing approaches learn embeddings of text and network structure by enforcing embeddings of connected nodes to be similar. Then for achieving a modality fusion they use the similarities between text embedding of a node with the structure embedding of its connected node and vice versa. %Then for achieving modality fusion they model intra-modal similarities involving networks structure and textual attributes of  nodes in an edge.  This implies that these approaches require edge information for learning embeddings and they cannot learn embeddings of unseen nodes. In this paper we propose an approach that achieves both modality fusion and the capability to learn embeddings of unseen nodes. The main feature of our model is that it uses an adversarial mechanism between text embedding based discriminator, and structure embedding based generator to learn efficient representations. Then for learning embeddings of unseen nodes, we use the supervision provided by the text embedding based discriminator. In addition this, we propose a novel architecture for learning text embedding that can combine both mutual attention and topological attention mechanism, which give more flexible text embeddings. Through extensive experiments on real-world datasets, we demonstrate that our model makes substantial gains over several state-of-the-art benchmarks. In comparison with previous state-of-the-art, it gives up to 7\% improvement in performance in predicting links among nodes seen in the training and up to 12\% improvement in performance in predicting links involving nodes not seen in training. Further, in the node classification task, it gives up to 2\% improvement in performance."
"Streaming Automatic Speech Recognition researches made way everyday products. Smart speakers transcribe utterances streaming fashion, allowing users downstream applications see instant output terms partial transcriptions. There growing interest community develop end-to-end streaming ASR models, transcribe accurately run compactly edge devices. Amongst streaming E2E models, Recurrent Neural Network Transducer candidate many applications. RNN-T trained loss function enforce temporal alignment training transcripts audio. As result, RNN-T suffers token emission delays - time token spoken transcript token emitted. Delayed emissions tokens adversely affects user experiences downstream applications end-pointer. Some existing work tried mitigate token emission delays streaming RNN-Ts. We introduce Section. Other works utilized semi-streaming non-streaming models predict better token emission time, cost overall latency transcripts. In work, propose novel loss function streaming RNN-T, resultant trained model called Alignment Restricted RNN-T . It utilizes audio-text alignment information guide loss computation. In Section, show theoretically, Ar-RNN-T loss function faster compute results better audio-token alignment. In Section, empirically compare proposed method existing works monotonic RNN-T training two data set: LibriSpeech voice command. In results section, Section, show improvement training speed used tandem end-pointer, Ar-RNN-T provides unprecedentedly refined control latency-WER trade-offs RNN-T models. include bib file like this:"," There is a growing interest in the speech community in developing Recurrent Neural Network Transducer  models for automatic speech recognition  applications. RNN-T is trained with a loss function that does not enforce temporal alignment of the training transcripts and audio. As a result, RNN-T models built with uni-directional long short term memory  encoders tend to wait for longer spans of input audio, before streaming already decoded ASR tokens. In this work, we propose a modification to the RNN-T loss function and develop Alignment Restricted RNN-T  models, which utilize audio-text alignment information to guide the loss computation. We compare the proposed method with existing works, such as monotonic RNN-T, on LibriSpeech and in-house datasets. We show that the Ar-RNN-T loss provides a refined control to navigate the trade-offs between the token emission delays and the Word Error Rate . The Ar-RNN-T models also improve downstream applications such as the ASR End-pointing by guaranteeing token emissions within any given range of latency. Moreover, the Ar-RNN-T loss allows for bigger batch sizes and 4 times higher throughput for our LSTM model architecture, enabling faster training and convergence on GPUs."
". % % % final paper: en-us version % % space normally used marker This work licensed Creative Commons Attribution 4.0 International License. License details: \url{http://creativecommons.org/licenses/by/4.0/}. } In work, present detailed analysis RNN-T model's token emission delays impact downstream applications. We propose modification RNN-T loss uses alignment information restrict paths optimized training. We call solution Alignment Restricted RNN-T show control token delays RNN-T models systematically using tunable parameters training, also significantly improving training throughput. Using proposed solution, show improve accuracy downstream applications ASR end-pointing system significantly reduce latency early cut-offs. For alignments, found splitting word time-steps equally among sub-words worked best, however bootstrapping hybrid model using target word-piece dictionary potentially give better alignments lead improved accuracy Ar-RNN-T, may explored future. References produced using bibtex program suitable BiBTeX files . The IEEEbib.bst bibliography style file IEEE produces unsorted bibliography list. -------------------------------------------------------------------------","   Interpretability and explainability of deep neural networks are challenging due to their scale, complexity, and the agreeable notions on which the explaining process rests. Previous work, in particular, has focused on representing internal components of neural networks through human-friendly visuals and concepts. On the other hand, in real life, when making a decision, human tends to rely on similar situations and/or associations in the past. Hence arguably, a promising approach to make the model transparent is to design it in a way such that the model explicitly connects the current sample with the seen ones, and bases its decision on these samples.   Grounded on that principle, we propose in this paper an explainable, evidence-based memory network architecture, which learns to summarize the dataset and extract supporting evidences to make its decision. Our model achieves state-of-the-art performance on two popular question answering datasets . Via further analysis, we show that this model can reliably trace the errors it has made in the validation step to the training instances that might have caused these errors. We believe that this error-tracing capability provides significant benefit in improving dataset quality in many applications."
". % % final paper: en-us version % space normally used marker This work licensed Creative Commons Attribution 4.0 International License. License details: \url{http://creativecommons.org/licenses/by/4.0/}. } Discourse parsing important upstream task within area Natural Language Processing active field research last decades. In work, focus discourse representations English language, research %on discourse analysis English language surrounding one two main theories behind discourse, Rhetorical Structure Theory proposed interpreting discourse according PDTB . While theories strengths, application RST theory, encoding documents complete constituency discourse trees , shown many crucial implications real world problems. A tree defined set EDUs , approximately aligning clause-like sentence fragments, acting leaves tree. Adjacent EDUs sub-trees hierarchically aggregated form larger constituents, internal nodes containing nuclearity label, defining importance subtree local context relation label, defining type semantic connection two subtrees . In work, focus structure nuclearity prediction, taking relations account. Previous research shown use RST-style discourse parsing system component enhance important tasks, sentiment analysis, summarization text categorization . More recently, also suggested discourse structures obtained RST-style manner complementary learned contextual embeddings, like popular BERT approach . Combining approaches shown support tasks linguistic information complete documents critical, argumentation analysis . Even though discourse parsers appear enhance performance variety tasks, full potential using linguistically inspired approaches downstream applications unleashed yet. The main open challenges integrating discourse NLP downstream tasks deliver even greater benefits combination discourse parsing difficult task itself, inherently high degree ambiguity uncertainty lack large-scale annotated datasets, rendering initial problem severe, data-driven approaches cannot applied full potential. The combination two limitations one main reasons limited application neural discourse parsing diverse downstream tasks. While neural discourse parsers proposed , still cannot consistently %strongly outperform traditional approaches applied RST-DT dataset, amount training data arguably insufficient data-intensive approaches. %due extra effort integrate discourse trees models well two major problems, big breakthrough usage discourse parsing still happened. In work, alleviate restrictions effective efficient use discourse mentioned introducing novel approach combining newly proposed large-scale discourse treebank data-driven neural discourse parsing strategy. More specifically, employ novel MEGA-DT ``silver-standard"" discourse treebank published containing 250,000 discourse annotated documents Yelp'13 sentiment dataset , nearly three orders magnitude larger commonly used RST-style annotated discourse treebanks . Given new dataset previously unseen number full RST-style discourse trees, revisit task neural discourse parsing, previously attempted others rather limited success. We believe one reason previous neural models could yet consistently outperform traditional approaches, heavily relying feature engineering , lack generalisation using deep learning approaches small RST-DT dataset, containing 385 discourse annotated documents. This makes us believe using advanced neural discourse parser combination large training dataset lead significant performance gains. %, also across datasets, capturing general discourse phenomena avoiding potential overfitting training corpus. Admittedly, even though MEGA-DT contains huge number datapoints train on, automatically annotated, potentially introducing noise biases, negatively influence performance newly proposed neural discourse parser solely trained dataset. A natural intuitive approach make use neural discourse parser datasets combine training, pretraining large-scale ``silver-standard"" corpus subsequently fine-tuning RST-DT human annotated datasets. This way, general discourse structures could learned large-scale treebank enhanced human-annotated trees. With results shown paper strongly suggesting new discourse parser encode discourse effectively, hope efforts prompt researchers develop linguistically inspired applications based discourse parser. % downstream models area NLP. Our contributions paper are: % %on train neural discourse parser large scale ``silver-standard"" discourse trees. With new approach, drastically increase amount available training data available discourse parsers sufficiently large train modern, data-driven deep learning approaches task, hindering application new methodologies shift domain discourse parsers training data domain application deminishes applicability performance generated discourse trees domain outside news , instructions domains. URL segmentation applications TTS web search. Our contributions include curated URL data set highly accurate RNN model boosted pre-training Knowledge Graph entities. We plan releasing version dataset conference. \paragraph{\LaTeX-specific details:} For anonymized submission, ensure {\small\verb|\aclfinalcopy|} top document commented out, filled paper ID number {\small\verb|***|} appears {\small\verb|\def\aclpaperid{***}|} definition top document. For camera-ready submission, ensure {\small\verb|\aclfinalcopy|} top document commented out."," RST-based discourse parsing is an important NLP task with numerous downstream applications, such as summarization, machine translation and opinion mining. In this paper, we demonstrate a simple, yet highly accurate discourse parser, incorporating recent contextual language models. Our parser establishes the new state-of-the-art  performance for predicting structure and nuclearity on two key RST datasets, RST-DT and Instr-DT. We further demonstrate that pretraining our parser on the recently available large-scale ``silver-standard"" discourse treebank MEGA-DT provides even larger performance benefits, suggesting a novel and promising research direction in the field of discourse analysis."
"The last several years seen land rush research machine reading comprehension various dataset proposed SQuAD1.1, SQuAD2.0, NewsQA CoQA . Different extractive MRC, RACE multi-choice MRC dataset proposed . RACE extracted middle high school English examinations China. Figure 1 shows example passage two related questions RACE. The key difference RACE previously released machine comprehension datasets answers RACE often cannot directly extracted passages, illustrated two example questions Table . Thus, answering questions needs inferences. \end{center} \end{table} % Recently, pretrained language models BERT , RoBERTa , ALBERT achieved great success MMRC tasks. Notably, Megatron-LM 48 layer BERT 3.9 billion parameters yields highest score RACE leaderboard single ensemble settings. The key point model MMRC is: first encode context, question, options BERT like LM, add matching network top BERT score options. Generally, matching network various . proposes option comparison network compare options word-level better identify correlations help reasoning. proposes dual co-matching network models relationship among passage, question answer options bidirectionally. All matching networks show promising improvements compared pretrained language models. One point common answer together distractors jointly considered name multi-choice models. We argue options concerned separately two reasons, 1) human works MMRC problem, always consider options one one select one highest confidence. 2) MMRC suffers data scarcity problem. Multi-choice models inconvenient take advantage MRC dataset. In paper, propose single-choice model MMRC. Our model considers options separately. The key component method binary classification network top pretrained language models. For option given context question, calculate confidence score. Then select one highest score final answer. In training decoding, right answer distractors modeled independently. Our proposed method gets rid multi-choice framework, leverage amount resources. Taking SQuAD example, take context, one question corresponding answer positive instance classification golden label 1. In way many QA dataset used enhance RACE. Experimental results show single-choice model performs better multi-choice models, addition transferring knowledge QA dataset, single model achieves 90.7\% ensemble model achieves 91.4\%, best score leaderboard. In work, proposed rather simple, yet highly effective discourse parser, utilizing recent neural BERT-based language models combination structural features. The integration input-features within standard shift-reduce framework well unprecedented use recent large-scale ``silver-standard"" discourse parsing datasets pretraining reaches new state-of-the-art performance both, RST-DT Instr-DT treebanks. We show new, neural discourse parser already achieves better similar performance trained evaluated RST-DT Instr-DT datasets, however, consistent significant SOTA result reached incorporating pretraining MEGA-DT corpus. This refutes previous findings , stating neural techniques word embeddings provide little gains task. We demonstrated gains achieved The presented pretraining even small subset approach silver-standard MEGA-DT dataset , also validates usefulness additional supervision task calls work area."," Multi-choice Machine Reading Comprehension  aims to select the correct answer from a set of options based on a given passage and question. Due to task specific of MMRC, it is non-trivial to transfer knowledge from other MRC tasks such as SQuAD, Dream. In this paper, we simply reconstruct multi-choice to single-choice by training a binary classification to distinguish whether a certain answer is correct. Then select the option with the highest confidence score. We construct our model upon ALBERT-xxlarge model and estimate it on the RACE dataset. During training, We adopt AutoML strategy to tune better parameters. Experimental results show that the single-choice is better than multi-choice. In addition, by transferring knowledge from other kinds of MRC tasks, our model achieves a new state-of-the-art results in both single and ensemble settings."
"% Images another important approach expressing feelings emotions addition using text communication. In mobile messaging apps, images generally classified emojis stickers. Emoji kind small picture already stored keyboard mobile operational systems, \ie iOS Android. Emojis pre-designed mobile phone vendor number emoji limited, users design emoji themselves. Different inflexible emojis, sticker image graphicon essentially, users draw modify images sticker upload chatting app themselves. The using stickers online chatting usually brings diversity expressing emotion. Since emojis sometimes used help reinforce simple emotions text message due small size, variety limited. Stickers, hand, regarded alternative text messages, usually include cartoon characters high definition. They express much complex vivid emotion emojis. Most messaging apps, WeChat, Telegram, WhatsApp, Slack provide convenient ways users download stickers free, even share self-designed ones. We show chat window including stickers Figure. % Stickers becoming popular online chat. First, sending sticker single click much convenient typing text 26-letter keyboard small mobile phone screen. Second, many implicit strong emotions difficult express words captured stickers vivid facial expressions body language. However, large scale use stickers means always straightforward think sticker best expresses one's feeling according current chatting context. Users need recall stickers collected selected appropriate one, difficult time-consuming. % Consequently, much research focused recommending appropriate emojis users according chatting context. Existing works as, mostly based emoji recommendation, predict probable emoji given contextual information multi-turn dialog systems. In contrast, works recommend emojis based text images posted user. As sticker recommendation, existing works apps like Hike QQ directly match text typed user short text tag assigned sticker. However, since lots ways expressing emotion, hard capture variants utterance tags. % To overcome drawbacks, propose sticker response selector sticker selection early work, address task sticker response selection multi-turn dialog. We focus two main challenges work: Since existing image recognition methods mostly built real-world images, capture semantic meaning sticker challenging. Understanding multi-turn dialog history information crucial sticker recommendation, jointly modeling candidate sticker multi-turn dialog challenging. % % % % % % Herein, propose novel sticker recommendation model, namely sticker response selector , sticker response selection multi-turn dialog. Specifically, SRS first learns representations dialog context history using self-attention mechanism learns sticker representation convolutional neural network . % % % Next, SRS conducts deep matching sticker utterance produces interaction results every utterance. % % Finally, SRS employs fusion network consists sub-network fusion RNN fusion transformer learn short long term dependency utterance interaction results. The final matching score calculated interaction function. To evaluate performance model, propose large number multi-turn dialog dataset associated stickers one popular messaging apps. Extensive experiments conducted dataset show SRS significantly outperforms state-of-the-art baseline methods commonly-used metrics. % However, user's sticker selection depend matching degree dialog context candidate sticker image, also depends user's preference using sticker. When users decide use sticker response multi-turn dialog, may choose favorite one appropriate stickers final response. % % % We assume user tends use recently used sticker dialog history, recently-used-sticker represent user's preference sticker selection. An example shown Figure. To verify assumption, retrieve 10 recently-used-stickers user calculate proportion whether currently used sticker appeared 10 stickers. The result shows 54.09\% stickers exist 10 recently used sticker set. Hence, reach conclusion users strong personal preference selecting sticker response current dialog context. However, cases, also indicates tendency re-use stickers, necessarily preference. % Motivated observation, work, take one step improve previously proposed SRS framework user preference modeling. Overall, propose novel sticker recommendation model considers user preference, namely Preference Enhanced Sticker Response Selector . Specifically, PESRS first employs convolutional network extract features candidate stickers. Then, retrieve recent user sticker selections user preference modeling module employed obtain user preference representation. Next, conduct deep matching candidate sticker utterance SRS. Finally, use gated fusion method combine deep matching result user preference final sticker prediction. % The key success PESRS lies design user preference modeling module, identify user's favorite sticker also consider current dialog context. % Motivated this, first propose recurrent neural network based position-aware sticker modeling module encodes recently used stickers chronological order. Then, employ key-value memory network store sticker representations values corresponding dialog context keys. Finally, use current dialog context query key-value memory obtain dynamic user preference current dialog context. % We empirically compare PESRS SRS public dataset\footnote{https://github.com/gsh199449/stickerchat} proposed early work. This large-scale real-world Chinese multi-turn dialog dataset, dialog context multiple text utterances response sticker image. Experimental results show dataset, newly proposed PESRS model significantly outperform existing methods. Particularly, PESRS yields 4.8\% 7.1\% percentage point improvement terms compared early work SRS. % In addition comprehensive evaluation, also evaluate proposed user preference memory fine-grained analysis. The analysis reveals model leverages user's recent sticker selection history provides us insights achieve big improvement state-of-the-art methods. This work substantial extension previous work reported WWW 2020. The extension article includes user preference modeling framework existing methods, proposal new framework sticker selection multi-turn dialog. Specifically, contributions work include following: The rest paper organized follows: We summarize related work \S. \S introduces data collection method statistics proposed multi-turn dialog sticker selection dataset. We formulate research problem \S elaborate approach \S. \S gives details experimental setup \S presents experimental results. Finally, \S concludes paper. % In paper, propose single-choice model MMRC consider options separately. Experiments results demonstrate method achieves significantly improvements taking advantage MRC datasets, achieve new state-of-the-art performance. We plan consider difference two methods combine together future study. File emnlp2019.tex Based style files ACL 2019, Based style files EMNLP 2018, Based style files ACL 2018, Based style files ACL-2015, improvements taken NAACL-2016 style Based style files ACL-2014, were, turn, based ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009, EACL-2009, IJCNLP-2008... Based style files EACL 2006 e.agirre@ehu.es Sergi.Balari@uab.es ACL 08 Joakim Nivre Noah Smith \documentclass[11pt,a4paper]{article} \usepackage[hyperref]{emnlp-ijcnlp-2019} \usepackage{latexsym} \usepackage{times} \usepackage{soul} \usepackage{url} \usepackage[utf8]{inputenc} \usepackage[small]{caption} \usepackage{graphicx} \usepackage{subfigure} \usepackage{amsmath} \usepackage{booktabs} \usepackage{natbib} \usepackage{xcolor} \urlstyle{same} \usepackage{fancyhdr,graphicx,amssymb} \usepackage[ruled,vlined]{algorithm2e} \usepackage{multirow} \usepackage{url} \usepackage{tikz} \usepackage{subfigure} \usepackage{xcolor} \usepackage{tcolorbox} \usepackage{helvet} Required \usepackage{courier} Required \newcommand\BibTeX{B{\sc ib}\TeX} \newcommand\confname{EMNLP-IJCNLP 2019} \newcommand\conforg{SIGDAT} \title{Multi-choice Machine Reading Comprehension Two-stage Training} \author{First Author \\ Affiliation / Address line 1 \\ Affiliation / Address line 2 \\ Affiliation / Address line 3 \\ email@domain \\\And Second Author \\ Affiliation / Address line 1 \\ Affiliation / Address line 2 \\ Affiliation / Address line 3 \\ email@domain \\} \date{}","   Stickers with vivid and engaging expressions are becoming increasingly popular in online messaging apps, and some works are dedicated to automatically select sticker response by matching the stickers image with previous utterances.   However, existing methods usually focus on measuring the matching degree between the dialog context and sticker image, which ignores the user preference of using stickers.   Hence, in this paper, we propose to recommend an appropriate sticker to user based on multi-turn dialog context and sticker using history of user.   Two main challenges are confronted in this task.   One is to model the sticker preference of user based on the previous sticker selection history.   Another challenge is to jointly fuse the user preference and the matching between dialog context and candidate sticker into final prediction making.   To tackle these challenges, we propose a Preference Enhanced Sticker Response Selector  model.   Specifically, PESRS first employs a convolutional based sticker image encoder and a self-attention based multi-turn dialog encoder to obtain the representation of stickers and utterances.   Next, deep interaction network is proposed to conduct deep matching between the sticker and each utterance.   Then, we model the user preference by using the recently selected stickers as input, and use a key-value memory network to store the preference representation.   PESRS then learns the short-term and long-term dependency between all interaction results by a fusion network, and dynamically fuse the user preference representation into the final sticker selection prediction.   Extensive experiments conducted on a large-scale real-world dialog dataset show that our model achieves the state-of-the-art performance for all commonly-used metrics.   Experiments also verify the effectiveness of each component of PESRS.   %"
".} Neural machine translation boosted machine translation significantly recent years . However, still unclear NMT models work due black-box nature neural networks. Better understandings NMT models could guide us improving NMT systems. Currently studies towards understanding NMT models take account subword-based models. Deeper character-based models shown perform better BPE-based models . In paper, try investigate working mechanism CHAR models. We explore ability CHAR models learn word senses morphological inflections attention mechanism. Previous studies tried interpret understand NMT models interpreting attention weights , using gradients , applying layer-wise relevance propagation , probing classification tasks , intrinsic analysis . However, probed character-based representations. explored character-aware word-level representations, investigate fully character-level representations, also studied . We apply composition methods explore CHAR models learn linguistic knowledge attention extracts features directly characters. Probing classification tasks emerged popular method interpret internal representations neural networks. Given probing classifier, input usually representation word output corresponding linguistic tag. CHAR models pose new challenges interpretability, investigate whether probe CHAR models way similar word-based models. In addition, extract word sense morphological information full word individual hidden states, information distributed across multiple states? This implications interpreting neural CHAR models, also inform novel architectures, sparse attention mechanisms. Thus first investigate ability CHAR models learn word senses morphology Section . We apply different methods compose information characters demonstrate word-level information distributed characters characters different positions play different roles learning linguistic knowledge. We also explore effect encoder depth answer CHAR models outperform BPE-based models settings deeper encoder. The probing results show CHAR models need layers learn word senses. Then Section , move explore attention mechanism. The distribution pattern shows separators attract much attention compared characters. To study effect enforcing characters capture full word-level information, investigate sparse attention mechanism, i.e. model attends separators, viewed word-level attention. The BLEU score drops 1.2 points apply word-level sparse attention. This implies attending separators single attention head workable enough extract necessary information. The main findings summarized follows: In previous work, propose task multi-turn sticker response selection, recommends appropriate sticker based multi-turn dialog context history without relying external knowledge. However, method focuses measuring matching degree dialog context sticker image, ignores user preference using stickers. Hence, paper, propose Preference Enhanced Sticker Response Selector recommend appropriate sticker user based multi-turn dialog context sticker using history user. Specifically, PESRS first learns representation utterance using self-attention mechanism, learns sticker representation CNN. Second, deep interaction network employed fully model dependency sticker utterances. The deep interaction network consists co-attention matrix calculates attention word utterance unit sticker representation. Third, bi-directional attention used obtain utterance-aware sticker representation sticker-aware utterance representations. Next, retrieve recent user sticker selections, propose user preference modeling module consists position-aware history encoding network key-value based memory network generate user preference representation dynamically according current dialog context. Then, fusion network models short-term long-term relationship interaction results, gated fusion layer applied fuse current dialog interaction results user preference representation dynamically. Finally, fully-connected layer applied obtain final sticker prediction using output gated fusion layer. Our model outperforms state-of-the-art methods including previous method SRS metrics experimental results also demonstrate effectiveness module model. In near future, aim propose personalized sticker response selection system."," Recent work has shown that deeper character-based neural machine translation  models can outperform subword-based models. However, it is still unclear what makes deeper character-based models successful. In this paper, we conduct an investigation into pure character-based models in the case of translating Finnish into English, including exploring the ability to learn word senses and morphological inflections and the attention mechanism. We demonstrate that word-level information is distributed over the entire character sequence rather than over a single character, and characters at different positions play different roles in learning linguistic knowledge. In addition, character-based models need more layers to encode word senses which explains why only deeper models outperform subword-based models. The attention distribution pattern shows that separators attract a lot of attention and we explore a sparse word-level attention to enforce character hidden states to capture the full word-level information. Experimental results show that the word-level attention with a single head results in 1.2 BLEU points drop."
"A prerequisite relation pedagogical relation indicates order concepts presented learners. The relation used guide presentation sequence topics subjects design academic programs, lectures, curricula instructional materials. %such textbooks study guides. In work, present systems automatically detect prerequisite relations Italian language context PRELEARN shared task EVALITA 2020 . %. The evaluation submissions considers: in-domain cross-domain scenarios defined either inclusion exclusion target domain training set. The four domains 'data mining' , 'geometry' , 'precalculus' , 'physics' . type resources used train model -- raw text VS. structured information. % four domains, namely 'data mining', 'geometry', 'precalculus' 'physics'. % PRELEARN participants submit systems per-domain evaluation, considering in-domain cross-domain scenarios, % well discriminate kind resources models used, namely raw text distributional textual corpora, structured information knowledge bases. % Additionally, difference in-domain cross-domain lies inclusion exclusion target domain training set. The combination settings defined four PRELEARN subtasks. Formally, prerequisite relation exists two concepts one known beforehand order understand other. For PRELEARN task, given pair concepts, relation exists latter concept prerequisite former. Therefore, task binary classification task. We approach problem two perspectives: handcrafted features based lexical complexity pre-trained embeddings. We employed static embeddings Wikipedia Wikidata, contextual embeddings Italian-BERT model. CHAR models shown perform better BPE-based models NMT yet pose new challenges interpretability. In paper, investigate CHAR models via WSD six morphological probing tasks learn CHAR models learn word senses morphology, case translating Finnish English. We also explore attention distribution pattern sparse word-level attention learn working mechanism attention. In probing tasks, find separators also captured linguistic knowledge. We apply different composition methods characters word, demonstrate word sense morphological information distributed characters rather specific characters. Moreover, characters different positions play different roles learning linguistic knowledge. CHAR models better learning morphology need complicated composition method, randomly initialized LSTM, extract encoded information. These results probing tasks show extract word sense information morphological features character-level hidden states features encoded different ways. In addition, explore effect encoder depth show CHAR models require layers encode word senses, explains deeper CHAR models outperform BPE-based models. The attention distribution shows separators attract lot attention, show sparse word-level attention attending separators workable enough translation. As shown characters different positions specialize learning word senses morphology, interesting explore sparse attention multiple heads future could learn extract features different aspects.",   English.   We present our systems and findings for the prerequisite relation learning task  at EVALITA 2020. The task aims to classify whether a pair of concepts hold a prerequisite relation or not. We model the problem using handcrafted features and embedding representations for in-domain and cross-domain scenarios.  Our submissions ranked first place in both scenarios with average F1 score of $0.887$ and $0.690$ respectively across domains on the test sets. We made our code freely available\footnote{\url{https://github.com/ajason08/EVALITA2020_PRELEARN}\label{code}}.
"Task-oriented dialog systems commonplace automated systems interact end users, including digital assistants, technical support agents, various website navigation helpers. An essential part task-oriented dialog system natural language generation , consumes data, typically fed form dialog act, converts natural language output served end user. The natural language response NLG component 1) contain essential information, 2) contextualized around user request, 3) natural sounding. Such system requires consideration content planning, correctness, grammaticality, naturalness. NLG systems employed commercial settings typically based template-based text generation techniques . In these, humans author minimal set responses templates placeholder slot values. These slots later filled runtime, dialog input. Although template-based NLG modules appealing due deterministic nature, inherent correctness, low latency, major drawbacks: First, separate templates need authored different response variations; behavior unfavorable scaling. Second, templates authored particular domain commonly reusable. Lastly, matter complexity language instilled templates, form strictly discrete set responses, therefore bound limited response naturalness. More recently, advances neural-network-based language generation prompted new direction NLG research . The process typically split two steps: serialization input data flattened meaning representation , using neural generation model generate natural language response conditioned MR. The models trained data includes MR, response pairs, therefore able generate desired responses MRs training data, also expected form coherent responses novel MRs, owing generalization ability machine learning backbone. However, deploying neural NLG systems industry setting quite challenging. First, trivial train model reliably presents input data high fidelity required user-serving dialog system. Second, models require much high-quality human-annotated data, resource intensive. Consequently, data annotation major limiting factor scaling model-based NLG across domains languages. In work, detail approach production-level neural NLG, focus scalability data efficiency. Adopting tree-structured MR framework introduced Balakrishnan et al.~\shortcite{Balakrishnan2019constrainednlg}, allows better control generated responses, train sequence-to-sequence RNN models produce high-fidelity responses. We employ multitude techniques reducing amount required data, primarily powered eliminating ``hidden'' redundancy grouping data points similar semantics buckets. We train models either reduced data, increasing size dataset using novel synthetic augmentation technique. We also employ large, pre-trained attention-based language models, fine-tuning datasets, using novel methods distill knowledge smaller sequence-to-sequence models. Further, train models data multiple domains, showing gains models trained individual domains domains semantically close together. We conclude compiled list best practices production-level NLG model development based analyses, present runbook. We tackle task prerequisite relation learning using variety systems explore three set features: handcrafted features based complexity intuitions, embedding models Wikipedia Wikidata, contextual embedding Italian-BERT model. We examine capabilities models in-domain cross-domain scenarios. 4 domains, raw-text versus structured-information settings. Our models ranked first subtask PRELEARN competition EVALITA 2020. We found although Italian-BERT model outperformed others, simpler models show competitive results. A limitation work used possible domains experiments. We plan examine impact using combination possible domains training set performance models."," Natural language generation  is a critical component  in conversational systems, owing to its role of formulating a correct and natural text response. Traditionally, NLG components have been deployed using template-based solutions. Although neural network solutions recently developed in the research community have been shown to provide several benefits, deployment of such model-based solutions has been challenging due to high latency, correctness issues, and high data needs.  In this paper, we present approaches that have helped us deploy data-efficient neural solutions for NLG in conversational systems to production.  We describe a family of sampling and modeling techniques to attain production quality with light-weight neural network models using only a fraction of the data that would be necessary otherwise, and show a thorough comparison between each. Our results show that domain complexity dictates the appropriate approach to achieve high data efficiency. Finally, we distill the lessons from our experimental findings into a list of best practices for production-level NLG model development, and present them in a brief runbook. Importantly, the end products of all of the techniques are small sequence-to-sequence models  that we can reliably deploy in production."
"Definitions important role scientific literature define major concepts article operates. They used many automatic text analysis tasks, question answering, ontology matching construction, formal concept analysis, text summarization. Intuitively, definitions basic building blocks scientific article used help properly describe hypotheses, experiments, analyses. It often difficult determine certain definition lies text sentences around may similar style. Automatic definition extraction important field natural language processing used improve text analysis search. %Natasha: adding formal definition formal definitions Definitions play key role mathematics, creation use differ \enquote*{everyday language} definitions. A comprehensive study given series works Edwards Ward~, , , %, inspired writings Richard Robinson~ lexicographer Sidney Landau~. %They distinguish extracted definitions report usage truth value~, stipulated create usage create concepts truth value. % Nat - fixed sentence %Moreover, stipulated definition term free associations acquired non-technical use. %For example, ""Suppose student person enrolled academic institution"" stipulated definition Mathematical definitions frequently history evolve time. The definition use function, instance, may one used hundred years ago. % Nat - fixed sentence The concept connectivity two definitions, one path connectivity another set-theoretic connectivity. In mathematical texts meaning defined concept determined context declared expected variance within specific mathematical text~. % Nat - updated Mathematical definitions many features, critical optional accepted within mathematical community. % Nat - added %Van Dormolen Zaslavsky~ describe good mathematical definition containing criteria hierarchy% , existence% , equivalence, axiomatization. Desired necessary criteria definition minimality, elegance, degenerations. We give short definitions concepts; detailed explanations examples found . % end formal definition formal definitions Not every definition appearing text mathematical sense. For example, Wikipedia articles contain definitions different style. We see Wikipedia definition Kane \& Abel musical group %shown Figure similar style Wikipedia definition Abelian group. % %\end{figure} % Current methods automatic DE view binary classification task, sentence classified definition non-definition. A supervised learning process usually employed task, employing feature engineering sentence representation. The absolute majority current methods study generic definitions mathematical definitions . In paper describe supervised learning method automatic DE mathematical texts. Our method applies Convolutional Neural Network , Long Short-Term Memory network , combinations raw text data sentence syntax structure, order detect definitions. Our method evaluated three different corpora; two well-known corpora generic DE one new annotated corpus mathematical definitions, introduced paper. The main contributions paper analysis introduction new annotated dataset mathematical definitions, evaluation state-of-the-art DE approaches new mathematical dataset, introduction evaluation upgraded sentence representations adapted mathematical domain adaptation deep neural networks new sentence representations, extensive experiments multiple network input configurations performed different datasets mathematical non-mathematical domains, experiments cross-domain multi-domain learning DE task, introduction new parsed non-annotated dataset composed Wiki articles near-mathematics topics, used additional--extrinsic--evaluation scenario. These contribute showing using specifically suited training data along adapting sentence representation classification models task mathematical DE significantly improves extraction mathematical definitions surrounding text. The paper organized follows. Section contains survey up-to-date related work. Section describes sentence representations structure neural networks used approach. Section provides description datasets, evaluation results, analysis. Section contains conclusions. Finally, Appendix contains supplementary materials -- annotation instructions, description Wikipedia experiment, figures. We proposed pre-train BERT hierarchical multitask learning approach. Our results restricted data show approach achieves better equal performance. We incorporate sentence-level information solve word-level tasks. This also shows slight increment performance. We propose additional pre-training task, bigram shift, causes embeddings contain word order information. We believe implementing techniques large-scale training advance state-of-the-art. Probing tasks show different training techniques lead embeddings contain different linguistic properties. This essential point since various problems NLP domain require different needs. Therefore selecting appropriate pre-training strategy important factor."," Automatic definition extraction from texts is an important task that has numerous applications  in several natural language processing fields such as summarization, analysis of scientific texts, automatic taxonomy generation, ontology generation, concept identification, and question answering. For definitions that are contained within a single sentence, this problem can be viewed as a binary classification of sentences into definitions and non-definitions.  In this paper, we focus on automatic detection of one-sentence definitions in mathematical texts, which are difficult to separate from surrounding text.  We experiment with several data representations, which include sentence syntactic structure and word embeddings, and apply deep learning methods such as the  Convolutional Neural Network  and the Long Short-Term Memory network , in order to identify mathematical definitions.  Our experiments demonstrate the superiority of CNN and its combination with LSTM, when applied on the syntactically-enriched input representation.  % %We use data representation that includes sentence syntactic structure; to this we apply deep learning methods such as Convolutional Neural Network  and Recurrent Neural Network , in order to identify mathematical definitions.  We also present a new dataset for definition extraction from mathematical texts. %We demonstrate that the use of this dataset for training learning models improves the quality of definition extraction when these models are then used for other definition datasets.  We demonstrate that this dataset is beneficial for training supervised models aimed at extraction of mathematical definitions.  %Marina: added new sentence from the conclusions section Our experiments with different domains demonstrate that mathematical definitions require special treatment, and that using cross-domain learning is inefficient for that task."
"Computer-assisted cross-lingual conversation automatic speech-to-speech translation one challenging problems spoken language technologies decades . Recent remarkable advances speech language processing led deep learning techniques benefit challenge real-time accurate speech translation. %\memo{} gogole multi-task model One crucial problem automatic speech-to-speech translation delay. Spoken language processing tasks usually handled utterance sentence level. Their application speech-to-speech translation suffers long delay proportional input length, process starts observation end utterance. That similar consecutive interpretation useful long monologues lecture talks. On hand, situations, simultaneous interpretation often used audience proficient language talk. Simultaneous interpretation challenging task listen talk speak interpretation different language. In work, tackle problem automatic simultaneous speech-to-speech translation develop neural system English Japanese. Here, call task simultaneous translation, simultaneous interpretation. We think task simultaneous interpretation includes additional efforts summarization make output concise small latency better understanding audience. The problem requires real-time incremental processing output generated simultaneously input. Previous attempts incremental neural speech translation focused speech-to-text translation . Our work aims speech-to-speech translation natural information delivery speech without need visual attention text-based subtitles. Our system based cascade three processing modules: incremental speech recognition , incremental machine translation , text-to-speech synthesis , rather recent end-to-end approaches %\memo{} due difficulty applying simultaneous translation. We follow existing studies incremental neural speech processing. For ASR, choose approach using teacher-student training framework train incremental student model help non-incremental teacher model . For MT, choose approach called wait-k, delays start decoding process simply k steps . For TTS, choose approach starting segmental speech synthesis observing next accent phrase . These modules exchange input/output symbols forms subwords work symbol-synchronous way, cascaded even different waiting strategies. We also conduct system-level evaluation system system-level latency module-level performance English-to-Japanese simultaneous translation TED Talks. The system-level latency measures are: processing delays waiting computation time, TTS speaking latency derived overlaps synthesized speech outputs. The module-level performance measured standard metrics ASR, MT, TTS. This work first attempt system-level evaluation simultaneous speech-to-speech translation system would beneficial future studies. %The remainder paper organized follows. %In section , review problem simultaneous speech-to-speech translation, mainly difficulty. %In section , describe details incremental processing modules ASR, MT, TTS. %In section , present system-wise evaluation system, followed discussions section . %We conclude paper section . In article, present first comprehensive review notable works date deep learning based multi-document summarization. We propose taxonomy scheme organizing clustering existing publications devise network design strategies based state-of-the-art methods. Furthermore, also provide overview existing multi-document objective functions, evaluation metrics datasets. Additionally, pressing open problems promising future extensions also discussed survey. We hope survey provide readers comprehensive understanding key aspects multi-document summarization task, clarify notable advances, shed light future studies. \setlength{\bibsep}{0.5ex}"," This paper presents a newly developed, simultaneous neural speech-to-speech translation system and its evaluation. The system consists of three fully-incremental neural processing modules for automatic speech recognition , machine translation , and text-to-speech synthesis . We investigated its overall latency in the system's Ear-Voice Span and speaking latency along with module-level performance."
"The emergence online collaboration platforms dramatically changed dynamics human teamwork, creating veritable army virtual teams, composed workers different physical locations. Software engineering requires tremendous amount collaborative problem solving, making excellent domain team cognition researchers seek understand manifestation cognition applied team tasks. Mining data social coding platforms GitHub yield insights thought processes virtual teams. Previous work issue comments focused emotional aspects team communication, sentiment politeness. Our aim map issue comments states team cognition information gathering, knowledge building problem solving. To employ dialogue act classification, order identify intent speaker. Dialogue act classification broad range natural language processing applications, including machine translation, dialogue systems speech recognition. Semantic-based classification human utterances challenging task, lack large annotated corpus represents class variations makes job even harder. Compared examples human utterances available standard datasets like Switchboard corpus CSI Meeting Recorder Dialogue Act corpus, GitHub utterances complex. The primary purpose study DA classification GitHub issue comments harnessing strength transfer learning, using word sentence level embedding models fine-tuned dataset. For word-level transfer learning, used GLoVe vectors, Universal Sentence Encoders BERT models used sentence-level transfer. This paper presents comparison performance various architectures GitHub dialogues limited resource scenario. A second contribution publicly available dataset annotated issue comments. The dataset available \url{https://drive.google.com/drive/folders/1kLZvzfE80VeEYA1tqua_aj6nSiT57f83?usp=sharing}. In field computational collective intelligence, people collaborate work teams achieve goals, dialogue act classification play vital role understanding human teamwork. The latency results revealed incremental system based cascade three modules worked successfully relatively small delays. However, quality results suggested task difficulty due error propagation ISR IMT lack in-domain corpora English-Japanese MT. We show two translation examples Table. The first one relatively good result, one typical error propagation example. Tight module integration would promising, lattice-to-sequence , extension simultaneous translation trivial. Besides, common evaluation metrics simultaneous speech-to-speech translation module-level ones. We used two latency metrics work, objective measurement content delivery speech-to-speech translation crucial evaluation. \section{Conclusions} In paper, presented English-to-Japanese simultaneous speech-to-speech translation system evaluation using English TED talks. The system works fully-incremental speech inputs, cascaded modules incremental ASR, incremental MT, incremental TTS. The latency evaluation revealed module-level computation could finished three seconds delay maximum. However, system suffers speaking latency. Our future work includes improvement modules accuracy efficiency, controlling speaking duration decrease speaking latency. \section{Acknowledgments} Part work supported JSPS KAKENHI Grant Numbers JP17H06101. References produced using bibtex program suitable BiBTeX files . The IEEEbib.bst bibliography style file IEEE produces unsorted bibliography list. -------------------------------------------------------------------------"," Social coding platforms, such as GitHub, serve as  laboratories for studying collaborative problem solving in open source software development; a key feature is their ability to support issue reporting which is used by teams to discuss tasks and ideas.  Analyzing the dialogue between team members, as expressed in issue comments, can yield important insights about the performance of virtual teams.  This paper presents a transfer learning approach for performing dialogue act classification on issue comments.  Since no large labeled corpus of GitHub issue comments exists, employing transfer learning enables us to leverage standard dialogue act datasets in combination with our own GitHub comment dataset. We compare the performance of several word and sentence level encoding models including Global Vectors for Word Representations , Universal Sentence Encoder , and Bidirectional Encoder Representations from Transformers . Being able to map the issue comments to dialogue acts is a useful stepping stone towards understanding cognitive team processes."
"Large, densely-labeled datasets critical requirement creation effective supervised learning models. The pressing need high quantities labeled data led many researchers collect data social media platforms online forums . Due presence noise lack structure exist data sources, manual quality analysis necessary extract structured labels, filter irrelevant examples, standardize language, perform preprocessing tasks data used. However, obtaining dataset annotations manner time-consuming expensive process often prone errors. In work, develop automated data cleaning verification mechanisms extracting high-quality data social media platforms\footnote{All code available \url{https://github.com/rachel-1/qa_plausibility}.}. We specifically focus creation question-answer datasets, data instance consists question topic corresponding answer. In order filter noise improve data quality, propose task question-answer plausibility, includes following three steps: Because assume social media users generally answer questions good faith , assume plausible answers correct ones . Necessarily, property satisfied, adequate solutions would require domain knowledge interest. Therefore, look apply approach toward data property. In study, demonstrate application QA plausibility context visual question answering , well-studied problem field computer vision . We assemble large VQA dataset images collected image-sharing social network, machine-generated questions related content image, responses social media users. We train multitask BERT-based model evaluate ability model perform three subtasks associated QA plausibility. The methods presented work hold potential reducing need manual quality analysis crowdsourced data well enabling use question-answer data unstructured environments social media platforms. This paper demonstrates dialogue act classification system GitHub issue comments. Due lack publicly available training sets formal teamwork dialogues, formulated problem transfer learning task, using sentence-level word-level embedding models leverage information SwDA dataset. A significant contribution work identifying embedding model performs best fine-tuning issue comments. We used GloVe, probabilistic representation, USE, BERT embedding train five different models. USE showed best performance accuracy 50.71\ . The low accuracy USE DA classification compared accuracy state-of-the-art NLP tasks shows complex nature dialogue act classification. We evaluated many different settings learning rates, epochs, batch size; even though minor accuracy improvements achievable, performance embedding models remained fairly stable. Our aim map issue comments cognitive states Macrocognition Teams Model . Drawing research externalized cognition, team cognition, group communication problem solving, collaborative learning adaptation, MITM provides coherent theoretically based conceptualization understanding complex team processes emerge change time. MITM consists five components: Team Problem-Solving Outcomes, Externalized Team Knowledge, Internalized Knowledge, Team Knowledge Building, Individual Knowledge Building. It captures parallel iterative processes engaged teams synthesize components service team cognitive processes problem solving, decision making planning. MITM applied team problem solving scenarios military logistics business planning never used analyze software engineering teams. Its usage domain software engineering would major research contribution field team cognition. Although possible directly label issue comments using MITM code book, type labeling would less compatible existing dialogue act datasets. Instead constructing mapping relates DAMSL tagset cognitive states. For instance, question tags DAMSL clearly relate information gathering processes. Also many DAMSL classes less relevant team cognition process could ignored. The commonly occurring classes GitHub issue comments relevant Macrocognition Teams Model, plan tune dialogue act classifiers bolster performance classes. In future work, continue improve size quality publicly-released dataset recruiting annotators help labeling task also systematically studying inter-coder reliability."," Datasets extracted from social networks and online forums are often prone to the pitfalls of natural language, namely the presence of unstructured and noisy data. In this work, we seek to enable the collection of high-quality question-answer datasets from social media by proposing a novel task for automated quality analysis and data cleaning: question-answer  plausibility. Given a machine or user-generated question and a crowd-sourced response from a social media user, we determine if the question and response are valid; if so, we identify the answer within the free-form response.   We design BERT-based models to perform the QA plausibility task, and we evaluate the ability of our models to generate a clean, usable question-answer dataset. Our highest-performing approach consists of a single-task model which determines the plausibility of the question, followed by a multi-task model which evaluates the plausibility of the response as well as extracts answers ."
"In recent times, pre-trained neural language models become preferred approach language representation learning, pushing state-of-the-art multiple NLP tasks~. These approaches rely two-step training process: first, self-supervised pre-training performed large-scale corpora; then, model undergoes supervised fine-tuning downstream task labels using task-specific prediction heads. While method found effective scenarios relatively large amount labeled data present, researchers highlighted case low-resource settings~. Recently, pattern-exploiting training~(PET, \citet{Schick2020ExploitingCQ,Schick2020ItsNJ} tackles dependence NLMs labeled data first reformulating tasks cloze questions using task-related patterns keywords, using language models trained annotate large sets unlabeled examples soft labels. PET thought offline version knowledge distillation~, well-established approach transfer knowledge across models different size, even different versions model self-training . While effective classification tasks easily reformulated cloze questions, PET cannot easily extended regression settings since cannot adequately verbalized. Contemporary work \citet{du-etal-2020-selftraining} showed self-training pre-training provide complementary information natural language understanding tasks. In paper, I propose simple self-supervised data augmentation approach used improve generalization capabilities NLMs regression classification tasks modest-sized labeled corpora. In short, ensemble fine-tuned models used annotate large corpus unlabeled text, new annotations leveraged multi-task setting obtain final predictions original test set. The method tested AcCompl-it shared tasks EVALITA 2020 campaign~, objective predict respectively complexity acceptability scores 1-7 Likert scale test sentence, alongside estimation standard error. Results show considerable improvements regular fine-tuning performances COMPL ACCEPT using UmBERTo pre-trained model~, suggesting validity approach complexity/acceptability prediction possibly language processing tasks. Deep learning studies often hindered lack access large datasets accurate labels. In paper, introduced question-answer plausibility task effort automate data cleaning process question-answer datasets collected social media. We presented multi-task deep learning model based BERT, accurately identified plausibility machine-generated questions user responses well extracted structured answer labels. Although specifically focused visual question answering problem paper, expect results useful question-answer scenarios, settings questions user-generated images available. Overall, approach help improve deep learning workflow processing cleaning noisy unstructured natural language text available social media platforms. Ultimately, work enable generation large-scale, high-quality datasets artificial intelligence models.","   English.  This work describes a self-supervised data augmentation approach used to improve learning models' performances when only a moderate amount of labeled data is available. Multiple copies of the original model are initially trained on the downstream task. Their predictions are then used to annotate a large set of unlabeled examples. Finally, multi-task training is performed on the parallel annotations of the resulting training set, and final scores are obtained by averaging annotator-specific head predictions. Neural language models are fine-tuned using this procedure in the context of the AcCompl-it shared task at EVALITA 2020, obtaining considerable improvements in prediction quality."
"Language modelling task transforming individual words vector representations based context appear in. Hence, distant term dependencies inherited issue within task. Language models always seek smart approaches towards incorporating context longer distances allows better representations compared limited context counterparts. Intuitively, imagine attempting start reading novel series second book onward, information first. The amount information previously missed something cannot acquired. However, case language models. While understanding words present due contextual information word's occurrence, entity information distant text lost transferred. Until recently, Recurrent Neural Networks , specifically Long Short-Term Memory networks, core state-of-the-art approaches . Thanks Transformers architecture , use attention mechanisms, models XLNet , GPT BERT account even longer sequences. However, computational limitations multi-head attention architecture make hard increase contextual information models . As result, research focused introducing variations transformer architecture, focus multi-head attention mechanism, order alleviate part computational cost increase contextual information available models. In paper present novel approach, makes use coreference information training language model via Entity-Transformer architecture, extends original Transformer block Transformer-Based language models. To end, incorporate important entity information would otherwise unreachable model. As result, effectively boost representations entity mentions, entity information present, without hindering performance language model entities present. In experiments, extend GPT2 architecture formulate model, named GPT2E train CoNLL-2012 dataset using annotated coreference information. We evaluate model's performance terms Perplexity ConLL 2012 LAMBADA datasets showcase effects training word representations well downstream task Named Entity Recognition using CoNLL 2012 dataset. To end, compare GPT2E's performance base model trained data, highlight effects coreference information paird Entity-Transformer architecture. Our study constitutes first attempt modeling automatic translation extremely low-resource language Bambara. We identified challenges future work, development alignment tools small-scale datasets, need general domain evaluation set. The current limitation processing written text input might furthermore benefit integration spoken resources speech recognition speech translation, since Bambara primarily spoken lack standardization writing complicates creation clean reference sets consistent evaluation.","     In the last decade, the field of Neural Language Modelling has witnessed enormous changes, with the development of novel models through the use of Transformer architectures. However, even these models struggle to model long sequences due to memory constraints and increasing computational complexity. Coreference annotations over the training data can provide context far beyond the modelling limitations of such language models. In this paper we present an extension over the Transformer-block architecture used in neural language models, specifically in GPT2, in order to incorporate entity annotations during training. Our model, GPT2E, extends the Transformer layers architecture of GPT2 to Entity-Transformers, an architecture designed to handle coreference information when present. To that end, we achieve richer representations for entity mentions, with insignificant training cost. We show the comparative model performance between GPT2 and GPT2E in terms of Perplexity on the CoNLL 2012 and LAMBADA datasets as well as the key differences in the entity representations and their effects in downstream tasks such as Named Entity Recognition. Furthermore, our approach can be adopted by the majority of Transformer-based language models."
"Sequence labeling task labeling token sequence. It important task natural language processing lot applications Part-of-Speech Tagging , Named Entity Recognition , Chunking . The neural CRF model one widely-used approaches sequence labeling achieve superior performance many tasks . It often employs encoder BiLSTM compute contextual vector representation word input sequence. The potential function position input sequence neural CRF typically decomposed emission function transition function . %The transition function computed previous current labels. In paper, design series increasingly expressive potential functions neural CRF models. First, compute transition function label embeddings instead label identities. Second, use single potential function current word previous current labels, instead decomposing emission transition functions, leading expressiveness. We also employ tensor decomposition order keep potential function tractable. Thirdly, take representations additional neighboring words input potential function, instead solely relying BiLSTM capture contextual information. To empirically evaluate different approaches, conduct experiments four well-known sequence labeling tasks: NER, Chunking, coarse- fine-grained POS tagging. We find beneficial potential function take representations neighboring words input, quadrilinear potential function decomposed tensor parameter leads best overall performance. Our work related \citet{reimers-gurevych-2017-reporting,yang-etal-2018-design}, also compared different network architectures configurations conducted empirical analysis different sequence labeling tasks. However, focus potential function design neural CRF models, sufficiently studied before. The framework presented paper several advantages modeling language change. The networks trained raw acoustic inputs levels abstraction pre-extracted features. Deep convolutional network GAN framework need learn produce data random noise never fully replicate data, produce innovative linguistically interpretable data. This means output data replicates, innovative original outputs. As argued elsewhere , innovative outputs Generator highly informative often replicate stages language acquisition. In paper additionally argue innovative outputs result phonetic phonological changes trained iterative learning tasks. The current model contains articulatory information. While generally unideal, human speech acquisition highly influenced articulators, allows us model language change cognition-general mechanisms involved language acquisition production. The results computational experiment suggest gradual change targets resembles phonetic change well phonological rule loss emerge deep convolutional networks trained iterative learning tasks without articulatory information without language-specific parameters model. In future work, able compare results articulation-free model models containing articulatory information get better understanding properties sound change derived domain-general cognitive mechanisms properties sound change require articulatory forces. The current paper offers initial step broader goal modeling language's cultural evolution based generations deep convolutional networks trained raw speech. Far complex interactions agents conceived future work. For example, GANs set communicate learn interactive ways already training. Further modeling kind shed light onto one widely studied still poorly understood phenomenon language --- sound change. \subsubsection*{Acknowledgements} This research funded grant new faculty University Washington University California, Berkeley. I would like thank Sameer Arshad slicing data TIMIT database. \clearpage {\small } \clearpage"," The neural linear-chain CRF model is one of the most widely-used approach to sequence labeling. In this paper, we investigate a series of increasingly expressive potential functions for neural CRF models, which not only integrate the emission and transition functions, but also explicitly take the representations of the contextual words as input. Our extensive experiments show that the decomposed quadrilinear potential function based on the vector representations of two neighboring labels and two neighboring words consistently achieves the best performance."
"Sequence labeling tasks essential web mining, named entity recognition , event extraction, relation identification. For example, NER models assign predefined labels tag tokens input sequences indicate entity boundaries types. In web services, question answering, sequence labeling also plays critical role, reads passage Web page context answers given question extracting text span inside given passage. This process often called machine reading comprehension . MRC also regarded sequence labeling task, since predicts whether token start, end, none answer span. There rich literature sequence labeling. Classical methods include Hidden Markov models , maximum entropy Markov models , conditional random field . Recently, combining neural networks representation layer CRF models boosted state-of-the-art performance. However, statistical models require large amounts training data. Consequently, show good performance languages rich training data, English. Sequence labeling low-resource languages still challenging, mainly due limited training data available. To tackle challenge sequence labeling low-resource languages, early works transfer knowledge rich-source languages low-resource ones information alignment manually built bilingual parallel corpora, language-independent features. In recent years, multilingual pre-trained language models, Unicoder, mBERT, XLM-Roberta , developed model transferring. For example, Wu et al. fine-tune mBERT pseudo training set meta-learning method. To better leverage unlabeled data target language, teacher-student framework proposed distill knowledge weighted teacher models. Inspired back translation neural machine translation , DualBERT developed learn source language target language features simultaneously. Although multilingual sequence labeling models effectively locate target spans, often fail give precise boundaries spans target languages. %when predicting text spans target languages. %that is, pairs sentences similar meanings different languages, %\jp{What conclusion draw paragraph?} %The previous multilingual sequence labeling models roughly identify correct target spans, often fail give precise boundaries predicting text spans target languages. We conduct empirical study quantitatively assess challenge. In Figure , categorize mismatches predicted span ground truth span four types: predicted answer super span ground truth; predicted answer sub span ground truth; predicted answer miss terms ground truth add extra terms ground truth , predicted answer adjacent ground truth contains common sub-span . We show Table statistics error cases cross-lingual NER task using XLM-R model, boundary errors, including super span, sub span, drifted span, adjacent span, contribute large portion error cases shown last column. The errors cases mainly entity type detection errors. This observation motivates us tackle bottleneck boundary detection sequence labeling models. % \end{center} \end{table} Accurately detecting answer boundaries becomes bottleneck sequence labeling. To tackle challenge, paper, propose separate model boundary calibration based output base model. Intuitively, base model captures global context whole input sequence roughly locates region answers. Then, calibration model conducts finer search within detected region neighborhood, focuses local context refine boundary. This analogous human perception cognition process, first locates target, sets local context, finally zooms details. Our design novel sequence labeling, orthogonal complements existing approaches. Using second model focus detecting answer boundaries accurately intuitive nice idea. However, construct high-quality training data calibration model remains challenging. One straightforward method transform original training data sequence labeling task new training set calibration model. However, data collected way still quite limited, especially low-resource languages. To address challenge, strategically propose novel phrase boundary recovery task pre-train model large-scale augmented datasets synthesized Wikipedia documents multiple languages. The new pre-training approach dramatically improves capability calibration module determine answer boundaries accurately. % Besides design employing two models, equip calibration model pre-training process emphasizing capability recovering meaningful phrases noisy input. Our approach shown Figure. CalibreNet consists two modules, base module calibration module. The base module take model sequence labeling. The predicted answers base module combined input sequence form input calibration module. The calibration module considers initial results base module whole passage refine span boundaries. In particular, calibration module pre-trained PBR task large-scale multilingual synthesized data Wikipedia-derived corpus. We make following technical contributions paper. First, propose CalibreNet framework task cross-lingual sequence labeling improve accuracy labeled answers. Second, propose novel phrase boundary recovery task weakly supervised pre-training method using Wikipedia data. This approach effectively enhances model sensitivity phrase boundaries. Last least, conduct extensive experiments zero-shot cross-lingual NER improve SOTA results. In addition, experiments MRC tasks also show consistent improvement strong baseline methods. The rest paper organized follows. We first review related work Section. We present approach Section. We report extensive experimental results Sections. We conduct analysis Section, conclude paper Section. In paper, investigate several potential functions neural CRF models. The proposed potential functions integrate emission transition functions, also take consideration representations additional neighboring words. Our experiments show D-Quadrilinear achieves best overall performance. Our proposed approaches simple effective could facilitate future research neural sequence labeling.","  \footnotetext[1]{Work done during the first author's internship at Microsoft STCA.} \footnotetext[2]{Daxin Jiang and Wanli Zuo are the corresponding authors.} % \footnotetext[3]{Jian Pei's research is supported in part by the NSERC Discovery Grant program. All opinions, findings, conclusions and recommendations in this paper are those of the authors and do not necessarily reflect the views of the funding agencies.}   Lack of training data in low-resource languages presents huge challenges to sequence labeling tasks such as named entity recognition  and machine reading comprehension . One major obstacle is the errors on the boundary of predicted answers. To tackle this problem, we propose CalibreNet, which predicts answers in two steps. In the first step, any existing sequence labeling method can be adopted as a base model to generate an initial answer. In the second step, CalibreNet refines the boundary of the initial answer. To tackle the challenge of lack of training data in low-resource languages, we dedicatedly develop a novel unsupervised phrase boundary recovery pre-training task to enhance the multilingual boundary detection capability of CalibreNet. Experiments on two cross-lingual benchmark datasets show that the proposed approach achieves SOTA results on zero-shot cross-lingual NER and MRC tasks."
"The Text-to-SQL task aims translate natural language texts SQL queries. Users understand SQL grammars benefit task acquire information databases inputting natural language texts. Previous works focus context-independent text-to-SQL generation. However, practice, users usually interact systems several turns acquire information, extends text-to-SQL task context-dependent text-to-SQL task conversational scenario. Throughout interaction, user inputs may omit information appeared before. This phenomenon brings difficulty context-dependent text-to-SQL task. Recently, context-dependent text-to-SQL task attracted attention. \citet{suhr2018learning} conduct experiments ATIS dataset . Besides, two cross-domain context-dependent datasets SParC CoSQL released. Cross-domain means databases test set differ training set, challenging. EditSQL previous state-of-the-art model SParC CoSQL datasets focuses taking advantages previous utterance texts previously predicted query predict query current turn. Table shows user inputs, ground truth queries predicted queries EditSQL interaction. In second turn, EditSQL views ``Kacey"" name dog owner. However, since context interaction dogs, ``Kacey"" name dog. This example shows model using historical information user inputs may fail keep context consistency maintain thematic relations. According , maintain thematic relations, users may change constraints, ask different attributes topic ask next questions. Thus, database schema items current turn relation items previous turn. For example, Table , second question adds constraint name asks age dog instead numbers dogs. The corresponding database schema items Dogs.age Dogs.name belong table Dogs.* previous query . Therefore, propose take historical information database schema items consideration. % % % %\end{table} In particular, first construct graph based corresponding database, graph nodes database schema items graph edges primary-foreign keys column affiliation. Short distance graph nodes appearing previous query current query reveal context consistency since usually edge different attributes topic. We propose database schema interaction graph encoder model database schema items together historical items. Empirical results two large cross-domain context-dependent text-to-SQL datasets - SParC CoSQL show schema interaction graph encoder contributes modeling context consistency proposed model database schema interaction graph encoder substantially outperforms state-of-the-art model. \end{table} Our main contributions summarized follows: In paper, tackle challenge detecting span boundaries precisely sequence labeling tasks low-resource languages. We propose CalibreNet architecture well novel Phrase Boundary Recovery task accurate boundary detection. Extensive experimental results verify effectiveness approach generalization capability multiple languages. As future works, plan introduce entity type prediction pre-training task, also develop better methods question generation MRC task. The acknowledgments section defined using ""acks"" environment . This ensures proper identification section article metadata, consistent spelling heading. The next two lines define bibliography style used, bibliography file. If work appendix, place put it."," Context-dependent text-to-SQL task has drawn much attention in recent years. Previous models on context-dependent text-to-SQL task only concentrate on utilizing historical user inputs. In this work, in addition to using encoders to capture historical information of user inputs, we propose a database schema interaction graph encoder to utilize historicalal information of database schema items. In decoding phase, we introduce a gate mechanism to weigh the importance of different vocabularies and then make the prediction of SQL tokens. We evaluate our model on the benchmark SParC and CoSQL datasets, which are two large complex context-dependent cross-domain text-to-SQL datasets. Our model outperforms previous state-of-the-art model by a large margin and achieves new state-of-the-art results on the two datasets. The comparison and ablation results demonstrate the efficacy of our model and the usefulness of the database schema interaction graph encoder."
"The recent survey conducted WHO shows total million people world living depression. % This increased 18.4\% . At severe, depression lead suicide responsible deaths every year . Early detection appropriate treatment encourage remission prevent relapse . However, stigma coupled depression makes patients reluctant seek support provide truthful answers physicians . Additionally, clinical diagnosis dependent self-reports patient behavior, requires reflect recall past, may obscured time. In contrast, social media offers unique platform people share experiences moment, express emotions stress raw intensity, seek social emotional support resilience. As such, depression studies based social media offer unique advantages scheduled surveys interviews . Social media self-narratives contain large amounts implicit reliable information expressed real-time, essential practitioners glean understand user behavior outside controlled clinical environment. % \indent Several studies literature explored various linguistic visual cues effectively detect user depression postings social media platform like Twitter Reddit . Majority existing studies formulated social media depression detection task binary classification problem therefore limited identifying depressive users. \\ \indent To assist healthcare professionals intervene timely manner automatic triaging, necessary develop intelligent decision support system provides HPs fine-grained depression related symptoms. The triage process critical step giving care patients because, prioritizing patients different triage levels based severity clinical condition, one enhance utilization healthcare facilities efficacy healthcare interventions. There efforts create datasets capturing depression severity, however limited clinical interviews questionnaires , individuals voluntarily participate study . \\ \indent In work, exploit Twitter data identify indications depression. We developed high quality dataset consisting total tweets, tweets posted self-reported depressed users weeks time, manually annotated using questionnaire based symptoms categories. In Table-, provide sample tweets associated nine item depression symptoms. % The self-reported questionnaire, based Diagnostic Statistical Manual Mental Disorders, Fourth Edition guidelines, screening, diagnosing, measuring severity depression. % The overall scores range , score linked major depressive disorders. Our research hypothesis depressed individuals discuss symptoms Twitter tracked reliably. } \end{table*} % Advancement Natural Language Processing one promising avenues discovering vital mental health information user-generated data. Nonetheless, user social-media post offer unique challenges discussed below: To account creative linguistic device widely observed utterances depressive users, propose Figurative Language enabled Multi-Task Learning framework works concept task sharing mechanism . In work, improve performance robustness primary task `symptom identification' combined supervisory task `figurative usage detection' multi-task learning setting. We introduce mechanism named `co-task aware attention' enables layer-specific soft sharing parameters tasks interest. The proposed attention mechanism parameterized task-specific scaling factor BERT layers. BERT enables even low-resource tasks benefit deep bi-directional architectures unsupervised training framework obtain context-aware encoded representation. The virtue model ability learn task-specific representation input tweet coordinating among layers tasks. \\ Contributions: %%%%%%%%%%%%%%%%%%%%% % % According Word Health Organization \footnote{http://www.who.int/news-room/fact-sheets/detail/mental-disorders}, ``depressive disorder characterized sadness, loss interest pleasure, feelings guilt low self worth, disturbed sleep appetite, feelings tiredness, poor concentration"". % Major depressive disorder world-wide impact society year causing almost one million deaths. % The recent survey conducted WHO shows total million people world living depression. This increased 18.4\% . At severe depression lead suicide responsible deaths every year . Early detection appropriate treatment encourage remission prevent relapse . However, stigma coupled depression makes patients reluctant seek support. Also, associated cognitive biases, inhibits patients provide truthful answer physicians add limitation .\\ % \indent Additionally, clinical diagnosis dependent hypothetical self-reports patients behaviour, requiring patients reflect thinking sometime past, may become obscured time. In contrast, social media offers unique platform people share experiences, exhaust emotion stress, seek social emotional support. As such, depression studies based social media offers several advantage . These self-narrative contains large amount implicit information, highly essential practitioner understand users behaviour outside controlled clinical environment real-time.\\ % \indent Several studies literature explored various linguistic visual cues effectively detect depression social media platform like Twitter Reddit. Majority existing studies formulated social media depression detection task binary classification problem therefore limited identify depressive users. \\ % \indent However, assist healthcare professional making timely intervention, required develop intelligent decision support system could provide HPs fine-grained depression related symptoms automatic triaging techniques. The triage process first critical step giving care patients prioritizing patients different triage levels based severity clinical conditions could potential enhance efficacy healthcare interventions. In literature, efforts create dataset capturing depression severity, however limited clinical interview questionnaire , individuals voluntary participated study. \\ % \indent In work, exploit Twitter data identify indications depression finally assign based severity labels: `None', `Mild', `Moderate', `Moderately Severe', `Severe' triaging. We developed new dataset consisting tweets posted self-reported depressed users weeks time, manually annotated symptom categories. In Table-, provide samples tweets associated nine item depression symptoms. % The self-report questionnaire based Diagnostic Statistical Manual Mental Disorders, Fourth Edition guidelines screening, diagnosing, measuring severity depression. The overall scoring ranges , highly linked major depressive disorders. % Our research hypothesis depressed individuals discuss symptoms Twitter.\\ % %This work aim develop intelligent decision support system context major depressive disorder providing healthcare professionals fine-grained depression related symptoms automatic triaging technique required HPs make timely intervention. The triage process first critical step giving care patients prioritizing patients different triage levels based severity clinical conditions could potential enhance efficacy healthcare interventions.\\ % % } % % % \end{table*} % Advancement Natural Language Processing technology one promising avenues discovering vital mental health information user-generated data. Nonetheless, texts offers inherently distinct challenges discussed follows: % % Furthermore, previous studies utilizing social media data biomedical natural language processing task reported prediction error drug symptom names utilized figurative sense. To account creative linguistic devices widely observed utterances depressive users, proposed multitask learning framework works concept task sharing mechanism. Multi-task learning proven useful instruments improve generalization performance primary task related auxiliary tasks. In work, focused improve performance generalization ability proposed model primary task `symptom identification' companionship supervisory task `figurative language detection'. We introduce mechanism named `co-task aware attention' enables layer-specific soft sharing parameters task interest. The proposed attention mechanism parameterize task-specific scaling factor BERT layers. To virtue this, model able learn task-specific representation input tweet coordinating among layers tasks. \\ % Contributions: % In paper, focus context-dependent cross-domain SQL generation task. We find previous state-of-the-art model takes historical user inputs previously predicted query consideration, ignores historical information database schema items. Thus propose model named IGSQL model database schema items conversational scenario. Empirical results demonstrate efficacy model. We also conduct ablation experiments reveal significance database schema interaction graph encoder. For future work, explore methods attempting solve hard extra hard questions. \section*{Acknowledgments} This work supported National Natural Science Foundation China , Beijing Academy Artificial Intelligence Key Laboratory Science, Technology Standard Press Industry . We appreciate anonymous reviewers helpful comments. Xiaojun Wan corresponding author."," Existing studies on using social media for deriving mental health status of users focus on the depression detection task. However, for case management and referral to psychiatrists, healthcare workers require practical and scalable depressive disorder screening and triage system. % for prevention or treatment of severe consequences.  This study aims to design and evaluate a decision support system  to reliably determine the depressive triage level by capturing fine-grained depressive symptoms expressed in user tweets through the emulation of Patient Health Questionnaire-9 \texttt{} that is routinely used in clinical practice. %However, the 280-character limit on tweets incentivizes the usage of creative artifacts in the utterances.  %Figurative language forms a general fabric of communication as it permits users to express themselves more effectively.  %Unfortunately, this complicates the reliable detection of depressive symptoms.  The reliable detection of depressive symptoms from tweets is challenging because the 280-character limit on tweets incentivizes the use of creative artifacts in the utterances and figurative usage contributes to effective expression.   We propose a novel BERT based robust multi-task learning framework to accurately identify the depressive symptoms using the auxiliary task of figurative usage detection. Specifically, our proposed novel task sharing mechanism, co-task aware attention\/, enables automatic selection of optimal information across the BERT layers and tasks by soft-sharing of parameters. Our results show that modeling figurative usage can demonstrably improve the model's robustness and reliability for distinguishing the depression symptoms. %Furthermore, our approach achieves statistically significant improvements over the SOTA models.  % Social media platforms have evolved as a vital source of information for mental-health studies, where the users exchange their emotional states and impressions. Majority of the existing studies on depression focus mainly on the depression detection task. However, for healthcare workers to have real-time access to resources for case management and referral to medical/psychiatric treatment, it is necessitate to enable practical, scalable, and sustainable depressive disorder screening, triage, and prevention/treatment interventions. This study aims to design and evaluate a decision support system  to % determine the depressive triage level by capturing fine-grained depressive symptoms appearing in depressed users tweets through emulating the clinically adopted Patient Health Questionnaire-9 \texttt{\texttt{PHQ-9}}.\\ % Nevertheless, the limitation on characters imposed by Twitter incentivize the usage of creative artifacts that are widely observed in the utterance of depressive users. Figurative language, such as metaphor, irony, and sarcasm forms a general fabric of communication as it permit users to express their health condition more memorably, concisely, and effectively. Inspired by that, we proposed a novel BERT based multi-task learning framework that learns to accurately identify the symptoms using the auxiliary task of figurative language detection. Specifically, we propose a new task sharing mechanism: co-task aware attention, which helps the model to borrow the new information across the task. With the help of soft-sharing of the parameters, our framework automatically detect and select optimal information across the layers of the BERT, that are useful for a task at hand. % The obtained results proves that modeling figurative language in depressive user tweets can improve the model learning ability in correctly distinguishing the symptoms. Furthermore, our proposed approach achieve statistically significant improvements over the state-of-the-art models on our primary task."
"Coherence refers properties text indicate meaningful sentential constituents connected convey document-level meaning. Different theories proposed describe properties contribute discourse coherence integrated computational models empirical evaluation. A popular approach entity-based model hypothesizes coherence assessed terms distribution transitions entities text -- constructing entity-grid representation, building Centering Theory. Subsequent work adapted extended Egrid representations. Other research focused syntactic patterns co-occur text semantic relatedness sentences key aspects coherence modeling. There also attempts model coherence identifying rhetorical relations connect textual units capturing topic shifts via Hidden Markov Models~\cite[HMM,][]{barzilay-lee-2004-catching}. Other work combined approaches study whether complementary. More recently, neural networks used model coherence. Some models utilize structured representations text~\cite[e.g. Egrid representations,][]{Dat2017,Joty2018} others operate unstructured text, taking advantage neural models' ability learn useful representations task. Coherence typically assessed model's ability rank well-organized document higher noisy counterparts created corrupting sentence order original document , neural models achieved remarkable accuracy task. Recent efforts targeted additional tasks recovering correct sentence order, evaluating realistic data focusing open-domain models coherence. However, less attention directed investigating analyzing properties coherence current models capture, knowledge encoded representations might relate aspects coherence. In work, systematically examine properties discourse coherence current coherence models capture. We devise two datasets exhibit various kinds incoherence analyze model ability capture syntactic semantic aspects text implicated discourse organisation. We furthermore investigate set probing tasks better understand information encoded representations might relate aspects coherence. We hope study shall provide insight frame task improve models coherence assessment further. Finally, release evaluation datasets resource community use test discourse coherence models. In research, explored new dimension social media Twitter identify depressive symptoms. Towards end, created new benchmark dataset identifying fine-grained PHQ-9 emulated depressive symptoms contains figurative language. We also introduce robust BERT based MTL framework jointly learns automatically discover complementary features required identify symptoms help auxiliary task figurative usage detection. Our experimental results convincingly show effectiveness introducing figurative usage detection depressive symptoms identification. In future, aim enhance dataset modalities like image memes assist model better understanding figurative sense symptom identification. ########################################################################","  In this work, we systematically investigate how well current models of coherence can capture aspects of text implicated in discourse organisation. We devise two datasets of various linguistic alterations that undermine coherence and test model sensitivity to changes in syntax and semantics. We furthermore probe discourse embedding space and examine the knowledge that is encoded in representations of coherence. We hope this study shall provide further insight into how to frame the task and improve models of coherence assessment further. Finally, we make our datasets publicly available as a resource for researchers to use to test discourse coherence models."
"Early detection dementia important improving clinical outcomes management dementia, well lifestyle, financial, future planning patients caregivers . Yet, dementia formally diagnosed coded claims 50\% older adults living probable dementia . Tools screen medical records warning signs present digested information providers may prove important step early intervention. In study, aim use NLP detect signs cognitive dysfunction clinician notes electronic health records applying deep learning techniques hitherto applied problem. We present attention-based transformer model allows long text sequences reveal signs cognitive concerns compare performance baseline models. Our evaluation experiments two coherence datasets reveal RNN- EGrid-based coherence models able detect syntactic alterations undermine coherence, less effecient detecting semantic ones even fine-tuning latter. We furthermore find particularly struggle recognizing minor lexical changes even result implausible meaning resolving pronominal references. On hand, models particularly good detecting cases prefix inserted subject pronoun substituted lexical item, suggesting capable capturing relevant syntactic patterns solely rely positional features. We find best performing model overall LCD use RNN sentence encoder rather builds sentence representations averaging BERT embeddings utilizes number linear transformations adjacent sentences facilitate learning richer representations. Our probing experiments reveal models better encoding information regarding subject object number followed verb number . These probing tasks align Centering theory probe subject object relevant information. The task tests knowledge coordination inversion lowest performing one overall, suggesting little capacity capturing information related intra-sentential coherence. Excluding LCD, MTL best performing model; nevertheless, still scope substantial improvement across probing tasks particularly CoordInv CorruptAgr. \section{Conclusion} We systematically studied well current models coherence capture aspects text implicated discourse organisation. We devised datasets various kinds incoherence examined model susceptibility syntactic semantic alterations. Our results demonstrate models robust respect corrupted syntactic patterns, prefix insertions lexical substitutions. However, fall short capturing rhetorical semantic corruptions, lexical perturbations corrupt pronouns. We furthermore find discourse embedding space encodes subject object relevant information; however, scope substantial improvement terms encoding linguistic properties relevant discourse coherence. Experiments coordination inversion suggest current models little capacity encoding information related intra-sentential coherence. We hope study shall provide insight frame task coherence modeling improve model performance further. Finally, make datasets publicly available researchers use test coherence models.","   Dementia is under-recognized in the community, under-diagnosed by healthcare professionals, and under-coded in claims data. Information on cognitive dysfunction, however, is often found in unstructured clinician notes within medical records but manual review by experts is time consuming and often prone to errors. Automated mining of these notes presents a potential opportunity to label patients with cognitive concerns who could benefit from an evaluation or be referred to specialist care.  In order to identify patients with cognitive concerns in electronic medical records, we applied natural language processing  algorithms and compared model performance to a baseline model that used structured diagnosis codes and medication data only. An attention-based deep learning model outperformed the baseline model and other simpler models."
"A spelling corrector important ubiquitous pre-processing tool wide range applications, word processors, search engines machine translation systems. %The popularity mobile devices makes increasingly crucial since typing virtual keyboards error-prone. Having surprisingly robust language processing system denoise scrambled spellings, humans relatively easily solve spelling correction . %spelling correction relatively easy task humans, surprisingly robust language processing system denoise scrambled spellings. However, spelling correction challenging task machine, words misspelled various ways, machine difficulties fully utilizing contextual information. Misspellings categorized non-word, out-of-vocabulary, opposite, real-word misspellings . The dictionary look-up method detect non-word misspellings, real-word spelling errors harder detect, since misspellings vocabulary . In work, address stand-alone spelling correction problem. It corrects spelling token without introducing new tokens deleting tokens, original information maximally preserved down-stream tasks. %\textcolor{red}{[The last sentences paragraph good, trying express?]} We formulate stand-alone spelling correction sequence labeling task jointly detect correct misspellings. Inspired human language processing system, propose novel solution following aspects: We encode spelling information global context information neural network. We enhance real-word correction performance initializing model pre-trained language model . We strengthen model's robustness unseen non-word misspellings augmenting training dataset synthetic character-level noise. As result, best model outperforms previous state-of-the-art result absolute score. %As result, present simple powerful solution stand-alone spelling correction simply fine-tuning pre-trained LM jointly detect correct non-word real-word misspellings sequence labeling task. %We propose novel solution using transformer-encoders jointly perform detection correction misspellings. We extensively explore various training techniques. Our results show transformer-encoder-based architecture encodes local character-level global word-level representations yields strong performance. Specifically, combination word embedding character embedding subword embedding produce strong models. We obtain state-of-the-art model initializing weight pre-trained language model training augmented training dataset synthetic character-level noise. \textcolor{red}{[this paragraph need rewrite. Please summarize contribution coherent story. ]} %We also explore additional training techniques leveraging pre-trained language model adding noise training corpora. Our results show fine-tuning pre-trained LM subword embedding yields strong model. Furthermore, obtain state-of-the-art model training noisy corpus synthesized randomly replacing correct words characters natural misspellings random character. Finally, condition pre-training, propose strong model outperforms subword model combining word character embedding. \iffalse We summarize contributions follows: \fi \iffalse \subsection{Stand-alone Spelling Correction} Formally, given noisy input sentence , noisy word drawn distribution possible misspellings correct word , vocabulary. We aim build corrector , correct sentence. %\textcolor{red}{[as I said, definition need section]} \fi We applied NLP algorithms identify patients cognitive concerns EHR compared model performance. While deep learning model's performance best, marginally better term based NLP models. We posit deeper representations required complex tasks requiring syntactical contextual information classifying stage cognitive impairment: MCI, mild, moderate severe dementia. Our gold standard set relatively smaller proportion patients subjective concerns mild cognitive impairment , overall sample size small. To address issues, plan implement active learning loop, starting querying additional at-risk patients age 65 without dementia related ICD code medication, apply fine-tuned model derive probability cognitive concerns patients. For edge cases, notes manually reviewed labeled. To improve efficiency review process, designed annotation tool highlights sections regular expression matches higher attention weights . The new gold-standard data serve basis next iteration active learning loop improve model performance potentially detect patients earlier stage cognitive decline. \clearpage \clearpage"," Existing natural language processing systems are vulnerable to noisy inputs resulting from misspellings.  On the contrary, humans can easily infer the corresponding correct words %\textcolor{red}{the semantics of unknown words:the corresponding correct words of misspellings}  from their misspellings and surrounding context. Inspired by this, we address the stand-alone spelling correction problem, which  %\textcolor{red}{[do not know which refers to what, confusing, please rewrite; at the same time, can you brief introduce your novel solution here?]}  only corrects the spelling of each token without additional token insertion or deletion, by utilizing both spelling information and global context representations. We present a simple yet powerful solution that jointly detects and corrects misspellings as a sequence labeling task by fine-turning a pre-trained language model. Our solution outperform the previous state-of-the-art result by $12.8\%$ absolute $F_{0.5}$ score. %Furthermore, we obtain a state-of-the-art model by augmenting the training data with synthetic character-level noise. %We also provide three useful training techniques. Our results show that a transformer-based model that encodes both local character-level and global word-level representations yields a strong performance. Furthermore, a state-of-the-art model is obtained by leveraging pre-trained language model and augmenting the training corpus with synthetic character-level noises. %fine-tuning a pre-trained language model with a subword embedding yields a strong model. Furthermore, we obtain a state-of-the-art model by training it on a noisy corpus synthesized by randomly replacing correct words and characters with common misspellings and random characters. We also propose a strong architecture that combines character and word level encoder without pre-training."
"We introduce \diagnnose, open source library analysing deep neural networks. The \diagnnose library allows researchers gain better insights internal representations networks, providing broad set tools state-of-the-art analysis techniques. The library supports wide range model types, main focus NLP architectures based LSTMs Transformers . Open-source libraries quintessential progress democratisation NLP. Popular packages include HuggingFace's -- allowing easy access pre-trained Transformer models; % AllenNLP -- providing useful abstractions components NLP pipeline, -- focusing multitask transfer learning within NLP; -- providing range feature attribution methods; -- platform visualising understanding model behaviour. We contribute open-source community incorporating several \mbox{interpretability} techniques present packages. Recent years seen considerable interest improving understanding deep neural networks operate . The high-dimensional nature models makes notoriously challenging untangle inner dynamics. This given rise novel subfield within AI focuses interpretability, providing us peak inside black box. \diagnnose aims unify several techniques one library, allowing interpretability research conducted streamlined accessible manner. \diagnnose's main focus lies techniques aid uncovering linguistic knowledge encoded within model's representations. The library provides abstractions allow recurrent models investigated way Transformer models, modular fashion. It contains extensive activation extraction module allows extraction model activations corpus. The analysis techniques currently implemented include: % <design principles> ? In paper present overview library, well case study subject-verb agreement within language models. We first present brief overview interpretability within NLP background analysis techniques part library . We provide overview \diagnnose expand briefly individual modules . % Next, provide extensive background feature attributions part library . We conclude case study subject-verb agreement, demonstrating several \diagnnose's features experimental setup . We presented multi-task learning framework enable training one universal incremental model four tasks disfluency detection, language modelling, part-of-speech tagging utterance segmentation. We observed tasks produce favorable inductive biases other, utterance segmentation disfluency detection getting benefits. We note task's optimal weighting relies heavily severity noise task. We showed word timing information helps utterance segmentation disfluency detection online setting, adding new tasks exception language modelling remarkable negative effect incremental metrics. The results show framework suitable online conversational systems, conversational agents mental health domain. In future work, intend analyze interactions different tasks occur real time. Monitoring interaction word could help highlight informative moments contribute optimisation models. Furthermore, intend use raw acoustic features input strongly time-linear model. include bib file like this:"," In this paper we introduce \diagnnose, an open source library for analysing the activations of deep neural networks. \diagnnose contains a wide array of interpretability techniques that provide fundamental insights into the inner workings of neural networks. We demonstrate the functionality of \diagnnose with a case study on subject-verb agreement within language models. \diagnnose is available at \url{https://github.com/i-machine-think/diagnnose}."
"% % % % \subsection{Motivation Problem} % % % % % \GW{Propaganda loosely defined ``misleading information spread deliberately deceive manipulate recipients'' . }% % % % % % % % % % Various factors propaganda studied humanities, including emotionality language, biased selection information deviation facts, manipulation cognition, . However, consensus decisive factors tell whether given article speech propagandistic not. % % % % % % In modern digital world, influence propaganda society drastically increased. % Hence, also major increase computer science, computational linguistics computational sociology research analyzing, characterizing and, ultimately, automatically detecting propaganda . To first degree, one may think propaganda variation fake news, works investigate propaganda refined type disinformation . % % % % % % % \GW{While false claims element propaganda, think fake news merely tip iceberg, persuasive manipulative nature propagandistic contents requires deeper approaches.} Classifiers propaganda detection need better capture propaganda expressed subtle ways language style rhetoric even demagogic wording. This holds news well social media posts speeches. In cases, correct information may presented incomplete form placed distorted contexts, along manipulative phrases, order mislead audience. % % % % Prior work mostly looked news articles tweets, typically focused strongly polarized topics like 2016 US election related Russian Internet Research Agency affair, UK Brexit discussion, political extremism. % % % % % % \LQ{All approaches consider propaganda detection classification task assuming sufficient amounts labeled in-domain training data.} \LQ{For example, ``Hack News'' datathon challenge, large number news articles sentences articles annotated distant supervision human judgment, respectively, train variety machine learning methods.} % % The resulting F1 scores leaderboard benchmark amazingly high, around 90\%. This may give impression propaganda detection solved problem. However, positively labeled samples simple cases ``loaded language'' strong linguistic cues independent topic. Moreover, learned classifiers % % benefit ample training data, self-guaranteed general. % % % % % % In paper, question prior assumptions, hypothesizing propagandistic sources speakers sophisticated creative find new forms deception evading trained classifiers. % % % \GW{The overall approach still text classification; novelty approach lies cross-domain learning, domains denote different kinds sources, news articles vs.\ social media posts vs.\ public speeches.} We acknowledge often shortage perfectly fitting labeled data, instead tap alternative sources require transfer step. Specifically, consider speeches tweets, addition news articles, article sentence levels. % \subsection{Approach Contribution} Our goal build general propaganda detectors, leverage different kinds data sources. In particular, tap political speeches notorious propagandists, Joseph Goebbels . As difficult label speeches sentences binary manner, pursue pairwise ordinal approach training data merely ranks samples strongly propagandistic speaker relatively temperate speaker. We investigate extent models learned data transferred classifying news tweets, also study inverse direction learning news tweets cope speeches. % % % % % % % Figure illustrates framework towards generalizable propaganda detection overcomes bottleneck directly applicable training labels instead leverages cross-domain learning. % % % % % The salient contributions paper follows. \newcommand{\myparagraph}[1]{{#1}.~} \newcommand{\var}[1]{\mbox{#1}} \newcommand{\svar}[1]{\mbox{\scriptsize#1}} \newcommand{\mycaption}[1]{}} \newcommand{\metric}[1]{{\mbox{#1}}} \newcommand{\Pat}{\metric{P}} \newcommand{\Patk}[1]{\mbox{\Pat@}} \newcommand{\Ratk}[1]{\mbox{\metric{R}@}} \newcommand{\gender}{``''} \newcommand{\age}{``''} \newcommand{\credit}{``''} \newcommand{\asset}{``''} \newcommand{\rcity}{``''} \diagnnose provides essential tools conducting interpretability research, providing cutting edge analysis techniques diagnostic classifiers feature attributions. The modular design library allows complex hypotheses tested rapidly, provides solid basis development novel interpretability techniques. The library code open source welcomes others contribute: eagerly looking forward collaborate adding new features library.","  As news and social media exhibit an increasing amount of manipulative polarized content, detecting such propaganda has received attention as a new task for content analysis. Prior work has focused  % on supervised learning with training data from the same domain. However, as propaganda can be subtle and keeps evolving, manual identification and proper labeling are very demanding. As a consequence, training data is a major bottleneck.   In this paper, we tackle this bottleneck and present an approach to leverage cross-domain learning, based on labeled documents and sentences from news and tweets, as well as political speeches with a clear difference in their degrees of being propagandistic. We devise informative features and build various classifiers for propaganda labeling, using cross-domain learning.  % % % Our experiments demonstrate the usefulness of this approach, and identify difficulties and limitations in various configurations of sources and targets for the transfer step. We further analyze the influence of various features, and characterize salient indicators of propaganda. %"
"\looseness=-1 Neural machine translation architectures~ make difficult users specify preferences could incorporated easily statistical MT models shown useful interactive machine translation~ domain adaptation~. Lexical constraints preferences previously incorporated re-training NMT models constraints inputs~ constrained beam search drastically slows decoding~. \looseness=-1 In work, introduce translation model seamlessly incorporate users' lexical choice preferences without increasing time computational cost decoding time, trained regular MT samples. We apply model MT tasks soft lexical constraints. As illustrated Figure, decoding soft lexical constraints, user preferences lexical choice output language provided additional input sequence target words order. The goal let users encode terminology, domain stylistic preferences target word usage, without strictly enforcing hard constraints might hamper NMT's ability generate fluent outputs. Our model Edit-Based TransfOrmer Repositioning , builds recent progress non-autoregressive sequence generation~. Specifically, Levenshtein Transformer~ showed iteratively refining output sequences via insertions deletions yields fast flexible generation process MT automatic post-editing tasks. \modelname replaces deletion operation novel reposition operation disentangle lexical choice reordering decisions. As result, \modelname exploits lexical constraints effectively efficiently Levenshtein Transformer, single reposition operation subsume sequence deletions insertions. To train \modelname via imitation learning, reposition operation defined preserve ability use Levenshtein edit distance~ efficient oracle. We also introduce dual-path roll-in policy lets reposition deletion models learn refine respective outputs effectively. \looseness=-1 Experiments Romanian-English, English-German, English-Japanese MT show \modelname achieves comparable better translation quality faster decoding speed Levenshtein Transformer standard MT tasks exploit soft lexical constraints better: achieves significantly better translation quality matches constraints faster decoding speed Levenshtein Transformer. It also drastically speeds decoding compared lexically constrained decoding algorithms~. Furthermore, results highlight benefits soft constraints hard ones \---\ \modelname soft constraints achieves translation quality par better \modelname Levenshtein Transformer hard constraints~. \balance Although propaganda become pervasive challenge online media, previous work mostly treated variation fake news, considered unrealistic settings test distribution precisely matches training data distribution. In paper, present \GW{a first preliminary} analysis problem propaganda detection cross-domain learning settings. This encompasses several novel aspects, ranging data collection methods, feature computation, designing different classifiers, corresponding analysis. \GW{We tap previously unexplored content source: speeches politicans known different levels propaganda, using collective relative signals.} On methodology side, devise pairwise ranking method customized loss functions improve classification. The experimental results demonstrate effectiveness method. Furthermore, conduct series experiments explore salient factors cross-domain generalizability propaganda detection learning. The observations analysis reveal insightful patterns lessons building general propaganda detectors. \GW{As datasets still fairly small, findings preliminary nature methodology subject ongoing research. We believe cross-domain learning crucial asset important topic propaganda detection, hope initial results useful research along lines.}"," We introduce an Edit-Based TransfOrmer with Repositioning , which makes sequence generation flexible by seamlessly allowing users to specify preferences in output lexical choice. Building on recent models for non-autoregressive sequence generation, \modelname generates new sequences by iteratively editing hypotheses. It relies on a novel reposition operation designed to disentangle lexical choice from word positioning decisions, while enabling efficient oracles for imitation learning and parallel edits at decoding time. Empirically, \modelname uses soft lexical constraints more effectively than the Levenshtein Transformer while speeding up decoding dramatically compared to constrained beam search. \modelname also achieves comparable or better translation quality with faster decoding speed than the Levenshtein Transformer on standard Romanian-English, English-German, and English-Japanese machine translation tasks."
"The goal relation extraction extract relationships two entities plain text. Supervised learning methods relation extraction widely used extract relations based training labeled data. Distant supervision crowdsourcing used collect examples labels train model relation extraction. However, methods limited quantity quality training data manually labeling data time-consuming labor-intensive data labeled distant-supervision noisy. To overcome problem insufficient high-quality data, few-shot learning designed require labeled sentences training. A lot research done few-shot learning computer vision~, work also includes few-shot learning methods relation extraction~. Although works require instances training, still work many scenarios training instances available. Some work open information extraction discovers new relationships open-domain corpora without labeling data. OpenIE aims extract relation phrases directly text. However, technique effectively select meaningful relation patterns discard irrelevant information. In addition, technique discover relations relation's name appear given sentence. For example, OpenIE identify relation sentence shown Figure. To address aforementioned limitations, focus relation extraction context zero-shot learning. Zero-shot learning similar way humans learn recognize new concepts. It novel learning technique use exemplars unseen categories training. We propose zero-shot learning model relation extraction , focuses recognizing new relations corresponding labeled data available training. ZSLRE modified prototypical networks utilizing side information. We construct side information labels synonyms, hypernyms two name entities keywords training sentences. The ZSL-based model recognize new relations based side information available instead using collection labeled sentences. We incorporate side information enable model extract relations never appear training datasets. We also build automatic hypernym extraction framework help us acquire hypernyms different entities directly web. Details side information construction described Section Side Information Extraction. Figure shows example side information used extract relations. Different side information given different relations. The query sentence example relation classmate\_of, word classmate never appears sentence. We first get two name entities Nell Newman Mayday Parker sentence extract hypernyms name entities person person based proposed hypernym extraction module Section Hypernyms Extraction. In example, relationship capital\_of eliminated hypernyms capital\_of location location. Then extract keywords course school query sentence compare distance keywords side information box. In way, relationship children\_of eliminated. To make relation extraction effective real-world scenarios, design models ability extract relations training instances relations without training instances. We modify vanilla prototypical networks deal scenarios compare distance query sentence prototype. If exponential minus distance threshold, consider query sentence new relation. For new relations extraction, take side information embedding query sentence compare distance side information embedding new relations. We conduct different experiments noisy clean dataset adding different percentages new relations evaluate effectiveness robustness proposed model. Besides, also evaluate proposed model supervised learning, few-shot learning zero-shot learning scenarios results show proposed model outperforms existing models three scenarios. The contributions paper summarized follows: The rest paper organized follows. Section Related Work reviews work supervised relation extraction, open relation extraction zero-shot learning. Section Methodology describes proposed ZSLRE model. Section Experiments presents experiments compares performance model different models two public datasets. Section Conclusion Future Work includes discussion conclusion promising future work. We proposed multi-source embedding model, MW2V, aimed dealing general language variations. Each slice obtained sources represent time, geography, field, among dimensions. To demonstrate feasibility, applied MW2V three newspaper datasets: The New York Times The Guardian study temporal variations, combination datasets model cultural variations. We performed exhaustive evaluation method text analysis tasks finding good quantitative qualitative results compared state art, even temporal case, MW2V specifically model time direction. Future work includes analysis applications, oriented exploitation datasets, also possible implications use regularization parameter dependent slices words, instead constant one. Moreover, insight needed answer open questions raised proposal, namely, try broader scope languages evaluate robustness."," Most existing supervised and few-shot learning relation extraction methods have relied on labeled training data. However, in real-world scenarios, there exist many relations for which there is no available training data. We address this issue from the perspective of zero-shot learning  which is similar to the way humans learn and recognize new concepts with no prior knowledge. We propose a zero-shot learning relation extraction  framework, which focuses on recognizing novel relations that have no corresponding labeled data available for training. Our proposed ZSLRE model aims to recognize new relations based on prototypical networks that are modified to utilize side  information. The additional use of side information allows those modified prototype networks to recognize novel relations in addition to recognized previously known relations. We construct side information from labels and their synonyms, hypernyms of name entities, and keywords. We build an automatic hypernym extraction framework to help get hypernyms of various name entities directly from web. We demonstrate using extensive experiments on two public datasets  that our proposed model significantly outperforms state-of-the-art methods on supervised learning, few-shot learning and zero-shot learning tasks. Our experimental results also demonstrate the effectiveness and robustness of our proposed model in a combination scenario. Once accepted for publication, we will publish ZSLRE's source code and datasets to enable reproducibility and encourage further research."
"Unlabeled data leveraged many ways natural language processing including back-translation~, self-training~, language model pre-training led improvements many natural language tasks~. While pre-training achieved impressive results tasks labeled data limited, improvements settings abundant labeled data modest~ controlled studies showing clear trend diminishing returns amount training data increases~. In paper, focus noisy channel modeling text generation tasks, classical technique statistical machine translation literature workhorse text generation tasks decades arrival neural sequence sequence models~. Unlike pre-training approaches, approach effective irrespective amount labeled data: since recent revival~, important part winning entries several high resource language pairs WMT 2019~, improving strong ensembles used 500M back-translated sentences. At low resource WAT 2019 machine translation competition, noisy channel modeling also key factor winning entry~. Noisy channel modeling turns text generation head: instead modeling output sequence given input, Bayes' rule applied model input given output, via backward sequence sequence model combined prior probability output, typically language model. This enables effective use strong language models trained large amounts unlabeled data. The role backward model, channel model, validate outputs preferred language model respect input. A straightforward way use language models pair standard sequence sequence models~. However, address explaining away effects modern neural sequence models still suffer~. As consequence, models susceptible producing fluent outputs unrelated input~. The noisy channel approach explicitly addresses via channel model. However, major obstacle efficient noisy channel modeling generating outputs much slower decoding standard sequence sequence model. We address issue introducing several simple yet highly effective approximations increase speed noisy channel modeling order magnitude make practical. This includes smaller channel models well scoring subset channel model vocabulary. Experiments WMT English-Romanian translation show noisy channel modeling outperform recent pre-training results. Moreover, show noisy channel modeling benefits much larger beam sizes strong pre-training methods. In paper, propose ZSLRE, zero-shot learning relation extraction framework based modified prototypical networks, detect new relations corresponding labeled data available training. ZSLRE utilizes side information constructed labels, keywords hypernyms entities extracted proposed automatic hypernym extraction framework. In experiments, evaluate model supervised learning, few-shot learning zero-shot learning scenarios, demonstrates proposed ZSLRE outperforms state-of-art models scenarios. In addition, results demonstrate effectiveness robustness proposed model. In future work, plan explore following directions: Due surprising improvement performance made side information embedding, explore whether simply learning good representation type relation achieve similar better performance state-of-art works using meta-learning algorithms. We explore ways better embed side information explore using popular sentence encoders besides CNN relation extraction. We explore zero-shot learning cross-sentence relation extraction."," Pre-training models on vast quantities of unlabeled data has emerged as an effective approach to improving accuracy on many NLP tasks. On the other hand, traditional machine translation has a long history of leveraging unlabeled data through noisy channel modeling.  The same idea has recently been shown to achieve strong improvements for neural machine translation. Unfortunately, na\""{i}ve noisy channel modeling with modern sequence to sequence models is up to an order of magnitude slower than alternatives.  We address this issue by introducing efficient approximations to make inference with the noisy channel approach as fast as strong ensembles while increasing accuracy. We also show that the noisy channel approach can outperform strong pre-training results by achieving a new state of the art on WMT Romanian-English translation."
"% Sentiment analysis text classification technique analyses given text returns nature underlying opinion. Therefore, sentiment analysis widely used tasks brand monitoring, political research analysis, product analysis, workforce analysis many more. Sentiment analysis techniques could fundamentally sub divided two categories lexicon-based approach machine learning based approach. Recently introduced deep learning based sentiment analysis techniques outperformed lexicon based approaches traditional machine learning approaches. With development deep learning techniques Convolutional Neural Networks , Recurrent Neural Networks language independent features, domain sentiment analysis reported impressive results. Over years, many variants combinations deep learning techniques feature representations used high resourced languages English. There also exist certain advancements sentiment analysis languages Chinese, Arabic, Spanish Indic languages. Sinhala, morphologically rich Indo-Aryan language, experienced advancements due insular under-resourced nature. One main challenges large enough annotated corpora. The data set from~\citet{liyanage2018sentiment} publicly available annotated data set sentiment analysis. However includes 5010 comments extracted one news source, contains POSITIVE NEGATIVE samples. %Work of~\citet{medagoda2017framework} example simple solutions Sinhala sentiment analysis. Under approaches, rule-based techniques, lexicon based techniques, supervised semi-supervised machine learning techniques employed traditional language dependent features. The st experiment using deep learning techniques Sinhala sentiment analysis conducted by~\citet{liyanage2018sentiment}. Under research, basic deep learning techniques Long Short-Term Memory network CNN used categorize news comments POSITIVE NEGATIVE. %The LSTM trained fastText embeddings outperformed traditional machine learning techniques Decision Tree, SVM, Na\""ive Bayes. ~\citet{DemotteSLSTM2020Sinhala} conducted experiment data set using Sentence-State LSTM , rather advanced technique analysis improved considering n-gram features text word embeddings. In paper, present comprehensive empirical study use deep learning techniques document-level sentiment analysis Sinhala respect four sentiment categories POSITIVE, NEGATIVE, NEUTRAL CONFLICT. The experiments conducted commonly used sequence models RNN, LSTM, Bi-LSTM, various improvements vanilla models stacking regularization, well recent ones hierarchical attention hybrid neural networks capsule networks. % multi-class sentiment analysis using word embeddings language independent features. These langauge independent features able outperform usage traditional language dependent features part speech tagging lexical resources. ~Furthermore, present data set 15059 comments, annotated four classes used sentiment analysis, based Sinhala news comments extracted online newspapers namely GossipLanka Lankadeepa. This publicly available multi-class, multi-source dataset Sinhala sentiment analysis. Our code implementation, word embedding models, annotated data set publicly available. % We introduced number approximations greatly speed noisy channel modeling neural sequence sequence models. This includes using channel models fraction size commonly used sequence sequence models, pruning channel model output vocabulary, reducing number beam candidates scored channel model. Our approximations simple, yet, highly effective enable comparable inference speed ensembles direct models delivering higher accuracy. Our experiments show noisy channel modeling outperform pre-training approaches able better exploit wider beams. Moreover, achieved using smaller amount monolingual data. \clearpage"," Due to the high impact of the fast-evolving fields of machine learning and deep learning, Natural Language Processing  tasks have further obtained comprehensive performances for highly resourced languages such as English and Chinese. However Sinhala, which is an under-resourced language with a rich morphology, has not experienced these advancements. For sentiment analysis, there exists only two previous research with deep learning approaches, which focused only on document-level sentiment analysis for the binary case. They experimented with only three types of deep learning models. In contrast, this paper presents a much comprehensive study on the use of standard sequence models such as RNN, LSTM, Bi-LSTM, as well as more recent state-of-the-art models such as  hierarchical attention hybrid neural networks, and capsule networks. Classification is done at document-level but with more granularity by considering POSITIVE, NEGATIVE, NEUTRAL, and CONFLICT classes. A data set of 15059 Sinhala news comments, annotated with these four classes and a corpus consists of 9.48 million tokens are publicly released. This is the largest sentiment annotated data set for Sinhala so far.   % In addition to that,  was extracted from both comments and articles of online newspapers. %Furthermore, the language-independent features such as Word2Vec and fastText were experimented for novel deep learning techniques which clearly indicate the importance of word embedding techniques for NLP tasks including sentiment analysis for Sinhala as a low resource language. % Due to the high impact of the fast-evolving field of machine learning and deep learning, the Natural Language Processing  tasks have further obtained comprehensive and prominent performances over the past few decades. Different variations and combinations of deep learning techniques have been employed for NLP tasks in general. These experiments illustrated highly improved performances with respect to the traditional rule-based and statistical machine learning techniques. These advancements were mainly impacted towards the development of popular languages such as English and Chinese. However, Sinhala which is an under-resourced language with rich morphology, have not experienced these advancements due to fewer resources for NLP tasks. For sentiment analysis, there exist only two previous research with deep learning approaches, which also conducted with less granularity while giving sub optimality with respect to recent advancements in deep learning techniques. In this paper, we present the use of state-of-the-art deep learning approaches such as RNN, LSTM, Bi-LSTM, Hierarchical Attention Hybrid Neural Networks, and capsule networks for multi-class sentiment analysis for Sinhala news comments while considering more granularity. Under this research, we present the multi-class annotated data set which consists of Sinhala news comments extracted from online newspapers. Furthermore, the language-independent features such as word2Vec and fastText were experimented for novel deep learning techniques which clearly indicates the importance of word embedding techniques for NLP tasks including sentiment analysis."
"% The first letter 2 line initial drop letter followed % rest first word caps. % % form use first word consists single letter: % \IAENGPARstart{A}{demo} file .... % % form use need single drop letter followed % normal text : % \IAENGPARstart{A}{}demo file .... % % Some journals put first two words caps: % \IAENGPARstart{T}{his demo} file .... % % Here typical use ""T"" initial drop letter % ""HIS"" caps complete first word. \IAENGPARstart{T}{he} Neural Machine Translation used model state-of-the-art translation systems many high-resource languages . For many language pairs though, amount and/or quality parallel data enough train NMT model whose accuracy reach acceptable standard . This category language pairs known low resource. Many works explored use easier-to-get monolingual data improve quality translation models category languages -- even high resource languages -- . The back-translation far one successful methods , involving use translations target language monolingual data increase amount training data . The additional parallel data consists authentic sentences target language translations -- synthetic sentences source language -- generated using reverse model trained available parallel data -- see procedure Algorithm 1. The approach proven successful improving quality translations high, middle low resourced languages . Many studies shown quality backward system influences performance ultimate NMT model . In low resource conditions, available parallel data may able train standard backward model quality additional data generated using model may hurt quality final model. Despite this, aim standard back-translation always improve performance target NMT model providing sufficient training data. Some previous works proposed various methods improve performance backward model training. These methods include iterative back-translation , transfer learning , self-training training bi-directional translation model backward forward translations . Others tried mask deficiencies backward model either inference generating multiple translations target sentence using sampling average-out errors individual translations noising output beam search ; reducing effects errors synthetic data training forward model methods tagged back-translation pre-training fine-tuning . We present hybrid approach utilizes monolingual target data improve forward backward models back-translation. In approach, used synthetic data enhance backward model self-learning standard back-translation improving forward model. The approach preliminary investigated shown achieve positive results. Earlier use stand-alone self-training machine translation proposed extra methods either using quality estimation freezing decoder weights training synthetic side training data. It suggested mistakes synthetic data hurt performance self-trained model . Instead, showed self-training capable improving quality backward model even without using either specialized approaches. It shown using synthetic data generated backward model help re-training backward model improved performance. The work, though, show benefits otherwise using specialized approach cleaning data, especially low resource languages. It also investigate model continue learn output iterating self-learning process. \end{table} This work, therefore, investigates effects synthetic data cleaning using automatic quality estimation training backward model. We observed approach may improve backward model, selecting subset synthetic data may result superior less generic model. We investigated use iterative self-training quality estimation proposed , enabling backward model trained monolingual data. For low resource languages, readily available quality estimation systems data train systems may available. This may limit implementation approach. We, therefore, proposed novel iterative approach relies available monolingual target data improve backward model finally generating much improved synthetic data forward model's training. Experimental results show approach superior standard back-translation approach proposed ; iterative approach superior iterative back-translation also requiring less number models trained. We thus make following contributions paper: \renewcommand{\labelitemi}{\textbullet} The remainder paper organized follows: In Section , reviewed related works. We presented proposed methods Section . We reported experiments results Section . We discussed results findings research work Sections respectively and, finally, paper concluded directions future work proposed Section . For experiments conducted identify effect punctuation marks dimension word embeddings towards sentiment analysis task, different preprocessing techniques, word embedding models, several neural network setups used. For experiments, data-set splitted train validation sets, ratio . First, different preprocessing techniques evaluated multi-level sentiment analysis task Sinhala language baseline models. For that, analysis conducted punctuation marks, without punctuation marks without punctuation marks except question mark. Next, Different dimensions Word2vec fastText models experimented baseline LSTM model identified fastText 300 dimensions could beat word embedding models per results Table. Therefore, The word embedding model fastText, dimension size 300 fixed succeeding experiments. The experiments conducted different baseline models identify best models improvements suggested BiLSTM optimal architecture primary baseline. As per results Table 10-fold cross validation, BiLSTM achieved best weighted accuracy 63.81\ weighted F1 score 57.71\ , beating Vanilla RNN, LSTM, GRU. Therefore LSTM BiLSTM selected improvements. After that, two strategies followed improve selected approaches. First strategy combining CNN baseline models. Even though expected increase weighted accuracy F1 score sentiment analysis process following improved model architecture based CNN, results suggest noticeable enhancement. One reason might enough data learn trainable parameters complex model due CNN integration. Results models listed Table along results improvements baseline models. As final improvement baseline approaches stacking implemented. As per results Table 10-fold cross validation, 'Stacked BiLSTM 3' model reached weighted accuracy 63.13\ weighted F1 socre 59.42\ outperforming approaches. This could justified ability stacked BiLSTM capture context level information left right direction considering substantial amount neural representation language modeling based stacking strategy. The capsule-B architecture went beyond experimented models producing weighted accuracy 63.23\ weighted F1-score 59.11\ 10-fold cross validation. This observation could elaborated based motivation behind capsule strategy represent neural architecture based vectors improve language representation considering exact order pose information. Furthermore, capsule-B outperformed capsule-A due sophisticated architecture designed capture n-grams features compared capsule-A. The HAHNN illustrate greater performance expected. This could due shorter length comments learn deeper neural representation attention mechanism. also employed HAHNN. The weighted accuracy experiment bounded value 65\ per inter-annotator agreement value. This direct result high volume noise dataset. As illustrated Table, CONFLICT NEUTRAL classes seem considerably mis-classified NEGATIVE comments, due impact large number NEGATIVE comments respect number CONFLICT NEUTRAL comments training set. Figure shows comments model confused classifying. The first example illustrates comment negatively classified truly CONFLICT comment. When considering interpretation comment, sentence includes two negative sentences positive sentence, indicates bias towards NEGATIVE sentiment. The second third comments include NEGATIVE NEUTRAL comments, classified POSITIVE CONFLICT, respectively. The observation second example could justified effect positive word ``{\iskool '' , greatly affects final sentiment comment, negative word ``{\iskool '' . The third example negative positive words ``{\iskool '' ``{\iskool '' , respectively. Therefore comment classified CONFLICT, even though overall sentiment comment neutral. } - m vidiya par vala ada vt pramava tulin vadikaru nidl innav. m tamayi praa adhikaraya tduvak ganna bi. mun davam da it hoda tduvak \\ {\iskool   .} - pavu ahiaka manussay. \\ {\iskool       .           .} - priyanta kiyanna deyak a nam ohoma kiyanna.otana i madi nis api dk issaraha p senaga piril innav. \\"," %\boldmath Many language pairs are low resource, meaning the amount and/or quality of available parallel data is not sufficient to train a neural machine translation  model which can reach an acceptable standard of accuracy. Many works have explored using the readily available monolingual data in either or both of the languages to improve the standard of translation models in low, and even high, resource languages. One of the most successful of such works is the back-translation that utilizes the translations of the target language monolingual data to increase the amount of the training data. The quality of the backward model which is trained on the available parallel data has been shown to determine the performance of the back-translation approach. Despite this, only the forward model is improved on the monolingual target data in standard back-translation. A previous study proposed an iterative back-translation approach for improving both models over several iterations. But unlike in the traditional back-translation, it relied on both the target and source monolingual data. This work, therefore, proposes a novel approach that enables both the backward and forward models to benefit from the monolingual target data through a hybrid of self-learning and back-translation respectively. Experimental results have shown the superiority of the proposed approach over the traditional back-translation method on English-German low resource neural machine translation. We also proposed an iterative self-learning approach that outperforms the iterative back-translation while also relying only on the monolingual target data and require the training of less models."
"End-to-end techniques automatic speech recognition , notably sequence-to-sequence models attention Recurrent Neural Network Transducer , becoming increasingly popular. Compared traditional hybrid system based Hidden Markov Model Deep Neural Network individually-trained components, parts end-to-end model optimized jointly, often leads better performance recognition tasks sufficient training data low training-testing mismatch. End-to-end systems simpler train; typically require pronunciation lexicons, decision trees, initial bootstrapping, forced alignment. End-to-end models also suitable on-device use cases due lack external language models decoding graphs, whose sizes prohibitively large hybrid setups large vocabulary support, complex LMs, context-dependent decision trees. End-to-end systems limitations, however. Their end-to-end nature leads lack composability, acoustic, language, pronunciation models hybrid setups. This lack composability turn leads challenges personalization, traditionally involves on-the-fly modification external LMs add, boost, penalize certain words phrases. Previous work end-to-end ASR addressed issue incorporating external LMs beam search , special modifications handle model's spiky output . A fundamental limitation shallow fusion relies late combination, hence model needs potential produce correct output first place without access biasing information. Another class method adds attention-based simple biasing module contextual phrases provide additional signal decoder component end-to-end models. While promising, methods shown problems scaling large highly confusable biasing lists. A closely related challenge ASR personalization entity recognition, since many cases biasing items entity names. Rare name recognition presents significant challenges end-to-end models two main reasons. First, output units end-to-end models typically graphemes WordPieces , work well spelling word correspond pronounced . Second, rare names often decompose target sequences seen enough training, making difficult recognize correctly. By contrast, problems alleviated hybrid systems due use phonetic lexicons and/or clustered context-dependent acoustic targets. Popular solutions problem include upsampling entity-heavy data generating synthetic training data names using text-to-speech . While method alleviates data sparsity issue, address underlying problems under-trained targets unconventional spelling rare names. In work, propose several novel techniques address challenges improve RNN-T personalization. To alleviate problem under-trained targets recognition unconventional names, adopt on-the-fly sub-word regularization increase WordPiece coverage training, perform pre-training multi-task learning strengthen encoder, leverage grapheme-to-grapheme generate alternative graphemic pronunciations names. To address limitation shallow fusion relying late combination, introduce deep personalized LM fusion influence model's predictions earlier. We show combination techniques results 15.4\%--34.5\% relative Word Error Rate improvement top strong RNN-T baseline leverages shallow fusion TTS augmentation. Our final model also competitive hybrid system significantly larger disk memory footprint. Neural machine translation systems relies huge amount parallel data train standard, state-of-the-art translation models. For low resource languages, models perform woefully deployed. Back-translation approach introduced NMT enable generation additional data improving translation low high resource languages. Subsequent studies shown approach require special methods reach acceptable standard translation quality especially low resource set-ups . In set-ups, backward model trained scarce data and, therefore, quality generated additional data may enough substantially improve target translation model. The target back-translation always improve performance forward model available monolingual data intermediary backward model. But standard forward model relies authentic data ability backward model generate good enough additional training data. } & \multicolumn{4}{c|}{Sample size} \\ \cline{3-6} & & 50 & 100 & 500 & 1000 \\ \hline backward\_ft better baseline & 11.06 &100\ & 100\ & 100\ & 100\ \\ \hline backward\_ft + QE better baseline & 13.41 &100\ & 100\ & 100\ & 100\ \\ \hline backward\_ibt better baseline & 13.64 &100\ & 100\ & 100\ & 100\ \\ \hline backward\_isl + QE better baseline & 13.80 &100\ & 100\ & 100\ & 100\ \\ \hline backward\_isl better baseline & 14.02 &100\ & 100\ & 100\ & 100\ \\ \hline backward\_ft + QE better backward\_ft & 2.35 &100\ & 100\ & 100\ & 100\ \\ \hline backward\_ibt better backward\_ft & 2.58 &100\ & 100\ & 100\ & 100\ \\ \hline backward\_isl + QE better backward\_ft & 2.74 &100\ & 100\ & 100\ & 100\ \\ \hline backward\_isl better backward\_ft & 2.96 &100\ & 100\ & 100\ & 100\ \\ \hline backward\_ibt better backward\_ft + QE & 0.23 &94\ & 94\ & 96\ & 94.4\ \\ \hline backward\_isl + QE better backward\_ft + QE & 0.39 &100\ & 100\ & 100\ & 99.8\ \\ \hline backward\_isl better backward\_ft + QE & 0.61 &100\ & 100\ & 100\ & 100\ \\ \hline backward\_isl + QE better backward\_ibt & 0.16 &86\ & 89\ & 88.8\ & 87.4\ \\ \hline backward\_isl better backward\_ibt & 0.38&100\ & 100\ & 99.6\ & 99.7\ \\ \hline backward\_isl better backward\_isl + QE & 0.22 &94\ & 92\ & 93.4\ & 94.9\ \\ \hline \arrayrulecolor{black} This work, therefore, presents new variant back-translation incorporates self-learning approach, forward translation, use target-side monolingual data improve forward model, backward model also. The back-translation used ultimately improve forward model using self-training enhance standard backward model. } & \multicolumn{4}{c|}{Sample size} \\ \cline{3-6} & & 50 & 100 & 500 & 1000 \\ \hline standard back-translation better baseline & 7.89 &100\ & 100\ & 100\ & 100\ \\ \hline self-learning enhanced back-translation better baseline & 8.96 &100\ &100\ & 100\ & 100\ \\ \hline self-learning quality estimation enhanced back-translation better baseline & 9.44 &100\ & 100\ & 100\ & 100\ \\ \hline iterative back-translation better baseline & 9.19 &100\ & 100\ & 100\ & 100\ \\ \hline iterative self-learning quality estimation enhanced back-translation better baseline & 9.10 &100\ & 100\ & 100\ & 100\ \\ \hline iterative self-learning enhanced back-translation better baseline & 9.38 &100\ & 100\ & 100\ & 100\ \\ \hline self-learning enhanced back-translation better standard back-translation & 1.08 &100\ & 100\ & 100\ & 100\ \\ \hline self-learning quality estimation enhanced back-translation better standard back-translation & 1.56 &100\ & 100\ & 100\ & 100\ \\ \hline iterative back-translation better standard back-translation & 1.31 &100\ & 100\ & 100\ & 100\ \\ \hline iterative self-learning quality estimation enhanced back-translation better standard back-translation & 1.22 &100\ & 100\ & 100\ & 100\ \\ \hline iterative self-learning enhanced back-translation better standard back-translation & 1.49 &100\ & 100\ & 100\ & 100\ \\ \hline self-learning quality estimation enhanced back-translation better self-learning enhanced back-translation & 0.48 &100\ & 100\ & 100\ & 100\ \\ \hline iterative back-translation better self-learning enhanced back-translation & 0.23 &96\ & 97\ & 95.6\ & 95.7\ \\ \hline iterative self-learning quality estimation enhanced back-translation better self-learning enhanced back-translation & 0.14 &86\ & 86\ & 85\ & 86.4\ \\ \hline iterative self-learning enhanced back-translation better self-learning enhanced back-translation & 0.42 &100\ & 100\ & 100\ & 100\ \\ \hline iterative back-translation better iterative self-learning quality estimation enhanced back-translation & 0.09 &78\ & 77\ & 74\ & 74.9\ \\ \hline iterative self-learning enhanced back-translation better iterative self-learning quality estimation enhanced back-translation & 0.28 &98\ & 99\ & 98.2\ & 99.1\ \\ \hline self-learning quality estimation enhanced back-translation better iterative self-learning quality estimation enhanced back-translation & 0.34 &100\ & 100\ & 99.2\ & 99.9\ \\ \hline iterative self-learning enhanced back-translation better iterative back-translation & 0.19 &96\ & 92\ & 91\ & 93.5\ \\ \hline self-learning quality estimation enhanced back-translation better iterative back-translation & 0.25 &100\ & 98\ & 96.2\ & 97.6\ \\ \hline self-learning quality estimation enhanced back-translation better iterative self-learning enhanced back-translation & 0.06 &60\ & 69\ & 66\ & 67.8\ \\ \hline \arrayrulecolor{black} In implementing self-learning approach, investigated various methods namely: self-training iterative self-training without quality estimation. We implemented methods using pre-training fine-tuning strategies enable model differentiate synthetic authentic data training, shown improve performance models trained settings . All performance scores obtained experiments shown statistically significant using paired bootstrap resampling implemented -- see Tables . The work evaluated low resource IWSLT'14 English-German neural machine translation. We observed even though proposed self-trained backward method outperformed standard back-translation's backward model without using quality estimation freezing parameters decoder proposed respectively, selecting using best synthetic data self-training improves performance. This shows although improved performance achieved, full potential proposed method may realized using vanilla self-training noise synthetic data degrade decoder's performance. We extended positive results obtained using self-learning determining benefit otherwise selecting fraction synthetic data self-training using quality estimation system. Experimental results indicated result affected reduction training data, performance improved significantly, achieving +2.35 BLEU. We showed synthetic data required -- quantity -- beneficial additional data -- quality -- enough train superior backward model. Also, backward model -- model -- able differentiate synthetic authentic parallel data training, effects lack quality synthetic data becomes less problematic qualitative synthetic data is, better model trained. We also implemented iterative approach continued enhance quality backward model synthetic data. Each improved backward model used generate synthetic training data training next improved model. The approach achieved significant +2.96 BLEU improvement one-time usage self-training IWSLT'14 En-De test set. We compared iterative self-learning approach iterative approaches method shown superior also requiring less number models -- \-less number models \ iterations -- trained needed approach . Also, unlike , showed without data selection quality estimation, achieved improved model baseline. While suggests models trained synthetic data reach performance similar models trained authentic data only, showed model trained sufficient number qualitative synthetic sentences achieve better performance model trained low resource authentic parallel data. claimed ratio synthetic parallel data affects translation model quality backward model. This, claimed, model tends learn synthetic data often contain noise. Instead, claim quality backward model affects performance approach ratio model able generate synthetic data close quality human translation, ratio synthetic data authentic data matter two data become inseparable. The iterative self-learning enhanced back-translation approach proposed avoid much reliant availability reliability, thereof, quality estimation systems successful implementation previously proposed approaches. We determined without systems reliably available, retraining backward model iterations capable achieving even superior performance methods. The forward models' performances shown reflect improvements backward models. We achieved improved +0.48 BLEU performance self-trained enhanced back-translation method. The proposed approaches achieved better performances previous methods similar quality observed them. This expected performances backward models far other. In Table , showed sample translation English German. Our proposed models able produce exact translations referenced translation: ''... wir 3 milliarden stunden pro woche mit online-spielen'' part translation generated different, meaning same: ''derzeit'' 'vs' ''jetzt''. The self-trained models able generate exact translation referenced text could specify adverb ''now'' instead referenced ''right now''. For forward model, effects improved backward models observed performances. In Table , also translated given German source text English. The performances last two models , especially last model, seemed superior rest. The pre-training fine-tuning approach shown better approach applying method proposed work. As proposed , found pre-training first synthetic data thereafter fine-tuning model authentic data best strategy. \section{Conclusion \& Future work} This first work proposed iterative utilization monolingual target data using joint backward forward translation improve neural machine translation low resource languages, best knowledge. It also first work combines quality estimation self-learning improve low resource back-translation NMT. This category languages shown straggle high resource counterparts even methods applied improve quality. The back-translation approach shown tremendous potential improving translation performance high resource languages, shown improved less desirable performance low resource languages. This shown result lack quality backward model. In work, applied joint backward forward translation utilize monolingual data target language train better neural machine translation systems especially low resource languages. We proposed variety techniques implementing approach based availability otherwise another supporting system -- quality estimation system. The self-learning used improve performance backward model. Experimental results obtained low resource English-German shown approach superior widely successful back-translation approach. The approach straightforward also easy implement low resource language translation train better model capable attaining acceptable standard translation. We showed approach capable enhancing standard model even without using specialized quality estimation data selection method. We also showed backward model able differentiate synthetic authentic data, quality gets better. As shown training forward models, also true models trained synthetic authentic data. The self-training approach shown perform better quality estimation used extract best translations used retrain generating backward model. We also extended self-training approach determine whether approach continue benefit backward model several iterations. We presented simplified iterative approach reduces number models required time taken achieve number iterations previous works. We showed possible rely large amounts synthetic data gets improved iteratively especially low resource conditions strictly relying quality fewer training data. Our work relies target monolingual data required traditional back-translation approach unlike target source monolingual data iterative back-translation approach. We showed approach works well low resource neural machine translation. For future work, aim determine appropriate sentences considered fit self-learning iteration especially using data selection alternative quality estimation. We also intend apply approach high resource languages. Can use something like put references page using endfloat captionsoff option. \ifCLASSOPTIONcaptionsoff \fi trigger given reference number - used balance columns last page adjust value needed - may need readjusted document modified later \IAENGtriggeratref{8} The ""triggered"" command changed desired: \IAENGtriggercmd{\enlargethispage{-5in}} references section use bibliography generated BibTeX .bbl file BibTeX documentation easily obtained at: http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/ The IAENGtran BibTeX style support page at: http://www.michaelshell.org/tex/IAENGtran/bibtex/ argument BibTeX string definitions bibliography database <OR> manually copy resultant .bbl file set second argument \begin number references >>>>>>>>>>>>>>>>>>>>>> Bibliography <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< biography section If EPS/PDF photo extra braces needed around contents optional argument biography prevent LaTeX parser getting confused sees complicated \includegraphics command within optional argument. photo all: insert needed balance two columns last page biographies You push biographies placing \vfill them. The appropriate use \vfill depends kind text last page whether columns equalized. \vfill Can used pull biographies bottom last one flush column. \enlargethispage{-5in} that's folks"," End-to-end models in general, and Recurrent Neural Network Transducer  in particular, have gained significant traction in the automatic speech recognition community in the last few years due to their simplicity, compactness, and excellent performance on generic transcription tasks. However, these models are more challenging to personalize compared to traditional hybrid systems due to the lack of external language models and difficulties in recognizing rare long-tail words, specifically entity names. In this work, we present novel techniques to improve RNN-T's ability to model rare WordPieces, infuse extra information into the encoder, enable the use of alternative graphemic pronunciations, and perform deep fusion with personalized language models for more robust biasing. We show that these combined techniques result in 15.4\%--34.5\% relative Word Error Rate improvement compared to a strong RNN-T baseline which uses shallow fusion and text-to-speech augmentation. Our work helps push the boundary of RNN-T personalization and close the gap with hybrid systems on use cases where biasing and entity recognition are crucial."
"Our goal improve information extraction business documents contribute field automated document processing. This work leads higher success metric enables less manual work regarding data entry and/or annotation industry. To put work context define terms closely let's briefly recall definition task, motivation add details. \paragraph{Information extraction task} The general problem information extraction new problem . A survey information extraction methods defines task as: ``Information Extraction starts collection texts, transforms information readily digested analyzed. It isolates relevant text fragments, extracts relevant information fragments, pieces together targeted information coherent framework''. The relevant collection texts study texts business documents invoices, pro forma invoices debit notes. The targeted information classification texts helps automating various business processes  automated payment invoices. \paragraph{Motivation} The typical user method would company medium-sized bigger because, point, companies start spend significant time document processing. Details harder find referenced peer-reviewed works since companies keep spending information secret. Approximations unofficial sources lead estimate success metric translates company savings. A typical medium-sized company approximately invoices per month even improvement roughly translates dollars saving monthly scales company size. Note heuristics thus define metric exactly. \paragraph{Details overview} As stated, focus business documents. The explicit category documents varies. Existing works information extraction define ``visually rich documents'', ``structured'', ``semi-structured''. We use name ``structured documents'' throughout work since structure documents clear understandable human working relevant fields, even though specific structure varies. Moreover, documents machine-readable detail individual words pictures page, machine, ``understandable'' respect goal important information extraction. It important classify information needed financial/accounting industry, ``users'' documents. For example, payment details, amount paid, issuer information etc. The input document's page goal identify output words entities document considered important, along respective classifications. One example input invoice output extraction seen \prettyref{fig:Example}. As see, documents easily understandable inputs. An example trivial inputs would XML document desired target classes incorporated machine-readable way. With study, aim expand previous work , already shown neural networks succeed task extracting important information even identifying whole, highly specific tables. As argued before, every improvement matters work, focus improving metrics selecting relevant techniques deep learning field. A classical heuristic way generally improve target metric provide relevant information network. Previously exhausted information present single invoice focus techniques related ``similarity''. Existing works similarity presented \prettyref{subsec:Inspiration} use notion similarity defined \prettyref{subsec:The-learning-framework}. In short, present similar annotated document another input. More details differences previous work described \prettyref{subsec:The-differences-to-prev}. Since idea providing information fundamental even simpler templating techniques , need stress that, due nature dataset , problem cannot solved using templates. To prove statement, reasonable template-based baseline presented evaluated . The research question focus ``similarity'' based mechanism various model implementations, whether improve existing solution . The hypothesis able create least one model significantly improve results. Moreover, since presented mechanism theoretically applicable beyond scope document processing, work contribute broader audience. Ultimately present model source code outperforms previous state-of-art results. An anonymized version dataset also included open-source resource notable contribution since size greater similar dataset known date. \subsection{Related works} This subsection focuses research previous works approaches relevant field information extraction. The text subsection heavily based text . The plethora methods used historically general information extraction hard fully summarize compare. Moreover, would fair compare methods developed evaluated fundamentally different datasets. However, assessed none methods well-suited working structured documents , since generally fixed layout, language, caption set, delimiters, fonts... For example, invoices vary countries, companies departments, change time. In order retrieve information structured document, must understand it. Our criterion considering method compare human-controlled preprocessing template specification layout fixing required aim fully automated general solution. Therefore including historical method baseline compare against. In recent works, significant number successfully use graph representation document use graph neural networks. Also, key idea close one-shot principle information extraction used examined example . Both works use notions finding similar documents reusing gold-standards . The latter applies principle form template matching without need learnable parameters. Our approach also called ``word classification'' approach written , work end-to-end architecture concept memory explored. At point, important clarify differences works stream research . The important difference comes dataset disposal. The dataset explored far greater datasets used elsewhere, allows exploring deeper models opposed using graph neural networks. Indeed previous paper, proven graph neural networks work synergy additional convolution-over-sequence layers even global self-attention. For clarity, roles said layers described \prettyref{subsec:Common-architecture}. Moreover, dataset quality allowed us discover information extraction line-item table detection targets boost other. As research focused deeper models, using works baselines commonly used graph neural networks incorporated one layer amidst many, special focus. In following pages, explore models would able benefit access known similar document's page. We hope model exploit similarities documents, even similar templates. \subsection{Broader inspiration} A broader section references provided since using great variety layers exploration deep network architectures. \paragraph{One-shot learning similarity} Presented model design concept aims improve models new data without retraining network. Typically, classification model trained recognize specific set classes. In one-shot learning, usually able correctly identify classes comparing already known data. Unlike traditional multi-class classification, one-shot learning allows us attain better scores even surprisingly low numbers samples . Sometimes work even classes present training set . This concept help areas ranging computer vision variants  omniglot challenge object detection , finding similar images , face detection , autonomous vision , speech also NLP area . Among methods make one-shot learning able work, fundamental one utilizes concept similarity. For similarity work, two types data  ``unknown'' ``known''. For known data, target values known method and/or model. To classify unknown input, usual practice assign class class similar known input. Technically speaking, architecture contains iamese part. In particular, inputs passed network architecture tied weights. We draw inspiration basic principle, leave advanced methods one-shot learning research. Usually due performance reasons model asked compare new inputs every known input  subset. Therefore, prior pruning technique needs incorporated  example form nearest neighbor search embedding space, done example work . Another option would incorporate memory concept . The loss used similarity learning called triplet loss applied triplet classes data-point: Where margin positive negative classes model function mapping inputs embedding space . Generally speaking, one-shot learning classified meta-learning technique. For meta-learning, suggest recent study, like . Taking concept one step yields concept called ``zero-shot learning'' . \paragraph{Other sources inspiration} It beneficial mention sources inspiration also meaningfully close one-shot learning. Since ask ``what labels similar new data'', ``query answer'' approach considered. Recently, attention principle successfully helped pave way language models . It uncommon use attention one-shot approaches also query answer problems various problems domains . The mentioned task similarity also approached pairwise classification, even dissimilarity . Deep context weights bias phrases In paper, showed RNN-T personalization improved significantly inducing better coverage rare WordPieces training, introducing extra information encoder, leveraging G2G produce additional pronunciation variants training decoding, biasing earlier deep PLM fusion. Together, techniques help push boundary RNN-T personalization close gap traditional hybrid systems use cases require contextual biasing accurate name recognition. For future work, plan incorporate proper WFST NNLM deep PLM fusion, apply techniques end-to-end models, tackle open-domain personalization strong context prefixes always available."," The automation of document processing is gaining recent attention due to the great potential to reduce manual work through improved methods and hardware. Any improvement of information extraction systems or further reduction in their error rates has a significant impact in the real world for any company working with business documents as lowering the reliability on cost-heavy and error-prone human work significantly improves the revenue. In this area, neural networks have been applied before  even though they have been trained only on relatively small datasets with hundreds of documents so far.  To successfully explore deep learning techniques and improve the information extraction results, a dataset with more than twenty-five thousand documents has been compiled, anonymized and is published as a part of this work. We will expand our previous work where we proved that convolutions, graph convolutions and self-attention can work together and exploit all the information present in a structured document. Taking the fully trainable method one step further, we will now design and examine various approaches to using siamese networks, concepts of similarity, one-shot learning and context/memory awareness. The aim is to improve micro $F_{1}$ of per-word classification on the huge real-world document dataset.  The results verify the hypothesis that trainable access to a similar  page together with its already known target information improves the information extraction. Furthermore, the experiments confirm that all proposed architecture parts  are all required to beat the previous results.  The best model improves the previous state-of-the-art results by an $8.25\,\%$ gain in $F_{1}$ score. Qualitative analysis is provided to verify that the new model performs better for all target classes. Additionally, multiple structural observations about the causes of the underperformance of some architectures are revealed.  All the source codes, parameters and implementation details are published together with the dataset in the hope to push the research boundaries since all the techniques used in this work are not problem-specific and can be generalized for other tasks and contexts.   \keywords{one-shot learning \and information extraction \and siamese networks \and  similarity \and attention}"
"Because fact obtaining supervised training labels costly time-intensive, unlabeled data relatively easy obtain, semi-supervised learning , utilizes in-domain unlabeled data improve models trained labeled dataset , growing interest. Under context large-scale language model pretraining , language model pretrained extremely large, open-domain dataset , make best use in-domain unlabeled dataset poorly understood. There basically two ways take advantages unlabeled, in-domain dataset : {\bf in-domain pretraining}\footnote{To note, pretraining in-domain dataset distinguished pretraining large-scale, open-domain dataset largeU. The model in-domain pretraining randomly initialized taking pretrained model based open-domain dataset largeU .}, language model pretrained in-domain dataset , fine-tuned ; {\bf pseudo-label} based approach , unlabeled data points assigned labels predicted model trained , forming new dataset . A new model trained final predictions considering . Many important questions regarding behavior semi-supervised learning models context large-scale LM pretraining remain unanswered: Is semi-supervised training still beneficial presence large scale pretraining largeU? Should used in-domain LM pretraining pseudo-label generation? How pseudo-label based semi-supervised models implemented? How different semi-supervised strategies affect performances regarding different sizes, different sizes, etc. In paper, conduct comprehensive studies behavior semi-supervised learning NLP presence large-scale language model pretraining. We use task text classification example, method easily adapted different NLP tasks. Our work sheds important lights behavior semi-supervised learning models: find presence in-domain pretraining LM , open-domain LM pretraining unnecessary, able achieve better performance pretraining in-domain dataset ; in-domain pretraining strategy pseudo-label based strategy lead significant performance boosts, former performing better larger , latter performing better smaller , combination performing best; pseudo-label based strategies, self-training yields better performances small, joint training combination yields better performances large. Using semi-supervised learning models, able achieve performance around accuracy 50 training data points IMDB dataset, competitive performance 96.6 full dataset. More importantly, work marks initial step toward understanding behavior semi-supervised learning models context large-scale pretraining. The rest paper organized follows: related work detailed Section 2. Different strategies training semi-supervised models shown Section 3. Experimental results findings shown Section 4, followed brief conclusion Section 5. We introduced framework creating general purpose NLP systems solve tasks natural language descriptions, synthesizing extending previous work zero-shot learning. To make progress toward goal, create dataset, \dataset{}, rigorously evaluates well model truly understands task. The dataset designed test models' ability systematically generalize across four different areas. State-of-the-art performance \dataset \finalscore\ , leaving much room future improvement. While focused zero shot learning task descriptions, framework also permits few-shot scenarios task description given along handful examples, making meta-learning approaches applicable. This interesting avenue future work, \dataset{} also useful. To facilitate future work, make models, code, data available ."," The goal of semi-supervised learning is to utilize the unlabeled, in-domain dataset $U$ to improve models trained on the labeled dataset $D$.     Under the context of   large-scale language-model  pretraining,   how we  can make the best use of   $U$   is poorly understood:   Is semi-supervised learning still beneficial   with the presence of  large-scale pretraining?  Should $U$ be used for in-domain LM pretraining or pseudo-label generation? How should the pseudo-label based semi-supervised model    be actually implemented? How different semi-supervised strategies  affect performances regarding $D$ of different sizes, $U$ of different sizes, etc.   In this paper, we conduct comprehensive studies  on  semi-supervised learning in the  task of text classification   under the context of  large-scale LM pretraining. Our studies shed important  lights on the  behavior of semi-supervised learning methods.   We find that:    with the presence of  in-domain LM pretraining  on $U$, open-domain LM pretraining \cite{devlin2018bert}  is unnecessary, and we are able to achieve better performance with pretraining on  the in-domain dataset $U$;  both the in-domain pretraining strategy and the pseudo-label based strategy introduce  significant performance boosts,  with the former performing better with larger $U$,  the latter performing better with smaller $U$, and the combination leading to the largest performance gain;   vanilla self-training  yields better performances when $D$ is small, while joint training on the combination of  $D'$ and $D$ yields better performances when $D$ is large.   %We use the task of text classification as an example,  the method of which can be easily adapted to different NLP tasks.  Using semi-supervised learning strategies, we are able to achieve a performance of around $93.8\%$ accuracy with only 50 training data points on the IMDB dataset, and   a competitive performance of 96.6$\%$ with the full  IMDB dataset.  Our work marks an initial step toward understanding the behavior of semi-supervised learning models under the context of large-scale pretraining.\footnote{Code, models and datasets  can be found at https://github.com/ShannonAI/Neural-Semi-Supervised-Learning-for-Text-Classification}"
"\todo{Completely rewrite - emphasize many methods proposed learning embeddings learn representations entities knowledge base typically based text entity's Wikipedia article surrounding local context mentions entity . % \clm{I would ""context surrounding mentions entity -- otherwise looks like redundant making clear calling henceforth, tho stylistic} context surrounding mentions entity Recent advances neural EL involved methods pretraining entity embeddings using link graph Wikipedia learn related entities words . Similar word embeddings, past work shown embeddings reside high-dimensional pseudo-semantic space, entities close space semantically similar . % \glarionov{""with entities close space being...} However, little work done understand information different entity embeddings capture underlying entities information affects downstream performance. Our goal work identify semantic information entity representations determine information linked performance downstream EL tasks. For this, develop series probing tasks, previously used examine lexical syntactic properties neural model layers sentence encoders decoders neural machine translation systems . % \glarionov{I would group two citations end readability} % \ees{for lexical syntactic properties [this split, move info citations]}. We extract structured data entities using DBpedia context words Wikipedia anchor links create probing tasks designed evaluate knowledge-based distributional semantic contents different entity embedding models. We compare five entity embedding methods, first two downstream EL tasks. We probe learned embeddings evaluate semantic information important downstream tasks represented different models. % \ees{We show strong relationship probing task performance performance downstream EL tasks. [too long, break up]} We find pretrained entity embedding methods generally effective representing distributional knowledge-based semantic information models generate embeddings byproduct training EL task. These improved representations lead better performance EL tasks, best model showing high performance distributional knowledge-based semantic tasks. We find entity embeddings trained predict related words entities skipgram-like model able learn fine-grained entity type information specific relationship types entities without explicitly providing information. Our primary contributions work to: % 1) describe methods evaluating semantic information learned methods 2) to\clm{either move first ""to"" ""1)"", delete one} empirically demonstrate importance information creating models entities use downstream tasks.\clm{I agree Liz bullet point this, want highlight contributions -- easier reviewers} % \ees{maybe bullet point two put 1) .. 2) make mad easy scan get} Our hope information provide guidance developing architectures better combine explicit structured information text improve methods representing entities used variety downstream tasks, similar existing word embeddings. Our methods additionally used potentially detect deficiencies new representation methods biases learned attributes probing tasks. % biases current methods probing .\clm{You might want briefly address means detects bias, otherwise question could feel unanswered reader's head} In paper, conduct comprehensive analysis semi-supervised learning NLP context large-scale language model pretraining. We find even presence large-scale LM pretraining, in-domain pretraining strategy pseudo-label based strategy introduce additional significant performance boost, former performing better larger , latter performing better smaller , combination leading best performance. Using semi-supervised learning models, able achieve performance around accuracy 50 training data points IMDB dataset, competitive performance 96.6 full dataset. Our work sheds light behavior semi-supervised learning models context large-scale pretraining.","  \todo{Complete rewrite} Pretrained entity embedding methods have shown strong results in entity linking  systems compared to methods that generate entity representations from text descriptions. Prior work has shown that these embeddings inhabit a pseudo-semantic space, but the semantic information they contain has not been thoroughly explored nor have  they been compared with other representations for differences in information.  We introduce methods for probing learned entity representations for information about their entity types, relationships, and context words using Wikipedia anchors and DBPedia structured data and use them to compare five entity embedding models. We show that improved representation of all types of semantic information is linked to improved performance on two downstream EL tasks. Our results provide potential directions for further research to better incorporate explicit semantic information into neural entity linking models."
"In section, mention different tokenization techniques SLT explain perspective problem. We mentioned basics SLT NMT. From research perspective, NMT methods provide successful results good tokens SL vides. Therefore, tokenization seen crucial part research. Firstly, visual properties involved tokenization part. Secondly, generic approach obtain strong tokens SLs. In addition that, clear discrete tokens obtained better translation quality. For reason, extend meaning tokenization NSLT covers overall process prepare frames NMT module. \par For spoken spoken languages, generally use words tokens feed NMT module. The current state-of-the-art method converts tokens continuous embeddings reach semantic representation. While learning translation, word embedding also trained learn relationship words. Eventually, meaningful embedding obtained NMT module seen Figure . Based this, may good idea learn good representation signs replace word embeddings achieve advancements NSLT NMT done. This representation cross-lingual; learning open problem. Our research mainly focused problem. Before introducing approach, discuss existing three tokenization approaches following subsection. \subsection{Input Tokenization NSLT} \par The first approach using glosses tokens. Glosses intermediate word-like representations signs words sentences. Therefore, directly applicable NMT framework without effort. However, certain shortcomings method. Firstly, glosses rarely exist real life. Gloss annotation requires laborious process special expertise. Secondly, glosses unique SLs. Therefore, SL requires special effort obtain glosses whereas sentences commonly available. The last drawback mistake gloss level produce dramatic meaning differences translation, since glosses high level annotations, similar words. \par The second approach first one terms tokens. On top that, approach learns extract glosses frames. In words, method uses glosses explicit intermediate representations seen Figure . It eliminates search tokenization, needs special network frame gloss conversion. There two main concerns. The first one network frame gloss conversion still dependent gloss annotations. The second clear glosses upper bound SLT sufficient evidence. The problem immature result provides clues whether glosses may restrict translation quality. The third approach called frame-level tokenization. This approach establish explicit intermediate representation seen Figure . It aims learn good sign embeddings replace word-embeddings. However, golden way represent signs embeddings feed NMT module. Furthermore, clear length embedding be. Embeddings obtained frame extracted inner short clips video. In addition that, representation learned sentence-video pairs trained outside NSLT system. There several ways frame-level tokenization. However, main difference gloss level tokenization discrete representation eliminated. If find proper one, would several advantages. The first one resulting framework applied SL translation task without requiring annotation. The second advantage opportunity inject additional supervision. The representations would trained different tasks different datasets whereas gloss level tokenization cannot cover different SLs. The third one token length adjusted. To boost translation speed, number tokens reduced pre-determined number. In work, propose new set probing tasks evaluating entity embeddings applied method creates one embedding per entity. Using tasks, find entity type information one strongest signals present one embedding models, followed coarse information likely entity mentioned. We show embeddings particularly able use entity type information bootstrap way improved performance entity relationship factual information prediction tasks propose methods counteract accurately estimate well encode relationships facts. Overall, find BERT-based entity embeddings perform well many tasks, high performance often attributed strong entity type information encoding. More specialized models Wikipedia2Vec better able detect identify relationships, embeddings \citet{ganea2017joint} better capture lexical distributional semantics entities. Additionally, provide direct comparison embeddings two downstream EL tasks, models performed well probing tasks Ganea, Wiki2V, BERT performed best downstream tasks. We find best performing embedding model depends greatly surrounding architecture encourage future practitioners directly compare newly proposed methods prior models consistent architecture, rather compare results. Our work provides insight information encoded static entity embeddings, entities change time, sometimes quite significantly. One future line work would like pursue using tests investigate changes entities time reflected embeddings, changes could modeled transformations embedding space. Context-based embeddings particular could dynamically updated new information, instead retrained scratch."," In this thesis, we propose a multitask learning based method to improve Neural Sign Language Translation  consisting of two parts, a tokenization layer and Neural Machine Translation . The tokenization part focuses on how Sign Language  videos should be represented to be fed into the other part. It has not been studied elaborately whereas NMT research has attracted several researchers contributing enormous advancements. Up to now, there are two main input tokenization levels, namely frame-level and gloss-level tokenization. Glosses are world-like intermediate presentation and unique to SLs. Therefore, we aim to develop a generic sign-level tokenization layer so that it is applicable to other domains without further effort. \par We begin with investigating current tokenization approaches and explain their weaknesses with several experiments. To provide a solution, we adapt Transfer Learning, Multitask Learning and Unsupervised Domain Adaptation into this research to leverage additional supervision. We succeed in enabling knowledge transfer between SLs and improve translation quality by 5 points in BLEU-4 and 8 points in ROUGE scores. Secondly, we show the effects of body parts by extensive experiments in all the tokenization approaches. Apart from these, we adopt  3D-CNNs to improve efficiency in terms of time and space. Lastly, we discuss the advantages of sign-level tokenization over gloss-level tokenization. To sum up, our proposed method eliminates the need for gloss level annotation to obtain higher scores by providing additional supervision by utilizing weak supervision sources."
"% Storytelling central part human socialization entertainment. Many popular forms storytelling throughout history \---such novels, plays, television, movies\--- passive audience experiences. However, gaming interesting medium interactivity large part entertainment experience, interactivity storytelling often conflict: much player freedom means storyline may never explored, hand, many restrictions player freedom risks reducing gaming passive medium. Thus, interactivity storytelling important challenge gaming, much design effort put striking balance entertaining gameplay compelling storytelling. As gaming technology advances, new opportunities interactive storytelling present themselves. Better storage technology made telling longer, intricate stories possible, better graphical capabilities helped foster immersive gaming experiences. Advances artificial intelligence lead challenging opponents, realistic NPC behavior, benefits. Better procedural content generation algorithms help ensure unique gameplay experiences stay fresh longer. Finally, recent breakthroughs language modeling present new opportunity: language, thus stories, potentially generated demand. In paper, introduce novel game collaborative storytelling, human player artificial intelligence agent construct story together. The game starts AI agent reciting one curated set story starters \---opening sentences meant kick-start participants' storytelling creativity\--- human player responds adding line, refer story continuation, story. The AI agent human player take turns adding continuations story human player concludes story. The game designed restrictions possible contrasts traditional storytelling settings narrative fixed advance. Collaborative storytelling builds rich tradition collaboration storytelling includes Dungeons Dragons, improvisational comedy, theater. It could useful tool encouraging creativity overcoming writer's block, well entertaining game right. Our end goal make possible intelligent agents, robot companions avatars , play collaborative storytelling game, shown Figure. %Our supplementary material includes simulation scenario, including real stories constructed humans collaborating Web-only version current system\footnote{Stories edited brevity.}. Our primary contributions follows: In paper, presented deep learning-based scheme analyze sentiment Bengali restaurant reviews. Word2vec embedding technique used consider semantic meaning Bengali reviews. BiLSTM network tuned find optimal hyperparameter combination. A corpus 8435 Bengali restaurant reviews developed evaluate performance proposed system. The outcome experimentation exhibits proposed system outperforms baseline ML algorithms previous techniques holdout dataset. Though approach acquires satisfactory results compared works, improv-ements still required take system production level. Thus, future, try add reviews classes conjoin aspect reviews well. ---- Bibliography ---- BibTeX users specify bibliography style 'splncs04'. References sorted formatted correct style.","   Storytelling plays a central role in human socializing and entertainment. However, much of the research on automatic storytelling generation assumes that stories will be generated by an agent without any human interaction. In this paper, we introduce the task of collaborative storytelling, where an artificial intelligence agent and a person collaborate to create a unique story by taking turns adding to it. We present a collaborative storytelling system which works with a human storyteller to create a story by generating new utterances based on the story so far. We constructed the storytelling system by tuning a publicly-available large scale language model on a dataset of writing prompts and their accompanying fictional works. We identify generating sufficiently human-like utterances to be an important technical issue and propose a sample-and-rank approach to improve utterance quality. Quantitative evaluation shows that our approach outperforms a baseline, and we present  qualitative evaluation of our system's capabilities."
"The vast amounts scientific literature provide significant source information biomedical research. Using literature identify relations entities important task various applications . Existing approaches biomedical relation extraction usually fall one two categories. Mention-level extraction aims classify relation pair entities within short span text . In contrast, pair-level extraction aims classify relation pair entities across entire paragraph, document corpus. For mention-level pair-level relation extraction, recent work focused representation learning. This considered one major steps towards making progress artificial intelligence . Representations relations understand context particularly important biomedical research, identifying fruitful targets crucial due high costs experimentation. Learning representations likely require large amounts unsupervised data due scarcity labelled data domain. Recent mention-level methods based using large unsupervised models Transformer networks learn representations sentences containing pairs entities. These representations used inputs much smaller models, perform supervised relation classification . Recent pair-level methods based encoding mention pair entities, designing mechanism pool encodings single representation. This representation used classify relation entity pair . However, representation learning methods mention-level pair-level extraction typically use point estimate representation. As result, may struggle capture nature true, potentially complex relations pair entities. For example, Figure shows sentences two entity pairs demonstrate relation statements different, typically depending biological circumstances . Such nuanced relations difficult capture single point estimate. We hypothesise true underlying relation entity pair, relation multimodal . The sentences containing pair textual observations underlying relations. We therefore propose probabilistic model uses continuous latent variable represent true relation entity pair. The distribution sentence containing pair conditioned latent variable. In order able model complex relations entity pair, use infinite mixture distribution latent representation. Our model provides unified architecture learning representations relations entity pairs mention pair level. We show posterior distribution latent variable used mention-level relation classification. We also demonstrate prior distribution model used pair-level classification. On tasks, achieve results competitive strong baselines model fewer parameters significantly faster train. The code released \url{ https://github.com/BenevolentAI/RELVM} %. In paper, introduced novel task collaborative storytelling, humans AI agents work together make stories. We presented collaborative storytelling system tunes large-scale neural LM storytelling data uses sampling-and-ranking approach select human-preferred story continuations. Quantitative evaluation system found tuning ranking greatly contribute capability generate story continuations human evaluators prefer consider acceptable. Qualitative evaluation human evaluator preferences showed humans found tuned+ranked preferable tuned tuned preferable untuned terms engagingness, interestingness, humanness metrics, well overall story quality preferences. Finally, identified areas potential future work, including evaluation stories produced humans system, integration system intelligent agents robots avatars, improvement generated story continuation quality allowing genres moods targeted. The next two lines define bibliography style used, bibliography file. \clearpage If work appendix, place put it. \clearpage","     Extracting biomedical relations from large corpora of scientific documents is a challenging natural language processing task. Existing approaches usually focus on identifying a relation either in a single sentence  or across an entire corpus . In both cases, recent methods have achieved strong results by learning a point estimate to represent the relation; this is then used as the input to a relation classifier. However, the relation expressed in text between a pair of biomedical entities is often more complex than can be captured by a point estimate. To address this issue, we propose a latent variable model with an arbitrarily flexible distribution to represent the relation between an entity pair. Additionally, our model provides a unified architecture for both mention-level and pair-level relation extraction. We demonstrate that our model achieves results competitive with strong baselines for both tasks while having fewer parameters and being significantly faster to train. We make our code publicly available."
"Human communication inherently multi-modal nature. Our expressions tone voice augment verbal communication.\ This include vocal features like speaking rate, intonation visual features like facial expressions . Non-verbal communication important tasks involve higher level cognitive expressions like emotions , persuasiveness mental health analysis . We focus multi-modal approach emotion recognition humans fundamentally express emotions verbally using spoken words , well acoustic signals visual expressions . Getting large-scale labeled datasets emotion recognition challenging.\ Our primary motivation paper study effective utilization large unlabeled datasets improve performance multi-modal emotion recognition systems.\ The signals consider speech, visual information spoken text.\ Our motivation stems popular use pre-trained models natural language, speech visual understanding tasks circumvent data limitations.\ BERT popular model natural language understanding trained using self-supervision.\ Devlin et al. use masked language modeling task Wikipedia corpus pre-training.\ The model successfully fine-tuned improve performance several tasks like question answering general language understanding evaluation benchmarks . Self-supervised learning also successfully applied speech based applications.\ Schneider et al.\ use unsupervised pre-training speech data distinguishing audio sample future noise samples.\ Fine-tuning model shows state art results automatic speech recognition . Liu et al.\ show BERT-like pre-training approach applied speech.\ By predicting masked frames instead masked words, performance tasks like speaker recognition,\ sentiment recognition phoneme classification improved. For emotion recognition, Tseng et al.\ show text-based self-supervised training outperform state art models. The authors use language modeling task, involves predicting word given context, pre-train model.\ Another area work leveraged unlabeled data detection localization visual objects spoken words multi-modal input.\ Harwath et al.\ train audio-visual model image-audio retrieval task.\ The models trained learn joint audio-visual representation shared embedding space.\ This model learn recognize word categories sounds without explicit labels.\ Motivated success approaches, study similar methods applied multi-modal emotion recognition.\ To best knowledge, joint self-supervised training approach using text, audio visual inputs well explored emotion recognition. Multi-modal emotion recognition models well studied literature typically outperform uni-modal systems .\ These models need combine inputs varying sequence lengths.\ In video, sequence lengths audio visual frames differ length text tokens orders magnitude.\ There considerable prior work fusing multi-modal features. Liang et al.\ studied multiple fusion techniques multi-modal emotion recognition sentiment analysis.\ Their methods included early late fusion modalities, dynamic fusion graph based network.\ They showed graph fusion model outperforms methods.\ Early fusion graph fusion techniques require alignment various modalities.\ Late fusion performed without alignment, allow interaction features different modalities frame level.\ To overcome limitation,\ Tsai et al.\ introduce cross-modal transformer .\ It scales features using cross-modal attention.\ In process, modalities projected sequences equal lengths, eliminating need alignment.\ This architecture successfully applied problems like emotion recognition, sentiment analysis speech recognition .\ Recently, another transformer-based method combine multi-modal inputs introduced Rahman et al. , uses multi-modal adaptation gate. In paper, propose using pre-training scheme BERT, extend model uses audio, visual text inputs. We discuss relevance approach Section .\ The multi-modal representations learned pre-training fine-tuned emotion recognition.\ We evaluate efficacy pre-training approach.\ We also perform experiments understand importance modality CMU-MOSEI dataset provide case-studies interpret results. This paper organized follows.\ In Section describe model architecture self-supervised approach pre-training, along motivation self-supervised learning choose.\ In Section , discuss training setup data.\ We present results analysis Section conclude Section . We presented model learning representations pairs biomedical entities unlabelled text corpora. We use latent variable arbitrarily flexible distribution order able capture complex relations pair entities. The unified architecture used mention-level pair-level relation extraction. On tasks, achieve results competitive strong baselines. We also show significant computational gains terms number parameters training times. Our model presents many avenues future work. The results Table show model's performance improves size hidden states networks; suggests gains achievable simply providing model parameters. The model could scaled using hierarchy latent variables increase expressive power representations. Other directions include evaluating benefits representation explicitly captures uncertainty relations. For example, done assessing model less confident making predictions entity pairs occur frequently unlabelled corpus. Additionally, since model produce representation pair entities , could used link prediction setting score unseen entity pairs.","  Emotion recognition is a challenging task due to limited availability of in-the-wild labeled datasets.\ Self-supervised learning has shown improvements on tasks with limited labeled datasets in domains like speech and natural language.\ Models such as BERT learn to incorporate context in word embeddings, which translates to improved performance in downstream tasks like question answering.\ In this work, we extend self-supervised training to multi-modal applications.\ We learn multi-modal representations using a transformer trained on the masked language modeling task with audio, visual and text features.\ This model is fine-tuned on the downstream task of emotion recognition.\ Our results on the CMU-MOSEI dataset show that this pre-training technique can improve the emotion recognition performance by up to 3\% compared to the baseline."
"% A long desired goal AI systems play important collaborative role everyday lives. Currently, predominant approach visual question answering relies encoding image question black-box transformer encoder. These works carry complex computation behind scenes yield single token prediction output . Consequently, struggle provide intuitive human readable form justification consistent predictions. In addition, recent study demonstrated unsettling behaviours models: tend ignore important question terms, look wrong image regions, undesirably adhere superficial even potentially misleading statistical associations. To address insufficiency, reformulate VQA full answer generation task rather classification one, i.e. single token answer. The reformulated VQA task requires model generate full answer natural language justification. We find state-of-the-art model answers significant portion questions correctly wrong reasons. To learn correct problem solving process, We propose \modelabbrevname{} , transparent neural-symbolic reasoning framework solves problem step-by-step mimicking humans. A human would first \underline{l}ook image, \underline{r}ead question, \underline{t}hink multi-hop visual reasoning, finally \underline{a}nswer question. % Following intuition, \modelabbrevname{} deploys four neural modules, mimicking one problem solving step humans would take: % A scene graph generation module first converts image scene graph; A semantic parsing module parses question multiple reasoning instructions; A neural execution module interprets reason instructions one time traversing scene graph recurrent manner and; A natural language generation module generates full answer containing natural language explanations. The four modules connected hidden states rather explicit outputs. Therefore, whole framework trained end-to-end, pixels answers. In addition, since \modelabbrevname{} also produces human-readable output individual modules testing, easily locate error checking modular output. % % Our experiments GQA dataset show \modelabbrevname{} outperforms state-of-the-art model large margin full answer generation task. Our perturbation analyses removing relation linguistic cues questions confirm \modelabbrevname{} makes step towards truly understanding question rather smart guess superficial data correlations. % We discuss related work Appendix A. To summarize, main contributions paper three-fold: % In paper, present state art results emotion recognition task using cross-modal transformer CMU-MOSEI dataset.\ We utilize BERT-like pre-training scheme using audio, visual text inputs.\ We use VoxCeleb2 dataset pre-train model fine-tune emotion recognition task.\ We demonstrate 3\ improvement baseline fine-tuned model. We presented subjective analysis contribution various modalities emotion recognition.\ We also show results missing input modalities understand importance modality emotion recognition task. For future work, propose initialize text encoder text-only model like BERT, multi-modal self-supervised training.\ VoxCeleb2 dataset, although large terms number hours video, smaller compared Wikipedia corpus billions words. Taking advantage larger text-only corpus could provide improvements.\ We would also like experiment adapting model CMU-MOSEI dataset.\ Both VoxCeleb2 CMU-MOSEI datasets obtained YouTube, could domain mismatch two datasets. Adapting could help bridge mismatch.\ We would also like explore weak labels adapt pre-trained representations downstream task.\ Tseng et al.\ showed weakly supervised labels used effectively bias embeddings learned pre-trained model. Even though study impact ASR errors emotion recognition, know errors impact self-supervised training. We would like study future. As noted before, model architecture allow ablation text. For future work, focus overcoming limitation.","   The predominant approach to visual question answering  relies on encoding the image and question with a ``black-box'' neural encoder and decoding a single token as the answer like ``yes'' or ``no''. Despite this approach's strong quantitative results, it struggles to come up with intuitive, human-readable forms of justification for the prediction process. To address this insufficiency, we reformulate VQA as a full answer generation task, which requires the model to justify its predictions in natural language. We propose LRTA [Look, Read, Think, Answer], a transparent neural-symbolic reasoning framework for visual question answering that solves the problem step-by-step like humans and provides human-readable form of justification at each step. Specifically, LRTA learns to first convert an image into a scene graph and parse a question into multiple reasoning instructions. It then executes the reasoning instructions one at a time by traversing the scene graph using a recurrent neural-symbolic execution module. Finally, it generates a full answer to the given question with natural language justifications. Our experiments on GQA dataset show that LRTA outperforms the state-of-the-art model by a large margin  on the full answer generation task. We also create a perturbed GQA test set by removing linguistic cues  in the questions for analyzing whether a model is having a smart guess with superficial data correlations. We show that LRTA makes a step towards truly understanding the question while the state-of-the-art model tends to learn superficial correlations from the training data."
"Duplicate question detection important application information retrieval NLP . It allows systems recognize two questions share answer. This significant community forums, StackExchange\footnote{https://stackexchange.com/} increase effectiveness avoiding redundant questions displaying relevant answers search questions. It also important FAQ retrieval question answering systems . To learn DQD models \stackexchange{}, question pairs usually annotated duplication information extracted community-provided meta-data. Such annotations sparse domains, e.g., new \stackexchange{} forum providing support new product. Therefore, leveraging training signals either unsupervised data supervised data domains important . Pre-trained language models like BERT RoBERTA great unsupervised textual representations. Several recent efforts adapt PLMs domains interest self-supervised fine-tuning unsupervised domain data, shown promising several scenarios . We follow tune BERT \stackexchange{} domains obtain richer representations task DQD. Recently, -nearest neighbors applied PLM representations language modeling dialogue . We extend line study apply \cdknn{} cross-domain generalization DQD, models trained data source domain, applied data target domain. To so, represent pairs source target common representation space score target pairs using nearest neighbors source pairs. \figref{knnprocess} shows illustration procedure. % The specific properties \stackexchange{} DQD % important make approach effective. Our study AskUbuntu target source datasets , include several domains \stackexchange{} also Quora Sprint, reveals \cdknn{} effective compared cross-entropy classification pair representation space PLMs rich target domain, i.e., adapted unsupervised data target similar domains; source target domains large distributional shifts. We make following contributions: We present first study combining strengths \cdknn{} neural representations cross-domain generalization sentence matching task, i.e., DQD. Our experimental results cross-domain DQD demonstrate \cdknn{} rich question-pair representations advances results cross-entropy classification, especially shifts source target domains substantial. We present \modelabbrevname{}, transparent neural-symbolic reasoning framework visual question answering, incorporates [look, read, think answer] steps provide human-readable form justification step. The modular design methodology enables whole framework trainable end-to-end. Our experiments GQA dataset show \modelabbrevname{} achieves high accuracy full answer generation task, outperforming state-of-the-art LXMERT results noticeable 15\ absolute margin. In addition, \modelabbrevname{} performance drops significantly LXMERT, object attributes relationships masked, hence indicating \modelabbrevname{} makes step forward, towards truly understanding question, rather making smart guess based superficial data correlations. In validation study, shown provided oracle scene graph, \modelabbrevname{} able achieve high accuracy short answers full answers , nearing theoretical bound 96\ short answers. These observations indicate better scene graph prediction methods offer great potential improving \modelabbrevname{} performance short-answer full-answer tasks.","  Duplicate question detection  is important to increase efficiency of  community and automatic question answering systems.  Unfortunately, gathering supervised data in a domain is time-consuming and expensive, and our ability to leverage annotations across domains is minimal.  In this work, we leverage neural representations and study nearest neighbors for  cross-domain generalization in DQD.   We first encode question pairs of the source and target domain in a rich representation space and then using a k-nearest neighbour retrieval-based method, we aggregate the neighbors' labels and distances to rank pairs. We observe robust performance of this method in different cross-domain scenarios of StackExchange, Spring and Quora datasets, outperforming cross-entropy classification in multiple cases. We will release our codes as part of the publication. % ervised adaptation to StackExchange domains by self-supervised finetuning of contextualized embedding models like BERt. %We show the effectiveness of this adaptation in scenarios when source domain comes from different types of distributions. %Our analysis also reveals that unsupervised domain adaptation on even small amounts of data boosts the performance significantly. %Further, we show how an approach based on nearest neighbors is effective  for this problem and outperforms training the full model using cross entropy."
"Learning vocabulary major component foreign language learning. In school context, initially vocabulary learning typically organized around words introduced text book. In addition incrementally growing vocabulary lists, textbooks also provide thematically organized word banks. When texts read, publisher teacher often provides annotations new vocabulary items appear text. A wide range digital tools developed support vocabulary learning, digital versions file cards digital text editions offering annotations. While applications serve needs formal learning setting initial foreign language learning phase, texts read primarily chosen systematically introduce language, later selection texts read principle follow individual interests student adult, boosts motivation engage book. Linking language learning functional goal someone actually wants achieve using language line idea Task-Based Language Teaching prominent strand foreign language education . Naturally, authentic texts accessible every learner, linguistically-aware search engines, FLAIR , make possible identify authentic texts right reading level rich language constructions next curriculum. Where unknown vocabulary reader encounters setting goes beyond around 2\% unknown words text present without substantial loss comprehension , many digital reading environments provide option look word dictionary. Yet, frequently looking words context cumbersome distracts reader world book trying engage with. Relatedly, one key criteria TBLT learners rely resources complete task . But naturally require pre-task activities preparing learner able successfully tackle task . But learner systematically prepare reading text book interested reading? In paper, explore computational linguistic methods distributional semantics, morphological clustering, exercise generation combined graph-based learner models answer question conceptually practice. On practical side, developed application supports vocabulary learning pre-task activity reading self-selected book. The conceptual goal automatically organize lexical semantic space given English book form graph makes possible sequence vocabulary learning way efficiently exploring space visualize graph users open learner model showing growing mastery book's lexical space. Lexical learning fostered monitored automatically generated multi-gap activities support learning revision words contexts occur book. In section discuss book text chosen learner turned graph encoding lexical space learner needs engage read book, words morphologically related word families automatically identified compactly represented graph . In section turn use graph representation lexical semantic space book determine reader's learning path represent growing lexical knowledge spreading activation graph. In section, conceptual ideas realized application. We discuss new learner cold-start problem avoided using quick word recognition task implemented, discussing content selection activity generation practice testing activities. Section provides conceptual evaluation approach compares related with, wrapping conclusion section. % learning rare words English purpose? And % relevance learning entire frequency bands words unclear % How combining goal reading book systematic % learning needed so? Problem: Individuals % interested different books, individual differ language % competence vocabulary knowledge. So vocabulary % books organizing individually adaptive organization % Goal: % % Solution: % In work, studied applying \cdknn{} DQD cross-domain generalization. We compared \cdknn{} cross-entropy classifier different question-pair representations available. Our results showed domain-adaptive pre-training target data gives rich representations, \cdknn{} robust distributional shifts compared classification question pairs encoded rich representations. We plan extend study tasks understand better strengths memorization learning robust models rich PLM embeddings utilized represent examples. We believe concurrently promising results findings presented study could benefit NLP research explore direction more.","   How can a learner systematically prepare for reading a book they are   interested in? In this paper, we explore how computational   linguistic methods such as distributional semantics, morphological   clustering, and exercise generation can be combined with graph-based   learner models to answer this question both conceptually and in   practice. Based on the highly structured learner model and concepts   from network analysis, the learner is guided to efficiently explore   the targeted lexical space. They practice using multi-gap learning   activities generated from the book focused on words that are central   to the targeted lexical space. As such the approach offers a unique   combination of computational linguistic methods with concepts from   network analysis and the tutoring system domain to support learners   in achieving their individual, reading task-based learning goals."
"Speaking listening common ways humans convey understand daily conversations. Nowadays, speech interface also widely integrated many applications/devices like Siri, Google Assistant, Alexa . These applications use speech recognition-based approaches understand spoken user queries. Like speech, text also widely used medium people converse. Recent advances language modeling representation learning using deep learning approaches proven promising understanding actual meanings textual data, capturing semantical, syntactical, contextual relationships textual words corresponding learned fixed-size vector representations. Such computational language modeling difficult case speech spoken language understanding unlike textual words, spoken words different meanings word spoken different tones/expressions , difficult identify sub-word units speech variable-length spacing overlapping spoke-words , use stress/emphasis syllables multi-syllabic word increase variability speech production . Although textual word representations capture semantical, syntactical, contextual properties, fail capture tone/expression. Using speech/audio data training spoken-word representations results semantically syntactically poor representations. So paper, propose novel spoken-word representation learning approach called STEPs-RL uses speech text entanglement learning phonetically sound spoken-word representations, captures acoustic contextual features also semantically, syntactically, phonetically sound. STEPs-RL trained supervised manner learned representations capture phonetic structure spoken-words along inter-word semantic, syntactic, contextual relationships. We validated proposed model evaluating semantical syntactical relationships learned spoken-word representations four widely used word similarity benchmark datasets, comparing performance textual word representations learned Word2Vec \& FastTexT , investigating phonetical soundness generated vector space. In paper, discussed methodological basis realization tool allowing learner systematically learn lexical material needed able read book interested in. Automatically structuring lexical space sequencing learning achieved distributional semantic methods, automatic identification word families, concepts network analysis. The graph-based domain model automatically derived given book serves foundation learner model supporting selection efficient learning path lexical space acquired. Multi-gap activities automatically generated targeted book used practice testing activities. The application also well suited dedicated vocabulary learning application indicated earlier. The teachers guide students master vocabulary books renowned authors also exposed intriguing language usage. In addition self-guided learning people interested reading specific books, may particularly useful context so-called intensive reading programs, approach particularly well-suited English Specific Purposes context, language particular content domain direct importance. Given kind integration language content learning, similar affinity exists so-called Content Language Integrated Learning . listhe thely auisd basis aab limitation mention point option overcoming it: This application also upgraded learn domain knowledge. Since distribution semantic space defined pre-trained vector space model. Some domain specific proper nouns missing. This could overcame training custom vector space chosen text. This leverage application facilitate domain knowledge learning/revising like jargon, scientific names, geographical names etc... Additional supporting materials could explored scaffold learning apart usage chose text, dictionary reference translation word learner's native language used currently. Though learn model pruned improve visualisation. The connectivity potentially overwhelming. There could considerable improvement reporting global local progress structured space. Or simplified approach visual thesaurus could adopted. This application provides lot scope gamification exploratory objective vocabulary space provided graph based framework maximise coverage. Which could themed around goal reaching/trained actual task engaging activities.","   In this paper, we present a novel multi-modal deep neural network architecture that uses speech and text entanglement for learning phonetically sound spoken-word representations. STEPs-RL is trained in a supervised manner to predict the phonetic sequence of a target spoken-word using its contextual spoken word's speech and text, such that the model encodes its meaningful latent representations. Unlike existing work, we have used text along with speech for auditory representation learning to capture semantical and syntactical information along with the acoustic and temporal information. The latent representations produced by our model were not only able to predict the target phonetic sequences with an accuracy of 89.47\% but were also able to achieve competitive results to textual word representation models, Word2Vec \& FastText , when evaluated on four widely used word similarity benchmark datasets. In addition, investigation of the generated vector space also demonstrated the capability of the proposed model to capture the phonetic structure of the spoken-words. To the best of our knowledge, none of the existing works use speech and text entanglement for learning spoken-word representation, which makes this work first of its kind."
"Recent decades brought increase use computer-based tools practically every field human endeavor. The field education exception. Such tools used augment even completely replace traditional face-to-face teaching methods. The emergence online learning platforms necessitated development means enable learning activities, group discussions, performed use technology. One example learning platform IMapBook software suite aimed increasing literacy reading comprehension skills elementary school-aged children use web-based eBooks, embedded games related contents, well moderated group discussions. Keeping discussions constructive relevant difficult usually requires discussion moderator present times. This limit opportunities discussions take place. Leveraging methods insights fields artificial intelligence machine learning, attempt develop systems automatically classify messages different categories detect discussion veered course necessitates intervention. Our research tackles problem using compilation discussions obtained pilot studies testing effectiveness using IMapBook software suite 4th-grade classrooms. The studies performed 8 different Slovene primary schools and, total, included 342 students. The discussions consist 3541 messages along annotations specifying relevance book discussion, type, category, broad category. The ID book discussed time posting also included, poster's school, cohort, user ID, username. Each message also manually translated English aid non-Slovene-speaking researchers. The use Slovene language presents unique challenges applying standard language processing methods, many readily available other, widely spoken languages. Given sequence one newly observed messages, want estimate relevance message actual topic discussion. Namely, want assign messages two categories  relevant book discussed not. Additionally, want predict whether message question, answer, statement call type message. Finally, want assign category label message possible labels either 'chatting', 'switching', 'discussion', 'moderating', 'identity'. Building predictive model capable performing predictions acceptable performance would allow us experiment including new level automation IMapBook software suite well related products. The research insights also applicable areas online user comments content moderation. In paper, introduced STEPs-RL learning phonetically sound spoken-word representations using speech text entanglement. Our approach achieved accuracy 89.47\ predicting phonetic sequences gender dialect speaker used auxiliary information. We also compared performance using different configurations observed performance proposed model improved increasing spoken word latent representation size, addition auxiliary information like gender dialect. We able validate capability learned representations capture semantical syntactical relationships spoken-words also able illustrate soundness phonetic structure generated vector space. For future work, plan extend model use attention mechanisms, improve performance using transformer-based architecture, experimenting larger datasets, using features MFCCs. \comment{"," The increasing adoption of technology to augment or even replace traditional face-to-face learning has led to the development of a myriad of tools and platforms aimed at engaging the students and facilitating the teacher's ability to present new information. The IMapBook project aims at improving the literacy and  reading comprehension skills of elementary school-aged children by presenting them with interactive  e-books and letting them take part in moderated book discussions. This study aims to develop and  illustrate a machine learning-based approach to message classification that could be used to  automatically notify the discussion moderator of a possible need for an intervention and also to collect other useful information about the ongoing discussion. We aim to predict whether a message posted in the discussion is relevant to the discussed book, whether the message is a statement, a question, or an answer, and in which broad category it can be classified. We incrementally enrich our used feature subsets and compare them using standard classification algorithms as well as the novel Feature stacking method.  We use standard classification performance metrics as well as the Bayesian correlated t-test to show  that the use of described methods in discussion moderation is feasible. Moving forward, we seek to  attain better performance by focusing on extracting more of the significant information found in the  strong temporal interdependence of the messages."
"The Winograd Schema Challenge\/ proposed means test whether machine human-like intelligence. It alternative well known Turing Test\/ designed motivation reducing certain problematic aspects affect TT. Specifically, TT subjective nature, WSC provides purely objective evaluation; whereas passing TT requires machine behave deceptive way, WSC takes form positive demonstration intelligent capability. The core problem WSC resolve reference pronouns occurring natural language sentences. To reduce possibility task accomplished procedures based superficial statistical characteristics, rather `understanding' sentence, specify test sentences used WSC, constructed pairs, similar structure differ key word phrase, correct referent pronoun different two cases. This sentence pair, together indication pronoun resolved pair two possible candidates, called Winograd Schema. The following example Winograd schemas original WSC273 data set : \item The trophy fit brown suitcase {\bf it} small\/. \end{enumerate} design Winograd schemas require background knowledge resolve pronoun, evidence thinking\/. Therefore, exclude sentences resolved statistical association within sentence. In paper, introduce keyword method define domains Winograd schemas. To best knowledge, first work use keywords defining domains WSC explore high-level patterns them. To use domain-specific high-level patterns, also develop advanced high-level knowledge-based reasoning method modifying method . Furthermore, suggest simple ensemble method combines knowledge-based reasoning machine learning. By experiments domain-specific data set, ensemble method gives better performance single method. Lastly, also propose `robust' accuracy measure objective improving switching method . The best results achieved using Feature stacking method model built complete feature subset. The results indicate performance sufficient methods used real-world tools platforms. A significant portion information needed correct classifications hidden strong temporal interdependence messages developed methods exploited marginally."," The Winograd Schema Challenge\/  is a common sense reasoning task that requires background knowledge. In this paper, we contribute to tackling WSC in four ways. Firstly, we suggest a keyword method to define a restricted domain where distinctive high-level semantic patterns can be found. A thanking domain was defined by keywords, and the data set in this domain is used in our experiments. Secondly, we develop a high-level knowledge-based reasoning method using semantic roles which is based on the method of \cite{sharma:2019}. Thirdly, we propose an ensemble method to combine knowledge-based reasoning and machine learning which shows the best performance in our experiments. As a machine learning method, we used Bidirectional Encoder Representations from Transformers  \citep{kocijan:2019}. Lastly, in terms of evaluation, we suggest a `robust' accuracy measurement by modifying that of \cite{trichelair:2018}. As with their switching method, we evaluate a model by considering its performance on trivial variants of each sentence in the test set."
"% overview + widespread applications Text classification, extensively applied fundamental cornerstone natural language processing applications, sentiment analysis, spam detection spoken dialogue systems, widely studied decades. In general, almost NLP tasks cast classification problems either document, sentence, word level. Here focusing means narrow sense, i.e., given sequence tokens arbitrary length, predicting likely categorization belongs to. % conventional approaches, CNN/LSTM pros, + cons: lack efficacy capture latent representations. Considerable compelling neural approaches text classification task empirically demonstrated remarkable behaviors recent years, orchestrate compose semantic syntactic representations texts central. Much work concentrated learning composition distributional word representations categorization, wherein plenty deep learning methods adopted, TextCNNs, RCNNs, recurrent neural networks , FastText, BERT, etc. Most learn word representations firstly projecting one-hot encoding token pretrained randomly initialized word embedding matrices acquire dense real-valued vectors, feed neural models classification. These methods, however, exploited low-dimensional semantic representations sample text supervised way. Some argued unsupervised latent representations topic cluster modeling mined latent variable models may benefit. maintained word clustering could deliver useful semantic information grouping words corpus thus promote classification accuracy. Moreover, incorporated neural topic models Variational Autoencoder classification tasks discover latent topics document level encode co-occurrence words bag-of-words statistics. Learning corpus-level representation administer enrichment globally informative features thus favorable task performance. There plenty works adopting VAE learning latent variables boost text classification performance. Nevertheless, remain problems cannot directly treat sampled latent space VAE clustering centroids since mechanism modulate representation different samples towards different mean variance better discrimination purpose Gaussian distribution assumption. alleviate issues minimizing distance learnable latent representation latent variable models clustering centers generated statistical clustering approaches. % trained with, projecting word indices dense word representations. Grounding this, design ad hoc Clustering-Enchanced neural model jointly learns distributional clustering alignment domain-aware clustering centroids word representations Euclidean hidden semantic space text classification, vector space assumption words similar meanings close other. Instead directly treating latent variables clustering centroids, employ co-adaptation strategy minimize difference hidden variables trainable clustering centroids initialized traditional clustering algorithms soft alignments. In present work, propose cluster-token alignment mechanism assigning relevance probability distribution clusters token, indicating likely tokens correlated cluster center. In clustering centroids co-regulated learned latent variables regarded domain- task-specific feature indicators. Our work illustrates jointly adapting clustering centroids learning cluster-token alignment holds promise advancing text classification performance incorporating clustering-aware representations. Our key contributions are: {} % graph GCN -> time cost building graphs % cluster explanation, importance, usage, application % inspiration % learn latent variables unsupervised approaches aid interaction multi-hop clusters word representations % contribution: % 1. unsupervised approaches learn maneuver cluster representation % 2. proposed cluster-token alignment mechanism assign word implied clusters % 3. methods outperform previous approaches eight different datasets short texts long texts. % % % % % \begin{figure*}[thb] % % We proposed two-way end-to-end bidirectional translation model, single, yet joint model based 2D grid. It permits sourcetarget targetsource decoding along axis, following joint training along axes. However, work-in-progress paper, work might needed prove effectiveness. On first attempt, experimental results show architecture able generate reasonably good translations source-to-target target-to-source. It yet reached parity tasks compared separate models multilingual model directions using language tags; however, offers different interesting modeling perspective. These first experiments using 2DLSTM cell bidirectional translation modeling, expect better results tuning. More work needs done, intend try tasks less reordering, translation related languages paraphrasing. Further exploration combination non-autoregressive approaches correct research direction. We also believe architecture motivates alignment model use bidirectional encoders source target sides align words. The traditional alignment models, like GIZA++ involve training models directions merging bidirectional alignments afterward. We believe two-way model combination attention mechanism appropriate candidate tasks allowed use bidirectional encoders. We also wish evaluate model multi-way setting, multi-dimensional LSTM cell utilized long complexity model computational power allow."," Distributional text clustering delivers semantically informative representations and captures the relevance between each word and semantic clustering centroids. We extend the neural text clustering approach to text classification tasks by inducing cluster centers via a latent variable model and interacting with distributional word embeddings, to enrich the representation of tokens and measure the relatedness between tokens and each learnable cluster centroid. The proposed method jointly learns word clustering centroids and clustering-token alignments, achieving the state of the art results on multiple benchmark datasets and proving that the proposed cluster-token alignment mechanism is indeed favorable to text classification. Notably, our qualitative analysis has conspicuously illustrated that text representations learned by the proposed model are in accord well with our intuition."
"Past work found variability speech signals often poorly modeled, despite recent advances speech representation learning using deep neural networks . An important source acoustic variability comes accent information embedded speech signals . Non-native accents frequently observed second language spoken, mainly caused first language background non-native speakers. The accent strength non-native speaker dependent amount transfer native language, generally influenced variety variables age second-language learning one valuable predictors . However, accent variability often overlooked modeling language, consequently high-resource languages English often treated homogeneous . That assumption problematic is, example, shown comparing number native non-native speakers English, latter group almost twice large former group . It therefore important accurately model pronunciation variation using representations speech allow variability incorporated. Traditionally, pronunciations often represented evaluated phonetically transcribing speech . However, transcribing speech using phonetic alphabet time consuming, labor intensive, interference transcriber variation might lead inconsistencies . Additionally, fine-grained pronunciation differences relevant studying accented speech may captured using set discrete symbols . \citet{acoustic-measure} therefore introduced acoustic-only measure comparing pronunciations. In method, represented accented speech 39-dimensional Mel-frequency cepstral coefficients , used compute acoustic-based non-native-likeness ratings non-native native speakers English. They found strong correlation automatically determined acoustic-based non-native-likeness ratings native-likeness ratings provided human raters . This result close to, still equal performance phonetic transcription-based approach . \citet{acoustic-measure} also conducted several small-scale experiments investigate whether fine-grained characteristics human speech captured compared phonetic transcription-based pronunciation difference measure. Their results showed acoustic-only measure captured segmental differences, intonational differences, durational differences, method invariant characteristics recording device. The quality MFCC representations known dependent presence additive noise . Recent work shown self-supervised representation learning models less affected noise, well-equipped model complex non-linear relationships . For example, models learn meaningful representations basis read English speech without direct supervision. Fine-tuning models using transcribed speech resulted representations resembled phonetic structure, offered significant improvements downstream speech recognition tasks . Consequently, paper, employ self-supervised neural models create automatically determined acoustic-only pronunciation difference measure, investigate whether results improved performance compared MFCC-based approach \citet{acoustic-measure} phonetic transcription-based approach \citet{wieling2014a}. In following, compare evaluate several neural models, namely , \citep[subsequently denoted ]{schneider2019wav2vec}, , \citep[subsequently denoted ]{baevski2019vq}, \citep[subsequently denoted ]{baevski2020wav2vec}. We evaluate performance algorithms using two different datasets. The first identical dataset used \citet{acoustic-measure} \citet{wieling2014a}. The second new dataset focuses accented speech single group non-native speakers human native-likeness judgements also available. For reproducibility, provide code via \url{https://github.com/Bartelds/neural-acoustic-distance}. The performance model assessed comparing obtained neural acoustic-only pronunciation differences phonetic transcription-based pronunciation distances, MFCC-based acoustic-only pronunciation distances, human perception. To understand aspects pronunciation variation neural models capture, conduct several additional small-scale experiments, line \citet{acoustic-measure}. We analysed adding explicit morphological information form embeddings POS tags morphological features two currently dominant neural network architectures used NLP: LSTM networks transformer-based BERT models. We compared models enhanced morphological information baselines three tasks . To obtain general conclusions, used subsets eight morphologically-rich languages different language families. The results indicate adding morphological information NER prediction models beneficial, improves performance NER DP tasks. For DP task, improvement depends quality morphological features. The additional morphological features consistently benefited LSTM-based models NER DP, high quality predicted . For BERT-based models, predicted features make practical difference NER DP task improve performance DP task high quality. Testing different variants BERT shows language specialised variants improve performance DP task additional morphological information beneficial, though less less shift multilingual towards monolingual models. The comparison different BERT variants indicates BERT models completely capture language morphology. Since release BERT, several new pre-training objectives proposed, syntactic semantic phrase masking~ span masking~. In work, makes sense apply models DP task order test well capture morphology. Further, effect morphological features could analysed additional tasks languages, since explicit morphological information seem benefit equally. \subsection*{Acknowledgements} This paper supported European Union Horizon 2020 Programme project EMBEDDIA . The research supported Slovene Research Agency research core funding no. P6-0411. The Titan X Pascal used part research donated NVIDIA Corporation."," Variation in speech is often represented and investigated using phonetic transcriptions, but transcribing speech is time-consuming and error prone. To create reliable representations of speech independent from phonetic transcriptions, we investigate the extraction of acoustic embeddings from several self-supervised neural models.  We use these representations to compute word-based pronunciation differences between non-native and native speakers of English, and evaluate these differences by comparing them with human native-likeness judgments.  We show that Transformer-based speech representations lead to significant performance gains over the use of phonetic transcriptions, and find that feature-based use of Transformer models is most effective with one or more middle layers instead of the final layer.  We also demonstrate that these neural speech representations not only capture segmental differences, but also intonational and durational differences that cannot be represented by a set of discrete symbols used in phonetic transcriptions."
"KR\&R systems work well certain knowledge-rich domains typically involve set axioms rules, use structured queries datasets, need precise logical inference explanations. Formal logic-based reasoning engines Cyc Ergo successfully deployed domains legal, healthcare finance. One main advantages using systems transparency  underlying reasoning system well-understood justified end-users. However, several known drawbacks logic-based approaches. For one, inference procedures highly brittle require precise matching/unification logical terms formulae order construct complete explanation. Secondly, traditional reasoners don deal uncertainty well , whereas rules real-world applications often probabilistic contextual. Thirdly, systems suffer knowledge acquisition problem . Often, rules hand-coded, approach doesn scale general. Our problem domain Natural Language Understanding , area issues mentioned come play  need acquire use implicit background knowledge understand text, application rules differently based context, use imperfect/fuzzy alignment concepts relations reasoning. To address issues, devise novel FOL-based reasoner, called Braid. Braid includes backward forward chainer, assumption based reasoner constraint solver. This paper refers backward chaining component, refer Braid-BC. Braid-BC supports rules confidences, uses notion custom unification functions dynamic rule generation overcome brittle matching knowledge-gap problem prevalent traditional reasoning engines. The custom-unifiers based statistical techniques, long propose score mappings terms two logical propositions . For example, use neural matching functions unifiers. Their purpose help reasoner find proofs even goals, rule conditions and/or facts align perfectly. The dynamic rule-generator given target proposition knowledge base input, outputs scored list hypothesized rules could used prove proposition. The purpose rule-generation connect dots knowledge required inference missing static KB. We describe two DRG implementations - one using neural rule generation model fine-tuned dataset crowd-sourced causal rules, known GLUCOSE , second uses rule-template based technique. We describe reasoning algorithms used Braid-BC, implementation distributed task-based framework builds proof/explanation graphs input query highly scalable manner. Our approach shares similarities RETE framework matching production rules makes several novel extensions: primarily backward chaining via heuristic best-first search , leverage Master-Worker architecture Master builds main proof graph Workers make local inferential updates, define general functions Unifiers Provers lets us plug various reasoning strategies combining standard reasoning statistical approaches . In work, investigated integration structural information constituent tree neural model Frame-semantic parsing. Constituent representations learned GCN, learn encoded representations syntactic constituents, trained specific task objective. used build constituency path features added every word representation sequence. Each word sequence enriched syntactic information summing constituent learned encodings path word task-specific node tree, e.g. target word predicate. We tested approach Frame-semantic parsing sub-tasks, namely Target Identification, Frame Identification, Semantic Role Labeling, showing features contribute mainly TI SRL tasks. Constituency path features applied Future work cover application proposed constituency path features sequence labelling based tasks, e.g. Named-Entity Recognition. Moreover, modifications GCNs tested framework, e.g. assess whether Attention-based GCN may learn refined constituent representations. Finally, representations may used node-classification approach, inspired seminal works , attempt move away well-used sequence labelling model recent years. \clearpage","  Traditional symbolic reasoning engines, while attractive for their precision and explicability, have a few major drawbacks: the use of brittle inference procedures that rely on exact matching  of logical terms, an inability to deal with uncertainty, and the need for a precompiled rule-base of knowledge . These issues are particularly severe for the Natural Language Understanding  task, where we often use implicit background knowledge to understand and reason about text, resort to fuzzy alignment of concepts and relations during reasoning, and constantly deal with ambiguity in representations.   To address these issues, we devise a novel FOL-based reasoner, called Braid, that supports probabilistic rules, and uses the notion of custom unification functions and dynamic rule generation to overcome the brittle matching and knowledge-gap problem prevalent in traditional reasoners. In this paper, we describe the reasoning algorithms used in Braid-BC , and their implementation in a distributed task-based framework that builds proof/explanation graphs for an input query in a scalable manner. We use a simple QA example from a children story to motivate Braid-BC design and explain how the various components work together to produce a coherent logical explanation."
"} {I}{n} past decade, seen emergence various Knowledge Graphs , YAGO DBPedia. They achieved great success academic industrial applications, ranging recommendation Question Answering. However, KGs far complete, limits benefits transferred knowledge. Relation Extraction vital step complete KGs extracting relations entities texts. It nontrivial since relation type may various textual expressions, meanwhile, different types relations also described words. Such ambiguity relations texts challenges supervision RE models. Due expensive human annotation cost, distant supervision proposed automatically annotate mappings sentences relations. It assumes two entities participate relation, a.k.a., triple express another relation . As shown Figure, given triple , collect two sentences include entity pair . Clearly, first sentence expresses similar meaning given relation type, second one implies another type relation city of, brings noise training corpora\footnote{As term relation refer either relation type relation instance , paper, simplify use term relation relation type unless otherwise stated.}. To highlight informative sentences, many existing works introduce attention mechanism assign sentences different learning weights. In terms quantity, hand, training data collected distant supervision concentrate mainly relations, leading issue lack sufficient annotations remaining relations. Take widely used dataset, New York Times , example, present number training instances relation Figure. Unsurprisingly, annotations long-tail concerning different relations, tail relations suffer insufficient training corpora. More specifically, relation refers multiple entity pairs smaller similar respect RE prediction distributions common textual contexts. Therefore, capture relation proximity precise general way remains challenging. Another major challenge distinguish different relations, case knowledge transfer introduces bias towards prediction proximate relations. For example, mentioned above, /location/us\_state/capital /location/fr\_region/capital indicate capital relation, difference two United States entities French entities. DPEN incorporates entity type information learn relation-specific classifier dynamically. However, entity type information sparse KGs , challenging scalability. To address first issue, propose learn relation prototypes capture proximity relationship among relations involved entity pairs. Inspired Prototypical Networks, represent relation prototype centroid training data, data point defined difference pair entity embeddings, namely implicit mutual relation . Given entity pair, compute implicit mutual relation distance relation prototype. These proximities suggest possible relations classifier, makes correct predictions extracting discriminative signals supportive sentences. Relation prototypes also enhanced prior information , applied arbitrary sentence encoder. To address second issue, enhance entity embeddings textual information implicit mutual relation learning. In specific, construct entity co-occurrence graph unlabeled texts modeling first-order second-order structural proximity. The massive textual contexts helpful infer entity types distinguishment. Besides, long-tail entity pairs also benefit additional textual information. We summarize main contributions follows: A preliminary version work published conference ICDE 2020. We summarize main changes follows: %The rest paper organized follows. In Section, formulate problem overview framework, Section introduces proposed method detail. We report promising experiment results real-world datasets Section. Section covers related works. Finally, conclude paper Section. In study, explored empirical study AL utilizing advantages uncertainty diversity selecting weighted diverse gradient embeddings perform sequence labeling task. We proposed efficient method empirically demonstrated could consistently achieve superior performance consuming much less data. It adds robustness dataset architecture, thus proving useful option solving real-world active learning problems","   Relation Extraction  is a vital step to complete Knowledge Graph  by extracting entity relations from texts. However, it usually suffers from the long-tail issue. The training data mainly concentrates on a few types of relations, leading to the lack of sufficient annotations for the remaining types of relations. In this paper, we propose a general approach to learn relation prototypes from unlabeled texts, to facilitate the long-tail relation extraction by transferring knowledge from the relation types with sufficient training data. We learn relation prototypes as an implicit factor between entities, which reflects the meanings of relations as well as their proximities for transfer learning. Specifically, we construct a co-occurrence graph from texts, and capture both first-order and second-order entity proximities for embedding learning. Based on this, we further optimize the distance from entity pairs to corresponding prototypes, which can be easily adapted to almost arbitrary RE frameworks. Thus, the learning of infrequent or even unseen relation types will benefit from semantically proximate relations through pairs of entities and large-scale textual information.      We have conducted extensive experiments on two publicly available datasets: New York Times and Google Distant Supervision. Compared with eight state-of-the-art baselines, our proposed model achieves significant improvements . Further results on long-tail relations demonstrate the effectiveness of the learned relation prototypes. We further conduct an ablation study to investigate the impacts of varying components, and apply it to four basic relation extraction models to verify the generalization ability. Finally, we analyze several example cases to give intuitive impressions as qualitative analysis. Our codes will be released later.    %Relation Extraction  is a paramount step to complete Knowledge Graph by extracting entity relations from texts. However, it usually suffers from the long-tail issue, as the training data mainly concentrates on a few types of relations, leading to the lack of sufficient annotations for the remaining types of relations. In this paper, we propose a general approach to learn relation prototypes from unlabeled texts, to facilitate the long-tail RE by transferring knowledge from those with sufficient data. We learn prototypes as an implicit factor between entities, to reflect the meanings of relations and their proximities. Specifically, we construct an entity co-occurrence graph from texts, and capture structural proximities for embedding learning. Furthermore, we optimize the distance from entity pairs to corresponding prototypes, which can be easily adapted to many RE framework. We have conducted extensive experiments on two publicly available datasets. Compared with eight state-of-the-art baselines, our model achieves significant improvements . Further results on long-tail relations demonstrate the effectiveness of the learned relation prototypes. We further conduct an ablation study to investigate the impacts of varying components and the generalization ability. Finally, we analyze several example cases to give intuitive impressions as qualitative analysis."
"% Understanding BERT works important. % presence blackbox nlp indication research community values ability understand internals deep neural networks. Pre-trained transformer models BERT currently ubiquitous within natural language processing research demonstrated improvements topics sentiment analysis semantic parsing . The widespread development use models led increased effort interpret models' decisions . % * understanding models important society % * BERT used over, important understand BERT As defined \citet{doshivelez2017rigorous}, model interpretability ``the ability [of model] explain present understandable terms human''. Intuitively, interpretable model easier understand, debug improve. % It's hard understand BERT % * neural model many, many parameters % * pre-training + fine-tuning newer training scratch -> read literature introductions/motivations Interpreting modern pre-trained transformer models difficult. First, modern deep learning models hundreds millions parameters, scale continues increase . Understanding impact single parameter nearly impossible models densely connected. Combined sheer number parameters, manual analysis infeasible. Secondly, pre-training fine-tuning required state-of-the-art performance, effort focused alternative pre-training methods . % Understanding impacts fine-tuning still well understood. \todo{do I need citation here?} % Previous work attempted use attention Previous work uses BERT's self-attention mechanism interpret model's predictions . However, body work shows models' attention mechanisms cannot interpreted single-sequence classification tasks. % We apply bert sequence classification task We apply BERT two BERT-based models existing sentence classification task proposed \citet{aesw}. We compare BERT-based models' performances previous baselines use methods presented \citet{vashishth2019attention} \citet{deyoung-etal-2020-eraser} evaluate BERT's interpretability single-sequence classification tasks. We find fine-tuning teach BERT recognize previously unknown patterns natural language BERT interpretable attention-based models analyzed \citet{jain-wallace-2019-attention} \citet{vashishth2019attention}. To summarize, key contributions paper are: % nice % * BERT applied % * professional data set, baseline, human-annotated % * marked spans edits % To best knowledge, BERT applied Automatic Evaluation Scientific Writing task. In conclusion, proposed general approach learn relation prototypes unlabeled texts. The prototype learning method applied current models better relation extraction transferring knowledge relations sufficient training data long-tail relations. We conducted extensive experiments verify effectiveness proposed method two publicly available datasets compared eight state-of-the-art baselines. The results present significant improvements, especially long-tail settings. Further ablation study case study also demonstrate effectiveness proposed method generalization ability current RE models quantitative qualitative perspectives. In future, interested enhancing entity embeddings KG including structure attribute information. investigating advanced entity embedding models, Graph Attention Networks , improve implicit mutual relation representation well relation prototypes. Also, side information incorporated enrich entity co-occurrence graph better modeling."," Pre-trained transformer language models such as BERT are ubiquitous in NLP research, leading to work on understanding how and why these models work. Attention mechanisms have been proposed as a means of interpretability with varying conclusions. We propose applying BERT-based models to a sequence classification task and using the data set's labeling schema to measure each model's interpretability. We find that classification performance scores do not always correlate with interpretability. Despite this, BERT's attention weights are interpretable for over 70\% of examples."
". % % final paper: en-us version % % space normally used marker % This work licensed Creative Commons % Attribution 4.0 International License. % License details: % \url{http://creativecommons.org/licenses/by/4.0/}. } Recently, neural machine translation demonstrated impressive performance improvements became de-facto standard . However, like neural methods, NMT data-hungry. This makes challenging train model low-resource scenarios . Researchers developed promising approaches low-resource NMT. Among data augmentation , transfer learning , pre-trained models . But approaches rely external data bi-text. To date, rare see work effective use bilingual data low-resource NMT. In general, way feeding samples plays important role training neural models. A good instance popular shuffle input data robust training state-of-the-art systems. More systematic studies issue found recent papers . For example, pointed deep neural networks tend prioritize learning ``easy'' samples first. This agrees idea curriculum learning easy-to-hard learning strategy yield better convergence training. In NMT, curriculum learning new. Several research groups applied large-scale translation tasks although discuss issue low-resource setup . The first question define ``difficulty'' training sample. Previous work resorts functions produce difficulty score training sample. This score used reorder samples training. But methods type enforce static scoring strategy somehow disagrees fact sample difficulty might changing model updated training. Another assumption behind curriculum learning difficulty sample fit competence model training. Researchers implicitly modeled issue hand-crafted curriculum schedules simple functions , whereas in-depth discussion yet. In paper, continue line research curriculum learning low-resource NMT. We propose dynamic curriculum learning method address problems discussed above. The novelty DCL two-fold. First, define difficulty sample decline loss . In way, measure hard sentence translated via real objective used training. Apart this, DCL method explicitly estimates model competence model updated, one select samples newly-updated model enough competence learn. DCL general applicable NMT system. In work, test Transformer-based system three low-resource MT benchmarks different sized data selected WMT'16 En-De task. Experimental results show system outperforms strong baselines several curriculum learning-based counterparts. future work applications might use edited versions negative cases seq2seq model? compare non-bert attention models? In paper, apply three BERT-based models sentence classification task, quantify interpretability small-scale manual study expanding larger-scale automated study. We find BERT's final attention layer clearly interpretable human annotators simple automated metrics. Future work might expand subset examples automatically annotated order understand BERT's interpretability different classes edits. Additionally, work needed understand impacts in-domain pre-training model interpretability.","      Large amounts of data has made neural machine translation  a big success in recent years.    But it is still a challenge if we train these models on small-scale corpora.   In this case, the way of using data appears to be more important.    Here, we investigate the effective use of training data for low-resource NMT.   In particular, we propose a dynamic curriculum learning  method to reorder training samples in training.   Unlike previous work, we do not use a static scoring function for reordering.   Instead, the order of training samples is dynamically determined in two ways - loss decline and model competence.   This eases training by highlighting easy samples that the current model has enough competence to learn.    We test our DCL method in a Transformer-based system.   Experimental results show that DCL outperforms several strong baselines on three low-resource machine translation benchmarks and different sized data of WMT'16 En-De."
"Searching code fragments common activity software development. The advent large code repositories like GitHub\footnote{https://github.com/} StackOverflow\footnote{https://stackoverflow.com/} increased number developers rely repositories search reuse existing code . Traditional Information Retrieval techniques work well code search retrieval tasks due limited shared vocabulary source code natural language search text . Often, developers new programming language, search code snippets context-free natural language. The choice words used search may overlap code snippets leading failure traditional information retrieval systems. Therefore, need gain deeper understanding code text order find semantically relevant code snippet. Consider example developer functional requirement validate age always lesser alert otherwise. The developer tasked enforce check Java. A naive Java developer familiar language might make query based requirement as: java check condition correctness. The top 10 results\footnote{As December 9, 2019} StackOverflow discuss assert keyword. A programming friendly query java boolean check assert keyword results code snippets demonstrating steps top result StackOverflow. Use deep neural network models shown tremendous improvements many tasks across domains including language tasks . This success largely attributed, part, ability learn meaningful relationships among words documents efficiently represent way semantically equivalent words tend similar representations . One family models popular determining text similarity Siamese networks. First introduced , typical Siamese network consists two identical sub networks share weights. They work tandem different inputs output networks evaluated distance measure also acts scoring function. This successfully applied many similarity tasks image domain recently text domain well . Another useful property models capability learn fewer data examples . Since code treated special kind text data, one possible way approach problem Semantic Code Search treat similarity task objective bring semantically equivalent code snippets natural language descriptions closer. Therefore, study application Siamese networks code corresponding text descriptions semantic code search. We apply multiple variations base Siamese network model two different datasets semantic code search study efficacy. We take state art baselines - datasets observe Siamese networks improve baseline results invariably . Finally, present analysis performance different Siamese network architectures explored identify conditions improved performance. The rest paper organized follows. We introduce relevant prior art section . Next, section , provide background Siamese networks semantic code search introduce terminology. In section , describe approach different architectures investigated. In section , describe experiments present results. Finally section , perform detailed analysis observations, followed conclusions section . % \tikz \draw[] rectangle node[pos=.2]{Answer Here:}; In paper, show using non-binary constituency trees beneficial, especially semantic similarity tasks. Moreover, highlight need powerful composition function exploit rich representation. To end, introduced new Tree-LSTM model leverages tensor canonical decomposition weight sharing process non-binary trees without adding new parameters. Such results pave way definition new tensor models leverage suitable tensor decomposition take advantage non-binary constituency trees. To end, next step would application tensor decompositions. Among others, tensor train decomposition seems promising define new composition functions sensitive child nodes order. Ultimately, would like test multiple tensor-based models different NLP tasks, studying relation bias introduced different tensor decomposition intrinsic property task."," % Availability of large code repositories and discussion forums, has enabled code search as a common activity among developers. They tend to express their intent as a query in natural language to find examples of related code. However performance of such systems are restricted due to 1) limited shared vocabulary across code and user query and 2) lack of semantic understanding of the user query.   % In this work, we evaluate Siamese network for the task of code retrieval. Building on two sub network, our siamese model can jointly learn between code and its description and represent them based on their semantic distance. We evaluate the performance of applying siamese networks 1) as a stand-alone model directly feeding code and its description 2) as a model stacked on existing state of the art models. We experiment on 2 datasets and 3 baseline models, and conclude that applying siamese networking on top of base models yield better embedding and improves the performance of the code sesearch taks significantly.  With the increase in the number of open repositories and discussion forums, the use of natural language for semantic code search has become increasingly common. The accuracy of the results returned by such systems, however, can be low due to 1) limited shared vocabulary between code and user query and 2) inadequate semantic understanding of user query and its relation to code syntax. Siamese networks are well suited to learning such joint relations between data, but have not been explored in the context of code search. In this work, we evaluate Siamese networks for this task by exploring multiple extraction network architectures. These networks independently process code and text descriptions before passing them to a Siamese network to learn embeddings in a common space. We experiment on two different datasets and discover that Siamese networks can act as strong regularizers on networks that extract rich information from code and text, which in turn helps achieve impressive performance on code search beating previous baselines on $2$ programming languages. We also analyze the embedding space of these networks and provide directions to fully leverage the power of Siamese networks for semantic code search."
"We motivated problem labelling dataset word sense disambiguation, want use limited budget collect annotations reasonable number examples sense word. This task thought active learning problem , two nonstandard challenges. First, given word get set candidate labels knowledge base WordNet . However, label set necessarily representative occurs data: may exist labels knowledge base occur corpus sense rare modern English; conversely, may also exist true labels exist knowledge base. For example, consider word ``bass.'' It frequently used noun modifier, e.g., ``the bass alto good singers'', ``I play bass guitar''. It also commonly used refer type fish, music widely discussed online, fish sense word orders magnitude less common low-frequency sound sense internet text. The Oxford dictionary also notes bass referred fibrous material used matting chords, sense common modern English. We want method collects balanced labels common senses, ``bass frequencies'' ``bass fish'', ignores sufficiently rare senses, ``fibrous material''. Second, empirical distribution true labels may exhibit extreme skew: word sense usage often power-law distributed frequent senses occurring orders magnitudes often rare senses. When considered individually, neither constraints incompatible existing active learning approaches: incomplete label sets pose problem method relies classifier uncertainty exploration ; extreme skew label distributions studied guided learning framework wherein annotators asked explicitly search examples rare classes rather simply label examples presented system . But taken together, constraints make standard approaches impractical. Search-based ideas guided learning far sample efficient skewed label distribution, require mechanism annotators search examples correct label set undesirable ask annotators find examples actually occur corpus. Our approach follows. We introduce frequency threshold, , sense deemed ``sufficiently rare'' % ignored = p_y < \thresholdp_y\hat{p}_y$ using importance-weighted samples. Once found examples common classes, switch standard active learning methods find additional examples reduce classifier uncertainty. Overall, paper makes two key contributions. First, present Exemplar Guided Active Learning algorithm offers strong empirical performance extremely skewed label distributions leveraging exemplar embeddings. Second, identify stopping rule makes EGAL robust misspecified label sets prove robustness imposes logarithmic cost hypothetical approach knows correct label set. Beyond key contributions, also present new Reddit word sense disambiguation dataset, designed evaluate active learning methods highly skewed label distributions. In section, analyze results obtained understand behavior DCS-Siamese network. We focus architecture since outperforms architectures baseline models considered. Specifically, would like analyze three observations: A. Regularization effect DCS-Siamese model original DCS architecture We visualize embeddings learnt DCS network output DCS extraction network, using Siamese network, text descriptions StaQC SQL dataset using t-SNE Figure . We consider SQL dataset visualization since raw queries code snippets available Java dataset. A quick examination reveals embedding space sharp, distinct clusters DCS-Siamese network , whereas clusters original DCS network relatively smaller scattered. Further, manually examined clusters evaluated questions mapped clusters. Few samples listed Table . For query groups DATE JOIN, clusters scattered different regions original DCS network. Also, cluster corresponding MAX query group adjacent DATE cluster. Comparatively, query groups, clusters DCS-Siamese network well separated coherent. This highlights role Siamese network regularizer applied top DCS network. The Siamese network seemingly helps rearranging embedding space leading meaningful representations, bringing similar inputs closer embedding space. This effect reflected better retrieval MRR DCS-Siamese network. We observe result datasets. In experiments, difference results variesd observe clear trend favor . To understand superior performance , visualize embeddings learnt DCS-Siamese network DCS layer two architectures respectively shown Figure . We consider SQL dataset visualization since raw queries code snippets available Java dataset. We use tSNE plot DCS embeddings . A quick examination reveals embedding space sharp, distinct clusters , whereas clusters relatively diffused. Further, manually examined clusters evaluated questions mapped clusters. Some samples listed Table . Apart fact clusters right figure smaller four sets queries examined, ones left blend points figure, implying network done poor job learning distinguish different queries DCS layer . Its unsurprising achieve better MRR Code retrieval performed DCS layer . This hints possibility narrow funnel network caused output units top Siamese network act regualarizer forces lower layers learn meaningful embeddings, turn helps overall task using embeddings retrieval. We observe 1 exception evaluated Siamese layer output . However, difference performance marginal. This visualization, coupled results Table clearly establishes value DCS-Siamese network. We also observed inverse regularization effect values model performance gradually degrades value increases. This also explains clusters corresponding given set similar queries extremely well defined embeddings DCS layer embeddings top layer network. The restriction compact information 2 dimensions led loss information 2-d embedding space given set queries, led DCS layer learn rich set embeddings. figure show pitch scatter plot correct-wrong pairs points, needed If argument indeed holds, would see gradual loss information look embeddings intermediate layers network, upto final layer. We visualize embeddings layer DCS output final layer using tSNE set queries table X. Indeed, see cluster representing queries embedding space intermediate layer somewhat scattered, much final layer. B. For DCS-Siamese network, retrieval output DCS layer achieves higher MRR We focus actual embeddings learnt different layers DCS-Siamese network . Figure shows embedding plots final Siamese layer output DCS layer . We focus two specific set queries shown Table . We believe output units results much stronger regularization effect leading two sets questions mapped well-defined regions embedding space. As discussed earlier, regularization effect output units results much better embedding DCS layer resulting sets questions mapped well-defined regions embedding space. However, due low dimensionality final layer, tremendous loss information deprives layer meaning representations. The purpose representations simply reduce loss function guide gradient forcing lower layers learn much meaningful embedding. The actual meaning representations code text hence obtained lower layer. This effect also consistent embeddings code, observed Figure , generated plot embeddings SQL queries corresponding questions Table . In comparison, DCS layer embedding DSC-Siamese network , observed one clusters sorting questions . This due fact question involves deletion would more-or-less always translated 'DELETE FROM' clause code . However, could several questions might explicitly ask sorting, still require sorting intermediate step answer. The code corresponding answers would 'ORDER BY' clause, depending actual question, might involve SQL clauses. To summarize, far stronger regularizing effect network compared larger values. However, due effect, seems loss information final layers Siamese network hence, embeddings learned lower layers network contain richer information Code retrieval task. C. The DCS extraction network greatly outperforms extraction networks combined Siamese networks We selected DCS setup extraction network leveraged variety features code. Although believe features collectively responsible impressive performance DCS-Siamese model, considered individually Siamese network, unable provide enough information training, leading extremely poor model. This hints possibility providing code input deep learning network may straightforward although DCS features worked well, possibly features need discovered. Also, code features might useful certain datasets, all. hav explained select find/get/fetch far apart, give solution fix This result surprising since [togther stand paper] reported impressive results using siamese networks simple preprocessing conv-pooling-relu-FC network. However, applied code retrieval, siamese networks different model architectures embeddings sizes perform well models [dcs][coacor] rely extracting multiple features code. We hypothesize due different vocabularies well different mearnings terms code text. For instance, question answer pairs [together stanbd paper] come language , even though distributions terms questions answers might different. actually, need think come convincing experiemnts results Using ablation study, identify API sequence tokens provide highest performance features used DCS model also mention training/model details simple models. \section{Conclusion Future Work} Siamese networks achieve impressive performance Code Retrieval tasks learning meaningful embedding code description text. This performance heavily reliant appropriate representation code observed DCS architecture achieve reasonably well. However, understanding regularization provided Siamese network, would like study effect detail future work. We would also like validate observations datasets tasks involving code natural language text Code Summarization Code Synthesis. File acl2020.tex Based style files ACL 2020, Based style files ACL 2018, NAACL 2018/19, Based style files ACL-2015, improvements taken NAACL-2016 style Based style files ACL-2014, were, turn, based ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009, EACL-2009, IJCNLP-2008... Based style files EACL 2006 e.agirre@ehu.es Sergi.Balari@uab.es ACL 08 Joakim Nivre Noah Smith \documentclass[11pt,a4paper]{article} \usepackage[hyperref]{acl2020} \usepackage{times} \usepackage{latexsym} \usepackage{tikz} \usepackage{multirow} \renewcommand{\UrlFont}{\ttfamily\small} This strictly necessary, may commented out, improve layout manuscript, typically save space. \usepackage{microtype} \aclfinalcopy Uncomment line final submission \def\aclpaperid{***} Enter acl Paper ID \setlength\titlebox{5cm} You expand titlebox need extra space show authors. Please make titlebox smaller 5cm ; check camera-ready version ask change back. \newcommand\BibTeX{B\TeX} \title{Evaluation Siamese Networks Semantic Code Search} \author{} \author{Raunak Sinha \\ IBM Research \\ \texttt{rsinha05@in.ibm.com} \\\And Utkarsh Desai \\ IBM Research \\ \texttt{udesai26@in.ibm.com} \\\And Srikanth Tamilselvam \\ IBM Research \\ \texttt{srikanth.tamilselvam@in.ibm.com} \\\And Senthil Mani \date{}"," We consider the problem of wisely using a limited budget to label a small subset of a large unlabeled dataset. We are motivated by the NLP problem of word sense disambiguation. For any word, we have a set of candidate labels from a knowledge base, but the label set is not necessarily representative of what occurs in the data: there may exist labels in the knowledge base that very rarely occur in the corpus because the sense is rare in modern English; and conversely there may exist true labels that do not exist in our knowledge base. Our aim is to obtain a classifier that performs as well as possible on examples of each mmon class that occurs with frequency above a given threshold in the unlabeled set while annotating as few examples as possible from re classes whose labels occur with less than this frequency. The challenge is that we are not informed which labels are common and which are rare, and the true label distribution may exhibit extreme skew. We describe an active learning approach that  explicitly searches for rare classes by leveraging the contextual embedding spaces provided by modern language models, and  incorporates a stopping rule that ignores classes once we prove that they occur below our target threshold with high probability. We prove that our algorithm only costs logarithmically more than a hypothetical approach that knows all true label frequencies and show experimentally that incorporating automated search can significantly reduce the number of samples needed to reach target accuracy levels."
"Argumentation paramount process society, debating socially relevant topics requires high-quality relevant arguments. In work, deal problem argument search, also known argument retrieval. The goal develop \acrfull{arg_ret_sys} organizes arguments, previously extracted various sources , accessible form. Users formulate query access relevant arguments retrieved \acrshort{arg_ret_sys}. The query defined topic, e.g. Energy case \acrshort{arg_ret_sys} retrieves possible arguments without specification. Our work deals advanced case, query formulated form claim, user expects premises attacking supporting query claim. An example claim related topic Energy could ``We abandon Nuclear Energy"" supporting premise, e.g., ``Accidents caused Nuclear Energy longstanding negative impacts"". % A popular search methodology find relevant premises similarity search, representations retrieved premises similar representation query claim. However, noted by, relevance premise necessarily coincide pure text similarity. Therefore, authors advocate utilize similarity query claim claims \acrshort{arg_ret_sys} database retrieve premises assigned similar claims. However, \acrshort{arg_ret_sys} requires ground truth information premise claim assignments therefore limited applicability: Either information sources restricted sources information already available automatically inferred, expensive human annotations required. To mitigate problem keep original system's advantages, propose use machine learning model learn relevance premises claims. Using model, omit claim-claim matching step evaluate importance candidate premises directly query claim. Since relevance defined semantic level, design appropriate training task enable model learn semantic differences relevant non-relevant premises. Furthermore, essential subtask \acrshort{arg_ret_sys} ensure retrieved premises repeat ideas. Previous approaches employ clustering eliminate duplicates. However, clustering approaches often group data instances criteria expected users, also observed \gls{argument-mining} applications. For method, propose alternative clustering based idea core-sets, goal cover space relevant premises well possible. % This samplepaper.tex, sample chapter demonstrating % LLNCS macro package Springer Computer Science proceedings; % Version 2.20 2017/10/04 % \documentclass[runningheads]{llncs} % \usepackage{graphicx} \usepackage{xcolor} \usepackage{amsmath} \usepackage{amssymb} %\usepackage{ulem} \usepackage{multirow} \usepackage{booktabs} \usepackage{footnote} \makesavenoteenv{tabular} \makesavenoteenv{table} \usepackage{cite} \usepackage[ruled,vlined]{algorithm2e} \usepackage{float} \interfootnotelinepenalty=10000 % Used displaying sample figure. If possible, figure files % included EPS format. % % If use hyperref package, please uncomment following line % display URLs blue roman font according Springer's eBook style: \usepackage{hyperref} \renewcommand\UrlFont{\color{blue}\rmfamily} % equal contribution \makeatletter \newcommand{\printfnsymbol}[1]{% \textsuperscript{\@fnsymbol{#1}}% } \makeatother % \title{Diversity Aware Relevance Learning Argument Search} % %\titlerunning{Abbreviated paper title} % If paper title long running head, set % abbreviated paper title % \author{ Michael Fromm\thanks{equal contribution}\inst{1} %\orcidID{0000-0002-7244-4191} \and Max Berrendorf\printfnsymbol{1} \inst{1} %\orcidID{0000-0001-9724-4009} \and Sandra Obermeier \inst{1} \and Thomas Seidl \inst{1} %\orcidID{0000-0002-4861-1412} \and Evgeniy Faerman \inst{1} } \authorrunning{Fromm et al.} % First names abbreviated running head. % If two authors, 'et al.' used. \institute{Database Systems Data Mining, LMU Munich, Germany \\ \email{fromm@dbs.ifi.lmu.de}} \newcommand{\todo}[1]{\textcolor{red}{#1}} % Acronyms \usepackage[acronym]{glossaries} %\makeglossaries % Example % \newacronym{acrid}{ACR}{Acronym Clustering Representations} % \acrshort{arcrid} -> ACR % \acrlong{arcid} -> Acronym Clustering Representations % \acrfull{arcid} -> Acronym Clustering Representations \newacronym{argument-mining}{AM}{Argument Mining} \newacronym{arg_ret_sys}{ARS}{Argument Retrieval System} \newacronym{bert-based-premise-representation}{BERT}{BERT} \newacronym{claim-based-premise-representation}{CLAIM-SIM}{CLAIM-SIM} \newacronym{relevance-model}{relevance-model}{relevance model} % methods \newacronym{dumani-first512}{first512}{Dumani first512} \newacronym{dumani-sentences}{sentences}{Dumani sentences} \newacronym{dumani-sliding-window}{sliding}{Dumani sliding} \newacronym{bert-zero-shot-knn}{BERT Zero-Shot}{BERT Zero-Shot} \newacronym{learned-similarity-knn}{Learned Similarity}{Learned Similarity} \newacronym{biased-coreset}{Biased Coreset}{Biased Coreset} \newacronym{bert-zero-shot-clustered}{BERT Zero-Shot + Cluster}{} \DeclareMathOperator*{\argmax}{argmax} %\newcommand{\relevanceModel}{relevance model } % \newcommand{\dumaniFirst}{Dumani first512 } % \newcommand{\dumaniSentences}{Dumani sentences } % \newcommand{\dumaniSliding}{Dumani sliding } % \newcommand{\topSimilar}{Premise Similarity } % \newcommand{\topSimilarClusterRepresentatives}{Clustered Premise Similarity } % \newcommand{\mostImportant}{Premise Importance } % \newcommand{\BertNegatives}{Bert-Negatives } % \newcommand{\SimpleNegatives}{Simple-Negatives } % \newcommand{\SameTopicNegatives}{Same-Topic-Negatives } % disable hyperref glossaries \glsdisablehyper \keywords{Argument Similarity \and Argument Clustering \and Argument Retrieval} We present Exemplar Guided Active Learning algorithm leverages embedding spaces large scale language models drastically improve active learning algorithms skewed data. We support empirical results theory shows method robust mis-specified target classes give practical guidance usage. Beyond word-sense disambiguation, using EGAL collect multi-word expression data, shares extreme skew property."," In this work, we focus on retrieving relevant arguments for a query claim covering diverse aspects. State-of-the-art methods rely on explicit mappings between claims and premises and thus cannot utilize extensive available collections of premises without laborious and costly manual annotation. Their diversity approach relies on removing duplicates via clustering, which does not directly ensure that the selected premises cover all aspects. This work introduces a new multi-step approach for the argument retrieval problem. Rather than relying on ground-truth assignments, our approach employs a machine learning model to capture semantic relationships between arguments. Beyond that, it aims to cover diverse facets of the query instead of explicitly identifying duplicates.  Our empirical evaluation demonstrates that our approach leads to a significant improvement in the argument retrieval task, even though it requires fewer data than prior methods. Our code is available at \url{https://github.com/fromm-m/ecir2021-am-search}."
"Speaker diarization process partitioning audio stream homogeneous segments according speaker identities. Thus, diarization determines ``who spoke when'' multi-speaker environment, variety applications conversations involving multiple speakers, meetings, television shows, medical consultations, call center conversations. In particular, speaker boundaries produced diarization system used map transcripts generated multi-speaker automatic speech recognition system speaker-attributed transcripts . Moreover, speaker embeddings inferred diarization help ASR system adapt to, focus speech targeted speaker . Conventional speaker diarization systems based clustering speaker embeddings. In approach, several components integrated single system: speech segments determined voice activity detection ; speech segments divided smaller chunks fixed size; speaker embeddings extracted speaker embedding extractors chunk; finally, speaker embeddings clustered map segment speaker identity . For embeddings, i-vectors , x-vectors , d-vectors commonly used. Clustering methods typically used speaker diarization agglomerative hierarchical clustering , k-means clustering , spectral clustering . Recently, neural network-based clustering explored . Clustering-based speaker diarization achieves good performance several shortcomings. First, relies multiple modules trained separately. Therefore, clustering-based systems require careful joint calibration building process. Second, systems jointly optimized minimize diarization errors; clustering particular unsupervised process. Finally, clustering accommodate overlapping speech naturally, even though recent work proposed ways handle regions simultaneously active speakers clustering . End-to-end neural diarization self-attention one approaches aim model joint speech activity multiple speakers. It integrates voice activity overlap detection speaker tracking end-to-end fashion. Moreover, directly minimizes diarization errors demonstrated excellent diarization accuracy two-speaker telephone conversations. However, EEND originally formulated limited fixed number speakers output dimension neural network needs prespecified. Several methods proposed recently overcome limitations EEND. One approach uses speaker-wise chain rule decode speaker-specific speech activity iteratively conditioned previously estimated speech activities . Another approach proposes encoder/decoder-based attractor calculation . The embeddings multiple speakers accumulated time course audio input, disentangled one-by-one, speaker identity assignment speech frame. However, state-of-the-art EEND methods work offline manner, means complete recording must available diarization output generated. This makes application impractical settings potentially long multi-speaker recordings need processed incrementally . In study, propose novel method perform EEND blockwise online fashion speaker identities tracked low latency soon new audio arrives, without much degradation accuracy compared offline system. We utilize incremental Transformer encoder, attend left contexts ignore right contexts, thus enabling blockwise online processing. Furthermore, incremental Transformer encoder uses block-level recurrence hidden states carry information block block, reducing computation time attending previous blocks. To knowledge, first method uses incremental Transformer encoder block-level recurrence enable online speaker diarization. In work, presented novel approach retrieval relevant original premises query claims. Our new approach applied flexibly previous methods since require mappings premises claims database. Thus, also applied inductive setting, new premises used without need first associate relevant claims manually. At time, achieves better results approaches make use information."," We present a novel online end-to-end neural diarization system, BW-EDA-EEND, that processes data incrementally for a variable number of speakers. The system is based on the Encoder-Decoder-Attractor  architecture of Horiguchi et al., but utilizes the incremental Transformer encoder, attending only to its left contexts and using block-level recurrence in the hidden states to carry information from block to block, making the algorithm complexity linear in time. We propose two variants: For unlimited-latency BW-EDA-EEND, which processes inputs in linear time, we show only moderate degradation for up to two speakers using a context size of 10 seconds compared to offline EDA-EEND. With more than two speakers, the accuracy gap between online and offline grows, but the algorithm still outperforms a baseline offline clustering diarization system for one to four speakers with unlimited context size, and shows comparable accuracy with context size of 10 seconds. For limited-latency BW-EDA-EEND, which produces diarization outputs block-by-block as audio arrives, we show accuracy comparable to the offline clustering-based system."
"{M}{usic} composition human creative process requires wide range strong musical knowledge expertise create soothing music continues remain heart forever. Given vast majority music lovers limited availability professional music composers, strong need machines assist human creativity. Recent advancement software based music creation technology helped professional amateur music creators produce music great joy ease production masses consumed music consumers personal computers hand-held devices. %The software applications Ableton Live, FL Studio, Logic Pro X, Garageband examples changed way music produced past. Though exists plenty machine assistance create high quality music relative ease production, process songwriting automatically generating lyrics, composing melody corresponding generated lyrics synthesizing singing voice corresponding generated melody lyrics remained mutually exclusive tasks. Till date, construction novel/original songs limited individuals possess following skills: ability create lyrics, compose melody combine lyrics melody create rational, relevant soothing final complete songs. %Though remixing technology, create new music extent satisfies music lovers, need creating truly novel songs multiple constraints remaking existing works. % ------------------------------------------------------------------------------------------------------------- In literature, find considerable amount research work published automatic music generation . Early machine assisted music generation mostly based music theory expert domain knowledge create novel works. With advent data driven approaches exploded public music collections internet, data driven methods Hidden Markov models, graphic models deep learning models showed potential music creation. Though exists substantial amount research unconditional music generation, exists considerably less amount work done far generating melody lyrics given form text, call conditional melody/song generation lyrics. The primary reasons substantially less research conditional melody generation attributed i) non-availability direct source lyrics-melody pair dataset train data driven models, ii) lyrics composition multiple melodic representations, makes hard learn correlation lyrics melodies, iii) hard evaluate generated melodies objective measures. This paper focuses challenging aspect algorithmic songwriting process enables human community discover original lyrics, melodies suitable generated lyrics. To best knowledge, proposed AutoNLMC first attempt make whole process songwriting automatic using artificial neural networks. We also present lyrics vector model trained large dataset popular English songs obtain dense representation lyrics syllables, words sentence levels. The proposed AutoNLMC attention based encoder-decoder sequential recurrent neural network model consists lyric generator, lyric encoder melody decoders trained end-to-end. We train several encoder-decoder models various dense representations lyric tokens learn correlation lyrics corresponding melodies. Further, prove importance dense representation lyrics various qualitative quantitative measures. AutoNLMC designed way generate lyrics corresponding melodies automatically amateur person without music knowledge accepting small piece initial seed lyrics input. It also take lyrics professional lyrics writer generate matching meaningful melodies. In paper, propose joint enhancement speech transformer training method gated recurrent fusion robust end-to-end speech recognition. The joint training compositional scheme used simultaneously optimize enhancement speech recognition. In addition, order address speech distortion problem extract robust features end-to-end ASR, apply gated recurrent fusion algorithm combine noisy enhanced features. Experiments Mandarin AISHELL-1 demonstrate proposed method effective robust end-to-end ASR solve speech distortion problem well. In future, explore time domain speech enhancement acquire better enhanced speech obtain greater performance improvement proposed method. In paper, propose jointly traning enhancement speech transformer imporove robustness end-to-end systems. We use jointly compositional scheme enhancement recognition. In addition, order alleviate speech distortion problem extract robust features ASR, propose deep attention fusion algorithm combine noisy enhanced features. Experiments AISHELL-1 demonstrate effectiveness proposed method. In future, explore time domain speech enhancement acquire better enhanced speech obtain greater performance improvement proposed method."," In this paper, we propose a technique to address the most challenging aspect of algorithmic songwriting process, which enables the human community to discover original lyrics, and melodies suitable for the generated lyrics. The proposed songwriting system, Automatic Neural Lyrics and Melody Composition  is an attempt to make the whole process of songwriting automatic using artificial neural networks. Our lyric to vector  model trained on a large set of lyric-melody pairs dataset parsed at syllable, word and sentence levels are large scale embedding models enable us to train data driven model such as recurrent neural networks for popular English songs. AutoNLMC is a encoder-decoder sequential recurrent neural network model consisting of a lyric generator, a lyric encoder and melody decoder trained end-to-end. AutoNLMC is designed to generate both lyrics and corresponding melody automatically for an amateur or a person without music knowledge. It can also take lyrics from professional lyric writer to generate matching melodies. The qualitative and quantitative evaluation measures revealed that the proposed method is indeed capable of generating original lyrics and corresponding melody for composing new songs."
"Deep Neural Networks current state-of-the-art models many speech related tasks. From computational neuroscience perspective, DNNs seen rate coding based models, sense neuron responsive given stimulus, augment stimulus intensity, neuron output intensity also increase. Temporal coding based models try also take account information carried temporal structure stimulus. In case Spiking Neural Networks , spike timing delays spikes important order retrieve patterns spike sequences given input model. %https://en.wikipedia.org/wiki/Neural_coding There growing interest SNNs applied speech recognition tasks, isolated word phone recognition,to large-vocabulary automatic speech recognition recently. Reasons audio speech signal particularly suited event-driven models SNNs, SNNs also biologically realistic DNNs, hardware friendly energy efficient models, implemented dedicated energy-efficient neuromorphic chips. Furthermore, shown recently SNNs trained efficiently, supervised manner, using backpropagation surrogate gradient trick. This new approach allows train SNNs one would DNNs. In work, propose use supervised SNNs speech command recognition. We explore Leaky Integrate-and-Fire neuron model task, show convolutional SNNs reach accuracy close one obtained state-of-the-art DNNs, task. Our main contributions following: i) propose use dilated convolution spiking layers, ii) define new regularization term penalize averaged number spikes keep spiking neuron activity sparse possible, iii) show leaky variant neuron model outperforms non-leaky one , used in. In order facilitate reproducibility, code using PyTorch available online\footnote{https://github.com/romainzimmer/s2net}. In work, applied IRM toxicity classification task order demonstrate Domain Generalization serve important framework building fair machine learning classifiers. Our findings show IRM outperforms ERM respect generalization accuracy group fairness learning invariant likely non-causal predictors toxicity. We hope results first steps future explorations relationship robustness fairness machine learning. {\small }"," Deep Neural Networks  are the current state-of-the-art models in many speech related tasks. There is a growing interest, though, for more biologically realistic, hardware friendly and energy efficient models, named Spiking Neural Networks . Recently, it has been shown that SNNs can be trained efficiently, in a supervised manner, using backpropagation with a surrogate gradient trick. In this work, we report speech command  recognition experiments using supervised SNNs. We explored the Leaky-Integrate-Fire  neuron model for this task, and show that a model comprised of stacked dilated convolution spiking layers can reach an error rate very close to standard DNNs on the Google SC v1 dataset: \ER{94.5}\%, while keeping a very sparse spiking activity, below 5\%, thank to a new regularization term. We also show that modeling the leakage of the neuron membrane potential is useful, since the LIF model outperformed its non-leaky model counterpart significantly."
"Books one important mediums recording information imparting knowledge human history. Books classified different categories based physical formats, contents, languages, on. In paper, focus task book classification genre using information provided cover. Book covers usually first impression readers often convey important information content book. Figure presents sample book covers. The information provided cover includes visual textual information . For instance, Figure 1, background picture contains different food items cookware give readers visual impression book, texts shown cover states book ``authentic recipes Malaysia"". Both visual textual information shown cover together indicate genre ``Cookbooks, Food \& Wine"". It worth mention visual information often makes task extremely hard without textual information. For instance, Figure 1 , without reading texts cover, someone may classify book ``Cookbooks, Food \& Wine"" well solely based visual information get cover includes food items table dining room setting. Therefore, sometimes essential consider visual information textual information extracted cover conduct book genre classification. The automatic classification books based covers without human intervention would utterly beneficial many modern retrieval systems, considering complete digitization books extremely expensive task. The challenges task following. First, exists wide variety book genres, many concretely defined. Second, book covers, graphic designs, varies many different ways colors, styles, textual information, etc, even books genre. Third, book cover designs may vary due many external factors country, culture, target reader populations, etc . To overcome difficulties, present deep learning framework involving two moralities: one visual information textual information extracted covers. Recently, deep learning approaches reached high performances across wide variety problems . In particular, deep convolutional neural networks achieve satisfactory level performance many visual recognition categorization tasks, exceeding human performances. One attractive qualities techniques perform well without external hand-designed resources task-specific feature engineering. The theoretical foundations deep learning well rooted classical neural network literature. It involves many hidden neurons layers architectural advantage addition input output layers . A deep convolutional neural network universal, meaning used approximate continuous function arbitrary accuracy depth neural network large enough . The main contributions paper fourfold: The rest paper structured follows. Section 2 presents related works book cover classification. Section 3 elaborates details proposed multi-modal architectures. In section 4, discuss experimental results. The last section concludes paper discusses future work. In work, explored LIF neuron model define dilated convolution spiking layers spoken command recognition application. Contrarily works using SNNs applied speech tasks, special mechanisms, usually non-trainable, needed first encode speech input features type neural encoding first step use SNNs , approach unified sense first convolution layer applied real-valued speech features trainable shares definition implementation ones processing spike trains input. Our proposed SNN, trained back-propagation time surrogate gradient, achieved results competitive standard deep convolutional neural networks. We defined regularization term penalize averaged number spikes keep spiking neuron activity sparse possible, desirable property biological point view future potential implementation low-energy dedicated chips. Finally, conducted ablation studies order estimate impact different components approach. In particular, interesting result LIF neuron model outperformed simpler non-leaky one , used ASR. Another experiment showed learning values thresholds leak coefficients training bring accuracy improvements using defaults constant values. In future work, try confirm results acoustic modeling speech recognition. We also would like explore possibility design variant, layer sends output spikes next layer soon produced, single time loop used whole model. This would efficient terms computation load. It would also eventually allow take classification decisions faster, audio streaming applications particular. Below example insert images. Delete ``\vspace'' line, uncomment preceding line ``\centerline...'' replace ``imageX.ps'' suitable PostScript file name. ------------------------------------------------------------------------- To start new column help balance last-page column length use \vfill\pagebreak. ------------------------------------------------------------------------- \vfill \pagebreak"," Book covers are usually the very first impression to its readers and they often convey important information about the content of the book. Book genre classification based on its cover would be utterly beneficial to many modern retrieval systems, considering that the complete digitization of books is an extremely expensive task. At the same time, it is also an extremely challenging task due to the following reasons: First, there exists a wide variety of book genres, many of which are not concretely defined. Second, book covers, as graphic designs, vary in many different ways such as colors, styles, textual information, etc, even for books of the same genre. Third, book cover designs may vary due to many external factors such as country, culture, target reader populations, etc. With the growing competitiveness in the book industry, the book cover designers and typographers push the cover designs to its limit in the hope of attracting sales. The cover-based book classification systems become a particularly exciting research topic in recent years. In this paper, we propose a multi-modal deep learning framework to solve this problem. The contribution of this paper is four-fold. First, our method adds an extra modality by extracting texts automatically from the book covers. Second, image-based and text-based, state-of-the-art models are evaluated thoroughly for the task of book cover classification. Third, we develop an efficient  and salable multi-modal framework based on the images and texts shown on the covers only.  Fourth, a thorough analysis of the experimental results is given and future works to improve the performance is suggested. The results show that the multi-modal framework significantly outperforms the current state-of-the-art image-based models. However, more efforts and resources are needed for this classification task in order to reach a satisfactory level."
"\vskip 0.15in Despite recent developments activation functions Machine Learning -based classifiers, m-arcsinh~ shallow Multi-Layer Perceptron ~, usable, repeatable reproducible functions shallow deep neural networks, e.g., Convolutional Neural Network ~, remained limited confined three activation functions regarded 'gold standard'. These include Rectified Linear Unit , sigmoid function modified version, hyperbolic tangent sigmoid 'tanh'~, extends range [0, +1] [-1, +1]. The sigmoid tanh well-known vanishing gradient issues; thus, ReLU function devised scalable deep neural networks, despite 'dying ReLU' problem, recently solved by~. These made freely accessible open source Python library named 'Keras'~ Deep Learning. The availability functions public domain enabled not-for-profit for-profit organisations leverage several intelligence-based applications, academic industrial applications~~. \\ Nevertheless, considering above-mentioned challenges Computer Science ML communities, activation functions lack robustness classification tasks varying degrees complexity, e.g., slow lack convergence~ ~, caused trapping local minima~. Moreover, amongst three above-mentioned activation functions, ReLU applicable shallow deep neural networks, novel quantum variations found scalable traditional version recently~. \\ On side, sciences dealing study human behaviour, last 20 years, considerable progress made towards prevention mental health disorders~~. Specifically, professionals working field counselling psychology slightly enhanced ability grasping relational issues subjects via novel ML-based tele-monitoring technologies~. Nevertheless, technologies yet changed traditional counselling psychology practice, still based structured methodology adopted help individuals become self-aware, conscious needs moods~. The main goal counsellors pursue guiding individuals get know deeper level help discover resurface resources better manage emotions daily life. This process first requires tailored dialogue counsellor individual and, subsequently, leveraging practical tools aid individual experience understand inner self deeply~. Moreover, still limitations within counselling setting. For instance, individuals, fear, may reveal fundamental aspects persona would help counsellors guide better getting know themselves. Furthermore, many cases, subjects may express verbal language opposite non-verbal one. Counsellors often hardly understand dynamic patterns observed behaviours subjects, thus unable provide required help support them. \\ In counselling, neural network algorithms, shallow deep depending amount good-quality data hardware available, potential support counsellors image text classification tasks understand guide subjects helping infer subtle dynamic changes behaviours. Via careful effective observation images, micro- macro- body movements, facial expressions~~, possible better interpret understand subjects' non-verbal language. Even emotions underlying written content subjects may reveal inner aspects persona fundamental counsellors help resurface increase subjects' self-awareness related capability 'self-healing'~. \\ Therefore, theoretical practical standpoints, increasing need accurate reliable open source activation functions, reach convergence faster, avoiding trapping local minima, stable also used scale across shallow deep neural network algorithms image text classification. Entirely written Python made freely available TensorFlow~ Keras~, proposed hyperbolic function demonstrated competitive function respect gold standard functions, suits shallow deep neural networks, thus accurate reliable pattern recognition aid image text classification tasks. Thanks liberal license, widely distributed part free software Python libraries TensorFlow~ Keras~, available use academic research commercial purposes.\\ %%%%%%%%%%%%%%% Methods section %%%%%%%%%%%%%%%%%%%%% \vskip 0.3in In paper, proposed two multi-modal models: one simple concatenation DCCA concatenation, task book genre classification solely based cover. In addition, evaluated several state-of-the-art image-based models text-based models. By comparison, text-based models perform better general image-based models proposed multi-modal model simple concatenation outperforms models. Based results experiments, simple concatenation model top-1 accuracy 56.1\","%   <- trailing '%' for backward compatibility of .sty file This paper presents the 'hyper-sinh', a variation of the m-arcsinh activation function suitable for Deep Learning -based algorithms for supervised learning, such as Convolutional Neural Networks . hyper-sinh, developed in the open source Python libraries TensorFlow and Keras, is thus described and validated as an accurate and reliable activation function for both shallow and deep neural networks.  Improvements in accuracy and reliability in image and text classification tasks on five  benchmark data sets available from Keras are discussed.  Experimental results demonstrate the overall competitive classification performance of both shallow and deep neural networks, obtained via this novel function.  This function is evaluated with respect to gold standard activation functions, demonstrating its overall competitive accuracy and reliability for both image and text classification."
"In grounded language theory, semantics language given symbols connect underlying real world---the so-called ``symbol grounding problem''. For example, want robotic system sees eggplant ground recognition object canonical symbol `eggplant.' When user asks ""Please grab eggplant,"" robot ground natural language word ""eggplant"" symbol denotes relevant visual percepts. Once language vision successfully ground symbol, becomes feasible robot complete task. We learn connection using physical sensors conjunction language learning: paired language perceptual data used train joint model linguistic constructs apply perceivable world. Machine learning grounded language often demands large-scale natural language annotations things world, expensive impractical obtain. It feasible build dataset encompasses every object possible linguistic description. Novel environments require symbol grounding occur real time, based inputs human interactor. Learning meanings language unstructured communication people attractive approach, requires fast, accurate learning new concepts, people unlikely spend hours manually annotating even hundred samples, let alone thousands millions commonly required machine learning. % Active learning, system queries specific training data, potential improve learning efficiency reduce number labels required learn grounded language model. In work study active learning, system deliberately seeks information lead improved understanding less data, minimize number samples/human interactions required. The field active learning typically assumes pool unlabeled samples available, model request specific example would like obtain label for. By model select informative data points labeling, number samples need labeled reduced. This maps goal human-robot learning minimum training data provided human. Furthermore, active learning part pipeline few-shot learning methods. However, active learning magic bullet. When carefully applied, outperform sequential random sampling baselines. Thoughtful selection suitable approaches problems required. While active learning used language grounding %, , best knowledge, present first broad exploration best methods active learning grounding vision-language pairs. % In paper, focus developing guidelines active learning methods might appropriately selected applied vision-language grounding problems. We test different active learning approaches grounded language problems varying linguistic sensory complexity, use results drive discussion select active learning methods different grounded language data acquisition problems informed way. We consider grounded language task learning novel language previously unseen object types characteristics. Our emphasis determining methods reduce amount training data needed achieve performance consistent human evaluation. Primarily, address five relevant questions concerning characteristic-based grounded language learning: % We make conclusions respect questions \cref{sec:results}. % In addition addressing research questions, verify generalizable learning techniques beyond characteristic-based grounding. We find right ordering training data makes possible learn successfully significantly fewer descriptions cases, also active learning methodology chosen specific nature learning problem. Our main contribution principled analysis using active learning methods unsupervised data sampling techniques language grounding discussion aspects problems relevant approach selection. While contributions primarily analytic rather algorithmic, argue address critical need within grounded language understanding, active research area questions efficiency data collection widespread, potential support additional algorithmic developments. \vskip 0.15in As demonstrated competitive results obtained 5 data sets evaluated, especially Tables 1 3 deep neural network CNN Tables 4 5 shallow neural network FC-NN, hyper-sinh deemed suitable activation function scales shallow deep neural networks. \\ In fact, accuracy reliability high across sets benchmark image- text-based data sets, quantified via appropriate metrics sub-section 2.4, better gold standard functions, e.g., considering Table 1 accuracy F1-score CNN using hyper-sinh 0.70 0.69 respectively CIFAR-10 image-based data set, opposed CNN using sigmoid 0.10 0.02 respectively. Moreover, accuracy reliability comparable FC-NN using ReLU , higher reliability FC-NN leveraging sigmoid function 'Reuters' text-based data set . The proposed hyper-sinh also led increased precision 'IMDB' text-based data set opposed sigmoid tanh , using FC-NN leveraged classify 'Reuters' data set. \\ Therefore, hyper-sinh demonstrates possible extend m-arcsinh generalise across shallow deep neural networks image text classification tasks, mathematical formulation extended function complex all. As accurate reliable activation function, hyper-sinh thus deemed new gold standard activation function shallow deep neural networks, freely available TensorFlow Keras. Conclusion section \section{Conclusion} hyper-sinh proven accurate robust activation function shallow deep neural networks image text classification, thus new gold standard scales well FC-NN CNN. Since made freely available, open source, Python, TensorFlow Keras ecosystems, adds selection activation functions not-for-profit for-profit organisations tackling image text classification tasks data sets various sizes. Importantly, proposed algorithm, accurate reliable, written high-level programming language , leveraged part ML-based pipelines specific use cases, wherein high accuracy reliability need achieved, healthcare sector , small large clinics suitability shallow deep neural networks. Future work involves improving function reduce computational cost. Acknowledgements go end, appendices references \acks{This research receive specific grant funding agencies public, commercial, not-for-profit sectors.} Manual newpage inserted improve layout sample file - needed general appendices/bibliography."," % In grounded language acquisition, a physical agent uses language combined with high-frequency sensor data to learn a model of how language refers to the physical world. This approach, while powerful, often requires extensive data annotation, which can be difficult to obtain. This work  % Ordering the selection of training data using active learning can lead to improvements in learning efficiently from smaller corpora. We present an exploration of active learning approaches applied to three grounded language problems of varying complexity in order to analyze what methods are suitable for improving data efficiency in learning. We present a method for analyzing the complexity of data in this joint problem space, and report on how characteristics of the underlying task, along with design decisions such as feature selection and classification model, drive the results. We observe that representativeness, along with diversity, is crucial in selecting data samples."
"Deep neural networks powerful widely applied natural language processing. However, recent studies demonstrate models vulnerable adversarial examples, malicious inputs intentionally crafted fool models. % The introduction adversarial example ushered new era understand improve neural network-based models. % Adversarial attacks defenses attacks drawn significant attention recent years . Although generating adversarial examples texts proven challenging task images due discrete nature, number methods proposed generate adversarial text examples reveal vulnerability deep neural networks natural language processing tasks including reading comprehension , text classification , machine translation , dialogue systems , dependency parsing . These methods attack text examples replacing, scrambling, erasing characters words language units. To settle susceptible attack direction, require large number queries target model predictions given inputs. Thus adversarial examples typically generated specific model. This motivates main questions aim answer paper: Are universal adversarial examples fool almost every neural network-based model? And universal attack rules constructing universal adversarial examples? %are universal adversarial examples transfer neural network-based models? It well known adversarial examples exhibit black-box transferability, meaning adversarial examples generated one model fool another model . Transfer attackers launch white-box attacks local models find candidate adversarial examples may transfer target model. % In white-box setting, adversary access model's architecture, parameters input feature representations black-box one. % However, adversarial examples typically overfitted particular architecture feature representation source model, resulting sub-optimal black-box transfer attacks target models. However, factors affect transferability adversarial examples still unclear, especially NLP models. In study, quantitatively investigate adversarial transferability impacted several critical factors, including network architecture, input form, word embedding type, model capacity. Based understanding transferability among various neural models, study whether possible craft universal, model-agnostic text adversarial examples almost existing models. Universal adversarial examples least two advantages. First, adversaries need white-box access target models. They launch attacks models trained similar data, transfer across models . Second, universal adversarial examples useful analysis tool because, unlike typical attacks, model-agnostic. Thus, highlight general input-output patterns learned model. We leverage study influence dataset biases identify biases learned models. \end{center} \end{table*} In study, first systematically investigated critical factors neural models, including network architectures , input forms , embedding types , model capacities impact transferability text adversarial examples extensive experiments two datasets text classification. We vary one factor time fixing others see factor significant, found input form greatest influence adversarial transferability, following network architecture, embedding type, model capacity. Then, propose genetic algorithm find optimal ensemble minimum number members basis understanding adversarial transferability among neural models. The adversarial examples generated attacking ensemble found algorithm strongly transfer models, models, exhibit better transferability generated attacking models different random initialization. Finally, generalize adversarial examples constructed ensemble method universal semantics-preserving word replacement rules induce adversaries text input strongly transferring neural network-based NLP model . Since rules model-agnostic, provide analysis global model behavior, help us identify dataset biases diagnose heuristics learned models. In work, present thorough exploration different active learning approaches grounding unconstrained natural language real-world sensor data. We demonstrate active learning potential reduce amount data necessary ground language objects, active area research NLP robotics well machine learning sparse data generally. We additionally provide suggestions approach may suitable given perceptual linguistic complexity problem. Given analysis causes performance different algorithms cases, believe results prove generalize beyond relatively simple data seen here, making possible guidelines apply complicated language grounding tasks future."," Deep neural network models are vulnerable to adversarial attacks. In many cases, malicious inputs intentionally crafted for one model can fool another model in the black-box attack setting. However, there is a lack of systematic studies on the transferability of adversarial examples and how to generate universal adversarial examples.  In this paper, we systematically study the transferability of adversarial attacks for text classification models.  In particular, we conduct extensive experiments to investigate how various factors, such as network architecture, input format, word embedding, and model capacity, affect the transferability of adversarial attacks.  Based on these studies, we then propose universal black-box attack algorithms that can induce adversarial examples to attack almost all existing models. These universal adversarial examples reflect the defects of the learning process and the bias in the training dataset.  Finally, we generalize these adversarial examples into universal word replacement rules that can be used for model diagnostics.     \if0 It has been known that adversarial examples exhibit black box transfer, i.e. malicious inputs intentionally crafted for one model can also cause another model to make mistakes. However, which factors affect the most and how they impact the transferability of adversarial examples are still unclear, especially for NLP models.  Through extensive experiments, we systematically investigate how adversarial transferability is impacted with a few critical, model-specific factors, including the network architecture, input form, pre-trained word embedding, and model capacity. Based on the understanding of the adversarial transferability among neural models, we propose a population-based algorithm to find an optimal ensemble with minimum number of models, which can be used to generate adversarial examples that strongly transfer across other neural models.  We also generalize the adversarial examples generated by the ensemble method into universal word replacement rules that can induce adversaries on any text input to fool almost all the existing models with a much higher success rate. Those rules also help us to identify dataset biases and diagnose heuristics improperly learned by the models. \fi"
"Recent works shown NN models trained solely maximize prediction performance often vulnerable adversarial attacks . Even though several works proposed defend NN models attacks, focus NLP domain . Since many recent NLP models shown vulnerable adversarial attacks--e.g., fake news detection dialog system , investigation robust defense methods textual NN models become necessary. To defend adversarial texts, one use either adversarial detection model enhancement . Adversarial texts often generated replacing inserting critical words characters sentence, usually exhibiting grammatical errors. Hence, many detection methods focused recognizing correcting misspellings texts--e.g., ScRNN DISP . While misspelling-based methods model-independent require neither re-training modifying models, work well character-based attacks. In contrast, model enhancement approaches perform well character word-based attacks. %Such generalization variety attacks critical since one might know type adversarial techniques employed adversaries. Particularly, model enhancement methods enrich NN models training adversarial data augmented via known attack strategies adversarial training , external information knowledge graphs . Nevertheless, augmentations usually induce overhead costs training. Therefore, search defense algorithms directly enhance models' structures achieving higher extendability without acquiring additional data. %While developing solutions challenging still exploration . Fortunately, recent literature computer vision shows ensemble NNs achieve high adversarial robustness . In theory, directly extending single NN model ensemble multiple diverse sub-models, challenge adversaries attack one set different models . This makes attacks significantly difficult. However, applying idea computer vision NLP domain faces one main challenge. Current ensemble methods require simultaneous training several NN sub-models . This introduces impractical computational overhead training inference, especially one wants maximize prediction accuracy utilizing complex state-of-the-art sub-models BERT ROBERTA , 100M parameters. Furthermore, applying current ensemble defensive approaches directly enhance model's architecture large-scale NN model would usually require re-training everything scratch, may practical many settings. % Second, current ensemble approaches aim promote diversity sub-models either feature-level class-level . % Second, current ensemble approaches promote diversity sub-models maximizing differences among either prediction output vectors gradient vectors w.r.t input image . However, NLP tasks, classification particularly, much less labels computer vision, results much smaller degree-of-freedom directly regularizing differences prediciton probability vectors. % On hand, forcing sub-models focus different tokens input text directly regularizing gradient vectors straightforward text discrete nature. This easily resolved regularizing gradients w.r.t continuous word-embedding vectors sentence instead. However, since every sub-model contributes equally input, many overlaps among key features, i.e., words phrases, sub-models input text short. To address challenges, borrowing ideas Software Engineering, first introducing notion {\bf Neural Patching} improve adversarial robustness NN models ``patching"" parts models . Next, develop novel neural patching algorithm, {\mymethod}, patches last layer already deployed textual NN model diverse architectures transforms ensemble multi-experts enhanced adversarial robustness. By patching last layer model, {\mymethod} introduces lightweight computational overhead requires additional training data. %requires low construction overheads without compromising much computational complexity additional training data. Distinguished current ensemble methods, sub-model trained {\mymethod} specialized specific subset features input, i.e., expert feature-level , sub-set labels, i.e., expert class-level , also texts distinguished topic, i.e., expert instance-level, prediction. Such diversity feature-, class-, instance-level expertise makes challenging adversaries exploit multiple sub-models time. In summary, contributions paper follows: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% In study, investigated four critical factors NLP neural models, including network architectures, input forms, embedding types, model capacities impact transferability text adversarial examples different models. Based understanding transferability among models, proposed genetic algorithm find optimal ensemble models used generate adversarial examples transfer well models. We also described algorithm discover universal adversarial word replacement rules applied craft adversarial examples strong transferability across various neural models without access them. Finally, since adversarial examples model-agnostic, provide analysis global model behavior help identify dataset biases. {\small }"," Neural network  models that are solely trained to maximize the likelihood of an observed dataset are often vulnerable to adversarial attacks. Even though several methods have been proposed to enhance NN models' adversarial robustness, they often require re-training from scratch. This leads to redundant computation, especially in the NLP domain where current state-of-the-art models, such as BERT and ROBERTA, require great time and space resources. By borrowing ideas from Software Engineering, we, therefore, first introduce the Neural Patching mechanism to improve adversarial robustness by ``patching"" only parts of a NN model. Then, we propose a novel neural patching algorithm, {\mymethod}, that transforms a textual NN model into a stochastic ensemble of multi-expert predictors by upgrading and re-training its last layer only. {\mymethod} forces adversaries to attack not only one but multiple models that are specialized in diverse sub-sets of features, labels, and instances so that the ensemble model becomes more robust to adversarial attacks. By conducting comprehensive experiments, we demonstrate that all of CNN, RNN, BERT, and ROBERTA-based textual models, once patched by {\mymethod}, witness an absolute increase of as much as 20\% in accuracy on average under 5 different white and black-box attacks, outperforming 6 defensive baselines across 4 public NLP datasets. All codes and datasets are to be released."
"Emotional analysis active research area decades, especially recognition domains text speech emotions. Even text speech emotions closely relevant, kinds emotions different challenges. One challenges text emotion recognition ambiguous words, resulting omitted words . On hand, one challenges speech emotion recognition creating efficient model. However, paper focuses recognition speech emotions. In area, two types information, linguistic paralinguistic, mainly considered speech emotion recognition. The linguistic information refers meaning context speech. The paralinguistic information implies implicit message meaning, like emotion speech . Speech characteristics interpret meaning speech; therefore, behavioral expression investigated speech emotion recognition works . In recent works, local feature learning block , one efficient methods, used integrating local global speech emotion features, provide better results recognition. Inside LFLB, convolution neural network used extracting local features, long short-term memory applied extracting contextual dependencies local features learn time-related relationship. However, vanishing gradient problems may occur CNN . Therefore, residual deep learning applied CNN using skip-connection reduce unnecessary learning add feature details may lost layers. Furthermore, accuracy speech recognition rely efficiency model, also speech feature selection . In terms speech characteristics, many distinctive acoustic features usually used recognizing speech emotion, continuous features, qualitative features, spectral features . Many investigated recognize speech emotions. Some researchers compared pros cons feature, one identify feature best one . As previously mentioned, proposed method improve efficiency LFLB deeper learning. The proposed method, deep residual local feature learning block , inspired concept human brain learning; is, epeated reading makes learning effective, way Sari Shanahan used. Responding inspired concept, implemented learning method speech emotion recognition three parts: Part 1 general learning, like human reading first time, Part 2 learning, like additional readings, last part associating parts learned decide types emotions. Besides, feature selection compared two types distinctive features find effective feature work: normal specific distinctive features log-mel spectrogram , fully filtered sound elements, %log-mel spectrogram, MFCC deltas, delta-deltas, chromagram clearly identify speech characteristics extracted based %according human mood. Our main contributions paper follows: Deep residual local feature learning block proposed. DeepResLFLB arranged internal network LFLB, batch normalization , activation function, normalization-activation-CNN , deep layers. Learning sequences DeepResLFLB imitated human re-reads. Speech emotion features, %according based human mood determination factors LMS LMSDDC, applied compared performances. This paper proposes novel algorithm, named {\mymethod}, consistently improves adversarial robustness textual NN models white-box black-box attacks upgrading re-training last layer. By extending single model ensemble multiple experts diverse among feature-, prediction-, instance-levels, {\mymethod} achieves much 20\ improvement accuracy average across 5 attacks 4 NLP datasets. Thanks model-agnostic design, {\mymethod} help improve adversarial robustness NLP domains. literature NLP also domains foreseeable future. \pagebreak \vfill\null"," Speech Emotion Recognition  is becoming a key role in global business today to improve service efficiency, like call center services. Recent SERs were based on a deep learning approach. However, the efficiency of deep learning depends on the number of layers, i.e., the deeper layers, the higher efficiency. On the other hand, the deeper layers are causes of a vanishing gradient problem, a low learning rate, and high time-consuming. Therefore, this paper proposed a redesign of existing local feature learning block . The new design is called a deep residual local feature learning block . DeepResLFLB consists of three cascade blocks: LFLB, residual local feature learning block , and multilayer perceptron . LFLB is built for learning local correlations along with extracting hierarchical correlations; DeepResLFLB can take advantage of repeatedly learning to explain more detail in deeper layers using residual learning for solving vanishing gradient and reducing overfitting; and MLP is adopted to find the relationship of learning and discover probability for predicted speech emotions and gender types. Based on two available published datasets: EMODBnd RAVDESS, the proposed DeepResLFLB can significantly improve performance when evaluated by standard metrics: accuracy, precision, recall, and F1-score.  \keywords{Speech Emotion Recognition \and Residual Feature Learning \and CNN Network \and Log-Mel Spectrogram \and Chromagram}"
"As growth robots interacting humans, different levels environment understanding required robot. A robot acting environment deal many open questions, thus needs different levels reasoning task. Usually, robots rely initial knowledge, perception cognitive abilities able understand reasoning situated environment. A recently hooked topic better Knowledge-Based cognition dialogic interaction human robot, robot captures fresh information environment user Natural Language. Information comes Natural Language together visually perceived information, Knowledge Base lets cognitive agent reach different levels understanding environment. The first level understanding seen classification detection sensory inputs, e.g. detection objects visual perception, role tagging lexical sentence. The second level understanding concerns finding relations different sensory inputs, e.g. finding common attributes language vision. Some famous problems symbol grounding anchoring concern finding correspondences different sensory input modalities. A higher abstract level understanding thought find relations entities environment. e.g. scene desk book top, relationships relative physical position semantics shows entities similar. Understanding relationships physical entities also extended attributes entities. Indeed definition relationship entities found attributes. For example, user declares freshness attribute 'apple-1' 'spoiled', well 'apple-2' 'apple-3', 'orange-1' 'banana-1' 'fresh', relation values freshness attribute exists connects semantic entities; In example, apples 'spoiled', rest fruits 'fresh', closed world assumption. Relation rules attributes entities help robot interacting human many applications. For example user utters ""bring fruit"", using rules obtained freshness attribute, robot notices fruits spoiled fresh eat. Such logical rules attributes let robot realize apples spoiled, apples thrown out, added shopping list. Moreover, obtained rule attributes used robot's low-level sensory input processing. Consider utterance user example declaring physical entity spoiled, robot's visual perception doubt whether perceived object apple pear. As robot already found apples spoiled fruits fresh, perceptual detection refines recognized object apple. %Different attributes represent characteristics entity, computed visual perception Natural Language interaction user. In work, deal nine different attributes, category, color, label, functionality, owner, size, weight, restriction, location entities scene; first two computed visual perception rest obtained Natural Language. %%It worth emphasis importance attributes come Natural Language. Such information almost impossible obtain visual perception, e.g. information user give owner entity, cannot obtained camera. Also, initial knowledge base gives information category entity, particular entity , assignments might temporary. On hand, information size, weight, location, may used refinement knowledge base camera, shortcut obtaining information user. In work, propose framework learning logical rules represent relations attributes semantic model robot's environment. Such logical rules help robot find attributes entail specific attribute. A distinctive novelty work generalize rules semantic model built via Human-Robot Interaction , integration visual linguistic cues. Our framework goes way sensory input data abstract First-Order Logic formulas describe abstract relationship attributes entities scene. %Our approach differs works system able capture attributes Natural Language addition attributes computer vision. %Our proposed framework compute First-Order Logic formulas, useful general reasoning upon entities common attributes. We focus latent rules, robot capture implicitly human describes objects robot. In words, require user give rules explicitly robot, rather let robot find rules reasoning based self-computed rules improving interaction user. This paper continues review related work, Section proposed framework described, followed implementation demonstrate viability proposed framework Section . In Section results test scenario given, followed discussion applicability framework. In end, conclusions work drawn. It seldom case data wild balanced distribution. In realistic settings, limitation acquiring relatively balanced data choices balanced data sources. Handling data skewness crucial problem learning imbalanced data inevitably brings bias toward frequently observed classes. Data-level manipulation tries under-sample majority classes over-sample minority classes. But methods tend discard valuable information observations majority classes overfit sparse representation minority classes, especially imbalance level gets higher. Moreover, recent methods SMOTE cannot applied directly text data. We propose ST, effectively circumvents issues simply decomposing data k splits sequentially training learner decreasing order KL divergence target distribution, case data imbalance problem discrete uniform distribution. Through extensive experiments, show architecture proves compatible previous methods outperforms existing methods validated simulated well real-application tasks. Our model shows superiority performance enables focus put minority instances forgetting majority instances. We believe work makes meaningful step towards handling data skewness text classification application incremental learning methods focused data imbalance problem. For future work, ensemble methods used varying ratio train multiple weak learners. Moreover, since ST applied simultaneously algorithm-level methods, proven methods focal loss cost-sensitive deep neural network could implemented together increase optimal performance."," Humans have a rich representation of the entities in their environment. Entities are described by their attributes, and entities that share attributes are often semantically related.  For example, if two books have ``Natural Language Processing'' as value of their `title' attribute, we can expect that their `topic' attribute will also be equal, namely, ``NLP''.  Humans tend to generalize such observations, and infer sufficient conditions under which the `topic' attribute of any entity is ``NLP''.  If robots need to interact successfully with humans, they need to represent entities, attributes, and generalizations in a similar way. This ends in a contextualized cognitive agent that can adapt its understanding, where context provides sufficient conditions for a correct understanding. In this work, we address the problem of how to obtain these representations through human-robot interaction.  We integrate visual perception and natural language input to incrementally build a semantic model of the world, and then use inductive reasoning to infer logical rules that capture generic semantic relations, true in this model.  These relations can be used to enrich the human-robot interaction, to populate a knowledge base with inferred facts, or to remove uncertainty in the robot's sensory inputs."
"In recent years, science, engineering mathematics education emphasized supporting students disciplinary ""practices"" inquiry. These practicesuch formulating questions, designing investigations, arguing evidencere difficult identify assess traditional objectives particular correct content knowledge. In order study students practices, researchers rely mainly qualitative analyses naturalistic data. These studies advanced field understanding practices.\\ These studies limited scope, however, extremely labor-intensive: Analysis naturalistic data requires significant extensive effort trained researchers, transcribing coding construction meaning. It time cost-prohibitive conduct qualitative studies large samples data. Our purpose project develop computational tools support qualitative research large scales students' inquiry practices science. In paper report initial progress towards applying natural language processing techniques research students' written arguments college biology laboratory reports. \\ In work, report success designing NLP approached reliability trained, human coders. Specifically, show contrastive learning Wasserstein space able achieve high level agreement average. \\ The rest paper organized follows. In section first overview current state-of-art automating assessment writing science using machine learning natural language processing. Following outline writing assessment setting particular case section . In section briefly survey relevant literature machine learning NLP introduce novel approach automatic scoring. In section evaluate performance proposed approach discuss results detail.\\ In paper, present neural group testing, general deep learning acceleration framework tests multiple samples one forward pass. We found multi-round tree merge best design neural group testing. It group images one forward pass reduce overall computation cost improving detection performance. Another benefit design easily combined approaches accelerate inference by, example, quantizing pruning network parameters downsamping input data. Evaluating gains combining orthogonal approaches interesting direction future work."," Qualitative analysis of verbal data is of central importance in the learning sciences. It is labor-intensive and time-consuming, however, which limits the amount of data researchers can include in studies. This work is a step towards building a statistical machine learning  method for achieving an automated support for qualitative analyses of students writing, here specifically in score laboratory reports in introductory biology for sophistication of argumentation and reasoning. We start with a set of lab reports from an undergraduate biology course, scored by a four-level scheme that considers the complexity of argument structure, the scope of evidence, and the care and nuance of conclusions. Using this set of labeled data, we show that a popular natural language modeling processing pipeline, namely vector representation of words, a.k.a word embeddings, followed by Long Short Term Memory  model for capturing language generation as a state-space model, is able to quantitatively capture the scoring, with a high Quadratic Weighted Kappa  prediction score, when trained in via a novel contrastive learning set-up. We show that the ML algorithm approached the inter-rater reliability of human analysis. Ultimately, we conclude, that machine learning  for natural language processing  holds promise for assisting learning sciences researchers in conducting qualitative studies at much larger scales than is currently possible."
"Neural text-to-speech techniques significantly improved naturalness speech produced TTS systems. We refer NTTS systems subset TTS systems use neural networks predict mel-spectrograms phonemes, followed use neural vocoder generate audio mel-spectrograms. In order improve prosody\footnote{We use subtractive definition prosody .} speech obtained NTTS systems, considerable work learning prosodic latent representations ground truth speech. These methods use target mel-spectrograms input encoder learns latent prosodic representations. These representations used decoder addition input phonemes, generate mel-spectrograms. The latent representations obtained encoding target mel-spectrogram sentence level information directly available phonemes, subtractive definition prosody, may claim representations capture prosodic information. Several variational non-variational methods proposed learning prosodic latent representations. While methods improve prosody synthesised speech, need input mel-spectrogram available running inference unseen text. This gives rise problem sampling learnt prosodic space. Sampling random prior may result synthesised speech contextually appropriate prosody, relationship text synthesised. In order improve contextual appropriateness prosody synthesised speech, work using textual features like contextual word embeddings grammatical information directly condition NTTS systems. These methods require NTTS model learn implicit correlation given textual features prosody sentence. One work also poses sampling problem selection problem uses syntactic distance BERT embeddings select latent prosodic representation ones seen training time. Bringing aforementioned ideas using ground truth speech learn prosodic latent representations using textual information, build Kathaka, model trained using two-stage training process generate speech contextually-appropriate prosody. In Stage~\Romannum{1}, learn distribution sentence-level prosodic representations ground truth speech using VAE. In Stage~\Romannum{2}, learn sample learnt distribution using text. In work, introduce BERT+Graph sampler, novel sampling mechanism uses contextual word-piece embeddings BERT syntactic structure constituency parse trees graph attention networks. We compare Kathaka strong baseline show obtains relative improvement naturalness. In paper, use eRisk 2018 dataset Early Detection Signs Depression depression classification Reddit posts. Our method uses Latent Semantic Indexing topic modelling generate embeddings used input neural network, focuses using learned out-of-distribution confidence score alongside classification output decide whether label user wait data. Besides initial use case out-of-distribution detection, repurposed confidence score measure much model trusts classification output correct. We showed that, general, significant difference writing topics depending users' mental health, extent contains enough information use classification."," 		In this paper, we introduce Kathaka, a model trained with a novel two-stage training process for neural speech synthesis with contextually appropriate prosody. In Stage, we learn a prosodic distribution at the sentence level from mel-spectrograms available during training. In Stage, we propose a novel method to sample from this learnt prosodic distribution using the contextual information available in text. To do this, we use BERT on text, and graph-attention networks on parse trees extracted from text. We show a statistically significant relative improvement of $13.2\%$ in naturalness over a strong baseline when compared to recordings. We also conduct an ablation study on variations of our sampling technique, and show a statistically significant improvement over the baseline in each case."
"Due growing presence AI-powered systems lives, affective computing become important part human-computer interaction. Emotion plays role thoughts actions integral part way communicate . The ability leverage context understand emotions communicated verbally non-verbally trivial humans remains difficult machines . Emotional responses depend psyche physiology governed perception situations, people objects. They also depend mental state . The way exhibit perceive emotion may also differ based age, gender, race, culture accent . In addition this, unlike targets classification tasks, emotions experience rarely distinct: often coexist without clear temporal boundaries, adding considerable complexity task . Despite difficulties, automated emotion recognition social commercial applications make worth pursuing. In medical domain, exciting potential: identify diagnose depression stress individuals , monitor help people bipolar disorder assist general public maintaining mental health. Commercial applications include call center customer management, advertising neuro-marketing social media engagement . As intelligent chatbots virtual assistants become widely used, emotion detection become vital component design, development deployment conversational agents . Early research emotion detection focused binary classification single modality, whether text, speech , images . Text-based classifiers used n-gram vocabulary sentences predict polarity speech models modeled vocal dynamics characterize emotions. These approaches inherently limited: binary granularity cues single modality far removed actual human process they're meant model. As result, joint approaches leverage available modalities promising. While existing multi-modal emotion corpora like IEMOCAP Crem-D critical progress affective computing date, suffer three issues focus work. First, corpora tend small due high costs annotating emotion. This precludes use deep neural models high model complexity require many training samples generalize well. This also compounds second difficulty inherent many emotion datasets: usually many neutral, happy sad training examples, often examples rarer emotions like disgust making difficult classify. This issue easily solved combining different corpora due third issue, lack mutual compatibility -- differ emotions identified, types dialogue number speakers represented naturalness recordings . This severely restricts generalizability models trained single corpus. Contemporary literature dealt problems dropping labels . Hard scarce emotions like disgust dropped corpus models trained evaluated trimmed corpus. This allows evaluating models different corpora using utterances exhibiting common emotions. While reasonable, resulting performance complete reflection models perform deployed production. When emotion models used real-world applications, expect encounter utterances corresponding dropped labels. For cases, models likely exhibit degraded performance predicting one known, incorrect labels. In work, address problem data sparsity transfer learning via pretrain-then-finetune paradigm. Deep complex models trained large datasets auxiliary related task learn network parameters reflect abstract notions related target task. As expression emotions highly dependent individual, train multilayer TDNN task speaker identification using VoxCeleb corpus fine-tune final layers task emotion identification using Crema-D corpus . Using network, extract speech embeddings Crema-D layers, generate concatenate text embeddings accompanying transcripts using fine-tuned BERT model train LDA - pLDA model resulting dense representations. pLDA allows model easily adapt previously unseen classes domains, requirement evaluating different emotion corpus incompatible label set performing well wild. To understand merits component, exhaustively evaluate predictive power every permutation: TDNN alone, speech embeddings layers alone, text embeddings alone every combination thereof. Our best variant, trained VoxCeleb Crema-D evaluated IEMOCAP, achieves Equal Error Rate \%. Including portion IEMOCAP training produces 5-fold averaged EER \%. We presented Kathaka, NTTS model trained using novel two-stage training approach generating speech contextually appropriate prosody. In first stage training, learnt distribution sentence-level prosodic representations. We introduced novel sampling mechanism using trained samplers sample learnt sentence-level prosodic distribution. We introduced two samplers, 1)~the BERT sampler uses contextual word-piece embeddings BERT 2)~the Graph sampler interpret constituency parse trees graphs use Message Passing based Graph Attention Network them. We combine samplers BERT+Graph sampler, used Kathaka. We also modify baseline duration model incorporate latent prosodic information. We conducted ablation study samplers showed statistically significant improvement baseline case. Finally, compared Kathaka baseline, showed statistically significant relative improvement ."," Automated emotion detection in speech is a challenging task due to the complex interdependence between words and the manner in which they are spoken. It is made more difficult by the available datasets; their small size and incompatible labeling idiosyncrasies make it hard to build generalizable emotion detection systems. To address these two challenges, we present a multi-modal approach that first transfers learning from related tasks in speech and text to produce robust neural embeddings and then uses these embeddings to train a pLDA classifier that is able to adapt to previously unseen emotions and domains. We begin by training a multilayer TDNN on the task of speaker identification with the VoxCeleb corpora and then fine-tune it on the task of emotion identification with the Crema-D corpus.  Using this network, we extract speech embeddings for Crema-D from each of its layers, generate and concatenate text embeddings for the accompanying transcripts using a fine-tuned BERT model and then train an LDA - pLDA classifier on the resulting dense representations. We exhaustively evaluate the predictive power of every component: the TDNN alone, speech embeddings from each of its layers alone, text embeddings alone and every combination thereof.  Our best variant, trained on only VoxCeleb and Crema-D and evaluated on IEMOCAP, achieves an EER of $38.05$\%. Including a portion of IEMOCAP during training produces a 5-fold averaged EER of $25.72$\% ."
"Vocoders originally used speech compression field communication. Recently, vocoders utilized various fields text-to-speech voice conversion speech-to-speech translation. Neural vocoders generate human-like voices using neural networks, instead using traditional methods contain audible artifacts . Recently, demonstrated vocoders exhibit superior performances generation speed audio fidelity trained single speaker utterances. However, models face difficulty generating natural sounds multiple domains speakers, language, expressive utterances. The ability models evaluated sound quality model trained data multiple speakers sound quality unseen domain . A vocoder generate high-fidelity audio various domains, regardless whether input encountered training come out-of-domain source, usually called universal vocoder. MelGAN vocoder based generative adversarial networks . It lightweight robust model unseen speakers yields lower fidelity popularly employed models . MelGAN alleviates metallic sound occurs mainly unvoiced breathy speech segments multi-scale discriminators receive different scale waveforms inputs. However, implemented efficiently learning multiple speakers universal vocoder. In study, propose Universal MelGAN. The generated waveform original MelGAN audible artifacts appears over-smoothing problem non-sharp spectrogram. We added multi-resolution spectrogram discriminators model address problem frequency domain. Our multi-scale discriminators enable fine-grained spectrogram prediction discriminating waveforms spectrograms. In particular, alleviate over-smoothing problem high frequency band large footprint model, enabling generation realistic multi-speaker waveforms. To evaluate performance proposed model, compare full-band MelGAN baseline two vocoders: WaveGlow WaveRNN. We designed experiments Korean English language independency. For evaluation, prepared multiple speaker utterances included unseen domain scenarios, new speakers, emotions, languages. The evaluation results indicate proposed model achieved best mean opinion score scenarios efficiently preserved fidelity unseen speakers. In addition, evaluations show model efficiently preserves original speech, even challenging domains expressive utterances unseen languages. In multi-speaker text-to-speech scenarios, model generate high-fidelity waveforms high MOS, model outperforms compared vocoders. This results without external domain information suggest possibility proposed model universal vocoder. In work, present multi-modal approach emotion detection first transfers learning related tasks speech text produce robust neural embeddings uses embeddings train pLDA classifier able adapt previously unseen emotions domains. We show that: In future, think promise adapting learning fine-tuned emotion detection models emotions, domains languages via one-shot classification pLDA. We also interested exploring effectiveness transferring auxiliary tasks like automated speech recognition."," We propose Universal MelGAN, a vocoder that synthesizes high-fidelity speech in multiple domains. To preserve sound quality when the MelGAN-based structure is trained with a dataset of hundreds of speakers, we added multi-resolution spectrogram discriminators to sharpen the spectral resolution of the generated waveforms. This enables the model to generate realistic waveforms of multi-speakers, by alleviating the over-smoothing problem in the high frequency band of the large footprint model. Our structure generates signals close to ground-truth data without reducing the inference speed, by discriminating the waveform and spectrogram during training. The model achieved the best mean opinion score  in most scenarios using ground-truth mel-spectrogram as an input. Especially, it showed superior performance in unseen domains with regard of speaker, emotion, and language. Moreover, in a multi-speaker text-to-speech scenario using mel-spectrogram generated by a transformer model, it synthesized high-fidelity speech of 4.22 MOS. These results, achieved without external domain information, highlight the potential of the proposed model as a universal vocoder."
"%What spoken term detection Unsupervised speech modeling task discovering modeling speech units various levels audio recording without using prior linguistic information. It interesting, challenging impactful research problem phonetic, lexical even semantic information could acquired without process transcribing understanding given speech data. The relevant technology particularly important facilitate data preparation especially scenarios where: 1) large amount audio data readily available online untranscribed; 2) large amount audio recording available unpopular language structured linguistic knowledge documentation found. Spoken term discovery representative task unsupervised speech modeling. It aims discover repetitively occurred words and/or phrases untranscribed audio. The problem commonly tackled two-stage approach. In first stage, set subword units automatically discovered untranscribed speech data units turn used represent speech data symbol sequence. In second stage, variable-length sequence matching clustering performed subword sequence representations. One major drawback subword decoding errors first stage would propagate deteriorate outcome spoken term discovery second stage. The present study investigates use Siamese Triplet networks spoken term discovery. Siamese network commonly applied pattern classification matching problems weak labels available. We propose train Siamese/Triplet network small dataset matched mismatched sequence pairs obtained use trained network generate feature representations unseen subword sequences. The training dataset constructed based hypothesized spoken term clusters baseline spoken term discovery system developed previous study. With new feature representations learned Siamese/Triplet network, re-clustering subword sequences carried generate improved set discovered spoken terms. In study, propose Universal MelGAN, robust neural vocoder high-fidelity synthesis multiple domains. We solved over-smoothing problem causes metallic sound, attaching multi-resolution spectrogram discriminators model. Our model stable generating waveforms fine-grained spectrograms large footprint models. The evaluation results indicate proposed model achieved highest MOS seen unseen domain scenarios. The result demonstrates universality proposed model. For general use model, study lightweight model future apply multi-band strategy reduce complexity preserving sound quality."," Spoken term discovery from untranscribed speech audio could be achieved via a two-stage process. In the first stage, the unlabelled speech is decoded into a sequence of subword units that are learned and modelled in an unsupervised manner. In the second stage, partial sequence matching and clustering are performed on the decoded subword sequences, resulting in a set of discovered words or phrases. A limitation of this approach is that the results of subword decoding could be erroneous, and the errors would impact the subsequent steps. While Siamese/Triplet network is one approach to learn segment representations that can improve the discovery process, the challenge in spoken term discovery under a complete unsupervised scenario is that training examples are unavailable. In this paper, we propose to generate training examples from initial hypothesized sequence clusters. The Siamese/Triplet network is trained on the hypothesized examples to measure the similarity between two speech segments and hereby perform re-clustering of all hypothesized subword sequences to achieve spoken term discovery.  Experimental results show that the proposed approach is effective in obtaining training examples for Siamese and Triplet networks, improving the efficacy of spoken term discovery as compared with the original two-stage method."
"Evidence-based medicine medical practice aims find evidence support medical decisions. This evidence nowadays obtained biomedical journals, usually accessible online databases like PubMed EMBASE, provide free access articles' abstracts cases, full articles. In context COVID-19 pandemic, EBM critical making decisions individual level public health since research articles address topics like treatments, adverse cases, effects public policies medicine. The EBM foundation Epistemonikos made essential contributions curating publishing updated guides treatments working COVID-19~\footnote{http://epistemonikos.org/}. Epistemonikos addresses EBM combination software tools data collection, storage, filtering , retrieval, well vital labor volunteer physicians curate label research articles based quality , type PICO labels . However, workflow challenged 2020 increasing growth rapidly evolving evidence COVID-19 articles published latest months. Moreover, ensure rapid collection latest evidence published, pre-print repositories medRXiv bioRXiv added traditional online databases. % In order support Epistemonikos' effort filter curate flood articles related COVID-19, present results applied AI project implement evaluate text classification system filter categorize research articles related COVID-19. The current model, based Random Forests, acceptable performance classifying systematic reviews fails classifying document categories. In article, show using BioBERT yields marginal improvements, XLNET results significant progress best performance. These results save considerable amount time volunteer physicians pre-filtering articles worth manual curation labeling EBM. In average, physician takes two minutes reviewing one article, system present article review within one hour. %With help volunteer physicians, classify emergent literature COVID-19 virus systematic reviews, broad syntheses, primary studies, first step finding relevant clinical evidence. Until now, produced Random Forest model classifying documents different categories. However, paper, show use Transformers-based Language Models helped foundation save significant effort collaborators. % The clusters compatible baseline model. The operation proposed system compatible baseline model. This shows even completely unsupervised scenario, well-performing Siamese network still trained segments soft labels generated unsupervised manner. By maintaining high confidence hypothesized segment labels, network capable generate segment representations new unseen segments spoken term discovery. It noted referring clustering results spoken term discovery system one complete unsupervised methods obtain segment boundaries cluster information generating soft labels Siamese network training. This method specifically considered work baseline comparison. Other segmentation confident data generation approaches also feasible. The term clusters discovered exhibit different properties work favorably different types segments. Post-processing work combining term clusters two systems considered improved overall term discovery performance. Multiple clusters representation shorter terms also grouped together. One way learn semantic relationship clusters treating segments words word2vec training . , shown possible audio segments . Clusters close semantic relationship small segment representation distances combined. similar meaning reflected learnt representations similar segment features combined. Depending goal clustering, alternative clustering algorithms considered. If aiming remove noise segment candidates, HDBCAN might good choice. But aiming full coverage possible words recording, clustering algorithms BPGMM considered assign segment candidates specific term cluster. Alternative clustering \section{Conclusion} In work, attempt using Siamese Triplet networks spoken term discovery complete unsupervised scenario made. The initial segmentation cluster information obtained spoken term discovery system. The clusters high confidence used generate matched mismatched pairs tuples training Siamese Triplet networks. The networks used generate representations available segments, follow HDBSCAN segment representations obtain new set spoken term clusters. It shown even exact labels segments unavailable, Siamese/Triplet network still trained small set high confidence matched mismatched data pairs presented. This shows even completely unsupervised scenario, well-performing Siamese/Triplet network still trained segments soft labels generated unsupervised manner. By maintaining high confidence hypothesized segment labels, network capable generate segment representations spoken term discovery. The segment representations generated Siamese Triplet networks outperform baseline two-stage model. In lecture recording experiment, result conclusive Triplet network. However, experiment Zerospeech dataset shows Triplet network slightly better Siamese network learning segment representations spoken term discovery trained sufficient data. Triplet network less favourable Siamese network generating segment representations spoken term discovery. In problem spoken term discovery, Triplet network less favourable experiments cluster boundaries less easy determine clustering algorithm.","  The COVID-19 has brought about a significant challenge to the whole of humanity, but with a special burden upon the medical community. Clinicians must keep updated continuously about symptoms, diagnoses, and effectiveness of emergent treatments under a never-ending flood of scientific literature. In this context, the role of evidence-based medicine  for curating the most substantial evidence to support public health and clinical practice turns essential but is being challenged as never before due to the high volume of research articles published and pre-prints posted daily. Artificial Intelligence can have a crucial role in this situation. In this article, we report the results of an applied research project to classify scientific articles to support Epistemonikos, one of the most active foundations worldwide conducting EBM. We test several methods, and the best one, based on the XLNet neural language model, improves the current approach by 93\% on average F1-score, saving valuable time from physicians who volunteer to curate COVID-19 research articles manually."
"The natural language processing community made tremendous progress using pre-trained language models improve predictive accuracy . Models surpassed human performance language understanding benchmarks SuperGLUE . However, studies shown results partially driven models detecting superficial cues correlate well labels may useful intended underlying task . This brittleness leads overestimating model performance artificially constructed tasks poor performance out-of-distribution adversarial examples. A well-studied example phenomenon natural language inference dataset MNLI . The generation dataset led spurious surface patterns correlate noticeably labels. highlight negation words often associated contradiction label. show model trained solely hypothesis, completely ignoring intended signal, reaches strong performance. We refer surface patterns dataset biases since conditional distribution labels given biased features likely change examples outside training data distribution . A major challenge representation learning NLP produce models robust dataset biases. Previous work targeted removing dataset biases explicitly factoring models. These works explicitly construct biased model, instance, hypothesis-only model NLI experiments, use improve robustness main model. The core idea encourage main model find different explanation biased model wrong. During training, products-of-experts ensembling used factor biased model. While works show promising results, assumption knowledge underlying dataset bias quite restrictive. Finding dataset biases established datasets costly time-consuming process, may require access private details annotation procedure, actively reducing surface correlations collection process new datasets challenging given number potential biases . In work, explore methods learning biased datasets require explicit formulation dataset biases. We first show model limited capacity, call weak learner, trained standard cross-entropy loss learns exploit biases dataset. We investigate biases weak learner relies show match several previously manually identified biases. Based observation, leverage limited capacity models product experts ensemble train robust model evaluate approach various settings ranging toy datasets large crowd-sourced benchmarks: controlled synthetic bias setup , natural language inference extractive question answering . Our contributions following: show weak learners prone relying shallow heuristics highlight rediscover previously human-identified dataset biases; demonstrate need explicitly know model dataset biases train robust models generalize better out-of-distribution examples; discuss design choices weak learners show trade-offs higher out-of-distribution performance expense in-distribution performance. %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% \subfile{sections/conclusions}"," State-of-the-art natural language processing  models often learn to model dataset biases and surface form correlations instead of features that target the intended underlying task. Previous work has demonstrated effective methods to circumvent these issues when knowledge of the bias is available. We consider cases where the bias issues may not be explicitly identified, and show a method for training models that learn to ignore these problematic correlations.  Our approach relies on the observation that models with limited capacity primarily learn to exploit biases in the dataset. We can leverage the errors of such limited capacity models to train a more robust model in a product of experts, thus bypassing the need to hand-craft a biased model. We show the effectiveness of this method to retain improvements in out-of-distribution settings even if no particular bias is targeted by the biased model."
"Topic models popularly used extract abstract topics occur commonly across documents corpus field Natural Language Processing. Each topic group semantically coherent words represent common concept. In addition gaining insights unstructured texts, topic models used several tasks practical importance learning text representations document classification , keyphrase extraction , review understanding recommendations e-commerce domain , semantic similarity detection texts etc. % order make topic sampling distribution converge desired posterior distribution Early popular works topic discovery include statistical methods Latent Dirichlet Allocation approximates topic probability distribution word vocabulary performs approximate inference document-topic topic-word distributions Variational Bayes . This followed modified inference algorithm - Collapsed Gibbs sampling follows Markov Chain Monte Carlo . However, methods require expensive iterative inference step performed document. This circumvented introduction deep neural networks emergence Variational Autoencoders particular, variational inference performed single forward pass. % estimating posterior distribution. % Laplace approximation % The re-parameterisation trick VAEs allows perform variational inference differentiable manner training neural network. Such neural variational inference based topic models outperformed traditional probabilistic sampling methods. Broadly, model document Bag-of-Words determined basis frequency count vocabulary token given document. The BoW input processed MLP followed variational inference samples latent document-topic vector. A decoder network reconstructs original BoW using latent document-topic vector allows capture relationship document-topic topic-word distributions. VAE family neural topic models categorised basis prior enforced latent document-topic distribution. Methods NVDM , NTM-R , NVDM-GSM use Gaussian prior. NVLDA ProdLDA use Dirichlet prior approximation enables model capture document stems sparse set topics. % perform better providing coherent topics compared Gaussian prior. % order capture latent document-topic distribution, % The context vector obtained result attention used perform variational inference % capture semantics effectively % help inferring latent document-topic vector % carried usual VAE based topic models % using final LSTM state outputs corresponding While main focus previous neural topic models enforce suitable priors, little effort spent explicitly improving document encoding framework order capture document semantics better. In work, build upon VAE based topic model using laplace approximation Dirichlet prior propose novel framework model input document sequence tokens. The sequence processed LSTM allows encode sequential order remain preserved BoW. To allow model focus specific parts document, use attention mechanism attend different document tokens. We hypothesise topic-word distribution learned model factored attention mechanism enable model attend tokens convey topic related information cues. We validate hypothesis propose TAN-NTM: Topic Attention Networks Neural Topic Modeling performs attention efficiently topic guided manner. We perform separate attention topic using corresponding word probability distribution obtain topic-wise context vectors. The context vectors composed using topic weights represent proportion topic present given document. These topic weights obtained using learned token embedding topic-word distribution. The final composed context vector used perform variational inference followed BoW decoding. We perform extensive ablations compare different ways composing topic-wise context vectors. % averages coherence score topics % generated model In order evaluate approach, estimate commonly used NPMI coherence measures extent probable words topic semantically related other. Using metric, compare TAN-NTM model several previous state-of-the-art topic models outperforming significantly 4 benchmark datasets varying scale complexity - 20NewsGroup , Yelp Review Polarity, DBpedia AGNews . We demonstrate efficacy model learning better document feature representations latent document-topic vectors achieving higher document classification accuracy baseline topics models. Furthermore, topic models previously used improve supervised keyphrase generation . We show proposed framework adapted modify topic model improve keyphrase generation achieving SOTA performance StackExchange Weibo datasets. Our contributions summarised as: % We presented effective method training models robust dataset biases. Leveraging weak learner limited capacity modified product experts training setup, show dataset biases need explicitly known modeled able train models generalize significantly better out-of-distribution examples. We discuss design choices weak learner investigate using higher-capacity learners leads higher out-of-distribution performance trade-off in-distribution performance. We believe approaches capable automatically identifying mitigating datasets bias essential tools future bias-discovery mitigation techniques. \clearpage"," Topic models have been widely used to learn representations from text and gain insight into document corpora. To perform topic discovery, existing neural models use document bag-of-words  representation as input followed by variational inference and learn topic-word distribution through reconstructing BoW. Such methods have mainly focused on analysing the effect of enforcing suitable priors on document distribution. However, little importance has been given to encoding improved document features for capturing document semantics better. In this work, we propose a novel framework: TAN-NTM which models document as a sequence of tokens instead of BoW at the input layer and processes it through an LSTM whose output is used to perform variational inference followed by BoW decoding. We apply attention on LSTM outputs to empower the model to attend on relevant words which convey topic related cues. We hypothesise that attention can be performed effectively if done in a topic guided manner and establish this empirically through ablations. We factor in topic-word distribution to perform topic aware attention achieving state-of-the-art results with $\sim$ $9$ - $15$ percentage improvement over score of existing SOTA topic models in NPMI coherence metric on four benchmark datasets - 20NewsGroup, Yelp, AGNews, DBpedia. TAN-NTM also obtains better document classification accuracy owing to learning improved document-topic features. We qualitatively discuss that attention mechanism enables unsupervised discovery of keywords. Motivated by this, we further show that our proposed framework achieves state-of-the-art performance on topic aware supervised generation of keyphrases on StackExchange and Weibo datasets."
". % % % final paper: en-us version % % space normally used marker This work licensed Creative Commons Attribution 4.0 International License. License details: \url{http://creativecommons.org/licenses/by/4.0/}. } Rhetorical Structure Theory one influential theories discourse analysis, document represented hierarchical discourse tree. As shown Figure a, leaf nodes RST tree text spans named Elementary Discourse Units , EDUs connected rhetorical relations form larger text spans entire document included. The rhetorical relations categorized Nucleus Satellite based relative importance. Thus, document-level discourse parsing consists three sub-tasks: tree construction, nuclearity determination relation classification. Moreover, downstream natural language processing tasks benefit RST-based structure-aware document analysis, summarization machine comprehension . By utilizing various linguistic characteristics , statistical approaches obtained substantial improvement English RST-DT benchmark . Recently, neural networks making inroads discourse analysis frameworks, attention-based hierarchical encoding integrating neural-based syntactic features transition-based parser . Lin et al. \shortcite{lin2019unified} follow-up work successfully explored encoder-decoder neural architectures sentence-level discourse analysis, top-down parsing procedure. Although discourse parsing received much research attention progress, models mainly optimized evaluated English. The main challenge shortage annotated data, since manual annotation RST framework labor-intensive requires specialized linguistic knowledge. For instance, popular benchmark English RST-DT corpus contains 385 samples, much smaller natural language processing tasks. The treebank size languages German , Dutch Basque even limited. Such limitations make difficult achieve acceptable performance languages required fully support downstream tasks, also lead poor generalization ability computational approaches. Since treebanks different languages share underlying linguistic theory, data-driven approaches benefit joint learning multilingual RST resources . Therefore, paper, investigate two methods build cross-lingual neural discourse parser: From embedding perspective: cross-lingual contextualized language models, train parser shared semantic space multilingual sources without employing language indicator; From text perspective: since EDU semantically-cohesive unit, unify target language space EDU-level translation, preserving original EDU segmentation discourse tree structures . To end, adapted enhanced end-to-end neural discourse parser, investigated two proposed approaches 6 different languages. While RST data training still small scale, achieved state-of-the-art performance fronts, significantly surpassing previous models, even approaching upper bound human performance. Moreover, conducted topic modeling analysis collected multilingual treebanks evaluate model generality across various domains. In paper, propose FedHumor approach - humorous text recognition model following federated learning paradigm provide personalized humor recognition based labels stored distributed sources. It able account diversity person's activation point perceived funniness text contents. Through extensive experiments comparing FedHumor 9 state-of-the-art approaches, show able achieve better personalization recognizing humor text contents. To best knowledge, first federated learning-based personalized humorous text recognition model."," Text discourse parsing plays an important role in understanding information flow and argumentative structure in natural language. Previous research under the Rhetorical Structure Theory  has mostly focused on inducing and evaluating models from the English treebank. However, the parsing tasks for other languages such as German, Dutch, and Portuguese are still challenging due to the shortage of annotated data. In this work, we investigate two approaches to establish a neural, cross-lingual discourse parser via:  utilizing multilingual vector representations; and  adopting segment-level translation of the source content. Experiment results show that both methods are effective even with limited training data, and achieve state-of-the-art performance on cross-lingual, document-level discourse parsing on all sub-tasks. \newline"
"In recent years, smart devices built-in personal assistants like Google Assistant Siri becoming omnipresent. Behind intelligent systems, key question identify underlying intent user utterance, triggered large amount work intent detection . Most existing intent detection systems built deep learning models trained large-scale annotated data. However, user demands functions smart devices continue grow, collecting supervised data every new intent becomes time-consuming labor-intensive. To address issue, studies tackle intent detection zero-shot learning manner, attempting utilize learned knowledge seen classes help detect unseen classes. The recent methods zero-shot intent detection roughly divided two categories: The first category , referred transformation-based methods, utilizes word embeddings label names establish similarity matrix, used transfer prediction space seen intents unseen intents. Another line work based compatibility-based methods , aims encode label names utterances representations semantic space calculate similarity. In kinds methods, critical problem learning intent representations. However, existing ZSID methods class-inductive, relies entirely labeled data seen intents training stage. Consequently, representations unseen intents cannot learned, resulting two limitations. First, ZSID methods good modeling relationship seen unseen intents. For transformation-based methods, label names given form raw phrases sentences, word embeddings label names inadequate associate connections seen unseen intents. For example, ookRestaurant similar ateBook measured word embeddings, share word ook . However, meaning two intents relevant. % As result, computed similarity matrix inadequate associating connections seen unseen intents . For compatibility-based methods, minimize similarity seen intent samples seen label names shared semantic space, directly transfer detect unseen intents. Since unseen intent representations learned, might entangled representations seen intents. This severely hurt accuracy predicted label-utterance similarity, especially expressions utterances diverse. %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Second, vanilla ZSL methods applicable generalized intent detection . Compared ZSL setting , assumes models presented utterances unseen classes test time, GZSID requires model detect seen unseen intents. In GZSID, existing ZSL models usually suffer dubbed domain shift problem, utterances unseen intents almost always mistakenly classified seen intents. Unlike class-inductive methods, class-transductive ZSL uses semantic information unseen classes model training . In context intent detection, label name provides proper sketch intent meaning. Motivated this, propose utilize label names unseen intents learn disentangled intent representations . Specifically, include unseen intents prediction space training, label names serving pseudo utterances. This allows model learn boundary seen unseen class semantic space. Under framework, introduce assistant task forces model find distinction seen unseen intents, thereby alleviating domain-shift problem. On basis, refine word embedding based similarity matrix averaging representations corresponding utterances label names. As result, better capture intent meanings similarity matrix reflects accurate intent connections. In summary, contribution three-fold: We believe potential class-transductive ZSL intent detection still fully exploited, encourage related studies future, release codes data. In paper, investigated two approaches cross-lingual neural discourse parsing. Experimental results show utilizing cross-lingual representation adopting segment-level translation contribute obtaining state-of-the-art performance various treebanks. Moreover, monolingual models also benefit cross-lingual training introducing data domains. For future work, consider conducting domain adaption via few-shot learning make approach generalizable."," Zero-shot intent detection  aims to deal with the continuously emerging intents without annotated training data. However, existing ZSID systems suffer from two limitations: 1) They are not good at modeling the relationship between seen and unseen intents, when the label names are given in the form of raw phrases or sentences. 2) They cannot effectively recognize unseen intents under the generalized intent detection  setting. A critical factor behind these limitations is the representations of unseen intents, which cannot be learned in the training stage. To address this problem, we propose a class-transductive framework that utilizes unseen class labels to learn Disentangled Intent Representations . Specifically, we allow the model to predict unseen intents in the training stage, with the corresponding label names serving as input utterances. Under this framework, we introduce a multi-task learning objective, which encourages the model to learn the distinctions among intents, and a similarity scorer, which estimates the connections among intents more accurately based on the learned intent representations. % Moreover, we present a novel approach to calculate the inter-intent similarities, on the basis of the learned intent representations, which estimates the connections among intents more accurately.  Since the purpose of DIR is to provide better intent representations, it can be easily integrated with existing ZSID and GZSID methods. Experiments on two real-world datasets show that the proposed framework brings consistent improvement to the baseline systems, regardless of the model architectures or zero-shot learning strategies."
"Multi-turn open-domain dialogue modeling active research topic field natural language processing. However, generating coherent informative response given dialogue context remains challenge. % However, still challenging dialogue models generate coherent informative response given dialogue context. %Research domain mainly addresses following two questions: 1) How learn represent context? 2) In presence context representation, infer distribution response? A critical challenge learning rich robust context representations dialogue utterances~, namely challenge encoding dialogue context vector adequately captures semantics . % A major challenge domain learn rich robust context representations dialogue utterances~, namely challenge encoding dialogue context vector adequately captures semantics . Large-scale pre-training language models using Transformer-based architectures recently achieved remarkable successes variety NLP tasks~. % Recently, large-scale pre-training language models using Transformer-based architectures achieved remarkable successes variety NLP tasks~. As such, increasingly work aims use pre-training language models conversation modeling~. For example, DialoGPT~ extends GPT-2~ generate conversation responses large-scale dialogue corpus. Meena~ trains sequence-to-sequence model~ Evolved Transformer~ large-scale multi-turn conversations. Blender, developed Facebook, provides recipes building open-domain chatbots perform well human evaluations~. However, existing pre-training conversation models usually view dialogue context linear sequence tokens learns generate next word token-level self-attention. One issue approach high-level relationships utterances harder capture using word-level semantics. % One issue approach relationships utterances scattered individual words, hindering capturing discourse-level coherence. For example, discourse-level relationship utterances ``coffee please'' ``here are'' apparent, word-level comparisons, coffee, please, are, obscures high-level relationship. % For example, utterance ``coffee please'' ``here are'' Figure strong certain relationship, contrast, pairs individual words two utterances coffee, please, obscure correlations. Furthermore, full pairwise attention inefficient since requires word context decoder interact words regardless distances semantic units. % Furthermore, full pairwise attention inefficient since requires word context decoder interact words regardless distances semantic units. To alleviate issues above, present DialogBERT, novel conversational response generation model. % To alleviate aforementioned issues, present DialogBERT, novel conversational response generation model. DialogBERT employs hierarchical Transformer architecture represent dialogue context. It first encodes dialogue utterances Transformer encoder encodes resulting utterance vectors using discourse-level Transformer obtain representation entire dialogue context. To efficiently capture discourse-level coherence among utterances, propose two training objectives analogy original BERT training: 1) masked context regression, masks randomly-selected utterance predicts encoding vector masked utterance directly; 2) distributed utterance order ranking, %reconstructs order utterances belong dialog context organizes randomly shuffled utterances conversation coherent dialogue context Learning-to-Rank~ neural network. We evaluate DialogBERT popular multi-turn conversation datasets, namely Weibo, MultiWOZ DailyDialog. Results show DialogBERT outperforms baselines terms perplexity, BLEU, NIST. Human evaluation supports superiority approach capturing discourse-level semantics generating plausible dialogue responses. %Our contributions summarized follows: % In paper, propose class-transductive framework overcome limitations existing ZSID models. The framework learns disentangled representations unseen intents including prediction space training. Under DIR framework, present multi-task learning objective training stage encourages model learn distinctions unseen seen intents. In inference stage, develop similarity scorer, better associate inter-intent connections based learned representations. Experiments two benchmarks show DIR effective robust, bring considerable improvement ZSID systems different zero-shot learning strategies backbone networks."," Recent advances in pre-trained language models have significantly improved neural response generation.  However, existing methods usually view the dialogue context as a linear sequence of tokens and learn to generate the next word through token-level self-attention.  Such token-level encoding hinders the exploration of discourse-level coherence among utterances.  This paper presents DialogBERT, a novel conversational response generation model that enhances previous PLM-based dialogue models. DialogBERT employs a hierarchical Transformer architecture.  To efficiently capture the discourse-level coherence among utterances, we propose two training objectives, including masked utterance regression and distributed utterance order ranking in analogy to the original BERT training.  Experiments on three multi-turn conversation datasets show that our approach remarkably outperforms the baselines, such as BART and DialoGPT, in terms of quantitative evaluation.  The human evaluation suggests that DialogBERT generates more coherent, informative, and human-like responses than the baselines with significant margins. % Pre-trained language models  have been successfully adapted to neural response generation.  % However, existing methods usually view the dialogue context as a linear sequence of tokens and learn to generate the next word through token-level self-attention. % Such token-level encoding hinders the exploration of discourse-level coherence among utterances. % In this paper, we present DialogBERT, a novel conversational response generation model that enhances previous PLM-based dialogue models. % %In order to model the utterance-level interactions, % Instead of a flat encoding of linear tokens, DialogBERT employs a hierarchical Transformer architecture.  % %DialogBERT consists of an utterance encoder for encoding utterances and a context encoder for learning to contextualize given utterances' representations. % To efficiently capture the discourse-level coherence among utterances, we propose two new training objectives including masked utterance regression and distributed utterance order ranking in analogy to the original BERT training.  % Experiments on three multi-turn conversation datasets show that  % our approach remarkably outperforms three baselines such as BART and DialoGPT in terms of quantitative evaluation.  % Human evaluation  % suggests  % %\jw{supports}  % that DialogBERT generates more coherent, informative and human-like responses than the baselines with significant margins."
"Event Detection , task involves identifying boundaries event triggers classifying corresponding event types, aims seek recognize events specific types given texts. As fundamental task information extraction, many high-level NLP tasks, information retrieval question answering, need event detector one essential components. %  Recent studies show English ED models achieved great performance treating problem word-by-word sequence labeling task. Different English ED, many East Asian languages, including Chinese, written without explicit word boundary, resulting much tricky ED task. An intuitive solution apply Chinese Word Segmentation tools first get word boundaries, use word-level sequence labeling model similar English ED models. However, word boundary ambiguous Chinese thus word-trigger mismatch problem exists Chinese ED, event trigger may exactly match word, likely part word cross multiple words Figure demonstrates. Meanwhile, character-level sequence tagging able alleviate problem, Chinese character embedding carry limited information due lack word word-sequence information, resulting ambiguous semantics. % Therefore, better integrate segmentation-related information character-level semantics key feature Chinese ED models. Several recent works demonstrated considering lexicon word information could provide exact information discriminate semantics characters. \citeauthor{lin-etal-2018-nugget}~\shortcite{lin-etal-2018-nugget} designed NPN, CNN-like network model character compositional structure trigger words introduced gate mechanism fuse information characters words. ~\citeauthor{ding-etal-2019-event}~\shortcite{ding-etal-2019-event} proposed TLNN, trigger-aware Lattice LSTM architecture, exploiting semantics matched lexicon words improve Chinese ED. Although methods achieved great success, continue difficulty fully exploiting interaction characters lexicon words. Specifically, character, NPN exploits gate mechanism fuse information one corresponding word. This means character could incorporated one matched word, actually one character likely match several words, leading information loss. For TLNN, constructs cut paths link start end character matched word, semantic information matched lexicon word fails flow characters covers except last one, due inherently unidirectional sequential nature Lattice LSTM. %For characters without matched words, extra information provided enhance representation. Besides, previous ED works usually ignore semantic information maintained event types. We observe event types usually semantically related corresponding event triggers. Such observation shows considering semantic information event labels may provide fine-grained semantic signals guide detection event triggers, accordingly benefit ED performance. In paper, propose novel neural architecture, named Label Enhanced Heterogeneous Graph Attention Networks , Chinese ED. To promote better information interaction words characters, transform sentence graph. We first connect lexicon words characters covers. And neighboring characters also linked provide local context information enhance character representations, especially without matched lexicon word. To capture different granularity semantic information words characters, formulate words characters two types nodes, thus heterogeneous graph attention networks utilized enable rich information propagation graph. Additionally, design matcher module leverage semantic information event labels. Specifically, transform event labels event-trigger-prototype based embedding matrix summarizing trigger representations belonging event label. Based generated event label representation, margin loss exploited enhance ability discriminate confusing event labels. Comparing previous works, contributions follows: In paper, proposed neural response generation model named DialogBERT. Instead encoding dialogue context linear sequence tokens, DialogBERT employs hierarchical Transformer encoder architecture. : utterances dialogue context first encoded vectors utterance encoder fed context encoder learns context sensitive encoding. As natural extension original BERT training, proposed two training objectives: masked utterance regression distributed utterance re-ordering. We showed proposed objectives enable conversation model capture multi-level coherences. Additionally, showed DialogBERT notably outperforms baseline models response generation tasks. KMY: future work?"," Event Detection  aims to recognize instances of specified types of event triggers in text. Different from English ED, Chinese ED suffers from the problem of word-trigger mismatch due to the uncertain word boundaries.  Existing approaches injecting word information into character-level models have achieved promising progress  to alleviate this problem, but they are limited by two issues. First, the interaction between characters and lexicon words is not fully exploited. Second, they ignore the semantic information provided by event labels.  We thus propose a novel architecture named Label enhanced Heterogeneous Graph Attention Networks .  Specifically, we transform each sentence into a graph, where character nodes and word nodes are connected with different types of edges, so that the interaction between words and characters is fully reserved. A heterogeneous graph attention networks is then introduced to propagate relational message and enrich information interaction. Furthermore, we convert each label into a trigger-prototype-based embedding, and design a margin loss to guide the model distinguish confusing event labels. Experiments on two benchmark datasets show that our model achieves significant improvement over a range of competitive baseline methods."
"% \rev{@Ileana: example indicate changes text, based revision.} % \todo[inline]{we need add color bars figures, promised reviewers} Given enough computational power, scalability attention mechanism~ allow building ever larger Natural Language Processing models billions parameters . While impressive, advances also pose responsibility NLP community interpret behavior hundreds attention heads single model, potentially reduce number computations. Responding challenge, previous work taken pioneering steps discover explain sparseness attention patters. Here, argue number heads grows range thousands, automatic measures would needed discover impose sparseness models. We introduce simple task-agnostic data-informed pruning method attention mechanisms: Attention Pruning. We train Transformer-based models analyze global observed attention patterns, averaged input sequences train set, order identify remove weak connections input tokens. Following \citet{lottery}, retrain models, enforcing sparseness masking, demonstrate attention mechanisms incorporate extraneous connections input tokens: obtain comparable % \question{or even marginally better performance} performance using sparse attention patterns NLP tasks language sequence-to-sequence modelling, well %Natural Language Inference . \rev{prediction GLUE tasks. Figure summarizes impact using pruning method standard NLP tasks.} These global sparseness patterns could help improve interpretability inference-time computational efficiency widely-used attention models. Our contributions follows: % The rest paper organized follows: In Section, present related work. In Section, introduce details behind attention pruning method. In Section, apply AP experiments language modelling. In Section, apply AP seq2seq modelling machine translation tasks. In Section, extend machine translation experiments demonstrate AP compatible -entmax regularization~, another promising sparseness technique. In Section, study effect AP BERT GLUE benchmark. % % Section. In Section discuss theoretically pruned Transformers could yield speedups terms MACs. % In Section, discuss hardware efficiency AP promise speeding modelling really long sequences. In Section, conclude point promising directions future work. In paper, propose novel architecture, label enhanced heterogeneous graph attention networks model , Chinese ED. To fully exploit information characters words, formulate characters words different types nodes, connect richly functional edges. The heterogeneous graph attention networks utilized enable adequate information propagation. Besides, utilize semantic clues event labels guide detection event triggers. Experiment results show L-HGAT consistently achieves superior performance previous competing approaches. In future, would like adapt L-HGAT information extraction tasks, named entity recognition aspect extraction. \def\year{2021}\relax File: formatting-instructions-latex-2021.tex release 2021.1 \documentclass[letterpaper]{article} DO NOT CHANGE THIS \usepackage{aaai21} DO NOT CHANGE THIS \usepackage{times} DO NOT CHANGE THIS \usepackage{helvet} DO NOT CHANGE THIS \usepackage{courier} DO NOT CHANGE THIS \usepackage[hyphens]{url} DO NOT CHANGE THIS \usepackage{graphicx} DO NOT CHANGE THIS \urlstyle{rm} DO NOT CHANGE THIS \def\UrlFont{\rm} DO NOT CHANGE THIS \usepackage{natbib} DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT \usepackage{caption} DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT \frenchspacing DO NOT CHANGE THIS \setlength{\pdfpagewidth}{8.5in} DO NOT CHANGE THIS \setlength{\pdfpageheight}{11in} DO NOT CHANGE THIS \usepackage{multirow} \usepackage{amssymb} \usepackage{booktabs} \usepackage{bm} \usepackage{CJKutf8} \usepackage[switch]{lineno} \newcommand{\tabincell}[2]{}  \nocopyright PDF Info Is REQUIRED. For /Author, add authors within parentheses, separated commas. No accents commands. For /Title, add Title Mixed Case. No accents commands. Retain parentheses. \pdfinfo{ /Title /Author /TemplateVersion } Leave /Title Put actual complete title within parentheses mixed case Leave space \Title beginning parenthesis alone /Author Put actual complete list authors within parentheses mixed case. Each author comma. If name contains accents, remove them. If LaTeX commands, remove them. DISALLOWED PACKAGES \usepackage{authblk} -- This package specifically forbidden \usepackage{balance} -- This package specifically forbidden \usepackage{color \usepackage{CJK} -- This package specifically forbidden \usepackage{float} -- This package specifically forbidden \usepackage{flushend} -- This package specifically forbidden \usepackage{fontenc} -- This package specifically forbidden \usepackage{fullpage} -- This package specifically forbidden \usepackage{geometry} -- This package specifically forbidden \usepackage{grffile} -- This package specifically forbidden \usepackage{hyperref} -- This package specifically forbidden \usepackage{navigator} -- This package specifically forbidden \indentfirst} -- This package specifically forbidden \layout} -- This package specifically forbidden \multicol} -- This package specifically forbidden \nameref} -- This package specifically forbidden \usepackage{savetrees} -- This package specifically forbidden \usepackage{setspace} -- This package specifically forbidden \usepackage{stfloats} -- This package specifically forbidden \usepackage{tabu} -- This package specifically forbidden \usepackage{titlesec} -- This package specifically forbidden \usepackage{tocbibind} -- This package specifically forbidden \usepackage{ulem} -- This package specifically forbidden \usepackage{wrapfig} -- This package specifically forbidden DISALLOWED COMMANDS \nocopyright -- Your paper published use command \addtolength -- This command may used \balance -- This command may used \baselinestretch -- Your paper published use command \clearpage -- No page breaks kind may used final version paper \columnsep -- This command may used -- No page breaks kind may used final version paper \pagebreak -- No page breaks kind may used final version paperr \pagestyle -- This command may used \tiny -- This acceptable font size. {0} May changed 1 2 section numbers desired. The file aaai21.sty style file AAAI Press proceedings, working notes, technical reports. Title Your title must mixed case, sentence case. That means verbs , nouns, adverbs, adjectives capitalized, including words hyphenated terms, articles, conjunctions, prepositions lower case unless directly follow colon long dash \title{Appendix} \begin{document} \maketitle"," The attention mechanism is a key component of the neural revolution in Natural Language Processing . As the size of attention-based models has been scaling with the available computational resources, a number of pruning techniques have been developed to detect and to exploit sparseness in such models in order to make them more efficient. The majority of such efforts have focused on looking for attention patterns and then hard-coding them to achieve sparseness, or pruning the weights of the attention mechanisms based on statistical information from the training data. In this paper, we marry these two lines of research by proposing Attention Pruning : a novel pruning framework that collects observations about the attention patterns in a fixed dataset and then induces a global sparseness mask for the model. Through attention pruning, we find that about 90\% of the attention computation can be reduced for language modelling and about 50\% for machine translation and %natural language inference \rev{prediction with BERT on GLUE tasks}, while maintaining the quality of  the results. Additionally, using our method, we discovered important distinctions between self- and cross-attention patterns, which could guide future NLP research in attention-based modelling. Our approach could help develop better models for existing or for new NLP applications, and generally for any model that relies on attention mechanisms. Our implementation and instructions to reproduce the experiments are available at \url{https://github.com/irugina/AP}."
"DEEP learning modern machine learning technique based artificial neural networks. The field natural language processing significantly benefited use deep learning techniques recent years . There three prevalent deep learning architectures concerned NLP tasks: long-short term memory %networks , transformer networks convolutional neural networks . LSTMs exhibit relatively slow inference speeds less performant transformers CNNs regards text classification accuracy . Transformers recent innovation shown significant successes many NLP tasks . Their massive complexity trainable parameters order hundreds millions presents critical experiment reproducibility challenges researchers. State-of-the-art transformers difficult reproduce lab conditions high training cost monetary terms. There limited number pre-trained transformer models available different languages. \par CNNs demonstrated excellent success text classification tasks . There two paradigms available using CNNs text classification tasks, namely: world-level character-level CNNs . \par Word-level approaches dependant word-model represent text. The reliance pre-trained word-model poses potential problem one available particular language. Training new word models computationally time-consuming costly. There also technical challenges dealing misspellings words may exist word-model. The paradigm char-CNNs. No pre-trained language word models required. They also require costly pre-processing step text data. In general, char-CNNs accurate word-level CNNs transformers. Adding depth given benefit improved classification accuracy, seen image classification tasks. There open question research literature optimal architecture char-CNNs. Little research performed address limitations. Deep learning iterative process requiring tuning many hyper-parameters repeated experiments test efficacy potential architecture. It time consuming, costly tedious process requires expert skills domain knowledge. The task finding optimal char-CNNs NP-hard problem. \par Evolutionary computation collection search algorithms inspired principals biological evolution, particular concept survival fittest. EC methods use population individuals conduct simultaneous search limited time frame improve optimisation specified objective function via exchange information individuals population. The exchange information one key motivating factors selecting EC methods evolving char-CNNs work. There potential information exchange may reveal essential characteristics makes non-performant char-CNN performant one. EC methods concerned locating near-optimal solutions NP-hard problems. \par Evolutionary deep learning technique using EC methods search candidate CNN architectures combined backpropagation algorithm train potential candidate network architecture. EDL demonstrated success searching performant CNN architectures image classification tasks . EDL used search performant char-CNN architectures. \par Motivated success applying EDL techniques image classification domain, propose novel surrogate-based EDL algorithm appropriate searching landscape char-CNN architectures text classification domain. The proposed algorithm based genetic programming indirect encoding capable representing novel char-CNN architectures. The algorithm employs use surrogate models significantly reduce training time candidate char-CNNs evolutionary process. In summary, contributions proposed algorithm work are: %------------------------------------------------------------------------------ We motivated Attention Pruning novel method pruning attention leveraging data-informed sparseness. By performing controlled experiments broad range tasks , demonstrated prune computations using pre-computed attention patterns maintaining comparable performance, sometimes even achieving improvements. We applied AP method seq2seq tasks, allowed us study attention patterns self- cross-attention, result discovered important distinctions two types attention. conducted controlled study find means incorporating positional awareness attention mechanisms. We observed positional awareness induces beneficial sparseness attention matrices, thus devised simple training procedure exploits sparseness. As result, demonstrated faster accurate Transformers. In future work, plan evaluate method models, NLP tasks, datasets various sizes. We also plan implement Attention Pruning efficiently existing hardware. We conjecture ``co-design'' approaches efficient sparse kernels successful utilization would helpful making Attention Pruning scalable. Therefore, release code, currently relies masking matrix multiplications GPU, community encourage co-design efforts attention pruning. Finally, would like explore usefulness using AP method guiding modelling NLP larger set NLP tasks well real-world applications."," Character-level convolutional neural networks  require no knowledge of the semantic or syntactic structure of the language they classify. This property simplifies its implementation but reduces its classification accuracy. Increasing the depth of char-CNN architectures does not result in breakthrough accuracy improvements. Research has not established which char-CNN architectures are optimal for text classification tasks. Manually designing and training char-CNNs is an iterative and time-consuming process that requires expert domain knowledge. Evolutionary deep learning  techniques, including surrogate-based versions, have demonstrated success in automatically searching for performant CNN architectures for image analysis tasks. Researchers have not applied EDL techniques to search the architecture space of char-CNNs for text classification tasks. This article demonstrates the first work in evolving char-CNN architectures using a novel EDL algorithm based on genetic programming, an indirect encoding and surrogate models, to search for performant char-CNN architectures automatically. The algorithm is evaluated on eight text classification datasets and benchmarked against five manually designed CNN architectures and one long short-term memory  architecture. Experiment results indicate that the algorithm can evolve architectures that outperform the LSTM in terms of classification accuracy and five of the manually designed CNN architectures in terms of classification accuracy and parameter count."
". % % % final paper: en-us version % % % space normally used marker % This work licensed Creative Commons % Attribution 4.0 International License. % License details: % \url{http://creativecommons.org/licenses/by/4.0/}. } Pre-trained language models received great interest natural language processing community last recent years . These models trained semi-supervised fashion learn general language model, example, predicting next word sentence . Then, transfer learning used leverage learned knowledge down-stream task, text-classification . \citet{devlin_bert:_2019} introduced ``Bidirectional Encoder Representations Transformers'' , pre-trained language model based Transformer architecture . BERT deeply bidirectional model pre-trained using huge amount text masked language model objective goal predict randomly masked words context . The fact is, BERT achieved state art results ``General Language Understanding Evaluation'' benchmark training single, task-specific layer output fine-tuning base model task. Furthermore, BERT demonstrated applicability many natural language tasks since including limited sentiment analysis , relation extraction word sense disambiguation , well adaptability languages English . However, fine-tuning data set often contains thousands labeled data points. This plethora training data often available real world scenarios . In paper, focus low-resource setting less 1,000 training data points. Our research attempts answer question pool-based active learning used increase performance text classifier based Transformer architecture BERT. That leads next question: How layer freezing techniques , i.e. reducing parameter space, impact model training convergence fewer data points? To answer questions, explore use recently introduced Bayesian approximations model uncertainty data selection potentially leads faster convergence fine-tuning introducing new data points maximize knowledge gain model. To best knowledge, work presented paper first demonstration combining modern transfer learning using pre-trained Transformer-based language model BERT model active learning improve performance low-resource scenarios. Furthermore, explore effect trainable parameters reduction model performance training stability analyzing layer-wise change model parameters reason selection layers excluded training. %Furthermore, explore whether sophisticated decoder architecture, i.e. convolutional neural networks improve overall performance added complexity hinders fast model adaption little training data. The main findings work summarized follows: a) found model's classification uncertainty unseen data approximated using Bayesian approximations therefore, used efficiently select data manual labeling active learning setting; b) analyzing layer-wise change model parameters, found active learning strategy specifically selects data points train first thus general natural language understanding layers BERT model rather later thus task-specific layers. This work proposed evolutionary deep learning approach discover performant char-CNN architectures. This goal achieved implementation genetic programming-based algorithm coupled reduced cellular encoding scheme backpropogation algorithm. The SurDG-EC algorithm located, average, higher accuracy models located SurDG-Random. The fittest evolved phenotype defeated one state-of-the-art char-CNN models achieved comparable results state-of-the-art VDCNN-29 architecture. The evolved model also generalised favourably across unseen datasets. There clear evidence width may potentially add efficacy char-CNNs.This mean width always result increased accuracy, also observed results. There many factors consider. It known much efficacy evolved phenotypes due increased width unknown variable combination variables. There are, however, clear indications importance width researched. The SurDG-EC algorithm also revealed two interesting properties char-CNNs. Building rich tapestry feature representations early stages network potentially aids improving accuracy networks grow deeper - turn constructing hierarchy relations rich feature tapestry. The evolutionary crossover operation also revealed combing widths two phenotypes produced wider phenotype greater validation accuracy. This clue may value making char-CNNs increased width. ------------------------------------------------------------------------------","     Recently, leveraging pre-trained Transformer based language models in down stream, task specific models has advanced state of the art results in natural language understanding tasks. However, only a little research has explored the suitability of this approach in low resource settings with less than 1,000 training data points. In this work, we explore fine-tuning methods of BERT - a pre-trained Transformer based language model - by utilizing pool-based active learning to speed up training while keeping the cost of labeling new data constant. Our experimental results on the GLUE data set show an advantage in model performance by maximizing the approximate knowledge gain of the model when querying from the pool of unlabeled data. Finally, we demonstrate and analyze the benefits of freezing layers of the language model during fine-tuning to reduce the number of trainable parameters, making it more suitable for low-resource settings."
"% % The following footnote without marker needed camera-ready % version paper. % Comment instructions uncomment 8 lines % ""final paper"" variant English. % . % % % final paper: en-us version % % space normally used marker This work licensed Creative Commons Attribution 4.0 International License. License details: \url{http://creativecommons.org/licenses/by/4.0/}. } Multilingual relation extraction important problem NLP, facilitating diverse set downstream tasks autopopulation knowledge graphs question answering . While early efforts relation extraction used supervised methods rely fixed set predetermined relations, research since shifted identification arbitrary unseen relations language. In paper, present method extracting high quality relation training examples date-marked news articles. This technique leverages predictable distributional structure articles build corpus denoised . We use corpus learn general purpose relation representations evaluate quality few-shot standard relation extraction benchmarks English Spanish little task-specific fine-tuning, achieving comparable results significantly data-intensive approach current state-of-the-art. The current state-of-the-art model, ``Matching Blanks"" MTB, distant supervision technique provides large gains many relation extraction benchmarks builds Harris' distributional hypothesis extensions. ~ assume informational redundancy large text corpora results sentences contain pair entities generally expressing relation. Thus, encoder trained collocate sentences used identify relation entities sentence finding labeled relation example whose embedding closest . While~ achieve state-of-the-art FewRel SemEval 2010 Task 8, approach relies huge amount data, making difficult retrain English language standard computational resources: fine-tune BERT large, mil parameters, mil+ relation pair statements batch size mil steps. In contrast method, relations statements language-model one-third size, achieves comparable performance fine-tuned little task-specific data. Our main contribution distant supervision approach assume sections news corpora exhibit even informational redundancy Wikipedia. Specifically, news days following event frequently re-summarizes event adding new details. As result, news exhibits strong form local consistency short rolling time windows otherwise fluid relations entities remain fixed. For example, relation Italy France expressed random piece text dynamic context-dependent, spanning wide range possibilities include ``enemies"", ``neighbors"" ``allies"". But, news coverage following 2006 World Cup, static -- sporting competitors. Therefore, considering sentences around specific events, extract groups statements express relation relatively free noise . Training multilingual BERT denoised corpus yields relation representations adapt well resource-constrained downstream tasks: evaluate quality FewRel SemEval 2010 Task 8, producing near state-of-the-art results finetuned little task-specific data. In addition strong performance approach English, easily generalizable languages, requiring news corpora event descriptions Wikipedia build high-quality training corpus. We evaluate Spanish find method outperforms mBERT TAC KBP 2016 relation corpus. We share code allow researchers apply approach news corpora own. In paper, evaluated performance pre-trained Transformer model - BERT - active learning scenario text classification low-resource settings. We showed using Monte-Carlo Dropout classification architecture effective way approximate model uncertainty unlabeled training elements. This technique enables us select data annotation maximize knowledge gain model fine-tuning process. Experimental results GLUE data set show improves model performance training stability. Finally, order improve efficiency fine-tuning process small amount data, explored reduction trainable model parameters freezing layers BERT model certain level depth. Comparing exclusion layers front back BERT model training, found advantageous training stability freezing layers closest output. We attribute effect reduction free parameters change little short training period low-resource setting. The exploration aspect subject future work combining observations layer-wise MAD previous advances language model fine-tuning sophisticated training strategies like gradual unfreezing discriminative fine-tuning . \ifcolingfinal"," General purpose relation extraction has recently seen considerable gains in part due to a massively data-intensive distant supervision technique from that produces state-of-the-art results across many benchmarks. In this work, we present a methodology for collecting high quality training data for relation extraction from unlabeled text that achieves a near-recreation of their zero-shot and few-shot results at a fraction of the training cost. Our approach exploits the predictable distributional structure of date-marked news articles to build a denoised corpus -- the extraction process filters out low quality examples. We show that a smaller multilingual encoder trained on this corpus performs comparably to the current state-of-the-art  on few-shot and standard relation benchmarks in English and Spanish despite using many fewer examples ."
"Domain shift common language applications. One likely find ""internet"" ""PC"" reviews electronics books, likely find ""writing"" ""B.C."" reviews books electronics. This proposes fundamental challenge NLP many computational models fail maintain comparable level performance across domains. Formally, distribution shift happens model trained data one distribution , goal make good predictions distribution shares label space source. We study unsupervised domain adaptation work, fully-labeled data source domain labeled data target domain. The prevailing methods field aim learn domain-invariant feature aligning source target domains feature space. The pioneering works field try bridge domain gap discrepancy-based approach. first introduce MMD measure domain discrepancy feature space use variant MK-MMD objective minimize domain shift. Another line work introduces domain classifier adversarial training induce domain invariant feature, followed works using generative models enhance adversarial training. However, note MMD-based approach adversarial training formulates minimax optimization procedure widely known hard converge satisfactory local optimum. Moreover, recent works discovered guarantee good adaptation introduce inevitable error target domain label distribution shift may render incorrect distribution matching. For example, thinking binary classification task, source domain 50\% positive samples 50\% negative samples target domain 30\% postive 70\% negative. Successfully aligning distributions representation space requires classifier predict fraction positive negative source target. If one achieves 100\% accuracy source, target accuracy 80\%, 20\% error best. % Self-supervised learning prominent feature representation learning. Recent works approached unsupervised domain adaptation computer vision SSL[][]. [] adopted rotation prediction, flip prediction patch location prediction induce domain-invarint feature find auxiliary tasks involving fine-grained semantics like pixel reconstruction may force model focus domain-specific feature, widening domain gap. Self-supervised representation learning could good workaround problem enforces predictive behaviour matching instead distribution matching. The main idea learn discriminative representation able genenralize across domains. use sentiment-indicating pivot prediction auxiliary task cross-domain sentiment analysis. The method proposed paper adopts contrastive learning extract generalizable discriminative feature. Contrastive learning subclass self-supervised learning gaining popularity thanks recent progress. It utilizes positive negative samples form contrast queried sample pretext tasks order learn meaningful representations. However, pretext tasks must carefully chosen. shows experiments computer vision tasks transfer performance suffer improper pretext tasks like pixel reconstruction. % Recent developments contrastive learning obtained promising results representation learning benchmarks CV[][][] NLP[][][]. % Like self-supervised learning[], joint learning pretext tasks contrastive learning able align domain feature space, illustrated figure. There group works adopting domain adaptation CV[][][]. However, method cannot easily adopted NLP due inherent signal difference domains. . Therefore, paper explore two classic data augmentation methods natural language processingynonym substitution back translation define pretext task. Experiments two cross-domain sentiment classification benchmarks show efficacy proposed method. We also examine whether in-domain contrastive learning entropy minimization helps cross-domain sentiment classification varied label distribution settings. Our main contributions work summarized follows: We present event-guided denoising approach relation extraction corpus creation that, used current state-of-the-art training procedure, achieves comparable results English low-resource regime fraction training cost. It also performs well Spanish, demonstrating adaptability resource-constrained relation extraction tasks non-English languages. Our technique affords broader research community ability approximate current state-of-the-art relation extraction significantly lowering associated training costs. However, requires fairly large date-marked news corpus may available low resource languages. We leave exploration broader language coverage minimal required corpus size future work. One promising direction expanding language coverage cross-lingual learning via ``codeswitched"" examples language modeling losses . We hypothesize methods could help knowledge transfer among languages improve results downstream tasks. Finally, note since approach extracts relation statements news corpora, likely resulting distribution underlying relation types different distribution found Wikipedia. For example, Wikipedia may contain expressions standard ontological relations characteristic factoids. Despite hypothesized difference, approach performs well FewRel SemEval 2010 Task 8, include subset relation types. In future intend investigate differences implications closely. Acknowledgments: AIDA, partially supported","   Contrastive learning  has been successful as a powerful representation learning method. In this paper, we propose a contrastive learning framework for cross-domain sentiment classification. We aim to induce domain invariant optimal classifiers rather than distribution matching. To this end, we introduce in-domain contrastive learning and entropy minimization. Also, we find through ablation studies that these two techniques behaviour differently in case of large label distribution shift and conclude that the best practice is to choose one of them adaptively according to label distribution shift. The new state-of-the-art results our model achieves on standard benchmarks show the efficacy of the proposed method."
"%Recently, Neural machine translation~ achieved great success reached satisfactory translation performances several language pairs~. %These NMT models sequence-to-sequence models trained large parallel data. % Ensemble learning, aggregates multiple diverse models inference, attracted huge interest academia industry communities thanks effectiveness variety computational intelligence problems classification, prediction function approximation. So far, many aggregating approaches developed bagging boosting improve practical performance. % % Ensemble learning primarily used improve classification task reduce likelihood poorly learned model. % Recently, ensemble different neural networks greatly improved accuracy neural machine translation , making vital widely used technique state-of-the-art Neural NMT systems. In scenario NMT, common implementation average probability token computed different individual models decode averaged probabilities. Previous studies show performance ensemble method heavily depends accuracy diversity base models, typically obtained independent training different sets attributes. % % Ensemble learning, aggregates multiple models inference, % Despite success various tasks applications, practice common challenges ensemble methods, prevent wide usage: 1) High computational cost. For ensemble learning, individual models conduct encoding decoding, prohibitively time memory consuming. It gets even worse context NMT due large size state-of-the-art networks like transformer. 2) Absence monolingual data. Ensemble exploit independence cannot make full use large scale monolingual data source side. Recently, self-training method shown remarkable success image recognition. % Taking advantage unlabeled data, Trained noisy augmented data, EfficientNet model finetuned self-training achieve 87.4\% top-1 accuracy ImageNet, 1.0\% better state-of-the-art model requires 3.5B weakly labeled images. Typically, self-training first train base model labeled data, utilize learned model label unannotated data. Finally, labeled pseudo data combined training set yield next level model. In context natural language processing, many works successfully applied self-training technique including word sense disambiguation parsing. % Nevertheless, performance gains achieved self-training still limited structured prediction tasks Neural Machine Translation~ target space vast. Originally designed classification problems, previous work suggests self-training effective predictions unlabeled samples good enough, otherwise suffer notorious reinforced mistakes. However, problem common NMT scenario, hypotheses generated single model often far away ground-truth target due compositionality target space. \citet{zhang2016exploiting} found training biased pseudo data may accumulate mistakes time step enlarge error, thus propose freeze decoder parameters training pseudo parallel data may negatively impact decoder model NMT. % We argue performance drop self-training NMT mainly comes reinforced mistakes. To overcome issue, paper borrow reciprocal teaching concept educational field revisit core idea classic ensemble approaches. Ensemble built upon assumption different models different inductive biases better predictions made majority voting. We propose replace self-supervision Reciprocal-Supervision NMT, leading novel co-EM scheme named \method. In \method, use multiple separately learned models provide diverse proper pseudo data, allowing us enjoy independence different models dramatically reduce error strategic aggregation. %Most NMT works use one type neural network model ConvS2S~ Transformer~. %Usually, different neural models different performances may also catch minor different patterns sequences. More specifically, first learn multiple different models parallel data. Then E-step individual models used translate monolingual data. And M-step generated pseudo data produced different models combined tune student models. %To combine advantages diversities, intuitive method ensemble, several models trained every model used inference, output models combined better prediction. \method inspired success ensemble method. However, ensemble resource-demanding inference, prevents wide usage. Besides, cannot make use large scale monolingual data source side. %The teacher-student framework used make one model learn others. Some works done explore assistance right-to-left decoding model usual left-to-right model~. These works shown regular NMT model learn right-to-left decoding model obtain better performance. However, best knowledge, work exploring assistance several different models. So work, try utilize multiple different models teachers, train student model learn them. Through procedure, student model better performance. %Following procedure, another advantage monolingual data source side language easily utilized extend training method self-training framework diverse teachers. %Similar teacher-student framework zero-shot NMT~, student model also learn teachers monolingual data. \method also related data augmentation approaches NMT. While previous works concentrate monolingual data target side back-translation~, pay attention source side. Knowledge distillation another relevant research topic. However, KD preliminary designed improve weak student model much stronger teacher model. By contrast, \method boosts performance base models reciprocal-supervision comparable even weaker learners. % Unsupervised machine translation~ also seen utilizing target side monolingual data. To best knowledge, first self-training framework reciprocal-supervision, correct bias model fully utilize monolingual data source side language. More precisely, advantages \method % cooperative-supervised framework diverse parameterized networks summarized follows: Through extensive experiments, \method achieves significant gains several standard translation tasks including En\{Ro, De\}. Surprisingly, also found \method much weaker learners could even outperform strong BERT enhanced NMT model big margins. We proposed powerful easy deploy approach augment text data conditional generation. By leveraging off-the-shelf language model , successfully guide generation towards specified direction , help reinforcement learning. We find Data Boost improves performance classification tasks, classifier-agnostic, surpasses several prior augmentation methods three diverse classification tasks. In future, plan implement sophisticated guidance augmentation adding syntactic position features reward function, enable augmentation diverse types text data. The code made available upon request.","  % Neural machine translation~ has achieved great success with the help of large amount of parallel data. % However, different model architectures have different advantages and translation abilities, but it is hard to integrate them all together to one model. % The ensemble method is too time-consuming for inference. % Besides, monolingual data are also not fully utilized. % Some works such as back-translation and unsupervised machine translation have tried to utilize monolingual data of target side, whereas the utilization of source side monolingual data still need be further explored. % In this work, we propose a self-training framework with diverse teachers to make one model be able to learn advantages and diversities from other models, and monolingual data of source side language can also be utilized to further improve the translation performances. % This method is very simple but much effective. % Empirical results show that our method can obtain further improvements on the standard En$\to$De and En$\to$Fr translation tasks.  Despite the recent success on image classification, self-training has only achieved limited gains on structured prediction tasks such as neural machine translation . This is mainly due to the compositionality of the target space, where the far-away prediction hypotheses lead to the notorious reinforced mistake problem. In this paper, we revisit the utilization of multiple diverse models and present a simple yet effective approach named Reciprocal-Supervised Learning . \method first exploits individual models to generate pseudo parallel data, and then cooperatively trains each model on the combined synthetic corpus. \method leverages the fact that different parameterized models have different inductive biases, and better predictions can be made by jointly exploiting the agreement among each other. Unlike the previous knowledge distillation methods built upon a much stronger teacher, \method is capable of boosting the accuracy of one model by introducing other comparable or even weaker models. \method can also be viewed as a more efficient alternative to ensemble. Extensive experiments demonstrate the superior performance of \method on several benchmarks with significant margins.\footnote{Code is available at \url{https://github.com/MinkaiXu/RSL-NMT}.} % \method takes advantage of different parameterized networks to generate diverse proper pseudo parallel data, and then dramatically reduce the bias through strategic combination of the pseudo data. %More specifically, we first train several NMT teachers with heterogeneous networks, then use the heterogeneous teacher models to label unlabeled data respectively and finally use the labeled data and unlabeled data to jointly train a student NMT model. % \method is very simple but much effective. % Empirical results demonstrate the effectiveness of \method on several benchmarks, where we even outperforms a strong BERT-enhanced baseline.   % Ensemble learning, which strategically aggregates multiple models for inference, has been shown effective to improve the accuracy of Neural Machine Translation . However, in practice it cannot be widely adopted due to the high computation and memory cost for involving all individual models.  % % Recently, transductive method has been proposed to overcome this obstacle, which, however, suffers the premise that the test data has to be available in advance.  % In this paper, we present a simple yet effective approach named Cooperative Training NMT , where we firstly use individual models to translate the source corpus into pseudo parallel data, and then cooperatively train all models on the translated synthetic corpus. \method leverages the fact that different parameterized models have different inductive biases, and better predictions can be made by jointly exploiting the independence between each other. Furthermore, given source monolingual data, \method enables us to avoid the reinforced mistakes problem of self-training and make the most of the monolingual set. Extensive experiments demonstrate our proposed approach can always achieve superior or comparable performance on several benchmarks with less computational cost."
"One first steps language acquisition learn word--meaning mappings, e.g., word ``dog'' sentence ``see dog'' refers tail-wagging animal kitchen table. This seemingly simple problem word learning complex puzzle; initial phases language development, children knowledge word meanings face great deal uncertainty. % Without prior information, given word , high level referential uncertainty -- great number potential meanings child's environment word could refer to. % Similarly, high level linguistic uncertainty mapping referent words utterances . % Moreover, additional difficulty arises mappings words referents one-to-one; sometimes words mapped one referent referents mapped one word . Strong empirical evidence suggests statistical cross-situational learning helps children adults navigate challenges, gradually keeping track statistical regularities across different situations , using help resolve ambiguous mappings \citep[\eg,][]{yu.smith.2007,smith.yu.2008}. However, cross-situational learning provide detailed account mechanisms responsible resolving type uncertainty different stages word learning. Moreover, large body developmental research studied inductive biases might facilitate word learning presence different types uncertainty \citep[\eg,][]{markman.1987}. A common theme among biases competition remove number possible hypotheses word meaning . For example, mutual exclusivity bias asserts referent mapped one word . % This competition among referents means given new word number possible referents, learner reduces uncertainty considering referents already associated words. It also suggested competitive processes play role locally globally: competition associating words meanings one observation well among observed words referents \citep[\eg,][]{yurovsky2013competitive}.\footnote{Work computational modeling cross-situational learning typically distinguish referent indicated word meaning. We use terms referent meaning interchangeably throughout paper, recognizing important notions relations two abstracted away approach.} Previous computational modeling work shed light mechanisms biases might involved cross-situational learning \citep[\eg,][]{frank.etal.2007, fazly.etal.2010.cogsci,trueswell.etal.2013,nematzadeh.etal.2017.cogsci.bias}. % However, knowledge, previous work done exhaustive analysis role competition in-the-moment learning mechanisms mechanisms interact different representations word meanings, may also influenced competition. % In work, contributions threefold: We provide general probabilistic formulation cross-situational word-learning show influential model \citet{fazly.etal.2010.csj} instance formulation. Using formulation, show % inductive biases modeled competitive processes in-the-moment overall word learning, well comprehension word meaning. Moreover, examine modeling choice affects learning presence different sources uncertainty, increased referential linguistic uncertainty, fewer exposures words, acquiring homonyms synonyms. We find best model across tasks one implements two types competition, among words referents. Moreover, competition happens in-the-moment learning comprehension . This result different previous modeling assumptions competition among referents introduced overall learning word meaning representations . It also suggests observed behavior people \citep[\eg,][]{yurovsky2013competitive} might explained competition comprehension global competitive process learning. We also observe best model performs better model \citet{fazly.etal.2010.csj} presence linguistic referential uncertainty, learn homonyms opposed model. In paper, propose reciprocal supervised learning, efficient effective co-EM framework neural machine translation. Different previous methods, \method strong NMT model benefit comparable even weaker models, source monolingual corpus also fully utilized seamlessly. Extensive experiments demonstrate effectiveness robustness \method provide insights \method work well. \method general framework extended NLP tasks, e.g., Q\&A, text summarization. One potential direction future work design better objective functions set learnable weights pseudo data different models. Second, make \method efficient another interesting topic. By \method, performances neural network models greatly improved experiments show considerable results standard EnDe EnFr translation tasks. Our \method rather simple effective. This must first 5 lines tell arXiv use pdfLaTeX, strongly recommended. \pdfoutput=1 In particular, hyperref package requires pdfLaTeX order break URLs across lines. \documentclass[11pt]{article} Remove ""review"" option generate final version. \usepackage[review]{naacl2021} \usepackage{naacl2021} Standard package includes \usepackage{times} \usepackage{latexsym} For proper rendering hyphenation words containing Latin characters \usepackage[T1]{fontenc} For Vietnamese characters \usepackage[T5]{fontenc} See https://www.latex-project.org/help/documentation/encguide.pdf character sets This assumes files encoded UTF8 \usepackage[utf8]{inputenc} This strictly necessary, may commented out, improve layout manuscript, typically save space. \usepackage{microtype} If title author information fit area allocated, uncomment following \setlength\titlebox{<dim>} set <dim> something 5cm larger. \newcommand{\method}{\xspace} \usepackage{microtype} \usepackage{graphicx} \usepackage{amsfonts,amssymb,amsmath} \usepackage{mathtools} \usepackage{tabu} \usepackage{multirow} \usepackage{algorithm} \usepackage{algorithmic} \usepackage{xspace} \usepackage{booktabs} To thicken table lines \title{Reciprocal Supervised Learning Improves Neural Machine Translation} Author information set various styles: For several authors institution: \author{Author 1 \and ... \and Author n \\ Address line \\ ... \\ Address line} names fit well one line use Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\ For authors different institutions: \author{Author 1 \\ Address line \\ ... \\ Address line \And ... \And Author n \\ Address line \\ ... \\ Address line} To start seperate ``row'' authors use \AND, \author{Author 1 \\ Address line \\ ... \\ Address line \AND Author 2 \\ Address line \\ ... \\ Address line \And Author 3 \\ Address line \\ ... \\ Address line} \author{Minkai Xu\textsuperscript{*\rm 1,2}, Mingxuan Wang\textsuperscript{\rm 3}, Zhouhan Lin\textsuperscript{\rm 4}, Hao Zhou\textsuperscript{\rm 3}, Weinan Zhang\textsuperscript{\rm 4}, Lei Li\textsuperscript{\rm 3} \\ \textsuperscript{\rm 1}University Montreal \textsuperscript{\rm 2}Mila - Quebec AI Institute \textsuperscript{\rm 3}ByteDance AI Lab \textsuperscript{\rm 4}Shanghai Jiao Tong University\\ \texttt{minkai.xu@umontreal.ca}\\ \texttt{\{wangmingxuan.89,zhouhao.nlp,lileilab\}@bytedance.com}\\ \texttt{lin.zhouhan@gmail.com}\\ \texttt{wnzhang@sjtu.edu.cn}} \begin{document} \maketitle \renewcommand{\thefootnote}{\fnsymbol{footnote}}"," Children learn word meanings by tapping into the commonalities across different situations in which words are used and overcome the high level of uncertainty involved in early word learning experiences. In a set of computational studies, we show that to successfully learn word meanings in the face of uncertainty, a learner needs to use two types of competition: words competing for association to a referent when learning from an observation and referents competing for a word when the word is used."
"Hypernym, sometimes also known hyperonym, term linguistics referring word phrase whose semantic field covers hyponym. The common relationship hypernym hyponym ``is-a'' relationship. For example, ``red color'' provides relationship ``red'' ``color'', ``color'' hypernym ``red''. The hypernym-hyponym relation essential element semantic network corresponding tasks related semantic network analysis . The hypernym graph built collection hyponym-hypernym relations enhance accuracy taxonomy induction . The linkage hyponym hypernym used improve performance link prediction network completion knowledge graph semantic network . In natural language processing , hyponym-hypernym relation help named entity recognition , question-answering tasks ``what is'' ``is a'' . The data mining, information search retrieval also benefit hyponym-hypernym relation . Given role application hypernym-hyponym relation, essential explore automatic method extract relation two entities, presents important task knowledge-driven NLP . Following landmark work focusing lexico-syntactic patterns , several pattern-based methods developed hypernym extraction . Then feature-based classification methods introduced , applies machine learning tools enhance recall rate. Recently, distributional methods hybrid distributional models successfully applied learn embedding words, based hypernym-hyponym relation inferred . The deep learning approach also effective many sequence labeling tasks including hypernym extraction . While extraction hyponym-hypernym relation done many different environments, work focus hypernym extraction definitions. More specifically, definition refers short statement description word. Take word ``red'' example, whose definition Wikipedia ``Red color end visible spectrum light, next orange opposite violet.'' The aim identify word ``color'' hypernym ``red'' nouns definition. Intuitively, task solved general resources WordNet dictionary Wikipedia. But given word's different meanings different contexts, resources sufficiently complete task. As example, term ``LDA'' Wikipedia denotes ``Linear Discriminant Analysis'' machine learning, ``Low dose allergens'' medicine, ``Landing distance available'' aviation. The combination general resources context identification would also fail domain-specific applications general resources cover special technical terms area. Moreover, existing technical approaches also demonstrate certain limitations task hypernym extraction definitions, summarize follows: To briefly illustrate difficulty, let us consider definition Stack-Overflow irregular format: ``fetch-api: fetch API improved replacement XHR''. The term ``fetch-api'' included common dictionary. While definition ``is an'' pattern, connect hypernym. The definition short every distinct word definition appears once, makes difficult accurately learn word representation. Overall, challenging find method would accurately identify ``API'' correct hypernym. The definition word represents certain type knowledge extracted collected disordered data. Indeed, tools capable extracting definitions corpora good accuracy . Nevertheless, tools extract hypernym definitions remain limited. % To cope issue, propose recurrent network method using syntactic features. Because definition directly points noun, hyponym already given. Therefore, hypernym extraction identify correct hypernym words definition sentence. This task considered binary classification, classifier judges candidate noun hypernym not. In order better learn syntactic feature, transfer definition sentence part speech sequence labeling PoS word standard tool . The syntactic structure surrounding candidate learned bidirectional gated recurrent units based model. To fine tune results, use set features including centrality word hypernym co-occurrence network. We use two corpora evaluate method. One Wikipedia, featuring definitions canonical syntax structure intensively used previous studies. The Stack-Overflow, whose definition domain-specific usually irregular format. Our method compared several existing ones. Overall, outperforms others corpora, demonstrates advantage combing tool RNN PoS information task hypernym extraction. This paper organized follows. We review related works Section introduce details method Section . Experiments evaluations proposed model presented Section . After that, draw conclusion research Section . The computational level analysis allows us contemplate problem cognitive phenomenon solves. For example, learning word meanings via cross-situational statistics formulated finding mappings words referents consistent observations. On hand, modeling cognition algorithmic level plays important role providing insight cognitive mechanisms; requires specifying details algorithms representations turn enables us study role interaction different stages learning. Previous research studied word learning algorithmic computational levels \citep[\eg,][]{siskind.1996,yu.ballard.2007,frank.etal.2007,fazly.etal.2010.csj}. We proposed framework modeling cross-situational word learning computational level unifies previous work domain -- approaches formulate word learning translation problem. We also show instantiating framework results different word learning models algorithmic level: model specific inductive biases define words referents compete association strength given word/referent. More specifically, examine competition among words referents plays role in: learning given observation in-the-moment learning, overall learning word meanings, comprehension given word. Moreover, investigate assumptions change performance model face uncertainty. Our results show models implement two complementary types referent word competition perform best. Each competition type addresses specific type uncertainty -- word referent competitions address linguistic referential certainty, respectively; word learning input uncertainties, important model implement two competitions. These models robust learn successfully examples. Moreover, find best model implements competition in-the-moment learning comprehension, global competition word meaning representations. By avoiding overall word meaning competition, model able successfully learn multiple meanings ambiguous words, given sufficient evidence. \section{The Derivation FAS model} The FAS model assumes referents generated independently given utterance ; instead calculating \Equation{eq:cll}, likelihood defined conditional probability referents given utterance: Here, alignment variable defines mappings words given referent. More specifically, value alignment variable selects word utterance mapped given referent . returns association referent given learned distribution . Note corresponds Expectation step EM algorithm, \Equation{eq:emfasalign} instantiation \Equation{eq:emalign}. \break \break In Maximization step, new value calculated finding maximizes model likelihood: set scenes utterances, respectively: word mapped , number times co-occurred corpus . The FAS model assumes conditional probability, . This means additional dependence assumption learned representations: word distribution features, thus given word, features compete associated word. To impose new assumption representation, constraint added expectation defined \Equation{eq:mstepfas}: Note Lagrange multipliers ensures new constraint -- distribution referents word -- satisfied. To find maximizes expectation \Equation{eq:mstepfas}, derivative objective function calculated equated zero: Given , calculate : Using \Equation{eq:emfasalign} calculate alignment probabilities, have: We approximate adding current alignment probability, , sum previously calculated ones . This approach approximation value alignment probability changes processing - pair, calculated incrementally; FAS defined association score, updated model process - pairs, initial value zero. Intuitively, score shows overall association strength word referent captures strongly word-referent pair associated observation, -. \documentclass[12pt]{article} \usepackage[group-separator={,}]{siunitx} \usepackage[natbibapa]{apacite} \usepackage[american]{babel} \usepackage{newtxtext} \usepackage{newtxmath} \usepackage[utf8]{inputenc} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{capt-of} \usepackage{csquotes} \usepackage{appendix} \usepackage{graphicx} \usepackage{caption} \usepackage{subcaption} \usepackage[usenames, dvipsnames]{xcolor} \usepackage[flushleft]{threeparttable} \usepackage{array} \usepackage{tabularx} \usepackage{enumitem} \usepackage{kantlipsum} \usepackage{multirow} \usepackage{bm} \usepackage{amsmath} \usepackage{setspace} \usepackage{siunitx} \usepackage{etoolbox} \usepackage{makecell} \usepackage[normalem]{ulem} \usepackage{numprint} \npthousandsep{,} FOR COGSCI SUBMISSION \renewcommand{\baselinestretch}{1.5} \usepackage[margin=1in]{geometry} \renewcommand{\figurename}{Fig.} \documentclass[alpha-refs]{wiley-article} \documentclass[blind,num-refs]{wiley-article} Add additional packages required \usepackage{siunitx} Update article type known \papertype{Original Article} Include section journal known, otherwise delete \paperfield{Journal Section} \usepackage[round,authoryear]{natbib} \usepackage{hyperref} \renewcommand{\ref}[1]{\hyperref{#1}} \usepackage{setspace} \doublespacing \usepackage{url} \usepackage{graphicx} \usepackage{csquotes} \usepackage{pslatex} \usepackage{latexsym} \usepackage{caption} \usepackage{subcaption} \usepackage{xcolor} \usepackage{amsmath} Latin phrase short forms \newcommand\etal{et~al.\ } \newcommand\ie{i.e.} \newcommand\eg{e.g.} \newcommand\cf{cf.\ } Command display content \newcommand{\ignore}[1]{} Content-specific labels \newcommand{\Table}[1]{Table} \newcommand{\Algorithm}[1]{Algorithm} \newcommand{\Example}[1]{Ex.} \newcommand{\Figure}[1]{Figure} \newcommand{\Equation}[1]{Eqn.~} \newcommand{\Section}[1]{Section} \newcommand{\Appendix}[1]{Appendix} Author note commands \newcommand\scream[1]{{#1}} \newcommand\oldtext[1]{{#1}} \newcommand\an[1]{{#1}} \newcommand\sxs[1]{{SS: #1}} \newcommand\zs[1]{{ZS: #1}} \newcommand\tlg[1]{{TLG: #1}} \newcommand{\replace}[2]{{OLD:} \st{#1} {NEW: #2}} \newcommand\todo[1]{{#1}} \newcommand\fas{FAS} \newcommand\simf{\mathrm{sim}} \newcommand{\rep}[3]{\mathrm{\theta}^{#1}_{#2#3}} \DeclareMathOperator*{\argmax}{arg\,max} HERE \graphicspath{{plots/zero_unseen_prob/}} \graphicspath{{plots/}} \author[1\authfn{1}]{Aida Nematzadeh} \author[2\authfn{2}]{Zahra Shekarchi} \author[3\authfn{3}]{Thomas L. Griffiths} \author[4\authfn{4}]{Suzanne Stevenson} \contrib[1\authfn{1}, 2\authfn{2}]{Equally contributing authors.} Work done prior joining DeepMind.} Include full affiliation details authors \affil[1]{DeepMind} \affil[2]{University Toronto} \affil[3]{Princeton University} \affil[4]{University Toronto} \corraddress{Author One PhD, Department, Institution, City, State Province, Postal Code, Country} \corremail{nematzadeh@google.com} Include name author appear running header \runningauthor{Nematzadeh et al.} \clearpage \tableofcontents \clearpage \break"," % The abstract should briefly summarize the contents of the paper in % 150--250 words. The hyponym-hypernym relation is an essential element in the semantic network. Identifying the hypernym from a definition is an important task in natural language processing and semantic analysis. While a public dictionary such as WordNet works for common words, its application in domain-specific scenarios is limited. Existing tools for hypernym extraction either rely on specific semantic patterns or focus on the word representation, which all demonstrate certain limitations. Here we propose a method by combining both the syntactic structure in definitions given by the word part of speech, and the bidirectional gated recurrent unit network as the learning kernel. The output can be further tuned by including other features such as a word centrality in the hypernym co-occurrence network. The method is tested in the corpus from Wikipedia featuring definition with high regularity, and the corpus from Stack-Overflow whose definition is usually irregular. It shows enhanced performance compared with other tools in both corpora. Taken together, our work not only provides a useful tool for hypernym extraction but also gives an example of utilizing syntactic structures to learn semantic relationships \footnote{Source code and data available at \url{https://github.com/Res-Tan/Hypernym-Extraction}}.  \keywords{Hypernym Extraction \and Syntactic Structure \and Word Representation \and Part of Speech \and Gated Recurrent Units.}"
"Although neural machine translation achieved great success sentence-level translation tasks, many studies pointed translation mistakes become noticeable document-level. They proved mistakes alleviated feeding inter-sentential contexts context-agnostic NMT models. Previous works explored various methods integrate context information NMT models. They usually take limited number previous sentences contexts learn context-aware representations using hierarchical networks extra context encoders . Different representation-based approaches, ~\citeauthor{tu2018learning}~\shortcite{tu2018learning} ~\citeauthor{kuang-etal-2018-modeling}~\shortcite{kuang-etal-2018-modeling} propose using cache memorize context information, either history hidden states lexicons. To keep tracking recent contexts, cache usually updated new translations generated. Therefore, long-distance contexts would likely erased. How use long-distance contexts drawing attention recent years. Approaches, like treating whole document long sentence using memory hierarchical structures , proposed take global contexts consideration. However, \citeauthor{kim2019and}~\shortcite{kim2019and} point words document beneficial context integration, suggesting essential word focus relevant context. \footnotetext{Dependency coreference relations Stanford CoreNLP .} To address problem, suppose build document graph document, word connected words direct influence translation. Figure shows example document graph. Explicitly, document graph %for document defined directed graph where: node represents word document; edge represents one following relations words: adjacency; syntactic dependency; lexical consistency; coreference. We apply Graph Convolutional Network document graph obtain document-level contextual representation word, fed conventional Transformer model additional attention gating mechanisms. We evaluate model four translation benchmarks, IWSLT English--French Chinese--English , Opensubtitle English--Russian , WMT English--German . Experimental results demonstrate approach consistently superior previous works language pairs. The contributions work summarized as: The hyponym-hypernym relationship plays important role many NLP tasks. Despite intensive studies topic, tools accurately extract hypernym definition limited. The definition, representing special type summarized knowledge, commonly observed, corpora Wikipedia GitHub directly give definition term, also tools capable extracting definitions good accuracy. Hence, useful develop capable tool task. Here construct bidirectional GRU model patterns learning. We use PoS tags words surrounding hypernym feature. Our model outperforms existing methods general corpus domain-specific corpus . It also demonstrates good balance performance complexity, compared kernels Transformer Bert. More importantly, feature kernel ablation, show PoS feature indeed key element guarantees final performance. The application tool proposed Stack-Overflow would help us understand evolution technology, group users social network study, build semantic network domain computer science. The performance tool limited accuracy PoS tagging. Hence, would useful try develop methods Stanford-NLP tool. The use PoS feature may also potential text sequence labeling tasks, may advantages word embedding. All problems addressed future studies.","     Previous works have shown that contextual information can improve the performance of neural machine translation . However, most existing document-level NMT methods failed to leverage contexts beyond a few set of previous sentences. How to make use of the whole document as global contexts is still a challenge. To address this issue, we hypothesize that a document can be represented as a graph     that connects relevant contexts regardless of their distances. We employ several types of relations, including adjacency, syntactic dependency, lexical consistency, and coreference, to construct the document graph. Then, we incorporate both source and target graphs into the conventional Transformer architecture with graph convolutional networks. Experiments on various NMT benchmarks, including IWSLT English--French, Chinese-English, WMT English--German and Opensubtitle English--Russian, demonstrate that using document graphs can significantly improve the translation quality."
"Automatic summarization fundamental task natural language generation computational linguistics. It crucial help user quickly read understand daily events, continuously studied decades. . In paper, focus meeting summarization, extensively studied task field automatic summarization. Given multiple speakers corresponding utterances text, task calls generating shorter transcript, covering salient information entire meeting. An example shown Figure , includes 3 speakers utterances , , well human-written summary. Meeting summarization typically regarded kind abstractive summarization problem literature. The majority existing studies build summarization systems based sequence-to-sequence model, adopts sequence modeling strategy encoding utterances . Despite effectiveness approaches, typically use sequential text information ignoring important influences dialogue structure. We claim dialogue-specific structural information important meeting summarization. For example, dialogue discourse effective structural feature. As shown Figure , ``Contrast, ``Question-Answer ``Continuation three dialogue discourse relations, provide precise semantic relationships utterance. Specifically, see existing sequence modeling method unable generate correct summary results ), attributed system knowing opposed  proposal. Differently, dialogue discourse provide key information via labeling ontrast relationship, shown Figure . Accordingly, effectively integrate discourse relationship existing summarization model become crucial step meeting summarization. In paper, propose Dialogue Discourse-Aware Graph Convolutional Networks address problem. In detail, first convert entire meeting dialogue discourse labeling discourse graph, represents utterances discourse relationships vertices. Afterwards, additionally design six types directed edges one global vertex discourse graph facilitate information flow. Finally, employ graph convolutional network encode graph pass semantic representation RNN decoder. Besides, use question-answer discourse relationship construct pseudo-summarization corpus pre-training DDA-GCN. In conversation, question often sparks discussion, naturally, question used pseudo-summary subsequent discussions. We conduct experiments widely used AMI benchmark . Our approach outperforms various baselines. Moreover, analyze effectiveness dialogue discourse pseudo-summarization corpus. In end, give brief summary contributions: To best knowledge, first apply dialogue discourse model structure meeting meeting summarization; We design discourse-aware graph model encode entire meeting; Our model achieves new SOTA AMI dataset. In paper, propose graph-based approach document-level translation, leverages source target contexts. Graphs constructed according inter-sentential intra-sentential relations. We employ GCN-based graph encoder learn graph representations, fed NMT model via attention gating mechanisms. Experiments four translation tasks show proposed approach consistently improves translation quality across different language pairs. Further analyses demonstrate effectiveness graphs capability leveraging long-distance context. In future, would like enrich types relations cover document phenomena."," Sequence-to-sequence methods have achieved promising results for textual abstractive meeting summarization. Different from documents like news and scientific papers, a meeting is naturally full of dialogue-specific structural information. However, previous works model a meeting in a sequential manner, while ignoring the rich structural information. In this paper, we develop a Dialogue Discourse-Aware Graph Convolutional Networks  for meeting summarization by utilizing dialogue discourse, which is a dialogue-specific structure that can provide pre-defined semantic relationships between each utterance. We first transform the entire meeting text with dialogue discourse relations into a discourse graph and then use DDA-GCN to encode the semantic representation of the graph. Finally, we employ a Recurrent Neural Network to generate the summary. In addition, we utilize the question-answer discourse relation to construct a pseudo-summarization corpus, which can be used to pre-train our model. Experimental results on the AMI dataset show that our model outperforms various baselines and can achieve state-of-the-art performance."
"Pre-trained language models BERT RoBERTa learn contextualized word representations large-scale text corpus self-supervised learning, obtain new state-of-the-art results many downstream NLP tasks . Recently, researchers observed pre-trained language models internalize real-word knowledge model parameters. For example, pre-trained language models able answer questions ``the sky }'' ``Beethoven born }'' moderate accuracy. To explore potential, researchers proposed various approaches guide pre-training language models injecting different forms knowledge them, structured knowledge graph linguistic knowledge . \end{table*} Table lists previous knowledge-guided pre-trained language models training methods. We group two categories: generative tasks discriminative tasks. Generative tasks often formulated predicting masked tokens given context. By particularly masking words contain certain types knowledge generative pre-training, model adept memorizing completing knowledge. While discriminative tasks often formulated classification problem respect sentence tokens. By training positive negative examples constructed according external knowledge, discriminator capable verifying true false knowledge natural language. Existing research demonstrated generative discriminative training advantages: former large negative sample space model learn fine-grained knowledge, latter avoids ``'' tokens pre-training, therefore consistent fine-tuning. On hand, generative discriminative capture different aspects data distribution could complementary knowledge consolidation. However, best knowledge, previous work combining two approaches systematic way. Inspired recent success generative-discriminative pre-trained model named ELECTRA, propose learn generator discriminator jointly knowledge-guided pre-training, call KgPLM model. In paper, design masked span prediction generative knowledge completion task, span replacement checking discriminative knowledge verification task. Hybrid knowledge, including link structure Wikipedia structured knowledge graph Wikidata, used guide tasks. The spans covering factual knowledge likely selected masking replacement, choices replacements also related proximity original span knowledge space. Figure shows example span masking replacement tasks. To explore effective ways joint training two tasks, design two learning schemes, called two-tower scheme pipeline scheme. Basically, generator discriminator trained parallel shared parameters two-tower scheme. While pipeline scheme, output generator input successive discriminative training. The generator discriminator KgPLM model pre-trained based RoBERTa. They additional benefits: 1) model readily extended much larger pre-training corpus, keeps potential room improvement; 2) model retains amount parameters RoBERTa, require modifications fine-tuning downstream tasks. We evaluate model performance LAMA~, consists several zero-shot knowledge completion tasks, MRQA shared tasks~, include several benchmark question answering datasets. The experiments show proposed KgPLM, especially trained pipeline scheme, achieves state-of-the-art performance, significantly outperform several strong baselines tasks. The results indicate knowledge-guided generative discriminative pre-training provides effective way incorporate external knowledge achieve competitive performance knowledge intensive NLP tasks. In paper, apply dialogue discourse model structure meeting meeting summarization. We first transform entire meeting text corresponding dialogue discourse relations discourse graph. Specifically, utterances discourse relations constructed vertices, design six types edge global vertex facilitate information flow. Moreover, develop Dialogue Discourse-Aware Graph Convolutional Networks consists utterance encoder, graph encoder, pointer decoder. In addition, construct pseudo-summarization corpus utilizing question-answer discourse relation, used pre-train model. Experiments AMI dataset show effectiveness model achieve SOTA performance."," Recent studies on pre-trained language models have demonstrated their ability to capture factual knowledge and applications in knowledge-aware downstream tasks. In this work, we present a language model pre-training framework guided by factual knowledge completion and verification, and use the generative and discriminative approaches cooperatively to learn the model. Particularly, we investigate two learning schemes, named two-tower scheme and pipeline scheme, in training the generator and discriminator with shared parameter. Experimental results on LAMA, a set of zero-shot cloze-style question answering tasks, show that our model contains richer factual knowledge than the conventional pre-trained language models. Furthermore, when fine-tuned and evaluated on the MRQA shared tasks which consists of several machine reading comprehension datasets, our model achieves the state-of-the-art performance, and gains large improvements on NewsQA  and TriviaQA  over RoBERTa."
"Knowledge graphs , WordNet , Freebase Wikidata , aggregate large amount human knowledge express structured way. % representative existing KGs, knowledge formalized triples. %, head entity, tail entity, relation two entities. The large number triples KGs constructed complex knowledge network, far complete. In recent years, knowledge graph completion tasks attracted great attention. Despite new state-of-the-art models emerge constently, methods ignore topological structure information KGs. Relation paths common topological structure KGs, Figure shows relation path instances. relation triple, two-step relation path. Similar word context language models , relation paths considered one kind contextual information KGs. We call ``graph contextual information''. And Harris's famous distributional hypothesis also extend knowledge graphs: shall know entity relationships involves. Although two kinds contextual information similar, latter specialities. In knowledge graphs, relation paths meaningful. For example, valid relation path, indicate must relationship . Unreliable relation paths common knowledge graphs, \citet{lin2015modeling} found necessary select reliable relation paths knowledge representation learning. %In work, path-constraint resource allocation algorithm proposed measure weights inference patterns. They learn inference patterns relations paths utilize knowledge contained relation paths. %Despite success, modeling objects limited inference patterns relations paths. %Recently, \citeauthor{wang2019coke} \shortcite{wang2019coke} propose method model contextual nature triples relation paths, explore benefits graph contextual information link prediction tasks two specific datasets. %However, simply adding graph contextual information training pool always effective, operation may reduce performance original model. Instead relying inference patterns, propose PPKE, path-based pre-training approach integrates graph contextual information contained relation paths model parameters. We think general way develop unexploited graph contextual information. During path-based pre-training procedure, two-step relation paths extracted knowledge graph fed pre-training module original triples. Then, pre-trained model finetuned downstream KGC tasks, link prediction relation prediction. Our contributions follows: We proposed pre-training method cooperatively modeling generative discriminative knowledge injecting approaches. Our model easily extended larger pre-training corpus introduce modifications downstream tasks finetuning. Experiments show model consistently outperforms \texttt{BASE} models variety question answering datasets, demonstrating KgPLM preferred choice knowledge intensive NLP tasks. Our method uses two-tower pipeline frameworks integrate knowledge span masking knowledge span checking pre-training. add TEK pre-training finetuning. train scratch"," 		Entities may have complex interactions in a knowledge graph , such as multi-step relationships, which can be viewed as graph contextual information of the entities. 		Traditional knowledge representation learning  methods usually treat a single triple as a training unit, and neglect most of the graph contextual information exists in the topological structure of KGs. 		In this study, we propose a Path-based Pre-training model to learn Knowledge Embeddings, called PPKE, which aims to integrate more graph contextual information between entities into the KRL model. 		Experiments demonstrate that our model achieves state-of-the-art results on several benchmark datasets for link prediction and relation prediction tasks, indicating that our model provides a feasible way to take advantage of graph contextual information in KGs."
"Machine reading comprehension challenging natural language understanding task lets machine predict appropriate answer question according given passage document . According answer styles, MRC tasks roughly divided generative , extractive multi-choice tasks . The multi-choice task focus work. Recently, various datasets tasks proposed, promoting rapid improvement MRC techniques . Early MRC datasets usually provide passages whose contents extracted articles . Recently, conversational reading comprehension aroused great interests whose passages derived multi-turn dialogue segments , making task challenging. The popular practice solve MRC problems adopting pre-trained language models encoder module . Instead better exploiting pre-trained LMs, paper motivated human reading strategies decouples MRC sketchy reading extracting critical spans passage, extensive reading seeking external knowledge. As result, propose knowledge enhancement model based extracted critical information called RekNet . In detail, proposed RekNet refines fine-grained critical information span extraction model defines Reference Span, quotes relevant external knowledge form quadruples co-occurrence information Reference Span answer options. An example process RekNet shown Figure . In summary, main contributions follows:\\ 1) We propose novel reference-based knowledge enhancement model RekNet, makes first attempt obtain fine-grained evidence inference knowledge retrieving MRC tasks.\\ 2) RekNet uses novel knowledge quadruples quote relevant credible knowledge.\\ 3) RekNet applied two multi-choice MRC benchmarks, RACE DREAM improves performance baseline models 1.0\% 1.1\% respectively, pass significance test MRC tasks. We propose novel approach integrate graph contextual information path-based pre-training model, focusing modeling one-step two-step relations entities. Then, pre-trained model finetuned link prediction relation prediction tasks. Experiments show model outperforms previous state-of-the-art methods, incorporating small portion graph context information existing knowledge graphs, validates intuition graph contextual information beneficial knowledge graph completion tasks. In follow-up work, try add relation prediction objective pre-training procedure, larger quantity wider variety graph contextual information explored. Besides, knowledge-driven tasks utilized validate effectiveness method. relation prediction pre-training"," Multi-choice Machine Reading Comprehension  is a major and challenging form of MRC tasks that requires model to select the most appropriate answer from a set of candidates given passage and question. Most of the existing researches focus on the modeling of the task datasets without explicitly referring to external fine-grained commonsense sources, which is a well-known challenge in multi-choice tasks. Thus we propose a novel reference-based knowledge enhancement model based on span extraction called 	extbf{Reference Knowledgeable Network }, which simulates human reading strategy to refine critical information from the passage and quote external knowledge in necessity. In detail, RekNet refines fine-grained critical information and defines it as Reference Span, then quotes external knowledge quadruples by the co-occurrence information of Reference Span and answer options. Our proposed method is evaluated on two multi-choice MRC benchmarks: RACE and DREAM, which shows remarkable performance improvement with observable statistical significance level over strong baselines."
"Data collection essential part field spoken dialogue systems conversational AI. %, requires developers make difficult decisions budget accordingly. In particular, designing dialogue system completely new domain still challenging task. Data collection options include running lab-based experiments, crowd-sourced tasks gathering data social media platforms, Reddit Twitter. Ambitious large scale data collections across multiple domains resulted widely used datasets, MultiWOZ . % collected various platforms .% create representations dialogues vector space. However, starting new domain scratch still challenges. Difficult costly decisions made collect data. A large majority recent dialogue corpora collected using crowd-sourcing either pairing workers letting chat, often given topic , asking add next utterance dialogue given set conditions . Other studies recruited subjects play role system, i.e., act wizard user . Each approaches advantages disadvantages, depending dialogue task-oriented not. By letting users type unrestricted way, richness dialogue increases, positive feature chit-chat. On hand, much variability could problem high stakes, task-oriented dialogues, medical domain. Letting multiple users contribute one utterance per dialogue , speeds data collection, however, dialogues may lack coherence severely diverge real dialogues. On hand, hiring training subjects chat perform wizard role results controlled data collection dramatically increases cost data collection makes less scalable. The quality datasets often assessed according degree variability observed lexical complexity utterances collected . %, however best knowledge, work assessing impact different methods directly training dialogue models. %This paper aims addressing issue investigating impact two different data collection methods performance model. Furthermore, above-mentioned datasets focus increasing size dataset available dialogue research, rather investigating impact data collection strategies performance models trained. The work presented paper aims highlighting pros cons, using methodology quickly leverage robust dialogue system, minimising cost effort involved data collection process. Analyses comparing different strategies data collection process across various platforms done past , aware similar study dialogue data. The data used study collected scope emergency response system used off-shore energy platform part EPSRC ORCA Hub programme . One collections done using crowd-sourcing second one done lab using Wizard-of-Oz setting, participants interacting either social robot smart speaker. Both datasets used train dialogue model using implementation Hybrid Code Network compare results achieved models trained data collected either method. To validate use crowd-sourced data bootstrap dialogue system situated interaction, ran experiments train model crowd-sourced data test lab data, order verify %This result estimate number dialogues needed %varied amount crowd-sourced dialogues training estimate necessary amount crowd-sourced data needed achieves comparable performances models trained lab data. The contributions paper follows: 1) comparison models trained two datasets collected different ways task, 2) evidence suggests specialised dialogue tasks, emergency response task, well covered current pre-trained dialogue models, 3) set recommendations regarding data collection dialogue research.\footnote{Please find code data in: \href{https://github.com/zedavid/TheLabVsTheCrowd}{}.} The paper organised follows. Section cover previous work related problem. Our experimental set-up introduced Section , followed results Section . The paper concludes discussion Section future work conclusions Section . To alleviate challenge knowledge role missing multi-choice MRC, work makes first attempt integrating external knowledge based span extraction MRC modeling, presenting extbf{Reference Knowledgeable Network }, simulate human strategy reading comprehension quote external knowledge multi-choice MRC tasks. RekNet helps achieve significantly performance improvement two multi-choice MRC benchmarks RACE DREAM, passed significance test. In future, apply RekNet forms MRC tasks.","  Challenges around collecting and processing quality data have hampered progress in data-driven dialogue models. %, particularly data-hungry neural and hybrid models.   Previous approaches are moving away from costly, resource-intensive lab settings, where collection is slow but where the data is deemed of high quality. The advent of crowd-sourcing platforms, such as Amazon Mechanical Turk, has provided researchers with an alternative cost-effective and rapid way to collect data.   %However, these platforms are sometimes notorious for data anomalies due to the rapid nature of which data is collected.   However, the collection of fluid, natural spoken or textual interaction can be challenging, particularly between two crowd-sourced workers. In this study, we compare the performance of dialogue models for the same interaction task but collected in two different settings: in the lab vs. crowd-sourced. We find that fewer lab dialogues are needed to reach similar accuracy, less than half the amount of lab data as crowd-sourced data.. We discuss the advantages and disadvantages of each data collection method. %, which is of interest to the community in terms of platform choice and how much data will be needed to be collected."
". % % % final paper: en-us version % % space normally used marker This work licensed Creative Commons Attribution 4.0 International License. \\ License details: \url{http://creativecommons.org/licenses/by/4.0/}. } The recent surge popularity voice assistants, Google Home, Apple Siri, Amazon Alexa resulted interest scaling products regions languages. This means components supporting Spoken Language Understanding devices, Automatic Speech Recognition , Natural Language Understanding , Entity Resolution facing challenges scaling development maintenance processes multiple languages dialects. When voice assistant launched new locale, underlying speech processing components often developed specifically targeted country, marketplace, main language variant country. Many people assume device ``understands'' ``speaks'' specific language, example English, able work equally well English-speaking country, misunderstanding. For instance, speaker UK English asks device trained data collected United States ``tell famous football player'', highly unlikely device provide user's desired answer, since football means different things US UK cultures. As result, developers need take account language dialectal differences, also local culture, provide right information right language setup. An increase number target marketplaces often means linear increase effort needed develop maintain locale-specific models. NLU models, classify user intent extract significant entities user utterance, face challenge maintaining high accuracy able accommodate multiple dialects language content. The major tasks NLU intent classification slot filling. Intent classification task predict action user intends voice assistant take. Slot filling task identify specific semantic arguments intention. For example, user request ``play Poker Face Lady Gaga'', user intention ``play music'', order fulfill command specified details, system needs capture slots \{song name = Poker Face\}, \{artist name = Lady Gaga\}. These tasks called intent classification named entity recognition , respectively. One common approach use max-entropy classification model IC task conditional random fields model NER task. Following advent deep learning techniques related fields, computer vision natural language processing, deep learning becoming popular NLU well. Some recent multilingual approaches NLU include, example, Convolutional Neural Network model sentence classification , Long Short-Term Memory model NER prediction . In deep neural network architecture, aforementioned NLU tasks combined single multi-task classification model. An increasing number experiments also focus multilingual setups, especially field machine translation, task translate input one language another . One recent thread multilingual research centers around learning multilingual word representation. Multilingual word embeddings shared cross-lingual vector space one main property: words different languages similar meaning must geometrically close. This property allows transfer learning one language another various multilingual tasks, dependency parsing classification NER . A number model architectures proposed pre-train multilingual word representations, leveraging large-scaled LSTM networks trained monolingual corpora adversarial setup space alignment , transformers trained multilingual corpora single language model . Although models used solve IC NER tasks appending corresponding decoders generate final predictions, straightforward use production environments due latency memory constrains. A different way benefitting larger models could use transfer learning smaller-size models improve performance initializing parts model close-to-optimal rather random weights. In paper, extend multi-task approach studied general multilingual model IC NER tasks, based deep learning techniques, bidirectional Long Short-Term Memory CRF sequence labeling model NER along multilayer perceptron IC. We also explore multilingual transfer learning benefits setup. Transfer learning widely adapted zero-shot few-shot setups, explored multilingual NLP studies , also used multi-task IC-NER models , yet best knowledge, study applying transfer learning data-rich target languages multilingual setup. In experiment, apply few-shot transfer learning data-rich languages language smaller amout training data. In additon, also apply transfer learning mimic situation expanding model ability same-level-resource language known context another high-resource language, new multilingual model ``inherit'' context information ancestors. We investigate approaches transfer learning effects model performance. We show transfer learning improve NLU model performance even data-rich conditions. Dialogue acts occur context inform\_time\_left hold\_on\footnote{Used keep users engaged new information available.} scenarios difficult learn. With use action mask, observed prediction robot state update states greatly improved use action mask BoW.previous_action.nlu: MTurk, seems failing get right timing. Dialogues states seem predicted adjacent turns. Problems predicting request action states, unlike lab unknown turns hardest predict. Results Table show model trained Lab data outperformed model trained complete dataset , features included. When comparing performance models trained number dialogues , model trained Lab data clearly outperformed model trained MTurk data. This surprising, since Lab data collected single wizard, mastered task. The wizard behaviour consistent behaviour crowd-workers, little time familiarise task. The perplexity scores also confirm dialogues trained Lab data unfold predictable way compared crowd-sourced data. A fine-grain analysis revealed outputs models trained MTurk data tend rush dialogue, minimising number robot status updates robot moving towards target location using fewer non-task based dialogue acts, ``Hold 2 seconds''. This pattern perhaps reflects crowd-sourced worker's tendency streamline tasks. However, even given time constraint task, above-mentioned dialogue acts important contributions dialogue, especially terms managing user's confidence stress levels. This effect could due fact face-to-face interactions require turn-taking management, reproducible text-based dialogues. driving emergency resolution. Furthermore, set-up used lab data collection close one get end application. As seen Figure , set-ups significantly different. In lab setting, participants immersed scene operations-like environment, unlike crowd-sourced scenario interaction takes place chat window crowd-worker's computer. This raises concerns methodology used data collections cases Clearly, lab setting appropriate tasks emergency response task described here, situation awareness full user engagement vital replicate conditions end application . Our results suggest much smaller amount data collected controlled conditions model learns faster accurately. While level control might hinder performance model dealing outliers, seems case domain, expect participants highly knowledgeable task compliant safety protocols followed complete task successfully. Models tended drive dialogue faster ground truth, Although behaviour shown throughout collected crowd-sourced dialogues, models seem learn notion time crucial domains ours. While significant progress generating dialogue responses recent years, timing aspects seem left behind. Our results suggest dialogues collected via crowd-sourcing aspect might lost. models struggle learn ``when'' use particular dialogue act. Results Table show pre-trained models perform poorly task. As hypothesised earlier, task significantly different tasks currently used task-oriented dialogue research. Hence, system developers assess whether domain similar enough allow use pre-trained models, specific requires model trained data within domain. collaborative task success helps understand performance model operator behaves reasonably. In cases, operator would resolved emergency dialogue \section{Conclusion Future work} Future work involve investigating use pre-trained models specific-task based systems extent used bootstrap models highly specific tasks own. We acknowledge datasets study small compared datasets used train state-of-the art neural models. This one reason used HCN method study shown work well small amounts data . One future direction would duplicate study dataset similar size MultiWOZ explore fine-grained increases data size training. This, however, challenging costly collect lab data match size crowd-sourced data. With in-lab data retrain models, would plan run in-depth systematic comparison variety dialogue modelling approaches, example using methodology proposed . Finally, investigate single-wizard impact, aiming run crowd-sourced data collection single wizard repeat experiments done paper. In paper, present study comparing different approaches data collection may impact hybrid neural dialogue model performance. Results suggest that, domain, models trained small sets lab-collected data outperform models trained larger crowd-sourced datasets pre-trained models. Given nature domain, focusing smaller lab data collections realistic settings likely best way rapidly improve model. However, challenge improve crowd-sourced data collection, making close possible end application, still remains."," 	With the recent explosion in popularity of voice assistant devices, there is a growing interest in making them available to user populations in additional countries and languages. However, to provide the highest accuracy and best performance for specific user populations, most existing voice assistant models are developed individually for each region or language, which requires linear investment of effort. In this paper, we propose a general multilingual model framework for Natural Language Understanding  models, which can help bootstrap new language models faster and reduce the amount of effort required to develop each language separately. We explore how different deep learning architectures affect multilingual NLU model performance. Our experimental results show that these multilingual models can reach same or better performance compared to monolingual models across language-specific test data while require less effort in creating features and model maintenance."
". % % % final paper: en-us version % % % space normally used marker This work licensed Creative Commons Attribution 4.0 International License. License details: \url{http://creativecommons.org/licenses/by/4.0/}. } The widespread dissemination fake news lead significant influence personal fame, public trust, security. For example, spreading misinformation, ``Asians vulnerable novel coronavirus''~\footnote{https://www.thestar.com.my/news/regional/2020/03/11/myth-busters-10-common-rumours-about-covid-19} COVID-19 serious repercussions, making people ignore harmfulness virus directly affecting public health. Research shown misinformation spreads faster, farther, deeper, widely true information. Therefore, fake news detection social media attracted tremendous attention recently research industrial fields. Early research fake news detection mainly focused design effective features various sources, including textual content, user profiling data, news diffusion patterns. Linguistic features, writing styles sensational headlines, lexical syntactic analysis, explored separate fake news true news. Apart linguistic features, studies also proposed series user-based features, temporal features news diffusion. However, feature-based methods time-consuming, biased, require lot labor design. Besides, features easily manipulated users. To solve problems, many recent studies apply various neural networks automatically learn high-level representations fake news detection. For example, recurrent neural network , convolutional neural network , matrix factorization graph neural network applied learn representation content diffusion graph news. These methods apply types information fake news detection, paying little attention early detection. Moreover, models detect fake news consideration fixed proportion repost information, practice cannot detect fake news early stage news propagation. Some studies explore detect fake news early relying minimum number posts. The main limitation methods ignore importance publishers' users' credibility early detection fake news. When humans see piece breaking news, firstly may use common sense judge whether factual errors it. At time, also consider reputation publishers reposted users. People tend believe news trusted authoritative source news shared lots users good reputation. If publisher reliable, tend believe news. On hand, news reposted many low-reputation users short period, may spammers tried heat news, resulting lower credibility news. Inspired observation, explicitly take credibility publishers users supervised information, model fake news detection multi-task classification task. We annotate small part publishers users historical publishing reposting behaviors. Although credibility publishers users always provide correct information, necessary complementary supervised information fake news detection. To make credibility information generalized unannotated users, construct heterogeneous graph build connections publishers, news, users. Through graph-based encoding algorithm, every node graph influenced credibility publishers users. In paper, address following challenges: How fully encode heterogeneous graph structure news content; How explicitly utilize credibility publishers users facilitating early detection fake news. To tackle challenges, propose novel structure-aware multi-head attention network early detection fake news. Firstly, design structure-aware multi-head attention module learn structure publishing graph produce publisher representations credibility prediction publishers. Then, apply structure-aware multi-head attention module encode diffusion graph news among users generate user representations credibility prediction users. Finally, apply convolutional neural network map news text word embedding semantic space utilize fusion attention module combine news, publisher, user representations early fake news detection. The contributions paper summarized follows: In paper, propose framework building general multilingual NLU models, used across different marketplaces languages. To choose model best performance, use language-specific test sets evaluate candidate models corresponding baseline models along four metrics, domain accuracy, intent accuracy, slot F1, frame accuracy. The models win evaluation metrics final picks. We find models built simple multi-task biLSTM-CRF model setup comparable standard production models terms latency constraints required on-the-fly voice assistant conversational models. We observe performance improvements models introduction transfer learning. Encoder transfer produced greatest improvements whereas transfer decoder bring much change compared baseline model performance, except tested English test set, transfer learning performed model trained English data. This due fact target non-English language contains slots intents included pre-trained model, thus decoder fails predict correct classes simply missing vocabulary. To mitigate effect, decoder default initialization gives better performance embrace available slots intents target language realm. Furthermore, find model pre-trained multilingual setup performs better one trained monolingual data set. This confirms multilingual model built based lexically orthographically similar languages may provide beneficial context information similar target language. Experimental result Hindi show multilingual model work even non-alike languages better performance improvement. This confirms common multilingual model used support multiple language better results set monolingual models. With single general multilingual NLU model, bootstrapping new languages faster use cross-lingual contextual information existing high-resource languages. At time, maintaining one model requires much less effort terms regular model updates."," The\let\thefootnote\relax\footnotetext{* Corresponding author.} dissemination of fake news significantly affects personal reputation and public trust. Recently, fake news detection has attracted tremendous attention, and previous studies mainly focused on finding clues from news content or diffusion path. However, the required features of previous models are often unavailable or insufficient in early detection scenarios, resulting in poor performance. Thus, early fake news detection remains a tough challenge. Intuitively, the news from trusted and authoritative sources or shared by many users with a good reputation is more reliable than other news. Using the credibility of publishers and users as prior weakly supervised information, we can quickly locate fake news in massive news and detect them in the early stages of dissemination.  In this paper, we propose a novel Structure-aware Multi-head Attention Network , which combines the news content, publishing, and reposting relations of publishers and users, to jointly optimize the fake news detection and credibility prediction tasks. In this way, we can explicitly exploit the credibility of publishers and users for early fake news detection. We conducted experiments on three real-world datasets, and the results show that SMAN can detect fake news in 4 hours with an accuracy of over 91\%, which is much faster than the state-of-the-art models. The source code and dataset can be available at https://github.com/chunyuanY/FakeNewsDetection."
"Real-world events sports games elections involve competing teams, capabilities tactics, aiming win . The performance teams typically dependent teams' abilities also environment within operate. For example, political party may best orators policies opponents may better getting votes key areas. Similarly, top football team may playing worst team league fact latter may facing relegation may provide extra motivation win game. Given this, many cases, performance teams may easily predictable. % In particular, sporting events many human factors impact team performs given games. There often situations would hard represent numbers statistics alone. For example, sporting rivalries often affect human emotions team performance teams fighting avoid relegation league often obtain unexpected results. Traditional AI machine learning techniques predict outcome real-world events tend focus use statistical machine learning using historical data individual teams . However, per examples above, historical performance may useful team performance may dependent dynamic factors human performance environmental variables . In turn, humans better judges algorithms faced previously unseen situations. Journalists, online communities, experienced analysts may better evaluating human environmental elements forecast outcome. For example, one approach looking statistics sports sentiment analysis social media platforms. Schumaker, Jarmoszko Labedz use approach predict English Premier League results achieve accuracy 50\% show use similar analysis performed American Football results National Football League predicting winner 63.8\% time. However, approaches focus opinion aggregation rather trying extract potential indicators performance individual human teams human experts. Against background, set new baselines results predicting real-world sporting events involving humans based combination Natural Language Processing statistical machine learning techniques. In detail, focus specifically football games EPL using match previews media alongside statistical machine learning techniques. The prediction football match outcomes challenging computational problem due range parameters influence match results. To date, probabilistic methods devised since seminal work Maher generated fairly limited results appear reached glass ceiling terms accuracy. By using media previews improve accuracy current approaches match outcome prediction. By doing, show incorporating human factors model, rather basic performance statistics, improve accuracy . Thus, contributions paper follows: In next section discuss match outcome prediction problem football new feature set explore. %The rest paper organised follows. Section discusses problem aiming solve, Section outlines model human opinion use predicting real-world football games. Section provides detail test models set baseline prediction accuracy. Finally, Section concludes. % Future Work This paper proposes novel structure-aware multi-attention network, combines news content, heterogeneous graphs among publishers users, jointly optimizes task false news detection user credibility prediction early fake news detection. Different existing research extracting hand-crafted features deep learning methods, explicitly treat credibility publishers users kind weakly supervised information facilitating fake news detection. Extensive experiments conducted three real-world datasets show proposed model significantly surpass state-of-the-art models fake news classification early detection task."," In this paper, we present a new application-focused benchmark dataset and results from a set of baseline Natural Language Processing and Machine Learning models for prediction of match outcomes for games of football . By doing so we give a baseline for the prediction accuracy that can be achieved exploiting both statistical match data and contextual articles from human sports journalists. Our dataset is focuses on a representative time-period over 6 seasons of the English Premier League, and includes newspaper match previews from The Guardian. The models presented in this paper achieve an accuracy of 63.18\% showing a 6.9\% boost on the traditional statistical methods."
"Deep neural networks successful various morphological tasks exemplified yearly SIGMORPHON Shared Task. However neural networks operate continuous representations weights stark contrast traditional, hugely successful, rule-based morphology. There attempts add rule-based discrete elements models various inductive biases. In paper tackle two morphological tasks copy task control interpretable model, \sopa. Soft Patterns \sopa finite-state machine parameterized neural network, learns linear patterns predefined size. The patterns may contain epsilon transitions self-loops otherwise linear. Soft refers fact patterns intended learn abstract representations may multiple surface representations, \sopa learn end-to-end fashion. We call surface representations subwords, abstract patterns, patterns throughout paper. An important upside \sopa interpretable patterns extracted sample. shows \sopa able retrieve meaningful word-level patterns sentiment analysis. Each pattern matched every possible subword highest scoring subword recovered via differentiable dynamic program, variant forward algorithm. We apply model encoder sequence-to-sequence seq2seq\footnote{also called encoder-decoder model} model, add LSTM decoder. We initialize decoder's hidden state final scores \sopa pattern also apply Luong's attention intermediate outputs generated \sopa. We call model \sopaseq. We compare setup sequence-to-sequence bidirectional LSTM encoder, unidirectional LSTM decoder Luong's attention. We show \sopaseq often competitive LSTM baseline also interpretable design. \sopaseq especially good \morphana, often surpassing LSTM baseline, confirm linguistic intuition namely subword patterns useful extracting morphological information. We also compare models using generalized form Jaccard-similarity find trends coincide linguistic intuition. This paper presented novel application-focused dataset set new baselines 63.19\ accuracy predicting games English Premier League football across three season period using novel dataset provide part paper. We showed application combining human opinion machine learning make predictions boost accuracy traditional methods using sentiment analysis social media. We show boost methods 6.9\ terms outcome accuracy model accuracy increases season progresses human factors/emotions begin play bigger part game. \clearpage","  We examine the role of character patterns in three tasks: morphological analysis, lemmatization and copy. We use a modified version of the standard sequence-to-sequence model, where the encoder is a pattern matching network. Each pattern scores all possible N character long subwords  on the source side, and the highest scoring subword's score is used to initialize the decoder as well as the input to the attention mechanism.  This method allows learning which subwords of the input are important for generating the output. By training the models on the same source but different target, we can compare what subwords are important for different tasks and how they relate to each other. We define a similarity metric, a generalized form of the Jaccard similarity, and assign a similarity score to each pair of the three tasks that work on the same source but may differ in target. We examine how these three tasks are related to each other in \goodlangno languages. Our code is publicly available.\footnote{https://github.com/juditacs/deep-morphology}"
"Infusing emotions conversation systems substantially improve usability promote customers' satisfaction. Moreover, perceiving emotions sufficiently core premise expressing emotions. In real-life scenarios, humans instinctively perceive complex subtle emotions multiple aspects, including emotion flow dialogue history, facial expressions personalities speakers, express suitable emotions feedback. Figure shows organization multi-source information dialogue graph relationship them. We presented application Soft Patterns -- finite state automaton parameterized neural network -- encoder sequence-to-sequence model. We show competitive popular LSTM encoder character-level copy morphological tagging, providing interpretable patterns. We analyzed behavior \sopa encoders \morphana, \lemmatization \copytask computing average Jaccard similarity patterns extracted source side. We found two trends coincide linguistic intuition. One \lemmatization morphological analysis require patterns match less similar subwords two task pairs. The one \copytask morphological analysis similar languages rich inflectional morphology."," The success of emotional conversation systems depends on sufficient perception and appropriate expression of emotions. In a real-world conversation, we firstly instinctively perceive emotions from multi-source information, including the emotion flow of dialogue history, facial expressions, and personalities of speakers, and then express suitable emotions according to our personalities, but these multiple types of information are insufficiently exploited in emotional conversation fields. To address this issue, we propose a heterogeneous graph-based model for emotional conversation generation. Specifically, we design a Heterogeneous Graph-Based Encoder to represent the conversation content  with a heterogeneous graph neural network, and then predict suitable emotions for feedback. After that, we employ an Emotion-Personality-Aware Decoder to generate a response not only relevant to the conversation context but also with appropriate emotions, by taking the encoded graph representations, the predicted emotions from the encoder and the personality of the current speaker as inputs. Experimental results show that our model can effectively perceive emotions from multi-source knowledge and generate a satisfactory response, which significantly outperforms previous state-of-the-art models."
"Text classification one fundamental tasks natural language processing wide applications sentiment analysis, news filtering, spam detection intent recognition. Plenty algorithms, especially deep learning-based methods, applied successfully text classification, including recurrent neural networks , convolutional networks . More recently, large pre-training language models ELMO , BERT , Xlnet also shown outstanding performance kinds NLP tasks, including text classification. Although numerous deep learning models shown success text classification problems, share learning paradigm: deep model text representation, simple classifier predict label distribution cross-entropy loss predicted probability distribution one-hot label vector. However, learning paradigm least two problems: In general text classification tasks, one-hot label representation based assumption categories independent other. But real scenarios, labels often completely independent instances may relate multiple labels, especially confused datasets similar labels. As result, simply representing true label one-hot vector fails take relations instances labels account, limits learning ability current deep learning models. The success deep learning models heavily relies large annotated data, noisy data labeling errors severely diminish classification performance, inevitable human-annotated datasets. Training one-hot label representation particularly vulnerable mislabeled samples full probability assigned wrong category. In brief, limitation current learning paradigm lead confusion prediction model hard distinguish labels, refer label confusion problem . A label smoothing method proposed remedy inefficiency one-hot vector labeling , however, still fails capture realistic relation among labels, therefore enough solve problem. In work, propose novel Label Confusion Model enhancement component current deep learning text classification models make model stronger cope label confusion problem. In particular, LCM learns representations labels calculates semantic similarity input text representations estimate dependency, transferred label confusion distribution . After that, original one-hot label vector added LCD controlling parameter normalized softmax function generate simulated label distribution . We use obtained SLD replace one-hot label vector supervise training model training. With help LCM, deep model capture relations instances labels, also learns overlaps among different labels, thus, performs better text classification tasks. We conclude contributions follows: We propose heterogeneous graph-based framework understand dialogue content fully perceive complex subtle emotions multi-source knowledge generate coherent emotional response. Experimental results analysis demonstrate effectiveness generalizability model, easily adapted different number knowledge sources. In future, would like infuse knowledge sources investigate various relations improve quality responses."," Representing a true label as a one-hot vector is a common practice in training text classification models. However, the one-hot representation may not adequately reflect the relation between the instances and labels, as labels are often not completely independent and instances may relate to multiple labels in practice. The inadequate one-hot representations tend to train the model to be over-confident, which may result in arbitrary prediction and model overfitting, especially for confused datasets  or noisy datasets . While training models with label smoothing  can ease this problem in some degree, it still fails to capture the realistic relation among labels. In this paper, we propose a novel Label Confusion Model  as an enhancement component to current popular text classification models. LCM can learn label confusion to capture semantic overlap among labels by calculating the similarity between instances and labels during training and generate a better label distribution to replace the original one-hot label vector, thus improving the final classification performance. Extensive experiments on five text classification benchmark datasets reveal the effectiveness of LCM for several widely used deep learning classification models. Further experiments also verify that LCM is especially helpful for confused or noisy datasets and superior to the label smoothing method."
"Over recent years, various task-oriented conversational agents, Amazon Alexa, Apple Siri, Google Assistant, Microsoft Cortana, become popular people everyday life expected highly intelligent. For NLU component, means expect models perform recognition actions entities within user request high accuracy. When first training NLU model new language , strong requirement high quality annotated data would support common user requests across range domains. As modeling space expands support new features additional languages, NLU models regularly re-trained updated data sets ensure support new functions. The major bottleneck processes labor cost associated collecting annotating new training utterances every new feature language. Recent advances machine learning methods, including use techniques transfer learning~ active learning, lead efficient data usage NLU models therefore decrease need annotated training data. Additionally, data augmentation models widely explored. The advantage data augmentation synthetic data generated, ingested subsequent models without additional effort, allowing faster experimentation. NLU models dialog systems perform variety tasks. In study, focus three them: Domain classification -- identify domain user request belongs , Intent classification -- extract actions requested users , Named Entity Recognition -- identify extract entities user requests. For utterance expect NLU model output domain, intent, set extracted entities corresponding tags. For example, user requests ``play Bohemian Rhapsody Queen'', expect NLU model return \{domain: music, intent: play\_song, named\_entities: [, ]\}. We call output annotation, utterance along annotation called annotated utterance. Named entities corresponding labels called slots. For NLU model perform well real-time user requests, need train large dataset diverse annotated utterances. However, could areas functionality large datasets training available. To boost model performance situations training data limited, use synthetic data generated small set unique utterances cover basic functionality user experience, called Golden utterances. We leverage Sequence Generative Adversarial Networks introduced by~\citet{Yu2016SeqGANSG} generate new utterances ``seed'' set, use generated utterances augment training data evaluate performance classification recognition tasks. We also investigate metrics use evaluate quality generated synthetic data links performance boost underlying tasks. In work, propose Label Confusion Model enhancement component current text classification models improve performance. LCM capture relations instances labels well dependency among labels. Experiments five benchmark datasets proved LCM's enhancement several popular deep learning models LSTM, CNN BERT. Our future work include following directions: Designing better LCM structure computer vision tasks conducting experiments image classification. Generalizing LCM method multi-label classification problems label distribution prediction."," Data sparsity is one of the key challenges associated with model development in Natural Language Understanding  for conversational agents. The challenge is made more complex by the demand for high quality annotated utterances commonly required for supervised learning, usually resulting in weeks of manual labor and high cost. In this paper, we present our results on boosting NLU model performance through training data augmentation using a sequential generative adversarial network . We explore data generation in the context of two tasks, the bootstrapping of a new language and the handling of low resource features. For both tasks we explore three sequential GAN architectures, one with a token-level reward function, another with our own implementation of a token-level Monte Carlo rollout reward, and a third with sentence-level reward. We evaluate the performance of these feedback models across several sampling methodologies and compare our results to upsampling the original data to the same scale. We further improve the GAN model performance through the transfer learning of the pre-trained embeddings. Our experiments reveal synthetic data generated using the sequential generative adversarial network provides significant performance boosts across multiple metrics and can be a major benefit to the NLU tasks."
"Encoder-decoder architecture~ extensively used neural machine translation ~. Given source sentence, encoder firstly converts hidden representations, conditioned decoder generate target sentence. Attention mechanism~ effective learning alignment source sentence target sentence. Hence, attention mechanism usually used architecture improve capability, capturing long-distance dependencies. Similar traditional machine learning efforts~, recent approaches deep learning attempt improve encoder-decoder architecture multiple passes decoding~. NMT refers polish mechanism~. Under scheme, one translations generated source sentence and, except first translation, based translation previous decoding pass. While methods achieved promising results, lack proper termination policy multi-turn process. \citet{xia2017deliberation,zhang2018asynchronous} adopt fixed number decoding passes inflexible deciding optimal number decoding passes. \citet{geng2018adaptive} use reinforcement learning ~ automatically decide optimal number decoding passes. However, RL unstable due high variance gradient estimation objective instability~. Since methods may premature termination translation, potential limited. To address problem, propose novel framework, Rewriter-Evaluator, paper. It consists rewriter evaluator. The translation process involves multiple passes. Given source sentence, every pass, rewriter generates new target sequence aiming improving translation prior passes, evaluator measures translation quality determine whether terminate rewriting process. We also propose prioritized gradient descent method facilitates training rewriter evaluator jointly. The essential idea using priority queue improve sampling efficiency collecting translation cases yield low scores evaluator next-pass rewriting. The size queue times larger batch size. Although Rewriter-Evaluator involves multiple decoding passes, training time using PGD method comparable training encoder-decoder~ multiple decoding passes. We apply Rewriter-Evaluator improve widely used NMT models, RNNSearch~ Transformer~. Extensive experiments conducted two translation tasks, Chinese-English English-German, verify proposed method. The results demonstrate proposed framework notably improves performance NMT models significantly outperforms prior methods. In paper, evaluate use SeqGAN model synthetic annotated data generation boost NLU model performance. We shown adding synthetic data bolster Goldens significantly improve DNN model performance intent classification named entity recognition tasks. We propose token-level reward Monte Carlo search rollout guide generator model, showed better performance compared regular token-level reward implementation, sentence-level reward implementations without Monte Carlo tree search, pure upsampling strategy. We also show using SeqGAN together embeddings pre-trained high-resource domains generate synthetic data significantly improve performance low-resource domains. Embeddings pre-trained different tasks carry information learned especially useful low-resource model building scenarios. \onecolumn"," 	 	Encoder-decoder architecture has been widely used in neural machine translation . A few methods have been proposed to improve it with multiple passes of decoding. However, their full potential is limited by a lack of appropriate termination policy. To address this issue, we present a novel framework, Rewriter-Evaluator. It consists of a rewriter and an evaluator. Translating a source sentence involves multiple passes. At every pass, the rewriter produces a new translation to improve the past translation and the evaluator estimates the translation quality to decide whether to terminate the rewriting process. We also propose a prioritized gradient descent  method that facilitates training the rewriter and the evaluator jointly. Though incurring multiple passes of decoding, Rewriter-Evaluator with the proposed PGD method can be trained with similar time to that of training encoder-decoder models. We apply the proposed framework to improve the general NMT models . We conduct extensive experiments on two translation tasks, Chinese-English and English-German, and show that the proposed framework notably improves the performances of NMT models and significantly outperforms previous baselines."
"Recent advances open domain question answering mostly revolved around machine reading comprehension task read comprehend given text answer questions based it. However, recent work MRC English \eg\ SQuAD , HotpotQA Natural Questions . Significant performance gains state-of-the-art datasets credited large pre-trained language models . Multilingual BERT , trained Wikipedia articles 104 languages equipped 120k shared wordpiece vocabulary, encouraged lot progress cross-lingual tasks \eg{} XNLI , NER QA performing zero-shot training: train one language test unseen target languages. In work, focus multilingual QA and, particular, two recent large-scale datasets: MLQA TyDiQA\footnote{All uses TyDiQA paper refer Gold Passage task.}. Both datasets contain English QA pairs also examples 13 diverse languages. Some examples shown Figure . MLQA evaluates two challenging scenarios: 1) Cross-Lingual Transfer question context language, 2) Generalized Cross-lingual Transfer question one language context another language . %In cases, MLQA zero-shot provide training data language. \avi{Maybe remove previous sentence?} % TyDiQA consists QA examples English 8 languages. TyDiQA designed XLT only. Both datasets challenging multilingual QA due large number languages variety linguistic phenomena encompass . Ideally, want build QA systems existing languages impractical collect manually labeled training data them. In absence labeled data, suggested several research directions pushing boundaries multilingual QA, including zero-shot QA, exploring data augmentation machine translation, well effective transfer learning. These avenues explore work addition asking following research questions:\\ 1. Is large pre-trained LM sufficient zero-shot multi-lingual QA? \\ Prior work proposes zero-shot transfer learning English SQuAD data languages using pre-trained LM competitive results achieved MLQA TyDiQA . We venture beyond zero-shot training first exploring data augmentation top underlying model. We achieve using translation methodologies augment English training data. We use machine translation obtain additional silver labeled data allowing us improve cross-lingual transfer low cost. Our approach introduces several multilingual extensions SQuAD training data: translating questions keeping context English, translating context keeping question English, translating question context languages. This enables us augment original English human-labeled training examples 14 times multilingual silver-labeled QA pairs.\\ 2. Can bring language-specific embeddings multi-lingual LMs closer effective cross-lingual transfer?\\ %To better MLQA, believe important model Our hypothesis make cross-lingual QA transfer effective bring embeddings multilingual pre-trained LM closer semantic space. To answer question French suffice train system Hindi necessary train system target language: hence, French Hindi look language. We propose two approaches explore cross-lingual transfer: In first approach, propose novel strategy based adversarial training . We investigate addition language-adversarial task QA finetuning pretrained LM significantly improve cross-lingual transfer performance causing embeddings LM become less language-dependent. In second approach, develop novel Language Arbitration Framework consolidate embedding representation across languages using properties translation. We train additional auxiliary tasks \eg{} making sure English question translation Arabic produces answer see input context Spanish. The intuition behind language arbitration training model English translated examples, proposed multi-lingual objectives bring language-specific embeddings closer English embeddings.\\ Overall, main contributions paper follows: In paper, propose task citation sequence prediction. For this, introduce dataset scholary documents based dynamic citation graph evolving years, starting single node growing large graph. We study effect temporal topological information, propose model benefit information . Our results show utilizing temporal topological information superior utilizing either temporal topological information. Using proposed model, study effect different features, identify information predictive paper's citation count time. We found author information predictive informative time. In future work, impact training single GCN dynamic graph could explored, since error time GCN deteriorating fast."," Prior work on multilingual question answering has mostly focused on using large multilingual pre-trained language models  to perform zero-shot language-wise learning: train a QA model on English and test on other languages. In this work, we explore strategies that improve cross-lingual transfer by bringing the multilingual embeddings closer in the semantic space.  Our first strategy augments the original English training data with machine translation-generated data. This results in a corpus of multilingual silver-labeled QA pairs that is 14 times larger than the original training set. In addition, we propose two novel strategies, language adversarial training and language arbitration framework, which significantly improve the  cross-lingual transfer performance and result in LM embeddings that are less language-variant. Empirically, we show that the proposed models outperform the previous zero-shot baseline on the recently introduced multilingual MLQA and TyDiQA  datasets."
"% \subsection{Problem Statement Motivation} Researchers' ability automate natural language processing grown exponentially past years, particularly advent Transformer architecture . Despite fact recent machine learning methods achieve impressive almost human-level performance tasks dialogue modeling natural language generation , many intelligent voice assistants still rely rule-based architectures cached responses open domain dialogue . This primarily due lack controls deep learning architectures producing specific phrases, tones, topics, makes models inherently unpredictable therefore risky entities - corporate otherwise - wish deploy public-facing intelligent agents. For example, often desirable conversational agent maintain specific identity throughout exchange dialogue currently impossible condition deep learning algorithms maintain coherent identity across dialogue without training highly specialized data sets. Fine-tuning specialized data sets comes additional, significant cost: lead catastrophic forgetting language model . Despite aspect fine-tuning, current state-of-the-art methods require fine-tuning entire network original data set proves unsuitable given task , even language modeled across tasks. Furthermore, models produced current methods almost entirely uninterpretable therefore generally difficult test egregious failure cases. % \subsection{Solution Overview} In paper, address issue content control well catastrophic forgetting induced fine-tuning. We define `content control' able command network either incorporate eschew exact word, phrase, topic, style, sentiment output, therefore attempt granular level control purely topic/style-level control published recent literature . We also introduce alternative fine-tuning neural language models demonstrate experimentation high-cost overwriting model weights fine-tuning often fails induce desired behavior generalized settings. %is inspired ``No Free Lunch"" theorems introduced Wolpert \& Macready seek avoid training neural network simultaneously model language act explicit commands. Instead, recast problem control natural language generation one combining separate models - one natural language one high-level command responses - produce desired linguistic output. In so, develop framework interpreting subsequently controlling hidden activations pretrained neural network without adjustments made pretrained model. This framework biologically consistent findings Knutson et al., discovered neural pathways humans inhibited neuron clusters , applications neural network architectures questions outside domain controllable text generation. In work, highlight open challenges existing multilingual approach . Specifically, show large pre-trained multi-lingual LMs enough task. We produce several novel strategies multilingual QA go beyond zero-shot training outshine previous baseline built top \mbert{}. We present translation model 14 times training data. Further, AT LAF strategies utilize translation data augmentation bring language-specific embeddings LM closer other. These approaches help us significantly improve cross-lingual transfer. Empirically, models demonstrate strong results approaches improve previous ZS strategy. We hope techniques spur research field exploring multilingual LMs invoking additional networks top large LMs multilingual NLP."," %   Current solutions to the problem of controlling generative neural language models are usually formulated under a training paradigm in which the language model is trained to simultaneously model natural language and respond to high-level commands. We recast the problem of control in natural language generation as that of learning to interface with a pretrained language model to generate desired output, just as Application Programming Interfaces  control the behavior of programs by altering hyperparameters. In this new paradigm, a specialized neural network  learns to interface with a pretrained language model by manipulating the hidden activations of the pretrained model in real time to produce desired outputs, such that no permanent changes are made to the weights of the original language model.     It is notoriously difficult to control the behavior of artificial neural networks such as generative neural language models. We recast the problem of controlling natural language generation as that of learning to interface with a pretrained language model, just as Application Programming Interfaces  control the behavior of programs by altering hyperparameters. In this new paradigm, a specialized neural network  learns to interface with a pretrained language model by manipulating the hidden activations of the pretrained model to produce desired outputs. Importantly, no permanent changes are made to the weights of the original model, allowing us to re-purpose pretrained models for new tasks without overwriting any aspect of the language model. We also contribute a new data set construction algorithm and GAN-inspired loss function that allows us to train NPI models to control outputs of autoregressive transformers. In experiments against other state-of-the-art approaches, we demonstrate the efficacy of our methods using OpenAI GPT-2 model, successfully controlling noun selection, topic aversion, offensive speech filtering, and other aspects of language while largely maintaining the controlled model's fluency under deterministic settings. %Finally, we describe the ethical implications of this work. %   Applications for this approach include re-purposing a pretrained model  for a new task without a specialized data set in the problem domain. We present experimental results from training several NPI models to control the outputs of OpenAI's GPT-2 language model \cite{openaiGPT2}, as well as a novel data curation approach in which hidden activations of an uninterpretable pretrained model are associated with specific outputs. Finally, we describe potential methods whereby NPIs might be leveraged to interpret the inner workings of pretrained networks, as well as the related ethical implications of this work."
"Emotion analysis user-generated content available web provides insights toward making meaningful decisions. Micro-blog platforms Twitter gained profuse popularity textual content holding people's opinions. The past decade seen active growth emotion analysis models many domains. Recently increasing interest analysis emotions informal short texts tweets. In paper, introduce analyze system accurately identify emotions individual tweets associated intensities~\footnote{Intensity refers degree amount emotion}. % explain important analyze emotions Analyzing emotions social media twitter benefits society number ways. Policymakers use emotional information social media accurately identify concerns people making decisions. Monitoring social media health issues benefits public health also government decision makers. Furthermore, organizations monitor opinion public products services provide better service society. Once emotions recognized, emotion intensity used prioritize major concerns. Studies emotion analysis often focused emotion classification. However, emotions may exhibit varying levels intensities. Here, emotion intensity defined degree intensity particular emotion felt speaker. Additionally, may observe multiple emotions simultaneously tweet varying intensities. One purpose study develop model accurately identify emotions associated emotion intensities given tweet. In paper, propose transfer learning approach backed neural network classifier regressor. Although proposed neural network alone inadequate beat benchmark, show features learned training neural networks used improve overall performance combined features. Another purpose study explain input word level features affect features extracted neural network. % [complete actual findings here] The findings make important contribution understanding features used neural network effectively select features improve effectiveness extracted features. Our main contributions study: \pagebreak Major challenge using deep learning train emotion intensity prediction models lack large labeled datasets. More recently, emoji hashtags used studies create large naturally labeled datasets. However, possible use similar technique obtain intensity emotions. Furthermore, creating large dataset manually time consuming expensive. existing datasets emotional intensity prediction. Due limited amount task-specific training data previous researches opted transfer learning approaches~\citet{baziotis2018ntua, duppada2018seernet} traditional machine learning. However, paper argue even reasonable size dataset train neural network obtain good performance provided proper regularization. Additionally, show features learned training neural network combined features improve overall performance emotion intensity prediction. % [explain methodology brief] % } \end{table} In \S, outline related works sentiment emotion mining. Next, \S discuss datasets used study. After, introduce background methodology \S \S accordingly. Then, \S discuss evaluation results. Finally, conclude paper \S. The key contribution insight paper use small, independently trained neural network called Neural Programming Interface influence behavior large pretrained model. In contrast fine-tuning, approach retains linguistic breadth versatility original model, allowing possibility control multiple factors either sequence simultaneously, induce behavior language model contrary patterns baked linguistic training data . We demonstrated approach used produce specific words within GPT-2 model's output text, pivot away specific word, create linguistic aversion offensive speech. We believe future avenues research include investigations use NPI models network interpretability, regulation, bias mitigation."," In this paper, we present an experiment on using deep learning and transfer learning techniques for emotion analysis in tweets and suggest a method to interpret our deep learning models. The proposed approach for emotion analysis combines a Long Short Term Memory  network with a Convolutional Neural Network . Then we extend this approach for emotion intensity prediction using transfer learning technique. Furthermore, we propose a technique to visualize the importance of each word in a tweet to get a better understanding of the model. Experimentally, we show in our analysis that the proposed models outperform the state-of-the-art in emotion classification while maintaining competitive results in predicting emotion intensity."
"Online reviewing businesses becomes important nowadays, customers publish reviews businesses, potential customers shop owners view them. Positive feedback customers may prosper store businesses, negative one could opposite consequences. Yelp, one largest company founded 2004 publishing crowd-sourced reviews businesses, provides one open dataset, Yelp Open Dataset , tremendously many data businesses, reviews, users. Such dataset proven good material personal, educational, academic purposes. Among multiple tasks Yelp Open Dataset, predicting ratings restaurants based reviews one fundamental important tasks. This task help Yelp classify reviews proper groups recommendation system, detect anomaly reviews protect businesses malicious competitions, assign rating texts automatically. Yelp review rating prediction done multiple ways, sentiment analysis 5-star rating classification. In paper, focus rating prediction restaurants based review texts. This task viewed multiclass classification problem, input textual data , output predicted class . We apply machine learning deep learning models. After analyzing data distribution, splitting datasets, extracting features, use four machine learning methods, including Naive Bayes, Logistic Regression, Random Forest, Linear Support Vector Machine . Then focus four transformer-based models, including BERT , DistilBERT , RoBERTa , XLNet , several different architectures tried hyperparameter tuning. This project done \href{https://colab.research.google.com/}{Google Colab}, multi-processors GPUs available. The code publicly available GitHub . In study, propose simple yet effective model emotion classification emotion intensity prediction Tweets suggesting method explain visualize trained DNN. We utilized neural network LSTM layer followed convolution layer max-pooling emotion category classification well emotion intensity prediction. We extend work transferring features models two state-of-the-art models trained different tasks XGBoost regressor predict emotion intensity Tweets accurately. Moreover, suggest technique visualize interpret feature importance trained DNNs emotion intensity prediction. In future, plan experimenting using attentive mechanisms improve emotion intensity prediction further. Our models outperformed existing state-of-the-art models emotion classification predicting fear anger emotion intensities, maintaining competitive results predicting emotions.","    We predict restaurant ratings from Yelp reviews based on Yelp Open Dataset. Data distribution is presented, and one balanced training dataset is built. Two vectorizers are experimented for feature engineering. Four machine learning models including Naive Bayes, Logistic Regression, Random Forest, and Linear Support Vector Machine are implemented. Four transformer-based models containing BERT, DistilBERT, RoBERTa, and XLNet are also applied. Accuracy, weighted $ F_1 $ score, and confusion matrix are used for model evaluation. XLNet achieves 70\% accuracy for 5-star classification compared with Logistic Regression with 64\% accuracy."
"Language processing requires tracking information multiple timescales. To able predict final word ``timescales"" previous sentence, one must consider short-range context long-range context . How humans neural language models encode multi-scale context information? Neuroscientists developed methods study human brain encodes information multiple timescales sequence processing. By parametrically varying timescale intact context, measuring resultant changes neural response, series studies showed higher-order regions sensitive long-range context change lower-order sensory regions. These studies indicate existence ``hierarchy processing timescales"" human brain. More recently, \citet{chien2020constructing} used time-resolved method investigate brain builds shared representation, two groups people processed narrative segment preceded different contexts. By directly mapping time required individual brain regions converge shared representation response shared input, confirmed higher-order regions take longer build shared representation. Altogether, lines investigation suggest sequence processing brain supported distributed hierarchical structure: sensory regions short processing timescales primarily influenced current input short-range context, higher-order cortical regions longer timescales track longer-range dependencies . How processing timescales organized within recurrent neural networks trained perform natural language processing? Long short-term memory networks widely investigated terms ability successfully solve sequential prediction tasks. However, long-range dependencies usually studied respect particular linguistic function , less attention broader question sensitivity prior context -- broadly construed -- functionally organized within RNNs. Therefore, drawing prior work neuroscience literature, demonstrate model-free approach mapping processing timescale RNNs. We focused existing language models trained predict upcoming tokens word level character level . The timescale organization two models revealed higher layers LSTM language models contained small subset units exhibit long-range sequence dependencies; subset includes previously reported units well previously unreported units. After mapping timescales individual units, asked: processing timescales unit network relate functional role, measured connectivity? The question motivated neuroscience studies shown human brain, higher-degree nodes tend exhibit slower dynamics longer context dependence lower-degree nodes . More generally, primate brain exhibits core periphery structure relatively small number ``higher order high-degree regions maintain large number connections one another, exert powerful influence large-scale cortical dynamics . Inspired relationships timescales network structure brain, set test corresponding hypotheses RNNs: Do units longer-timescales tend higher degree neural language models? Do neural language models also exhibit ``core network"" composed functionally influential high-degree units? Using exploratory network-theoretic approach, found units longer timescales tend projections units. Furthermore, identified set medium-to-long timescale ``controller"" units exhibit distinct strong projections control state units, set long-timescale ``integrator units"" showed influence predicting words long context relevant. In summary, findings advance understanding timescale distribution functional organization LSTM language models, provide method identifying important units representing long-range contextual information RNNs. In paper, predicted ratings Yelp review texts. Yelp Open Dataset used. The imbalanced data distribution presented, balanced training dataset built. Four machine learning models including Naive Bayes, Logistic Regression, Random Forest, Linear Support Vector Machine used based numerical features tf-idf vectorizer. Four transformer-based models including BERT, DistilBERT, RoBERTa, XLNet also trained tested textual data. Comparisons models hyperparameters done, 64\ accuracy score machine learning model 70\ accuracy score transformer-based one achieved testing set. Transformer-based models summarized experimented. Cased, large BERT models found giving better performances uncased, base ones. DistilBERT faster computation speed bit lower metrics, RoBERTa XLNet give higher evaluation metrics computational resources required. We hope work could give inspirations insights work Yelp review rating prediction based machine learning deep learning models. ----------------------------------------------------------- {\small }"," In the human brain, sequences of language input are processed within a distributed and hierarchical architecture, in which higher stages of processing encode contextual information over longer timescales. In contrast, in recurrent neural networks which perform natural language processing, we know little about how the multiple timescales of contextual information are functionally organized. Therefore, we applied tools developed in neuroscience to map the ``processing timescales of individual units within a word-level LSTM language model. This timescale-mapping method assigned long timescales to units previously found to track long-range syntactic dependencies. Additionally, the mapping revealed a small subset of the network  with long timescales and whose function had not previously been explored. We next probed the functional organization of the network by examining the relationship between the processing timescale of units and their network connectivity. We identified two classes of long-timescale units: ``controller units composed a densely interconnected subnetwork and strongly projected to the rest of the network, while ``integrator units showed the longest timescales in the network, and expressed projection profiles closer to the mean projection profile. Ablating integrator and controller units affected model performance at different positions within a sentence, suggesting distinctive functions of these two sets of units. Finally, we tested the generalization of these results to a character-level LSTM model and models with different architectures. In summary, we demonstrated a model-free technique for mapping the timescale organization in recurrent neural networks, and we applied this method to reveal the timescale and functional organization of neural language models.\footnote{The code and dataset to reproduce the experiment can be found at \url{https://github.com/sherrychien/LSTM_timescales}}"
"We summarize contribution follows: We demonstrated new method mapping timescale organization recurrent neural language models. Using method, mapped timescale distributions units within word-level character-level LSTM language models, identified small set units long timescales. We used network analyses understand relationship timescale unit connectivity profile, distinguished two subsets long-timescale units seemingly distinctive functions. Altogether, proposed methods combining timescale connectivity analyses discovering timescale functional organization language models. The units longer processing timescales included units whose role long-range language dependencies already established , almost long timescale units unknown function. The timescale mapping procedure described provides model-free method identifying nodes necessary long-range linguistic discursive processes . Future studies neural language models could focus specific linguistic information tracked long-timescale units, especially ``controller'' units control information flow units network. The current study measured unit timescales using simple token distance, method may applied understanding recurrent neural nets beyond language models. It insightful future studies investigate whether processing timescales characterized via token distance comparable measured using functional measures, syntactic distance. Relatedly, explored timescale variance several context conditions, thorough investigation needed examine timescales individual units may vary different positions within sentence, terms token location syntactic location. Processing timescales may exhibit analogous hierarchical organization LSTMs human cerebral cortex: cases, subset nodes high degree high inter-connectivity express unusually long timescales. More detailed testing apparent correspondence required, however, units within LSTM layer spatially embedded constrained biological brains, thus LSTM units express spatially graded timescale topography. \subsubsection*{Acknowledgments} C.J.H H-Y.S.C gratefully acknowledge support National Institutes Mental Health \section{Appendix} \subsection {Units excluded timescale analysis} We excluded 1 unit WLSTM model 5 units CLSTM model properly fit using logistic function; excluded 14 units WLSTM model 7 units CLSTM model either show non-zero activation difference shared segment started, whose activation differences increased started process shared segment. After exclusions, 635 units remained WLSTM 1012 units remained CLSTM analysis. \subsection {Timescale analyses across different datasets context conditions} \subsubsection {Wikipedia test dataset} The Anna Karenina corpus used current study different linguistic structure Wikipedia corpus WLSTM CLSTM models trained. Although analyzed Anna Karenina sentences low perplexity, important test robustness results across datasets. Thus, mapped timescale unit using Wikipedia test set, used \citet{gulordava2018colorless}. Specifically, sampled 500 long sentences containing ``, and"" Intact Context condition. As before, generated sentences preceding ``shared input'' segment either original prior context segment, randomly chosen prior context segment. Same original analysis, replaced context segment 30 context segments randomly sampled parts test set generating Random Context condition. The mapped timescales using Wikipedia test set highly correlated novel corpus, suggesting robustness unit timescales . \subsubsection {Timescales measured middle sentence} To examine timescales individual units may vary across different positions sentence, varied location segmentation point. Instead using conjunction segmentation point, chose arbitrary segmentation point: 15th token long sentence, separate context segment shared input segment. In Random Context condition, replaced context segment first 15 tokens sentences corpus. We found unit timescales highly correlated condition used conjunction segmentation point several units shift timescales either directions . This analysis conducted using Wikipedia test set. \subsubsection {Timescale reset beginning sentence} To examine timescales individual units flexibly reset beginning sentence, conducted timescale analysis using ``full stop"" segmentation point instead conjunction ``, and"". Thus, original test string ``The girl kicked call, boy caught it"", full-stop version test string would ``The girl kicked ball. The boy caught it."" In setting, context segment shared input segment Intact Context condition two consecutive sentences. To ensure temporal dependence context segment shared input segment, sampled 100 consecutive sentence pairs Anna Karenina corpus. Note possible using Wikipedia test set \citet{gulordava2018colorless}, set composed unrelated sentences. The Random Context condition generated replacing first sentence randomly sampled sentences parts novel. We found using ``full stop"" segment context shared input, units network showed timescale near 0, indicating near-zero dependence linguistic context text preceding full stop . This suggests units LSTM tend ``reset"" context representation beginning sentence. \subsubsection {Context representation shaped individual words } Inspired token-shuffling procedure \citet{khandelwal2018sharp}, explored whether context representations individual units LSTM shaped individual words, rather coherent sequences words. For analysis, instead replacing context syntactically structured segments part corpus, generated ``random context"" shuffling order words within context segment. We mapped unit timescales before, examining unit activation difference function distance onset shared input. Intriguingly, found units showed similar timescales across context-replacement context-shuffling procedures . This suggests context representations LSTMs largely depend presence individual words context, rather appearance within coherent linguistic sequences. However, observe subset units whose timescales longer context replaced rather shuffled. For subset units, ability maintain representation prior context many tokens depends prior context coherent linguistic sequence. This subset units promising target future studies syntactic representations LSTMs. \subsection {Identifying strong hidden-to-gate projections} First, hidden unit, concatenated corresponding rows matrices, generate single ``hidden-to-gate"" projection vector hidden unit. Next z-scored vector get standardized projection values unit units network. Using z-score 5 criterion, identified total 258 ``strong projections"" hidden units input gate forget gate WLSTM. The projection strength unit calculated based number ""strong projections"" . Although criterion z-score selected better visualize results Figure , different criteria change results units longer timescales strong projections. For example, using z-score 3 threshold obtained corr = 0.30, p0.001; z-score 4 obtained corr = 0.35, p0.001. Next, identified edges corresponding top 258 magnitude weight-values within combined matrices. Together, edges formed ""strong-projection network"". Finally, used k-core analysis identify main core strong-projection network. This main core composed ""controller units"" . Using criteria method, identified total 390 ``strong projections"" hidden units input gate forget gate CLSTM. We extracted top 390 weight values weight matrices construct ``strong-projection network"" identified main core network, composed ``controller units"" CLSTM model \subsection {Ablation analyses putative controller integrator units} To examine non-trivial roles controller integrator units identified word-level LSTM model, performed preliminary group ablation analysis look ablating controller units influences model performance predicting next token, relative ablation random set units. Specifically, since long-timescale integrator units effect predicting tokens later part sentences , examined model performance predicting tokens two different positions: tokens regardless positions sentences , last tokens sentences . We evaluated effects ablation model performance measuring differences probabilities assigned target words . Ablation effects controller units integrator units compared baseline ablating number randomly-selected units layer 2 LSTM . We used test corpus used \citet{gulordava2018colorless} measured average performance model across 100 text-batches, randomly sampled Wikipedia test dataset. Each text-batch composed 1000 tokens start beginning sentence. In ``All tokens"" condition, calculated P every token tested text, ``Final tokens"" condition, calculated P last token every sentence . We average P conditions across text-batches get mean performance difference ablated model intact model. Ablating controller units reduced probabilities assigned target words, ablating random units . In contrast, ablating integrator units reduced probabilities less ablating random units . We hypothesized integrator units mostly influence model's prediction performance tokens long-range information especially relevant, later portions clauses sentences. Consistent this, found that, examined ablation effects tokens final position sentence, ablating integrator units reduced probabilities ablating random units . Interestingly, ablating controller units reduced probability sentence-final targets less random units . In summary, ablation results indicate non-trivial functional role controller integrator units, despite fact subset units composed 10 amongst 650 total hidden units. Also, putative controller integrator sets appear distinctive roles within WLSTM, controllers supporting accurate predictions overall, integrator units appear boost accurate predictions end sentences. \subsection {Mapping timescale organization GRU language model} \subsubsection {Training} To explore whether timescale mapping methods, findings, may generalize model architectures, trained studied word-level GRU language model . As far possible, applied similar parameters GRU used LSTM \citet{gulordava2018colorless}: Wikipedia training corpus, loss function , hyperparameters except learning rate initialized 0.1, found optimal train GRU. The GRU model also two layers, 650 hidden units layer. We trained GRU model 30 epochs, point GRU converged validation perplexity 118.36. Note since adapted similar training settings used training LSTM model Gulordava et al. without model-specific optimization, perplexity higher LSTM model reported \citet{gulordava2018colorless} . We analyzed timescale hidden units using method used analyzing LSTMs, using test data derived training Wikipedia corpus. \subsubsection {Timescale organization GRU model} Similar LSTM model Gulordova et al, majority units GRU also showed shorter timescales. More specifically, found: second layer GRU model sensitive prior context first layer, LSTM ; distribution timescales across units similar GRU LSTM, although GRU showed right-skewed distribution larger proportion short-timescale units . \subsubsection {Timescale versus network connectivity GRU model} We also performed timescale vs. network connectivity analyses GRU model. Because update hidden states GRU controlled reset update gate, measured projection patterns hidden units analyzing matrix combined hidden-to-update-gate hidden-to-reset-gate weights. In contrast LSTM models, hidden units GRU trained show relationship longer timescales stronger hidden-to-gate projections . Moreover, using k-core analysis identify subunits interconnected high-degree units, core network GRU contained many units long short timescales. Interestingly, visualized position k-core units MDS space, tended locate edge space, similar found LSTM. This indicates that, LSTM, core units GRU distinctive profiles, distant one another units network . However, observe pattern ``integrator units"" GRU LSTM. These apparent similarities differences LSTM GRU intriguing, emphasize perplexity GRU model much higher LSTM, due sub-optimal parameter settings, comparing LSTM GRU connection patterns straightforward, overall distribution weights different. Further work required determine comparable thresholds trong projections igh-degree units case. As noted manuscript above, connectivity results exploratory; however, believe GRU analysis demonstrates methods extended map compare functional organization language models different architecture. Finally, note conducting timescale analysis incompletely trained GRU model , timescale distribution right-skewed better-trained GRU . Altogether, results suggest long-timescale units GRU gradually formed training process. \subsection {Mapping timescale organization word-level LSTM different hidden size} To examine whether number hidden units model would affect timescale organization LSTM, trained another 2-layer word-level LSTM model Wikipedia corpus similar parameter settings \citet{gulordava2018colorless}, 100 hidden units layer. We called model LSTM-100. We trained model 56 epochs model converged validation perplexity 98.75, conducted analysis described main text map timescales LSTM-100. Because LSTM-100 overall less weight connections, use z-score 3 criteria determine ``strong"" hidden-to-gate projections connectivity analyses. Regarding timescale distribution LSTM-100, found results similar 650-unit word-level LSTM model, that: second layer LSTM-100 showed context sensitivity first layer, although difficult quantitatively compare unit-level timescale distribution LSTM-100 model LSTM 650 units, contain similarly small subset long-timescale units. . We observe significant correlation unit timescale number strong projections generated unit LSTM-100 model: long-timescale units LSTM-100 connections short-timescale units. When visualizing MDS space connectivity similarity LSTM-100, ``controller units"" identified using k-core analysis located edge space, similar 650-unit LSTM model. Interestingly, observed subset long-timescale units center MDS space, analogous ``integrator units"" found 650-unit LSTM model. Altogether, pattern ``integrator units"" might commonly evolved feature shared LSTM model architectures, GRU architectures. \counterwithin{figure}{section} \renewcommand{\thefigure}{A.\arabic{figure}} \setcounter{figure}{0} \"," Keyphrase Generation  is the task of generating central topics from a given document or literary work, which captures the crucial information necessary to understand the content. Documents such as scientific literature contain rich meta-sentence information, which represents the logical-semantic structure of the documents.  However, previous approaches ignore the constraints of document logical structure, and hence they mistakenly generate keyphrases from unimportant sentences. To address this problem, we propose a new method called Sentence Selective Network  to incorporate the meta-sentence inductive bias into KG. In SenSeNet, we use a straight-through estimator for end-to-end training and incorporate weak supervision in the training of the sentence selection module. Experimental results show that SenSeNet can consistently improve the performance of major KG models based on seq2seq framework, which demonstrate the effectiveness of capturing structural information and distinguishing the significance of sentences in KG task."
"% With recent development end-to-end text-to-speech system, synthesised speech achieved high intelligibility quality various languages . Recently neural network based text-to-speech systems achieved certain success prosody naturalness synthesized speech conventional methods . % Because Chinese non-alphabet character set large, grapheme-to-phoneme essential hiring end-to-end model Chinese . By applying encoder-decoder framework attention , systems directly predict speech parameters graphemes phonemes learning acoustic prosodic patterns via flexible mapping linguistic acoustic space. % But still model part prosody structural information raw text limited model capacity, resulting poor expressiveness even prosody errors. % V_1021But still model part prosody structural information raw text resulting poor expressiveness even prosody errors. However, learnt prosodic patterns contain part prosodic structural information , resulting poor prosody naturalness performance even improper prosody. % So additional prosody structure information important improve naturalness synthesized speech text-to-speech system. % V5: So adding prosody information, prosody structure annotations, encoder-decoder based models important improve expressiveness synthesized speech TTS systems. % The G2P module converts text input sequence phonemes tones, intelligibility naturally synthesised Chinese speech perform better conventional TTS . % However, limited coverage phoneme permutation training data causes decline ability predict prosody, resulting unnatural prosody unexpected pause. % % V4: There many attempts improve prosody prediction ability TTS system introducing prosody structure information explicitly. % V5: Prosody structure annotations successfully applied TTS systems improve expressiveness. % V1020: To improve expressiveness synthesized speech, directly adding prosodic structure annotations, Tones break indices labels The MATE meta-scheme % v_1021:To improve expressiveness synthesized speech, adding prosodic structure annotations tones break indices labels prosodic structure annotation input sequence encoder-decoder based models proposed. To improve prosody naturalness synthesized speech, adding prosodic structure annotations tones break indices labels prosodic structure labels input sequence neural network based TTS models proposed. Prosodic structure annotations need subjectively labeled speech, time-consuming. Although annotations automatically annotated training another prosodic structure prediction model , accuracy predicted prosodic structure labels still limited using subjectively labeled annotations ground-truths. The high correlation syntactic structure prosodic information proved successful syntactic-to-prosodic mapping . % V1020: The syntactic parsing models trained large text database rich grammatical structure provide text TTS dataset usefully syntactic structure information. A set rule-based syntactic features part-of-speech positions current word parent phrases proposed used hidden Markov model based acoustic model . % So subjective labeled prosodic structure annotations replaced syntactic structure information, obtained text without referring speech. % In hidden markov model based acoustic model, set rules create syntactic features including part speech positions current word parent phrases hired syntactic structure information improve prosody naturalness exceeds prosodic structure annotations comprehensiveness granularity . % This provides us another method implicitly improve prosody using syntactic structure information, exceeds using prosody structure information explicitly comprehensiveness granularity. % Early hidden markov based TTS model, rich syntactic context instead prosody structure information used improved prosody synthesized . % The word relation based features proposed prior features, require expert knowledge designed. % explore syntactic information parse tree, improve generalization synthesised speech. To utilize syntactic structure information, phrase structure based feature word relation based feature proposed neural network based TTS . PSF WRF expand set syntactic features used HMM model. More features highest-level phrase beginning current word lowest common ancestor introduced model syntactic structure . However, expanded features still manually designed features rather automatically learned high-level representations. PSF contains features limited layers whole syntactic tree structure. WRF exposes information partial nodes edges whole syntactic parse tree. % PSF WRF model syntactic relation among limited subtrees rather whole syntactic parse tree structure. % contain feature syntactic tree structure design. % needs expert knowledge select % V1020: makes harder extract useful information leads instability. % way select specific layers parse tree, makes harder extract useful information leads instability. % V1020: And WRF focuses relation two adjacent words parsing tree structure, model limited information whole syntactic parse tree. For example, one WRF features highest-level phrase beginning current word . % WRF models . % expand partial higher structure . % limited manual selection strategy, WRF considers influence former word next word specific layer parent nodes, cannot model whole structure parse tree. % This makes prosody performance largely determined selected strategy, time unstable. %In Fig., show example synthesised speech phoneme sequence input different reference speech failing respect syntax structure. % Without parsing tree's limit, third word ""cu4 jin4"" pronounced separately . % Besides, without parsing tree information, synthesised speech pause fifth word ""ti2 xiao4"" sixth word ""shi4"", obvious gap parsing tree reflected reference speech . % simply plugging parsing tree information TTS perform well. Limited manual design rule, features disadvantages model syntax tree structure information. Firstly, using phrase structure feature needs fix number tree layers way select specific layer, using word relation feature make model select part parse tree structure, cannot proved useful part prosody modeling. This makes prosody performance largely determined selected strategy, time unstable. Secondly, word relation feature consider former words' influence next word ignore impact backward structure importance. Last least, manual design features require high accuracy syntax tree annotation, easily achieved. Otherwise, Otherwise influence manual selection strategy, destructive influence mislabeling prosody prediction magnified. % A syntactic parse tree traversal based method proposed learn syntactic representation employed neural machine translation . To maker better use syntactic information, motivated syntactic parse tree traversal approach neural machine translation , propose syntactic representation learning method improve prosody naturalness synthesized speech neural network based TTS. % To make better use syntactic information, paper, propose syntactic representation learning method improve prosody neural network based TTS. % also known phrase structure parsing, TTS system control prosody effective. Syntactic parse tree linearized two constituent label sequences left-first right-first traversal. % Word level bidirectional Then syntactic representations extracted constituent label sequences using different uni-directional GRU network sequence. After which, syntactic representations up-sampled word level phoneme level concatenated phoneme embeddings. Tacotron 2 employed generate spectrogram concatenated syntactic representations phoneme embeddings, Griffin-Lim reconstruct waveform. % directly Nuclear-norm maximization loss introduced constituent label embedding layer enhance discriminability diversity. Compared hiring left-first traversal , right-first traversal proposed alleviate ambiguity. Experimental results show proposed model outperforms baseline terms prosody naturalness. Mean opinion score increases compared baseline approach . % compared baseline approach, one-way ANOVA test. ABX preference rate exceeds baseline approach . % One-way ANOVA test reveals significant improvement . % We go explore enhanced controllability prosody benefit eliminate ambiguity. For sentences multiple different syntactic parse trees, prosodic differences clearly perceived corresponding synthesized speeches. %We linearize phrase parse tree structural label sequence propose rnn-based model learn useful syntactic information itself, experimental shows significantly better method manually extracting features. %To best known, first exploite syntactic information chinese TTS system first apply syntactic information lower input level word. %We also introduce rank loss syntactic label embedding enhance ability syntax structure control prosody, expanded specific application parsing tree information, including different sentences parsing tree structure bring prosodic structure, different trees sentence produce different prosodic readings. The latter brings solutions ambiguity caused grammatical structure In paper, propose novel method named SenSeNet keyphrase generation, automatically estimate whether sentences tended generate keyphrase. We use straight-through estimator solve model discontinuity problem. We incorporate weakly-supervised signal guide selection significant sentences efficiently. The experiment results show model successfully generate present keyphrase absent keyphrase. In addition, model training method applied encoder-decoder architectures. Further analysis suggests model edge semi-present keyphrase, although predicting absent keyphrase challenging."," Syntactic structure of a sentence text is correlated with the prosodic structure of the speech that is crucial for improving the prosody and naturalness of a text-to-speech  system.  Nowadays TTS systems usually try to incorporate syntactic structure information with manually designed features based on expert knowledge.  In this paper, we propose a syntactic representation learning method based on syntactic parse tree traversal to automatically utilize the syntactic structure information.  Two constituent label sequences are linearized through left-first and right-first traversals from constituent parse tree. Syntactic representations are then extracted at word level from each constituent label sequence by a corresponding uni-directional gated recurrent unit  network.  Meanwhile, nuclear-norm maximization loss is introduced to enhance the discriminability and diversity of the embeddings of constituent labels.  Upsampled syntactic representations and phoneme embeddings are concatenated to serve as the encoder input of Tacotron2.  Experimental results demonstrate the effectiveness of our proposed approach, with mean opinion score  increasing from $3.70$ to $3.82$ and ABX preference exceeding by $17\%$ compared with the baseline. In addition, for sentences with multiple syntactic parse trees, prosodic differences can be clearly perceived from the synthesized speeches."
"Semantic parsing task mapping natural language utterances machine interpretable meaning representations. Many semantic parsing methods based principle semantic compositionality ~, main idea put together meanings utterances combining meanings parts~. However, methods suffer heavy dependence handcrafted grammars, lexicons, features. To overcome problem, many neural semantic parsers proposed achieved promising results~. %\textcolor{red}{However, compared compositional semantic parsers, neural semantic parsers aware compositional structure utterances, often limits generalization various compound-complex utterances: However, due lack capturing compositional structures utterances, neural semantic parsers usually poor generalization ability handle unseen compositions semantics~. For example, parser trained ``How many rivers run oklahoma?'' ``Show states bordering colorado?'' may perform well ``How many rivers run states bordering colorado?''. \end{table} \end{table*} In paper, propose novel framework boost neural semantic parsers principle compositionality~. It iterates segmenting span utterance parsing partial meaning representation. Table shows example. Given utterance ``How many rivers run states bordering colorado?'', parse three iterations: segment span ``the states bordering colorado'' utterance, parse ; utterance reduced ``How many rivers run \?'', segment span ``rivers run \'' it, parse ; utterance reduced ``How many \?'', parse . We compose partial meaning representations final result. Our framework consists two neural modules: utterance segmentation model base parser . The former charge segmenting span utterance, latter charge parsing span meaning representation. These two modules work together parse complex input utterances divide-and-conquer fashion. One key advantage framework require handcraft templates additional labeled data utterance segmentation: achieve proposing novel training method, base parser provides pseudo supervision utterance segmentation model. Specifically: train preliminary base parser original train data; then, train sample , use preliminary base parser check whether spans parsed \textcolor{red}{be} part \textcolor{red}{or not}. If true, leverage spans pseudo supervision signals training utterance segmentation model, thereby require handcraft templates additional labeled data.} %The key implement framework address challenge lacking labeled data utterance segmentation. %We achieve cooperative training segmentation model base parser: %leverage pre-trained base parser derive synthetic supervision signals training segmentation model, leverage segmentation model derive synthetic supervision signals updating base parser. % \textcolor{green}{Moreover, considering usually labeled data utterance segmentation, propose search reasonable segmentation points utterances via base parser, use distant supervision. This improves domain adaptability framework.} % While lacking direct supervision segmentation model, seek address challenge distantly supervised way. % shaped like %\textcolor{red}{ %Firstly, train base parser, use search evaluate viable ways segment training utterances. %Then, segmentations leveraged distant supervision training utterance segmentation model fine-tuning base neural semantic parser.} In summary, proposed framework four advantages: base parser learns parse simpler spans instead whole complex utterances, thus alleviating training difficulties improving compositional generalization ability; framework flexible incorporate various popular encoder-decoder models base parser; framework require handcraft templates additional labeled data utterance segmentation; % framework addresses challenge lacking labeled data utterance segmentation cooperative training. framework improves interpretability neural semantic parsing providing explicit alignment spans partial meaning representations. We conduct experiments three datasets: Geo~, ComplexWebQuestions~, Formulas . They use different forms meaning representations: FunQL, SPARQL, Spreadsheet Formula. Experimental results show framework consistently improves performances neural semantic parsers different domains. On data splits require compositional generalization, framework brings significant accuracy gain: Geo , Formulas , ComplexWebQuestions . In study, investigate syntactic representation learning method automatically utilize syntactic structure information neural network based TTS. Nuclear-norm maximization loss introduced enhance discriminability diversity synthsized speech prosody. Experimental results demonstrate effectiveness proposed approach. For sentences multiple syntactic parse trees, prosodic difference clearly observed synthesized speeches. To start new column help balance last-page column length use \vfill\pagebreak. ------------------------------------------------------------------------- \vfill \pagebreak \vfill\pagebreak References produced using bibtex program suitable BiBTeX files . The IEEEbib.bst bibliography style file IEEE produces unsorted bibliography list. -------------------------------------------------------------------------"," Neural semantic parsers usually fail to parse long and complex utterances into correct meaning representations, due to the lack of exploiting the principle of compositionality. To address this issue, we present a novel framework for boosting neural semantic parsers via iterative utterance segmentation. Given an input utterance, our framework iterates between two neural modules: a segmenter for segmenting a span from the utterance, and a parser for mapping the span into a partial meaning representation. Then, these intermediate parsing results are composed into the final meaning representation. One key advantage is that this framework does not require any handcraft templates or additional labeled data for utterance segmentation: we achieve this through proposing a novel training method, in which the parser provides pseudo supervision for the segmenter. Experiments on Geo, ComplexWebQuestions and Formulas show that our framework can consistently improve performances of neural semantic parsers in different domains. On data splits that require compositional generalization, our framework brings significant accuracy gains: Geo $63.1\to 81.2$, Formulas $59.7\to 72.7$, ComplexWebQuestions $27.1\to 56.3$."
"Word alignment task finding corresponding words sentence pair used key component statistical machine translation . Although word alignment longer explicitly modeled neural machine translation , often leveraged interpret analyze NMT models . Word alignment also used many scenarios, imposing lexical constraints decoding process , improving automatic post-editing providing guidance translators computer-aided translation . Recently, unsupervised neural alignment methods studied outperformed GIZA++ many alignment datasets . However, methods trained translation objective, computes probability target token conditioned source tokens previous target tokens. This bring noisy alignments prediction ambiguous . To alleviate problem, previous studies modify Transformer adding alignment modules re-predict target token , computing additional alignment loss full target sequence . Moreover, \citet{chen2020accurate} propose extraction method induces alignment to-be-aligned target token decoder input. Although methods demonstrated effectiveness, two drawbacks. First, retain translation objective tailored word alignment. Consider example Figure . When predicting target token ``Tokyo'', translation model may wrongly generate ``1968'' considers previous context, result incorrect alignment link . A better modeling needed obtaining accurate alignments. Second, need additional guided alignment loss outperform GIZA++, requires inducing alignments entire training corpus. In paper, propose self-supervised model specifically designed word alignment task, namely Mask-Align. Our model masks target token recovers source rest target tokens. For example, shown Figure , target token ``Tokyo'' masked re-predicted. During process, model identify source token ``Tokio'' translated yet, to-be-predicted target token ``Tokyo'' aligned ``Tokio''. Comparing translation model, masked modeling method highly related word alignment, based model generates accurate predictions alignments. % We model target token conditioned tokens source target, disambiguate prediction thus lead accurate alignment ). As vanilla transformer architecture requires sequential time model probability, modify attention decoder separating queries keys values % updating former layer. This allows model predict target tokens single forward pass without information leakage. Besides, also propose variant attention called leaky attention allieviates unexpected high attention weights specific tokens periods, helpful alignment extraction attention matrix. Finally, leverage attention weights models two directions incorporating agreement loss training process. % Experiments four public datasets show model significantly outperforms existing statistical neural methods without using guided alignment loss. To summarize, main contributions work listed follows: In paper, propose novel framework boosting neural semantic parsers via iterative utterance segmentation. The insight bottom-up divide-and-conquer mechanism, significantly improves compositional generalization ability interpretability neural semantic parsers. Considering usual absence labeled data utterance segmentation, propose cooperative training method tackle problem. Experimental results show framework consistently improves performance different neural semantic parsers across tasks. In future, plan improve robustness framework various complex language phenomena. We also plan apply framework semantic parsing tasks text-to-SQL text-to-code."," Neural word alignment methods have received increasing attention recently. These methods usually extract word alignment from a machine translation model. However, there is a gap between translation and alignment tasks, since the target future context is available in the latter. In this paper, we propose Mask-Align, a self-supervised model specifically designed for the word alignment task. Our model parallelly masks and predicts each target token, and extracts high quality alignments without any supervised loss. In addition, we introduce leaky attention to alleviate the problem of unexpected high attention weights on special tokens. Experiments on four language pairs show that our model significantly outperforms all existing unsupervised neural baselines and obtains new state-of-the-art results.  % However, the original translation objective ignores the future context in the target, which is available in the alignment task."
"The sequence-to-sequence models~, learn map arbitrary-length input sequence another arbitrary-length output sequence, successfully tackled wide range language generation tasks. % including machine translation, text summarization, question generation, name few. Early seq2seq models used recurrent neural networks encode decode sequences, leveraging attention mechanism allows decoder attend specific token input sequence capture long-term dependencies source target sequences. Recently, Transformer~, all-attention model effectively captures long-term relationships tokens input sequence well across input output sequences, become de facto standard text generation tasks due impressive performance. Moreover, Transformer-based language models trained large text corpora shown significantly improve model performance text generation tasks. %Seq2seq tasks becoming increasingly important, show text-based language problems cast sequence-to-sequence problems. However, crucial limitation seq2seq models mostly trained teacher forcing, ground truth provided time step thus never exposed incorrectly generated tokens training ), hurts generalization. This problem known ``exposure bias"" problem often results generation low-quality texts unseen inputs. Several prior works tackle problem, using reinforcement learning maximize non-differentiable reward . % --- BLEU Rouge. Another approach use RL gumbel softmax match distribution generated sentences ground truth, case reward discriminator output Generative Adversarial Network . Although aforementioned approaches improve performance seq2seq models text generation tasks, either require vast amount effort tuning hyperparameters stabilize training. %Moreover, show RL methods machine translation often optimize expected reward performance gain attributed side effects, increasing peakiness output distribution. In work, propose mitigate exposure bias problem simple yet effective approach, contrast positive pair input output sequence negative pairs, expose model various valid incorrect sentences. Naely, construct negative pairs simply using random non-target sequences batch~. However, nae construction yields meaningless negative examples already well-discriminated embedding space ), highlight reason existing methods~ require large batch size. This clearly shown Fig., large portion positive-negative pairs easily discriminated without training, gets worse batch size decreases reduce chance meaningfully difficult examples batch. Moreover, discriminating positive nae negative pairs becomes even easier models pretrained large text corpora. To resolve issue, propose principled approaches automatically generate negative positive pairs constrastive learning, refer Contrastive Learning Adversarial Perturbation Seq2seq learning . Specifically, generate negative example adding small perturbation hidden representation target sequence, conditional likelihood minimized ). Conversely, construct additional positive example ) adding large amount perturbation hidden representation target sequence perturbed sample far away source sequence embedding space, enforcing high conditional likelihood minimizing Kullback-Leibler divergence original conditional distribution perturbed conditional distribution. This yield negative example close original representation target sequence embedding space largely dissimilar semantics, generated positive example far away original input sequence semantic target sequence. This generate difficult examples model fails correctly discriminate , Fig.2), helping learn meaningful pairs. To verify efficacy method, empirically show significantly improves performance seq2seq model three conditional text generation tasks, namely machine translation, text summarization question generation. Our contribution work threefold: In paper, propose self-supervised neural alignment model Mask-Align. Different NMT-based methods, model adopts novel masked modeling objective suitable word alignment tasks. Moreover, Mask-Align alleviate problem high attention weights special tokens introducing leaky attention. Experiments show Mask-Align achieves new state-of-the-art results without guided alignment loss. We leave future work extend model semi-supervised setting."," Recently, sequence-to-sequence  models with the Transformer architecture have achieved remarkable performance on various conditional text generation tasks, such as machine translation. However, most of them are trained with teacher forcing with the ground truth label given at each time step, without being exposed to incorrectly generated tokens during training, which hurts its generalization to unseen inputs, that is known as the ``exposure bias"" problem. In this work, we propose to mitigate the conditional text generation problem by contrasting positive pairs with negative pairs, such that the model is exposed to various valid or incorrect perturbations of the inputs, for improved generalization. However, training the model with na contrastive learning framework using random non-target sequences as negative examples is suboptimal, since they are easily distinguishable from the correct output, especially so with models pretrained with large text corpora. Also, generating positive examples requires domain-specific augmentation heuristics which may not generalize over diverse domains. To tackle this problem, we propose a principled method to generate positive and negative samples for contrastive learning of seq2seq models. Specifically, we generate negative examples by adding small perturbations to the input sequence to minimize its conditional likelihood, and positive examples by adding  large perturbations while enforcing it to have a high conditional likelihood. Such ``hard'' positive and negative pairs generated using our method guides the model to better distinguish correct outputs from incorrect ones. We empirically show that our proposed method significantly improves the generalization of the seq2seq on three text generation tasks --- machine translation, text summarization, and question generation."
"Task-specific finetuning pretrained deep networks become dominant paradigm contemporary NLP, achieving state-of-the-art results across suite natural language understanding tasks . While straightforward empirically effective, approach difficult scale multi-task, memory-constrained settings , requires shipping storing full set model parameters task. Inasmuch models learning generalizable, task-agnostic language representations self-supervised pretraining, finetuning entire model task seems especially profligate. A popular approach parameter-efficiency pretrained models learn sparse models task subset final model parameters exactly zero~. Such approaches often face steep sparsity/performance tradeoff, substantial portion nonzero parameters still typically required match performance dense counterparts. An alternative use multi-task learning feature-based transfer parameter-efficient transfer learning pretrained models~. These methods learn small number additional parameters top shared model. However, multi-task learning generally requires access tasks training prevent catastrophic forgetting~, feature-based transfer learning typically outperformed full finetuning~. Adapters~ recently emerged promising approach parameter-efficient transfer learning within pretrain-finetune paradigm~. Adapter layers smaller, task-specific modules inserted layers pretrained model, remains fixed shared across tasks. These approaches require access tasks training, making attractive settings one hopes obtain share performant models new tasks arrive stream. \citet{houlsby2019adapters} find adapter layers trained BERT match performance fully finetuned BERT GLUE benchmark requiring 3.6\% additional parameters per task. In work, consider similar setting adapters propose new diff pruning approach goal even parameter-efficient transfer learning. Diff pruning views finetuning learning task-specific \underline{diff}erence vector%\footnote{Similar command Unix operating systems.} \ applied top pretrained parameter vector, remains fixed shared across different tasks. In order learn vector, reparameterize task-specific model parameters , pretrained parameter vector fixed task-specific diff vector finetuned. The diff vector regularized differentiable approximation -norm penalty~ encourage sparsity. This approach become parameter-efficient number tasks increases requires storing nonzero positions weights diff vector task. The cost storing shared pretrained model remains constant amortized across multiple tasks. On GLUE benchmark~, diff pruning match performance fully finetuned BERT baselines finetuning pretrained parameters per task, making potential alternative adapters parameter-efficient transfer learning. In paper, propose Disentanglement-based Attractive Headline Generator generate attractive headline. Our model built fact attractiveness headline comes style content aspects. Given prototype document-headline pair, DAHG disentangles attractive content style space prototype attractive headline. The headline generator generates attractive headlines guidance both. Our model achieves state-of-the-art results terms ROUGE scores human evaluations large margin. In near future, aim bring model online. \clearpage"," While task-specific finetuning of pretrained networks has led to significant empirical advances in NLP, the large size of networks makes finetuning difficult to deploy in multi-task, memory-constrained settings. We propose diff pruning as a simple approach to enable parameter-efficient transfer learning within the pretrain-finetune framework. This approach views finetuning as learning a task-specific ``diff"" vector that is applied on top of the pretrained parameter vector, which remains fixed and is shared across different tasks. The diff vector is adaptively pruned during training with a differentiable approximation to the $L_0$-norm penalty to encourage sparsity. Diff pruning becomes parameter-efficient as the number of tasks increases, as it requires storing only the nonzero positions and weights of the diff vector for each task, while the cost of storing the shared pretrained model remains constant. It further does not require access to all tasks during training, which makes it attractive in settings where tasks arrive in stream or the set of tasks is unknown. We find that models finetuned with diff pruning can match the performance of fully finetuned baselines on the GLUE benchmark while only modifying 0.5$\%$ of the pretrained model's parameters per task.\blfootnote{ \hspace{-6mm} Our code is available at \url{https://github.com/dguo98/DiffPruning}}"
"Goal-oriented dialogue systems hot topic machine learning research. The systems widespread applications industry foundation many successful products, including Alexa, Siri, Google Assistant, Cortana. One core component dialog system spoken language understanding , consists two main problems, intent classification slot labeling . In IC, attempt classify goal user query, usually input text transcribed automatic speech recognition system audio. SL, similar named-entity recognition problem, aims label token query entity type. The difference entity types SL domain-specific based upon dialog ontology. Recent advances neural models enabled greatly improved SLU . However, two significant challenges hinder broad application expansion SLU models industrial settings. First all, neural methods require large amount labeled data training . SLU often coupled ontology underlying dialog system thus domain-dependent. Collecting large number in-domain labeled data neural models prohibitively expensive time-consuming. Secondly, performance SLU models practice often suffers fluctuations due various types noises. One common noise adaptation data perturbation. In many industrial applications cloud services\footnote{Alexa ASK: https://developer.amazon.com/en-US/alexa/alexa-skills-kit; Google DialogFlow: https://dialogflow.com/}, SLU model built fine-tuning pre-trained, shared network target domain data provided developers. The developers often limited background SLU machine learning. Thus data provided varies quality subject different types perturbations, missing replaced data samples typos. Another common noise comes mismatch input modalities adaptation inference stages. For instance, model adapted human transcription yet deployed understand ASR decoded text, input adaptation inference stages relies recognition different versions ASR models. Given neural methods comprise large number parameters heavily optimized training data provided, resulting model usually sensitive noises. The requirement noise-free adaptation inference conditions also prohibits use neural SLU techniques often infeasible achieve conditions. Transfer learning meta-learning two conventional techniques applied address challenge data scarcity. Transfer learning usually refers pre-training initial models using mismatched domains rich human annotations adapting models limited labels targeted domains. Previous works shown promising results applying transfer learning SLU. Note pre-training discussed covers methods including using pre-trained language model like BERT directly training downstream tasks data mismatched domains pre-trained model. In following, focus latter due utilizing data domains better yielding higher accuracy. In recent years, meta-learning gained growing interest among machine learning fields tackling few-shot learning scenarios. Model-Agnostic Meta-Learning focuses learning parameter initialization multiple subtasks, initialization fine-tuned labels yield good performance targeted tasks. Metric-based meta-learning, including prototypical networks matching networks , aim learn embedding metric space generalized domains unseen training set adaptation small number examples unseen domains. Recent work unveils excellent potential applying meta-learning techniques SLU few-shot learning context . As compared data scarcity, another challenge SLU, robustness noises, also gaining attention. Simulated ASR errors used augment training data SLU models . Researchers also leverage information confusion networks lattices , adversarial training techniques models learn query embeddings robust ASR errors. For text input, methods also explored model robustness noises misspelling acronym . In contrast noise types gained attention, best knowledge, prior work investigating impact missing replaced examples adaptation data. Moreover, intersection data scarcity noise robustness unexplored. Since scarcity labeled data data noisiness usually co-occur SLU applications , lack studies intersectional areas hinders use neural SLU models expansion broader use cases. Given deficiency, establish novel few-shot noisy SLU task introducing two common types natural noise, adaptation example missing/replacing modality mismatch, previously defined few-shot IC/SL splits . The task built upon three public datasets, ATIS , SNIPS , TOP . We propose noise-robust few-shot SLU model based ProtoNets established task. In summary, primary contributions 3-fold: 1) formulating first few-shot noisy SLU task evaluation framework, 2) proposing first working solution few-shot noisy SLU existing ProtoNet algorithm, 3) context noisy scarce learning examples, comparing performance proposed method conventional techniques, including MAML fine-tuning based adaptation. We propose diff pruning simple approach parameter-efficient transfer learning pretrained models. Experiments standard NLP benchmarks models show diff pruning match performance fully finetuned baselines requiring additional parameters per task. We also propose structured variant diff pruning provides improvements. Avenues future work include applying approach architectures , injecting parameter-efficiency objectives directly pretraining process , combining diff pruning techniques achieve even greater parameter-efficiency.","    Recently deep learning has dominated many machine learning areas, including spoken language understanding . However, deep learning models are notorious for being data-hungry, and the heavily optimized models are usually sensitive to the quality of the training examples provided and the consistency between training and inference conditions. To improve the performance of SLU models on tasks with noisy and low training resources, we propose a new SLU benchmarking task: few-shot robust SLU, where SLU comprises two core problems, intent classification  and slot labeling . We establish the task by defining few-shot splits on three public IC/SL datasets, ATIS, SNIPS, and TOP, and adding two types of natural noises  to the splits. We further propose a novel noise-robust few-shot SLU model based on prototypical networks. We show the model consistently outperforms the conventional fine-tuning baseline and another popular meta-learning method, Model-Agnostic Meta-Learning , in terms of achieving better IC accuracy and SL F1, and yielding smaller performance variation when noises are present."
"In modern world, social media playing part several ways, instance news dissemination information sharing, social media outlets, Twitter, Facebook, Instagram, proved effective . However, also comes several challenges, collecting information several sources, detecting filtering misinformation . Similar events pandemics, one deadly pandemics history, COVID-19 subject discussion social media since emergence. Without surprise, lot misinformation pandemic circulated social networks. In order identify misinformation spreaders filter fake news COVID-19 5G conspiracy, task namely ""FakeNews: Corona Virus 5G Conspiracy Multimedia Twitter-Data-Based Analysis"" proposed benchmark MediaEval 2020 competition . This paper provides detailed description methods proposed team DCSE\_UETP fake news detection task. The task consists two parts, namely text-based misinformation detection , structure-based misinformation detection . The first task based textual analysis COVID-19 related information shared Twitter January 2020 15th July 2020, aims detect different types conspiracy theories COVID-19 vaccines, ""the 5G weakens immune system thus caused current corona-virus pandemic etc., . In SMD task, participants provided set graphs, representing sub-graph Twitter, corresponds single tweet vertices graphs represent accounts. Similar TMD, task, participants need detect differentiate 5G COVID-19 conspiracy theories. In paper, establish novel SLU task, few-shot noisy SLU, existing public datasets. We propose ProtoNets based approach, Proto, build IC SL classifiers noisy examples. When noise few-shot examples, Proto yields better performance approaches utilizing MAML fine-tuning frameworks. Proto also achieves highest robust IC accuracy SL F1 two types noise, adaptation example missing/replacing modality mismatch, injected adaption evaluation set respectively. We believe ensemble nature ProtoNets benefits model robustness, simplicity Proto's model architecture also helpful few-shot noisy scenario. Our contribution step toward efficient robust deployment SLU models. While results promising, still substantial work, creation few-shot SLU datasets covering noises studies faster stabler learning algorithms, pursuit goal. \renewcommand{\thesection}{\Alph{section}}"," The paper presents our solutions for the MediaEval 2020 task namely FakeNews: Corona Virus and 5G Conspiracy Multimedia Twitter-Data-Based Analysis. The task aims to analyze tweets related to COVID-19 and 5G conspiracy theories to detect misinformation spreaders. The task is composed of two sub-tasks namely  text-based, and  structure-based fake news detection. For the first task, we propose six different solutions relying on Bag of Words  and BERT embedding. Three of the methods aim at binary classification task by differentiating in 5G conspiracy and the rest of the COVID-19 related tweets while the rest of them treat the task as ternary classification problem. In the ternary classification task, our BoW and BERT based methods obtained an F1-score of .606\% and .566\% on the development set, respectively. On the binary classification, the BoW and BERT based solutions obtained an average F1-score of .666\% and .693\%, respectively. On the other hand, for structure-based fake news detection, we rely on Graph Neural Networks  achieving an average ROC of .95\% on the development set."
"Sentiment classification task analyzing piece text predict orientation attitude towards event opinion. The sentiment text either positive negative. Sometimes, neutral perspective also considered classification. SA many different applications, reducing early age suicide rate identifying cyberbullying , discouraging unwarranted activities towards particular community hate-speech detection , monitoring public response towards proposed government bill among many others. The task SA achieved superior improvement languages, i.e. English - 97.1\% accuracy 2-class 91.4\% accuracy 3-class SA . But research works published SA Bengali. This lack quality datasets Bengali training computation model sentiment classification. However, last years, seen rise Internet users Bengali domain mostly due development wireless network infrastructure throughout South East Asia. This resulted massive increase total number online social network users well newspaper readers. So became comparatively easier collect public comments posted online Bengali news websites. % \end{table} Thus created two SA datasets 2-class 3-class SA Bengali trained multi-lingual BERT model via transfer learning approach sentiment classification Bengali, referred paper. achieves accuracy 71\% 2-class 60\% 3-class manually tagged dataset. We use model analyze sentiment 1,002 public comments collected online daily newspaper. Table shows general, sentiment public comments positive religious news articles, negative political sports news articles. In paper, present following contributions: % \makeatletter % \patchcmd{\@makecaption} % {\scshape} % {} % {} % {} % \makeatletter % \patchcmd{\@makecaption} % {\\} % {.\ } % {} % {} % \makeatother % \def\tablename{Table} We introduce Sequence Mixup, set regularization data augmentation techniques RNNs. Our work thought extending input mixup manifold mixup , originally porposed feed-forward neural nets. For case manifold mixup, propose two distinct variants called Pre-Output Throgh-Time Mixup, respectively. An asymptotic theoretical analysis reveals Pre-Output Mixup imposes locally linear behavior network's output generating section. In classification task, property leads partitioning hidden representation space set orthogonal affine subspaces, corresponds unique class. Experimental results showed improvement loss F-1 scores 1) baseline 2) state-of-the-art model CoNLL-2003 NER task. We studied correlation mixup coefficients consecutive time-steps, found using identical coefficients achieves better loss F-1 NER task. However, time, conjecture optimal correlation values mixup coefficients across time may vary task task thus requires experimental exploration adjusted. Lastly, considerable reduction test loss achieved sequence mixup methods implies employing sequence mixup methods language models may lead substantial improvement test perplexity."," Sentiment analysis  in Bengali is challenging due to this Indo-Aryan language's highly inflected properties with more than 160 different inflected forms for verbs and 36 different forms for noun and 24 different forms for pronouns. The lack of standard labeled datasets in the Bengali domain makes the task of SA even harder. In this paper, we present manually tagged 2-class and 3-class SA datasets in Bengali. We also demonstrate that the multi-lingual BERT model with relevant extensions can be trained via the approach of transfer learning over those novel datasets to improve the state-of-the-art performance in sentiment classification tasks. This deep learning model achieves an accuracy of 71\% for 2-class sentiment classification compared to the current state-of-the-art accuracy of 68\%. We also present the very first Bengali SA classifier for the 3-class manually tagged dataset, and our proposed model achieves an accuracy of 60\%. We further use this model to analyze the sentiment of public comments in the online daily newspaper. Our analysis shows that people post negative comments for political or sports news more often, while the religious article comments represent positive sentiment. The dataset and code is publicly available \footnote{ https://github.com/KhondokerIslam/Bengali\_Sentiment}."
"Methods automatically learning phone- word-like units unlabelled speech audio could enable speech technology severely low-resourced settings could lead new cognitive models human language acquisition. The goal unsupervised representation learning phone units learn features capture phonetic contrasts invariant properties like speaker channel. Early approaches focussed learning continuous features. In attempt better match categorical nature true phonetic units, recent work considered discrete representations. One approach use self-supervised neural network intermediate layer quantizes features using learned codebook. While discrete codes vector quantized networks given improvements intrinsic phone discrimination tasks, still encode speech much higher bitrate true phone sequences. As example, top Figure shows code indices vector-quantized variational autoencoder overlaid input spectrogram. While correspondence code assignments true phones , although repetition codes adjacent frames , input speech often assigned codes distinct surrounding frames. This surprising since VQ model explicitly encouraged so. The result encoding much higher bitrate true phone sequences . In paper consider ways constrain VQ models contiguous feature vectors assigned code, resulting low-bitrate segmentation speech discrete units. We specifically compare two VQ segmentation methods. Both based recent method segmenting written character sequences. The first method greedy approach, closest adjacent codes merged set number segments reached. The second method allows arbitrary number segments. A squared error blocks feature vectors VQ codes used together penalty term encouraging longer-duration segments. The optimal segmentation found using dynamic programming. We apply two segmentation approaches using encoders codebooks two VQ models . The first type VQ-VAE. The second vector-quantized contrastive predictive coding model. The combination two models two segmentation approaches gives total four VQ segmentation models consider. %Applying models segmentation approaches gives total four model combinations. We evaluate four different tasks: unsupervised phone segmentation, ABX phone discrimination, same-different word discrimination, inputs symbolic word segmentation algorithm. The last-mentioned particularly important since segmentation clustering % word-like units %from unlabelled speech remains major important challenge. On metrics four tasks combination VQ-VAE penalized dynamic programming approach best VQ segmentation method. Example output shown middle Figure. Compared existing methods, achieve state-of-the-art performance four evaluation tasks. However, achieves reasonable performance much lower bitrate existing methods. This noteworthy since, methods tailored respective tasks, single VQ segmentation approach used without alteration directly range problems. In paper, solid experimental proof behind unseen co-relational depth task, embedding feature extractor end-to-end models, achieved state-of-the-art 2-class 3-class sentimental tasks Bengali language yet bloom domain. Primarily, showing limitations drawbacks available pre-processing tools, claimed operating different level embedding first step reap immediate success. Thereby extensive analysis relative findings, shown sub-word level functional embedding, BERT, RNN architecture must Bengali sentiment classification tasks. Moreover, took step closer real world letting model identify public sentiment newspaper topics never done language. However, data-set expansion making BERT suitable Bengali linguistics, huge room improvement research team already started working on. Furthermore, fixing issues, many ground-breaking applications like cyberbullying identification, hate-speech detection introduced help make Bengali potential language NLP practitioner also ease life many native Bengali speakers. In paper, presented two manually tagged novel datasets SA Bengali. We also introduced BERT extsubscript{BSA}, deep learning model SA Bengali, outperforms models. We achieved state-of-the-art performance 2-class 3-class SA tasks Bengali. Moreover, took step closer apply SA model real world application analyzing public sentiment newspaper topics. The result shows religious news comments people tend possess positive sentiment whereas political sports news comments, people possess negative sentiment. However, research work progress regularly updated new insights. We continuing increase size SA datasets Bengali explore application deep learning models better results. We hope improved performance SA multi-class classification tasks presented paper help many ground-breaking applications like cyberbullying identification well hate-speech detection Bengali."," We investigate segmenting and clustering speech into low-bitrate phone-like sequences without supervision. We specifically constrain pretrained self-supervised vector-quantized~ neural networks so that blocks of contiguous feature vectors are assigned to the same code, thereby giving a variable-rate segmentation of the speech into discrete units. Two segmentation methods are considered. In the first, features are greedily merged until a prespecified number of segments are reached. The second uses dynamic programming to optimize a squared error with a penalty term to encourage fewer but longer segments. We show that these VQ segmentation methods can be used without alteration across a wide range of tasks: unsupervised phone segmentation, ABX phone discrimination, same-different word discrimination, and as inputs to a symbolic word segmentation algorithm. The penalized method generally performs best. While results are only comparable to the state-of-the-art in some cases, in all tasks a reasonable competing approach is outperformed at a substantially lower bitrate."
"Content based websites Quora, Reddit, StackOverflow primarily used seeking genuine answers questions. People different domains put questions educators people knowledgeable certain field answer them. One major impediment plain sailing execution information exchange proliferation toxic comments. The key challenge weed toxic comments termed Insincere Questions. An Insincere Question designated comment intended make statement look genuine answers. An Insincere Question characterised by: This major class problem pertains Text classification benchmark problem evaluating various research advancements natural language processing. While traditional machine learning algorithms naive bayes, logistic regression decision trees rightfully applied problem, suffer major impediments constructs. Vanilla RNNs, Gated Recurrent Unit Long Short Term Memory Networks replaced usage new state art. Even though LSTMs GRUs performed well, failed capture dependencies long range sentences. Now advent Transfer Learning, Language model pre-training proven useful learning universal language representations. Researchers field developing new better language models unprecedented speed. Applying new state art models could improve current methods replace manual labeling tasks text classification, also find widespread application similar fields, machine translation question answering. In paper, test applying new transformer models BERT-family improve current method binary text classification context Insincere Questions Classification. We make use Quora Insincere Questions Classification dataset purpose We find models achieve remarkable results classifying given data , BERT achieving best results compared RoBERTa, DistilBERT, ALBERT. This indicates models well equipped take tasks researchers previously solved less optimal ways. Active transfer learning using amalgamation results multiple models novel and, proved above, successful methodology identifying causal sentences. This two class problem, whereby aimed correctly identify causal sentences, shows high maintainable recall rates. While performance methodology, terms accuracy precision improved incorporating additional active learning iterations, results still significant enough used practically solving two class textual mining problem. In future, shall look towards application methodology solving real world problems, generation patient summaries clinical text. Copyright 2007, 2008, 2009 Elsevier Ltd This file part 'Elsarticle Bundle'. --------------------------------------------- It may distributed conditions LaTeX Project Public License, either version 1.2 license later version. The latest version license http://www.latex-project.org/lppl.txt version 1.2 later part distributions LaTeX version 1999/12/01 later. The list files belonging 'Elsarticle Bundle' given file `manifest.txt'. Template article Elsevier's document class `elsarticle' numbered style bibliographic references SP 2008/03/01 \documentclass[preprint,12pt,3p]{elsarticle} Use option review obtain double line spacing \documentclass[preprint,review,12pt]{elsarticle} Use options 1p,twocolumn; 3p; 3p,twocolumn; 5p; 5p,twocolumn journal layout: \documentclass[final,1p,times]{elsarticle} \documentclass[final,1p,times,twocolumn]{elsarticle} \documentclass[final,3p,times]{elsarticle} \documentclass[final,3p,times,twocolumn]{elsarticle} \documentclass[final,5p,times]{elsarticle} \documentclass[final,5p,times,twocolumn]{elsarticle} \usepackage{float} use PostScript figures article use graphics package simple commands \usepackage{graphics} use graphicx package complicated commands \usepackage{amsmath,amssymb,amsfonts} \usepackage{algorithmic} \usepackage{algorithm2e} \usepackage{textcomp} \usepackage{float} \usepackage{longtable} \usepackage{xcolor} use epsfig package prefer use old commands \usepackage{epsfig} The amssymb package provides various useful mathematical symbols \usepackage{amssymb} The amsthm package provides extended theorem environments \usepackage{amsthm} The lineno packages adds line numbers. Start line numbering . Or switch whole article \linenumbers . \usepackage{lineno} natbib.sty loaded default. However, natbib options provided \biboptions{...} command. Following options valid: round - round parentheses used square - square brackets used [option] curly - curly braces used {option} angle - angle brackets used <option> semicolon - multiple citations separated semi-colon colon - semicolon, earlier confusion comma - separated comma numbers- selects numerical citations super - numerical citations superscripts sort - sorts multiple citations according order ref. list sort&compress - like sort, also compresses numerical citations compress - compresses without sorting \biboptions{comma,round} \biboptions{} \journal{Journal Biomedical Informatics} Start line numbering want \linenumbers main text","  The internet today has become an unrivalled source of information where people converse on content based websites such as Quora, Reddit, StackOverflow and Twitter asking doubts and sharing knowledge with the world. A major arising problem with such websites is the proliferation of toxic comments or instances of insincerity wherein the users instead of maintaining a sincere motive indulge in spreading toxic and divisive content. The straightforward course of action in confronting this situation is detecting such content beforehand and preventing it from subsisting online. In recent times Transfer Learning in Natural Language Processing has seen an unprecedented growth. Today with the existence of transformers and various state of the art innovations, a tremendous growth has been made in various NLP domains. The introduction of BERT has caused quite a stir in the NLP community. As mentioned, when published, BERT dominated performance benchmarks and thereby inspired many other authors to experiment with it and publish similar models. This led to the development of a whole BERT-family, each member being specialized on a different task. In this paper we solve the Insincere Questions Classification problem by fine tuning four cutting age models viz BERT, RoBERTa, DistilBERT and ALBERT"
"The term ``Readability'' measures much energy reader expend order understand writing optimal speed find interesting. Readability measuring formulas, Automated Readability Index , Flesch Reading Ease , Dalehall formula calculate score estimates grade level years education reader based U.S. education system, illustrated Figure . These formulas still used many widely known commercial readability measuring tools Grammarly Readable. This measurement plays significant role many places, education, health care, government . Government organizations use ensure official texts meet minimum readability requirement. For instance, Department Insurance Texas requirement insurance policy documents Flesch Reading Ease score 40 higher, translates reading level first-year undergraduate student based U.S. education system. A legal document hard read lead someone sign contract without understanding agreeing to. Another common usage area healthcare sector ensure proper readability care treatment documents . Better readability attract visitors readers different websites blogs, whereas poor readability may decrease number readers . Readability measures also often used assess financial documents annual reports company economic performance information transparent reader . Dyslexia disorder causes difficulties skills associated learning, namely reading writing, affects 20\% general population. Readability formulas applied measure difficulty reading texts people dyslexia . The scores readability formulas generally found correlate highly actual readability text written English language. The adaptation readability formulas no-English texts straightforward. Measuring readability also essential every non-English language, readability formulas mentioned language-independent. These formulas require resources like 3000-word list, easily understandable fourth-grade American students, syllable counting dictionary, stemmer, lemmatizer etc. Resource availability Natural Language Processing research obstacle low-resource-languages . In paper, aim develop readability analysis tool Bengali Language. Bengali native language Bangladesh, also used India approximately 230 million native speakers. Despite spoken language world, Bengali suffers lack fundamental resources NLP. For low resource language like Bengali, research area far considered narrow sometimes incorrect. \citet{islam2012text, sinha2012new} tried adapt formula-based approaches used English language. Unfortunately, straightforward formulas developed U.S. based education system predicts U.S. grade level reader. Since Bangladeshi education system grade levels different U.S., therefore, mapping faulty led incorrect results. There strong relationship reading skills human cognition, varies depending different age groups . Therefore, eliminate incompatibility, paper, map grade level different age groups present age-to-age comparison. Moreover, used traditional machine learning models address task small scale dataset, publicly available. There readability analysis tools available English , Arabic , Italian , Japanese language. Unfortunately, tool available Bengali language validate readability text. On hand, large-scale human annotated readability analysis dataset available train supervised neural models extremely low-resource language. Our main contributions summarized follows: In paper, aimed identify Insincere Questions text state art NLP models. Starting simple methods cutting edge, illustrated NLP models compete others. In order so, explored BERT three BERT-based transformer models approach text classification. RoBERTa, DistilBERT, ALBERT improve original model different way regards performance speed. In application, demonstrated easiest way implement transformer models, modify standard settings else pay attention to. On task identifying insincere user intent BERT performed best. However, field NLP fast moving - excited see next transformational generation models bring.","  Determining the readability of a text is the first step to its simplification. In this paper, we present a readability analysis tool capable of analyzing text written in the Bengali language to provide in-depth information on its readability and complexity. Despite being the $7^{th}$ most spoken language in the world with 230 million native speakers, Bengali suffers from a lack of fundamental resources for natural language processing. Readability related research of the Bengali language so far can be considered to be narrow and sometimes faulty due to the lack of resources.  Therefore, we correctly adopt document-level readability formulas traditionally used for U.S. based education system to the Bengali language with a proper age-to-age comparison. Due to the unavailability of large-scale human-annotated corpora, we further divide the document-level task into sentence-level and experiment with neural architectures, which will serve as a baseline for the future works of Bengali readability prediction. During the process, we present several human-annotated corpora and dictionaries such as a document-level dataset comprising 618 documents with 12 different grade levels,  a large-scale sentence-level dataset comprising more than 96K sentences with simple and complex labels, a consonant conjunct count algorithm and a corpus of 341 words to validate the effectiveness of the algorithm, a list of 3,396 easy words, and an updated pronunciation dictionary with more than 67K words. These resources can be useful for several other tasks of this low-resource language. \footnote{We make our Code \& Dataset publicly available at \url{https://github.com/tafseer-nayeem/BengaliReadability} for reproduciblity.}"
"A contract legally binding agreement recognizes governs rights duties parties agreement. Correctly composing contracts crucial ensure legal validity. In many real-world scenarios, standard contract prepared filling blanks precompiled form. Due carelessness, two blanks filled content may incorrectly filled different content. This result contract inconsistencies, may severely impair legal validity contract. Contract review widely used companies check contract inconsistencies. However, contract review labor-intensive costly. Big companies hire tens thousands lawyers conduct contract review, estimated Fortune Global Fortune companies spend 35281299,62194.05\%90.90\%$. Our contributions summarized follows: We formulate Contract Inconsistency Checking problem. As far know, problem yet studied AI community. We propose novel Pair-wise Blank Resolution framework address CIC problem. In PBR, propose extends Transformer encoder architecture efficiently model meaningless blanks. We collected labeled large-scale Chinese contract corpus CIC. The experimental results show promising performance PBR method. In paper, introduce new task, Writing Polishment Similes, curate large-scale Chinese simile dataset. Our experiments demonstrate feasibility potential task, consider first step towards figurative writing polishment real-world setting. We establish Locate\&Gen model benchmark developed dataset. Future works include limited to: Furthermore, AI writing assistant perspective, surmise assisting humans writing polishment likely develop potentials current AI models letting AIs write fly . Given figurative language essential creative aspect language use, encourage use CS dataset various contexts look forward emergence intelligent writing assistant tools like magic\footnote[1]{We applied Locate\&Gen model generate simile, ``'' Chinese translated English.} future."," Contract consistency is important in ensuring the legal validity of the contract. In many scenarios, a contract is written by filling the blanks in a precompiled form. Due to carelessness, two blanks that should be filled with the same  content may be incorrectly filled with different  content. This will result in the issue of contract inconsistencies, which may severely impair the legal validity of the contract. Traditional methods to address this issue mainly rely on manual contract review, which is labor-intensive and costly. In this work, we formulate a novel Contract Inconsistency Checking  problem, and design an end-to-end framework, called Pair-wise Blank   Resolution , to solve the CIC problem with high accuracy. Our PBR model contains a novel \texttt{BlankCoder} to address the challenge of modeling meaningless blanks. \texttt{BlankCoder} adopts a two-stage attention mechanism that adequately associates a meaningless blank with its relevant descriptions while avoiding the incorporation of irrelevant context words. Experiments conducted on real-world datasets show the promising performance of our method with a balanced accuracy of $94.05\%$ and an F1 score of $90.90\%$ in the CIC problem."
"Building human-like open-domain conversational agent one milestones artificial intelligence . Early conversational agents primarily based rules , e.g., Eliza , first CA developed 60's, simulates Rogerian psychotherapist based hand-crafted pattern matching rules. In recent years, advancement data-driven neural networks, neural open-domain conversational models becoming dominant . Recent efforts open-domain neural conversational models primarily aiming improve response diversity endowing responses knowledge , personality , emotion empathy . All efforts mentioned focusing models passively respond user messages. However, many real-world scenarios, e.g., conversational recommendation, psychotherapy education, conversational agents required actively lead conversation smoothly changing conversation topic designated one. For example, casual conversation, agent may actively lead user specific product service agent wants introduce recommend. In paper, follow line research study problem imposing conversational goals/keywords open-domain conversational agents, agent required lead conversation target keyword smoothly fast. As illustrated Figure , given target keyword ``juice"" random starting keyword ``comics"", agent required converse user multiple exchanges lead conversation ``juice"". The challenge problem lies balance tradeoff maximizing keyword transition smoothness minimizing number turns taken reach target. On one hand, passively responding user solely based conversation context would achieve high smoothness may take many turns reach target, hand, directly jumping target word ignoring conversation context would minimize number turns produce non-smooth keyword transitions. \citet{tang2019target} proposed break problem two sub-problems: next-turn keyword selection keyword-augmented response retrieval. \citet{tang2019target} proposed next-turn keyword predictor rule-based keyword selection strategy solve first sub-problem, allowing agent know next keyword talk given conversation history target keyword. In addition, \citet{tang2019target} proposed keyword-augmented response retrieval model solve second sub-problem, allowing agent produce response relevant selected keyword. However, two major limitations existing studies . First, training evaluation datasets next-turn keyword prediction directly extracted conversations without human annotations, thus, majority ground-truth keyword transitions noisy low correlations human judgements. As illustrated Figure , keyword transitions conversation considered relevant. In fact, human annotation studies 600 keyword transitions, found around 70\% keyword transitions next-turn keyword prediction datasets rated relevant, renders trained next-turn keyword predictor existing studies less reliable. Second, rule-based keyword selection strategy primarily leverages cosine similarity word embeddings select keywords closer target keyword. Word embeddings trained based distributional hypothesis words similar contexts similar meanings, may reflect humans relate words conversational turn-taking. In paper, assume human conversations grounded commonsense propose keyword-guided neural conversational model leverage external commonsense knowledge graphs next-turn keyword selection keyword-augmented response retrieval. Humans rely commonsense reason, commonsense reasoning plays important role cognitive process conversational turn-taking . Relying CKG keyword transition would allow agent select target-related keyword next-turn. Moreover, leverage commonsense triplets CKG using Graph Neural Networks next-turn keyword prediction keyword-augmented response retrieval achieve accurate predictions. In summary, contributions follows: In work, formulate Contract Inconsistency Checking problem, automatic contract analysis task significant practical importance, propose novel end-to-end Pair-wise Blank Resolution framework predict consistency relation every two blanks high accuracy. In PBR, extend Transformer encoder architecture propose \texttt{BlankCoder}, off-the-shelf effective blank modeling method could easily generalize tasks text infilling. Extensive experiments show model significantly consistently outperform existing baselines, yielding promising balanced accuracy F1 score . In future, plan consider complex cases explore complex consistency checking scenarios require logical reasoning. ."," We study the problem of imposing conversational goals/keywords on open-domain conversational agents, where the agent is required to lead the conversation to a target keyword smoothly and fast. Solving this problem enables the application of conversational agents in many real-world scenarios, e.g., recommendation and psychotherapy. The dominant paradigm for tackling this problem is to 1) train a next-turn keyword classifier, and 2) train a keyword-augmented response retrieval model. However, existing approaches in this paradigm have two limitations: 1) the training and evaluation datasets for next-turn keyword classification are directly extracted from conversations without human annotations, thus, they are noisy and have low correlation with human judgements, and 2) during keyword transition, the agents solely rely on the similarities between word embeddings to move closer to the target keyword, which may not reflect how humans converse. In this paper, we assume that human conversations are grounded on commonsense and propose a keyword-guided neural conversational model that can leverage external commonsense knowledge graphs  for both keyword transition and response retrieval. Automatic evaluations suggest that commonsense improves the performance of both next-turn keyword prediction and keyword-augmented response retrieval. In addition, both self-play and human evaluations show that our model produces responses with smoother keyword transition and reaches the target keyword faster than competitive baselines."
"Despite remarkable progress made NMT recently , NMT systems still prone translation errors caused noisy input sequences. One common type input noise homophone noise, words characters mis-recognized others similar pronunciation ASR input systems non-phonetic languages , illustrated example Table. Previous works suggest incorporating phonetic embeddings NMT augmenting training data adversarial examples injected homophone noise would alleviate issue. Intuitively, humans usually trouble disambiguating sentences corrupted moderate homophone noise via context syllable information. We propose human-inspired robust NMT framework tailored homophone noise Chinese-English translation, composed homophone noise detector syllable-aware NMT model. \\ Output NMT~&~build primary school \\ \specialrule{0.05em}{3pt}{3pt} Noisy Input~&~ \\ Output NMT~&~suggest primary school \\ \specialrule{0.05em}{3pt}{3pt} Mixed Transcript~&~ \\ Output Ours~&~build primary school\\ \bottomrule[1pt] \end{tabular} Due lack data annotated homophone noise, propose train detector monolingual data self-supervised manner, Chinese characters sequences input corresponding syllables sequence label predict possibility character homophone noise. The identified homophone errors source sentence converted corresponding syllables produce new source sequence mixed characters syllables. Augmenting bilingual training data instances original source sentences substituted corresponding character-syllable-mixed sequences, train SANMT model translate unconventional inputs. To examine effectiveness proposed model, conduct extensive experiments artificial noisy test sets real-world noise test set homophone noise speech translation scenario. The test set released soon. Our experimental results ChineseEnglish translation clearly show proposed method significantly superior previous approaches alleviating impact homophone noise NMT, also achieves substantial improvement clean text. %Due lack data annotated homophone noise, propose train detector monolingual data self-supervised manner, Chinese characters automatically transformed syllables predict homophone noise. The identified homophone errors source sentence converted corresponding syllables produce new source sequence mixed characters syllables. Augmenting training data instances original source sentences substituted corresponding character-syllable-mixed sequences, train SANMT model translate unconventional inputs. To examine effectiveness proposed model, conduct extensive experiments artificial noisy test sets real-world noise test set homophone noise speech translation scenario. The test set released soon. Our experimental results ChineseEnglish translation clearly show proposed method significantly superior previous approaches alleviating impact homophone noise NMT, also achieves substantial improvement clean text. We study problem imposing conversational goals/keywords open-domain conversational agents. The keyword transition module existing approaches suffer noisy datasets unreliable transition strategy. In paper, propose ground keyword transitions commonsense propose two GNN-based models tasks next-turn keyword transition keyword-augmented response retrieval, respectively. Extensive experiments show proposed model obtains substantially better performance two tasks competitive baselines. In addition, model analysis suggests CKG triplets proposed CKG-guided keyword selection strategy helpful learning utterance representation keyword transition, respectively. Finally, self-play simulations human evaluations show model achieve better success rate, reach target keyword faster, produce smoother conversations baselines."," In this paper, we propose a robust neural machine translation  framework. The framework consists of a homophone noise detector and a syllable-aware NMT model to homophone errors. The detector identifies potential homophone errors in a textual sentence and converts them into syllables to form a mixed sequence that is then fed into the syllable-aware NMT. Extensive experiments on Chinese$\rightarrow$English translation demonstrate that our proposed method not only significantly outperforms baselines on noisy test sets with homophone noise, but also achieves a substantial improvement on clean text."
"In recent years, dramatic surge adoption voice assistants Amazon Alexa, Apple Siri, Google Assistant. Customers use variety tasks playing music online shopping. These voice assistants built complex Spoken Language Understanding systems typically large store edge device mobile phone smart speaker. Hence, user traffic routed cloud server process requests. This led privacy concerns fueled push tiny AI edge processing, user requests processed device itself. Traditional SLU systems consist two-stage pipeline, Automatic Speech Recognition component processes customer speech generates text transcription , followed Natural Language Understanding component maps transcription actionable hypothesis consisting intents slots . An end-to-end system goes directly speech hypothesis would help make SLU system smaller faster, allowing stored edge device. It could potentially also better optimized pipeline since eliminates cascading errors. However, E2E systems used practice key issues. These systems hard build since consist large neural components transformers require massive amounts E2E training data. They also make use vastly available training data ASR NLU components could used enhance performance, examples datasets may aligned create E2E training sample. Another issue feature expansion, scenario new domain, new intents slots, added voice assistant's capabilities. Here, developers typically access synthetically generated text-hypothesis examples. Speech data readily available expensive collect. E2E models thus fail require lots new audio hypothesis data learn new domain. In work, build E2E model mitigates issues using transfer learning. We call Audio-Text All-Task Model. AT-AT E2E transformer-based model jointly trained multiple audio-to-text text-to-text tasks. Examples tasks include speech recognition , hypothesis prediction speech , masked LM prediction , hypothesis prediction text . Our model achieves converting data tasks single audio-to-text text-to-text format. Figure shows joint training phase detail. Our findings indicate significant knowledge transfer taking place multiple tasks, turn helps downstream model performance. We see AT-AT pretrained model shows improved performance SLU hypothesis prediction internal data collected Alexa traffic. We also report state-of-the-art results two public datasets: FluentSpeech , SNIPS Audio . Furthermore, since model contains text encoder, consume audio text inputs generate target sequence. By jointly training audio-to-text text-to-text tasks, hypothesize model learns shared representation audio text inputs. This allows us simply train new text-to-text data get audio-to-text performance free, giving us way E2E hypothesis prediction zero-shot fashion feature expansion. We test approach internal dataset Alexa traffic, external dataset, Facebook TOP . Since TOP consists text data, collected speech data test split using internal tool Amazon. We soon release dataset. In summary, contributions follows. In paper, presented novel framework composed homophone error detector SANMT model cope homophone noise. Experimental results show method achieves substantial improvement previous robust NMT baselines test sets artificial real-world noise, also outperforms NMT baseline clean test sets. We consider future studies could modeling noise detection NMT jointly. References produced using bibtex program suitable BiBTeX files . The IEEEbib.bst bibliography style file IEEE produces unsorted bibliography list. ------------------------------------------------------------------------- \clearpage"," Voice Assistants such as Alexa, Siri, and Google Assistant typically use a two-stage Spoken Language Understanding pipeline; first, an Automatic Speech Recognition  component to process customer speech and generate text transcriptions, followed by a Natural Language Understanding  component to map transcriptions to an actionable hypothesis. An end-to-end  system that goes directly from speech to a hypothesis is a more attractive option. These systems were shown to be smaller, faster, and better optimized. However, they require massive amounts of end-to-end training data and in addition, don't take advantage of the already available ASR and NLU training data.  In this work, we propose an E2E system that is designed to jointly train on multiple speech-to-text tasks, such as ASR  and SLU , and text-to-text tasks, such as NLU . We call this the Audio-Text All-Task  Model and we show that it beats the performance of E2E models trained on individual tasks, especially ones trained on limited data. We show this result on an internal music dataset and two public datasets, FluentSpeech and SNIPS Audio, where we achieve state-of-the-art results. Since our model can process both speech and text input sequences and learn to predict a target sequence, it also allows us to do zero-shot E2E SLU by training on only text-hypothesis data  from a new domain. We evaluate this ability of our model on the Facebook TOP dataset and set a new benchmark for zeroshot E2E performance. We will soon release the audio data collected for the TOP dataset for future research."
"Neural Machine Translation achieved state art various MT systems, including rich low resource language pairs . However, quality low-resource MT quite unpretentious due lack parallel data achieved better results systems available resource. Therefore, low-resource MT one essential tasks investigated many previous works . Recently, works present MT systems achieved remarkable results low-resource language . Inspired works, collect data TED Talks domain, attempt build multilingual MT systems French, English-Vietnamese. Experiments demonstrate language pairs: French-Vietnamese English-Vietnamese achieved significant performance joining training. % Although multilingual MT reduce sparse data shared space using word segmentation, however, rare words still exist, evenly increased languages significant disparity term vocabulary. Previous works suggested strategies reduce rare words using translation units sub-word character levels generating universal representation word sentence levels . These help downgrade dissimilarity tokens shared various languages. However, works require learning additional parameters training, thus increasing size models. Our paper presents two methods augment translation rare words source space without modifying architecture model size MT systems: exploiting word similarity. This technique mentioned previous works . They employ monolingual data require supervised resources like bilingual dictionary WordNet, leverage relation multilingual space MT systems. Adding scalar value rare word embedding order facilitate translation training process. % Due fact NMT tends bias translating frequent words, rare words often less opportunity considered. Our ideal inspired works . proposed various solutions urge translation rare words, including modification embedding training. They experimented recurrent neural networks work uses state-of-the-art transformer architecture. transforms word embedding token universal space, learn plus parameters method not. We apply strategies fine-tuning processes, show substantial improvements systems epochs only. Monolingual data widely used NMT augment data low-resource NMT systems . Back-translation known popular technique exploiting target-side monolingual data enhance translation systems self-learning method focuses utilizing source-side monolingual data. Otherwise, dual-learning strategy also suggests using source- target-side monolingual data tackle problem. Our work investigates self-learning method low-resource multilingual NMT systems specifically related Vietnamese. Besides, monolingual data also leveraged unsupervised zero-shot translation. % learn lexical relative one token source language another source language without modifying system architecture well model size. We also use additional resources systems. The main contributions work are: In section 2, review transformer architecture used experiments. The brief multilingual translation shown section 3. Section 4 presents methods deal rare words multilingual translation scenarios. The exploitation monolingual data low-resource multilingual MT discussed section 5. Our results described section 6, related work shown section 7. Finally, paper ends conclusions future work. % Our evaluation clearly shows lot knowledge transfer happening various speech processing tasks. AT-AT evaluated downstream SLU tasks benefits significantly pretrained additional ASR data. This result holds ASR data domain also data different domain . It also holds across different dataset sizes. We see pretraining extremely helpful datasets training data size 1000 SNIPS, remains helpful way limited internal music dataset full music dataset . We believe decoder learns good language model seeing additional ASR data. We also think additional pretraining tasks good regularizers. Our zeroshot results AT-AT even interesting. We designed way train end-to-end model new data without using corresponding audio data, real synthetically generated, model's performance, matching end-to-end model trained real audio data, still remarkable. Our approach adapted make use synthetic data access TTS system improve performance. We managed learn shared audio-text model, explicitly enforcing loss penalty force audio text hidden states space, constraining decoder forcing model learn jointly different input sources. On closing note, would like remark AT-AT somewhat mimics actual human learning. We typically read lot words hear. But hear word first time, transfer knowledge word read it. AT-AT similarly learns understand perform NLU tagging text applies knowledge given speech. \section{Conclusion} We propose Audio-Text All-Task model uses transfer learning improve performance end-to-end SLU. AT-AT beat performance E2E models internal music data, full low-resource settings. It also achieved state-of-the-art performance FluentSpeech SNIPS audio datasets significant improvements prior models. AT-AT also demonstrated ability perform zeroshot E2E SLU, without access TTS system, learning shared audio-text representation without explicit loss penalty force audio text hidden states space. We also showed AT-AT work conjunction TTS system improve E2E performance. It achieves zeroshot E2E EM Accuracy 70.60 TOP dataset. We set new benchmark release audio data TOP dataset future research. On closing note, would like remark AT-AT somewhat mimics actual human learning. We typically read lot words hear. But hear word first time, transfer knowledge word read it. AT-AT similarly learns understand perform NLU tagging text applies knowledge given speech."," % Prior works have demonstrated that a low-resource language pair can be benefited from a multilingual machine translation  system which relies on the jointly training many language pairs. In this paper, we propose two simple strategies to address the rare word issue in multilingual MT systems for two low-resource language pairs: French-Vietnamese,  English-Vietnamese. The first strategy learns  dynamically word similarity of tokens in the shared space among source languages whilst the other one augments the translation ability of rare words through updating their embeddings during the training. In addition, we attempt to leverage monolingual data which is generated from multilingual MT to reinforce synthetic parallel in the data sparsity situation. We show that significant improvements of up to +1.62 and +2.54 BLEU points over the bilingual baseline systems for both language pairs and release datasets for the research community.  Prior works have demonstrated that a low-resource language pair can benefit from multilingual machine translation  systems, which rely on many language pairs' joint training. This paper proposes two simple strategies to address the rare word issue in multilingual MT systems for two low-resource language pairs: French-Vietnamese and English-Vietnamese. The first strategy is about dynamical learning word similarity of tokens in the shared space among source languages while another one attempts to augment the translation ability of rare words through updating their embeddings during the training. Besides, we leverage monolingual data for multilingual MT systems to increase the amount of synthetic parallel corpora while dealing with the data sparsity problem. We have shown significant improvements of up to +1.62 and +2.54 BLEU points over the bilingual baseline systems for both language pairs and released our datasets for the research community."
"% Fabian: Describing Entity linking task mapping entity mentions text documents standard entities given knowledge base. For example, word ``Paris'' ambiguous: It refer either capital France hero Greek mythology. Now given text ``Paris son King Priam'', goal determine that, sentence, word refers Greek hero, link word corresponding entity knowledge base YAGO DBpedia . %Intriguingly, Greek hero also goes name ``Alexander''. Thus, words ``Paris'' ``Alexander'' synonymous, refer Greek hero input text, linked entity knowledge base. % Fabian: Describing important In biomedical domain, entity linking maps mentions diseases, drugs, measures normalized entities standard vocabularies. It important ingredient automation medical practice, research, public health. Different names entities Hospital Information Systems seriously hinder integration use medical data. If medication appears different names, researchers cannot study impact, patients may erroneously prescribed medication twice. % Fabian: Describing difficult The particular challenge biomedical entity linking ambiguity: word usually refers single entity. Rather, challenge surface forms vary markedly, due abbreviations, morphological variations, synonymous words, different word orderings. For example, ``Diabetes Mellitus, Type 2'' also written ``DM2'' ``lung cancer'' also known ``lung neoplasm malignant''. In fact, surface forms vary much possible expressions entity cannot known upfront. This means standard disambiguation systems cannot applied scenario, assume forms entity known. %, thus cannot applied scenario. One may think variation surface forms big problem, long variations entity sufficiently close canonical form. Yet, case. For example, phrase ""decreases hemoglobin"" could refer least 4 different entities MedDRA, look alike: ""changes hemoglobin"", ""increase hematocrit"", ""haemoglobin decreased"", ""decreases platelets"". In addition, biomedical entity linking cannot rely external resources alias tables, entity descriptions, entity co-occurrence, often used classical entity linking settings. % Fabian: done For reason, entity linking approaches developed particularly biomedical entity linking. Many methods use deep learning: work \citet{li2017cnn} casts biomedical entity linking ranking problem, leveraging convolutional neural networks . More recently, introduction BERT advanced performance many NLP tasks, including biomedical domain . BERT creates rich pre-trained representations unlabeled data achieves state-of-the-art performance large suite sentence-level token-level tasks, outperforming many task-specific architectures. However, considering number parameters pre-trained BERT models, improvements brought fine-tuning come heavy computational cost memory footprint. This problem energy efficiency, smaller organizations, poorer countries. In paper, introduce lightweight model achieves performance statistically indistinguishable state-of-the-art BERT-based models. The central idea use alignment layer attention mechanism, capture similarity difference corresponding parts candidate mention names. Our model 23x smaller 6.4x faster BERT-based models average; twice smaller faster lightweight BERT models. Yet, show, model achieves comparable performance standard benchmarks. Further, show adding complexity model necessary: entity-mention priors, context around mention, coherence extracted entities \cite[as used, e.g., in][]{hoffart2011robust} improve results further. \footnote{All data code available \url{https://github.com/tigerchen52/Biomedical-Entity-Linking}.} We built multilingual MT systems two low-resource language pairs: English-Vietnamese French-Vietnamese, proposed two approaches tackle rare word translation. We show approaches bring significant improvements MT systems. We find pseudo bilingual furthermore enhance multilingual NMT system case French Vietnamese translation task. In future, would like use language pairs systems combine proposed methods order evaluate effectiveness MT systems."," Biomedical entity linking aims to map biomedical mentions, such as diseases and drugs, to standard entities in a given knowledge base.  The specific challenge in this context is that the same biomedical entity can have a wide range of names,  including synonyms, morphological variations, and names with different word orderings.  Recently, BERT-based methods have advanced the state-of-the-art by allowing for rich representations of word sequences. However, they often have hundreds of millions of parameters and require heavy computing resources, which limits their applications in resource-limited scenarios. Here, we propose a lightweight neural method for biomedical entity linking, which needs just a fraction of the parameters of a BERT model and much less computing resources.  Our method uses a simple alignment layer with attention mechanisms to capture the variations between mention and entity names. Yet, we show that our model is competitive with previous work on standard evaluation benchmarks."
"% background Sentence semantic matching fundamental Natural Language Processing~ task tries infer suitable label given sentence pair. For example, Natural Language Inference~ targets classifying input sentence pair one three relations~. Paraphrase Identification~ aims identifying whether input sentence pair expresses meaning. Figure gives examples different semantic relations different datasets. % Current state As fundamental technology, sentence semantic matching applied successfully many NLP fields, e.g., information retrieval, question answering, dialog system. Currently, work leverages advancement representation learning techniques tackle task. They focus input sentences design different architectures explore sentence semantics comprehensively precisely. Among methods, BERT plays important role. It adopts multi-layer transformers make full use large corpus~ powerful pre-trained model. Meanwhile, two self-supervised learning tasks~ designed better analyze sentence semantics capture much information possible. % citation Based BERT, plenty work made big step sentence semantic modeling. In fact, since relations predicting targets sentence semantic matching task, methods pay enough attention relation learning. They leverage annotated labels represent relations, formulated one-hot vectors. However, independent meaningless one-hot vectors cannot reveal rich semantic information guidance relations, cause information loss. \citeauthor{gururangan2018annotation}~ observed different relations among sentence pairs imply specific semantic expressions. Taking Figure example, sentence pairs ``contradiction'' relation contain negation words~. ``entailment'' relation often leads exact numbers replaced approximates~. ``Neutral'' relation import correct irrelevant information~. Moreover, expressions sentence pairs different relations different. Therefore, comparison contrastive learning among different relations~ help models learn semantic information implied relations, turn helps strengthen sentence analysis ability models. They treated meaningless one-hot vectors. One solutions better relation utilization embedding method inspired Word2Vec. Some researchers try jointly encode input sentences labels embedding space better relation utilization sentence semantic modeling. Despite progress achieved, label embedding method requires data parameters achieve better utilization relation information. It still cannot fully explore potential relations due small number relation categories lack explicit label embedding initialization. To end, paper, propose novel \fullname~approach make full use relation information simple effective way. In concrete details, first utilize pre-trained BERT model semantic meanings input words sentences global perspective. Then, develop CNN-based encoder obtain partial information~ sentences local perspective. Next, inspired self-supervised learning methods BERT training processing, propose Relation Relation~ classification task enhance learning ability \shortname~for implicit common features corresponding different relations. Moreover, triplet loss used constrain model, intra-class inter-class relations analyzed better. Along line, input sentence pairs relations represented much closer vice versa apart. Relation information properly integrated sentence pair modeling processing, favor tackling challenges improving model performance. Extensive evaluations two sentence semantic matching tasks demonstrate effectiveness proposed \shortname~and advantages state-of-the-art sentence semantic matching baselines. In work, proposed novel method extract rationales neural predictions. Our method uses adversarial-based technique make selector-predictor model learn guider model. In addition, proposed novel regularizer based language models, makes extracted rationales semantically fluent. In way, ``guider"" model tells selector-predictor model kind information remains unselected over-selected. We conducted experiments task sentiment analysis three tasks legal domain. The experimental results showed method improves selection rationales large margin. This regularizer also gives priority important adjacent word pairs considering whether select unselect simultaneously, refines rationales. Finally, conducted experiments two datasets prove effectiveness model. We conducted experiments two datasets prove effectiveness model. As future work, main architecture model directly applied domains, e.g., images tabular data. However, remains open question would good regularizer domains. For example, variational autoencoders discrete latent space, providing rationales different kinds deep learning applications."," 	% background 	Sentence semantic matching is one of the fundamental tasks in natural language processing, which requires an agent to determine the semantic relation among input sentences.  	% current state 	Recently, deep neural networks have achieved impressive performance in this area, especially BERT.  	% problem 	Despite their effectiveness, most of these models treat output labels as meaningless one-hot vectors, underestimating the semantic information and guidance of relations that these labels reveal, especially for tasks with a small number of labels.  	% solution 	To address this problem, we propose a \fullname~for sentence semantic matching. 	 	Specifically, we first employ BERT to encode the input sentences from a global perspective. 	Then a CNN-based encoder is designed to capture keywords and phrase information from a local perspective.  	To fully leverage labels for better relation information extraction, we introduce a self-supervised relation of relation classification task for guiding \shortname~to consider more about relations.  	Meanwhile, a triplet loss is employed to distinguish the intra-class and inter-class relations in a finer granularity. 	% result 	Empirical experiments on two sentence semantic matching tasks demonstrate the superiority of our proposed model.  	As a byproduct, we have released the codes to facilitate other researches."
"Discovering novel user intents important improve service quality dialogue systems. By analyzing discovered new intents, may find underlying user interests, could provide business opportunities guide improvement direction. Intent discovery attracted much attention recent years. Many researchers regard unsupervised clustering problem, manage incorporate weak supervised signals guide clustering process. For example,~\citet{hakkani-tr2013a} propose hierarchical semantic clustering model collect web page clicked information implicit supervision intent discovery.~\citet{hakkani2015clustering} utilize semantic parsing graph extra knowledge mine novel intents clustering.~\citet{Padmasundari2018} benefit consensus predictions multiple clustering techniques discover similar semantic intent-wise clusters.~\citet{haponchyk2018supervised} cluster questions user intent categories supervision structured outputs.~\citet{shi2018auto} extract intent features autoencoder automatically label intents hierarchical clustering method. However, methods fail leverage prior knowledge known intents. These methods assume unlabeled samples composed undiscovered new intents. A common case labeled data known intents accessible unlabeled data mixed known new intents. As illustrated Figure, may labeled samples known intents advance. The remaining known new intent samples unlabeled. Our goal find known intents discover new intents prior knowledge limited labeled data. Our previous work CDAC+ directly tackles problem. Nevertheless, uses pairwise similarities weak supervised signals, ambiguous distinguish mixture unlabeled known new intents. Thus, performance drops new intents. To summarize, two main difficulties task. On one hand, challenging effectively transfer prior knowledge known intents new intents limited labeled data. On hand, hard construct high-quality supervised signals learn friendly representations clustering unlabeled known new intents. To solve problems, propose effective method leverage limited prior knowledge known intents provide high-quality supervised signals feature learning. As illustrated Figure, firstly use pre-trained BERT model extract deep intent features. Then, pre-train model limited labeled data supervision softmax loss. We retain pre-trained parameters use learning information obtain well-initialized intent representations. Next, perform clustering extracted intent features estimate cluster number eliminating low-confidence clusters. As training samples unlabeled, propose original alignment strategy construct high-quality pseudo-labels supervised signals learning discriminative intent features. For training epoch, firstly perform k-means extracted intent features, use produced cluster assignments pseudo-labels training neural network. However, inconsistent assigned labels cannot directly used supervised signals, use cluster centroids targets obtain alignment mapping pseudo-labels consequent epochs. Finally, perform k-means inference. Benefit relatively consistent aligned targets, method inherit history learning information boost clustering performance. We summarize contributions follows. Firstly, propose simple effective method successfully generalizes mass new intents estimate number novel classes limited prior knowledge known intents. Secondly, propose effective alignment strategy obtain high-quality self-supervised signals learning discriminative features distinguish known new intents. Finally, extensive experiments two benchmark datasets show approach yields better robust results state-of-the-art methods. In paper, presented simple effective method named \shortname~for sentence semantic matching. This method uses powerful BERT CNN encode sentences global local perspectives, also makes full use relation information better performance enhancement. Specifically, design R classification task help \shortname~for learning implicit common knowledge pairwise relation learning processing. Moreover, triplet loss employed constrain \shortname~for better triplet based relation learning intra-class inter-class information analyzing. Extensive experiments NLI PI tasks demonstrate superiority \shortname. In future, plan combine advantages label embedding method better sentence semantic comprehension."," 		Discovering new intents is a crucial task in dialogue systems. Most existing methods are limited in transferring the prior knowledge from known intents to new intents. They also have difficulties in providing high-quality supervised signals to learn clustering-friendly features for grouping unlabeled intents. In this work, we propose an effective method, Deep Aligned Clustering, to discover new intents with the aid of the limited known intent data. Firstly, we leverage a few labeled known intent samples as prior knowledge to pre-train the model. Then, we perform k-means to produce cluster assignments as pseudo-labels. Moreover, we propose an alignment strategy to tackle the label inconsistency problem during clustering assignments. Finally, we learn the intent representations under the supervision of the aligned pseudo-labels. With an unknown number of new intents, we predict the number of intent categories by eliminating low-confidence intent-wise clusters. Extensive experiments on two benchmark datasets show that our method is more robust and achieves substantial improvements over the state-of-the-art methods. The codes are released at \url{https://github.com/thuiar/DeepAligned-Clustering}."
"The U.S.~NIH's precision medicine initiative calls designing treatment preventative interventions considering genetic, clinical, social, behavioral, environmental exposure variability among patients. The initiative rests widely understood finding considering individual variability critical tailoring healthcare interventions achieve substantial progress reducing disease burden worldwide. Cancer chosen near term focus eventual aim expanding conditions. As biomedical research enterprise strives fulfill initiative's goals, computing needs also rise drug discovery, predictive modeling disease onset progression, building NLP tools curate information evidence base generated. \subsection{TREC Precision Medicine Series} \end{table} In dovetailing move, U.S.~NIST's TREC running PM track since 2017 focus cancer. The goal TREC-PM task identify relevant biomedical articles clinical trials input patient case. Each case composed disease name, gene name genetic variation type, demographic information . Table shows two example cases 2019 track. So search ad hoc sense free text input facet facets highlight PM related attributes ought characterize retrieved documents. We believe style faceted retrieval going common across medical IR tasks many conditions PM initiative continues mission. \subsection{Vocabulary Mismatch Neural IR} The vocabulary mismatch problem prominent issue medical IR given large variation expression medical concepts events. For example, query ``What potential side effect Tymlos?'' drug referred brand name. Relevant scientific literature may contain generic name Abaloparatide frequently. Traditional document search engines clear limitations resolving mismatch issues. The IR community extensively explored methods address vocabulary mismatch problem, including query expansion based relevance feedback, query term re-weighting, query reconstruction optimizing query syntax. Several recent studies highlight exploiting neural network models query refinement document retrieval settings. \citet{nogueira2017task} address issue generating transformed query initial query using neural model. They use reinforcement learning train agent learns reformulate initial query maximize expected return actions . In different approach, \citet{narayan2018ranking} use RL sentence ranking extractive summarization. \subsection{Our Contributions} In paper, building BERT architecture, focus different hybrid document scoring reranking setup involving three components: .~a document relevance classification model, predicts whether document relevant given query ; .~a keyword extraction model spots tokens document likely seen PM related queries; .~an abstractive document summarization model generates pseudo-query given document context facet type via BERT encoder-decoder setup. The keywords ) pseudo-query ) together compared original query generate score. The scores components combined rerank top documents returned basic Okapi BM25 retriever Solr index corpora. %This critical neural document-query matching summarization expensive operations cannot practically scale full corpus. Our main innovation pivoting focus queries previous methods emphasis transforming candidate documents pseudo-queries via summarization. Additionally, generating pseudo-query, also let decoder output concept codes biomedical terminologies capture disease gene names. We embedding words concepts common semantic space letting decoder generate summaries include concepts. Our overall architecture evaluated using TREC-PM datasets 2019 dataset used test set. The results show absolute improvement P@10 compared prior best approaches obtaining small gain R-Prec. Qualitative analyses also highlight summarization able focus document segments highly relevant patient cases. In work, introduced effective method discovering new intents. Our method successfully transfers prior knowledge limited known intents estimates number intents eliminating low-confidence clusters. Moreover, provides stable concrete supervised signals guide clustering process. We conduct extensive experiments two challenging benchmark datasets evaluate performance. Our method achieves significant improvements compared methods obtains accurate estimated cluster numbers limited prior knowledge. In future, try different clustering methods produce supervised signals explore self-supervised methods representation learning."," Information retrieval  for precision medicine  often involves looking for multiple pieces of evidence that characterize a patient case. This typically includes at least the name of a condition and a genetic variation that applies to the patient. Other factors such as demographic attributes, comorbidities, and social determinants may also be pertinent. As such, the retrieval problem is often formulated as ad hoc search but with multiple facets  that may need to be incorporated. In this paper, we present a document reranking approach that combines neural query-document matching and text summarization toward such retrieval scenarios. Our architecture builds on the basic BERT model with three specific components for reranking: . document-query matching . keyword extraction and . facet-conditioned abstractive summarization. The outcomes of  and  are used to essentially transform a candidate document into a concise summary that can be compared with the query at hand to compute a relevance score. Component  directly generates a matching score of a candidate document for a query. The full architecture benefits from the complementary potential of document-query matching and the novel document transformation approach based on summarization along PM facets. Evaluations using NIST's TREC-PM track datasets  show that our model achieves state-of-the-art performance. To foster reproducibility, our code is made available here: \url{https://github.com/bionlproc/text-summ-for-doc-retrieval}."
". } In real-world dialogue systems, substantial portion user queries ambiguous ones system unable precisely identify underlying intent. %For example, nearly 30\% user queries real-world QA system ambiguous questions. % Can't give statistics academic paper without mentioning details We observed many queries question answering system exhibited one following two characteristics. %The ambiguous questions QA system summarized 2 types:\\ % \\ % Given limited information, difficult system accurately respond user's ambiguous queries, often resulting user's needs cannot addressed. For example, specific intent underlying utterance ``How apply?"" remains obscure, many products related action ``applying"". In practice, one often needs fall back human agents assist requests, increasing workload cost. The main purpose deployed automated systems reduce human workload scenarios customer service hotlines. The lack ability deal ambiguous questions may directly lead sessions transferred human agents. In real-world customer service system, affects 30\% sessions. Hence, valuable find effective solution clarify ambiguous questions automatically, greatly reducing number cases requiring human assistance. Automated question clarification involves confirming user's intent interaction. %. % essential Question Answering system. Previous work explored asking questions . Unfortunately, clarification asking questions requires substantial customization specific dialogue setting. It challenging define appropriate questions guide users towards providing accurate information. Coarse questions may leave users confused, overly specific ones may fail account specific information user wishes convey. In work, thus instead investigate interactive clarification providing user specific choices options, intent options . Unlike previous work, propose end-to-end model suggests labels clarify ambiguous questions. % In experiments, show method significantly performs rule based method recall potential FAQs. % This paper focused closed-domain question clarification dialogue, solving kinds ambiguous questions one method. %AQ %Previous methods either solve lack semantic elements questions solve entity ambiguity questions. % %revious work: %1., %2.query  %MIDF % %Question clarification asking question generated model may receive unexpected reply user like ``I'm sure"" generate weird question real application. Query refinement method helps improve search results applicable clarification dialogue. We aimed interact user concise phrases clarify user's question. % In closed-domain QA system, believe ambiguous question series potential clear questions. For example, Figure~, least three FAQ questions corresponding ambiguous question ``How apply"". We argue essence clarifying ambiguous questions lies finding key points differentiation potential questions. It's possible clarify user's true intents confirming key points users shown Figure~. % % % An example sort approach given Figure. Here, consider closed-domain QA system, typical method build intent inventory address high-frequency requests. In setting, set unambiguous candidate labels ambiguous user utterance corresponds set frequently asked questions covered intent inventory. %By constraining problem close-domain, potential clear questions ambiguous question finite set. In closed domain, consider candidate set finite. For example, Figure, three specific intents corresponding ambiguous question ``How apply"". Our approach induces phrase tags labels intent. Thus, catalog intents corresponding labels presented user. The challenge lies selecting suitable list labels effectively clarify ambiguous question. In approach, problem finding label sequence formulated collection partitioning problem, objective cover many elements possible distinguishing elements clearly possible. % According Aristotle, definition species consists genus proximum differ. The differential attribute one species distinguished others genus. The task question clarification thus amounts obtaining suitable set labels. %is get differential intents set potential FAQs. % \todo{update section 3.2} % We illustrate method finding intents set detail methodology section. % % introduced methods ask clarification questions information missing given linguistic context. use generative model generate clarification questions solving entity ambiguities. But obstacles use methods real application. One reason users real world sometimes respond clarification question expected like reply ``I'm sure"". Compared withing ask clarification question, directly list potential ambiguities options. proposed query refinement method based reinforcement learning, helps improve search results search engine. Limited form dialogue system, practical show long list potential results dialogue. We aimed interact user concise phrases clarify user's question. % % A similar idea \citet{DBLP:conf/chiir/RadlinskiC17} also suggests that, conversational interface may easier users clarify needs given precise choices rather expecting come particular terms. % The complete question clarification process work illustrated Figure . Through real-world application experiments, method lower rate transferring human agents significant higher CTR baselines. Our method also performs better baselines recall potential FAQs annotated corpora. %This paper focuses closed-domain question clarification dialogue, solving kinds ambiguous questions one method. The main contributions work are: %% This part comparison related works. % We investigated related works clarify ambiguous questions QA. The classic solution rank semantic similar questions [ranker ref] ambiguous questions. However, considering limitation display information dialogue based QA system, generally three results displayed, resulting method cannot cover enough potential clear questions. In experiments, use relevance ranker baseline comparison. The results show human transferring rate method much lower ranking method. The second method ask clarification questions . . However, method generative clarification question limitations real-word QA system. The biggest obstacle user's answer space maybe open answer, complicates dialogue. In addition, lot works disambiguate questions question refinement, refinement methods usually supplements information single key point, able achieve key point recall mentioned earlier. % Question clarification essential question answering system. In real-world QA system, nearly 30\% user queries ambiguous questions. Without clarification, dialogue participants risk missing information ambiguous failing achieve mutual understanding. The ability ask clarification questions one key desired components conversational systems . introduced methods ask clarification questions information missing given linguistic context. use generative model generate clarification questions solving entity ambiguities. % However, difficult achieve high success rate. For example, ``how apply?"" ambiguous, many products related ``apply"". By asking one option question, ``Do want apply credit card?"" two options question, ``Do want apply credit card loan ?"", less efficient. Phenomena mentioned exist real world customer service robot system. CSRobot based FAQ question answering widely used real world, especially financial industry. When user enter question CSRobot system , information retrieved computing semantic similarity user question pre-manually prepared FAQ. Due factors user's age, gender, geography, familiarity system, urgency user's problem, user may enter many ambiguous questions. In CSRobot environment, ratio nearly 30\%. The ambiguous questions system summarized 5 types: Missing subject object, e.g. ``how apply"", ``how change back"", Missing predicate, e.g. ``credit card"", ``my QR code"", Missing subject predicates objects, e.g. ``How benefit"", ``its right"", Entity ambiguous, e.g. ``My health insurance"", health insurance contains many sub-categories, Misspelling ambiguous. ``how exist"" , ``exist"" may misspelling ``exit"". In work, focus asking clarification questions using intents recommendation FAQ-based question answering system. Previous methods either solve missing information questions solve entity ambiguity questions, proposed method handle missing information entity ambiguous mentioned above. % The complete question clarification process work seen Figure . The user enters incomplete ambiguous question, agent recommends list candidate intents, clarifies user's question clicked. Then user clicks intent associated himself, agent finds list related FAQ FAQ knowledge base clarified question. Our work focuses recommend list candidate intents question clarification. A similar idea \citet{DBLP:conf/chiir/RadlinskiC17} also suggests that, conversational interface may easier users clarify needs given precise choices rather expecting come particular terms. % introduce question clarification collection partition thought detail % % One challenges designing method design cold start scenario. We use end-to-end sequential intents recommendation method based reinforcement learning user question clarification. We use supervised method mainly difficult human annotators directly labeling intents related user's ambiguous question . The reward designed recommend closest clear question list maximize information gain clicking one intent better question clarification. We conducted offline online experiments real-world CSRobot environment collected data 100 million online real-users' interactions system one month. To best knowledge, first use intents recommendation question clarification real-world CSRobot environment, interactions 100 million real users. The experiments proved effectiveness scalability proposed method. Contributions summarized follows: % %% FORMATTING \newcommand{\NTCIR}{NTCIR-13} \newcommand{\metric}[1]{{\mbox{#1}}} \newcommand{\metricfont}[1]{{\small\sf{#1}}} \newcommand{\ydata}{{\metricfont{Y!S1}}} \newcommand{\govdata}{{\metricfont{GOV2}}} \newcommand{\RBP}{\metric{RBP}} \newcommand{\Pat}{\metric{P}} %\newcommand{\AP}{\metric{AP}} \newcommand{\AP}{\metric{MAP}} \newcommand{\NDCG}{\metric{NDCG}} \newcommand{\ERR}{\metric{ERR}} \newcommand{\BPref}{\metric{BPref}} \newcommand{\Qmeasure}{\metric{Qmeasure}} \newcommand{\Patk}[1]{\mbox{\Pat@}} \newcommand{\RBPatp}[1]{\mbox{\RBP@}} \newcommand{\RBPatptok}[2]{\mbox{\RBP@}} \newcommand{\NDCGatk}[1]{\mbox{\NDCG@}} \newcommand{\ERRatk}[1]{\mbox{\ERR@}} \newcommand{\APtok}[1]{\mbox{\AP}} \newcommand{\APatk}[1]{\mbox{\AP}} \newcommand{\NDCGtok}[1]{\mbox{\NDCG}} \newcommand{\ERRtok}[1]{\mbox{\ERR}} \newcommand{\ssvar}[1]{\mbox{\tiny#1}} \newcommand{\trisk}{} \newcommand{\urisk}{} \newcommand{\combsum}{\method{CombSUM}\xspace} \newcommand{\rrf}{\method{RRF}\xspace} %-- Baselines \newcommand{\gbrt}{\method{GBRT}} \newcommand{\lstm}{\method{LSTM}} \newcommand{\dqn}{\method{DQN}} \newcommand{\dodqn}{\method{DoDQN}} \newcommand{\doddqn}{\method{DoDDQN}} \newcommand{\ddqn}{\method{DDQN}} \newcommand{\pdodqn}{\method{PER-DoDQN}} \newcommand{\pdoddqn}{\method{PER-DoDDQN}} \newcommand{\per}{\method{PER}} \newcommand{\mlp}{\method{MLP}} %\newcommand{\gbdtbl}{\method{GBDT-BL}} %\newcommand{\gbrtbl}{\method{GBRT-BL}} %\newcommand{\lambdamartbl}{\method{LambdaMART-BL}} %\newcommand{\gbdtbbl}{\method{GBDT-Budget-BL}} %\newcommand{\qlbl}{\method{QL-BL}} %\newcommand{\bmbl}{\method{BM25-BL}} %\newcommand{\sdmbl}{\method{SDM-BL}} %\newcommand{\adarankbl}{\method{AdaRank-BL}} %\newcommand{\wlmbl}{\method{WLM-BL}} % %%-- Experimental methods %\newcommand{\lmccost}{\method{LM-C3-Cost}} %\newcommand{\lmcce}{\method{LM-C3-CE}} %\newcommand{\lmcrnd}{\method{LM-C3-Rnd}} %\newcommand{\gbdtccost}{\method{GBDT-C3-Cost}} %\newcommand{\gbdtcce}{\method{GBDT-C3-CE}} %\newcommand{\gbdtcrnd}{\method{GBDT-C3-Rnd}} %\newcommand{\gbrtccost}{\method{GBRT-C3-Cost}} %\newcommand{\gbrtcce}{\method{GBRT-C3-CE}} %\newcommand{\gbrtcrnd}{\method{GBRT-C3-Rnd}} %\newcommand{\lambdamartccost}{\method{LambdaMART-C3-Cost}} %\newcommand{\lambdamartcce}{\method{LambdaMART-C3-CE}} %\newcommand{\lambdamartcrnd}{\method{LambdaMART-C3-Rnd}} % %\newcommand{\lmc}{\method{LM-C3-C}} %\newcommand{\lme}{\method{LM-C3-E}} %\newcommand{\lmf}{\method{LM-C3-F}} %\newcommand{\gbdtc}{\method{GBDT-C3-C}} %\newcommand{\gbdte}{\method{GBDT-C3-E}} %\newcommand{\gbdtf}{\method{GBDT-C3-F}} %\newcommand{\gbrtc}{\method{GBRT-C3-C}} %\newcommand{\gbrte}{\method{GBRT-C3-E}} %\newcommand{\gbrtf}{\method{GBRT-C3-F}} %\newcommand{\lambdamartc}{\method{LambdaMART-C3-C}} %\newcommand{\lambdamarte}{\method{LambdaMART-C3-E}} %\newcommand{\lambdamartf}{\method{LambdaMART-C3-F}} %-- Tools \newcommand{\xgboost}{} \newcommand{\scikit}{} \newcommand{\tensorflow}{} %-- misc formatting \def\D{\hphantom{1}} \def\C{\hphantom{1,}} %-- Misc control commands \newcommand\method[1]{{\sf\small{#1}}} \newcommand\smethod[1]{{\sf\scriptsize{#1}}} \newcommand\mytt[1]{{\bf{\tt{\small{#1}}}}} \newcommand{\alginp}[1]{\makebox[15mm][l]{\sc Input:}\\[0.5ex]} \newcommand{\algout}[1]{\makebox[15mm][l]{\sc Output:}\\} %--- Ranking Stuff \newcommand{\smin}{\var{s\_min}} \newcommand{\smax}{\var{s\_max}} \newcommand{\Answers}{\var{Ans}} \newcommand{\docweight}{\var{docweight}_{d}} \newcommand{\score}{\var{score}_{d,t}} \newcommand{\pivot}{\var{pivot}} \newcommand{\cpivot}{c_{\mbox{\scriptsizepivot}}} \newcommand{\tpivot}{t_{\mbox{\scriptsizepivot}}} \newcommand{\posting}{\ensuremath{}} \newcommand{\dfdt}{\rangle d,f_{d,t} \langle} \newcommand{\tf}{\mbox{ extsc{TF}}\xspace} \newcommand{\tfidf}{\mbox{tfidf}\xspace} \newcommand{\tftd}{\ensuremath{\tf_{t,d}}} \newcommand{\tfqd}{\ensuremath{\tf_{q,d}}} \newcommand{\idf}{\mbox{ extsc{idf}}\xspace} \newcommand{\idft}{\ensuremath{idf_t}} \newcommand{\idfq}{\ensuremath{idf_q}} \newcommand{\idld}{\ensuremath{idl_d}} \newcommand{\idl}{\mbox{ extsc{idl}}\xspace} \newcommand{\wtd}{\ensuremath{wt_d}} %--- Ops %% \newcommand{\opstyle}[1]{\mbox{ extsc{#1}}} \newcommand{\opstyle}[1]{\mbox{{#1}}} \newcommand{\bmax}{\opstyle{BLOCK-MAX}\xspace} \newcommand{\wand}{\opstyle{WAND}\xspace} \newcommand{\bmwand}{\opstyle{BM-WAND}\xspace} \newcommand{\maxscore}{\opstyle{MAXSCORE}\xspace} \newcommand{\hsv}{\opstyle{HSV}\xspace} \newcommand{\pst}{\opstyle{PST}\xspace} \newcommand{\gtaat}{\opstyle{Greedy-TAAT}\xspace} \newcommand{\taat}{\opstyle{TAAT}\xspace} \newcommand{\daat}{\opstyle{DAAT}\xspace} %--- Misc \newcommand{\bwt}{{\sc bwt}\xspace} \newcommand{\fmindex}{{\sc FM-index}\xspace} \def\xbwt{\mtxt^{\mbox{\scriptsize {\sc bwt}}}} \newcommand{\sa}{\mbox{\mbox{\sc sa}}} \newcommand{\lf}{\mbox{\mbox{\sc lf}}} \newcommand{\cpu}{{\sc cpu}\xspace} \newcommand{\ram}{{\sc ram}\xspace} \newcommand{\ascii}{{\sc ascii}\xspace} \newcommand{\sgml}{{\sc sgml}\xspace} \newcommand{\trec}{{\sc trec}\xspace} \newcommand{\collection}[1]{\mbox{\small\sc{#1}}} \newcommand{\newswire}{\collection{NewsWire}} \newcommand{\wt}{\collection{WT10G}} \newcommand{\gov}{\collection{GOV2}} \newcommand{\raw}{\mbox{\sc raw}\xspace} %--- Macros \newcommand{\bm}{\opstyle{BM25}} \newcommand{\lm}{\opstyle{LMDS}} \newcommand{\pl}{\mbox{\bf\scriptsize PL2}\xspace} \newcommand{\tfbm}{\ensuremath{\mbox{TF}_{\mbox{\scriptsize{BM25}}}\xspace}} \newcommand{\utf}{\mbox{\scriptsize UTF-8}\xspace} %%\newcommand{\newt}{\mbox{\method{NeWT}\xspace}} \newcommand{\newt}{\mbox{\method{NewSys}\xspace}} \newcommand{\indri}{\method{Indri\xspace}} \newcommand{\lynx}{\method{Lynx\xspace}} \newcommand{\boilerpipe}{\method{Boilerpipe\xspace}} \newcommand{\terrier}{\method{Terrier\xspace}} \newcommand{\Space}{space\xspace} \newcommand{\prefix}{prefix\xspace} \newcommand{\suffix}{suffix\xspace} \newcommand{\plain}{plain\xspace} \newcommand{\intent}{{\sc intent}\xspace} \newcommand{\slarge}{S-Lrg} \newcommand{\ssmall}{S-Sml} \newcommand{\realdat}{R-Data} \newcommand{\SA}{\mbox{SA}} \newcommand{\Tmin}{T_{\mbox{\scriptsize{min}}}} \newcommand{\Tmax}{T_{\mbox{\scriptsize{max}}}} \newcommand{\argmin}{\operatornamewithlimits{argmin}} \newcommand{\argmax}{\operatornamewithlimits{argmax}} \newcommand{\lmax}{\operatornamewithlimits{max}} \newcommand{\llim}{\operatornamewithlimits{lim}} %-- Sizes \newcommand\kb[1]{\,kB} \newcommand\mb[1]{\,MB} \newcommand\gb[1]{\,GB} \newcommand\tb[1]{\,TB} %-- maths \newcommand{\ith}{\ensuremath{i^{\mbox{\scriptsize th}}}} \newcommand{\nmax}{n_{\mbox{\tiny max}}} \newcommand{\newlne}{{}n} \newcommand{\var}[1]{\mbox{#1}} \newcommand{\svar}[1]{\mbox{\scriptsize#1}} %-- misc formatting \def\D{\hphantom{1}} \def\C{\hphantom{1,}} \newcommand{\myurl}[1]{{\url{#1}}} \newcommand{\mycaption}[1]{}} \newcommand{\myquery}[1]{{``{\tt{#1}}''}} %%AM \newcommand{\myparagraph}[1]{\paragraph*{\normalsize\it{#1}}} %% \newcommand{\myparagraph}[1]{\mysubsection{#1}} %\newcommand{\myparagraph}[1]{~\\{#1}.~} \newcommand{\myparagraph}[1]{{#1}.~} \newcommand{\mysubsection}[1]{\subsubsection*{{#1}}} \newcommand{\noi}{} \newcommand{\mycomment}[1]{} \newcommand{\mylabel}[1]{} \newcommand{\mytab}{\makebox[6mm]{~}} \newcommand{\fixed}[1]{\makebox[18mm]{#1}} %-- big iron \newcommand{\haatheshort}{ Intel Xeon E5640 fgcessors {\mb{12}} cache {\gb{144}} SDRAM} %\newcommand{\haathee}{ Intel Xeon E5640 Processors %a {\mb{12}} smart cache, {\gb{144}} DDR3 DRAM, eight {\tb{2}} SATA-II disks, %and running Ubuntu Linux 11.10} \newcommand{\haathee}{ Intel Xeon E5640 Processors {\mb{12}} smart cache, {\gb{144}} DDR3 DRAM, running Ubuntu Linux 11.10} %-- table formatting \newlength{\onedigit} \settowidth{\onedigit}{} \newcommand{\w}{\makebox[\onedigit]{~}} \newcounter{todocount} \setcounter{todocount}{1} \newcommand{\todo}[1]{{\color{blue}*** [\thetodocount] #1 ***\addtocounter{todocount}{1}}} % % File acl2020.tex % %% Based style files ACL 2020, %% Based style files ACL 2018, NAACL 2018/19, %% Based style files ACL-2015, improvements %% taken NAACL-2016 style %% Based style files ACL-2014, were, turn, %% based ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009, %% EACL-2009, IJCNLP-2008... %% Based style files EACL 2006 %%e.agirre@ehu.es Sergi.Balari@uab.es %% ACL 08 Joakim Nivre Noah Smith \documentclass[11pt]{article} \usepackage{coling2020} \usepackage{times} \usepackage{url} \usepackage{latexsym} \renewcommand{\UrlFont}{\ttfamily\small} \usepackage{booktabs} % For formal tables \usepackage[normalem]{ulem} \usepackage{xcolor} %%xl: I need xcolour.... \usepackage{algorithm} \usepackage{algpseudocode} \usepackage{amsmath} \usepackage{mathrsfs} \usepackage{amssymb} \usepackage{subfigure} \usepackage{makecell} \usepackage{mathtools} \usepackage[font=rm]{caption} % \usepackage{subcaption} \DeclareCaptionType{copyrightbox} \usepackage{shortvrb} \usepackage{tabularx} \usepackage{verbatim} \usepackage{xspace} \usepackage{listings} \lstset{basicstyle=\small\ttfamily,mathescape,columns=fullflexible,keepspaces=true} \usepackage{fontawesome} \usepackage[multiple]{footmisc} \usepackage[all]{nowidow} \usepackage{balance} % This strictly necessary, may commented out, % improve layout manuscript, % typically save space. \usepackage{microtype} \usepackage{wrapfig} %\aclfinalcopy % Uncomment line final submission %\def\aclpaperid{***} % Enter acl Paper ID %\setlength\titlebox{5cm} % You expand titlebox need extra space % show authors. Please make titlebox % smaller 5cm ; check % camera-ready version ask change back. \usepackage[medium,compact]{titlesec} \usepackage{enumitem} \setlist{itemsep=0pt,parsep=0pt} \colingfinalcopy \title{Interactive Question Clarification Dialogue via Reinforcement Learning} \author{ Xiang Hu\footnotemark[2] \\ % Ant Financial Services Group\footnotemark[2]\\ Hasso Plattner Institute, University Potsdam\footnotemark[3]\\ \\\And Zujie Wen\footnotemark[2] \\ % Rutgers University\\ % \\\And Yafang Wang \footnotemark[2] \thanks{\ \ corresponding author, email: yafang.wyf@antfin.com} \\ % Ant Financial Services Group\\ % % \thanks{Corresponding author, Email: yafang.wyf@antfin.com} \\\And Xiaolong Li\footnotemark[2] \\ % Rutgers University\\ % \\\And Gerard de Melo\footnotemark[3] \\ % Ant Financial Services Group\\ % \\ tu } \date{} In paper, proposed ensemble document reranking approach PM queries. It builds pretrained BERT models combine strategies document relevance matching extractive/abstractive text summarization arrive document rankings complementary eventual evaluations. Our experiments also demonstrate entity embeddings trained annotated domain specific corpus help document retrieval settings. Both quantitative qualitative analyses throw light strengths approach. One scope advances lies improving summarizer generate better pseudo-queries \texttt{ABS} starts perform better own. At high level, training data hard generate large amounts IR tasks biomedicine holds TREC-PM datasets too. To better train \texttt{ABS}, may better adapt biomedical IR datasets. For example, TREC clinical decision support task ran 2014 2016 related PM task. A future goal see apply neural transfer learning domain adaptation efforts repurpose CDS datasets PM task. Another straightforward idea reuse generated pseudo-query sentences eDisMax query Solr, form pseudo relevance feedback. The expression Section focuses asymmetric formulation starts query term looks best match pseudo-query. Considering symmetric formulation, where, also begin pseudo-query terms average summands may provide better estimate reranking. Additionally, thorough exploration external biomedical knowledge bases incorporated neural IR framework PM also important. {}"," %uestion reformulation % \todo{explain defect of previous works} Coping with ambiguous questions has been a perennial problem in real-world dialogue systems. Although clarification by asking questions is a common form of human interaction, it is hard to define appropriate questions to elicit more specific intents from a user. In this work, we propose a reinforcement model to clarify ambiguous questions by suggesting refinements of the original query. We first formulate a collection partitioning problem to select a set of labels enabling us to distinguish potential unambiguous intents. We list the chosen labels as intent phrases to the user for further confirmation. The selected label along with the original user query then serves as a refined query, for which a suitable response can more easily be identified. The model is trained using reinforcement learning with a deep policy network.  We evaluate our model based on real-world user clicks and demonstrate significant improvements across several different experiments. % The ability to ask clarification questions to solve ambiguity and missing information phenomena is essential for question answering systems. The current research mainly uses questions generation or questions ranking to ask a clarification question, which lead to low success rate and redundant information. Insufficient use of the graphic user interface  results in more interactions with users. There is usually no guarantee for replying the user after the clarification. To solve these problems, we propose a question clarification method based on intents recommendation. intents are extracted from the historical Frequently Asked Questions of our system. The recommended intents can provide more concise candidates for user to click. Once an intent is clicked, the system guaranteed to provide a clear question list relative to the real question. We use the reinforcement learning method to recommend intents, and the most challenging problem is cold start. The reward is designed to recommend the most relevant clear question list and maximize the information gain after clicking one intent for better question clarification. The method we proposed for question clarification can solve both ambiguity and missing information phenomena. Experiments on interactions with more than 100 million real-world online users shows the effectiveness of this method."
"%Discourse Parsing key NLP %an important task, aiming establish better understanding multi-sentential natural language. %, inherently ambiguous intent-driven. %Most research area thereby focuses one two main discourse theories RST PDTB , proposed decade ago. Discourse Parsing key Natural Language Processing task processing multi-sentential text. Most research area focuses one two main discourse theories -- RST PDTB . The latter thereby postulates shallow discourse structures, combining adjacent sentences mainly focuses explicit implicit discourse connectives. The RST discourse theory, hand, proposes discourse trees complete documents constituency-style manner, tree leaves called Elementary Discourse Units , representing span-like sentence fragments. Internal tree-nodes encode discourse relations sub-trees tuple \{Nuclearity, Relation\}, nuclearity defines sub-tree salience local context, relation specifies type relationship binary child nodes automatically inferred discourse structures nuclearity attributes large-scale sentiment datasets already reached state-of-the-art performance inter-domain discourse parsing task. Similarly, \citet{liu2018learning} infer latent discourse trees text classification task, \citet{liu2019single} employ downstream task summarization using transformer model generate discourse trees. Outside area discourse parsing, syntactic trees previously inferred according several strategies, e.g. \citet{socher2011semi, yogatama2016learning, choi2018learning, maillard2019jointly}. %including: Discrete decisions frameworks using Gumbel-softmax component , applying reinforcement approach syntactic parsing , using reconstruction error adjacent spans indicator syntactic coherence within sentence employing CKY approach select syntactic trees soft model . In general, approaches mentioned %to automatically annotate text discourse structures syntactic trees shown capture valuable structural information. Some models outperform baselines trained human-annotated datasets , others proven enhance diverse downstream tasks . However, despite initial successes, one critical limitation aforementioned models share task-specificity, possibly capturing downstream-task related information. %of discourse, This potentially compromises generality resulting trees, instance shown model using text classification data \citet{ferracane2019evaluating}. %For instance, approach \citet{huber2019predicting} uses document-level sentiment information inform discourse tree generation, others %have %using summarization data sentence-level sentiment cues achieve results. In order alleviate limitation task-specificity, propose new strategy generate tree structures task-agnostic, unsupervised fashion extending latent tree induction framework proposed \citet{choi2018learning} auto-encoding objective. %. Our system thereby extracts important knowledge natural text optimizing underlying tree structures distributed representations. We believe resulting discourse structures effectively aggregate related commonly appearing patterns data merging coherent text spans intermediate sub-tree encodings, similar intuition presented \citet{drozdov2019unsupervised}. However, contrast approach \citet{drozdov2019unsupervised}, model makes discrete structural decisions, rather joining possible subtrees using soft attention mechanism. We believe discrete tree structures allow model efficiently achieve autoencoder objective reconstructing inputs, directly learning written language aggregated wild . In general, proposed approach applied tree-structured objective, syntactic parsing, discourse parsing problems outside NLP, like tree-planning decision-tree generation . Yet, due especially difficult annotation process generate discourse trees, initially develop method %complement task-specific models generate much larger diverse discourse treebanks. We present end-to-end model resolve ambiguous questions dialogue clarifying using label suggestions. We cast question clarification problem collection partition problem. In order improve quality interactive labels well reduce semantic overlap labels user's question, propose novel reward based recall potential intents information gain. We establish effectiveness series experiments, suggest novel notion clarification may well adopted kinds disambiguation problems. Our experiments shows way intent interaction effective solving user problems returning relevant results. At time, comparison online ctr, fully proves intents recommend policy model trained via new reward helpful users."," Discourse information, as postulated by popular discourse theories, such as RST and PDTB, has been shown to improve an increasing number of downstream NLP tasks, showing positive effects and synergies of discourse with important real-world applications. While methods for incorporating discourse become more and more sophisticated, the growing need for robust and general discourse structures has not been sufficiently met by current discourse parsers, usually trained on small scale datasets in a strictly limited number of domains. This makes the prediction for arbitrary tasks noisy and unreliable. The overall resulting lack of high-quality, high-quantity discourse trees poses a severe limitation to further progress.  In order the alleviate this shortcoming, we propose a new strategy to generate tree structures in a task-agnostic, unsupervised fashion by extending a latent tree induction framework with an auto-encoding objective. The proposed approach can be applied to any tree-structured objective, such as syntactic parsing, discourse parsing and others. However, due to the especially difficult annotation process to generate discourse trees, we initially develop a method to generate larger and more diverse discourse treebanks. In this paper we are inferring general tree structures of natural text in multiple domains, showing promising results on a diverse set of tasks.  %With this paper, we intend to initiate a new line of research on inferring discourse structures in an unbiased manner. %With a growing need for robust and general discourse structures in many downstream tasks and real-world applications, the current lack of high-quality, high-quantity discourse trees poses a severe shortcoming. %In order the alleviate this limitation, we propose a new strategy to generate tree structures in a task-agnostic, unsupervised fashion by extending a latent tree induction framework with an auto-encoding objective. The proposed approach can be applied to any tree-structured objective, such as syntactic parsing, discourse parsing and others. However, due to the especially difficult annotation process to generate discourse trees, we initially develop such method to complement task-specific models in generating much larger and more diverse discourse treebanks."
"Retrieval technique response selection popular elegant approach framing chatbot i.e. open-domain dialog system. Given conversation context, retrieval-based chatbot aims select appropriate utterance response pre-constructed database. %that saves large number human written utterances. In order balance effectiveness efficiency, mosts retrieval-based chatbots employ coarse-grained selection module recall set candidate semantic coherent conversation context speed processing. %  % elated work To best knowledge, two kinds approaches build coarse-grained selection module retrieval-based chatbots: sparse representation: TF-IDF BM25 widely used method. It matches keywords inverted index seen representing utterances highdimensional sparse vectors ; %This method runs quickly, lacks rich semantic information. dense representation: Large scale pre-trained langauge models , e.g. BERT commonly used obtain semantic representation utterances, could used recall semantic coherent candidates using cosine similarity . %Due high computational burden similarity calculating, %this method runs slowly, could consider rich semantic information %. % dense vectorseaknessoposed method % Luan2020SparseDABM25 % Dense  BERT So far, systematic comparison two kinds approaches retrieval-based chatbots, kind method appropriate real scenarios still open question confuses researchers dialog system community. Thus, paper, first conduct extensive experiment compare two approaches four important aspects: effectiveness; search time cost; index storage occupation; human evaluation. Extensive experiment results four popular response selection datasets demonstrate dense representation significantly outperforms sparse representation expense lower speed bigger storage sparse representation, unsufferable real scenarios. Then, order overcome fatal weaknesses dense representation methods, propose ultra-fast, low-storage highly effective Deep Semantic Hashing Coarse-grained selection module %based given dense representation method, effectively balances effectiveness efficiency. Specifically, first stack novel hashing optimizing module consists two autoencoders given dense representation method. Then, three well designed loss functions used optimize two autoencoders hashing optimizing module: preserved loss; hash loss; quantization loss. After training, autoencoders could effectively preserve rich semantic similarity information dense vectors hash codes, computational storage efficient . \iffalse first all, train dense representation method using dual-architecture , contains context BERT encoder candidate BERT encoder. Then, separately stack deep autoencoder model encoder. The auto-encoder model could encode semantic information dense vectors hashing codes. Finally, novel deep semantic hashing approach used learn binary compressed representation dense vectors. % parseense It noted that, different dense vectors, binary hashing code storage-efficient ultra-fast calculate , also keeps rich semantic information dense vectors. \fi Extensive experiment results four popular response selection datasets demonstrate proposed DSHC model achieve much faster search speed lower storage occupation sparse representation method, limited performance loss compared given dense representation method. In paper, contributions three-fold: The rest paper organized follows: introduce important concepts background covered paper Section 2. The experiment settings presented Section 3. In Section 4, systematically compare current two kinds methods coarse-grained selection module: sparse representation; dense representation. In Section 5, introduce proposed DSHC model, detailed experiment results elaborated. In Section 6, conduct case study. Finally, conclude work Section 7. Due page limitation, details extra analysis found Appendix. In paper, proposed truly unsupervised purely data-driven tree-style autoencoder compress reconstruct textual data. We show potential T-AE approach task discourse parsing, severely suffers training-data sparsity, due tedious expensive annotation process. Our unsupervised model outperforms one commonly used, linguistically supervised approaches, without making assumptions underlying data, except sentence/document split. The superior performance compared hierarchical left branching baseline plausibly indicates unsupervised structures could valuable combined supervised distantly supervised models improve joint performance. Furthermore, superior performance large out-of-domain model trained Yelp'13 dataset small-scale within-domain model trained raw text RST-DT dataset shows synergies corpora well strong potential even larger datasets enhance performance approach. In future, intend extend work several ways: First, want explore application generative models, employing variational autoencoder. Second, plan study tasks besides predicting discourse, syntactic parsing, well additional synergistic downstream tasks . To improve model important downstream tasks , want explore pre-training/fine-tuning approach, similar contextualized language models, BERT. Combining novel approach distantly-supervised supervised models another future direction want explore. Lastly, plan evaluate additional model adaptions, two independent models sentence- document-level, incorporating BERT EDU encoder end-to-end model soft-constraints sentence-level. Try generative models \\ Try syntactic parsing \\ Try downstream tasks \\ separate models\\ soft constraint model\\ add bert\\","   We study the coarse-grained selection module in retrieval-based chatbot.   Coarse-grained selection is a basic module in a retrieval-based chatbot,   which constructs a rough candidate set from the whole database to speed up the interaction with customers.   So far, there are two kinds of approaches for coarse-grained selection module:     sparse representation;  dense representation.   To the best of our knowledge, there is no systematic comparison between these two approaches in retrieval-based chatbots,   and which kind of method is better in real scenarios is still an open question.   In this paper, we first systematically compare these two methods from four aspects:     effectiveness;  index stoarge;  search time cost;  human evaluation.   Extensive experiment results demonstrate that dense representation method    significantly outperforms the sparse representation,    but costs more time and storage occupation.   In order to overcome these fatal weaknesses of dense representation method,    we propose an ultra-fast, low-storage, and highly effective    Deep Semantic Hashing Coarse-grained selection method, called DSHC model.   Specifically, in our proposed DSHC model,   a hashing optimizing module that consists of two autoencoder models is    stacked on a trained dense representation model,   and three loss functions are designed to optimize it.   The hash codes provided by hashing optimizing module effectively    preserve the rich semantic and similarity information in dense vectors.   Extensive experiment results prove that,   our proposed DSHC model can achieve much faster speed and lower storage than sparse representation,   with limited performance loss compared with dense representation.   Besides, our source codes have been publicly released for future research\footnote{\url{https://github.com/gmftbyGMFTBY/HashRetrieval}}."
"With huge quantities natural language documents, search engines essential time saved information retrieval tasks. Usually, deployed search engines achieve task ranking documents relevance according query. \\ Recently, research focused task extracting span text exactly matches user's query Machine Reading Comprehension Question Answering. \\ Question Answering deals extraction span text short paragraph exactly answers natural language question. Recent deep learning models based heavy pretrained language models like BERT achieved better human performances tasks . \\ One could try apply QA models Open-Domain Question Answering paradigm aims answer questions taking big amount documents knowledge source. Two main issues emerge : first, applying 100M parameters language models potentially millions documents requires unreasonable GPU-resources. Then, QA models allow compare spans text coming exclusively single paragraph open-domain QA paradigm, one needs compare spans text coming wide range documents. \\ Our system, done previous work, deals resources issue thanks Retriever module, based BM25 algorithm, allows reduce search space millions articles hundred paragraphs. The second issue tackled adding deep learning based Scorer module re-ranks precision paragraphs returned Retriever. Eventually, Extractor module uses QA deep learning model extract best span text first paragraph returned Scorer. To avoid heavy hardly scalable pipeline consisting two huge deep learning models, parallelize re-ranking span extraction tasks thanks multitask learning : maintaining high performances, allows significantly reduce memory requirements inference time. Our system achieve state-of-the-art results open-squad benchmark.  REALM  In paper, first systematically compare dense sparse representation method retrieval-based chatbot four important aspects: effectiveness; search time cost; index stoarge; human evaluation. Extensive experiment results demonstrate dense representation method could achieve better performance expense time cost higher storage occupation, In order overcome fatal weaknesses, propose deep semantic hashing based corase-grained selection method. Extensive experiment results prove effectiveness efficiency DSHC model.","   In this paper, we introduce MIX : a   multi-task deep learning approach to solve Open-Domain Question  Answering. First, we design our system as a multi-stage pipeline made of 3 building blocks : a BM25-based Retriever, to reduce the search space; RoBERTa based Scorer and Extractor, to rank retrieved paragraphs and extract relevant spans of text respectively. Eventually, we further improve computational efficiency of our system to deal with the scalability challenge : thanks to multi-task learning,   we parallelize the close tasks solved by the Scorer and the Extractor. Our system is on par with state-of-the-art performances on the squad-open benchmark while being simpler conceptually."
"Named Entity Recognition task identifying span class Named Entity unstructured text. NEs typically include limited persons, companies, dates, geographical locations . Legal NER central task language processing legal documents, especially extracting key information name parties case, court name case number, references laws judgements, name few. The extracted NEs could integrated legal research workflows functionalities search, document anonymization case summarization thereby enabling expediting insights legal professionals . NER commonly formalized sequence labeling task: token document assigned single label indicates whether token belongs entity predefined set categories . To create training dataset format annotator required manually label token sentence respective category. In format, NE location NE source text known. This format training data refer hereafter old standard data. Obtaining required voluminous gold standard data train models is, therefore, laborious costly task. In paper, perform NER filed lawsuits US courts. Specifically, aim identify party names case, i.e. names plaintiffs defendants, large collection publicly available cases 200 courts different US jurisdictions. The party names identified legal annotators exact location text unknown. In respect, access old standard training data even though target NEs available. This feature dataset introduces key difference task NER tasks. One solution problem generate old standard training data searching locations known NEs source text . By performing additional transformation data, would able train sequence labeling NER models. For following reasons, solution nontrivial. First, source text also extracted scanned PDF files , contains Optical Character Recognition mistakes and/or typos may present target NEs. Second, besides potential OCR errors character level, closely spaced, two-column page layouts often found headers filed cases, represent additional challenge OCR, tends concatenate text across columns . In cases, tokens make NEs source text may intertwined words and/or sentences. Third, variations names may also present source text human-generated labels, presence first and/or middle names whole initials and, lesser extent, typos. To address challenges imposed format training data inspired work field abstractive summarization, propose reformulate NER task, sequence labeling problem, text-to-text sequence generation problem use pointer generator network . With reformulation, contrast sequence labeling, require knowledge NE locations text training labels. A recent study \citet{Li2020} proposed different formulation NER task question answering task achieved state-of-the-art performance number published NER datasets . In study, adopt hybrid extractive-abstractive architecture, based recurrent neural networks coupled global attention copying attention mechanisms . The proposed architecture successfully used abstractive summarization since copy words source text via pointing deal effectively out-of-vocabulary words  words seen training. Our approach conceptually simple empirically powerful show pointer generator outperforms typical NER architectures case noisy lengthy inputs NE's location text known. In addition, examine approach used related NER task case number extraction. The case number unique combination letters, numbers special characters single token are, therefore, particularly challenging NER models often dealt OOV words model. As party names task discussed above, case number task old standard labels case number location text. We show character level sequence generation network dramatically increase ability extract case numbers source text, compared word level sequence generation network. The rest paper organized follows. In Section 2, discuss related work field NER legal domain. In Section 3, describe proposal NER text-to-text sequence generation task absence gold standard data formulate task two ways: combination automatically labeling NE's location using conventional sequence labeling method NER, text-to-text sequence generation task NEs directly generated text. Section 4 presents experimental design, results analysis. Section 5 presents case number case study. Finally, conclude discuss directions future work. \iffalse For papers accepted main conference, invite authors provide translation title abstract 1-2 page synopsis paper second language authors' choice. Appropriate languages include limited authors' native languages, languages spoken authors' place affiliation, languages focus research presented. \fi"," Named Entity Recognition  is the task of identifying and classifying named entities in unstructured text. In the legal domain, named entities of interest may include the case parties, judges, names of courts, case numbers, references to laws etc. We study the problem of legal NER with noisy text extracted from PDF files of filed court cases from US courts. The old standard training data for NER systems provide annotation for each token of the text with the corresponding entity or non-entity label. We work with only partially complete training data, which differ from the gold standard NER data in that the exact location of the entities in the text is unknown and the entities may contain typos and/or OCR mistakes. To overcome the challenges of our noisy training data, e.g. text extraction errors and/or typos and unknown label indices, we formulate the NER task as a text-to-text sequence generation task and train a pointer generator network to generate the entities in the document rather than label them. We show that the pointer generator can be effective for NER in the absence of gold standard data and outperforms the common NER neural network architectures in long legal documents."
"Speech translation~, translates audio signals speech one language text foreign language, hot research subject nowadays widespread applications, like cross-language videoconferencing customer support chats. Traditionally, researchers build speech translation system via cascading manner, including automatic speech recognition~ machine translation~ subsystem. Cascade systems, however, suffer error propagation problems, inaccurate ASR output would theoretically cause translation errors. Owing recent progress sequence-to-sequence modeling neural machine translation~ end-to-end speech recognition, becomes feasible efficient train fully end-to-end ST model. This end-to-end fashion attracts much attention due appealing properties: a) modeling without intermediate ASR transcriptions obviously alleviates propagation errors; b) single unified ST model beneficial deployment lower latency contrast cascade systems. % However, end-to-end paradigm far reaching industry requirements requires large-scale end-to-end corpora audios paired textual translations, hard acquire. Recent studies show end-to-end ST models achieve promising performance comparable cascaded models. The end-to-end solution great potential dominant technology speech translation, however challenges remain. The first benchmarks. Many ST studies conduct experiments different datasets. ~\citet{liu2019end} evaluate method TED English-Chinese; ~\citet{dong2020ted} use Augmented Librispeech English-French IWSLT2018 English-German dataset; ~\citet{wu2020self} show results CoVoST dataset FR/RO portions MuST-C dataset. Different datasets make difficult compare performance approaches. Further, even dataset, baseline results necessarily kept consist. Take Augmented Librispeech English-French dataset example. ~\citet{dong2020ted} report pre-trained baseline 15.3 result ~\citet{liu2019end} 14.3 terms tokenized BLEU, while~\citet{inaguma2020} report 15.5 . The mismatching baseline makes comparison final results meaningless. One primary reasons preprocessing audio data complex, ST model training involves many tricks, pre-training data augmentation. Therefore reproducible reliable benchmark required. In work, present \method, toolkit easily building training end-to-end ST models, well end-to-end ASR NMT cascade systems. We implement start-of-the-art Transformer-based models provide step-by-step recipes feature extraction, data preprocessing, model training, inference researchers reproduce benchmarks. Though exist several counterparts, Lingvo, fairseq-ST Kaldi~ style~ESPnet-ST, \method specially designed speech translation tasks, encapsulates details speech processing frees developers data engineering. It easy use extend. The contributions work follows: % \method provides straightforward preprocessing several publicly available audio datasets, encourages researchers concentrate innovating ST technology less aware speech processing. % \method aims ST tasks using end-to-end framework. Moreover, knowledge, pioneer community, follows Kaldi~ style data processing recipes. But \method, stand perspective natural language processing~. This work presents simple yet powerful reformulation NER task text-to-text sequence generation task applying pointer generator network, model architecture predominantly used NLP field summarization. There several key advantages proposed formalization: need acquire old data NER task target NEs known indices source text, Pointer Generator network outperforms popular sequence-labeling architectures NER task case longer text inputs, Pointer Generator able accurately generate NEs corrupted due OCR errors extracting two-column formatted text. In future, would like explore capacity Pointer Generator extract additional types NEs."," \method is an open-source toolkit for neural speech translation developed by Bytedance AI Lab.  The toolkit mainly focuses on end-to-end speech translation, which is easy to use, modify, and extend to advanced speech translation research and products.   \method aims at facilitating the speech translation research for NLP researchers and provides a complete setup for speech translation benchmarks, including feature extraction, data preprocessing, distributed training, and evaluation.  Moreover, The toolkit implements several major architectures for end-to-end speech translation. It shows experimental results for different benchmark datasets, which can be regarded as reliable baselines for future research. The toolkit is publicly available at \url{https://github.com/bytedance/neurst}."
"Query reformulation paraphrase generation techniques employed variety purposes natural language processing , dialogue generation , machine translation , especially question answering systems . Generating coherent clean texts reduce potential errors downstream systems. In cases users receiving end NLP pipelines, essential show fluent human-like languages lose faith recede requiring human agents sake better understanding communication. In search question answering systems, query reformulation aims paraphrase restructure original question sequences, transforming ones interpretable natural well-formedness grammar semantics. Typically, users may patience input entirely grammatical coherent question, cause issues downstream components understand give accurate predictions answers. When human representatives present, originally noisy query question reiterated rephrased double-check users asking for. This costly operation every convoluted question needs restated. By NLP model reformulate input queries, reformulations fed back users confirm original intentions automated way. As result, unnecessary errors eliminated noises prevented propagating NLP pipeline, contain series models intent classification, information retrieval question answering. Traditionally, rule-based statistical methods studied paraphrase reformulation generation . The advent sequence-to-sequence learning made feasible train deep neural networks new paradigm. We investigate paraphrase denoise queries generate well-formed reformulations using Seq2Seq learning models LSTMs transformers . Following framework AQA , Seq2Seq model pre-trained supervised tasks tuned using reinforcement learning machine comprehension QA dataset SearchQA , learning pre-trained BiDAF QA system generates rewards. SearchQA suitable challenging dataset queries contain noisy phrases associated contexts concatenated web text snippets Google's search engine. Our goal obtain model generate better-formed reformulations based original query sequences achieve good QA performance reformulations. We use transfer learning pre-trained transformers text-to-text task formulations . In approach, pre-trained T5 models first fine-tuned paraphrase generation denoising datasets gain general paraphrasing capabilities. Then, reinforcement learning downstream QA rewards performed encouraged model produce task-specific reformulations. To knowledge, first attempt fine-tune text-to-text transformers RL, nudging model generate reward-acquiring query trajectories get better answers. We show fine-tuned text-to-text transformers better starting points RL sample efficient achieving level QA performance, acquiring rewards faster previous AQA approach uses translation-based LSTMs. T5 models also generate reformulations better readability generalize out-of-sample data. We provide new way evaluate fluency sequence level using trained metric well-formedness dataset, based real evaluations humans, reliable source widely-used algorithmic metrics based overlapping n-grams. We introduce \method toolkit easily building training end-to-end speech translation models. We provide straightforward recipes audio data pre-processing, training, inference, believe friendly NLP researchers. Moreover, report strong reproducible benchmarks, regarded reliable baselines research. This must first 5 lines tell arXiv use pdfLaTeX, strongly recommended. \pdfoutput=1 In particular, hyperref package requires pdfLaTeX order break URLs across lines. \documentclass[11pt]{article} Remove ""review"" option generate final version. \usepackage[review]{naacl2021} \usepackage[]{naacl2021} Standard package includes \usepackage{times} \usepackage{latexsym} For proper rendering hyphenation words containing Latin characters \usepackage[T1]{fontenc} For Vietnamese characters \usepackage[T5]{fontenc} See https://www.latex-project.org/help/documentation/encguide.pdf character sets This assumes files encoded UTF8 \usepackage[utf8]{inputenc} This strictly necessary, may commented out, improve layout manuscript, typically save space. \usepackage{microtype} \usepackage{multirow} If title author information fit area allocated, uncomment following \setlength\titlebox{<dim>} set <dim> something 5cm larger. \newcommand{\method}{NeurST\space} \newcommand{\red}[1]{{\color{red} #1}} \title{NeurST: Neural Speech Translation Toolkit} Author information set various styles: For several authors institution: \author{Author 1 \and ... \and Author n \\ Address line \\ ... \\ Address line} names fit well one line use Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\ For authors different institutions: \author{Author 1 \\ Address line \\ ... \\ Address line \And ... \And Author n \\ Address line \\ ... \\ Address line} To start seperate ``row'' authors use \AND, \author{Author 1 \\ Address line \\ ... \\ Address line \AND Author 2 \\ Address line \\ ... \\ Address line \And Author 3 \\ Address line \\ ... \\ Address line} \author{Chengqi Zhao, Mingxuan Wang \and Lei Li \\ ByteDance Inc. \\ \texttt{\{zhaochengqi.d, wangmingxuan.89, lileilab\}@bytedance.com} \\} \begin{document} \maketitle Entries entire Anthology, followed custom entries"," Query reformulation aims to alter potentially noisy or ambiguous text sequences into coherent ones closer to natural language questions. In this process, it is also crucial to maintain and even enhance performance in a downstream environments like question answering when rephrased queries are given as input. We explore methods to generate these query reformulations by training reformulators using text-to-text transformers and apply policy-based reinforcement learning algorithms to further encourage reward learning. Query fluency is numerically evaluated by the same class of model fine-tuned on a human-evaluated well-formedness dataset. The reformulator leverages linguistic knowledge obtained from transfer learning and generates more well-formed reformulations than a translation-based model in qualitative and quantitative analysis. During reinforcement learning, it better retains fluency while optimizing the RL objective to acquire question answering rewards and can generalize to out-of-sample textual data in qualitative evaluations. Our RL framework is demonstrated to be flexible, allowing reward signals to be sourced from different downstream environments such as intent classification."
"Identifying user's open intent plays significant role dialogue systems. As shown Figure, two known intents specific purposes, book flight restaurant reservation. However, also utterances irrelevant unsupported intents system cannot handle. It necessary distinguish utterances known intents much possible. On one hand, effectively identifying open intent improve customer satisfaction reducing false-positive error. On hand, use open intent discover potential user needs. We regard open intent classification -class classification task suggested in, group open classes class . Our goal classify n-class known intents corresponding classes correctly identifying class open intent. To solve problem,~\citet{scheirer2013toward} propose concept open space risk measure open classification.~\citet{fei-liu-2016-breaking} reduce open space risk learning closed boundary positive class similarity space. However, fail capture high-level semantic concepts SVM. ~\citet{bendale2016towards} manage reduce open space risk deep neural networks , need sample open classes selecting core hyperparameters.~\citet{hendrycks17baseline} use softmax probability confidence score, also need select confidence threshold negative samples.~\citet{Shu2017DOCDO} replace softmax sigmoid activation function, calculate confidence thresholds class based statistics. However, statistics-based thresholds learn essential differences known classes open class.~\citet{lin-xu-2019-deep} propose learn deep intent features margin loss detect unknown intents local outlier factor. However, specific decision boundaries distinguishing open intent, needs model architecture modification. Most existing methods need design specific classifiers identifying open class perform poorly common classifier. Moreover, performance open classification largely depends decision conditions. Most methods need negative samples determining suitable decision conditions. It also complicated time-consuming process manually select optimal decision condition, applicable real scenarios. To solve problems, use known intents prior knowledge, propose novel post-processing method learn adaptive decision boundary open intent classification. As illustrated Figure, first extract intent representations BERT model. Then, pre-train model supervision softmax loss. We define centroids known class suppose known intent features constrained closed ball areas. Next, aim learn radius ball area obtain decision boundaries. Specifically, initialize boundary parameters standard normal distribution use learnable activation function projection get radius decision boundary. The suitable decision boundaries satisfy two conditions. On one hand, broad enough surround in-domain samples much possible. On hand, need tight enough prevent out-of-domain samples identified in-domain samples. To address issues, propose new loss function, optimizes boundary parameters balancing open space risk empirical risk. The decision boundaries automatically learn adapt intent feature space balance boundary loss. We find post-processing method still learn discriminative decision boundaries detect open intent even without modifying original model architecture. We summarize contribution follows. Firstly, propose novel post-processing method open classification, need prior knowledge open class. Secondly, propose new loss function automatically learn tight decision boundaries adaptive feature space. To best knowledge, first attempt adopt deep neural networks learn adaptive decision boundary open classification. Thirdly, extensive experiments conducted three challenging datasets show approach obtains consistently better robust results compared state-of-the-art methods. \end{table*} In paper, propose novel regularized attentive capsule network overlapped relation extraction. RA-CapNet embeds relation query multi-head attention capsule network uses novel disagreement regularization term encourage diversity among heads capsules, making capable gathering salient information diverse semantic spaces. Our model resistant noise distant supervision achieves significant improvements standard complex datasets. In future, experiment different forms regularization terms application components model."," 		Open intent classification is a challenging task in dialogue systems. On the one hand, we should ensure the classification quality of known intents. On the other hand, we need to identify the open  intent during testing. Current models are limited in finding the appropriate decision boundary to balance the performances of both known and open intents. In this paper, we propose a post-processing method to learn the adaptive decision boundary  for open intent classification. We first utilize the labeled known intent samples to pre-train the model. Then, we use the well-trained features to automatically learn the adaptive spherical decision boundaries for each known intent. Specifically, we propose a new loss function to balance both the empirical risk and the open space risk. Our method does not need open samples and is free from modifying the model architecture. We find our approach is surprisingly insensitive with less labeled data and fewer known intents. Extensive experiments on three benchmark datasets show that our method yields significant improvements compared with the state-of-the-art methods.\footnote{Code: https://github.com/thuiar/Adaptive-Decision-Boundary}"
"Recently, deep contextual language models shown effective modeling ability text, achieving state-of-the-art results series NLP tasks. These models capture syntactic semantic information input text, generating fine-grained contextual embeddings, easily applied downstream models. Despite success large scale pre-trained language models various tasks, less clear extend semantic parsing tasks text-to-SQL, requires joint reasoning natural language utterance structured database schema information. Recent work shows powerful pre-trained language models, highly domain-specific semantic parsers improved, even though language models trained pure text encoding. % % \end{table}% However, based error analysis output neural language model-based text-to-SQL systems, observe models enhanced could mitigate following three pain points, also illustrated Table. The model ineffective match detect column names utterances. The model learn detect column names mentioned utterances matching utterance tokens schema, use matched columns generated SQL. The error analysis indicates that, cases, models miss columns synthesizing target SQL, column mentioned explicitly utterance. The model fails infer columns implicitly cell values. This problem trickier first one, model expected infer column name based cell values mentioned utterance, instead matching utterance tokens schema. This requires model domain knowledge. For example, presented second section Table, model know . The model learn compose complex queries. Besides column selection, generate correct SQL, model learn attach selected columns correct clauses. This non-trivial task, especially target SQL complex, e.g., query nested. As shown last section Table, model learn use corresponding column nested SQL, instead using column . Recent work demonstrated jointly pre-training utterances table contents benefit downstream tasks table parsing semantic parsing . These models pre-trained using Masked Language Modeling task either masking tokens utterance input tokens schema input. However, learning objective model alignment utterance schema implicitly. We hypothesize that, order cope three pain points previously listed, necessary use pre-training objectives enforce learning contextual representations better capture alignment utterances schema/table contents. In work, present language model pre-training framework, Generation-Augmented Pre-training~, exploits multiple learning objectives synthetic data generation jointly learn contextual representations natural language utterances table schema. We propose following three new learning objectives enforce joint learning also improve ability model grasp domain knowledge, helpful cross-domain scenarios: column prediction task, pre-training task consists giving label column input schema decide whether used input utterance not. This task intent improve column detection ability model. column recovery task, consists randomly replacing column names one cell values asking model recover original column name either based cell value based contextual information utterance column explicitly mentioned utterance. This learning objective meant enhance column inferring ability model. SQL generation, consists generating SQL queries given utterances schema. This task boost ability model compose complex queries leveraging large scale SQL datasets Web.%, Github. A key challenge use proposed pre-training tasks training data. Although easy obtain large scale datasets crawled tables SQL queries, difficult obtain high-quality utterances interrelated tables logically consistent crawled SQL queries. Recent work used surrounding text tables proxy natural language utterances. However, option far optimal texts dissimilar user utterances terms text length, composition content. The surrounding text table usually paragraph, natural language utterances downstream task short sentences. Furthermore, content surrounding text tables quite noisy text may irrelevant table. In \modelname, overcome pre-training data challenge use synthetic data. We propose two sequence-to-sequence generative models, SQL-to-text table-to-text, produce large scale datasets enough quality pre-training. We train generative models finetuning BART, state-of-the-art pre-trained language model. Concurrently,~\citet{yu2020grappa} and~\citet{deng2020structure} utilized synthetic data generated synchronized context-free grammar existing data-to-text datasets pre-training, respectively, requires extra crowd expert annotation efforts. The outcome \modelname pre-trained model plugged neural semantic parsers compute contextual representations utterances schema. We apply \modelname text-to-SQL semantic parsing datasets, experimental results show systems augmented \modelname~outperform state-of-the-art semantic parsers Spider Criteria-to-SQL datasets. In summary, work presents following main contributions: In paper, propose novel post-processing method open intent classification. After pre-training model labeled samples, model learn specific tight decision boundaries adaptive known intent feature space. Our method require open intent model architecture modification. Extensive experiments three benchmark datasets show method yields significant improvements compared baselines robust less labeled data fewer known intents.","  Most recently, there has been significant interest in learning contextual representations for various NLP tasks, by leveraging large scale text corpora to train large neural language models with self-supervised learning objectives, such as Masked Language Model~. However, based on a pilot study, we observe three issues of existing general-purpose language models when they are applied to text-to-SQL semantic parsers: fail to detect column mentions in the utterances, fail to infer column mentions from cell values, and fail to compose complex SQL queries. To mitigate these issues, we present a model pre-training framework, Generation-Augmented Pre-training~, that jointly learns representations of natural language utterances and table schemas by leveraging generation models to generate pre-train data. \modelnamelm\footnote{This refers to the language models that are pre-trained with GAP framework.} is trained on 2M utterance-schema pairs and 30K utterance-schema-SQL triples, whose utterances are produced by generative models. Based on experimental results, neural semantic parsers that leverage \modelnamelm~as a representation encoder obtain new state-of-the-art results on both Spider and Criteria-to-SQL benchmarks."
"Neural Machine Translation yields state-of-the-art translation performance large number parallel sentences available. However, parallel corpora available majority language pairs domains. It known NMT perform well specific domains domain-specific corpora limited, medical domain. As such, high-quality domain-specific machine translation systems high demand whereas general purpose MT limited applications. There many studies domain adaptation NMT, mainly divided two categories: data-centric model fine-tuning. Data-centric methods focus selecting generating target domain data general domain corpora, effective well explored. In paper, focus second approach. Fine-tuning common domain adaptation, first trains base model general domain data fine-tunes target domain . However, unconstrained full fine-tuning requires careful hyper-parameter tuning, prone over-fitting target domain well forgetting general domain. To tackle problems, researchers proposed several constructive approaches, view limiting size plasticity parameters fine-tuning stage, roughly divided two categories: regularization partial-tuning strategy. Regularization methods often integrate extra training objectives prevent parameters large deviations, model output regularization , elastic weight consolidation . Regularization methods, impose arbitrary global constraints parameter updates, may restrict adaptive process network, especially domain-specific corpora scarce. Partial-tuning methods either freeze several sub-layers network fine-tune others, integrate domain-specific adapters network. By fine-tuning domain-specific part model, alleviate over-fitting forgetting problem fine-tuning. However, structure designed adapting usually hand-crafted, relies experienced experts adapter brings additional parameters. Therefore, adaptive, scalable, parameter-efficient approach domain adaptation valuable worth well studying. In paper, propose \method, novel domain adaptation method via adaptive structure pruning. Our motivation inspired Continual Learning lottery hypothesis randomly-initialized, dense neural network contains sub-network match test accuracy original network training number iterations. We therefore suppose multiple machine translation models different domains share different sparse subnetworks within single neural network. Specifically, first apply standard pruning technique automatically uncover subnetwork well-trained NMT model general domain. The subnetwork capable reducing parameter without compromising accuracy. Therefore, potential keep much general information possible. Then freeze informative sparse network leave unnecessary parameters unfixed target domain, enables approach parameter efficient, eases scalability approach domains. The capacity non-fixed parameters tuned match requirements target domain, keeping parameters general domain. Our method successfully circumvents catastrophic forgetting problem retains quality general domain. As benefits flexible design, \method easily extended transfer learning problems, multilingual machine translation. We summarize main contribution follows: % --------------------Background-------------------- In work, spot three pain points Text-to-SQL semantic parsing task, propose generation-augmented pre-training framework alleviate them, four different learning objectives. Experimental results dataset dataset show effectiveness framework, achieves state-of-the-art performance datasets. \clearpage"," Fine-tuning is a major approach for domain adaptation in Neural Machine Translation .  However, unconstrained fine-tuning requires very careful hyper-parameter tuning otherwise it is easy to fall into over-fitting on the target domain and degradation on the general domain.  To mitigate it, we propose \method, a novel domain adaptation method via gradual pruning.  It learns tiny domain-specific subnetworks for tuning. During adaptation to a new domain, we only tune its corresponding subnetwork.  \method alleviates the over-fitting and the degradation problem without model modification. Additionally, with no overlapping between domain-specific subnetworks, \method is also capable of sequential multi-domain learning.    Empirical experiment results show that \method outperforms several strong competitors in the target domain test set without the quality degradation of the general domain in both single and multiple domain settings. \footnote{The source code and data are available at \url{https://github.com/ohlionel/Prune-Tune}}"
"As important task dialogue system, response selection aims find best matched response set candidates given context conversation. The retrieved responses usually natural, fluent diverse expressions rich information owing abundant resources. Therefore, response selection widely used industry attracted great attention academia. Most existing studies task pay attention matching problem utterances responses, insufficient concern reasoning issue multi-turn response selection. Just recently, MuTual, first human-labeled reasoning-based dataset multi-turn dialogue, released promote line research. Reasoning quite different matching conversations. Specifically, matching focuses capturing relevance features utterances responses, reasoning needs identify key features , also needs conduct inference based clue words. The challenges new task include: identify clue words utterances, fundamental inference; conduct inference according clue words utterances. Figure illustrates motivating example. To infer current time, must first identify clue words `10:45' `15 minutes' . Then must conduct logical inference based clue words . To tackle challenges, first, need better contextual representation identifying clue words conversations. This clue word identification inevitably relies context conversation. Although previous literature publications achieved promising results context modeling, still several limitations approaches. More concretely, existing studies either concatenate utterances form context process utterance independently, leading loss dependency relationships among utterances important contextual information. It validated chronological dependency utterances, well semantical dependency utterances, crucial multi-turn response selection. Thus, model dependencies utterances remains challenging problem context representation. Second, need devise new strategy collect clue words scattered multiple utterances need reason according clue words. In recent years, witnessed great success KBQA MRC tasks. However, new obstacles emerge transferring current reasoning approaches KBQA MRC conversational reasoning. A clear reasoning path based entities well-structured knowledge base exists KBQA, similar reasoning path utterances. Current approaches MRC conduct inference based graph taking shared entities nodes, difficult construct graphs based entities short utterances, usually suffer greater coreference resolution, poor content serious semantic omission problems comparison document text. In paper, propose new model named GRN tackle challenges end-to-end way. We first introduce two pre-training tasks called NUP UOP specially designed response selection. NUP endows GRN context-aware ability semantical dependency, UOP facilitates GRN ability capture chronological dependency. These customized pre-training methods beneficial modeling dependencies contained utterances achieve better context representation. We perform task-adaptive pre-training combined NUP UOP tasks based ALBERT model. To conduct reasoning based clue words, devise graph neural network called UDG , models dependencies utterances utterance node also collects clue words different utterances. Reasoning achieved propagating messages clue words nodes along various utterance paths UDG, graph reasoning structure realizes inference based utterance-level context vector local perspective. On hand, also implement reasoning network output trained model self-attention mechanism. This sequence reasoning structure realizes inference based highly summarized context vector global perspective. To summarize, make following contributions: In work, propose \method, effective way adapting neural machine translation models first generates informative subnetwork general domain via gradual pruning fine-tunes unnecessary parameters target domain. By so, \method able retain much general information possible alleviate catastrophic forgetting problems. Experiments show proposed \method outperforms fine-tuning several strong baselines shown much robust compared fine-tuning due complete retainment general information. Beyond that, \method extended adapting multiple domains iteratively pruning tuning, naturally suitable multi-lingual scenario. We leave multi-lingual problem future work. --------------------Acknowledgements--------------------"," We investigate response selection for multi-turn conversation in retrieval-based chatbots. Existing studies pay more attention to the matching between utterances and responses by calculating the matching score based on learned features, leading to insufficient model reasoning ability. In this paper, we propose a graph reasoning network  to address the problem. GRN first conducts pre-training based on ALBERT using next utterance prediction and utterance order prediction tasks specifically devised for response selection. These two customized pre-training tasks can endow our model with the ability of capturing semantical and chronological dependency between utterances. We then fine-tune the model on an integrated network with sequence reasoning and graph reasoning structures. The sequence reasoning module conducts inference based on the highly summarized context vector of utterance-response pairs from the global perspective. The graph reasoning module conducts the reasoning on the utterance-level graph neural network from the local perspective. Experiments on two conversational reasoning datasets show that our model can dramatically outperform the strong baseline methods and can achieve performance which is close to human-level."
"A disease abnormal medical condition poses negative impact organisms enabling access disease information goal various information extraction well text mining tasks. The task disease normalization consists assigning unique concept identifier disease names occurring clinical text. However, task challenging diseases mentioned text may display morphological orthographical variations, may utilize different word orderings equivalent words. Consider following examples: %} \end{center} In Example 1, disease mention short trunk extremities mapped candidate Knowledge Base entry containing synonyms like Growth Disorder. In Example 2, Renal amyloidosis assigned Knowledge Base ID synonyms as, Amyloidosis 8. Based studies analysis medical literature, observed disease name may occur multiple variant forms as. synonyms replacement , spelling variation , short description modifier precedes disease name , different word orderings . In paper, formulated task learning mention-candidate pair similarity using Triplet Networks . Furthermore, explored in-domain word\footnote{http://evexdb.org/pmresources/vec-space-models/} subword embeddings input representations. We find sub-word information boosts performance due gained information out-of-vocabulary terms word compositionality disease mentions. The primary contributions paper three-fold: 1) By identifying positive negative candidates concerning disease mention, optimize Triplet Network loss function influences relative distance constraint 2) We explored capability in-domain sub-word level information\footnote{https://github.com/ncbi-nlp/BioSentVec.git} solving task disease normalization. 3) Unlike existing systems , , present robust portable candidate generation approach without making use external resources hand-engineered sieves deal morphological variations. Our system achieves state-of-the-art performance NCBI disease dataset In paper, propose new architecture multi-turn response reasoning. Concretely, first propose NUP UOP pre-training tasks response selection. We design UDG utterance reasoning. We introduce sequence graph reasoning structure jointly, sequence reasoning module capture key information global perspective graph reasoning module responsible capturing clue words information local perspective. The experiment results MuTual achieve new heights. There still expansive room improvement performance . In future work, investigate balance safe response meaningful candidate response.","   Entity linking  is an essential task in text mining that maps the entity mentions in the medical text to standard entities in a given Knowledge Base . This task is of great importance in the medical domain. It can also be used for merging different medical and clinical ontologies. In this paper, we center around the problem of disease linking or normalization. This task is executed in two phases: candidate generation and candidate scoring. In this paper, we present an approach to rank the candidate Knowledge Base entries based on their similarity with disease mention. We make use of the Triplet Network for candidate ranking. While the existing methods have used carefully generated sieves and external resources for candidate generation, we introduce a robust and portable candidate generation scheme that does not make use of the hand-crafted rules. Experimental results on the standard benchmark NCBI disease dataset demonstrate that our system outperforms the prior methods by a significant margin."
"As fundamental task natural language processing , coherence analysis benefit various downstream tasks, sentiment analysis document summarization . Rhetorical Structure Theory one influential theories text coherence, document represented hierarchical discourse tree, consists set semantic units organized form dependency structure, labeled rhetorical relations. As shown Figure , leaf nodes RST discourse tree basic text spans called Elementary Discourse Units , EDUs iteratively connected rhetorical relations form larger text spans entire document included. The rhetorical relations categorized Nucleus Satellite based relative importance, Nucleus corresponds core part Satellite corresponds subordinate part. While manual coherence analysis RST theory labor-intensive requires specialized linguistic knowledge, discourse parser serves automatically transform document discourse tree. Document-level discourse parsing consists three sub-tasks: hierarchical span splitting, rhetorical nuclearity determination, rhetorical relation classification. Models RST-style discourse parsing made much progress past decade. While statistical methods utilize hand-crafted lexical syntactic features , data-driven neural approaches reduce feature-engineering labor effective representation learning, capable characterizing implicit semantic information. Neural networks first used feature extractors along traditional shift-reduce approaches dynamic programming approaches . Then, \citet{yu2018transition} bridges gap neural traditional methods end-to-end transition-based neural parser via encoder-decoder architecture. Recently, pointer networks introduced achieve linear-time complexity, models top-down parsing procedures achieve favorable results sentence-level discourse analysis tasks . However, still much space improvement document-level discourse parsing. First, compared sentence-level parsing, document-level parsing challenging due deeper tree structures longer dependencies among EDUs: benchmark dataset RST Discourse Tree Bank , average EDU number document level 56, 20 times larger sentence-level parsing. Thus modeling context information across long span essential, especially considering top-down parsing procedure poor accuracy top tree propagate toward leaf nodes. Second, three sub-tasks discourse parsing strongly rely nuanced semantic judgments, require comprehensive contextual representation various types linguistic information. Take discourse relation classification example, explicit relations overtly signaled connective word ``although'' ``because'', determined lexical syntactic features. However, approach readily adapted implicit discourse relations determination, requires high-order features semantic information. Moreover, compensate lack large-scale corpora, prior work neural modeling leveraged inductive biases syntactic features part-of-speech tagging improve performance. However, models still suffer insufficient linguistics information lack data, thus incapable acquiring deeper richer contextual representations useful discourse processing. In paper, tackle aforementioned challenges, propose document-level neural discourse parser robust representation modeling EDU document level, based top-down parsing procedure. To take advantage widely-adopted vector representations encode rich semantic information, first exploit large-scale pre-trained language model contextual representation backbone. Then incorporate boundary information implicit semantic syntactic features EDU representations, introduce hierarchical encoding architecture comprehensively characterize global information long dependency modeling. To improve inference accuracy alleviate aforesaid error propagation problem, present breadth-first span splitting propose layer-wise beam search algorithm. We train evaluate proposed model benchmark corpus RST-DT\footnote{https://catalog.ldc.upenn.edu/LDC2002T07} , achieve state-of-the-art performance fronts, significantly surpassing previous models approaching upper bound human performance. We also conduct extensive experiments analyze effectiveness proposed method. In paper, formulated task entity linking candidate ranking approach. Using Triplet Network, learn high-quality representations candidates, tailored reveal relative distances disease mention positive negative candidates. Furthermore, take step towards eliminating need generate candidates based hand-crafted rules external knowledge resources. Though method outperforms existing systems strong margin, scope improvement terms attention-based disease similarity . An intriguing course future work explore robustness scalability approach clinical datasets entity normalization."," Document-level discourse parsing, in accordance with the Rhetorical Structure Theory , remains notoriously challenging. Challenges include the deep structure of document-level discourse trees, the requirement of subtle semantic judgments, and the lack of large-scale training corpora. To address such challenges, we propose to exploit robust representations derived from multiple levels of granularity across syntax and semantics, and in turn incorporate such representations in an end-to-end encoder-decoder neural architecture for more resourceful discourse processing. In particular, we first use a pre-trained contextual language model that embodies high-order and long-range dependency to enable finer-grain semantic, syntactic, and organizational representations. We further encode such representations with boundary and hierarchical information to obtain more refined modeling for document-level discourse processing. Experimental results show that our parser achieves the state-of-the-art performance, approaching human-level performance on the benchmarked RST dataset."
"Due substantial growth effortless access Internet recent years, enormous amount unstructured textual contents generated. It crucial task organize structure voluminous unstructured text manually. Thus, automatic classification useful manipulate huge amount texts, extract meaningful insights save lot time money. Text categorization classical NLP problem aims categorize texts organized groups. It wide range applications like machine translation, question answering, summarization, sentiment analysis. There several approaches available classify texts according labels. However, deep learning method outperforms rule-based machine learning-based models ability capture sequential semantic information texts . We propose classifier using CNN , BiLSTM classify technical texts computer science domain. Furthermore, sequentially adding networks, remarkable accuracy several shared classification tasks obtained. The rest paper organized follows: related work given section 2. Section 3 describes dataset. The framework described section 4. The findings presented section 5. %%%%%%%%%%%% Related Work %%%%%%%%% We proposed exploit robust representations multiple levels granularity syntactic semantic levels turn incorporated representations end-to-end encoder-decoder neural architecture resourceful discourse processing. Our document-level discourse parser compares favorably current state-of-the-art. Experimental results show document-based neural discourse parser benefits incorporating boundary information EDU level modeling global information."," This paper illustrates the details description of technical text classification system and its results that developed as a part of participation in the shared task TechDofication 2020. The shared task consists of two sub-tasks:  first task identify the coarse-grained technical domain of given text in a specified language and  the second task classify a text of computer science domain into fine-grained sub-domains. A classification system  is developed to perform the classification task using three techniques: convolution neural network , bidirectional long short term memory  network, and combined CNN with BiLSTM. Results show that CNN with BiLSTM model outperforms the other techniques concerning task-1 of sub-tasks  and task-2a. This combined model obtained $f_1$ scores of 82.63 , 81.95 , 82.39 , 84.37 , and 67.44  on the development dataset. Moreover, in the case of test set, the combined CNN with BiLSTM approach achieved that higher accuracy for the subtasks 1a , 1b , 1c , 1g  and 2a ."
"The traditional task-oriented dialogue systems, focuses providing information performing actions given databases APIs, often meet limitation DB/API cover enough necessary cases. A good enhance achieved lots relevant domain knowledge form descriptions, FAQs customer reviews, call unstructured knowledge. Track 1 9th Dialogue System Technology Challenges , Beyond Domain APIs: Task-oriented Conversational Modeling Unstructured Knowledge Access, aims generating response based dialogue history unstructured knowledge access. The whole task divided three subtasks, knowledge-seeking turn detection, knowledge selection knowledge-grounded response. Test set track includes seen unseen parts. The unseen test set collected different domains, entities, locales, aiming evaluate models' generalization ability. Knowledge-seeking turn detection, first subtask, needs determine whether related knowledge contained unstructured knowledge base. In words, subtask modeled binary classification problem. If model predicts exists related knowledge, subtask 2 search relevant knowledge snippets pass generation process . If model predicts related knowledge specific question, remaining two subtasks performed. In paper, first conduct entity matching question add domain label matching results end dialogue history model input. Knowledge selection retrieve relevant knowledge snippets database according dialogue history provide information subsequent response generation. The dialogue history conversation human speaker machine. Close end conversation, human speaker brings question certain place service . The given knowledge database consists question-answer pairs involving diverse facts organized different domains entities. % Note knowledge-seeking turn detection model determines whether dialog system needs access knowledge database generating response. % We perform knowledge selection samples requires relevant knowledge database. The retrieved knowledge snippets provide information subsequent response generation. % Information retrieval techniques widely applied search related candidates retrieval-based knowledge-grounded system. Some researchers compute traditional tf-idf score search relevant document user's query, others leverage power neural networks learn ranking score directly end-to-end learning process. Recently, due significant improvements numerous natural language processing tasks, large scale pre-trained language models also applied better model semantic relevance knowledge selection. In paper, first apply retrieval techniques narrow searching space use neural network initialized pre-trained model formulate ranking function. % We propose two base models knowledge selection, final ensemble model combines predictions different base models improve selection performance. % The Retrieve \& Rank model first gathers knowledge snippets potentially relevant entities knowledge base, ranking model trained select plausible knowledge snippets retrieved candidates. % Different Retrieve \& Rank model, Three-step model divides ranking model three cascade parts rank domain, entity documents respectively order force model take knowledge hierarchy account. % We also ensemble two models together experiments show ensemble model better performance two base model separately. % briefly introduce three-step pipeline model. Knowledge-grounded response generation requests give response automatically model using dialogue history unstructured knowledge input. There two different types dialogue systems, retrieval-based system, generation-based system. Retrieval-based dialogue system, giving responses list candidate sentences, fixed answer forms candidate sets. To deal problem, needs flexible natural responses, generation-based model better choice. Dialogue generation requires encoder represent input decoder generate response. The network often needs minimize cross-entropy loss output ground truth. In paper, use latent variable encode dialog history selected knowledge better generate responses combined copy mechanism. % Pre-trained language models make great progress dialogue generation. Note bi-directional model designed dialogue generation task, thus PLATO PLATO-2 use uni- bi-directional processing pre-training. Moreover, large-scale Reddit Twitter conversations utilized pre-train generation model reduce data distribution gaps. Furthermore, latent variable used capture one-to-many relations post-response pairs. As shown released evaluation results, proposed system ranks second objective metrics ranks fourth human metrics. In following sections, explain details proposed model. Experiment results shown next analysis conclusions. This paper presents detail description proposed system evaluation technical texts classification different languages. As baseline method, used CNN BiLSTM, compare methods proposed model . Each model trained, tuned evaluated separately subtasks 1 2. The proposed method showed better performance terms accuracy subtasks task 1 task 2a development set. However, case test set, system performed better subtasks 1a, 1b, 1c, 1g 2a. More dataset included improved performance. In future, attention mechanism may explored observe effects text classification tasks."," Task-oriented conversational modeling with unstructured knowledge access, as track 1 of the 9th Dialogue System Technology Challenges , requests to build a system to generate response given dialogue history and knowledge access. This challenge can be separated into three subtasks,  knowledge-seeking turn detection,  knowledge selection, and  knowledge-grounded response generation. We use pre-trained language models, ELECTRA and RoBERTa, as our base encoder for different subtasks. For subtask 1 and 2, the coarse-grained information like domain and entity are used to enhance knowledge usage. For subtask 3, we use a latent variable to encode dialog history and selected knowledge better and generate responses combined with copy mechanism. Meanwhile, some useful post-processing strategies are performed on the model's final output to make further knowledge usage in the generation task.  As shown in released evaluation results, our proposed system ranks second under objective metrics and ranks fourth under human metrics."
"% Recent years witnessed rapid advancement online recruitment platforms. With increasing amount online recruitment data, interview related studies emerged person-job fit automatic analysis asynchronous video interviews , aim enable automated job recommendation candidate assessment. Among studies, person-job fit casting task supervised text match problem. Given set labeled data , aims predict matching label candidate resumes job description. More recently, deep learning enhanced person-job fit methods training effective text match text representations models. AVI determine whether candidate hirable evaluating answers interview questions. In AVIs, interview usually considered sequence questions answers containing salient socials signals. To evaluate candidates comprehensively, AVI models extract features video , text, voice process answering questions. In work, focus scoring multiple QA pairs, extract features text modality define task scoring competency candidates rather score whether employed. Based anatomy human interviewers' evaluation process, solutions consist two stages: analyzing evaluating individual QA pair one one, acquiring evaluation status, grading competency candidate based evaluation status multiple QA pairs. For first stage, existing methods tend employ text matching attentional text matching algorithms evaluate QA pairs, feeds concatenated representation question answer subsequent classifier. As know, questions asynchronous video interview limited specific domains. That say, candidates answer questions according work study experience. In way, candidates' answers varied difficult evaluate answer accurately text matching. Intuitively, reasonable evaluate QA pairs semantic interaction questions answers. A critical challenge along line reveal latent relationships question answer. %Intuitively, experienced interviewers could discover semantic-level correlation interview questions candidates' answers, obtain preliminary judgement answer current question, finally give assessment based judgements several problems. Therefore, %In work, propose sentence-level reasoning GNN assess single QA pair semantic interaction level. Graph neural networks learn effective representation nodes encoding local graph structures node attributes. Due compactness model capability inductive learning, GNNs widely used modeling relational data logical reasoning. Recently, ~\citet{zhang2020efficient} proposed GNN variant, Named ExpressGNN, strike nice balance representation power simplicity model probabilistic logic reasoning.~\citet{ghosal2019dialoguegcn} constructed DialogeGCN address context propagation issues present RNN-based methods. Specifically, leverage self inter-speaker dependency interlocutors model conversational context emotion recognition. Inspired by, present sentence-level relational GCN represent internal temporal QA interaction dependency process answering questions. %Recently, graph neural network graph emebedding attracted wide attention. Graph neural networks effective tasks thought rich relational structure preserve global structure information graph graph emebedding. %In work, aim address task automatically scoring textual answer candidates semantic interaction level. %The automatic short answer scoring task estimating score short text answer written response given prompt basis whether answer satisfies rubrics prepared human advance. ASAS systems mainly constructed markedly reduce scoring cost human rater. % %ep learning proven effective long text NLP tasks. Due lack information short sentence ASAS corpus, seems good enough ASAS task. For second stage grading candidate, based representation QA pairs, exists methods prefer encoder question-answer pairs sequence directly. However, kind approaches lead insufficient interaction semantic information question answer pairs. Therefore, difficult ensure rationality explainability evaluation. To mitigate issue, first stage, present semantic-level graph attention network model interaction states QA session. %Automatic scoring answer transcriptions job interview aims evaluate multiple question-answer pairs. %To alleviate limitation previous approaches, To end, propose Hierarchical Reasoning Graph Neural Network automatic scoring answer transcriptions job interviews. Specifically, proposed sentence-level relational graph convolutional neural network used capture contextual dependency, semantic-level Reasoning graph attention network applied acquire latent interaction states. And contribution work summarized follows: This paper describes overall system evaluated Track 1 DSTC 9. Pre-trained language models, ELECTRA RoBERTa, used base encoder, task-specific components applied improve performance. In released evaluation results, rank second objective metrics rank fourth human metrics. Considering gap validation test set, worthwhile us study generalize model better way, is, transferring in-domain system out-of-domain scenario."," %Automatic scoring of answer transcripts in job interview aims to evaluate multiple question-answer pairs. The key challenge is how to conduct deep interaction on the semantic level for each question-answer pair, and give the evaluation results combined with multiple interaction states. Recent studies either use text matching approaches to evaluate each question-answer pair roughly, or employ the sequential model to deal with disordered question-answer pairs which fail to take advantages of the semantic association between questions and answers, and the logical connection between question-answer pairs. In this work, we propose a hierarchical reasoning Graph Neural Network  for the automatic assessment of multi-question answering. Specifically, we construct a sentence-level reasoning GNN to assess the single question-answer pair. Based on these graphs, we propose a document-level reasoning GNN to model the interaction states of question-answer pairs. The first module utilizes each sentence in the question and answer to establish the connection between them. The second module adopts a graph convolutional network to encoder interaction states of each pair and aggregates evidence with graph attention mechanism for predicting the final score. Empirical results on Chinese and English interview datasets show that our proposed model outperforms both sequence-based and pre-training based  benchmark models.  %We address the task of automatically scoring the answer competency of candidates based on textual features from the automatic speech recognition transcriptions. The key challenge is how to conduct deep interaction on the semantic level for each question-answer  pair, and give the evaluation results combined with multiple interaction states. Recent studies either use text matching approaches to evaluate each QA pair roughly, or employ the sequential model to deal with disordered QA pairs which fail to take advantages of the semantic association between questions and answers, and the logical connection between QA pairs. In this work, we propose a hierarchical reasoning Graph Neural Network  for the automatic assessment of question-answer pairs. Specifically, we construct a sentence-level reasoning GNN to assess the single QA pair. Based on these graphs, we propose a document-level reasoning GNN to model the interaction states of QA pairs. The first module utilizes each sentence in the question and answer to establish the connection between them. The second module adopts a graph convolutional network to encoder interaction states of each pair and aggregates evidence with graph attention mechanism for predicting the final score. Empirical results conducted on CHNAT and ENGIAT  clearly validate that our proposed model outperforms both text matching based benchmark models.  %We address the task of automatically scoring the competency of candidates based on textual features, from the automatic speech recognition  transcriptions in the video job interview. The key challenge is how to conduct deep interaction on the semantic level for each question-answer  pair, and then give the evaluation results combined with multiple interaction states. Recent studies tend to use text matching approaches to evaluate each QA pair roughly, which fails to take advantage of the semantic association between questions and answers. In this work, we propose a Hierarchical Reasoning Graph Neural Network  for the automatic assessment of question-answer pairs. Specifically, we construct a sentence-level relational graph neural network to capture the latent semantic interaction of sentences in the question or the answer. Based on these graphs, we employ a semantic-level reasoning graph attention network to model the interaction states of the current QA session. Finally, we propose a gated recurrent unit with a global fusion mechanism to aggregates evidence of temporal QA pairs for the final score. Empirical results conducted on CHNAT  clearly validate that our proposed model significantly outperforms text-matching based benchmark models. Ablation studies and experimental results with 10 random seeds also show the effectiveness and stability of our models.    We address the task of automatically scoring the competency of candidates based on textual features, from the automatic speech recognition  transcriptions in the asynchronous video job interview . The key challenge is how to construct the dependency relation between questions and answers, and conduct the semantic level interaction for each question-answer  pair. However, most of the recent studies in AVI focus on how to represent questions and answers better, but ignore the dependency information and interaction between them, which is critical for QA evaluation. In this work, we propose a Hierarchical Reasoning Graph Neural Network  for the automatic assessment of question-answer pairs. Specifically, we construct a sentence-level relational graph neural network to capture the dependency information of sentences in or between the question and the answer. Based on these graphs, we employ a semantic-level reasoning graph attention network to model the interaction states of the current QA session. Finally, we propose a gated recurrent unit encoder to represent the temporal question-answer pairs for the final prediction. Empirical results conducted on CHNAT  validate that our proposed model significantly outperforms text-matching based benchmark models. Ablation studies and experimental results with 10 random seeds also show the effectiveness and stability of our models."
"Social media unique source information. On one hand, low cost, easy access distribution speed make possible quickly share news. On hand, quality reliability social media news difficult verify . This source lot false information negative impact society. Over past year, world watching situation developing around novel coronavirus pandemic. The COVID-19 pandemic become significant newsworthy event 2020. Therefore, news related COVID-19 actively discussed social media topic generates lot misinformation. Fake news related pandemic large-scale negative social consequences, provoke huge public rumor spreading misunderstanding COVID-19 aggravate effects pandemic. Moreover, recent studies show increase symptoms anxiety depression connection pandemic. This closely related spread misinformation, fake news successful population experiencing stressful psychological situation . The popularity fake news social media rapidly increase, rebuttal always published late. In regard, evidence development tools automatic COVID-19 fake news detection plays crucial role regulation information flows. In paper, present approach Constraint@AAAI2021 Shared Task: COVID-19 Fake News Detection English attracted 433 participants CodaLab. This approach achieved weighted F1-score 98.69 test set among 166 submitted teams total. The rest paper organized follows. A brief review related work given Section 2. The definition task summarized Section 3, followed brief description data used Section 4. The proposed methods experimental settings elaborated Section 5. Section 6 contains results error analysis respectively. Section 7 conclusion. In paper, propose hierarchical reasoning graph neural network automatic scoring answer transcriptions video job interview. The ASAT task score competency candidates based several textual question-answer pairs. Unlike matching based methods frameworks, HRGNN utilize relational dependency sentences questions answers, aggregate semantic level reasoning flow different graph layers. Particularly, proposed relational graph convolutional network module constructs internal temporal dependency question-answer interaction dependency represent relations sentences question answer. And graph-based reasoning part, propose graph attention network aggregate semantic interactions question answer. Finally, apply GRU-based classifier discriminate candidate competent not. Empirical results 10 random seeds show model achieves state-of-the-art Chinese real-world dataset . We address task automatically scoring competency candidates based textual features, automatic speech recognition transcriptions video job interview. The key challenge conduct deep interaction semantic level question-answer pair, give evaluation results combined multiple interaction states. Recent studies tend use text matching approaches evaluate QA pair roughly, fails take advantage semantic association questions answers. In work, propose Hierarchical Reasoning Graph Neural Network automatic assessment question-answer pairs. Specifically, construct sentence-level reasoning graph neural network capture latent semantic interaction sentences question answer. Based graphs, employ semantic-level graph attention network model interaction states current QA session. Finally, propose gated recurrent unit global fusion mechanism aggregates evidence temporal QA pairs final score. Empirical results conducted CHNAT clearly validate proposed model significantly outperforms text-matching based benchmark models. \begin{comment}"," The COVID-19 pandemic has had a huge impact on various areas of human life. Hence, the coronavirus pandemic and its consequences are being actively discussed on social media. However, not all social media posts are truthful. Many of them spread fake news that cause panic among readers, misinform people and thus exacerbate the effect of the pandemic. In this paper, we present our results at the Constraint@AAAI2021 Shared Task: COVID-19 Fake News Detection in English. In particular, we propose our approach using the transformer-based ensemble of COVID-Twitter-BERT  models. We describe the models used, the ways of text preprocessing and adding extra data. As a result, our best model achieved the weighted F1-score of 98.69 on the test set  of this shared task that attracted 166 submitted teams in total.  \keywords{COVID-Twitter-BERT, social media, fake news, ensembling learning, coronavirus, infodemic, text classification}"
"Medical dialogue system aims converse patients inquire additional symptoms beyond self-reports make diagnosis automatically, gained increasing attention . It significant potential simplify diagnostic process relieve cost collecting information patients . Moreover, preliminary diagnosis reports generated MDS may assist doctors make diagnosis efficiently. Because considerable benefits, many researchers devote substantial efforts address critical sub-problems MDS, natural language understanding , dialogue policy learning, dialogue management, make promising progress build satisfactory MDS. Medical dialogue generation , generates responses natural language request additional symptoms make diagnosis, critical MDS rarely studied. Conventional generative dialogue models often employ neural sequence modeling cannot applied medical dialogue scenario directly absence medical knowledge. Recently, large-scale pre-training language models unsupervised corpora achieved significant success. However, fine-tuning large language models medical domain requires sufficient task-specific data learn correlations diseases symptoms. Unfortunately, depicted Fig., large portion diseases instances practice, means newly-coming diseases realistic diagnosis scenario often low-resource conditions. Therefore, highly desirable transfer diagnostic experience high-resource diseases others data scarcity. Besides, existing knowledge-grounded approaches may fail perform transfer well, learn one unified model diseases ignore specificity relationships different diseases. Finally, practice, disease-symptom relations disease may vary evolve along cases, also considered prior works. Contributions. To address challenges, first propose end-to-end dialogue system low-resource medical dialogue generation. This model integrates three components seamlessly, hierarchical context encoder, meta-knowledge graph reasoning network graph-guided response generator. Among them, context encoder encodes conversation hierarchical representations. For MGR, mainly contains parameterized meta-knowledge graph, initialized prior commonsense graph characterizes correlations among diseases symptoms. When fed context information, MGR adaptively evolve meta-knowledge graph reason disease-symptom correlations predict related symptoms patient next response determine disease. Finally, response generator generates response symptoms request guidance meta-knowledge graph. The second contribution develop novel Graph-Evolving Meta-Learning framework transfer diagnostic experience low-resource scenario. Firstly, GEML trains medical dialogue model meta-learning framework. It regards generating responses handful dialogues task learns meta-initialization dialogue model fast adapt task new disease limited dialogues. In way, learnt model initialization contains sufficient meta-knowledge\footnote{We name knowledge ``meta-knowledge"" since obtained meta-training different source diseases.} source diseases serve good model initialization quickly transfer meta-knowledge new disease. More importantly, GEML also learns good parameterized meta-knowledge graph MGR module characterize disease-symptom relationships source diseases. Concretely, meta learning framework, disease, GEML enriches meta-knowledge graph via constructing global-symptom graph online dialogue examples. In way, learnt meta-knowledge graph bridge gap commonsense medical graph real diagnostic dialogues thus fast evolved new target disease. Thanks graph evolving, dialogue model request patients underlying symptoms efficiently thus improve diagnostic accuracy. Besides, GEML also well address real-world challenge disease-symptom correlations could vary along cases, since meta-knowledge graph trainable based collected dialogue examples. Finally, construct large medical dialogue dataset, called Chunyu\footnote{Code dataset released https://github.com/ha-lins/GEML-MDG.}. It covers 15 kinds diseases 12,842 dialogue examples totally, much larger existing CMDD medical dialogue dataset. The challenging benchmark better comprehensively evaluate performance medical dialogue systems. Extensive experimental results datasets demonstrate superiority method state-of-the-arts. In work, propose simple effective approach COVID-19 fake news detection based CT-BERT ensembling learning. Our experiments confirmed BERT-based models specialized subject area successfully cope tasks perform high-quality binary classification. The experimental results showed solution achieved 98.69\ weighted F1-score test data ranked first place Constraint@-AAAI2021 shared task. For future work, experiment different training data augmentation techniques. We also apply evaluate hybrid models combining BERT-based architectures methods natural language processing . ---- Bibliography ---- BibTeX users specify bibliography style 'splncs04'. References sorted formatted correct style.","  	 	Human doctors with well-structured medical knowledge can diagnose a disease merely via a few conversations with patients about symptoms. In contrast, existing knowledge-grounded dialogue systems often require a large number of dialogue instances to learn as they fail to capture the correlations between different diseases and neglect the diagnostic experience shared among them. To address this issue, we propose a more natural and practical paradigm, i.e., low-resource medical dialogue generation, which can transfer the diagnostic experience from source diseases to target ones with a handful of data for adaptation. It is capitalized on a commonsense knowledge graph to characterize the prior disease-symptom relations.  	Besides, we develop a Graph-Evolving Meta-Learning  framework that learns to evolve the commonsense graph for reasoning disease-symptom correlations in a new disease, which effectively alleviates the needs of a large number of dialogues. More importantly, by dynamically evolving disease-symptom graphs, GEML also well addresses the real-world challenges that the disease-symptom correlations of each disease may vary or evolve along with more diagnostic cases. Extensive experiment results on the CMDD dataset and our newly-collected Chunyu dataset testify the superiority of our approach  over state-of-the-art approaches.  	Besides, our GEML can generate an enriched dialogue-sensitive knowledge graph in an online manner, which could benefit other tasks grounded on knowledge graph."
"Machine translation shown exhibit gender bias , several solutions already proposed mitigate . The general gender bias Natural Language Processing mainly attributed data . Several studies show pervasiveness stereotypes book collections , Bollywood films , among many others. As consequence, systems trained data exhibit biases. Among strategies, several studies proposed work data augmentation balance data forcing gender-balanced datasets . In parallel, initiatives focus documenting datasets prioritize transparency. However, data reason biases, recent studies show %algorithms training strategies matter. models trained robust way reduce effects data correlations . In , authors explored available mitigations increasing dropout, resulted improving models reasoned different stereotypes WinoGender examples . The purpose current paper explore Multilingual Neural Machine Translation architecture impact amount gender bias. To answer question, compare MNMT architectures trained data quantify amount gender bias standard WinoMT evaluation benchmark . Results show Language-Specific encoders-decoders exhibit less bias Shared encoder-decoder . Then, analyze visualize MNMT architecture impacts mitigating amplifying bias studying internal workings. We study amount gender information source embeddings encode, see Language-Specific surpasses Shared terms, allowing better prediction gender. Additionally, taking advantage Shared Language-Specific based Transformer , study coefficient variation attention , shows attention span narrower Shared system Language-Specific one. Therefore, context taken account smaller Shared system, causes higher gender bias. %We observe caused using Shared encoder-decoder several languages since pairwise Bilingual systems wider attention span. Given similarities sharing modules parameters across languages Bilingual Language-Specific, characteristic Bilingual systems prevails language-specific architecture. Finally, also manual analysis investigate biases linguistic explanation. %implications gender bias target language linguistic social point view. In work, build ensemble deep learning framework top several attention-based deep neural networks achieve task objective predicting categories GIF response. We effectively incorporate tweets text responses building automated systems. Our participation EmotionGIF 2020 wonderful learning experience team achieved 5\textsuperscript{th} rank rounds attained MR@6 scores 0.5292 0.5380, respectively. We look forward learn best-performing systems. Results indicate models serve strong baselines alternative framework transformer-based approaches. In future, try enrich learning developed end-to-end systems effectively incorporating multimodal features extracted GIFs training data map unlabelled test data ."," Multilingual Neural Machine Translation architectures mainly differ in the amount of sharing modules and parameters among languages. In this paper, and from an algorithmic perspective, we explore if the chosen architecture, when trained with the same data, influences the gender bias accuracy. Experiments in four language pairs show that Language-Specific encoders-decoders exhibit less bias than the Shared encoder-decoder architecture. Further interpretability analysis of source embeddings and the attention shows that, in the Language-Specific case, the embeddings encode more gender information, and its attention is more diverted. Both behaviors help in mitigating gender bias."
"Commonsense question answering recently attractive field requires systems understand common sense information beyond words, normal human beings nontrivial machines. There plenty datasets proposed purpose, instance, CommonsenseQA , CosmosQA , WIQA . Different traditional machine reading comprehension tasks SQuAD NewsQA key information answering questions directly given context paragraph, solving commonsense questions requires comprehensive understanding context relevant common knowledge, reasoning hidden logic them. There varieties knowledge bases meet need, including text corpora like Wikipedia, large-scale knowledge graphs . Recent popular solution resorts external supporting facts knowledge bases evidence, enhance question commonsense knowledge logic reasoning . However, quality supporting facts guaranteed, weak interpretability help question answering. Specifically, current methods mainly two-fold. The first group methods pre-train language models external supporting facts models could remember common knowledge, empirically proven Tandon et al. \shortcite{tandon2019wiqa} Trinh Le \shortcite{trinh2018do}. The second group methods incorporates question knowledge subgraphs paths carry information relation among concepts show multi-hop reasoning process. The structured information typically encoded via graph models GCN , merged question features. Generally, current methods handle evidence brute force, without selection refinement according interpretability supporting facts. But example shown Figure, supporting facts interpret question, regardless semantically related. Thus, need models processing evidence. In paper, introduce new recursive erasure memory network refines candidate supporting fact set. The REM-Net consists three main components: query encoder, evidence generator, novel recursive erasure memory module. Specifically, query encoder pre-trained encoder encodes question. The evidence generator pre-trained generative model produces candidate supporting facts based question. Compared retrieved supporting facts, generated facts provides new question-specific information beyond existing knowledge bases. The REM module refines candidate supporting fact set recursively matching supporting facts question feature space estimate fact's quality. This estimation helps updating question feature supporting fact set. The question feature updated residual term, whereas supporting fact set updated removing low-quality facts. Compared standard attention mechanisms allocate weights supporting facts once, multi-hop operation REM module widens gap much supporting fact contributes question answering number recursive steps features incorporated feature update. Therefore procedure leads refined use given supporting facts. We conduct experiments two commonsense QA benchmarks, WIQA CosmosQA . The experimental results demonstrate REM-Net outperforms current methods, refined supporting facts qualified questions. Our contributions mainly three-fold: This paper shows MNMT architecture impact gender accuracy. Language-Specific outperforms Shared two different language sets: English, German, Spanish, French English, German, Spanish, French, Russian. We observe difference gender accuracy higher language set including Russian. Further interpretability analysis results shows source embeddings Language-Specific architecture retain higher information gender. Moreover, architecture also keeps enough diversion attention, especially including Russian. Both elements help better inferring correct gender. Finally, manual analysis shows errors made assuming masculine occupation instead feminine one. In contrast, inverse error tends come feminine version word another meaning."," When answering a question, people often draw upon their rich world knowledge in addition to the particular context. While recent works retrieve supporting facts/evidence from commonsense knowledge bases to supply additional information to each question, there is still ample opportunity to advance it on the quality of the evidence. It is crucial since the quality of the evidence is the key to answering commonsense questions, and even determines the upper bound on the QA systems' performance. In this paper, we propose a recursive erasure memory network  to cope with the quality improvement of evidence. To address this, REM-Net is equipped with a module to refine the evidence by recursively erasing the low-quality evidence that does not explain the question answering. Besides, instead of retrieving evidence from existing knowledge bases, REM-Net leverages a pre-trained generative model to generate candidate evidence customized for the question. We conduct experiments on two commonsense question answering datasets, WIQA and CosmosQA. The results demonstrate the performance of REM-Net and show that the refined evidence is explainable."
"Neural machine translation advanced significantly recent years . In particular, Transformer model become popular well-designed architecture ability capture dependency among positions entire sequence . Early systems kind stack 4-8 layers encoder decoder sides , improvement often comes use wider networks . More recently, researchers try explore deeper models Transformer. Encouraging results appeared architecture improvements creating direct pass low-level encoder layers decoder , proper initialization strategies . Despite promising improvements, problems still remain deep NMT. Deep Transformer stacked dozens encoder layers always large number parameters, computationally expensive memory intensive. For example, 48-layer Transformer larger 6-layer system slower inference. It difficult deploy models resource-restricted devices, mobile phones. Therefore, crucial compress heavy systems light-weight ones keeping performance. Knowledge distillation promising method address issue. Although several studies attempted compress 12-layer BERT model knowledge distillation, effectively compressing extremely deep Transformer NMT systems still open question MT community. In addition, methods leverage sophisticated layer-wise distillation loss functions minimize distance teacher student models, requires huge memory consumption enormous training cost. In paper, investigate simple efficient compression strategies deep Transformer. We propose novel Transformer compression approach ) transfer knowledge extremely deep teacher model shallower student model. We disturb computation order among layer group teacher training phase, easy implement memory friendly. Moreover, enhance performance teacher network, introduce vertical ``dropout'' training randomly omitting sub-layers prevent co-adaptations over-parameterized teacher network. Although similar technique discussed \citet{fan2019reducing}'s work, believe finding complementary theirs. Both Gpkd regularization training methods well incorporated teacher training process, essential obtaining strong light-weight student model. \pgfdeclarepatternformonly{soft horizontal lines}{\pgfpointorigin}{\pgfqpoint{100pt}{1pt}}{\pgfqpoint{100pt}{3pt}}% { \pgfsetstrokeopacity{0.3} \pgfsetlinewidth{0.1pt} \pgfpathmoveto{\pgfqpoint{0pt}{0.5pt}} \pgfpathlineto{\pgfqpoint{100pt}{0.5pt}} \pgfusepath{stroke} } \pgfdeclarepatternformonly{soft crosshatch}{\pgfqpoint{-1pt}{-1pt}}{\pgfqpoint{6pt}{6pt}}{\pgfqpoint{5pt}{5pt}}% { \pgfsetstrokeopacity{0.3} \pgfsetlinewidth{0.4pt} \pgfpathmoveto{\pgfqpoint{4.5pt}{0pt}} \pgfpathlineto{\pgfqpoint{0pt}{4.5pt}} \pgfpathmoveto{\pgfqpoint{0pt}{0pt}} \pgfpathlineto{\pgfqpoint{4.5pt}{4.5pt}} \pgfusepath{stroke} } \definecolor{ugreen}{rgb}{0,0.5,0} %reoder 1 \draw[line width=1pt,draw=red!30,fill=red!20] -- -- -- -- -- -- -- ; \draw[line width=1pt,draw=blue!35,fill=blue!20] -- -- -- -- -- -- -- ; %reoder 2 \draw[line width=1pt,draw=red!30,fill=red!20] -- -- -- -- -- -- -- ; \draw[line width=1pt,draw=blue!35,fill=blue!20] -- -- -- -- -- -- -- ; %reoder 3 \draw[line width=1pt,draw=red!30,fill=red!20] -- -- -- -- -- -- -- ; \draw[line width=1pt,draw=blue!35,fill=blue!20] -- -- -- -- -- -- -- ; \node[anchor=north,inner sep=0pt] {}; \node[anchor=north,inner sep=0pt] {}; \node[anchor = south,font=; \node[anchor = south,font=; \node[anchor = south,font=; \node[anchor = south,font=\footnotesize] {}; \node[anchor = east,font=\footnotesize,rotate=-90] {}; \node[anchor = east,font=\footnotesize,rotate=-90] {}; \node[anchor = east,font=\footnotesize,rotate=-90] {}; \node[anchor = north,font=\scriptsize] {reorder}; \node[anchor = west] {}; \node[anchor = west] {}; \node[anchor = west] {}; %draw \draw[-latex',thick] -- ; \draw[-latex',thick] -- ; \draw[-latex',thick] -- ; \draw[-latex',thick] -- ; \draw[-latex',thick] -- ; \draw[-latex',thick] -- ; \draw[-latex',thick] -- ; \draw[-latex',thick] -- ; \draw[-latex',thick] -- ; \draw[-latex',thick] -- ; \draw[-latex',thick] -- ; \draw[-latex',thick] -- ; \draw[-latex',thick] -- ; \draw[-latex',thick] -- ; \draw[-latex',thick] -- ; \draw[-latex',thick] -- ; \draw[-latex',thick] -- ; \draw[-latex',thick] -- ; \draw[-latex',thick] -- ; \draw[-latex',thick] -- ; \draw[-latex',thick] -- ; \draw[-latex',thick] -- ; \draw[-latex',thick] -- ; \draw[-latex',thick] -- ; \draw[-latex',thick] -- ; \draw[-latex',thick] -- ; \draw[-latex',thick] -- ; \draw[-latex',thick] -- ; \draw[-latex',thick] -- ; \draw[-latex',thick] -- ; \draw[-latex',thick] -- ; \draw[-latex',thick] -- ; \draw[-latex',very thick,red!40] ..controls + +..; \draw[-latex',very thick,blue!40] ..controls + +..; \node[font=\tiny] {sampling}; \node [auto,anchor=west,font=\footnotesize,rotate=-90] {Teacher Training} ; \node [auto,anchor=west,font=\footnotesize,rotate=-90] {Student Training} ; \node[auto,anchor=south,font=\footnotesize,inner sep=0pt] {Generate Skd-data} ; \node[draw=gray!70,line width=1pt,fill=gray!10,single arrow,minimum height=2.2em,minimum width=4pt,single arrow head extend=3pt] {}; \node[draw=gray!70,line width=1pt,fill=gray!10,single arrow,minimum height=1.6em,minimum width=4pt,single arrow head extend=3pt,rotate=-90] {}; \end{tikzpicture} \end{figure*} We ran experiments WMT16 English-German, NIST OpenMT12 Chinese-English WMT19 Chinese-English translation tasks. The Gpkd method compressed 48-layer Transformer 6-layer system almost loss BLEU. It outperformed baseline depth + BLEU points. Through skipping sub-layer method, teacher network achieved BLEU score BLEU newstest2014 English-German task, student obtains additional improvements BLEU points. % Moreover, present deep-encoder shallow-decoder architecture achieves speedup times almost loss BLEU. In work, curated large-scale dialogue dataset, OSED, comprising 1M emotional dialogues movie subtitles. This dataset general-purpose, larger size, contains fine-grained emotion categories empathetic response intents existing emotional dialogue datasets. To facilitate annotation, developed dialogue emotion classifier capable recognizing 32 fine-grained emotions 9 empathetic response intents significant accuracy. It trained movie dialogues initially annotated using human computation extended using self-labeling, sentence similarity approaches. As future work, intend extend taxonomy empathetic response intents using new labels discovered process utilize OSED dataset develop controllable neural chatbot capable generating empathetic responses social chitchat.","     Recently, deep models have shown tremendous improvements in neural machine translation . However, systems of this kind are computationally expensive and memory intensive. In this paper, we take a natural step towards learning strong but light-weight NMT systems. We proposed a novel group-permutation based knowledge distillation approach to compressing the deep Transformer model into a shallow model. The experimental results on several benchmarks validate the effectiveness of our method. Our compressed model is $8\times$ shallower than the deep model, with almost no loss in BLEU. To further enhance the teacher model, we present a Skipping Sub-Layer method to randomly omit sub-layers to introduce perturbation into training, which achieves a BLEU score of 30.63 on English-German newstest2014. The code is publicly available at https://github.com/libeineu/GPKD."
"{S}{emantic} role labeling , also known shallow semantic parsing, conveys meaning sentence forming predicate-argument structure predicate sentence, generally described answer question ""Who whom, when?"". The relation specific predicate argument provides extra layer abstraction beyond syntactic dependencies , labels insensitive syntactic alternations also applied nominal predicates. Given sentence Figure , SRL pipeline framework consists 4 subtasks, including predicate identification , predicate disambiguation , arguments identification arguments classification . SRL core task natural language processing wide range applications neural machine translation , information extraction , question answering , emotion recognition text , document summarization etc. Semantic role labeling categorized two categories, span dependency. Both types SRL useful formal semantic representations dependency based SRL better convenience effectiveness semantic machine learning. Johansson Nugues concluded best dependency based SRL system outperforms best span based SRL system gold syntactic structure transformation. The conclusion also verified Li et al. solid empirical verification. Furthermore, since 2008, dependency based SRL studied compared span based SRL. With motivation, focus dependency based SRL, mainly popularized CoNLL-2008 CoNLL-2009 shared tasks . The traditional approaches SRL focus feature engineering struggles apprehending discriminative information neural networks proficient enough extract features automatically . Specifically, since large scale empirical verification Punyakanok et al. , syntactic information proven extremely beneficial SRL task. Later works achieve satisfactory performance SRL syntax-agnostic models creates conflict long-held belief syntax essential high-performance SRL . The study Li et al. shows empirical results neural models less importance syntax indicate potential challenge despite satisfactory performance syntax-agnostic SRL systems, reasons behind absence syntax models three-fold. First, effective incorporation syntax neural SRL models quite challenging compared traditional approaches. Second, neural SRL models may cover partial syntactic clues less. Third, syntax always complicated formalism linguistics easy encode syntax later usage. %Despite satisfactory performance syntax-agnostic SRL models, reasons behind absence syntax models two-fold. First, effective incorporation syntax information neural SRL models quite challenging. Second, unreliability syntactic parsers account risk erroneous syntactic input may lead error proliferation. This proven Li et al. strong empirical verification. They show effective method syntax incorporation high quality syntax promote SRL performance.% Our contributions work two folds. We propose method compress deep model shallower one minor performance sacrifice, outperforms method large margin. The proposed Skipping Sub-Layer method reduces overfitting problem training extremely deep encoder systems randomly omitting sub-layers training phase. The experimental results three widely-used benchmarks validate effectiveness proposed methods. After incorporating two methods, strong light-weight student models show competitive performance application friendly."," Semantic role labeling  aims at elaborating the meaning of a sentence by forming a predicate-argument structure. Recent researches depicted that the effective use of syntax can improve SRL performance. However, syntax is a complicated linguistic clue and is hard to be effectively applied in a downstream task like SRL. This work effectively encodes syntax using adaptive convolution which endows strong flexibility to existing convolutional networks. The existing CNNs may help in encoding a complicated structure like syntax for SRL, but it still has shortcomings. Contrary to traditional convolutional networks that use same filters for different inputs, adaptive convolution uses adaptively generated filters conditioned on syntactically-informed inputs. We achieve this with the integration of a filter generation network which generates the input specific filters. This helps the model to focus on important syntactic features present inside the input, thus enlarging the gap between syntax-aware and syntax-agnostic SRL systems. We further study a hashing technique to compress the size of the filter generation network for SRL in terms of trainable parameters. Experiments on CoNLL-2009 dataset confirm that the proposed model substantially outperforms most previous SRL systems for both English and Chinese languages."
"Learning dialogue policies typically formulated reinforcement learning problem . However, dialogue policy learning via RL scratch real-world dialogue scenarios expensive time-consuming, requires real users interact adjusts policies online . A plausible strategy use user simulators inexpensive alternative real users, randomly sample user goal user goal set dialogue agent training . In task-oriented dialogue settings, entire conversation revolves around sampled user goal implicitly. Nevertheless, dialogue agent's objective help user accomplish goal even though agent knows nothing sampled user goal , shown Figure. The randomly sampling-based user simulator neglects fact human learning supervision often accompanied curriculum . For instance, human-teacher teaches students, order presented examples random meaningful, students benefit . Therefore, randomly sampling-based user simulators bring two issues: Most previous studies dialogue policy focused efficiency issue, reward shaping , companion learning , incorporate planning , etc. However, stability pre-requisite method work well real-world scenarios. It because, matter effective algorithm is, unstable online leaned policy may ineffective applied real dialogue environment. This lead bad user experience thus fail attract sufficient real users continuously improve policy. As far know, little work reported stability dialogue policy. Therefore, essential address stability issue. In paper, propose novel policy learning framework combines curriculum learning deep reinforcement learning, namely Automatic Curriculum Learning-based Deep Q-Network . As shown Figure, framework replaces traditional random sampling method user simulator teacher policy model arranges meaningful ordered curriculum dynamically adjusts help dialogue agent automatic curriculum learning. As scheduling controller student agents, teacher policy model arranges students learn different user goals different learning stages without requirement prior knowledge. Sampling user goals match ability student agents regarding different difficulty user goal, increases feedback environment student agent also makes learning student agent stable. There two criteria evaluating sampling order user goal: learning progress student agent over-repetition penalty. The learning progress student agent emphasizes efficiency user goal, encouraging teacher policy model choose user goals match ability student agent maximize learning efficiency student agent. The over-repetition penalty emphasizes sampled diversity, preventing teacher policy model cheating\footnote[1]{The teacher policy model repeatedly selects user goals student agent mastered obtain positive rewards.}. The incorporation learning progress student agent over-repetition penalty reflects sampled efficiency sampled diversity improve efficiency well stability ACL-DQN. Additionally, proposed ACL-DQN framework equip different curriculum schedules. Hence, order verify generalization proposed framework, propose three curriculum schedule standards framework experimentation: i) Curriculum schedule A: standard, single teacher model; ii) Curriculum schedule B: user goals sampled easiness hardness proportion; iii) Curriculum schedule C: ensure student agents mastered simpler goals learning complex goals. Experiments demonstrated ACL-DQN significantly improves dialogue policy automatic curriculum learning achieves better stable performance DQN. Moreover, ACL-DQN equipped curriculum schedules improved. Among three curriculum schedules provided, ACL-DQN curriculum schedule C strength supervision controllability, better follow learning progress students performs best. In summary, contributions follows: This paper presents neural framework semantic role labeling, effectively incorporating filter generation network extract important syntactic features encoded BiLSTM Tree-LSTM generating filters conditioned inputs. The adaptive convolution endows flexibility existing convolution operations. With extraction important syntax information, able enlarge gap syntax aware syntax agnostic SRL systems. We study hashing technique drastically decreases size filter generation network. Lastly, explore effects syntax quality SRL systems conclude high quality syntax improve SRL performance. Experiments CoNLL-2009 dataset validate proposed model outperforms previous SRL systems English Chinese languages."," Dialogue policy learning based on reinforcement learning is difficult to be applied to real users to train dialogue agents from scratch because of the high cost. User simulators, which choose random user goals for the dialogue agent to train on, have been considered as an affordable substitute for real users. However, this random sampling method ignores the law of human learning, making the learned dialogue policy inefficient and unstable. We propose a novel framework, Automatic Curriculum Learning-based Deep Q-Network , which replaces the traditional random sampling method with a teacher policy model to realize the dialogue policy for automatic curriculum learning. The teacher model arranges a meaningful ordered curriculum and automatically adjusts it by monitoring the learning progress of the dialogue agent and the over-repetition penalty without any requirement of prior knowledge. The learning progress of the dialogue agent reflects the relationship between the dialogue agent's ability and the sampled goals' difficulty for sample efficiency. The over-repetition penalty guarantees the sampled diversity. Experiments show that the ACL-DQN significantly improves the effectiveness and stability of dialogue tasks with a statistically significant margin. Furthermore, the framework can be further improved by equipping with different curriculum schedules, which demonstrates that the framework has strong generalizability."
"Exponential growths micro-blogging sites social media provide platforms empowering freedom expressions individual voices, also enables people express anti-social behavior online harassment, cyberbullying, rumors, spreading hatred statements. %In recent years, micro-blogging sites social media sites grown exponentially, enabling users express anti-social behavior, false political religious rumor, spreading hatred activities. Besides, abusive threatening speech expresses prejudice certain group, religious, political, geopolitical, personal, gender abuse common basis race, religion, sexual orientation getting pervasive. United Nations Strategy Plan Action Hate Speech defines hate speech ``any kind communication speech, writing behaviour, attacks uses pejorative discriminatory language reference person group basis are, words, based religion, ethnicity, nationality, race, colour, descent, gender identity factor''. Bengali spoken 230 million people Bangladesh India, making one major languages world. Although, rich language lot diversity, Bengali severely low-resourced natural language processing~, due scarcity computational resources language models, labeled datasets, efficient machine learning~ methods required different NLP tasks. Similar major languages like English, use hate speech Bengali also getting rampant. This mainly due unrestricted access use social media digitalization. Some examples Bengali hate speech respective English translations shown \cref{cdec_wf3} either directed towards specific person entity generalized towards group. These examples signify severe Bengali hateful statements could be. Nevertheless, potential chance could lead serious consequences hate crimes, regardless languages, geographic locations, ethnicity. Automatic identification hate speech creating awareness among people challenging. However, manual reviewing verification vast amount online content labor-intensive also time-consuming. Nevertheless, accurate identification requires automated, robust, efficient machine learning~ methods. Compared traditional ML neural network~-based approaches, state-of-the-art~ language models becoming increasingly effective. On serious drawback: prediction made many models neither traced back input, clear output transformed certain way. This makes even efficient DNN models `black-box' methods. On hand, General Data Protection Regulation~ European Parliament enforces `right explanation', prohibits use ML automated decisions unless clear explanation logic used make decision well explained. Therefore, prediction made algorithm transparent possible order gain human trust. %Recent research efforts NLP ML communities proven useful well-resourced languages like English. %Nevertheless, accurate identification requires automated, robust, efficient machine learning~ methods. As state-of-the-art language models becoming increasingly effective, decisions made transparent possible order improve human trust. %Some techniques based model local gradient information methods seek redistribute function value input variables, typically reverse propagation neural network graph. Bach et al. proposed specific propagation rules neural networks . These rules shown produce better explanations e.g. gradient-based techniques computer vision also text data. To overcome shortcomings `black-box'-based methods inspired outstanding success transformer language models~, propose explainable approach hate speech detection under-resourced Bengali language. Our approach based ensemble several BERT variants, including monolingual Bangla BERT-base, m-BERT~, XLM-RoBERTa. Further, provide global local explanations predictions, post-hoc fashion also provide measure explanations terms faithfulness. The rest paper structured follows: \Cref{rw} reviews related work hate speech Bengali word embedding. \Cref{section:3} describes data collection annotation process. \Cref{nettwork} describes process Bengali neural embedding, network construction, training. \Cref{er} illustrates experiment results, including comparative analysis baseline models datasets. \Cref{con} summarizes research potential limitations points possible outlook concluding paper. This paper formally introduces task universal representation learning presents pre-trained language model purpose map different granular linguistic units vector space similar sequences similar representations enable unified vector operations among different language hierarchies. In detail, focus less concentrated language representation, seeking learn uniform vector form across different linguistic unit hierarchies. Far apart learning either word sentence representation, method extends BERT's masking training objective general level, leverage information sequences different lengths comprehensive way effectively learns universal representation words, phrases sentences. Overall, proposed BURT outperforms baselines wide range downstream tasks regard sequences different lengths English Chinese languages. We especially provide universal analogy task, insurance FAQ dataset NLG dataset extensive evaluation, well-trained universal representation model holds promise demonstrating accurate vector arithmetic regard words, phrases sentences real-world retrieval applications. use section* acknowledgment \ifCLASSOPTIONcompsoc The Computer Society usually uses plural form","   The exponential growths of social media and micro-blogging sites not only provide platforms for empowering freedom of expressions and individual voices, but also enables people to express anti-social behavior like online harassment, cyberbullying, and hate speech. Numerous works have been proposed to utilize the textual data for social and anti-social behavior analysis, by predicting the contexts mostly for highly-resourced languages like English. However, some languages are under-resourced, e.g., South Asian languages like Bengali, that lack  computational resources for accurate natural language processing~. In this paper, we propose an explainable approach for hate speech detection from the under-resourced Bengali language, which we called \texttt{DeepHateExplainer}. In our approach, Bengali texts are first comprehensively preprocessed, before classifying them into political, personal, geopolitical, and religious hates, by employing the neural ensemble method of different transformer-based neural architectures~. Subsequently, important~ terms are identified with sensitivity analysis and layer-wise relevance propagation~, before providing human-interpretable explanations. Finally, to measure the quality of the explanation~, we compute the comprehensiveness and sufficiency. Evaluations against machine learning~ and deep neural networks~ baselines yield F1 scores of 84\%, 90\%, 88\%, and 88\%, for political, personal, geopolitical, and religious hates, respectively, outperforming both ML and DNN baselines.%, during 3-fold cross-validation tests."
"Many seemingly convincing rumors ``Most humans use 10 percent brain'' widely spread, ordinary people able rigorously verify searching scientific literature. In fact, trivial task verify scientific claim providing supporting refuting evidence rationales, even domain experts. %Such The situation worsens misinformation proliferated %by social media news websites, manually programmatically, every moment. As result, automatic fact-verification tool becomes crucial combating %against spread misinformation. %There many existing datasets %the corresponding %systems fact-verification tasks %, emphasizing %in various domains, Wikipedia , social media , politics . These tasks %are The existing fact-verification tasks usually consist three sub-tasks: document retrieval, rationale sentence extraction, fact-verification. However, due nature scientific literature requires domain knowledge, challenging collect large scale scientific fact-verification dataset, further, perform fact-verification low-resource setting limited training data. \citet{Wadden2020FactOF} collected scientific claim-verification dataset, SciFact, proposed scientific claim-verification task: given scientific claim, find evidence sentences support refute %such claim %from corpus scientific paper abstracts. \citet{Wadden2020FactOF} also proposed simple, pipeline-based, sentence-level model, VeriSci, baseline solution based \citet{deyoung2019eraser}. %Despite simplicity VeriSci , VeriSci pipeline model runs modules abstract retrieval, rationale sentence selection, stance prediction sequentially, thus error generated %the upstream module may propagate downstream modules. To overcome drawback, hypothesize module jointly optimized multiple sub-tasks may mitigate error-propagation problem improve overall performance. %On hand, In addition, observe complete set rationale sentences usually contains multiple inter-related sentences paragraph. Therefore, propose novel, paragraph-level, multi-task learning model SciFact task. In work, employ compact paragraph encoding, novel strategy computing sentence representations using BERT-family models. We directly feed entire paragraph single sequence BERT, encoded sentence representations already contextualized neighbor sentences taking advantage attention mechanisms BERT. In addition, jointly train modules rationale selection stance prediction multi-task learning leveraging confidence score rationale selection attention weight stance prediction module. Furthermore, compare two methods transfer learning mitigate low-resource issue: pre-training domain adaptation . Our experiments show that: % compact paragraph encoding method beneficial separately computing sentence embeddings, negative sampling, joint training rationale selection stance prediction beneficial pipeline solution. %\todo{you may want create list contribution. -Violet} We present novel sentence representation learning method Conditional Masked Language Modeling training large scale unlabeled corpus. CMLM outperforms previous state-of-the-art English sentence embeddings models, including trained supervised signals. For multilingual representations learning, discover co-training CMLM bitext retrieval cross-lingual NLI finetuning achieves state-of-the-art performance. We also discover multilingual representations language bias principal component removal eliminate bias separating language identity information semantics."," Even for domain experts, it is a non-trivial task to verify a scientific claim by providing supporting or refuting evidence rationales. The situation worsens as misinformation is proliferated on social media or news websites, manually or programmatically, at every moment. As a result, an automatic fact-verification tool becomes crucial for combating the spread of misinformation. %\citet{Wadden2020FactOF} collected a scientific claim-verification dataset, SciFact, to facilitate research on scientific claim-verification.  In this work, we propose a novel, paragraph-level, multi-task learning model for the SciFact task by directly computing a sequence of contextualized sentence embeddings from a BERT model and jointly training the model on rationale selection and stance prediction."
"Self attention networks widely studied many natural language processing tasks, machine translation , language modeling natural language inference . It well accepted SANs leverage local long-term dependencies attention mechanism, highly parallelizable thanks position-independent modeling method. However, position-independent models incapable explicitly capturing boundaries sequences words, thus overlook structure information proven robust inductive biases modeling texts . Unlike RNNs model sequential structure information words using memory cells, CNNs focus learning local structure dependency words via convolution kernels, SANs learn flexible structural information indirect way almost scratch. One way integrate structural information SAN models via pre-training, BERT , learns represent sentences using unsupervised learning tasks large-scale corpus. Recent studies shown ability pre-training models capturing structure information sentences. Another method deal structural information introducing structure priors SANs mask strategies. \citeauthor{shen2018disan} \shortcite{shen2018disan} proposed directional self-attention mechanism, employs two SANs forward backward masks respectively encode temporal order information. \citeauthor{guo2019gaussian} \shortcite{guo2019gaussian} introduced Gaussian prior transformers capturing local compositionality words. Admittedly, structure priors strengthen model's capability modeling sentences meanwhile assist capturing proper dependencies. With help learned structure priors, SANs model sentences accurately even resource-constrained conditions. Though models get success many NLP tasks, studies commonly focus integrating one single type structure priors SANs, thus fail making full use multi-head attentions. One straightforward advantage using multi-head attentions lies fact different heads convey different views texts . In words, multi-head attentions enable model capture information texts multiple aspects, return brings thorough views modeling texts. Besides, well accepted one type structural prior reveal part structural information one single perspective. A variety types structural priors needed order gain complete structural information texts. This achieved introducing different structural priors different parts attention heads, different structural priors complement other, guiding SAN models learn proper dependencies words. Therefore, gain better representation texts, desirable solution make full use multi-head attention mechanism utilize multiple types structural priors. To better alleviate aforementioned problems, paper, propose lightweight self attention network, i.e., Multiple Structural Priors Guided Self Attention Network . The novel idea behind model lies usage multi-mask based multi-head attention , helps model better capture different types dependencies texts. Thanks MM-MH Attention mechanism, model capture multiple structural priors, return brings benefits modeling sentences. Especially, structural priors employed come two categories: sequential order relative position words. Since standard SANs incapable distinguishing order words, apply direction mask directly attention head. Motivated Bidirectional RNNs , split attention heads two parts. For given word, apply forward mask first half attention heads, allows attend previous words modeling reference word. Accordingly, backward mask applied rest attention heads. Since direction masks take consideration difference long-distance words nearby words, employ second category structural prior complement, could measured distance pair words. We integrate two types distance masks different attention heads. The first one utilized word distance mask, describes physical distance pair words. Besides, purpose capturing latent hierarchical structure sentences, integrate another kind distance information, i.e., dependency distance defined distance pair words dependency syntax tree. The word distance mask helps model focus local words dependency distance mask enables model capture hierarchical relationships words. Consequently, provide model ability capturing local non-local dependency words properly. To illustrate effectiveness model, conduct experiments two NLP tasks: natural language inference sentiment classification. Experimental results show MS-SAN outperforms baselines achieves competitive performance comparing state-of-the-art models. Our contributions listed follows: In work, propose novel paragraph-level multi-task learning model task. Experiments show The compact paragraph encoding method beneficial separately computing sentence embeddings. With negative sampling, joint training rationale selection stance prediction beneficial pipeline solution."," Self attention networks  have been widely utilized in recent NLP studies. Unlike CNNs or RNNs, standard SANs are usually position-independent, and thus are incapable of capturing the structural priors between sequences of words. Existing studies commonly apply one single mask strategy on SANs for incorporating structural priors while failing at modeling more abundant structural information of texts. In this paper, we aim at introducing multiple types of structural priors into SAN models, proposing the Multiple Structural Priors Guided Self Attention Network  that transforms different structural priors into different attention heads by using a novel multi-mask based multi-head attention mechanism. In particular, we integrate two categories of structural priors, including the sequential order and the relative position of words. For the purpose of capturing the latent hierarchical structure of the texts, we extract these information not only from the word contexts but also from the dependency syntax trees. Experimental results on two tasks show that MS-SAN achieves significant improvements against other strong baselines."
"Sequence-to-Sequence learning~ advanced state art various natural language processing tasks, machine translation~, text summarization~, grammatical error correction~. Seq2Seq models generally implemented encoder-decoder framework, multi-layer encoder summarizes source sequence sequence representation another multi-layer decoder produces target sequence conditioned encoded representation. Recent studies reveal fusing intermediate encoder layers beneficial Seq2Seq models, layer attention~, layer aggregation~, layer-wise coordination~. Despite effectiveness, much known fusing encoder layer representations work. The intuitive explanation fusing encoder layers exploits surface syntactic information embedded lower encoder layers~. However, studies show attending lower encoder layers improve model performance~, conflicted existing conclusions. It still unclear fusing encoder layers work Seq2Seq models. This paper tries shed light upon behavior Seq2Seq models augmented EncoderFusion method. To end, propose novel fine-grained layer attention evaluate contribution individual encoder layers. We conduct experiments several representative Seq2Seq NLP tasks, including machine translation, text summarization, grammatical error correction. Through series analyses, find uppermost decoder layer pays attention encoder embedding layer. Masking encoder embedding layer significantly drops model performance generating hallucinatory predictions. The encoded representation standard Seq2Seq models may enough capacity model semantic surface features . We call problem described source representation bottleneck. Based observation, simplify EncoderFusion approaches connecting encoder embedding layer softmax layer . The SurfaceFusion approach shortens path distance source target embeddings, help learn better bilingual embeddings direct interactions. Experimental results several Seq2Seq NLP tasks show method consistently outperforms vanilla Seq2Seq model layer attention model. Extensive analyses reveal approach produces aligned bilingual word embeddings shortening path distance them, confirm claim. Our main contributions follows: In work, propose novel hierarchical curriculum learning framework training response selection models multi-turn conversations. During training, proposed framework simultaneously employs corpus-level instance-level curriculum dynamically select suitable training data based state learning process. Extensive experiments analysis two benchmark datasets show approach significantly improve performance various strong matching models. To test approach, conduct extensive experiments analysis using three representative matching models. The results two benchmark datasets demonstrate effectiveness proposed approach. Experimental results two benchmark datasets using three representative matching models verify effectiveness proposed approach."," Encoder layer fusion  is a technique to fuse all the encoder layers  for sequence-to-sequence  models, which has proven effective on various NLP tasks. However, it is still not entirely clear why and when EncoderFusion should work. In this paper, our main contribution is to take a step further in understanding EncoderFusion. Many of previous studies believe that the success of EncoderFusion comes from exploiting surface and syntactic information embedded in lower encoder layers. Unlike them, we find that the encoder embedding layer is more important than other intermediate encoder layers.  In addition, the uppermost decoder layer consistently pays more attention to the encoder embedding layer across NLP tasks. Based on this observation, we propose a simple fusion method, SurfaceFusion, by fusing only the encoder embedding layer for the softmax layer. Experimental results show that SurfaceFusion outperforms EncoderFusion on several NLP benchmarks, including machine translation, text summarization, and grammatical error correction.   It obtains the state-of-the-art performance on WMT16 Romanian-English and WMT14 English-French translation tasks. Extensive analyses reveal that SurfaceFusion learns more expressive bilingual word embeddings by building a closer relationship between relevant source and target embeddings. Source code is freely available at \url{https://github.com/SunbowLiu/SurfaceFusion}.  \iffalse To model the inter-dependence of two sequences, sequence-to-sequence  learning extracts the source surface and abstract features through its encoder output representations. However, an overloaded use of the encoder output representations might lead to an insufficient representation capacity, which we call it source representation bottleneck. Recent studies have found that widening the bottleneck by fusing the surface features from lower level representations can boost the performance of Seq2Seq, but none of them explain the intrinsic mechanism of this benefit. In this paper, we take the first step to probe into the essence of the bottleneck on three typical Seq2Seq tasks, i.e.~machine translation, text summarization, and grammatical error correction. We observe that the representation learning of higher decoder layer suffers from the bottleneck, and thus propose a simple yet effective surface fusion method to mitigate the issue. The results over a variety of benchmarks confirm the effectiveness of the proposed method. Source code will be released. \fi"
"Indonesian colloquialism everyday everywhere, e.g. social media posts conversational transcripts. Yet, existing research Indonesian NLP models including NMTs often disregards qualitative analysis models given strictly colloquial inputs. This mainly due fact data readily available training testing models formal Indonesian. %This follow naturally due fact models style-agnostic, is, Colloquial Indonesian several different word choices formal language due diversity regional languages dialects. We define spoken colloquial clean colloquial. In addition, written media, colloquial Indonesian often abbreviated, disemvoweled, written voice alteration, define noisy colloquial . \end{table} To better evaluate English-Indonesian MT systems colloquial text, first create 2 new test-sets Indonesian-English colloquial pairs. The first test clean colloquial taken YouTube transcript. The second test-set noisy colloquial Twitter annotated team annotators. We found NMT systems trained formal dataset perform well test-sets. Next, develop synthetic colloquial text data performing word-level translation several words formal text colloquial form based word-to-word dictionary. By combining formal dataset synthesized colloquial dataset, increase NMT performance colloquial test-set 2.5 BLEU points. The word segmentation essential non-trivial task Sindhi language. The white spaces words good sign predicting word boundaries, existence space-omission space-insertion bring ambiguity segmentation process. We proposed SGNWS model, keeping view challenges related SWS, respectively. The proposed model ability learn extract subword features automatically eliminating constraints hand-craft features segmentation type prior domain-specific knowledge. paper, propose deep BiLSTM-CRF based framework subword representation learning. The novel character-level For task, construct five benchmark datasets empirically analyze proposed SGNWS chosen baselines approaches. The proposed model also surpases existing Sindhi word segmenters achieving high F-Score 98.13\ , 97.62\ developed benchmark datasets Awami-Awaz, 96.26\ Wiki-dumps, 97.37\ twitter, 97.93\ books corpus, best F-Score 98.51\ SGSEG dataset. The performance Wiki-dumps comparatively lesser due existence noise text. In paper, empirically demonstrate proposed model yields best performance SWS high efficiency robustness sequential modeling tasks great ability capture word information morphemic level prediction word boundaries. The SGNWS model effective elegant neural solution SWS, also applied sequence tagging problems.","  Neural machine translation  is typically domain-dependent and style-dependent, and it requires lots of training data. State-of-the-art NMT models often fall short in handling colloquial variations of its source language and the lack of parallel data in this regard is a challenging hurdle in systematically improving the existing models. In this work, we develop a novel colloquial Indonesian-English test-set collected from YouTube transcript and Twitter. We perform synthetic style augmentation to the source formal Indonesian language and show that it improves the baseline Id-En models  over the new test data. %Our experimental data and code are available on github.com."
"Large-scale language models greatly advanced NLP research various sub-areas, question answering, text summarization, story generation . However, generation models still suffer least three major problems applied dialogue system building, 1) generic repeated responses , 2) inconsistent statements dialogue context , 3) uncontrollable task-oblivious replies . Many previous studies attempted address problems . For instance, \citet{li2019inconsisent} penalized repetitive inconsistent behaviors unlikelihood loss open-domain chats. \citet{song2020generate} detected rewrote contradicting responses achieve consistent personality. However, methods optimize language model minimizing loss supervised learning, may lead exposure bias uninterpretable behaviors, consequently, makes harder humans regulate model. To alleviate problems, previous work explored RL-based methods dialogue system building . %For instance, integrated goal coherent reward design made first step towards .designed better generation. However, methods rely hand-crafted user simulators inherently hard build , also require meaningful rewards difficult design. To address issues, propose teach model extract policy directly data learn mistakes without use simulators. Leveraging decoding methods Nucleus Sampling , language model finetuned persuasion task able generate lexically diverse response candidates given context. %One example shown Figure. Some candidates appropriate, others repetitive inconsistent context. These good bad examples used positive negative feedback model meaningful rewards RL, help refine language model. During testing, fully utilize refined language model, use generate multiple candidates again, filter repetition inconsistency afterwards. Beyond nonrepetitive consistent, good response also needs accomplish dialogue task, case, persuade people. Therefore, ask humans demonstrate persuasion process, build response imitator imitate human demonstrations select persuasive response. The issues language models especially salient complex strategic dialogue tasks persuasion negotiation. These dialogues involve specific task goal social contents build rapport better task completion, therefore, richer complicated language structures . Furthermore, due inherent similarity task-oriented open-domain dialogues, improvements made systems would also help dialogue settings. Therefore, choose strategic donation persuasion task perform study, conduct automatic human evaluations evaluate models. This work makes multiple contributions. First, propose DialGAIL, RL-based generative algorithm refine MLE-based language models dialogue generation without use user simulators. Second, design effective practicable framework strategic dialogue systems achieves state-of-the-art performance complex persuasion task, small amount human demonstration efforts. %Such system achieves diverse, consistent fluent conversations better persuasion outcomes complex persuasion task compared MLE-based baselines. %a framework automatically detect repetitive inconsistent responses, imitate human demonstration select persuasive responses. %Furthermore, experiments show model produces diverse, consistent fluent conversations better persuasion outcomes complex persuasion task compared MLE-based baselines. Previous dialogue research mostly focused pure task-oriented dialogues pure social conversations; looking forward, becomes important pay attention strategic dialogues involves task social components. We sincerely hope work could inspire research discussions strategic dialogues community. % refine dialogue generation limited amount data? MLE fine-tuning woldn't work limited data % social content + specific end-goal --> persuasionforgood. advance research area % easily get usable lm without computational resources? % explore possibility apply GAIL dialogue generation simple way % first explore GAIL % raise attention persuasion community % small amount human demo % task-independent repetition detection strengthen Despite broad applications transformer model, struggles perform well NLP tasks training data limited. In work, propose theoretically justified optimization strategy train deeper transformer model improved generalization faster convergence speed small datasets, generally applicable different NLP tasks neural architectures. The proposed strategy applied Text-to-SQL semantic parsing, important structural prediction task achieve state art successfully training significantly deeper relational transformer models. Further analyses show increasing depth transformer model trained limited data helpful generalization complicated structural prediction tasks, instead harmful previously assumed. Such observations indicate current understanding transformer architecture still incomplete shed light directions future research."," Despite the recent success of large-scale language models on various downstream NLP tasks, the repetition and inconsistency problems still persist in dialogue response generation. Previous approaches have attempted to avoid repetition by penalizing the language model's undesirable behaviors in the loss function. However, these methods focus on token-level information and can lead to incoherent responses and uninterpretable behaviors. To alleviate these issues, we propose to apply reinforcement learning to refine an MLE-based language model without user simulators, and distill sentence-level information about repetition, inconsistency and task relevance through rewards. In addition, to better accomplish the dialogue task, the model learns from human demonstration to imitate intellectual activities such as persuasion, and selects the most persuasive responses. Experiments show that our model outperforms previous state-of-the-art dialogue models on both automatic metrics and human evaluation results on a donation persuasion task, and generates more diverse, consistent and persuasive conversations according to the user feedback.% We will release the code and data upon acceptance."
"Large-scale pre-training draw much attention community Compute Vision Natural Language Processing due strong capability generalization efficient usage large-scale data. Firstly CV, series models designed pre-trained large-scale dataset ImageNet, AlexNet , VGG ResNet , effectively improved capability image recognition numerous tasks. Recent years witnessed burst pre-training NLP, BERT , RoBERTa , XLNet BART , greatly improve capability language understanding generation. However, researches towards single-modal learning used single-modal scenarios. %which greatly restricts ability process multi-modal information. In order adapt multi-modal scenarios, series multi-modal pre-training methods proposed pre-trained corpus image-text pairs, ViLBERT , VisualBERT UNITER , greatly improve ability process multi-modal information. However, models utilize limited corpus image-text pairs cannot effectively adapted single-modal scenarios . %Moreover, size corpus image-text pairs limited, large scale single-modal data can't effectively utilized. A smarter AI system able process different modalities information effectively. There large scale data different modalities Web, mainly textual visual information. The textual knowledge visual knowledge usually enhance complement other. As example shown Figure , difficult answer question correctly visual information image. However, connect visual information textual information describes background baseball game, easy determine correct answer. Also, visual information make easier understand scene described text. The research neuroscience \citet{van2018neuronal} reveals parts human brain responsible vision learn process kinds information, including touch sound. Inspired research, propose design unified-modal architecture UNIMO process multi-scene multi-modal data input, including textual, visual vision-and-language data, shown Figure . The greatest challenge unify different modalities align unify semantic space generalizable different modalities data. Existed cross-modal pre-training methods try learn cross-modal representations based limited image-text pairs simple image-text matching masked language modeling . They learn specific representations image-text pairs, generalizable single-modal scenarios. So performance drop dramatically applied language tasks . In work, UNIMO learns visual representations textual representations similar ways, unify semantic space via cross-modal contrastive learning based large-scale corpus image collections, text corpus image-text pairs. %Our unified-modal architecture utilize large scale image collections text corpus, align visual textual information semantic space via cross-modal contrastive learning image-text pairs. %Effectively utilizing large-scale images text corpus improve capability vision textual understanding respectively. UNIMO effectively utilizes large-scale text corpus image collections learn general textual visual representations. The CMCL aligns visual representation textual representation, unifies semantic space based image-text pairs. To facilitate different levels semantic alignment vision language, propose utilize series text rewriting techniques improve diversity cross-modal information. As shown Figure , utilize back-translation generate several positive examples image-text pair. Also, enhance detail semantic alignment text image, parse caption scene graph randomly replace either objects, attributes relations caption generate various negative samples. Sentence-level retrieval replacement also utilized enhance sentence-level alignment. In way, model effectively unify different levels visual textual representations semantic space. The unified-modal architecture mainly following advantages compared previous methods: The problems repetition inconsistency still persist dialogue response generation. Large-scale language models still suffer repetition inconsistency problems applied dialogue response generations. Current large-scale language models still suffer repetition inconsistency applied dialogue response generation. Current dialogue systems suffer repetition inconsistency. The repetition inconsistency problems still persist dialogue response generation large-scale language models. Large-scale language models still suffer repetition inconsistency applied dialogue generation. To address exposure bias issue MLE, propose DialGAIL refine MLE-based language model extract policy directly data without user simulators learning mistakes. penalizing mistakes. With context, model generates multiple response candidates, repetitive inconsistent. These negative examples send feedback model via reward function reduce repetition inconsistency. Furthermore, provide human demonstration model imitate human persuasion activity select persuasive candidate. Experiments show model achieves state-of-the-art performance complex persuasion task, produces diverse, consistent, persuasive conversations small amount human efforts. Looking future, strategic dialogues task social contents become important, sincere hope work could inspire research discussion strategic dialogue tasks community. besides nonrepetitive inconsistent, good response also contributes task success. To achieve this, provide human demonstration model imitate human persuasion activities. Our experiments show model performs better baselines automatic metrics human evaluations, produces diverse persuasive conversations. \clearpage","  Existed pre-training methods either focus on single-modal tasks or multi-modal tasks, and cannot effectively adapt to each other. They can only utilize single-modal data  or limited multi-modal data . In this work, we propose a unified-modal pre-training architecture, namely UNIMO, which can effectively adapt to both single-modal and multi-modal understanding and generation tasks. Large scale of free text corpus and image collections can be utilized to improve the capability of visual and textual understanding, and cross-modal contrastive learning  is leveraged to align the textual and visual information into a unified semantic space over a corpus of image-text pairs. As the non-paired single-modal data is very rich, our model can utilize much larger scale of data to learn more generalizable representations. Moreover, the textual knowledge and visual knowledge can enhance each other in the unified semantic space. The experimental results show that UNIMO significantly improves the performance of several single-modal and multi-modal downstream tasks."
"Although 7,000 languages spoken worldwide~, several dozen enough data available support supervised speech recognition, many languages even employ writing system~. In contrast, people learn use spoken language long learn read write, suggesting linguistic annotation prerequisite speech processing systems. This line reasoning motivates research aims discover meaningful linguistic abstractions directly speech signal, intention could reduce reliance spoken language systems text transcripts. A rich body work recently emerged investigating representation learning speech using visual grounding objectives~, well word-like subword-like linguistic units made emerge within models~. So far, efforts predominantly focused inference, goal learn mapping speech waveforms semantic embedding space. Generation speech conditioned point semantic space less explored, focus work. We hypothesize generative approaches offer interesting advantages relying solely inference. For example, prior works demonstrated capability recognizing visually descriptive words, shown learn non-visual words grammar. Our experiments show aspects spoken language learned degree visually-grounded generative model speech. Specifically, introduce model capable directly generating fluent spoken audio captions images without need natural language text, either intermediate representation form supervision training . Tremendous progress made recently natural language image caption generation~ naturalistic text-to-speech synthesis ~. Combining models provides means generating spoken image descriptions, existing approaches training models reliant text training. Instead, leverage sub-word speech units discovered using self-supervised learning objective drop-in replacement text. We hypothesize using techniques, even wider variety traditionally text-based NLP models could applied speech data without need transcription automatic speech recognition systems. Because human languages utilize small, discrete phonetic inventories~, posit framework applicable language world. In experiments, demonstrate set discovered speech units function role. We find greatest success units discrete, exhibit low frame-rate, highly robust speaker environmental variability. The main contributions paper follows: 1. The first methodology fluent image-to-speech synthesis rely text. A critical aspect approach factorizing model Image-to-Unit module Unit-to-Speech module, speech units discovered self-supervised fashion. This approach enables disentanglement linguistic variability acoustic/speaker variability. 2. Extensive analysis properties required learned units replace text. While idea may seem simple straightforward, obtaining proper units trivial task. In fact, units experimented paper fail serve drop-in replacements. Moreover, demonstrate deemed good units vary significantly inference generation. 3. Demonstrating insufficiency beam search-based evaluation. We show even I2U model fails generate sensible caption beam search decoding, still produce reasonable captions sampling posterior, hinting posterior mode-based evaluation inspect limited aspects model. 4. Proposing semantic diversity-aware metric. We identify issues existing metric~ propose M-SPICE sampling-based evaluation address problems. 5. Over 600,000 spoken audio captions MSCOCO dataset. We collect 742 hours speech 2,352 people tasked reading caption loud. This dataset made publicly available support work intersection speech, language, vision. In work, propose unified-modal pre-training architecture UNIMO, leverage large-scale non-paired text corpus image collections cross-modal learning. Our model effectively adapt single-modal multi-modal understanding generation tasks. Based unified-modal architecture, textual knowledge visual knowledge enhance unified semantic space. Our UNIMO model outperforms previous methods multi-modal single-modal downstream tasks. In future, utilize larger scale image collections text corpus unified-modal learning, extend UNIMO modalities data video, audio on."," In this paper we present the first model for directly synthesizing fluent, natural-sounding spoken audio captions for images that does not require natural language text as an intermediate representation or source of supervision. Instead, we connect the image captioning module and the speech synthesis module with a set of discrete, sub-word speech units that are discovered with a self-supervised visual grounding task. We conduct experiments on the Flickr8k spoken caption dataset in addition to a novel corpus of spoken audio captions collected for the popular MSCOCO dataset, demonstrating that our generated captions also capture diverse visual semantics of the images they describe. We investigate several different intermediate speech representations, and empirically find that the representation must satisfy several important properties to serve as drop-in replacements for text."
"Knowledge distillation technique train smaller, efficient student models learning larger teacher models, usually mimicking teacher's output. In scope neural machine translation , source-side monolingual data run teacher model produce output learnt student. The absence parallel data requirements allows student model trained data choices. This research focuses exploring use monolingual datasets knowledge distillation find data used. This research focuses three aspects. The first language origin monolingual data. Student models trained additional data form source-side monolingual data. Besides that, model also trained back-translation data constructed target-side monolingual data. We show using source-side target-side data important improves performance , depending test-set's language origin. Secondly, explore source monolingual data. Some research suggests uses data teacher student. On hand, research makes use knowledge distillation NMT uses additional dataset, top dataset learnt teacher. We explore whether using seen data necessary, find student trained new unseen monolingual data performs equally one trained dataset teacher. The amount data, including synthetic ones affects model performance. Therefore, last thing explore monolingual data size. We find adding monolingual data generally better. However, varied training data based language origin much important. In paper, presented first model capable generating fluent spoken captions images without relying text, almost matches performance early text-based image captioning models. Our comprehensive experiments demonstrated learned units need robust, low framerate, encoding little none duration information drop-in replacement text. We also identified caveats mode-based evaluation proposed new metric address semantic diversity. As part work, novel dataset 600k spoken captions MSCOCO dataset introduced, make publicly available research community. Future work investigate applying proposed method additional languages, devising improved speech unit representations, jointly training speech unit model I2S model. This would offer opportunity explore new analysis-by-synthesis training objectives.","  % Smaller, lightweight Neural Machine Translation  models can be trained with interpolated knowledge distillation by learning from the output of larger NMT model. To do so, the teacher translates text from source-language to target-language, which are then combined into a dataset for student.   We explore two types of monolingual data that can be included in knowledge distillation training for neural machine translation . The first is the source-side monolingual data. Second, is the target-side monolingual data that is used as back-translation data. Both datasets are translated by a teacher model from source-language to target-language, which are then combined into a dataset for smaller student models.  We find that source-side monolingual data improves model performance when evaluated by test-set originated from source-side. Likewise, target-side data has a positive effect on the test-set in the opposite direction. We also show that it is not required to train the student model with the same data used by the teacher, as long as the domains are the same. Finally, we find that combining source-side and target-side yields in better performance than relying on just one side of the monolingual data."
"% Background: % What MT, history MT, current state MT % What NMT, current state NMT % Reason: % Sufficient necessity condition writing article % Organization article Machine Translation important task aims translate natural language sentences using computers. The early approach machine translation relies heavily hand-crafted translation rules linguistic knowledge. As natural languages inherently complex, difficult cover language irregularities manual translation rules. With availability large-scale parallel corpora, data-driven approaches learn linguistic information data gained increasing attention. Unlike rule-based machine translation, Statistical Machine Translation learns latent structures word alignments phrases directly parallel corpora. Incapable modeling long-distance dependencies words, translation quality SMT far satisfactory. With breakthrough deep learning, Neural Machine Translation emerged new paradigm quickly replaced SMT mainstream approach MT. Neural machine translation radical departure previous machine translation approaches. On one hand, NMT employs continuous representations instead discrete symbolic representations SMT. On hand, NMT uses single large neural network model entire translation process, freeing need excessive feature engineering. The training NMT end-to-end opposed separately tuned components SMT. Besides simplicity, NMT achieved state-of-the-art performance various language pairs. In practice, NMT also becomes key technology behind many commercial MT systems. As neural machine translation attracts much research interest grows area many research directions, believe necessary conduct comprehensive review NMT. In work, give overview key ideas innovations behind NMT. We also summarize resources tools useful easily accessible. We hope tracing origins evolution NMT, stand shoulder past studies, gain insights future NMT. The remainder article organized follows: Section review methods NMT. We first introduce basics NMT, selectively describe recent progress NMT. We focus methods related architectures, decoding, data augmentation. Section summarize resources parallel monolingual corpora publicly available researchers. Section describe tools useful training evaluating NMT models. Finally, conclude discuss future directions Section. In paper, proposed benchmark Continual Learning task-oriented dialogue systems, 37 tasks learned continuously four settings Intent recognition, Dialogue State Tracking, Natural Language Generation, end-to-end. Then, implemented three different Continual Learning methodologies regularization, rehearsal architectural. In latter, propose simple yet effective methods based residual adapters uses entropy-based classifier select adapter use testing time. Finally, analyse trade-off performance, number-of-parameters, episodic memories size evaluated baselines, unveiling no-free lunch among methods."," Machine translation  is an important sub-field of natural language processing that aims to translate natural languages using computers. In recent years, end-to-end neural machine translation  has achieved great success and has become the new mainstream method in practical MT systems. In this article, we first provide a broad review of the methods for NMT and focus on methods relating to architectures, decoding, and data augmentation. Then we summarize the resources and tools that are useful for researchers. Finally, we conclude with a discussion of possible future research directions. %Machine translation  is an important sub-field of natural language processing which aims to translate natural language sentences between different languages using computers. Recent years has witnessed the great success of end-to-end neural machine translation  models, which has dominated the mainstream approach in commercial machine translation systems. In this work, we first provide a broad review of the methods and challenges in NMT. We introduce three basic components in NMT methods, namely modeling, inference, and learning. The modeling part starts with the encoder-decoder framework and the celebrated attention mechanism, which is followed by Recurrent Neural Networks , Convolutional Neural Networks , and Self-Attention Networks  as potential instances in an NMT architecture. The inference part focuses on the generation of translation sentences from NMT models, which consists of autoregressive,  non-autoregressive, and bidirectional decoding methods. The learning part concentrates on the methods that enhances the expressive capacity of NMT models to learn from data. We highlight the design of training objectives and the use of monolingual data in this part. In addition to the three basic parts, we highlight some of the most significant challenges in NMT, including open vocabulary, prior knowledge integration, as well as the interpretability and robustness issues. Then we summarize useful resources and tools for MT research and maintainance. Finally, we conclude with a discussion of promising future research directions."
"NMT task transforming source sequence new form particular target language using deep neural networks. Such networks commonly encoder-decoder architecture , encoder maps given input sequence intermediate representation decoder uses representation generate candidate translations. Both encoder decoder neural networks trained jointly. Due sequential nature NMT task, early models usually relied recurrent architectures , benefited sliding feature convolutional kernels encode/decode variable-length sequences . Recently, Transformers shown promising results NMT become new standard field. They follow concept encoding decoding relatively different fashion. A Transformer fundamentally feed-forward model unique neural components alter traditional translation pipeline accordingly. Therefore, expected model behaves differently recurrent convolutional counterparts. Our goal research study aspect presence noise. NMT engines trained clean samples provide high-quality results tested similarly clean texts, break easily noise appears input . They designed handle noise default Transformers exception. Many previous works focused issue studied different architectures . In work, particularly focus Transformers\footnote{We assume reader already familiar Transformer architecture.} relatively new extent understudied. A common approach make NMT models immune noise fine-tuning , noisy version input tokens intentionally introduced training decoder forced generate correct translations despite deformed inputs. FT quite useful almost situations needs run optimal setting effective. In experiments, propose slightly different learning-rate scheduler improve FT. We also define new extension modifies input words also adds complementary tokens target side. We refer extension Target Augmented Fine-Tuning , first contribution paper. In study, realized data augmentation techniques might sufficient enough cases need compatible training process neural architecture deal noise. Therefore, propose Controlled Denoising whereby noise added source sequences training encoder supposed fix noisy words feeding decoder. This approach implemented via auxiliary loss function similar adversarial training. CD second contribution. CD takes care noise encoder side, propose Dual-Channel Decoding strategy study happens decoder also informed input noise. DCD supports multi-tasking -channel decoder samples target tokens corrects noisy input words simultaneously. This form fusing translation knowledge noise-related information led interesting results experiments. DCD third last contribution work. The remainder paper organised follows: First, review previously reported solutions problem noise NMT Section , present details methods intuition behind Section . To validate methods, report experimental results Section . Finally, conclude paper discuss possible future directions Section . ~ Neural machine translation become dominant approach machine translation research practice. This article reviewed widely used methods NMT, including modeling, decoding, data augmentation, interpretation, well evaluation. We summarize resources tools useful NMT research. Despite great success achieved NMT, still many problems explored. We list important challenging problems NMT follows:"," Transformers \cite{transformer} have brought a remarkable improvement in the performance of neural machine translation  systems, but they could be surprisingly vulnerable to noise. Accordingly, we tried to investigate how noise breaks Transformers and if there exist solutions to deal with such issues. There is a large body of work in the NMT literature on analyzing the behaviour of conventional models for the problem of noise but it seems Transformers are understudied in this context.  Therefore, we introduce a novel data-driven technique to incorporate noise during training. This idea is comparable to the well-known fine-tuning strategy. Moreover, we propose two new extensions to the original Transformer, that modify the neural architecture as well as the training process to handle noise. We evaluated our techniques to translate the English--German pair in both directions. Experimental results show that our models have a higher tolerance to noise. More specifically, they perform with no deterioration where up to $10$\% of entire test words are infected by noise."
"Cross-lingual word embeddings represent words two languages shared space, semantically similar words different languages close other. Early work focused jointly learning CLWEs two languages, relying strong cross-lingual supervision form parallel corpora bilingual dictionaries . However, approaches later superseded offline mapping methods, separately train word embeddings different languages align unsupervised manner self-learning adversarial training . Despite advantage requiring parallel resources, mapping methods critically rely underlying embeddings similar structure, known isometry assumption. Several authors observed assumption generally hold, severely hindering performance methods . In later work, \citet{ormazabal-etal-2019-analyzing} showed issue arises trying align separately trained embeddings, joint learning methods susceptible it. In paper, propose alternative approach limitation, still work without parallel resources. The core idea method fix target language embeddings, learn aligned embeddings source language scratch. This prevents structural mismatches result independently training embeddings different languages, learning source embeddings tailored particular set target embeddings. For purpose, use extension skip-gram leverages translated context words anchor points. So translate context words, start weak initial dictionary, iteratively improved self-learning, incorporate restarting procedure make method robust. Thanks this, approach effectively work without human-crafted bilingual resources, relying simple heuristics existing unsupervised mapping method build initial dictionary. Our experiments confirm effectiveness approach, outperforming previous mapping methods bilingual dictionary induction obtaining competitive results zero-shot cross-lingual transfer learning XNLI. In paper, studied problem noise context NMT particularly focused Transformers. We proposed three novel techniques augment data change training procedure well neural architecture. Experimental results show techniques protect NMT engines noise. Our models affect training phase add overhead terms space and/or time complexities inference time. Findings research summarized follows: In research, ran extensive number experiments order find best configuration model optimize hyper-parameters, still exist unexplored topics/areas. In future work, planning experiment language pairs different morphological grammatical structures. , e.g. would interesting see models deal language Mandarin mainly relies characters. We also interested studying noise classes. We could afford work one class selected natural noise find realistic among others, work extended noise classes. Finally, models unique Transformer NMT. We aim evaluate language processing/understanding tasks.","  Recent research on cross-lingual word embeddings has been dominated by unsupervised mapping approaches that align monolingual embeddings. Such methods critically rely on those embeddings having a similar structure, but it was recently shown that the separate training in different languages causes departures from this assumption. In this paper, we propose an alternative approach that does not have this limitation, while requiring a weak seed dictionary  as the only form of supervision. Rather than aligning two fixed embedding spaces, our method works by fixing the target language embeddings, and learning a new set of embeddings for the source language that are aligned with them. To that end, we use an extension of skip-gram that leverages translated context words as anchor points, and incorporates self-learning and iterative restarts to reduce the dependency on the initial dictionary. Our approach outperforms conventional mapping methods on bilingual lexicon induction, and obtains competitive results in the downstream XNLI task."
"Supervised semi-supervised Machine Learning algorithms ubiquitous analysis social media data. At core algorithms ability make sense vast amount semi-structured real-time data streams, allowing automatically categorize filter new data examples into, usually pre-defined, classes. Multi-class text classification successfully used public health surveillance, election monitoring, vaccine stance prediction~\parencite{salathe2011assessing,bermingham2011using,brownstein2009digital}. In recent years algorithms also developed mitigate negative effects social media, detection cyber-bullying, hate speech, misinformation, automated accounts ~\parencite{reynolds2011using,davidson2017automated,shu2017fake,davis2016botornot}. The microblogging service Twitter played central role efforts, serves public medium provides easy access real-time data public APIs, making primary focus work. Twitter well described classical example non-stationary system frequently emerging disappearing topical clusters~\parencite{costa2014concept}. This poses problems aforementioned applications, underlying data distribution different training time time algorithm's application real world. This phenomenon known concept drift~\parencite{schlimmer1986incremental} lead change performance algorithm time. It important distinguish concept drift reasons performance differences training testing, random noise due sampling biases differences data preprocessing~\parencite{vzliobaite2010learning,webb2016characterizing}. A classic example concept drift change meaning classes, requires update learned class decision boundaries classifier. This sometimes also referred real concept drift. Often, however, observed performance change consequence change underlying data distribution, leading known virtual drift~\parencite{widmer1996learning,tsymbal2004problem}. Virtual drift overcome supplemental learning, i.e.\ collecting training data new environment. A good example periodic seasonality effects, may fully represented initial training data become fully visible time. However, practice usually difficult disentangle virtual real concept drift, consequence treated effect~\parencite{vzliobaite2010learning}. On Twitter concept drift might appear different time scales different rates. Sudden shifts debate might triggered quickly evolving news cycle catastrophic event. Concept drift may also slow process way topic discussed gradually changes time. A substantial amount work dedicated detecting overcoming concept drift~\parencite{widmer1996learning,vzliobaite2010learning,elwell2011incremental}. Three basic re-training procedures overcoming concept drift proposed: time-window approach, incremental model, ensemble model~\parencite{costa2014concept}. In time-window approach, sliding window recent training examples used train algorithm. In approach, algorithm ignores training data collected outside time window. The incremental model, contrast, uses previously collected training examples re-train model. Lastly, ensemble model trains model time window uses consensus previous models future predictions. As found in~\parencite{costa2014concept}, case hashtag prediction Twitter data, incremental method gave best results. Although sophisticated methods proposed estimate concept drift unsupervised way~\parencite{katakis2010tracking,yang2008conceptual}, practice, certain amount re-annotation detection re-training models seems unavoidable. The decision newly collected data annotate points exploration-exploitation dilemma, usually addressed context active learning framework~\parencite{settles2009active}. The Crowdbreaks platform~\parencite{muller2019crowdbreaks} example framework built goal exploring optimal solutions problem order overcome concept drift. A change underlying data distribution might necessarily negative impact classifier performance. It conceivable, example, polarisation debate Twitter topic could even lead improvement classifier performance. It therefore important ask much worried concept drift: even model performance decrease, real impacts analysis interpretation might negligible. The consequences concept drift task-, environment-, model-dependent~\parencite{vzliobaite2016overview}. Here, address concept drift specific case vaccine stance classification. Vaccine stance classification Twitter data widely studied shown promising links vaccination decision making vaccine uptake rates different countries~\parencite{salathe2011assessing,bello2017detecting}. The COVID-19 pandemic emphasizes importance, evolving concerns vaccines may significantly influence effect~\parencite{johnson2020online,burki2020online}. To best knowledge, one study directly addressed concept drift vaccine stance classification. In study~\parencite{d2019monitoring} tweets posted September 2016 January 2017 Italian language, authors find substantial improvement model incremental re-training specific events. Re-training performed 60 newly annotated tweets seven manually selected events. The authors conclude either original algorithm already quite robust towards concept change, newly collected training data small see effect. Here, use FastText~\parencite{joulin2016bag} BERT ~\parencite{devlin2018bert}, two commonly used models social media text classification. Most work topic concept drift conducted using classical machine learning models, also FastText belongs. These types models reliant high-quality annotation data. More recently, models transformer family, BERT~\parencite{devlin2018bert}, proposed, require significantly less annotation data. In follows, examine whether two models also share different concept drift characteristics. The goal work emulate typical social media analysis study, data collected certain period time, supervised machine learning model trained subset annotated data. The model published used predict newly collected data. First, try answer whether concept drift observed, so, rate occurs. Second, investigate influence study duration amount annotation data used. Lastly, examine extent concept drift influences final analysis outcomes, case sentiment index. {\small } \clearpage","   Social media analysis has become a common approach to assess public opinion on various topics, including those about health, in near real-time.   The growing volume of social media posts has led to an increased usage of modern machine learning methods in natural language processing.   While the rapid dynamics of social media can capture underlying trends quickly, it also poses a technical problem: algorithms trained on annotated data in the past may underperform when applied to contemporary data.   This phenomenon, known as concept drift, can be particularly problematic when rapid shifts occur either in the topic of interest itself, or in the way the topic is discussed.   Here, we explore the effect of machine learning concept drift by focussing on vaccine sentiments expressed on Twitter, a topic of central importance especially during the COVID-19 pandemic.   We show that while vaccine sentiment has declined considerably during the COVID-19 pandemic in 2020, algorithms trained on pre-pandemic data would have largely missed this decline due to concept drift.   Our results suggest that social media analysis systems must address concept drift in a continuous fashion in order to avoid the risk of systematic misclassification of data, which is particularly likely during a crisis when the underlying data can change suddenly and rapidly."
"In paper, tackle problem screening finite pool documents, aim retrieve relevant documents satisfying given set predicates verified human machines . In context, document satisfy least one predicate, treated irrelevant. A predicate represents property, unit meaning, given natural language . By means predicate might interpreted variety ways text, making keywords-based search hard reach high recall keeping decent level precision . We interpret screening problem high recall problem, i.e., aim retrieve relevant documents maximizing precision. %we assume predicates candidate documents given % Since predicates interpreted variety ways, makes problem document screening challenging especially little training data. The screening finds application many domains, i) systematic literature reviews ; % -SLRs- AND papers studying older adults }; ii) database querying - items filtered in/out based predicates ; iii) hotel search - hotels retrieve based upon filters interest . Consequently, document screening instance finite pool binary classification problems , need classify finite set objects minimizing cost. % As instance problem, choose screening phase SLRs makes problem rather challenging since review different unique set predicates . Typically, authors SLR retrieve candidate pool documents executing keywords-based query database Scopus. To avoid missing papers, query tends inclusive, means returns hundreds thousands results later manually screened researchers based predefined predicates. For example, researchers might look papers describe following predicates time: 1) ""include papers study older adults 85+ years"", 2) ""include papers conducted randomized controlled trial"", 3) "" include papers behavioral intervention"". Therefore, conjunctive query three inclusive predicates. A bottleneck screening process predicate evaluation, i.e., identifying given predicates satisfied current document. For example, literature reviews, authors validate predicates, however, time-consuming, exhaustive, expensive . An effective technique solve screening problems crowdsourcing crowd solve even complex screening tasks high accuracy lower cost compared expert screening . However, achieving good performance crowd-based screening requires deep understanding design tasks model complexity , test filter workers , aggregate results classification decision, improve worker engagement . Machine learning algorithms also made impressive progress solving complex screening tasks. However, obtaining sufficiently large set training data still key bottleneck accurate ML classifiers. Active learning accelerates process minimizing size training data required train better classifiers via selecting informative instances annotation. The effectiveness AL proven many domains , work considers single-label cases multi-label AL problems far less investigated. The challenge applying AL multi-label classification problem algorithm measure unified informativeness unlabeled item across labels. The state art multi-label AL strategies follow object-wise labeling, AL algorithm first finds relevance scores pairs, aggregates scores find informativeness items . However, may ignore interaction labels . \paragraph{Original contribution.} We investigate efficiently combine crowdsourcing ML item screening. It challenging task since budget limited countless number ways spend problem. We propose multi-label AL screening specific sampling technique querying unlabelled items annotating. Our algorithm takes decision choose unlabeled data annotate crowd workers order maximize performance screening task. Unlike existing multi-label AL approaches rely global labeling, choose local labeling method, label determine relevancy item. In work, investigated effects concept drift vaccination-related Twitter data streams duration three years. Using sliding time window approach, emulate social media study data collected one year, algorithm trained, algorithm used real-time monitoring new data. While may correspond common setup social media analytics, demonstrate without taking concept drift account, quality results decay. Using vaccine-related dataset 2018--2020, demonstrate failing take concept drift account would largely missed rather dramatic decay vaccine sentiment COVID-19 pandemic 2020. We find overall, concept drift indeed occurred, led decline model performance 20\ course three years. However, decline happened ten months. Concept drift therefore affected model performance different rates throughout observation period. Furthermore, relative performance loss consistently negative reverted initial levels, even slightly that. These findings consistent various ways real virtual concept drift occur. Although BERT models yielded higher performance scores, immune issues related concept drift. On relative scale, BERT models show degree drift much less sophisticated FastText models. In order better understand reasons phenomena, investigate properties used datasets. We explain large differences initial performance models differences semantic ambiguity text, indicated low inter-annotator agreement low corpus variability. Occurrence concept drift could linked differences corpus similarity. In particular, find negative class responsible decay performance time also shows strongest signs drift. Anti-vaccine content may therefore change topics increased rate compared positive neutral content. A caveat study results based classifiers mediocre performance. Given fact negative class affected concept drift time also smallest class dataset, fair question ask whether concept drift would disappear given annotation data higher performance models. It conceivable annotation data would lead better representation training window. However, results study automated geo-location tweets show, concept drift still occur also vast amounts annotated data adaptive re-training even relatively small corpus overcome drift. Our results overlap previous study vaccination-related Twitter data, find concept drift observation period September 2016 January 2017 Italian language. The reason could time scale analysed small see effect, concept drift much smaller particular dataset. It safe assume COVID-19 pandemic led severe topical shifts vaccine debate, ultimately translated strong concept drift model performance loss. Based results, expected future crisis situations would lead similarly strong concept drift, thereby severely undermining utility social media monitoring tools take concept drift account. This especially true applications intended used exactly circumstances. Although work focused singular task vaccine stance prediction, believe results stress general importance addressing concept drift real-time social media monitoring project. Overcoming concept drift complex task, many algorithmic solutions proposed. However, order succeed practice, tightly coordinated fine-tuned framework annotation retraining models required. The Crowdbreaks platform built intention address issue provide solutions it. \section{Materials methods} This study based Twitter data collected Crowdbreaks platform. Between July 1st, 2017 October 1st, 2020 total \num{57.5}M tweets English language \num{9.9}M unique users collected using public filter stream endpoint Twitter API. The tweets matched one keywords ``vaccine'', ``vaccination'', ``vaxxer'', ``vaxxed'', ``vaccinated'', ``vaccinating'', ``vacine'', ``overvaccinate'', ``undervaccinate'', ``unvaccinated''. The data considered complete respect keywords. Human annotation subset tweets performed Crowdbreaks platform. Tweets anonymized replacing user mentions URLs placeholders. Tweets February 2nd 2018 November 11th 2020 sampled annotation contained least 3 words. Exact duplicates removed. Annotators asked question ``What attitude author tweet regarding vaccines?'' given three options ``negative'', ``neutral'', ``positive''. Annotation performed Amazon Turk and, smaller extent public users Crowdbreaks website. We yield dataset \num{44843} annotations , resulted \num{11893} three-fold annotated tweets. Tweets less two-third agreement excluded conflicts decided majority vote. In work leverage two different classifiers: FastText BERT. For models, hyperparameters first tuned full annotation data yield optimal performance fixed experiments. For FastText used 10 dimensions, 500 epochs, learning rate \num{0.01}, using 1-gram embeddings. Optimal results yielded lower casing texts, converting ASCII using tags ``user'' ``url'' anonymization. BERT models type bert-large-uncased trained \num{20} epochs, training batch size \num{32}, learning rate \num{2e-5} , recommended recent literature. FastText models trained university cluster using Crowdbreaks library\footnote{} BERT models trained using Google Cloud v3-8 TPUs library\footnote{}. For purpose predictions, text preprocessed using respective preprocessing approach. \paragraph{Data availability.} All data code found public GitHub repository . \paragraph{Author contributions.} M.M.\ collected data, designed experiments analysed data. M.M.\ M.S.\ conceptualized work wrote manuscript. \paragraph{Acknowledgments.} The authors would like acknowledge Dr.\ Per Egil Kummervold Dr.\ Burcu Tepekule valuable comments discussions. \paragraph{Competing interests.} The authors declare competing interests. \paragraph{Funding.} This work received funding Versatile Emerging infectious disease Observatory grant part European Commission Horizon 2020 framework programme . Compute resources provided Google TensorFlow Research Cloud work supported Google Cloud credits context COVID-19-related research. \printbibliography SUPPLEMENTARY \pagebreak \beginsupplement \section{Supplementary figures} \pagebreak \pagebreak \pagebreak"," In this paper, we explore how to efficiently combine crowdsourcing and machine intelligence for the problem of document screening, where we need to screen documents with a set of machine-learning filters. Specifically, we focus on building a set of machine learning classifiers that evaluate documents, and then screen them efficiently. It is a challenging task since the budget is limited and there are countless number of ways to spend the given budget on the problem. We propose a multi-label active learning screening specific sampling technique -objective-aware sampling- for querying unlabelled documents for annotating. Our algorithm takes a decision on which machine filter need more training data and how to choose unlabeled items to annotate in order to minimize the risk of overall classification errors rather than minimizing a single filter error.  We demonstrate that objective-aware sampling significantly outperforms the state of the art active learning sampling strategies. % on multi-filter classification problems."
"One hallmarks human intelligence ability generalize seamlessly across heterogeneous sensory inputs different cognitive tasks. We see objects, hear sounds, feel textures, smell odors, taste flavors learn underlying concepts present world. Much AI's existing progress multimodal learning, however, focuses primarily fixed set predefined modalities tasks consistent training testing. As result, unclear transfer knowledge models trained one modality another test time. This scenario particularly important low-resource target modalities unlabeled data scarce labeled data even harder obtain . In unimodal case, regarded meta-learning few-shot learning. In contrast, formally define cross-modal generalization setting learning paradigm train model quickly perform new tasks target modality trained different source modality. In paper, study data algorithmic challenges cross-modal generalization succeed. %Such learning paradigm particularly useful leveraging high-resource source modalities help low-resource target modalities, unlabeled data scarce labeled data even harder obtain, audio low-resource languages, real-world environments, medical images. As motivating example, Figure illustrates scenario large-scale image classification benchmarks help audio classification, less studied problem fewer large-scale benchmarks. In ambitious problem statement, key research question becomes: obtain generalization across modalities despite using separate encoders different source target modalities? The technical challenge involves aligning shared knowledge learned source image tasks target audio tasks. Our problem statement differs conventional meta-learning domain adaptation one take advantage source target modality shared encoders helps generalization representation space. In case, discrepancies modalities requires one learn new output concepts expressed new input modalities. As result, cross-modal generalization requires new ideas synchronize multimodal sources targets. What minimal extra supervision required perform alignment? In paper, formalize conditions required successful generalization show another level supervision necessary partial observability across modalities tasks. Supervision comes form cross-modal meta-alignment capture space representations similar concepts different modalities close together ensuring quick generalization new tasks . We introduce novel algorithm called \names\ leverages readily available multimodal data internet meta-alignment cross-modal generalization. Through theoretical analysis empirical ablations, study proposed algorithm strongly weakly paired multimodal data, showing cross-modal generalization possible even limited extra supervision. %How one transfer knowledge learned image classification task speech event classification? The problem cross-modal generalization brings fundamental differences regarding data expressed across different modalities . In comparison meta-learning domain adaptation, different input spaces consist extremely high-dimensional, complex, heterogeneous source target modalities. As result, unable use shortcut sharing encoders commonly seen same-modality, different domain settings allow representation space source target domains. This raises fundamental research question: obtain generalization across modalities despite using separate encoders different source target modalities? These discrepancies modalities requires one learn new output concepts expressed new input modalities. %We show existing domain adaptation, meta-learning, transfer learning approaches unable bridge gap heterogeneous paradigms input modalities output tasks different. % emphasize cant share encoders, need explicit alignment % emphasize different label space, generalize meta-learning % formulate crossmodal ml therefore propose meta alignment % first para ok. like learn different modalities. % second para. compared ml da, 1 critical issue trying crossmodal - hetero data source target. cant use shortcut encoder images different domains. need different encoders 1 each. solve this? need another level supervision help - meta alignment comes in. propose - technique address core technical challenge crossmodal ml learn different encoders. meta alignment way that, contrastive learning approach. %To account technical challenge, formalize conditions required successful generalization show another level supervision necessary partial observability across modalities tasks. This form supervision comes form cross-modal alignment capture space representations similar concepts different modalities close together ensuring quick generalization new low-resource tasks . Our analysis leads novel algorithm based contrastive learning called \names\ leverages either strongly weakly paired multimodal data abundant internet. Finally, carefully study data algorithmic requirements approach succeed theoretical analysis empirical ablations. %Very hard problem crossmodal meta-learning. What minimal amount supervision required solve hard task cross-modal meta-learning? In paper explore theory empirics %We highlight two crucial distinctions: different input spaces consist extremely high-dimensional, complex, heterogeneous source target modalities, exist different task distributions source target modalities, inherent differences label spaces transferring image audio classification tasks. These discrepancies input output spaces requires one learn new output concepts expressed new input modalities. We show existing domain adaptation, meta-learning, transfer learning approaches unable bridge gap heterogeneous paradigms input modalities output tasks different. % handle limited resource modalities task, explore cross-modal approach % note: define modality, concept, task % note: better way saying cross-modal cross-task %, allows us learn classifier transfer source target tasks. %This makes particularly suitable generalization across modalities tasks due presence unseen concepts annotations target modality. %We show space: groups similar concepts expressed across different modalities, well-clustered across concepts, generalizes well new concepts, making particularly suitable generalization across modalities tasks. %While first attempt meta-alignment uses strong pairings across source target modalities , provide extension use weak pairs modalities. Weak pairs represent coarse groupings semantic correspondence better capture many-to-many relations real-world multimodal data allow us use large banks weakly paired multimodal data available internet prepared machine learning studies video data image captioning data . %Finally, quantify trade-offs labeling data target modality versus obtaining better source-target alignment. %provide theoretical justification quantify benefits approach: {\color{red} ZIYIN TODO} \zing[ziyin: mention focus difficulty definition formalization] %instead classical generalization error target modality scales wrt sample complexity target modality, approach bounded sample complexity source modality. As result, error therefore reduced ample samples source modality well-aligned space. We present experiments three cross-modal tasks: generalizing text image, image audio, text speech. In cases, goal classify data new target modality given labeled samples. %We find \names\ accurately performs few-shot alignment concepts different modalities, thereby allowing generalization concepts source modality new concepts target modality. We perform extensive experiments compare related approaches including target modality meta-learning would expected perform well since seen thousands labeled examples target modality meta-training. Surprisingly, \names\ competitive baselines significantly outperforms cross-modal approaches. In addition, study settings target modality suffers noisy limited data, scenario particularly prevalent low-resource modalities. %While setting makes difficult directly train target modality, approach efficiently leverages cross-modal information perform well. In paper, proposed evaluated objective-aware active learning strategy designed screening classification selecting efficiently item, predicate annotating based overall classification objective. We demonstrated objective-aware sampling outperforms uncertainty random AL techniques different conditions. We aim examine screening datasets, extend study classes screening problems hybrid crowd-machine algorithms."," The natural world is abundant with concepts expressed via visual, acoustic, tactile, and linguistic modalities. Much of the existing progress in multimodal learning, however, focuses primarily on problems where the same set of modalities are present at train and test time, which makes learning in low-resource modalities particularly difficult. In this work, we propose algorithms for cross-modal generalization: a learning paradigm to train a model that can  quickly perform new tasks in a target modality  and  doing so while being trained on a different source modality. We study a key research question: how can we ensure generalization across modalities despite using separate encoders for different source and target modalities? Our solution is based on meta-alignment, a novel method to align representation spaces using strongly and weakly paired cross-modal data while ensuring quick generalization to new tasks across different modalities. We study this problem on 3 classification tasks: text to image, image to audio, and text to speech. Our results demonstrate strong performance even when the new target modality has only a few  labeled samples and in the presence of noisy labels, a scenario particularly prevalent in low-resource modalities. %Despite vast differences in these raw modalities, humans seamlessly perceive multimodal data, learn new concepts, and show extraordinary capabilities in generalizing across input modalities. %In addition, our method works particularly well when the target modality suffers from noisy or limited labels, a scenario particularly prevalent in low-resource modalities. %, sometimes outperforming within modality few-shot baselines that have seen thousands of labeled examples from that target modality during meta-training. %\zing[Ziyin: heterogeneous -> multimodal? since we are assuming there is an underlying shared space, so maybe not heterogeneous] %\zing[Ziyin: since this is the first sentence in the intro, maybe remove this?] %Similarly, truly general artificial intelligence  systems must learn to generalize across multiple input modalities and output tasks. %In this work, we define and propose algorithms for a new notion of generalization:  %, languages, and concepts. %We believe that our proposed methods could open new doors towards better generalization in multimodal AI systems."
"Cloud services become increasingly popular expected gain 331.212.6\%\ billion every year Fortune 1,000 . Amazon estimated 1004.11\%-91.58\%82.9\%76.3\% - 91.3\%$ high impacted incidents. Model ablation analysis showed ML models used provided lift final ensemble different incident types. To best knowledge, first one present deployed incident triage service cloud-scale online services. This paper makes three key contributions: This paper organized follow: Section presents background incident management system; Section provides details {\TransferAssistant}; Section shows experimental results; Section describes deployment {\TransferAssistant} Azure; Section discusses lessons learned implications implementing deploying incident triage service cloud scale; Section presents related work; Section concludes paper. In work, proposed cross-modal generalization: learning paradigm abundant source modalities used help low-resource target modalities. We showed meta-alignment using cross-modal data allow quick generalization new concepts across different modalities. Our experiments demonstrate strong performance classifying data entirely new target modality limited samples noisy labels, particularly useful generalization low-resource images, speech, languages. \iffalse","   As cloud services are growing and generating high revenues, the cost of downtime in these services is becoming significantly expensive. To reduce loss and service downtime, a critical primary step is to execute incident triage, the process of assigning a service incident to the correct responsible team, in a timely manner. An incorrect assignment risks additional incident reroutings and increases its time to mitigate by 10x. However, automated incident triage in large cloud services faces many challenges:  a highly imbalanced incident distribution from a large number of teams,    wide variety in formats of input data or data sources,     scaling to meet production-grade requirements, and     gaining engineers' trust in using machine learning recommendations.    To address these challenges, we introduce {\TransferAssistant}, an intelligent incident transfer service combining multiple machine learning techniques -- gradient boosted classifiers, clustering methods, and deep neural networks -- in an ensemble to recommend the responsible team to triage an incident. Experimental results on real incidents in Microsoft Azure show that our service achieves $82.9\%$ F1 score. For highly impacted incidents, {\TransferAssistant} achieves F1 score from $76.3\% - 91.3\%$. We have applied best practices and state-of-the-art frameworks to scale {\TransferAssistant} to handle incident routing for all cloud services. {\TransferAssistant} has been deployed in Azure since October 2017 and is used by thousands of teams daily."
"% Every day pharmaceutical companies receive numerous medical inquiries related products patients, healthcare professionals, research institutes, public authorities variety sources . % These medical inquiries may relate drug-drug-interactions, availability products, side effects pharmaceuticals, clinical trial information, product quality issues, comparison competitor products, storage conditions, dosing regimen, like. % On one hand, single medical inquiry simply question given person searching specific information related medicinal product. On hand, plurality medical inquiries different persons may provide useful insight matters related medicinal products associated medical treatments. % Examples insights could early detection product quality supply chain issues, anticipation treatment trends market events, improvement educational material standard answers/frequently asked question coverage, potential changes treatment pattern, even suggestions new possible indications investigate. % From strategic perspective, information could enable organizations make better decisions, drive organization results, broadly create benefits healthcare community. % transition paragraph - machine learning help However, obtaining high-level general insights complicated task since pharmaceutical companies receive copius amounts medical inquiries every year. Machine learning natural language processing represent promising route automatically extract insights large amounts unstructured medical text. % % % text mining general biomedical domain Natural language processing text mining techniques widely used medical domain, particular emphasis electronic health records. In particular, deep learning successfully applied medical text, overwhelming majority works supervised learning, representation learning learn specialized word vector representations . % %There little work however unsupervised learning unstructured medical text. Conversely, literature unsupervised learning medical text scarce despite bulk real-world medical text unstructured, without labels annotations. % Unsupervised learning unstructured medical text mainly limited development topic models based latent Dirichlet allocation . Examples applications medical domain clinical event identification brain cancer patients clinical reports, modeling diseases predicting clinical order patterns electronic health records, detecting cases noncompliance drug treatment patient forums. % Only recently, word embeddings unsupervised learning techniques combined analyze unstructured medical text study concept diseases, medical product reviews, extract informative sentences text summarization. % real-world corpus medical inquiries challenges In work, combine biomedical word embeddings unsupervised learning discover topics real-world medical inquiries received Bayer\texttrademark. % A real-world corpus medical inquiries presents numerous challenges. From inquirer perspective, often goal convey information requested words possible save time. This leads extensive use acronyms, sentences atypical syntactic structure, occasionally missing verb subject, inquiries comprising exclusively single noun phrase. % Moreover, since medical inquiries come different sources, common find additional information related text source; examples references internal computer systems, form frames alongside actual form content, lot numbers, email headers signatures, city names. % % mixture layman medical language The corpus contains mixture layman medical language depending inquirer either patient healthcare professional. Style content medical inquiries vary quite substantially according therapeutic areas given medicinal product belongs to. % add sentence refer text representation %as one see Fig., As already mentioned, medical inquiries short. More specifically, comprise less fifteen words vast majority cases. % Standard techniques topic modelling based LDA apply, since main assumption - document/text distribution topics - clearly hold given text short. % Approaches based pseudo-documents using auxiliary information also suitable since meaningful pseudo-document auxiliary information available medical inquiries. % Moreoever, models aim learn semantics directly corpus interest. However, recent success pretrained embeddings shows beneficial include semantics learned general corpus, thus providing semantic information difficult obtain smaller corpora. This particularly important limited data short text settings. To end, recently work aimed incorporating word embeddings probabilistic models similar LDA - contrary LDA - satisfies single topic assumption . Even though models include semantic information topic model, evident choose required hyper-parameters, example determining appropriate threshold filtering semantically related word pairs. Concurrently work, document-level embeddings hierarchical clustering combined obtain topic vectors news articles question-answer corpus. % summary Here, propose approach based specialized biomedical word embeddings unsupervised learning discover topics short, unstructured, real-world medical inquiries. This approach - schematically depicted Fig. - used discovery topics medical inquiries received Bayer\texttrademark\ Medical Information regarding oncology medicinal product Stivarga\texttrademark. In paper, put forward idea heterogeneity program ASTs, presented framework representing source code heterogeneous program graphs using ASDL grammars. By applying heterogeneous graph transformer HPG, approach significantly outperforms previous GNN models two graph-level prediction tasks source code: comment generation method naming. In future, plan evaluate approach tasks, especially node link prediction tasks. We would also extend approach programming languages propose new models suited heterogeneous program graphs."," %141 words % the motivation Millions of unsolicited medical inquiries are received by pharmaceutical companies every year.  It has been hypothesized that these inquiries represent a treasure trove of information, potentially giving insight into matters regarding medicinal products and the associated medical treatments.  % the challenge However, due to the large volume and specialized nature of the inquiries, it is difficult to perform timely, recurrent, and comprehensive analyses. % the solution Here, we propose a machine learning approach based on natural language processing and unsupervised learning to automatically discover key topics in real-world medical inquiries from customers. This approach does not require ontologies nor annotations.  % the results The discovered topics are meaningful and medically relevant, as judged by medical information specialists, thus demonstrating that unsolicited medical inquiries are a source of valuable customer insights. % the implications and outlook Our work paves the way for the machine-learning-driven analysis of medical inquiries in the pharmaceutical industry, which ultimately aims at improving patient care."
"Dynamic models text aim characterizing temporal changes patterns document generation. Most successful dynamic language models Bayesian nature, lag behind state-of-the-art deep language models terms expressibility. A natural space study temporal aspects language large review datasets found e-commerce sites. The availability millions reviewed items, business services, books movies, whose reviews recorded time scales years, opens possibility develop deep scalable models predict change taste preference users time evolves. Originally, interaction users e-commerce sites studied context collaborative filtering, goal predict user ratings, based user interaction metrics. Here aim look directly content reviews time evolves. %More KDD probably, much focus ratings recommendations %-------- %The shear size e-commerce review web sites naturally lend development data mining tools able provide users way sort relevant information. This task assigned recommender systems. Originally kick started Netflix competition, matrix factorization methods collaborative filtering, aim predicting user ratings based user interaction metrics. This rating based methods lacking unable clarify nature user preferences, particular preferences change time. In order address issue, methodologies exploit costumers reviews gaining attention. %--------- Costumer reviews provide rich natural source unstructured data leverage improve recommender system performance . Indeed, reviews effectively form recommendation. % Recently, variety deep learning solutions recommendation profit ability extract latent representations review data, encoding rich information related users items. % %Review content naturally encodes % This type data % Review content contextual nature, text arises interaction user preferences items hand. % Time represents yet another dimension context, user preference item availability change time % -- indeed, % causal temporal relations known improve performance recommender systems . % Despite fact, % recent natural language processing methodologies rating reviews lag behind incorporating temporal structure language representations. In present work exploit recurrent neural network models point processes, feed neural representations text, characterize costumer reviews. Our goal capture changes user taste item importance time, exploit changes better predict new reviews arriving, actually say. We summarize contributions follows: {} % We present related work Section introduce model Section . The baseline models used comparison paper presented Section . The experimental setup results presented Section . Finally, Section conclude discuss future work. advantages This study introduces unsupervised machine learning approach automatically discover topics medical inquiries. After initial effort preprocessing hyper-parameters determination, algorithm runs without requiring human intervention, discovering key topics medical inquiries received. Topics discovered even small number inquiries present, generally specific, thus enabling targeted, informed decisions medical experts. Being completely unsupervised, algorithm discover topics neither known expected advance, topics often valuable. This stark contrast ontology supervised based approaches, topics need defined priori , incoming text associated predefined lists topics, thus hindering discovery priori unknown topics. The machine learning approach introduced use ontologies , instead incorporates domain knowledge via specialized biomedical word embeddings. This allows readily apply topic discovery algorithm different medicinal products, without burden develop specialized ontologies product therapeutic area. Indeed, algorithm periodically analyzing medical inquiries total sixteen Bayer\texttrademark\ medicinal products, encompassing cardiology, oncology, gynecology, hematology, ophthalmology. disadvantages Our approach several limitations. First, happen small fraction inquiries associated given topic actually extraneous it, especially semantically broad topics. This - due noise present real-world dataset - soft clustering HDBSCAN algorithm must applied low probability threshold cluster assignment avoid majority inquiries considered outliers . Second, even though topic names generally quite informative, medical expert needs read actual inquiries fully grasp topic meaning, especially decision made grounds discovered topics. This however burdensome inspection limited inquiries associated given topic . Last, discovered topics judged medical experts - based expert knowledge - similar could merged single topic, considered distinct algorithm. In cases, manual topic grouping might required determine top topics inquiry volumes. Still, similar topics often appear close topic map. value despite limitations Despite limitations, study demonstrates medical inquiries contain useful information, machine learning extract information automatic way, discovering topics judged medical information specialists meaningful valuable. The hope stimulate mining medical inquiries, generally use natural language processing unsupervised learning medical industry. Interesting future directions inclusion priori expert knowledge time maintaining ability discover new previously unknown topics, grouping topics meta-topics though clustering hierarchy. \section{Methods} Since dataset comprises real-word medical inquiries, preprocessing crucial step limit amount noise corpus. acronyms The corpus contains numerous acronyms: first step thus acronym resolution, i.e. substitute given acronym extended form. A dictionary recurring acronyms compiled help medical experts. Acronym resolution performed via curated dictionary two reasons. First, data scarce noisy train reliable, custom-made word embedding learn acronym meanings corpus. Second, pretrained word embeddings typically suitable representation acronym, acronym corpus used indicate something different natural language . For example, corpus lode refer vein metal, stands lack product effect. Regular expressions used remove non-informative strings . Next. text split sentences, tokenized lemmatized using scispaCy library . We disable scispaCy parser; gives significant speed-up without affecting topic discovery outcome. Finally stopwords removed. In addition standard English stopwords, non-standard stopwords arise dataset composed medical inquiries e.g. ask, request, email, inquiry, patient, doctor, product-dependent stopwords, typically brand chemical name medicinal product inquiries refer to. It also case medical inquiry corpus single words bear value, combined longer relevant medical topic discovery. For example, word years old generally relevance, contiguous longer significant since expression simply originates medical information specialists logging age patient inquiry refers to. Another example word morning: appearing alone relevance, preceded word good loses relevance since expression good morning bear significance medical topic discovery. We compile short list stop n-grams remove corpus. To represent medical inquiries, scispaCy word embedding model en\_core\_sci\_lg-0.2.5 used. No model re-training fine-tuning performed small amount data sparsity problem; since labels available, one would need train language model noisy short text instances would likely lead model forget semantics learned scispaCy model. For token, scispaCy embedding vector retrieved; sentence representation obtained simply calculating arithmetic average vectors representing token tokens belonging given sentence. Even though overwhelming majority out-of-vocabulary words interest medical topic discovery, small subset important oov words would missed one simply use word2vec model. We thus devise strategy overcome this, described below. For product, recurring oov words automatically detected; words need included word2vec model represented vector accurately captures meaning. Training new embedding include new terms good approach given sparseness problem described above. To overcome this, combine definition mapping embedding strategy. definition mapping out-of-vocabulary words Specifically, first relevant oov terms manually mapped short definition; example, oov ReDOS mapped dose optimization study since ReDOS refers dose-optimisation phase 2 study regorafenib . definition embedding Then, using text definitions, meaningful vector representation oov words obtained embedding strategy described . This procedure two main benefits. First, require training data training effort. Second, ensures construction added word vectors compatible word representation model use. Pharmaceutical product trade names oov words particular interest medical topic discovery. Indeed, able take consideration drug trade names importance since substantial amount questions mention instance drug interactions. However, generally included scispaCy model. Thus, slightly different procedure used ensure trade names appearing medical inquiries added model, regardless belonging recurring oov words not. Luckily, international non-proprietary names drugs included. For instance, oncology product trade name Stivarga\texttrademark\ present, corresponding INN is. Thus, automatically detect drug trade names utilize scispaCy named entity recognizer scispaCy UmlsEntityLinker follows. First, NER used extract entities text; then, entity, UmlsEntityLinker performs linking Unified Medical Language System searching within knowledge base approximately 2.7 million concepts via string overlap described Ref. \onlinecite{neumann-2019}. To limit number false positive matches increase UmlsEntityLinker threshold 0.85 default 0.7. For entities successfully linked UMLS, several information regarding identified concepts returned UmlsEntityLinker: concept unique identifier , concept preferred name, concept definition, concept aliases, concept type unique identifier . In particular, latter defines semantic group linked concept belongs ; up-to-date list semantic type mappings found . A TUI value T121 indicates concept found Pharmacologic Substance. Extracting entities TUI equal T121 allows automatically identify drug trade names. Each drug trade name mapped concept preferred name; present, concept definition used; also present, drug trade name replaced phrase pharmaceutical medication drug. Once mapping performed, embedding strategy used oov words followed order obtain semantically meaningful word vector representations. The HDBSCAN algorithm starts defining mutual reachability distance based density estimation; data represented weighted graph vertices data points edges weight equal mutual reachability distance points. The minimum spanning tree built, converted hierarchy connected components via union-find data structure: starting initial cluster containing points, data subsequently split level hierarchy according distance, ultimately returning many clusters data points threshold distance approaches zero. This cluster hierarchy commonly depicted dendogram. To obtain meaningful set clusters, hierarchy needs condensed. The crucial point discern - given split - two new meaningful clusters formed splitting parent cluster, instead parent cluster simply loosing points . In HDBSCAN, decision governed minimum cluster size hyper-parameter : cluster split accepted newly formed clusters least min\_cluster\_size points. The final clusters chosen set condensed clusters means measure stability defined Ref. \onlinecite{campello-2013}. define hyperparameters The main factor defining min\_cluster\_size number inquiries given product: want obtain 100 clusters results easily analyzed medical experts. It important point min\_cluster\_size strictly specify number clusters formed, rather provides algorithm indication regarding desired granularity, outlined above. In case, min\_cluster\_size ranges 5 10 depending number inquiries. This small range variation substantially facilitate hyper-parameter search. Moreover, noticed - approximately amount inquiries min\_cluster\_size - number returned clusters increases data variety, data variety qualitatively evaluated manual inspection: products diverse inquiries HDBSCAN tends return higher number clusters, ceteris paribus. ceteris paribus means things equal We utilize leaf cluster selection method instead excess mass algorithm former known return homogeneous clusters . use soft clustering Due noise dataset, using standard HDBSCAN clustering results large portion dataset considered outliers consistently across products. To overcome this, use soft HDBSCAN clustering, returns - instead cluster assignment - probability inquiry belongs given cluster. We define probability threshold point considered outlier; points threshold, associate cluster highest probability argmax operation. This probability threshold ranges chosen approximately 10\ inquiries classified outliers. As mentioned main text, computational reasons, project via UMAP lower dimensional space clustering performed. Specifically, project 100 dimensions products less 15,000 inquiries, 20 dimensions products 15,000 inquiries. Moreover, inquiries longer 800 characters also considered outliers: text representation degrades long sentences. These inquiries gathered outlier cluster made available medical experts manual inspection. Given topic, vector representation word topic name calculated; topic name vector obtained averaging word vectors words present topic name. Topics merged similarity - evaluated cosine similarity topic name vectors - larger threshold. Threshold values range 0.8 0.95 depending medicinal product considered. The popular topic evaluation metrics topic modelling long text UCI UMass . However, UCI UMass metrics good indicators quality topics short text topic modelling due sparseness problem. In Ref. \onlinecite{quan-2015}, purity measure introduced evaluate short text topic modelling; however, requires pairs short long documents , thus applicable long document associated given medical inquiry. Indeed, evaluation short text topic modelling open research problem . An additional challenge absence labels. Performing annotations would require substantial manual effort specialized medical professionals, would limited use one main goals discover previously unknown topics new inquiries received. The absence labels precludes use metrics based purity normalized mutual information proposed Ref. \onlinecite{rosenberg-2007}, \onlinecite{huang-2013}, \onlinecite{yin-2014}. distributional semantic Ref. \onlinecite{aletras-2013} bring forward valuable idea using distributional semantic evaluate topic coherence, exploiting semantic similarity learned word2vec models. Topic coherence assessed calculating similarity among top n-words given topic: semantically similar top n-words lead higher topic coherence. If might general desirable, case discovering medical topics actually detrimental: interesting topics often characterized top n-words semantically similar. For example, medical topic top 2-words rivaroxaban glutine clearly relevant medical topic discovery standpoint. However, rivaroxaban glutine semantically similar, thus metric proposed Ref. \onlinecite{aletras-2013} would consider low coherence topic, stark contrast human expert judgment. Analogous considerations apply indirect confirmation measures Ref. \onlinecite{roeder-2015}: words emerging novel topics would rarely appeared shared context. For reason, introduce new measure topic compactness takes account semantics inquiries, require labeled data. Specifically, compute similarity inquiries belonging given topic , sum elements resulting similarity matrix, divide total number elements matrix. The topic semantic compactness topic reads cardinality topic , word vector representing inquiry , function quantifying semantic similarity inquiry , taking values 0 1 . Given chosen normalization factor , thus directly used topic quality score. The topic compactness maximum attained every sentence contains exactly words. It important point automatically takes semantics account: different semantically similar medical inquiries would still high similarity score, thus would lead high topic semantic compactness, despite inquiries using different words express similar content. add example glutine Contrary Ref. \onlinecite{aletras-2013}, topic semantic compactness introduced Eq. artificially penalize novel topics associate semantically different words appearing inquiry. To come back previous example, numerous inquiries discovered topic contain words rivaroxaban glutine, topic semantic compactness would high , regardless fact top 2-words semantically similar since similarity evaluated inquiry level . It also beneficial evaluate representative topic name topic represents. To end, calculate name saliency medical topic calculating similarity word vector representing topic name word vectors representing inquiries topic, sum similarity values, divide total number inquiries topic. This reads cardinality topic , word vector representing name topic , vector representing inquiry . This returns score quantifies representative name topic represents. As case topic semantic compactness, name saliency takes natively semantics account via Eq. . In Eq. Eq. , cosine similarity used similarity measure. \section{Competing interests} Financial support research provided Bayer AG. The authors reports patent application Topic Modelling Short Medical Inquiries submitted April 21st, 2020 . {} \section*{Author Contributions} A.Z. led thereby ideated implemented topic discovery algorithm, main author manuscript. M.S., C.B., D.R. provided valuable suggestions topic discovery algorithm. C.B., O.T., T.W. designed implemented software architecture data engineering pipeline algorithm deployment. T.W., J.V., J.L., S.K., X.M., A.M., D.R., M.S. provided in-house resources study, supervised overall project, provided domain knowledge expertise. All authors revised commented manuscript. \section*{Data availability} The data used study proprietary Bayer AG, publicly available. A.Z. thanks Robin Williams Nikki Hayward Bayer\texttrademark\ Medical Information providing expert insightful in-depth feedback results topic discovery. include bib file like this:"," Deep neural network models represent the state-of-the-art methodologies for natural language processing.  % Here we build on top of these methodologies to incorporate temporal information and model how review data changes with time. % Specifically, we use the dynamic representations of recurrent point process models, % % which encode the nonlinear relations between content and timing of the reviews received by e.g. businesses or services,  % which encode the history of how business or service reviews are received in time,  % to generate instantaneous language models with improved prediction capabilities.  % Simultaneously, our methodologies enhance the predictive power of our point process models by incorporating summarized review content representations.  % % as that encoded in recurrent point process models, and improve the predictive power of these model by incorporating the text representations.  % % Our methodologies resemble that of a hierarchical model, whereupon the temporal information is used as a  representation for the language model.  % We provide recurrent network and temporal convolution solutions for modeling the review content. % We deploy our methodologies in the context of recommender systems,  % as to enhance the expressibility of current models, % effectively characterizing the change in preference and taste of users as time evolves. Source code is available at \cite{source_code}."
"Most authentication methods commonly used today rely users setting custom passwords access accounts devices. Password-based authentications popular due ease use, ease implementation established familiarity users developers method. However studies show users tend set individual passwords predictably, favoring short strings, names, birth dates reusing passwords across sites. Since chosen passwords exhibit certain patterns structure, begs question whether possible simulate patterns generate passwords human user realistically might chosen. Password guessing active field study, recently dominated statistical analysis password leaks construction corresponding generation algorithms . These methods rely expert knowledge analysis various password leaks multiple sources generate rules algorithms efficient exploitation learned patterns. On hand, recent years major advances machine-driven text generation made, notably novel deep-learning based architectures efficient training strategies large amounts training text data. These methods purely data driven, meaning learn structure input training text, without external knowledge domain structure data. % Deep learning models recently shown remarkable performance concerning text classification text generation. Major advancements field fueled development several central directions as: In paper continue exploration data driven deep-learning text generation methods task password-guessing. While applications password guessing already show promising results, frameworks still reach surpass state-of-the-art password generation algorithms. % On hand, considering password guessing problems, popular frameworks well large body state-of-art research suggest advanced deep learning methodologies still explored. Ideally, one would attempt design efficient password-guessing models aided neural networks cutting-edge practices. Our findings contributions summarized follows: In work introduced neural dynamic language models text review data. We able leverage dynamic representations point process models language modelling tasks, augment point processes text representations. . We provide two dynamical models, well extension two different language models: recurrent temporal convolution networks. We showed approach improves performance content arrival times prediction, well opens door dynamic generative language models. Future work includes implementation attention mechanisms, well inclusion neural factorization machines aimed predicting ratings values.","     Password guessing approaches via deep learning have recently been investigated with significant breakthroughs in      their ability to generate novel, realistic password candidates.     In the present work we study a broad collection of deep learning and probabilistic based models in the light of password guessing:      attention-based deep neural networks, autoencoding mechanisms and generative adversarial networks.      We provide novel generative deep-learning models in terms of variational autoencoders exhibiting state-of-art sampling performance,     yielding additional latent-space features such as interpolations and targeted sampling.     Lastly, we perform a thorough empirical analysis in a unified controlled framework over well-known datasets .      Our results not only identify the most promising schemes driven by deep neural networks, but also illustrate the strengths of each approach in terms of generation variability and sample uniqueness."
"% 1 page % Definition importance causality knowledge. % causality knowledge, important knowledge artificial intelligence systems, proven helpful many downstream tasks, especially NLP domain. % % In work, follow ConceptNet COPA focus causal relations daily events. % However, due lack high-quality large-scale causality knowledge resource, application causality knowledge downstream tasks still limited. Humans possess basic knowledge facts understandings commonsense causality everyday life. For example, leave five minutes late, late bus; sun out, likely rain; hungry, need eat. %Causality important commonsense reasoning humans use time, Such causality knowledge shown helpful many NLP tasks. Thus, valuable teach machines understand causality. Causal relations commonsense domain typically contributory contextual. % By contributory\footnote{The two levels absolute causality conditional causality , commonly appear scientific domain rather daily life.}, mean cause neither necessary sufficient effect, strongly contributes effect. By contextual, mean causal relations make sense certain context. The contextual property causal relations important acquisition application causal knowledge. For example, people tell AI assistant ``they hungry'' meeting, basic assistant may suggest order food knowledge `being hungry' causes `eat food'. A better assistant may suggest ordering food meeting knows causal relation `being hungry' `eat food' may plausible meeting context. % \ye{I made small adaptation paragraph } % For example, person middle meeting, he/she may tell AI assistant he/she hungry, good AI assistant may suggest him/her eat food knowledge `being hungry' cause `eat food', extraordinary AI assistant may suggest ``I help order food eat meeting'' knows causal relation `being hungry' `eat food' may plausible context meeting. Without understanding contextual property causal knowledge, achieving level intelligence would challenging. To help machines better understand causality commonsense, many efforts devoted developing causality knowledge bases. For example, ConceptNet ATOMIC leverage human-annotation acquire small-scale high-quality causality knowledge. After that, people try leverage linguistic patterns acquire causality knowledge textual corpus. However, causality knowledge, especially trivial knowledge humans, rarely formally expressed documents, pure text-based approach might struggle covering causality knowledge. Besides that, none take aforementioned contextual property causal knowledge consideration, may restrict usage downstream tasks. % Causal relations commonsense domain typically contributory contextual. % By contributory\footnote{The two levels causality absolute causality conditional causality , commonly appear scientific domain rather daily life.}, mean cause neither necessary sufficient effect, strongly contributes effect. % By contextual, mean causal relations make sense certain context. % The contextual property causal relations important acquisition application causality knowledge. % For example, people tell AI assistant ``they hungry'' meeting, basic assistant may suggest order food knowledge `being hungry' causes `eat food'. A better assistant may suggest ordering food meeting knows causal relation `being hungry' `eat food' may plausible meeting context. % Without understanding contextual property causality knowledge, achieving level intelligence would challenging. % % % } % % % \end{table} % % limitation existing acquisition methods % Conventional approaches \ye{i think elaborated. maybe give example?} However, two drawbacks approaches significantly limit usage downstream tasks: % In paper, propose ground causality knowledge real world explore possibility acquiring causality knowledge visual signals . By so, three major advantages: Videos easily acquired cover rich commonsense knowledge may mentioned textual corpus; Events contained videos naturally ordered time. As discussed by, exists strong correlation temporal causal relations, thus time-consecutive images become dense causality knowledge resource; Objects visual signals act context detected causality knowledge, remedy aforementioned lack contextual property issue existing approaches. To specific, first define task mining causality knowledge time-consecutive images propose high-quality dataset . To study contextual property causal relations, pair events, provide two kinds causality annotations: one causality given certain context one causality without context. Distribution analysis case studies conducted analyze contextual property causality. An example Vis-Causal shown Figure, causal relation ``dog running'' ``blowing leaves'' makes sense context provided dog running leaves, high speed quickly-moved pow cause leaves blow around. Without context ``leaves ground'', causal relation implausible. After that, propose Vision-Contextual Causal model, effectively leverage pre-trained textual representation visual context acquire causality knowledge used baseline method future works. Experimental results demonstrate even though task still challenging, jointly leveraging visual contextual representation, proposed model better identify meaningful causal relations time-consecutive images. To summarize, contributions paper three-fold: We formally define task mining contextual causality visual signal; We present high-quality dataset Vis-Causal; We propose Vision-Contextual Causal model demonstrate possibility mining contextual causality vision signal. % Experimental results prove considering context crucial understanding causality representing visual context textual representation helpful. % Further analysis shows proposed task still challenging current models, may need consider injecting external knowledge better understand videos acquire causality knowledge. % \ye{there's real reference text part into, NLP people might think suitable ACL? maybe add models use description objects represented textual form} % % The present work illustrates various deep learning password generation techniques. Conducting thorough unified analysis discuss password-matching capabilities, variability quality sampling robustness training. On one hand, bridge extend previous methods based attention schemes, GANs Wasserstein autoencoding; hand, provide promising novel approach based Variational Autoencoders allows efficient latent space modeling sampling mechanisms. Lastly, hope work facilitate provide benchmark lines deep learning ML practitioners interested field password guessing. In terms investigation, application deep learning techniques password generation poses intriguing questions interplay classical probabilistic methods neural networks, one would ultimately hope construct efficient reliable domain-inspired password representation schemes - e.g. based carefully crafted fragmentations.","  Causality knowledge is crucial for many artificial intelligence systems. Conventional textual-based causality knowledge acquisition methods typically require laborious and expensive human annotations. As a result, their scale is often limited. Moreover, as no context is provided during the annotation, the resulting causality knowledge records  typically do not take the context into consideration. To explore a more scalable way of acquiring causality knowledge, in this paper, we jump out of the textual domain and investigate the possibility of learning contextual causality from the visual signal. Compared with pure text-based approaches, learning causality from the visual signal has the following advantages:  Causality knowledge belongs to the commonsense knowledge, which is rarely expressed in the text but rich in videos;  Most events in the video are naturally time-ordered, which provides a rich resource for us to mine causality knowledge from;  All the objects in the video can be used as context to study the contextual property of causal relations. In detail, we first propose a high-quality dataset Vis-Causal and then conduct experiments to demonstrate that with good language and visual representation models as well as enough training signals, it is possible to automatically discover meaningful causal knowledge from the videos. Further analysis also shows that the contextual property of causal relations indeed exists, taking which into consideration might be crucial if we want to use the causality knowledge in real applications, and the visual signal could serve as a good resource for learning such contextual causality. Vis-Causal and all used codes are available at: \url{https://github.com/HKUST-KnowComp/Vis_Causal}. % In detail, we first identify events from the videos, which are represented with natural sentences, and then leverage the visual signal to predict the contextual causal relations among these events.     % In this work, we mimic how human beings learn causality and explore the possibility of acquiring causality knowledge with visual signal. % To do so, we first define the task of mining contextual causality knowledge from visual signals, which aims at evaluating models' abilities to identify causal relation given certain visual context, and then employ the crowd-sourcing to annotate a high-quality dataset Vis-Causal. % On top of that, we propose a Vision-Contextual Causal  model that can utilize the images as context to better acquire causality knowledge. % Different from existing \revisehm{causality knowledge acquisition works}, \revisehm{to the best of our knowledge, }the proposed solution \revisehm{is the first one that }has the potential to preserve contextual property  of causal relations."
"% The advent deep learning techniques dramatically improved accuracy speech recognition models . Deep learning techniques first saw success replacing Gaussian Mixture Model Acoustic Model part conventional speech recognition systems Feed-Forward Deep Neural Networks , Recurrent Neural Network Long Short-Term Memory networks Convonlutional Neural Networks . In addition this, improvements noise robustness using models motivated auditory processing , data augmentation techniques , beam-forming . Thanks advances, voice assistant devices Google Home Amazon Alexa widely used home environments. Nevertheless, easy run high-performance speech recognition systems devices largely size Weighted Finite State Transducer handling lexicon language model. Fortunately, all-neural end-to-end speech recognition systems introduced need large WFST n-gram Language Model . These complete end-to-end systems started surpassing performance conventional WFST-based decoders large training dataset better choice target unit Byte Pair Encoded subword units. In paper, provide comprehensive review various components algorithms end-to-end speech recognition system. In Sec., give brief overview various neural building blocks E2E Automatic Speech Recognition model. The popular E2E ASR architectures reviewed Sec.. Additional techniques used improve performance E2E ASR models discussed Sec.. Techniques used compression quantization all-neural E2E ASR models covered Sec.. Sec. gives summary paper. % % %# Data augmentation overfitting In paper, explore possibility learning causality knowledge time-consecutive images. To so, first formally define task create high-quality dataset Vis-Causal , contains 4,000 image pairs, 23,558 event pairs, causal relation annotations two settings. On top collected dataset, propose Vision-Contextual Causal model demonstrate help strong pre-trained textual visual representations careful training, possible directly acquire contextual causality visual signals. Further analysis shows even though VCC outperform baseline methods, still perfect. As visual signal could serve important causality knowledge resource, keep exploring better acquire causal knowledge visual signal future. effectively leverage pre-trained textual representation visual context learn causality visual signals, also preserve contextual property extracted causality knowledge. Experiments analysis demonstrate importance pre-trained textual representation visual context. Experiment results show task challenging current models. Further analysis also proves observation context crucial understanding causal relations. Further analysis also suggests importance leveraging external knowledge better causal relation extraction. Both dataset code released encourage research causality acquisition.","   In this paper, we review various end-to-end automatic speech recognition   algorithms and their optimization techniques for on-device applications.   Conventional speech recognition systems comprise a large number of discrete   components such as an acoustic model, a language model, a pronunciation model,    a text-normalizer, an inverse-text normalizer, a decoder based on a Weighted Finite State   Transducer , and so on. To obtain sufficiently high speech recognition   accuracy  with such conventional speech recognition systems, a very large   language model  is usually needed. Hence, the corresponding   WFST size becomes enormous, which prohibits their on-device implementation. Recently, fully neural network end-to-end speech recognition algorithms have been   proposed. Examples include speech recognition systems based on  Connectionist Temporal Classification , Recurrent Neural Network Transducer , Attention-based Encoder-Decoder models , Monotonic   Chunk-wise Attention ,    transformer-based speech recognition systems, and so on. These fully neural   network-based systems require much smaller memory footprints compared to   conventional algorithms, therefore their on-device implementation has become   feasible. In this paper, we review such end-to-end speech recognition models.   We extensively discuss their structures, performance, and advantages compared   to conventional algorithms."
"Intuitively, see lot examples natural language questions TV shows, ought also help understand similar syntax questions movies, questions refer movies TV shows together. Ideally, training examples related domain strictly improve performance, hurt it. %[nkscales] FYI -- I reverted sentence close original form better match tone first paragraph. If sentence still sound right, let know. If satisfy property, least chance eventually achieving arbitrarily robust performance across range domains, given sufficient training data aggregate. %You need satisfy property order shot achieving arbitrarily robust performance across range domains, given simply sufficient data across domains aggregate. How extent current machine learning approaches made robustly solve natural language understanding scale arbitrary natural language across domain -- without access large quantities training data -- remains, however, open question. On one hand, research scaling behavior deep learning systems found generalization loss decrease reliably training size model size power law related logarithmic relationship across range architectures tasks, image classification convolutional neural networks~ language modeling Transformers~. Recent results i.i.d.\ setting show pattern persist across many orders magnitude, established upper limit~. At time, shown current ML systems continue struggle achieve robust performance classes tasks require compositional generalization % [nikola] IMO part sentence contribute much. I suggest skipping keeping citation. %-- is, tasks known building blocks must composed test time ways unseen training ~ -- ability argued crucial robust language understanding~. In paper, combine two lines research investigating effect training size error rates context compositional task. Specifically, derive suite extended datasets based Compositional Freebase Questions semantic parsing benchmark~. We use compositional structure example construct controlled experiments measure error rates increasing training size settings requiring compositional generalization settings simulating scaling broader scope natural language. We apply experiments analysis Transformers~ setting fixed computational cost -- is, fixed model size fixed training steps -- demonstrate key limits scalability setting. Our contributions following: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% In paper, reviewed various end-to-end neural automatic speech recognition systems optimization techniques on-device applications. On-device speech recognition huge advantages compared server-side ones terms user privacy, operation without internet, server-cost, latency. To operate speech recognition systems embedded processors, need consider several factors recognition accuracy, computational cost, latency, model size. We compared pros cons different neural network components Long Short-Term Memory , Convolutional Neural Network , attention mechanism. We explained compared different end-to-end neural speech recognition architectures stack LSTM layers Connectionist Temporal Classification loss , Recurrent Neural Network-Transformer , attention-based models, models based Monotonic Chunk-wise Attention . Further improvement achieved combining streaming model low-latency non-streaming model, applying shallow-fusion Language Model , applying spell correction using list named entities . We also discussed several model compression techniques including quantization, singular value decomposition, pruning, knowledge distillation. These recent advances neural end-to-end speech recognition made possible commercialize neural on-device end-to-end speech recognition systems ."," We present \starcfq{} : a suite of large-scale datasets of varying scope based on the \cfq{} semantic parsing benchmark, designed for principled investigation of the scalability of machine learning systems in a realistic compositional task setting. Using this suite, we conduct a series of experiments investigating the ability of Transformers to benefit from increased training size under conditions of fixed computational cost. We show that compositional generalization remains a challenge at all training sizes, and we show that increasing the scope of natural language leads to consistently higher error rates, which are only partially offset by increased training data. We further show that while additional training data from a related domain improves the accuracy in data-starved situations, this improvement is limited and diminishes as the distance from the related domain to the target domain increases."
"Designing robust spoken language identification algorithm important wide usability multi-lingual speech applications . With resurgence deep model learning, SLID performance significantly improved current supervised deep feature classifier learning algorithms . In algorithms, implicit assumption training testing data sets share similar statistical distribution property. However, due complex acoustic linguistic patterns, often case testing data set training data set quite different domains . An intuitive solution domain adaptation, i.e., align statistical distribution testing data set match training data set thus improve performance. Although large collected labeled testing data set, difficult obtain domain transfer function supervised learning algorithms, real applications, label information testing data set often unknown. Therefore, study, mainly focus preferable challenge situation, i.e., unsupervised domain adaptation. Unsupervised domain adaptation algorithms proposed speaker verification, e.g., probabilistic linear discriminant analysis parameter adaptation , feature-based correlation alignment , feature-distribution adaptor different domain vectors . However, algorithms, proposed speaker verification framework PLDA . As experiments showed PLDA framework perform well SLID task due less discriminative power modeling. Instead, SLID algorithms, multiple mixture logistic regression model used classifier model. Moreover, due complex shapes distributions training testing domains, difficult guarantee match different domain distributions. The purpose domain adaptation reduce domain discrepancy. Recently, optimal transport intensively investigated domain adaptation machine learning field . The initial motivation OT machine learning find optimal transport plan convert one probability distribution shape another shape least effort . By finding optimal transport, naturally defines distance measure different probability distributions. Based property, OT promising tool domain adaptation shape matching image processing, classification, segmentation . In paper, inspired OT based unsupervised adaptation , propose unsupervised neural adaptation framework cross-domain SLID tasks. Our main contributions are: We propose unsupervised neural adaptation model SLID deal domain mismatch problem. In model, explicitly formulate adaptation transformed feature space classifier space order reduce probability distribution discrepancy source target domains. We coincide OT distance metric measuring probability distribution discrepancy, integrate network optimization order learn adaptation model parameters. Based adaptation model, significant improvements obtained. %The remainder paper organized follows. Section introduces background fundamental theory . Section describes implementation details . Section presents SLID experiments results based proposed framework analyzing contribution CSA model detail. Section presents discussion results conclusion study. We proposed approach detecting hate speech internet memes multimodally, i.e. considering visual textual information holistically. We took part Hateful Memes Challenge placed third 3,173 participants. Our approach utilizes pre-trained VisualBERT , fine-tuned expanded train dataset, finally applying Majority Voting 27 best models. Our approach achieves 0.811 AUROC accuracy 0.765 challenge test set, considerable result also shows still far accuracy human judgement. \small"," Due to the mismatch of statistical distributions of acoustic speech between training and testing sets, the performance of spoken language identification  could be drastically degraded. In this paper, we propose an unsupervised neural adaptation model to deal with the distribution mismatch problem for SLID. In our model, we explicitly formulate the adaptation as to reduce the distribution discrepancy on both feature and classifier for training and testing data sets. Moreover, inspired by the strong power of the optimal transport  to measure distribution discrepancy, a Wasserstein distance metric is designed in the adaptation loss. By minimizing the classification loss on the training data set with the adaptation loss on both training and testing data sets, the statistical distribution difference between training and testing domains is reduced. We carried out SLID experiments on the oriental language recognition  challenge data corpus where the training and testing data sets were collected from different conditions. Our results showed that significant improvements were achieved on the cross domain test tasks."
"In traditional ad-hoc retrieval, queries documents represented variants bag-of-words representations. This leads called vocabulary mismatch problem: query contains words exactly match words relevant document, search engine may fail retrieve document. Query expansion document expansion, methods adding additional terms original query document, two popular solution alleviate vocabulary mismatch problem. Document expansion shown particularly effective short text retrieval language-model based retrieval . Most existing works document expansion unsupervised: using information corpus augment document representation, e.g., retrieval based clustering based , using external information augment document representation . Recently, \citet{nogueira2019DE} proposed new approach document expansion, based popular generative sequence-to-sequence model NLP, transformers . It leverages supervision train model predict expansion terms conditional document. The paper shown significant improvement passage datasets, trained in-domain. In paper, follow line supervised neural document expansion approach explore performance standard IR benchmarking dataset. Our main contributions are: 1. Adapting method unlabeled datasets exploring transfer learning weak-supervision approaches. 2. Adapting method traditional IR datasets, large number long documents present. In work, propose simple yet effective set techniques help detect hate speech unique labeled dataset high quality multimodal memes Facebook AI. The goal identify hate speech using multimodal model, also robust ""benign confounders"" cause binary label indicating whether meme hateful flip. We experiment number large pre-trained Transformer based architectures fine-tune single stream state-of-the-art models VL-BERT, VLP UNITER dual stream models LXMERT. We compare performance baselines provided show single-stream models significantly outperform these. We justify choice transformers architectures possible advantages coming fact pre-trained wide spectrum datasets different domains. We also propose adapt novel bidirectional cross-attention mechanism couple inferred caption information meme text obtained optical character recognition. This addition achieves higher classification accuracy labeling memes hateful. Furthermore show deep ensembles, simple yet powerful trick improve single model predictions significant margin. As expected, find training large architectures scratch performs poorly small set examples Hateful Memes dataset. However, also find choice pre-training datasets also matters terms domain similarity fine-tuning dataset. We conclude although multimodal models becoming increasingly sophisticated, still large gap comparing human performance. This leaves considerable room developing new algorithms deal multimodal understanding. \medskip \small \clearpage","     Recently, \citet{nogueira2019DE} proposed a new approach to document expansion based on a neural Seq2Seq model, showing significant improvement on short text retrieval task. However, this approach needs a large amount of in-domain training data.     In this paper, we show that this neural document expansion approach can be effectively adapted to standard IR tasks, where labels are scarce and many long documents are present."
"A speech signal considered variable-length temporal sequence, many features used characterize pattern. Short-term spectral features used extensively quasi-stationary property speech signal. After short-term processing, raw waveform converted two-dimensional~ matrix size , represents frequential feature dimension related number filter coefficients, denotes temporal frame length related utterance duration. For text-independent speaker verification~ system, main procedure extract fixed-dimensional speaker representation variable-length spectral feature sequence. One widely used spectral features Mel-frequency cepstral coefficient ~. Typically, MFCC feature vectors frames assumed independent identically distributed. They projected Gaussian components phonetic units accumulate statistics time axis form high-dimensional supervector. Then, factor analysis-based dimension reduction performed generate fixed-dimensional low rank i-vector representation. Recently, progress deep learning, many approaches directly train deep neural network~ distinguish different speakers. Systems comprising x-vector speaker embedding followed probabilistic linear discriminant analysis~ shown state-of-the-art performances multiple TISV tasks. In x-vector system, time-delay neural network~ followed statistic pooling time axis used modeling long-term temporal dependencies MFCC features. \end{figure*} For i-vector, x-vector, many speech modeling methods, feature matrix viewed multi-channel 1-D time series. Although duration may vary among utterances, feature dimension must fixed value. In paper, consider feature matrix single-channel 2-D image. From new perspective, spectral feature viewed ``picture"" sound, 2-D CNN implemented way traditional image recognition paradigms. This kind process brings type flexibility, i.e., size input ``image,"" including width height , arbitrary numbers. In words, 2-D CNN trained 64-dimensional spectrogram could potentially also process spectrogram 48 dimensions. We aim utilize flexibility 2-D CNN tackle mixed-bandwidth~ joint modeling problem. Currently, many devices equipment capture speech data different sampling rates, thus solving sampling rate mismatch problem become research topic speech community. The traditional way accomplish goal train specific model every target bandwidth since sampling rates different . An alternative solution uniformly downsample wideband~ speech data extend bandwidth narrowband~ data, combined . In paper, present unified solution solve MB joint modeling problem. The key idea view NB spectrogram sub-image WB spectrogram. The major contributions work summarized follows. {We proposed \modelfull, paradigm learning object-centric representations vision language.} Experiments Shop-VRB-Simple PartNet-Chairs show language significantly contributes learning better representations. This behavior consistent across two unsupervised image segmentation models. {Through systematic studies, also shown \model helps models learn object representations encode conceptual information, useful downstream tasks retrieval, visual reasoning, referring expression comprehension.} \clearpage \clearpage"," This paper proposes a unified deep speaker embedding framework for modeling speech data with different sampling rates. Considering the narrowband spectrogram as a sub-image of the wideband spectrogram, we tackle the joint modeling problem of the mixed-bandwidth data in an image classification manner. From this perspective, we elaborate several mixed-bandwidth joint training strategies under different training and test data scenarios. The proposed systems are able to flexibly handle the mixed-bandwidth speech data in a single speaker embedding model without any additional downsampling, upsampling, bandwidth extension, or padding operations. We conduct extensive experimental studies on the VoxCeleb1 dataset. Furthermore, the effectiveness of the proposed approach is validated by the SITW and NIST SRE 2016 datasets."
"% Automatic speech recognition systems typically trained vast quantity paired audio text data attain competitive performance. Obtaining paired data requires substantial human annotation efforts often time-consuming, expensive error-prone. With emerging popularity end-to-end ASR models, need large amounts training data demanding conventional hybrid-based ASR systems. For purpose, semi-supervised learning often investigated speech recognition, model trained using finite amount labeled data much larger amount unlabeled data. In long history semi-supervised learning speech recognition, self-training approach knowledge distillation , known teacher-student model training two commonly used SSL methods. Recent success representation learning enables new approach towards leveraging unlabeled data. In natural language processing community, BERT, ELMo, XLNet , GPT follow-ups classical examples representation learning. The key philosophy representation learning based using self-supervised learning, obtain `free' labels unlabeled data train supervised manner via proxy tasks. In context BERT, two proxy tasks defined including masked language model task two-sequence prediction task. These proxy tasks designed force learning robust, meaningful representation. After representation learned, downstream task model trained using labeled data learned representation. Optionally, representation learning block downstream task block fine-tuned together. Learning efficient speech representation traced back restricted Boltzmann machine , allows pre-training large amounts unlabeled data training deep neural network speech models. More recently, speech representation learning drawn increasing attention speech processing community shown promising results semi-supervised speech recognition . The design proxy tasks learning speech representation categorized two types. The first type based contrastive loss applied speech representation wav2vec variants . The model trained learn representations containing information discriminates future masked frame set negative samples via contrastive loss. The second type based reconstructive loss. The proxy task representation learning methods reconstruct temporal slices acoustic features based contextual information. These reconstruction tasks defined autoregressive reconstruction, masked-based reconstruction. APC follow-up examples use autoregressive reconstruction loss. In many state-of-the-art pretrained language model task, masked-based prediction adopted proxy tasks BERT XLNet . In speech, instead prediction, randomly mask temporal slices acoustic features attempt reconstruct . Orthogonal contrastive-/reconstructive-loss based speech representation learning, vector-quantized speech representations proposed. One motivation apply vector quantization enforcing quantization lead better linguistic unit discovery due discrete nature phonetic units. In VQ-APC , authors use VQ way limit model capacity control information needed encoding representation. In VQ-wav2vec wav2vec 2.0 , author use VQ facilitate direct application BERT NLP algorithms. In paper, introduce DeCoAR 2.0, Deep Contextualized Acoustic Representation vector quantization. We take inspirations many recent advances speech representation learning, propose multiple improvements vanilla DeCoAR. We summarize contributions paper follows: % The rest paper organized follows. Section gives brief overview previous DeCoAR method related work vector quantized speech representation learning. Section describes proposed DeCoAR 2.0 approach. Experimental results semi-supervised speech recognition presented Section followed conclusion Section. % Learning robust speech representation exploited recent years. Among approaches, wav2vec 2.0 uses 10 minutes labeled data 53k hours unlabeled data achieve word error rate 5.2/8.6 LibriSpeech benchmark. The model relies diverse codebook learnt correlates underlying speech units speech representations via contrastive loss. However, contrastive loss formulation result several locally optimal codebooks, exmaples, acoustic condition-sensitive codebooks: model easily optimized assign acoustic condition codebooks, temporally invariant codebooks: model assigns specific codes fixed temporal locations. %Furthermore, codes time step model select right feature encoder hardly contained meaningful phonetic information. So contrastive approach might generalize well datasets, espically real world data consisted lot nausence factor like noise, different recording environment. % A simple workaround could using frame reconstruction objective, network allows flow information input feature back latent space preserve meaningful information codes, helping mitigatate codebook learning problems contrastive loss discussed above. And compared simple reconstruction utilize information available achieved maximal prediction information less relevant ASR. By utilizing VQ layer, model able keep representation unwanted information flowing. % Automatic speech recognition systems typically trained vast quantity paired audio text data attain competitive performance. Obtaining paired data requires substantial human annotation efforts often time-consuming, expensive error-prone. With emerging popularity end-to-end ASR models, need large amounts training data demanding conventional hybrid-based ASR systems. For purpose, semi-supervised learning often investigated speech recognition, model trained using finite amount labeled data much larger amount unlabeled data. % In long history SSL speech recognition, self-training approach commonly used approach. In self-training methods, `seed' ASR model trained using paired audio/text data. The resulting model applied transcribe unlabeled audio data. The resulting hypotheses, combined different data selection criteria, treated `pseudo-labels' added original labeled dataset retrain new model. Simple concept, self-training works well practice one major caveat - pseudo-label injects systematic bias introduced seed model. To alleviate this, careful confidence calibration system combinations often used . Another family SSL based knowledge distillation , teacher-student model training , mostly applied acoustic model training hybrid-based ASR. In setups, teacher model generates frame-wise soft label instead hard label, student model trained soft labels via KL divergence loss instead standard cross-entropy loss based forced alignment. The knowledge distillation based SSL partially mitigates systematic bias rarely investigated towards sequence-level loss end-to-end ASR systems. % Recent success efficient representation learning, particular natural language processing , enables new approach towards leveraging unlabeled data. Classical examples representation learning NLP include BERT, ELMo, XLNet , GPT follow-ups , name few. The key philosophy representation learning based self-supervised learning, obtain `free' labels unlabeled data train supervised manner via proxy tasks. In context well-known BERT, two proxy tasks defined including masked language model task two-sequence prediction task. These proxy tasks defined way force learning robust, meaningful representation. A downstream task trained labeled data learned representation. Optionally, representation learning block downstream task fine-tuned together. % This paper presents DeCoAR 2.0, follow-up DeCoAR . We take inspiration many recent advances speech representation learning, propose multiple improvements vanilla DeCoAR. We summarize contributions paper follows: % % The rest paper organized follows. Section gives overview related work speech representation learning, brief recap previous DeCoAR method. Section describes proposed vector quantized DeCoAR approach. Experimental results semi-supervised speech recognition presented Section followed conclusion Section. % In work, propose improved speech representation learning paradigms towards semi-supervised speech recognition based previous work . % Current state-of-the-art models speech recognition require vast amounts transcribed audio data attain good performance. In particular, end-to-end ASR models demanding amount training data required compared traditional hybrid models. While obtaining large amount labeled data requires substantial effort resources, much less costly obtain abundant unlabeled data. % For reason, semi-supervised learning often used training ASR systems. Recently, self-supervised learning paradigm treats input modifications input learning targets obtained promising results. Those self-supervised speech representation fall main categories: Contrastive Predictive Coding incorporates contrastive objective learn representations containing information discriminates future masked frame set negative samples. Another approach Autoregressive Predictive Coding , tries directly predict reconstruct frame based context. % More recently, vector-quantized representations audio data drawn increasing attention speech processing . The motivation enforcing quantization leads better representation acoustic unit discovery due discrete nature phonetic units. VQ-APC also try exactly quantified information , control capacity models. And use vector quantization limited capacity forced retain information achieve maximal prediction. % Despite success wav2vec 2.0 model , model relies diverse codebook learnt correlates underlying speech units speech representations via contrastive loss. However, codes time step model select right feature encoder hardly contained meaningful phonetic information. More importantly, contrastive loss formulation result several locally optimal codebooks. A highly probable optima observed acoustic condition-sensitive codebooks: model easily optimized assign acoustic condition codebooks, temporally invariant codebooks: model assigns specific codes fixed temporal locations enable good contrastive loss. Hence, codebook learning methodology using contrastive loss might generalize well datasets, espically real world data consisted lot nausence factor like noise, different recording environment. % A simple solution could enforce codes explicitly carry information input features process. Using frame reconstruction objective, network allows flow information input feature back latent space preserve meaningful information, helping mitigatate codebook learning problems contrastive loss discussed above. Thus, propose novel self-supervised model learns vector quantized deep transformer acoustic representations based frames reconstruction. Since simple reconstruction utilize information available achieved maximal prediction information less relevant ASR. And utilizing VQ layer limit unwanted information flow final representation, Vector Quantized Deep Contextualized Acoustic Representations able achieve much better representation that's better suited semi-supervised ASR tasks. By using large amount unlabeled data, applies representations ASR tasks using limited amount labeled data. In implementation, perform acoustic representation learning using deep transformer training objective minimizes reconstruction error temporal slice filterbank features given context frames. After pre-training, fix parameters add output layers connectionist temporal classification loss ASR task. We train small ASR model instead fine-tuning computing-efficiency. Our approach showed supervision 10 hours labeled data DeCoAR 2.0 achieves performance par training 960 hours directly. In work, focused data-driven classification chemical reactions natural language processing methods use embedded information design reaction fingerprints. Our transformer-based models able learn classification schemes using broad set chemical reactions ground-truth, labeled commercially available reaction classification tool. With BERT classifier, match rule-based classification accuracy 98.2\.\","  Recent success in speech representation learning enables a new way to leverage unlabeled data to train speech recognition model. In speech representation learning, a large amount of unlabeled data is used in a self-supervised manner to learn a feature representation. Then a smaller amount of labeled data is used to train a downstream ASR system using the new feature representations. Based on our previous work DeCoAR \cite{ling2020deep} and inspirations from other speech representation learning, we propose DeCoAR 2.0, a Deep Contextualized Acoustic Representation with vector quantization. We introduce several modifications over the DeCoAR: first, we use Transformers in encoding module instead of LSTMs; second, we introduce a vector quantization layer between encoder and reconstruction modules; third, we propose an objective that combines the reconstructive loss with vector quantization diversity loss to train speech representations. Our experiments show consistent improvements over other speech representations in different data-sparse scenarios. Without fine-tuning, a light-weight ASR model trained on 10 hours of LibriSpeech labeled data with DeCoAR 2.0 features outperforms the model trained on the full 960-hour dataset with filterbank features.   % \yuzong{rewrite this} % We propose a novel approach for vector quantized deep contextualized acoustic representations. Following the same schema in DeCoAR\cite{ling2020deep}, we first exploit a large amount of unlabeled audio data via representation learning, where we reconstruct a temporal slice of filterbank features from context frames. The new resulting deep contextualized acoustic vector quantized representations  are then used to train a small CTC-based ASR system using a small amount of labeled audio data. In our experiments, we show that systems trained on DeCoAR 2.0 consistently outperform ones trained on other acoustic representations, giving the state-of-art and comparable results with wav2vec 2.0 \cite{baevski2020wav2vec} on semi-supervised experiments on Librispeech. Our approach can drastically reduce the amount of labeled data required; unsupervised training on LibriSpeech then supervision with 10 hours of labeled data achieves performance on par with training on all 960 hours directly."
"% % {A}{utomatic} speech recognition , one core components speech technology, achieved significant advancements past decade . A key driving force behind advancements rapid development deep learning techniques . % State-of-the-art ASR systems usually trained thousands hours transcribed speech data massive amount text data. % % State-of-the-art ASR systems usually requires thousands hours transcribed speech data massive amount text data train hybrid deep neural network-hidden Markov model based acoustic model recurrent neural network language model . % Moreover, hand-crafted pronunciation lexicon phoneme inventory based linguistic expertise often needed. Recently, end-to-end ASR architectures, AM LM training integrated single pipeline, gradually become mainstream ASR academic research , compared hybrid deep neural network-hidden Markov model architectures . E2E architectures advantage removing need pronunciation lexicon phoneme inventory system development. However, training E2E ASR system tends require even transcribed speech data hybrid DNN-HMM ASR system . There around spoken languages world . For them, amount transcribed speech data resources limited, even non-existent . Many low-resource languages, ethnic minority languages China languages Africa, may never formally studied. In addition lack enough transcribed speech data, linguistic knowledge languages incomplete, may even entirely lacking. Conventional supervised acoustic modeling therefore applied directly. This leads current situation high-performance ASR systems available small number major languages, e.g., English, Mandarin, French. To facilitate ASR technology low-resource languages, investigation unsupervised acoustic modeling methods necessary, aims find model set basic speech units represents sounds language interest, i.e., low-resource, target language. Recently, growing research interest UAM . A strict assumption UAM target language raw speech data available, transcriptions, phoneme inventory pronunciation lexicon unknown. This known zero-resource assumption . %It challenging task, yet significant research impact broad area speech language science technology, e.g., query-by-example spoken term detection , text-to-speech without text , understanding mechanisms underlying infant language acquisition , documentation endangered languages . There two main research strands UAM. The first strand formulates problem discovering finite set phoneme-like speech units . This often referred acoustic unit/model discovery . The second strand formulates problem learning acoustic feature representations distinguish subword units target language, robust linguistically-irrelevant factors, speaker . This often referred unsupervised subword modeling . In essence, second strand focused learning intermediate representation towards ultimate goal UAM, first strand aims directly ultimate goal. These two strands closely connected benefit other; instance, good subword-discriminative feature representation % good feature representation discriminative subword units robust speaker variation shown beneficial AUD , conversely, discovered speech units good consistency true phonemes helpful % could provide phoneme-like pseudo transcriptions assist learning subword-discriminative acoustic feature representations . This study addresses unsupervised subword modeling UAM. Learning subword-discriminative feature representations zero-resource scenario shown non-trivial task . The major difficulty separation linguistic information non-linguistic information . For instance, speech sound [\ae]\footnote{International Phonetic Alphabet symbol.} produced different speakers might mistakenly modeled different speech units . There many interesting attempts unsupervised subword modeling . One typical research direction leverage purely unsupervised learning techniques. One method clustering speech sounds acoustically similar patterns potentially correspond subword units , results phoneme-like pseudo transcriptions used facilitate subword-discriminative feature learning . % , e.g. cluster posteriorgrams DNN bottleneck features . Unsupervised self-supervised representation learning algorithms applied learn, without using external supervision, speech features retain linguistic content original data ignoring linguistically-irrelevant information, particularly speaker variation . A second research direction unsupervised subword modeling exploit cross-lingual knowledge . Speech text resources out-of-domain resource-rich languages shown beneficial modeling subword units in-domain low-resource languages. For instance, used OOD AM extract cross-lingual bottleneck features , used OOD ASR generate cross-lingual phone labels. % past studies . % One idea utilize pre-trained DNN AM OOD language generate phoneme-discriminative representations target speech, bottleneck features . % The second idea would leverage OOD ASR system decode speech utterances target language obtain cross-lingual phone labels supervision subsequent subword modeling . % These two ideas realize cross-lingual knowledge transfer AM level phone label level respectively. % Cross-lingual knowledge transfer done AM level, i.e., OOD pretrained AM used generate speech target language. % It also done phone label level, i.e., OOD ASR system decoding target speech utterances generate phone labels cross-lingual supervision . % This study adopts two-stage learning framework combines research directions within area unsupervised subword modeling. % The high-level overview proposed framework shown Fig. . %, At first stage, front-end, self-supervised representation learning model named autoregressive predictive coding trained. APC preserves phonetic speaker information original speech signal, makes two information types separable . %This makes APC suitable method unsupervised subword modeling. At second stage, back-end, cross-lingual, OOD DNN model bottleneck layer trained using APC pretrained features input features create missing frame labels. % , seen Fig. . %Frame labels required DNN-BNF model training directly available due zero-resource assumption. In framework, labels obtained using OOD ASR system. %By so, cross-lingual phonetic knowledge exploited. This system framework proposed recent study , showed state-of-the-art performances subword discriminability task two databases UAM: ZeroSpeech 2017 Libri-light . In work, expand extend work . Specifically, compare proposed approach supervised topline system trained transcribed data target language; compare proposed approach another cross-lingual knowledge transfer method ; % investigate AM-level phone label-level knowledge transfer methods effective; % investigate effects recently proposed APC model architectures front-end pretraining detail; investigate potential approach relation amount unlabeled training material varying data hours hours, compare models' performance topline model. Throughout experiments, English chosen target low-resource language. Its phoneme inventory transcriptions assumed unavailable system development. Dutch Mandarin chosen two OOD languages phoneme inventories transcriptions available. Unsupervised subword modeling typically evaluated using overall performance measures, ABX , purity , normalized mutual information . These metrics, however, provide insights approaches ability modeling individual phonemes phoneme categories. As ultimate goal beyond unsupervised subword modeling discover basic speech units good consistency true phonemes target language, we, best knowledge first time literature, additionally present detailed analyses explore question effectiveness proposed approach capturing phoneme articulatory feature information target language. % To answer question The analyses based standard ABX error rate evaluation , adapted work , consist two parts, i.e., analysis phoneme level AF level. The analyses aimed investigating phoneme AF information captured learned subword-discriminative feature representation, used guide future research improve unsupervised subword modeling well AUD. Moreover, correlate phoneme-level ABX error rates quality cross-lingual phone labels used train back-end DNN-BNF model order study proposed approach performs differently capturing different target phonemes' information, performance affected quality cross-lingual phone labels. %The analysis AF level carried interested extent AF information target language learned subword-discriminative feature representation. % AFs describe target articulators vocal tract pronouncing specific phone . The use AFs shown beneficial low-resource ASR acoustic unit discovery . % {\color{cyan}do need introduction AF?} % The AFs describe movement tongue, lips organs produce speech sounds. % {\color{cyan}state this} % The AF compact universal representation speech, language-independent phoneme inventory representation. % We interested extent AF information target language learned subword-discriminative feature representation. %In AF-level analysis, new evaluation metric proposed measure efficacy approach capturing AF information. This metric replaces phoneme inventory ABX discriminability task AF category. % Specifically, task predict whether test speech segment belongs AF attribute , contain speech sounds belonging different AF attributes. %Several AFs investigated study, including place articulation manner articulation consonants, tongue height tongue backness monophthong vowels. %The AF-level analysis could potentially provide guidance future research improve unsupervised subword modeling well AUD. To knowledge previous studies AF-level analysis unsupervised subword modeling AUD. % For instance, two systems achieving overall subword modeling performance might vary greatly linguistic implications. % overall performance metrics, ABX subword discriminability , purity , normalized mutual information . % , used input perform subword-discriminative learning . % , i.e., unsupervised feature representation learning problem. % {\color{cyan} high-level review representative approaches. purely unsupervised learning approaches 1-1. clustering; 1-2 unsupervised feature learning. leveraging OOD resources.} % {\color{red}Text colored} % train deep neural network -based acoustic model massive amount text data train % The remainder paper organized follows. Section provides review related works unsupervised subword modeling task. In Section , provide detailed description proposed approach unsupervised subword modeling, introduce comparative approaches compare approach. Section describes methodology used phoneme-level AF-level analyses. Section introduces experimental design study, Section reports results. Section describes setup conducting phoneme- AF-level analyses, discusses results analyses. Finally, Section draws conclusions. In paper, present vector quantized Deep Contextualized Acoustic Representation , improved speech representation learning approach based DeCoAR vector quantization. DeCoAR 2.0 multiple modification predecessor, deep Transformer encoding block, addition vector quantization module reconstruction module. In extreme data-limited semi-supervised conditions, observe using 10 hours labeled data DeCoAR 2.0 achieved performance par system trained 960 hours conventional filterbank features. DeCoAR 2.0 also performed comparably wav2vec 2.0 different semi-supervised scenarios. Future work includes exploring efficacy representation learning real world data including noisy adverse conditions, extension neural transducers end-to-end ASR systems downstream tasks."," % This study addresses unsupervised subword modeling, i.e., learning acoustic feature representations that can distinguish between subword units of a language. We propose a two-stage learning framework that combines self-supervised learning and cross-lingual knowledge transfer. The framework consists of autoregressive predictive coding  as the front-end and a cross-lingual deep neural network  as the back-end.  This study addresses unsupervised subword modeling, i.e., learning acoustic feature representations that can distinguish between subword units of a language. We propose a two-stage learning framework that combines self-supervised learning and cross-lingual knowledge transfer. The framework consists of autoregressive predictive coding  as the front-end and a cross-lingual deep neural network  as the back-end.  % Experiments on the ABX subword discriminability task conducted with the Libri-light and ZeroSpeech 2017 databases show our approach is competitive or superior to state-of-the-art studies. APC pretraining brings improvement to the entire framework, and brings larger improvement with increased amount of training data. Our best performance achieved by using unlabeled training data without linguistic knowledge of the target language is very close to that of a supervised system trained with labeled data of that language. The back-end of our approach is found more effective than a cross-lingual AM based BNF in cross-lingual knowledge transfer. Experiments on the ABX subword discriminability task conducted with the Libri-light and ZeroSpeech 2017 databases showed that our approach is competitive or superior to state-of-the-art studies.  % A comprehensive and systematic analysis at the phoneme- and articulatory feature - level is carried out to investigate the type of information that is captured by our learned feature representation. New metrics are proposed for the phoneme-level ABX subword discriminability task and attribute-level ABX AF task. The phoneme-level analysis showed that compared to MFCC, our approach achieves larger improvement in capturing diphthong information than monophthong vowel information, and the improvement varies greatly to different consonants. Results found there is a positive correlation between the effectiveness of the back-end in capturing a phoneme's information and the quality of cross-lingual phone labels assigned to that phoneme. The AF-level analysis showed that the proposed approach is better than MFCC and APC features in capturing manner of articulation , place of articulation , vowel height and backness information. Results indicate MoA is better captured by the proposed approach than PoA, and both MoA and PoA are better captured than vowel height and backness. Results implies AF information is less language-dependent than phoneme information.   Comprehensive and systematic analyses at the phoneme- and articulatory feature -level showed that our approach was better at capturing diphthong than monophthong vowel information, while also differences in the amount of information captured for different types of consonants were observed. Moreover, a positive correlation was found between the effectiveness of the back-end in capturing a phoneme's information and the quality of the cross-lingual phone labels assigned to the phoneme. The AF-level analysis together with t-SNE visualization results showed that the proposed approach is better than MFCC and APC features in capturing manner and place of articulation information, vowel height, and backness information.  % Taking all the analyses together, the two stages in our approach are both effective in capturing phoneme information. Monophthong vowel information is much more difficult to be captured than consonant information, which suggests a future research direction to improve the effectiveness of capturing monophthong vowel information.  Taken together, the analyses showed that the two stages in our approach are both effective in capturing phoneme and AF information. Nevertheless, monophthong vowel information is less well captured than consonant information, which suggests that future research should focus on improving capturing monophthong vowel information."
