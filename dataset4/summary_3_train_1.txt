 Sequence-to-sequence neural translation models learn semantic and syntactic relations between sentence pairs by optimizing the likelihood of the target given the source, i.e., $p$, an objective that ignores other potentially useful sources of information. We introduce an alternative objective function for neural MT that maximizes the mutual information  between the source and target sentences, modeling the bi-directional dependency of sources and targets. We implement the model with a simple re-ranking method, and also introduce a decoding algorithm that increases diversity in the N-best list produced by the first pass. Applied to the WMT German/English and French/English tasks, the proposed models offers a consistent performance boost on both standard LSTM and attention-based neural MT architectures.  \footnote{Code available upon publication.}  %We  %introduce a simple, straightforward re-ranking method to fulfill such a propose .  
 We build a multi-source machine translation model and train it to maximize the probability of a target English string given French and German sources.  Using the neural encoder-decoder framework, we explore several combination methods and report up to +4.8 Bleu increases on top of a very strong attention-based neural translation model. 
 We provide the first extensive evaluation of how using different types of context to learn skip-gram word embeddings affects performance on a wide range of intrinsic and extrinsic NLP tasks. Our results suggest that while intrinsic tasks tend to exhibit a clear preference to particular types of contexts and higher dimensionality, more careful tuning is required for finding the optimal settings for most of the extrinsic tasks that we considered. Furthermore, for these extrinsic tasks, we find that once the benefit from increasing the embedding dimensionality is mostly exhausted, simple concatenation of word embeddings, learned with different context types, can yield further performance gains.  As an additional contribution, we propose a new variant of the skip-gram model that learns word embeddings from weighted contexts of substitute words.  
     We propose multi-way, multilingual neural machine translation. The proposed     approach enables a single neural translation model to translate between     multiple languages, with a number of parameters that grows only linearly     with the number of languages. This is made possible by having a single     attention mechanism that is shared across all language pairs. We train the     proposed multi-way, multilingual model on ten language pairs from WMTé–³15     simultaneously and observe clear performance improvements over models     trained on only one language pair. In particular, we observe that the     proposed model significantly improves the translation quality of     low-resource language pairs.      %We propose multi-way, multilingual neural machine translation based on the     %attention mechanism. The proposed approach enables a single neural     %translation model to translate between multiple languages, while keeping the     %number of parameters grow only linearly with respect to the number of     %languages. This is made possible by having a single attention mechanism that     %is shared across multiple language pairs. We empirically show that the     %proposed multi-way, multilingual model works well in a realistic scenario by     %training it on ten language pairs from WMT'15 simultaneously. Furthermore,     %we observe that the proposed model significantly improves the translation     %quality of low-resource language pairs. 
 Semantic parsing aims at mapping natural language to machine interpretable meaning representations. Traditional approaches rely on high-quality lexicons, manually-built templates, and linguistic features which are either domain- or representation-specific. In this paper we present a general method based on an attention-enhanced encoder-decoder model. We encode input utterances into vector representations, and generate their logical forms by conditioning the output sequences or trees on the encoding vectors. Experimental results on four datasets show that our approach performs competitively without using hand-engineered features and is easy to adapt across domains and meaning representations. 
 Named Entity Disambiguation  refers to the task of resolving multiple named entity mentions in a document to their correct references in a knowledge base  . In this paper, we propose a novel embedding method specifically designed for NED. The proposed method jointly maps words and entities into the same continuous vector space. We extend the skip-gram model by using two models. The KB graph model learns the relatedness of entities using the link structure of the KB, whereas the anchor context model aims to align vectors such that similar words and entities occur close to one another in the vector space by leveraging KB anchors and their context words. By combining contexts based on the proposed embedding with standard NED features, we achieved state-of-the-art accuracy of 93.1\% on the standard CoNLL dataset and 85.2\% on the TAC 2010 dataset. % Our code and pre-trained vectors will be made available online. 
   We describe a question answering model that applies to both images and   structured knowledge bases.   The model uses natural language strings to   automatically assemble neural networks from a collection of   composable modules. Parameters for these modules are learned   jointly with network-assembly parameters via reinforcement    learning, with only  triples as supervision. Our    approach, which we term a ,    achieves state-of-the-art results on benchmark datasets in both    visual and structured domains. 
 Recently, recurrent neural networks  as powerful sequence models have re-emerged as a potential acoustic model for statistical parametric speech synthesis . The long short-term memory  architecture is particularly attractive because it addresses the vanishing gradient problem in standard RNNs, making them easier to train. Although recent studies have demonstrated that LSTMs can achieve significantly better performance on SPSS than deep feed-forward neural networks, little is known about why. Here we attempt to answer two questions: a) why do LSTMs work well as a sequence model for SPSS; b) which component  is most important. We present a visual analysis alongside a series of experiments, resulting in a proposal for a simplified architecture. The simplified architecture has significantly fewer parameters than an LSTM, thus reducing generation complexity considerably without degrading quality. 
   Nowadays, neural networks play an important role in the task of relation classification. By designing different neural architectures, researchers have improved the performance to a large extent in comparison with traditional methods. However, existing neural networks for relation classification are usually of shallow architectures . They may fail to explore the potential representation space in different abstraction levels. In this paper, we propose deep recurrent neural networks  for relation classification to tackle this challenge. Further, we propose a data augmentation method by leveraging the directionality of relations. We evaluated our DRNNs on the SemEval-2010 Task~8, and achieve an $F_1$-score of 86.1\%, outperforming previous state-of-the-art recorded results.$^1$ 
 We present an approach to structured prediction from bandit feedback, called , where only the value of a task loss function at a single predicted point, instead of a correct structure, is observed in learning. We present an application to discriminative reranking in Statistical Machine Translation  where the learning algorithm only has access to a $1- \textrm{BLEU}$ loss evaluation of a predicted translation instead of obtaining a gold standard reference translation. In our experiment bandit feedback is obtained by evaluating BLEU on reference translations without revealing them to the algorithm. This can be thought of as a simulation of interactive machine translation where an SMT system is personalized by a user who provides single point feedback to predicted translations. Our experiments show that our approach improves translation quality and is comparable to approaches that employ more informative feedback in learning. 
  Attention mechanism has enhanced state-of-the-art Neural Machine Translation  by jointly learning to align and translate. It tends to ignore past alignment information, however, which often leads to over-translation and under-translation. To address this problem, we propose coverage-based NMT in this paper. We maintain a coverage vector to keep track of the attention history. The coverage vector is fed to the attention model to help adjust future attention, which lets NMT system to consider more about untranslated source words. Experiments show that the proposed approach significantly improves both translation quality and alignment quality over standard attention-based NMT.\footnote{Our code is publicly available at \protect\url{https://github.com/tuzhaopeng/NMT-Coverage}.} 
 In real-time speech recognition applications, the latency is an important issue.  We have developed a character-level incremental speech recognition  system that responds quickly even during the speech, where the hypotheses are gradually improved while the speaking proceeds.  The algorithm employs a speech-to-character unidirectional recurrent neural network , which is end-to-end trained with connectionist temporal classification , and an RNN-based character-level language model . The output values of the CTC-trained RNN are character-level probabilities, which are processed by beam search decoding. The RNN LM augments the decoding by providing long-term dependency information. We propose tree-based online beam search with additional depth-pruning, which enables the system to process infinitely long input speech with low latency. This system not only responds quickly on speech but also can dictate out-of-vocabulary  words according to pronunciation. The proposed model achieves the word error rate  of 8.90\% on the Wall Street Journal  Nov'92 20K evaluation set when trained on the WSJ SI-284 training set. 
            In this paper we address the question of how to render           sequence-level networks better at handling structured input.           We propose a machine reading simulator which processes text           incrementally from left to right and performs shallow           reasoning with memory and attention. The reader extends the           Long Short-Term Memory architecture with a memory network in           place of a single memory cell. This enables adaptive memory           usage during recurrence with neural attention, offering a           way to weakly induce relations among tokens.  The system is           initially designed to process a single sequence but we also           demonstrate how to integrate it with an encoder-decoder           architecture.  Experiments on language modeling, sentiment           analysis, and natural language inference show that our model           matches or outperforms the state of the art. 		 	
 In the last two years, there have been numerous papers that have looked into using Deep Neural Networks to replace the acoustic model in traditional statistical parametric speech synthesis. However, far less attention has been paid to approaches like DNN-based postfiltering where DNNs work in conjunction with traditional acoustic models. In this paper, we investigate the use of Recurrent Neural Networks as a potential postfilter for synthesis. We explore the possibility of replacing existing postfilters, as well as highlight the ease with which arbitrary new features can be added as input to the postfilter. We also tried a novel approach of jointly training the Classification And Regression Tree and the postfilter, rather than the traditional approach of training them independently.  
 Semantic parsing methods are used for capturing and representing semantic meaning of text. Meaning representation  capturing all the concepts in the text may not always be available or may not be sufficiently complete. Ontologies provide a structured and reasoning-capable way to model the content of a collection of texts. In this work, we present a novel approach to joint learning of ontology and semantic parser from text. The method is based on semi-automatic induction of a context-free grammar from semantically annotated text. The grammar parses the text into semantic trees. Both, the grammar and the semantic trees are used to learn the ontology on several levels -- classes, instances, taxonomic and non-taxonomic relations.  The approach was evaluated on the first sentences of Wikipedia pages describing people. 
 Document classification tasks were primarily tackled at word level. Recent research that works with character-level inputs shows several benefits over word-level approaches such as natural incorporation of morphemes and better handling of rare words. We propose a neural network architecture that utilizes both convolution and recurrent layers to efficiently encode character inputs. We validate the proposed model on eight large scale document classification tasks and compare with character-level convolution-only models. It achieves comparable performances with much less parameters. 
    % This paper summarizes the work done by the authors for the Zero Resource Speech Challenge organized in the technical program of Interspeech 2015. The goal of the Challenge is  In this work we aim to discover high quality speech features and linguistic units directly from unlabeled speech data in a zero resource scenario.  The results are evaluated using the metrics and corpora proposed in the Zero Resource Speech Challenge organized at Interspeech 2015. A Multi-layered Acoustic Tokenizer  was proposed for automatic discovery of multiple sets of acoustic tokens from the given corpus. Each acoustic token set is specified by a set of hyperparameters that describe the model configuration.  These sets of acoustic tokens carry different characteristics fof the given corpus and the language behind, thus can be mutually reinforced.  The multiple sets of token labels are then used as the targets of a Multi-target Deep Neural Network  trained on low-level acoustic features. Bottleneck features extracted from the MDNN are then used as the feedback input to the MAT and the MDNN itself in the next iteration.  We call this iterative deep learning framework the Multi-layered Acoustic Tokenizing Deep Neural Network , which generates both high quality speech features for the Track 1 of the Challenge and acoustic tokens for the Track 2 of the Challenge. In addition, we performed extra experiments on the same corpora on the application of query-by-example spoken term detection. The experimental results showed the iterative deep learning framework of MAT-DNN improved the detection performance due to better underlying speech features and acoustic tokens. 
 We study the problem of recognition of fingerspelled letter sequences in American Sign Language in a signer-independent setting.  Fingerspelled sequences are both challenging and important to recognize, as they are used for many content words such as proper nouns and technical terms. Previous work has shown that it is possible to achieve almost $90$\% accuracies on fingerspelling recognition in a signer-dependent setting.  However, the more realistic signer-independent setting presents challenges due to significant variations among signers, coupled with the dearth of available training data.  We investigate this problem with approaches inspired by automatic speech recognition.   %Based on prior work, we use  %  edited wording We start with the best-performing approaches from prior work, based on tandem models and segmental conditional random fields , with features based on deep neural network  classifiers of letters and phonological features.  Using DNN adaptation, we find that it is possible to bridge a large part of the gap between signer-dependent and signer-independent performance.   Using only about 115 transcribed words for adaptation from the target signer, we obtain letter accuracies of up to $82.7$\% with frame-level adaptation labels and $69.7$\% with only word labels. 
 Understanding open-domain text is one of the primary challenges in natural language processing . Machine comprehension benchmarks evaluate the system's ability to understand text based on the text content only. In this work, we investigate machine comprehension on MCTest, a question answering  benchmark. Prior work is mainly based on feature engineering approaches. We come up with a neural network framework, named hierarchical attention-based convolutional neural network , to address this task without any manually designed features. Specifically, we explore HABCNN for this task by two routes, one is through traditional joint modeling of \passage{}, question and answer, one is through textual entailment. HABCNN employs an attention mechanism to detect key phrases, key sentences and key s that are relevant to answering the question. Experiments show that HABCNN outperforms prior deep learning approaches by a big margin. 
  In practice, training language models for individual authors is often expensive because of limited data resources. In such cases, Neural Network Language Models , generally outperform the traditional non-parametric N-gram models. Here we investigate the performance of a feed-forward NNLM on an authorship attribution problem, with moderate author set size and relatively limited data. We also consider how the text topics impact performance. Compared with a well-constructed N-gram baseline method with Kneser-Ney smoothing, the proposed method achieves nearly $2.5\%$ reduction in perplexity and increases author classification accuracy by $3.43\%$ on average, given as few as 5 test sentences. The performance is very competitive with the state of the art in terms of accuracy and demand on test data. The source code, preprocessed datasets, a detailed description of the methodology and results are available at \textrm{\url{https://github.com/zge/authorship-attribution}}.     
 We propose to train bi-directional neural network language model with noise contrastive estimation. Experiments are conducted on a rescore task on the PTB data set. It is shown that NCE-trained bi-directional NNLM outperformed the one trained by conventional maximum likelihood training. But still, it did not out-perform the baseline uni-directional NNLM. 
  This paper shows how one can directly apply natural language processing  methods to classification problems in cheminformatics.  Connection between these seemingly separate fields is shown by considering standard textual representation of compound, SMILES.  The problem of activity prediction against a target protein is considered, which is a crucial part of computer aided drug design process.  Conducted experiments show that this way one can not only outrank state of the art results of hand crafted representations but also gets direct structural insights into the way decisions are made.  
 In this work, we propose a semi-supervised method for short text clustering, where we represent texts as distributed vectors with neural networks,  and use a small amount of labeled data to specify our intention for clustering. We design a novel objective to combine the representation learning process and the k-means clustering process together,  and optimize the objective with both labeled data and unlabeled data iteratively until convergence through three steps:   assign each short text to its nearest centroid based on its representation from the current neural networks;   re-estimate the cluster centroids based on cluster assignments from step ;   update neural networks according to the objective by keeping centroids and cluster assignments fixed.  Experimental results on four datasets show that our method works significantly better than several other text clustering methods.  
   This document contains the instructions for preparing a camera-ready   manuscript for the proceedings of COLING-2016. The document itself   conforms to its own specifications, and is therefore an example of   what your manuscript should look like. These instructions should be   used for both papers submitted for review and for final versions of   accepted papers.  Authors are asked to conform to all the directions   reported in this document. 
Authorship attribution refers to the task of automatically determining the author based on a given sample of text. It is a problem with a long history and has a wide range of application. Building author profiles using language models is one of the most successful methods to automate this task. New language modeling methods based on neural networks alleviate the curse of dimensionality and usually outperform conventional N-gram methods. However, there have not been much research applying them to authorship attribution. In this paper, we present a novel setup of a Neural Network Language Model  and apply it to a database of text samples from different authors. We investigate how the NNLM performs on a task with moderate author set size and relatively limited training and test data, and how the topics of the text samples affect the accuracy. NNLM achieves nearly 2.5\% reduction in perplexity, a measurement of fitness of a trained language model to the test data. Given 5 random test sentences, it also increases the author classification accuracy by 3.43\% on average, compared with the N-gram methods using SRILM tools.  An open source implementation of our methodology is freely available at \url{https://github.com/zge/authorship-attribution/
  We introduce recurrent neural network grammars, probabilistic models of sentences with explicit phrase structure.  We explain efficient inference procedures that allow application to both parsing and language modeling.  Experiments show that they provide better parsing in English than any single previously published supervised generative model and better language modeling than state-of-the-art sequential RNNs in English and Chinese\footnote{The code to reproduce our results after the bug fix is publicly available at \url{https://github.com/clab/rnng}.}. 
 % In this paper, we develop a model to generate natural language description for images. In this model, we use the convolutional neural network  to extract image features, and adopt a deep multilayer recurrent neural network  to generate sentences describing the corresponding image. We also add memory cells for image features in this deep neural network. The intuition is enabling our model to memorize how much information from images it has learned already. Based on the experiments on Flickr8K and Flickr30K datasets, our model outperforms other state-of-the-art models with higher BLEU scores.   % Generating natural language description for images is a challenging task. The traditional way is to use the convolutional neural network  to extract image features, followed by a deep multilayer recurrent neural network  to generate sentences. In this paper, we developed a new model that added memory cells to gate the feeding of image feature to the deep neural network. The intuition is enabling our model to memorize how much information from images should be fed at each stage of the RNN.  Experiments on Flickr8K and Flickr30K datasets showed that our model outperforms other state-of-the-art models with higher BLEU scores.  Generating natural language descriptions for images is a challenging task. The traditional way is to use the convolutional neural network  to extract image features, followed by recurrent neural network  to generate sentences. In this paper, we present a new model that added memory cells to gate the feeding of image features to the deep neural network. The intuition is enabling our model to memorize how much information from images should be fed at each stage of the RNN. Experiments on Flickr8K and Flickr30K datasets showed that our model outperforms other state-of-the-art models with higher BLEU scores.  % remove the terms of "deep multilayer" before recurrent neural network, most of work still use single hidden layer RNN or LSTM.  
 	Recurrent neural network has been broadly applied to natural language processing problems. This kind of neural network is designed for modeling sequential data and has been testified to be quite efficient in sequential tagging tasks. In this paper, we propose to use bi-directional RNN with long short-term memory units for Chinese word segmentation, which is a crucial preprocess task for modeling Chinese sentences and articles. Classical methods focus on designing and combining hand-craft features from context, whereas bi-directional LSTM network does not need any prior knowledge or pre-designing, and it is expert in keeping the contextual information in both directions. Experiment result shows that our approach gets state-of-the-art performance in word segmentation on both traditional Chinese datasets and simplified Chinese datasets. 
In this chapter we give an overview of the application of complex network theory to quantify some properties of language. Our study is based on two fables in Ukrainian, {\em Mykyta the Fox
 Traditional convolutional layers extract features from patches of data by applying a non-linearity on an affine function of the input. We propose a model that enhances this feature extraction process for the case of sequential data, by feeding patches of the data into a recurrent neural network and using the outputs or hidden states of the recurrent units to compute the extracted features. By doing so, we exploit the fact that a window containing a few frames of the sequential data is a sequence itself and this additional structure might encapsulate valuable information. In addition, we allow for more steps of computation in the feature extraction process, which is potentially beneficial as an affine function followed by a non-linearity can result in too simple features. Using our convolutional recurrent layers, we obtain an improvement in performance in two audio classification tasks, compared to traditional convolutional layers. Tensorflow code for the convolutional recurrent layers is publicly available in \url{https://github.com/cruvadom/Convolutional-RNN}. 
 We study temporal networks of characters in literature focusing on { . The former, one of the most influential pieces of nonsense literature ever written, describes the adventures of Alice in a fantasy world with logic plays interspersed along the narrative. The latter, a song of heroic deeds,  depicts the Battle of Roncevaux in 778 A.D. during Charlemagne's campaign on the Iberian Peninsula. We apply methods recently developed by Taylor { to find time-averaged eigenvector centralities, Freeman indices and vitalities of characters. We show that temporal networks are more appropriate than static ones for studying stories, as they capture features that the time-independent approaches fail to yield. 
 We study the segmental recurrent neural network for end-to-end acoustic modelling. This model connects the segmental conditional random field  with a recurrent neural network  used for feature extraction. Compared to most previous CRF-based acoustic models, it does not rely on an external system to provide features or segmentation boundaries. Instead, this model marginalises out all the possible segmentations, and features are extracted from the RNN trained together with the segmental CRF. Essentially, this model is self-contained and can be trained end-to-end. In this paper, we discuss practical training and decoding issues as well as the method to speed up the training in the context of speech recognition. We performed experiments on the TIMIT dataset. We achieved 17.3\% phone error rate  from the first-pass decoding --- the best reported result using CRFs, despite the fact that we only used a zeroth-order CRF and without using any language model.   
 Neural Machine Translation  has reached state-of-the-art results. However, one of the main challenges that neural MT still faces is dealing with very large vocabularies and morphologically rich languages.   In this paper, we propose a neural MT system using character-based embeddings in combination with convolutional and highway layers to replace the standard lookup-based word representations. The resulting unlimited-vocabulary and affix-aware source word embeddings are tested in a state-of-the-art neural MT based on an attention-based bidirectional recurrent neural network. The proposed MT scheme provides improved results even when the source language is not morphologically rich.  %The number of target words is still limited by the standard word-based softmax output layer. However the number of unknowns at the output of the translation network is dramatically reduced  with a significant overall improvement over both neural and phrase-based baselines.  Improvements up to 3 BLEU points are obtained in the German-English WMT task. 
  Moving from limited-domain natural language generation  to open domain is difficult because the number of semantic input combinations grows exponentially with the number of domains. Therefore, it is important to leverage existing resources and exploit similarities between domains to facilitate domain adaptation. In this paper, we propose a procedure to train multi-domain, Recurrent Neural Network-based  language generators via multiple adaptation steps.  In this procedure, a model is first trained on counterfeited data synthesised from an out-of-domain dataset, and then fine tuned on a small set of in-domain utterances with a discriminative objective function. Corpus-based evaluation results show that the proposed procedure can achieve competitive performance in terms of  BLEU score and slot error rate while significantly reducing the data needed to train generators in new, unseen domains.  In subjective testing, human judges confirm that the procedure greatly improves generator performance when only a small amount of data is available in the domain.  
 Automatic event schema induction  means to extract meta-event from raw text, in other words, to find out what types  of event may exist in the raw text and what roles  may exist in each event type. In this paper, we propose a joint entity-driven model to learn templates and slots simultaneously based on the constraints of templates and slots in the same sentence. In addition, the entities' semantic information is also considered for the inner connectivity of the entities. We borrow the normalized cut criteria in image segmentation to divide the entities into more accurate template clusters and slot clusters.  The experiment shows that our model gains a relatively higher result than  previous work. 
 State-of-the-art named entity recognition systems rely heavily on hand-crafted features and domain-specific knowledge in order to learn effectively from the small, supervised training corpora that are available. In this paper, we introduce two new neural architectures---one based on bidirectional LSTMs and conditional random fields, and the other that constructs and labels segments using a transition-based approach inspired by shift-reduce parsers. Our models rely on two sources of information about words: character-based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora. Our models obtain state-of-the-art performance in NER in four languages without resorting to any language-specific knowledge or resources such as gazetteers. \footnote{The code of the  LSTM-CRF and Stack-LSTM NER systems are available at \url{https://github.com/glample/tagger} and \url{https://github.com/clab/stack-lstm-ner} } 
  Without discourse connectives, classifying implicit discourse relations is a challenging task and a bottleneck for building a practical discourse parser. Previous research usually makes use of one kind of discourse framework such as PDTB or RST to improve the classification performance on discourse relations. Actually, under different discourse annotation frameworks, there exist multiple corpora which have internal connections. To exploit the combination of different discourse corpora, we design related discourse classification tasks specific to a corpus, and propose a  novel Convolutional Neural Network embedded multi-task learning system to synthesize these tasks by learning both unique and  shared representations for each task. The experimental results on the PDTB implicit discourse relation classification task demonstrate that our model achieves significant gains over  baseline systems. 
 Recent approaches based on artificial neural networks  have shown promising results for short-text classification.  However, many short texts occur in sequences , and most existing ANN-based systems do not leverage the preceding short texts when classifying a subsequent one. In this work, we present a model based on recurrent neural networks and convolutional neural networks that incorporates the preceding short texts.  Our model achieves state-of-the-art results on three different datasets for dialog act prediction. 
  % Automatically identifying the sense of implicit discourse relations remains challenging due to the lack of discourse connectives. For human readers, however, this does not seem like a serious problem because the world knowledge behind can help to understand and interpret the relations.   Humans comprehend the meanings and relations of discourses heavily relying on their semantic memory that encodes general knowledge about concepts and facts. Inspired by this, we propose a neural recognizer for implicit discourse relation analysis, which builds upon a semantic memory that stores knowledge in a distributed fashion. We refer to this recognizer as { to generate a distributed surface representation for a discourse. A {. Experiments on the benchmark data set show that SeMDER benefits from the semantic memory and achieves substantial improvements of 2.56\% on average over current state-of-the-art baselines in terms of F1-score.  
  Implicit discourse relation recognition is a crucial component for automatic discourse-level analysis and nature language understanding. Previous studies exploit discriminative models that are built on either powerful manual features or deep discourse representations. In this paper, instead, we explore generative models and propose a variational neural discourse relation recognizer. We refer to this model as VarNDRR. VarNDRR establishes a directed probabilistic model with a latent continuous variable that generates both a discourse and the relation between the two arguments of the discourse. In order to perform efficient inference and learning, we introduce neural discourse relation models to approximate the prior and posterior distributions of the latent variable, and employ these approximated distributions to optimize a reparameterized variational lower bound. This allows VarNDRR to be trained with standard stochastic gradient methods. Experiments on the benchmark data set show that VarNDRR can achieve comparable results against state-of-the-art baselines without using any manual features.  
 We propose , a convolution neural network  architecture for sentence classification.   It  combines diverse versions of pretrained word embeddings and  extracts features of multigranular phrases with variable-size convolution filters.  We also show that  MVCNN is critical for good performance.  MVCNN achieves state-of-the-art performance on four tasks: on small-scale binary, small-scale multi-class and large-scale Twitter sentiment prediction and on subjectivity classification. 
 We address relation classification in the context of slot filling, the task of finding and evaluating fillers like ``Steve Jobs'' for the slot X in ``X founded Apple''. We propose a convolutional neural network which splits the input sentence into three parts according to the  relation arguments and compare it to state-of-the-art and traditional approaches of relation classification. Finally, we combine different methods and show that the combination is better  than individual approaches. We also analyze the effect of genre  differences on performance. 
 <Text of the summary of your article> 
 We present a novel method for jointly learning compositional and non-compositional phrase embeddings by adaptively weighting both types of embeddings using a compositionality scoring function. The scoring function is used to quantify the level of compositionality of each phrase, and the parameters of the function are jointly optimized with the objective for learning phrase embeddings. In experiments, we apply the adaptive joint learning method to the task of learning embeddings of transitive verb phrases, and show that the compositionality scores have strong correlation with human ratings for verb-object compositionality, substantially outperforming the previous state of the art. Moreover, our embeddings improve upon the previous best model on a transitive verb disambiguation task. We also show that a simple ensemble technique further improves the results for both tasks. 
 Most of the existing Neural Machine Translation  models focus on the conversion of sequential data and do not directly use syntactic information.  We propose a novel end-to-end syntactic NMT model, extending a sequence-to-sequence model with the source-side phrase structure.  Our model has an attention mechanism that enables the decoder to generate a translated word while softly aligning it with phrases as well as words of the source sentence.  Experimental results on the WAT'15 English-to-Japanese dataset demonstrate that our proposed model considerably outperforms sequence-to-sequence attentional NMT models and compares favorably with the state-of-the-art tree-to-string SMT system. 
 Transfer learning is aimed to make use of valuable knowledge in a source domain to help model performance in a target domain. It is particularly important to neural networks, which are very likely to be overfitting. In some fields like image processing, many studies have shown the effectiveness of neural network-based transfer learning. For neural NLP, however, existing studies have only casually applied transfer learning, and conclusions are inconsistent. In this paper, we conduct systematic case studies and provide an illuminating picture on the transferability of neural networks in NLP.\footnote{Code released on https://sites.google.com/site/transfernlp/} 
     The existing machine translation systems, whether phrase-based or neural,     have relied almost exclusively on word-level modelling with explicit     segmentation. In this paper, we ask a fundamental question: can neural     machine translation generate a character sequence without any explicit     segmentation? To answer this question, we evaluate an     attention-based encoder--decoder with a subword-level encoder and a     character-level decoder on four language pairs--En-Cs, En-De, En-Ru and     En-Fi-- using the parallel corpora from WMT'15. Our experiments show that     the models with a character-level decoder outperform the ones with a     subword-level decoder on all of the four language pairs.     Furthermore, the ensembles of neural models with a character-level     decoder outperform the state-of-the-art non-neural machine translation     systems on En-Cs, En-De and En-Fi and perform comparably on En-Ru. 
 % % We present persona-based models for handling the issue of speaker consistency in neural response generation.  A speaker model encodes personas in distributed embeddings that capture individual characteristics such as background information and speaking style. A dyadic speaker-addressee model captures properties of interactions between two interlocutors. % Our models yield qualitative performance improvements in both perplexity and \bleu scores over baseline sequence-to-sequence models, with similar gains in speaker consistency as measured by human judges.  
 In aspect-based sentiment analysis, extracting aspect terms along with the opinions being expressed from user-generated content is one of the most important subtasks. Previous studies have shown that exploiting connections between aspect and opinion terms is promising for this task. In this paper, we propose a novel joint model that integrates recursive neural networks and conditional random fields into a unified framework for explicit aspect and opinion terms co-extraction. The proposed model learns high-level discriminative features and double propagates information between aspect and opinion terms, simultaneously. Moreover, it is flexible to incorporate hand-crafted features into the proposed model to further boost its information extraction performance. Experimental results on the dataset from SemEval Challenge 2014 task 4 show the superiority of our proposed model over several baseline methods as well as the winning systems of the challenge. 
 % Phil's edit Many language generation tasks require the production of text conditioned on both structured and unstructured inputs. We present a novel neural network architecture which generates an output sequence conditioned on an arbitrary number of input functions. Crucially, our approach allows both the choice of conditioning context and the granularity of generation, for example characters or tokens, to be marginalised, thus permitting scalable and effective training.  Using this framework, we address the problem of generating programming code from a mixed natural language and structured specification. We create two new data sets for this paradigm derived from the collectible trading card games Magic the Gathering and Hearthstone. On these, and a third preexisting corpus, we demonstrate that marginalising multiple predictors allows our model to outperform strong benchmarks.  %	We present a novel generative neural network architecture, which generates the output sequence using an arbitrary number of generator functions in a principled way. This enables the integration of multiple generative approaches, such as using the token-level softmax to generate new words and pointer networks to copy from the input sequence. Using this framework, we address the problem of generating programming code from a natural language specification, where many elements of the natural language must be copied to the code. We identify a domain where annotations are naturally generated, namely collectible trading card games, and build two datasets from this domain. We show that our model outperforms the baselines in both in-domain and out-of-domain datasets, which emphasises the need of more expressive generative models that can account for multiple predictors. 
 Over the past decade, large-scale supervised learning corpora have enabled machine learning researchers to make substantial advances. % in applications ranging from automatic speech recognition and machine translation to computer vision. %~. %However, so far question answering systems have been trained on corpora generated by heuristic question answering. %However, to this date, there are no available corpora for training large-scale question answering systems. However, to this date, there are no large-scale question-answer corpora available. %Consequently, current state-of-the-art question answering systems heavily rely on heuristics, %such as feature engineering and artificially synthesized question answering corpora. In this paper we present the 30M Factoid Question-Answer Corpus, an enormous question answer pair corpus produced by applying a novel neural network architecture on the knowledge base Freebase  to transduce facts into natural language questions. The produced question answer pairs are evaluated both by human evaluators and  using automatic evaluation metrics, including well-established machine translation and sentence similarity metrics. %including BLEU, METEOR and word embedding-based similarity metrics, %and by human evaluators. Across all evaluation criteria the question-generation model outperforms the competing template-based baseline. Furthermore, when presented to human evaluators, the generated questions appear comparable in quality to real human-generated questions. % untrained human evaluators 
 Determining the intended sense of words in text -- word sense disambiguation  -- is a long-standing problem in natural language processing. Recently, researchers have shown promising results using word vectors extracted from a neural network language model as features in WSD algorithms. However, a simple average or concatenation of word vectors for each word in a text loses the sequential and syntactic information of the text.  In this paper, we study WSD with a sequence learning neural net, LSTM, to better capture the sequential and syntactic patterns of the text. To alleviate the lack of training data in all-words WSD, we employ the same LSTM in a semi-supervised label propagation classifier. We demonstrate state-of-the-art results, especially on verbs.  
 We apply a general recurrent neural network  encoder framework to community question answering  tasks. Our approach does not rely on any linguistic processing, and can be applied to different languages or domains. Further improvements are observed when we extend the RNN encoders with a neural attention mechanism that encourages reasoning over entire sequences.  To deal with practical issues such as data sparsity and imbalanced labels, we apply various techniques such as transfer learning and multitask learning. Our experiments on the SemEval-2016 cQA task show 10\% improvement on a MAP score compared to an information retrieval-based approach, and achieve comparable performance to a strong handcrafted feature-based method. 
           Traditional approaches to extractive summarization rely           heavily on human-engineered features. In this work we           propose a data-driven approach based on neural networks and           continuous sentence features. We develop a general framework           for single-document summarization composed of a hierarchical           document encoder and an attention-based extractor. This           architecture allows us to develop different classes of           summarization models which can extract sentences or           words. We train our models on large scale corpora containing           hundreds of thousands of document-summary           pairs\footnote{Resources are available for download at           	\url{http://homepages.inf.ed.ac.uk/s1537177/resources.html}}. Experimental results on two summarization datasets           demonstrate that our models obtain results comparable to the           state of the art without any access to linguistic           annotation. 		 		 	
 This paper proposes a model to learn word embeddings with weighted contexts based on part-of-speech  relevance weights. POS is a fundamental element in natural language. However, state-of-the-art word embedding models fail to consider it. This paper proposes to use position-dependent POS relevance weighting matrices to model the inherent syntactic relationship among words within a context window. We utilize the POS relevance weights to model each word-context pairs during the word embedding training process. The model proposed in this paper paper jointly optimizes word vectors and the POS relevance matrices. Experiments conducted on popular word analogy and word similarity tasks all demonstrated the effectiveness of the proposed method. 
 Identifying topics of discussions in online health communities  is critical to various applications, but can be difficult because topics of OHC content are usually heterogeneous and domain-dependent. In this paper, we provide a multi-class schema, an annotated dataset, and supervised classifiers based on convolutional neural network  and other models for the task of classifying discussion topics. We apply the CNN classifier to the most popular breast cancer online community, and carry out a longitudinal analysis to show topic distributions and topic changes throughout members' participation. Our experimental results suggest that CNN outperforms other classifiers in the task of topic classification, and that certain trajectories can be detected with respect to topic changes. 
  This paper introduces the visually informed embedding of word , a continuous vector representation for a word extracted from a deep neural model trained using the Microsoft COCO data set to forecast the spatial arrangements between visual objects, given a textual description. The model is composed of a deep multilayer perceptron  stacked on the top of a Long Short Term Memory  network, the latter being preceded by an embedding layer. The VIEW is applied to transferring multimodal background knowledge to Spatial Role Labeling  algorithms, which recognize spatial relations between objects mentioned in the text. This work also contributes with a new method to select complementary features and a fine-tuning method for MLP that improves the $F1$ measure in classifying the words into spatial roles. The VIEW is evaluated with the Task 3 of SemEval-2013 benchmark data set, SpaceEval. 
  Corpora and web texts can become a rich language learning resource if we have a means of assessing whether they are linguistically appropriate for learners at a given proficiency level. In this paper, we aim at addressing this issue by presenting the first approach for predicting linguistic complexity for Swedish second language learning material on a 5-point scale. After showing that the traditional Swedish readability measure, Lé©çž«barhetsindex , is not suitable for this task, we propose a supervised machine learning model, based on a range of linguistic features, that can reliably classify texts according to their difficulty level. Our model obtained an accuracy of 81.3\% and an F-score of 0.8, which is comparable to the state of the art in English and is considerably higher than previously reported results for other languages. We further studied the utility of our features with single sentences instead of full texts since sentences are a common linguistic unit in language learning exercises. We trained a separate model on sentence-level data with five classes, which yielded 63.4\% accuracy. Although this is lower than the document level performance, we achieved an adjacent accuracy of 92\%. Furthermore, we found that using a combination of different features, compared to using lexical features alone, resulted in 7\% improvement in classification accuracy at the sentence level, whereas at the document level, lexical features were more dominant. Our models are intended for use in a freely accessible web-based language learning platform for the automatic generation of exercises.     
 We present a discriminative model for single-document summarization that integrally combines compression and anaphoricity constraints. Our model selects textual units to include in the summary based on a rich set of sparse features whose weights are learned on a large corpus. We allow for the deletion of content within a sentence when that deletion is licensed by compression rules; in our framework, these are implemented as dependencies between subsentential units of text. Anaphoricity constraints then improve cross-sentence coherence by guaranteeing that, for each pronoun included in the summary, the pronoun's antecedent is included as well or the pronoun is rewritten as a full mention. When trained end-to-end, our final system\footnote{Available at http://nlp.cs.berkeley.edu} outperforms prior work on both ROUGE as well as on human judgments of linguistic quality. 
  Deep learning has dramatically improved the performance of speech recognition systems through learning hierarchies of features optimized for the task at hand.   However, true end-to-end learning, where features are learned directly from waveforms, has only recently reached the performance of hand-tailored representations based on the Fourier transform. In this paper, we detail an approach to use convolutional filters to push past the inherent tradeoff of temporal and frequency resolution that exists for spectral representations.  At increased computational cost, we show that increasing temporal resolution via reduced stride and increasing frequency resolution via additional filters delivers significant performance improvements. Further, we find more efficient representations by simultaneously learning at multiple scales, leading to an overall decrease in word error rate on a difficult internal speech test set by 20.7\% relative to networks with the same number of parameters trained on spectrograms.   
 This paper presents a dataset collected from natural dialogs which enables to test the ability of dialog systems to learn new facts from user utterances throughout the dialog.  This \emph{interactive learning
    Although highly correlated, speech and speaker recognition have been regarded as   two independent tasks and studied by two communities. This is certainly not the way   that people behave: we decipher both speech content and speaker traits at the same time.    This paper presents a unified model to perform speech and speaker recognition   simultaneously and altogether.   The model is based on a unified neural network where the output of   one task is fed to the input of the other, leading   to a multi-task recurrent network. Experiments show that the joint model outperforms   the task-specific models on both the two tasks.     
 Recursive neural networks  and their recently proposed extension recursive  long short term memory networks  are models that compute representations  for sentences, by recursively combining word embeddings according to an externally  provided parse tree. Both models thus, unlike recurrent networks, explicitly make  use of the hierarchical structure of a sentence. In this paper, we demonstrate that RNNs nevertheless suffer from the vanishing gradient and long distance  dependency problem, and that RLSTMs greatly improve over RNN's on these problems.  We present an artificial learning task that allows us to quantify the severity of  these problems for both models. We further show that a ratio of gradients  is highly indicative of the success of backpropagation at optimizing the relevant weights low in the tree. This paper thus provides an  explanation for existing, superior results of RLSTMs on tasks such as sentiment  analysis, and suggests that the benefits of including hierarchical structure and of  including LSTM-style gating are complementary. % %Many experiments demonstrate that, compared   %with traditional Recurrent neural networks, the LSTM architecture is  %more capable of capturing long term dependencies and %effectively overcomes the vanishing gradient problem.  %Being aware of the current situation that such experiments are mostly absent  %for the case of tree structures, we carry out several experiments  %on artificial data to examine whether the LSTM for trees is also  %superior to traditional Recursive neural networks.  %Our experimental results confirm this.  
  %Content: % DMN, QA, Multimodal, state of the art results in both modalities % No supporting fact supervision % More generalized input module %Intro domain Neural network architectures with memory and attention mechanisms exhibit certain reasoning capabilities required for question answering. One such architecture, the dynamic memory network , obtained high accuracy on a variety of language tasks. % that go beyond other deep learning architectures. %What's the problem However, it was not shown whether the architecture achieves strong results for question answering when supporting facts are not marked during training or whether it could be applied to other modalities such as images. %Our solution Based on an analysis of the DMN, we propose several improvements to its memory and input modules. Together with these changes we introduce a novel input module for images in order to be able to answer visual questions. % Gets SOTA Our new DMN+ model improves the state of the art on both the Visual Question Answering dataset and the \babi-10k text question-answering dataset without supporting fact supervision. 
%   <- trailing '%' for backward compatibility of .sty file  We present an approach based on feed-forward neural networks for learning the distribution of textual documents. This approach is inspired by the Neural Autoregressive Distribution Estimator  model, which has been shown to be a good estimator of the distribution of discrete-valued  high-dimensional vectors. In this paper, we present how NADE can successfully be adapted to the case of textual data, retaining from NADE the property that sampling or computing the probability of observations can be done exactly and efficiently. The approach can also be used to learn deep representations of documents that are competitive to those learned by the alternative topic modeling approaches. Finally, we describe how the approach can be combined with a regular neural network N-gram model and substantially improve its performance, by making its learned representation sensitive to the larger, document-specific context.   
   Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce uninterpretability of the neural models. We propose a general framework capable of enhancing various types of neural networks  with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems.   
 We report an implementation of a clinical information extraction tool that leverages deep neural network to annotate event spans and their attributes from raw clinical notes and pathology reports. Our approach uses context words and their part-of-speech tags and shape information as features. Then we hire temporal  convolutional neural network to learn hidden feature representations. Finally, we use Multilayer Perceptron  to predict event spans. The empirical evaluation demonstrates that our approach significantly outperforms baselines. 
   %abstract å¨‘ cite paper  Recurrent neural network architectures combining with attention mechanism, or neural attention model, have shown promising performance recently for the tasks including speech recognition, image caption generation, visual question answering and machine translation.  In this paper, neural attention model is applied on two sequence labeling tasks, dialogue act detection and key term extraction.  In the sequence labeling tasks, the model input is a sequence, and the output is the label of the input sequence. The major difficulty of sequence labeling is that when the input sequence is long, it can include many noisy or irrelevant part. If the information in the whole sequence is treated equally, the noisy or irrelevant part may degrade the classification performance.  The attention mechanism is helpful for sequence classification task because it is capable of highlighting important part among the entire sequence for the classification task. The experimental results show that with the attention mechanism, discernible improvements were achieved in the sequence labeling task considered here.  The roles of the attention mechanism in the tasks are further analyzed and visualized in this paper.   %Recurrent neural network  architectures combining with attention mechanism have shown promising performance recently for some tasks, which includes speech recognition , image caption generation ,  visual question answering , and machine translation . This mechanism is capable of highlighting important part among the entire data, which may be helpful in some prediction tasks. We thus explore the role of attention mechanism in the tasks of language understanding. This paper focuses on the sub-problems of spoken language understanding, dialogue act classification and key term extraction, using a novel attention-based recurrent neural networks for solving problems. The experiments show encouraging performance on the AMI meeting corpus and a database collected from website articles.   
     The goal of this paper is to use multi-task learning to efficiently scale     slot filling models for natural language understanding to handle multiple     target tasks or domains. The key to scalability is reducing the amount of     training data needed to learn a model for a new task. The proposed     multi-task model delivers better performance with less data by leveraging     patterns that it learns from the other tasks. The approach supports an open     vocabulary, which allows the models to generalize to unseen words,     which is particularly important when very little training data is used.     A newly collected crowd-sourced data set, covering four different domains,     is used to demonstrate the effectiveness of the domain adaptation and      open vocabulary techniques.        
 In this study we address the problem of  training a neural-network for language identification  using both labeled and unlabeled speech samples in the form of i-vectors. We propose a neural network architecture that can also handle out-of-set languages.   We utilize a modified version of the recently proposed Ladder Network semisupervised training procedure that optimizes the reconstruction costs of a stack of denoising autoencoders.   We show that this approach can be  successfully  applied  to the case where the training dataset is composed of both labeled and unlabeled acoustic data.   The results show enhanced  language identification on the  NIST 2015 language identification dataset. 
   %\fontsize{10pt}{12pt}   demonstration of the pragmatic behavior the model ultimately exhibits. In   human evaluations on a referring expression game, our approach succeeds 81\%   of the time, compared to 69\% using existing techniques.  
 A key challenge in entity linking is making effective use of contextual information to disambiguate mentions that might refer to different entities in different contexts. We present a model that uses convolutional neural networks to capture semantic correspondence between a mention's context and a proposed target entity. These convolutional networks operate at multiple granularities to exploit various kinds of topic information, and their rich parameterization gives them the capacity to learn which $n$-grams characterize different topics. We combine these networks with a sparse linear model to achieve state-of-the-art performance on multiple entity linking datasets, outperforming the prior systems of  and .\footnote{Source available at \\ github.com/matthewfl/nlp-entity-convnet} 
 Nearly all previous work on neural machine translation  has used quite restricted vocabularies, perhaps with a subsequent method to patch in unknown words. This paper presents a novel word-character solution to achieving open vocabulary NMT.  We build hybrid systems that translate mostly at the { components for rare words.  Our character-level recurrent neural networks compute source word representations and recover unknown target words when needed. The twofold advantage of such a hybrid approach is that it is much faster and easier to train than character-based ones; at the same time, it never produces unknown words as in the case of word-based models.  On the WMT'15 English to Czech translation task,  this hybrid approach offers an addition boost of +$\gain{}$ BLEU points over models  that already handle unknown words.  Our best system achieves a new state-of-the-art result with $$ BLEU score. We demonstrate that our character models can successfully learn to not only generate well-formed words for Czech, a highly-inflected language with a very complex vocabulary, but also build correct representations for English source words. 
 Very deep CNNs with small $3\times3$ kernels have recently been shown to achieve very strong performance as acoustic models in hybrid NN-HMM speech recognition systems. In this paper we investigate how to efficiently scale these models to larger datasets. Specifically, we address the design choice of pooling and padding along the time dimension  which renders convolutional evaluation of sequences highly inefficient. We propose a new CNN design without timepadding and without timepooling, which is slightly suboptimal for accuracy, but has two significant advantages: it enables sequence training and deployment by allowing efficient convolutional evaluation of full utterances,  and, it allows for batch normalization to be straightforwardly adopted to CNNs on sequence data. Through batch normalization, we recover the lost peformance from removing the time-pooling, while keeping the benefit of efficient convolutional evaluation.  We demonstrate the performance of our models both on larger scale data than before,  and after sequence training. % rather than just cross-entropy training. %We show results from sequence training on both the 300h switchboard-1 and  %the 2000h switchboard dataset. Our very deep CNN model sequence trained on the 2000h switchboard dataset obtains 9.4 word error rate on the Hub5 test-set, matching with a single model the performance of the 2015 IBM system combination, which was the previous best published result. %We report the best published single-system results on the standard  %300h switchboard-1 and 2000h switchboard datasets after both cross-entropy training and sequence training.  %Rephrase into investigation into time pooling, timepadding, and batch normalization. %+ Problem: how does timepadding, timepooling, and BN work for CNNs on sequences? %+ Interesting cause doing it right in deep CNNs gives great results, state of the art on swb. %+ We propose design principle: do not pool and pad in time . %+ This allows BN to be adapted for asr cnns. Allows efficient ST and deployment. SOA results. 
 	 Recently, neural models have been proposed for headline generation by learning to map documents to headlines with recurrent neural networks. Nevertheless, as traditional neural network utilizes maximum likelihood estimation for parameter optimization, it essentially constrains the expected training objective within word level rather than sentence level. Moreover, the performance of model prediction significantly relies on training data distribution. To overcome these drawbacks, we employ minimum risk training strategy in this paper, which directly optimizes model parameters in sentence level with respect to evaluation metrics and leads to significant improvements for headline generation. Experiment results show that our models outperforms state-of-the-art systems on both English and Chinese headline generation tasks. 	 
  Word sense disambiguation helps identifying the proper sense of ambiguous words in text. With large terminologies such as the UMLS Metathesaurus ambiguities appear and highly effective disambiguation methods are required. Supervised learning algorithm methods are used as one of the approaches to perform disambiguation. Features extracted from the context of an ambiguous word are used to identify the proper sense of such a word. The type of features have an impact on machine learning methods, thus affect disambiguation performance. In this work, we have evaluated several types of features derived from the context of the ambiguous word and we have explored as well more global features derived from MEDLINE using word embeddings. Results show that word embeddings improve the performance of more traditional features and allow as well using recurrent neural network~s based on Long-Short Term Memory  nodes\replace{, which further improve the disambiguation performance}{}. The combination of unigrams and word embeddings  set a new state of the art performance with a\replace{n}{macro }accuracy of 95.97 in the MSH WSD data set.  
 Human communication is often executed in the form of a narrative, an account of connected events composed of characters, actions, and settings. A coherent narrative structure is therefore a requisite for a well-formulated narrative -- be it fictional or nonfictional -- for informative and effective communication, opening up the possibility of a deeper understanding of a narrative by studying its structural properties. In this paper we present a network-based framework for modeling and analyzing the structure of a narrative, which is further expanded by incorporating methods from computational linguistics to utilize the narrative text. Modeling a narrative as a dynamically unfolding system, we characterize its progression via the growth patterns of the character network, and use sentiment analysis and topic modeling to represent the actual content of the narrative in the form of interaction maps between characters with associated sentiment values and keywords.  This is a network framework advanced beyond the simple occurrence-based one most often used until now, allowing one to utilize the unique characteristics of a given narrative to a high degree.  Given the ubiquity and importance of narratives, such advanced network-based representation and analysis framework may lead to a more systematic modeling and understanding of narratives for social interactions, expression of human sentiments, and communication. 
 There is compelling evidence that coreference prediction would benefit from modeling global information about entity-clusters. Yet, state-of-the-art performance can be achieved with systems treating each mention prediction   independently, which we attribute to the inherent   difficulty of crafting informative cluster-level features. We instead propose to use recurrent neural networks    to learn latent, global representations of entity clusters directly   from their mentions. We show    that such representations are especially useful for the prediction of pronominal mentions, and can be incorporated into   an end-to-end coreference system that outperforms the state of the art    without requiring any additional search. 
 We show how eye-tracking corpora can be used to improve sentence compression models, presenting a novel multi-task learning algorithm based on multi-layer LSTMs. We obtain performance competitive with or better than state-of-the-art approaches.  
 Teaching machines to accomplish tasks by conversing naturally with humans is challenging. Currently, developing task-oriented dialogue systems requires creating multiple components and typically this involves either a large amount of handcrafting, or acquiring costly labelled datasets to solve a statistical learning problem for each component. In this work we introduce a neural network-based text-in, text-out end-to-end trainable goal-oriented dialogue system along with a new way of collecting dialogue data based on a novel pipe-lined Wizard-of-Oz framework. This approach allows us to develop dialogue systems easily and without making too many assumptions about the task at hand. The results show that the model can converse with human subjects naturally whilst helping them to accomplish tasks in a restaurant search domain. 
 Many natural language processing  tasks can be generalized into segmentation problem. In this paper, we combine semi-CRF with neural network to solve NLP segmentation tasks. Our model represents a segment both by composing the input units and embedding the entire segment. We thoroughly study different composition functions and different segment embeddings. We conduct extensive experiments on two typical segmentation tasks: named entity recognition  and Chinese word segmentation . Experimental results show that our neural semi-CRF model benefits from representing the entire segment and achieves the state-of-the-art performance on CWS benchmark dataset and competitive results on the CoNLL03 dataset. 
 Recent works using artificial neural networks based on distributed word representation greatly boost performance on answer selection problem. Nevertheless, most of the previous works used deep learning methods  mainly to capture semantic representation of each sentence separately, ignoring the interdependence between each other on lexical level. In this paper, we constitutes a deep convolutional network directly on pairwise token matching, multi-modal similarity metric learning is then adopted to enrich the lexical modality matching. The proposed model demonstrates its performance by surpassing previous state-of-the-art systems on the answer selection benchmark, i.e., TREC-QA dataset, in both MAP and MRR metrics. 
 In this work we propose a novel attention-based neural network model for the task of fine-grained entity type classification that unlike previously proposed models recursively composes representations of entity mention contexts. Our model achieves state-of-the-art performance with $74.94\%$ loose micro F1-score on the well-established FIGER dataset, a relative improvement of $2.59\%$ . We also investigate the behavior of the attention mechanism of our model and observe that it can learn contextual linguistic expressions that indicate the fine-grained category memberships of an entity. 
 Existing approaches for Chinese zero pronoun resolution overlook semantic information. This is because zero pronouns have no descriptive information, which results in difficulty in explicitly capturing their semantic similarities with antecedents. Moreover, when dealing with candidate antecedents, traditional systems simply take advantage of the local information of a single candidate antecedent while failing to consider the underlying information provided by the other candidates from a global perspective. To address these weaknesses, we propose a novel zero pronoun-specific neural network, which is capable of representing zero pronouns by utilizing the contextual information at the semantic level. In addition, when dealing with candidate antecedents, a two-level candidate encoder is employed to explicitly capture both the local and global information of candidate antecedents. We conduct experiments on the Chinese portion of the OntoNotes 5.0 corpus. Experimental results show that our approach substantially outperforms the state-of-the-art method in various experimental settings. 
 A long-term goal of machine learning research is to build an intelligent dialog agent. Most research in natural language understanding has focused on learning from fixed training sets of labeled data, with supervision either at the word level  or sentence level . This kind of supervision is not realistic of how humans learn, where language is both learned by, and used for, communication. In this work, we study dialog-based language learning, where supervision   is given naturally and implicitly in the response of the dialog partner during  the conversation. % %In this work, we study dialog-based supervision  by %defining a new set of tasks  %in two domains: the bAbI tasks of  and large-scale %question answering from .  %In both cases the supervision differs from the  %classical setting because it is given implicitly in the response of the dialog partner during  %the conversation. We study this setup in two domains:  the bAbI dataset of  and large-scale                               question answering from .  We evaluate a set of baseline learning strategies  on these tasks, and show that a novel model incorporating predictive lookahead is a promising  %research  direction approach  for learning from a teacher's response. In particular, a surprising result is that it can learn to answer questions correctly without any reward-based supervision at all. 
 A speaker cluster-based speaker adaptive training  method under deep neural network-hidden Markov model  framework is presented in this paper. During training, speakers that are acoustically adjacent to each other are hierarchically clustered using an i-vector based distance metric. DNNs with speaker dependent layers are then adaptively trained for each cluster of speakers. Before decoding starts, an unseen speaker in test set is matched to the closest speaker cluster through comparing i-vector based distances. The previously trained DNN of the matched speaker cluster is used for decoding utterances of the test speaker.  The performance of the proposed method on a large vocabulary spontaneous speech recognition task is evaluated on a training set of with 1500 hours of speech, and a test set of 24 speakers with 1774 utterances. Comparing to a speaker independent DNN with a baseline word error rate of 11.6\%, a relative  6.8\% reduction in word error rate is observed from the proposed method.   
   Learning and generating Chinese poems is a charming yet challenging task. Traditional approaches involve various language modeling and machine translation techniques, however, they perform not as well when generating poems with complex pattern constraints, for example Song iambics, a famous type of poems that involve variable-length sentences and strict rhythmic patterns.    This paper applies the attention-based sequence-to-sequence model to generate Chinese Song iambics. Specifically, we encode the cue sentences by a bi-directional Long-Short Term Memory  model and then predict the entire iambic with the information provided by the encoder, in the form of an attention-based LSTM that can regularize the generation process by the fine structure of the input cues. Several techniques are investigated to improve the model, including global context integration, hybrid style training, character vector initialization and adaptation. Both the automatic and subjective evaluation results show that our model indeed can learn the complex structural and rhythmic patterns of Song iambics, and the generation is rather successful. 
We present a new resource for Swedish, SweLL, a corpus of Swedish Learner essays linked to learnersé–³ performance according to the Common European Framework of Reference . SweLL consists of three subcorpora é–³ SpIn, SW1203 and Tisus, collected from three different educational establishments. The common metadata for all subcorpora includes age, gender, native languages, time of residence in Sweden, type of written task. Depending on the subcorpus, learner texts may contain additional information, such as text genres, topics, grades.  Five of the six CEFR levels are represented in the corpus: A1, A2, B1, B2 and C1 comprising in total 339 essays. C2 level is not included since courses at C2 level are not offered. The work flow consists of collection of essays and permits, essay digitization and registration, meta-data annotation, automatic linguistic annotation. Inter-rater agreement is presented on the basis of SW1203 subcorpus. The work on SweLL is still ongoing with more that 100 essays waiting in the pipeline. This article both describes the resource and the é–³ãƒ¦ç¬ow-toé–³ behind the compilation of SweLL. \newline \newline \Keywords{CEFR levels, learner corpus, digital resources for second language research
 % Recently, the long short-term memory neural network   has attracted wide interest due to its success in many tasks. LSTM architecture consists of a memory cell and three gates, which looks similar to the neuronal networks in the brain. However, there still lacks the evidence of the cognitive plausibility of LSTM architecture as well as its working mechanism. In this paper, we study the cognitive plausibility of LSTM by aligning its internal architecture with the brain activity observed via fMRI when the subjects read a story. Experiment results show that the artificial memory vector in LSTM can accurately predict the observed sequential brain activities, indicating the correlation between LSTM architecture and the cognitive process of story reading. % 
 Spammer detection on social network is a challenging problem. The rigid anti-spam rules have resulted in emergence of "smart" spammers. They resemble legitimate users who are difficult to identify. In this paper, we present a novel spammer classification approach based on Latent Dirichlet Allocation , a topic model. Our approach extracts both the local and the global information of topic distribution patterns, which capture the essence of spamming. Tested on one benchmark dataset and one self-collected dataset, our proposed method outperforms other state-of-the-art methods in terms of averaged F1-score.  
 Aspect phrase grouping is an important task in aspect-level sentiment analysis. It is a challenging problem due to polysemy and context dependency. We propose an Attention-based Deep Distance Metric Learning  method, by considering aspect phrase representation as well as context representation. First, leveraging the characteristics of the review text, we automatically generate aspect phrase sample pairs for distant supervision. Second, we feed word embeddings of aspect phrases and their contexts into an attention-based neural network to learn feature representation of contexts. Both aspect phrase embedding and context embedding are used to learn a deep feature subspace for measure the distances between aspect phrases for K-means clustering. Experiments on four review datasets show that the proposed method outperforms state-of-the-art strong baseline methods. 
  Query relevance ranking and sentence saliency ranking are the two main tasks in extractive query-focused summarization.    Previous supervised summarization systems often perform the two tasks in isolation.  However, since reference summaries are the trade-off between relevance and saliency, using them as supervision, neither of the two rankers could be trained well.  This paper proposes a novel summarization system called AttSum, which tackles the two tasks jointly.  It automatically learns distributed representations for sentences as well as the document cluster.    Meanwhile, it applies the attention mechanism to simulate the attentive reading of human behavior when a query is given.  Extensive experiments are conducted on DUC query-focused summarization benchmark datasets.    Without using any hand-crafted features, AttSum achieves competitive performance. We also observe that the sentences recognized to focus on the query indeed meet the query need.  
 We propose Sentence Level Recurrent Topic Model , a new topic model that assumes the generation of each word within a sentence to depend on both the topic of the sentence and the whole history of its preceding words in the sentence. Different from conventional topic models that largely ignore the sequential order of words or their topic coherence, SLRTM gives full characterization to them by using a Recurrent Neural Networks  based framework. Experimental results have shown that SLRTM outperforms several strong baselines on various tasks. Furthermore, SLRTM can automatically generate sentences given a topic , which is a key technology for real world applications such as personalized short text conversation. 
 Recurrent neural networks , including long short-term memory  RNNs, have produced state-of-the-art results on a variety of speech recognition tasks. However, these models are often too large in size for deployment on mobile devices with memory and latency constraints. In this work, we study mechanisms for learning compact RNNs and LSTMs via low-rank factorizations and parameter sharing schemes. Our goal is to investigate redundancies in recurrent architectures where compression can be admitted without losing performance. A hybrid strategy of using structured matrices in the bottom layers and shared low-rank factors on the top layers is found to be particularly effective, reducing the parameters of a standard LSTM by 75\%, at a small cost of 0.3\% increase in WER, on a 2,000-hr English Voice Search task. 
 We develop a natural language interface for human robot interaction that implements reasoning about deep semantics in natural language. To realize the required deep analysis, we employ methods from cognitive linguistics, namely the modular and compositional framework of Embodied Construction Grammar  . Using ECG, robots are able to solve fine-grained reference resolution problems and other issues related to deep semantics and compositionality of natural language.  This also includes verbal interaction with humans to clarify commands and queries that are too ambiguous to be executed safely.  We implement our NLU framework as a ROS package and present proof-of-concept scenarios with different robots, as well as a survey on the state of the art.  
 We present a new tool for training neural network language models , scoring sentences, and generating text. The tool has been written using Python library Theano, which allows researcher to easily extend it and tune any aspect of the training process. Regardless of the flexibility, Theano is able to generate extremely fast native code that can utilize a GPU or multiple CPU cores in order to parallelize the heavy numerical computations. The tool has been evaluated in difficult Finnish and English conversational speech recognition tasks, and significant improvement was obtained over our best back-off n-gram models. The results that we obtained in the Finnish task were compared to those from existing RNNLM and RWTHLM toolkits, and found to be as good or better, while training times were an order of magnitude shorter. 
   Dropped pronouns  are ubiquitous in pro-drop languages like Chinese, Japanese etc.   Previous work mainly focused on painstakingly exploring the empirical features for DPs recovery.   In this paper, we propose a neural recovery machine  to model and recover DPs in Chinese, so that to avoid the non-trivial feature engineering process.   The experimental results show that the proposed NRM significantly outperforms the state-of-the-art approaches on both two heterogeneous datasets.   Further experiment results of Chinese zero pronoun  resolution show that the performance of ZP resolution can also be improved by recovering the ZPs to DPs. 
 In this paper, we enhance the attention-based neural machine translation  by adding explicit coverage embedding models to alleviate  issues of repeating and dropping translations in NMT. For each source word, our model starts with a { status, and then keeps updating it with neural networks as the translation goes. Experiments on the large-scale Chinese-to-English task show that our enhanced model improves the translation quality  significantly on various test sets over the strong large vocabulary NMT system.  
 In order to capture rich language phenomena, neural machine translation models have to  use a large vocabulary size, which requires high computing time and large memory usage. In this paper, we alleviate this issue by introducing  a sentence-level or batch-level vocabulary, which is only a very small sub-set of  the full output vocabulary. For each sentence or batch, we only predict the target words in  its sentence-level or batch-level vocabulary. Thus, we reduce both the computing time and the memory usage. Our method simply takes into account the  translation options of each word or phrase in the source sentence, and   picks a very small target vocabulary for each sentence %or mini-batch  based on a word-to-word translation model or a bilingual phrase library learned  from a traditional machine translation model. Experimental results on the large-scale English-to-French task show that our method  achieves better translation performance by 1 BLEU point over the large vocabulary neural machine translation system of . 
 Machine comprehension plays an essential role in NLP and has been widely explored with dataset like MCTest.~However, this dataset is too simple and too small for learning true reasoning abilities.~ therefore release a large scale news article dataset and propose a deep LSTM reader system for machine comprehension. However, the training process is expensive. We therefore try feature-engineered approach with semantics on the new dataset to see how traditional machine learning technique and semantics can help with machine comprehension. Meanwhile, our proposed L2R reader system achieves good performance with efficiency and less training data. 
 We introduce polyglot language models, recurrent neural network models trained to predict symbol sequences in many different languages using shared representations of symbols and conditioning on typological information about the language to be predicted. We apply these to the problem of modeling phone sequences---a domain in which universal symbol inventories and cross-linguistically shared feature representations are a natural fit. Intrinsic evaluation on held-out perplexity, qualitative analysis of the learned representations, and extrinsic evaluation in two downstream applications that make use of phonetic features show  that polyglot models better generalize to held-out data than comparable monolingual models and  that polyglot phonetic feature representations are of higher quality than those learned monolingually. 
     Recent advances in conditional recurrent language modelling have mainly     focused on network architectures , learning     algorithms  and novel     applications  On the other hand, we notice that decoding algorithms/strategies have     not been investigated as much, and it has become standard to use greedy or     beam search. In this paper, we propose a novel decoding strategy motivated     by an earlier observation that nonlinear hidden layers of a deep neural     network stretch the data manifold. The proposed strategy is embarrassingly     parallelizable without any communication overhead, while improving an     existing decoding algorithm. We extensively evaluate it with attention-based     neural machine translation on the task of En$\to$Cz translation. 
 We use Bayesian optimization to learn curricula for word representation learning, optimizing performance on downstream tasks that depend on the learned representations as features. The curricula are modeled by a linear ranking function which is the scalar product of a learned weight vector and an engineered feature vector that characterizes the different aspects of the complexity of each instance in the training corpus. We show that learning the curriculum improves performance on a variety of downstream tasks over random orders and in comparison to the natural corpus order. 
   We propose a novel framework for the analysis of learning algorithms that allows us to say when such algorithms can and cannot generalize certain patterns from training data to test data. In particular we focus on situations where the rule that must be learned concerns two components of a stimulus being identical. We call such a basis for discrimination an {} rule. Identity-based rules have proven to be  difficult or impossible  for certain types of learning algorithms to acquire from limited datasets. This is in contrast to human behaviour on similar tasks. Here we provide a framework for rigorously establishing which learning algorithms will fail at generalizing identity-based rules to novel stimuli.  We use this framework to show that such algorithms are unable to generalize identity-based rules to novel inputs unless trained on virtually all possible inputs. We demonstrate these results computationally with a multilayer feedforward neural network.   %We propose a novel framework for the analysis of learning algorithms that allows us to say when such algorithms can and cannot generalize certain patterns from training data to test data. %In particular we focus on situations where the rule that must be learned concerns two components of a stimulus being identical. %We call such a basis for discrimination an {} rule. %Identity-based rules have proven to be  difficult or impossible  for certain types of learning algorithms to acquire from limited datasets. This is in contrast to human behaviour on similar tasks. %Here we provide a framework for rigorously establishing which learning algorithms will fail at generalizing identity-based rules to novel stimuli.  %We develop a theory of symmetries of inputs, training sets, and algorithms, and show that if an algorithm has a particular symmetry that is shared by the data it is trained on, it cannot generalize to novel inputs in a human-like way. We use this theory to show that such algorithms are unable to generalize identity-based rules to novel inputs unless trained on virtually all possible inputs. We demonstrate these results using a multilayer feedforward neural network, using both localist and distributed encodings.    %first on a simple two-layer feed-forward neural network, and then on a state-of-the-art deep neural network.  % %We apply our theory to show that a certain class of learning algorithms cannot learn  without a nearly complete set of inputs. Here identity effects are situations where an input being classified as grammatical  in some way hinges on whether to components of the input are identical. % %% unlike the behaviour of human subjects. In particular, connectionist algorithms or any type of algorithm that  % %{ without a nearly complete set of inputs. Here identity effects are situations where an input being classified as grammatical  in some way hinges on whether to components of the input are identical. % %{. The algorithms are unable to generalize to outside the training set. %I provide an explicit criterion that means an algorithm cannot learn an identify effect. The result is a consequence of a larger theory of training sets, algorithms, and grammars. % %  Keywords:  phonology; learning algorithms; symmetries; connectionism 
 We introduce the Treebank of Learner English ,  the first publicly available syntactic treebank for  English as a Second Language .  The TLE provides manually annotated POS tags and Universal Dependency   trees for 5,124 sentences from the Cambridge First Certificate  in English  corpus. The UD annotations are tied to  a pre-existing error annotation of the FCE, whereby full  syntactic analyses are provided for both the original and  error corrected versions of each sentence. Further on, we  delineate ESL annotation guidelines that allow for consistent  syntactic treatment of ungrammatical English. Finally, we  benchmark POS tagging and dependency parsing performance on  the TLE dataset and measure the effect of grammatical errors  on parsing accuracy. We envision the treebank to support a  wide range of linguistic and computational research on second  language acquisition as well as automatic processing of  ungrammatical language\footnote{The treebank is available at   universaldependencies.org. The annotation manual used in this project and a graphical query engine are available at esltreebank.org.}. 
  We present a new Convolutional Neural Network  model for text classification that jointly exploits labels on documents and their constituent sentences. Specifically, we consider scenarios in which annotators explicitly mark sentences  that support their overall document categorization, i.e., they provide . Our model exploits such supervision via a hierarchical approach in which each document is represented by a linear combination of the vector representations of its component sentences. We propose a sentence-level convolutional model that estimates the probability that a given sentence is a rationale, and we then scale the contribution of each sentence to the aggregate document representation in proportion to these estimates. Experiments on five classification datasets that have document labels and associated rationales demonstrate that our approach consistently outperforms strong baselines. Moreover, our model naturally provides explanations for its predictions.  % Exploiting rationales can improve the performance of CNNs for text classification by up to $\sim$5\% in absolute accuracy while . %This is the first work to incorporate rationales into neural models for text classification.   
 We investigate the use of hierarchical phrase-based SMT lattices in end-to-end neural machine translation .    Weight pushing transforms the Hiero scores for complete translation hypotheses, with the full translation grammar score and full n-gram language model score,  into posteriors compatible with NMT predictive probabilities.   With a slightly modified NMT beam-search decoder  we find gains over both Hiero and NMT decoding alone, with practical advantages in extending NMT to very large input and output vocabularies.  
 	We consider the problem of Recognizing Textual Entailment 	within an Information Retrieval context, where we must simultaneously 	determine the relevancy as well as degree of entailment for individual 	pieces of evidence to determine a yes/no answer to a binary 	natural language question.  	We compare several variants of neural networks for sentence embeddings 	in a setting of decision-making based on evidence of varying relevance. 	We propose a basic model to integrate evidence for entailment, 	show that joint training of the sentence embeddings to model 	relevance and entailment is feasible even with no explicit per-evidence 	supervision, and show the importance of evaluating strong baselines. 	We also demonstrate the benefit of carrying over text comprehension model 	trained on an unrelated task for our small datasets.  	Our research is motivated primarily by a new open dataset we introduce, 	consisting of binary questions and news-based evidence snippets. 	We also apply the proposed relevance-entailment model on a similar task 	of ranking multiple-choice test answers, evaluating it on 	a preliminary dataset of school test questions as well as 	the standard MCTest dataset, where we improve the neural model state-of-art. 
 This paper describes the submission of the AMU  team to the Automatic Post-Editing  task of WMT 2016. We explore the application of neural translation models to the APE problem and achieve good results by treating different models as components in a log-linear model, allowing for multiple inputs  that are decoded to the same target language . A simple string-matching penalty integrated within the log-linear model is used to control for higher faithfulness with regard to the raw machine translation output. To overcome the problem of too little training data, we generate large amounts of artificial data. Our submission improves over the uncorrected baseline on the unseen test set by -3.2\% TER and +5.5\% BLEU and outperforms any other system submitted to the shared-task by a large margin. 
 Neural network based methods have obtained great progress on a variety of natural language processing tasks. However, in most previous works, the models are learned based on single-task supervised objectives, which often suffer from insufficient training data. In this paper, we use the multi-task learning framework to jointly learn across multiple related tasks. Based on recurrent neural network, we propose three different mechanisms of sharing information to model text with task-specific and shared layers. The entire network is trained jointly on all these tasks. Experiments on four benchmark text classification tasks show that our proposed models can improve the performance of a task with the help of other related tasks. 
 In this paper, we explore the use of convolutional networks   for the purpose of cognate identification. We compare our architecture with binary  classifiers based on string similarity measures on different language families. Our experiments  show that convolutional networks achieve competitive results across concepts and across language  families at the task of cognate identification.  % We train our convolutional networks on words of  % short length and linguistically informed phonetic features. 
   Recent work in learning vector-space embeddings for multi-relational data  has focused on combining relational information derived from knowledge bases with distributional information derived from large text corpora.  We propose a simple approach that leverages the descriptions of entities or phrases available in lexical resources, in conjunction with distributional semantics, in order to derive a better initialization for training relational models. Applying this initialization to the TransE model results in significant new state-of-the-art performances on the WordNet dataset, decreasing the mean rank from the previous best of 212 to 51. It also results in faster convergence of the entity representations.  We find that there is a trade-off between improving the mean rank and the hits@10 with this approach. This illustrates that much remains to be understood regarding performance improvements in relational models.  %, and leads to . , and can be used in combination with other recent work on structural improvements to relational models. 
 This paper investigates two different neural architectures for the task of relation classification: convolutional neural networks and recurrent neural networks. For both models, we demonstrate the effect of different architectural choices. We present a new context representation for convolutional neural networks for relation  classification . Furthermore, we propose connectionist bi-directional recurrent neural networks and introduce ranking loss for their optimization. Finally, we show that combining convolutional and recurrent neural networks using  a simple voting scheme is accurate enough to improve results.  Our neural models achieve state-of-the-art results on the SemEval 2010 relation classification task. 
 This paper introduces a novel model for semantic role labeling that makes use of neural sequence modeling techniques. Our approach is motivated by the observation that complex syntactic structures and related phenomena, such as nested subordinations and nominal predicates, are not handled well by existing models. Our model treats such instances as sub-sequences of lexicalized dependency paths and learns suitable embedding representations. We experimentally demonstrate that such embeddings can improve results over previous state-of-the-art semantic role labelers, and showcase qualitative improvements obtained by our method. 
  The ability to compute an accurate reward function is essential for optimising a dialogue policy via reinforcement learning. In real-world applications, using explicit user feedback as the reward signal is often unreliable and costly to collect. This problem can be mitigated if the user's intent is known in advance or data is available to pre-train a task success predictor off-line. In practice neither of these apply for most real world applications.  Here we propose an on-line learning framework whereby the dialogue policy is jointly trained  alongside the reward model via active learning with a Gaussian process model. This Gaussian process operates on a continuous space dialogue representation generated in an unsupervised fashion using a recurrent neural network encoder-decoder. The experimental results demonstrate that the proposed framework is able to significantly reduce data annotation costs and mitigate noisy user feedback in dialogue policy learning.  
   Traditional dialog systems used in goal-oriented applications   require a lot of domain-specific handcrafting, which hinders scaling up to new   domains.  End-to-end dialog systems, in which all components are trained from   the dialogs themselves, escape this limitation.  But the   encouraging success recently obtained in chit-chat dialog may not carry over to   goal-oriented settings.  This paper proposes a testbed to break down   the strengths and shortcomings of end-to-end dialog systems in goal-oriented   applications.   %   Set in the context of restaurant reservation, our tasks require   manipulating sentences and symbols in order to properly conduct   conversations, issue API calls and use the outputs of such calls.   %   We show that an end-to-end dialog system based on Memory Networks   can reach promising, yet imperfect, performance and learn to perform   non-trivial operations. We confirm those results by comparing our   system to a hand-crafted slot-filling baseline on data from the   second Dialog State Tracking Challenge .   We show similar result patterns on data extracted from   an online concierge service. 
 Models of neural machine translation are often from a discriminative family of encoder-decoders that learn a conditional distribution of a target sentence given a source sentence. In this paper, we propose a variational model to learn this conditional distribution for neural machine translation: a variational encoder-decoder model that can be trained end-to-end. Different from the vanilla encoder-decoder model that generates target translations from hidden representations of source sentences alone, the variational model introduces a { conditioned on both the source and the target sides, %. Additionally, we employ  and equip it with a reparameterization technique to estimate the variational lower bound.  %so as to enable standard stochastic gradient optimization and large-scale training for the proposed model.  Experiments on both Chinese-English and English-German translation tasks show that the proposed variational neural machine translation achieves significant improvements over the vanilla neural machine translation baselines.  
 In this paper, we propose a bidimensional attention based recursive autoencoder  to integrate clues and source-target interactions at multiple levels of granularity into bilingual phrase representations. We employ recursive autoencoders to generate tree structures of phrases with embeddings at different levels of granularity . Over these embeddings on the source and target side, we introduce a {, from which we extract two soft attention weight distributions simultaneously. These weight distributions enable BattRAE to generate compositive phrase representations via convolution. Based on the learned phrase representations, we further use a bilinear neural model, trained via a max-margin method, to measure bilingual semantic similarity. To evaluate the effectiveness of BattRAE, we incorporate this semantic similarity as an additional feature into a state-of-the-art SMT system. Extensive experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.63 BLEU points on average over the baseline. 
 Previous studies in Open Information Extraction  are mainly based on extraction patterns. They manually define patterns or automatically learn them from a large corpus. However, these approaches are limited when grasping the context of a sentence, and they fail to capture implicit relations. In this paper, we address this problem with the following methods. First, we exploit long short-term memory  networks to extract higher-level features along the shortest dependency paths, connecting headwords of relations and arguments. The path-level features from LSTM networks provide useful clues regarding contextual information and the validity of arguments. Second, we constructed samples to train LSTM networks without the need for manual labeling. In particular, feedback negative sampling picks highly negative samples among non-positive samples through a model trained with positive samples. The experimental results show that our approach produces more precise and abundant extractions than state-of-the-art open IE systems. To the best of our knowledge, this is the first work to apply deep learning to Open IE. 
 Large-scale automated  meta-analysis of neuroimaging data has recently established itself as an important tool in  advancing our understanding of human brain function. %the study of the human  % connectome.  This research has been pioneered by NeuroSynth, a database collecting both brain activation coordinates and %activation and  associated text across a large cohort of  neuroimaging research papers.  One of the fundamental aspects of such meta-analysis is text-mining.  % Word counts and more sophisticated  % methods such as LDA have been proposed To date, word counts and more sophisticated methods such as  Latent Dirichlet Allocation have been proposed.  In this work we present an unsupervised study of the NeuroSynth text corpus using Deep Boltzmann Machines .  The use of DBMs yields several advantages over the aforementioned methods,  principal among which is the fact that it yields both word and document  embeddings in a high-dimensional vector space.  Such embeddings serve to facilitate the use of traditional machine learning techniques  on the text corpus.  The proposed DBM model is shown to learn  embeddings with a clear semantic structure.   
 % We propose a novel module, the reviewer module, to improve the encoder-decoder learning framework. % We propose a novel architecture, the Review Network, to improve the encoder-decoder learning framework.  We propose a novel extension of the encoder-decoder framework, called a review network. The review network is generic and can enhance any existing encoder-decoder model: in this paper, we consider RNN decoders with both CNN and RNN encoders. The review network performs a number of review steps with attention mechanism on the encoder hidden states, and outputs a thought vector after each review step; the thought vectors are used as the input of the attention mechanism in the decoder. We show that conventional encoder-decoders are a special case of our framework. Empirically, we show that our framework improves over state-of-the-art encoder-decoder systems on the tasks of image captioning and source code captioning.\footnote{Code and data available at \url{https://github.com/kimiyoung/review_net}.}  % The reviewer module is generic, and can be plugged into an existing encoder-decoder model. The reviewer module performs a number of review steps with attention mechanism on the encoder hidden states, and outputs a fact vector after each review step; the fact vectors are used as the input of the attention mechanism in the decoder. We show that the conventional encoder-decoders are a special case of our framework. Empirically, we show that our framework can improve over state-of-the-art encoder-decoder systems on the tasks of image captioning and source code captioning. 
 This technical report details several improvements to the visual concept detector banks built on images from the Multilingual Visual Sentiment Ontology  . The detector banks are trained to detect a total of 9,918 sentiment-biased visual concepts from six major languages: English, Spanish, Italian, French, German and Chinese. In the original MVSO release , adjective-noun pair  detectors were trained for the six languages using an AlexNet-styled architecture  by fine-tuning from DeepSentiBank . Here, through a more extensive set of experiments, parameter tuning, and training runs, we detail and release higher accuracy models for detecting ANPs across six languages from the same image pool and setting as in the original release using a more modern architecture, GoogLeNet , providing comparable or better performance with reduced network parameter cost.  In addition, since the image pool in MVSO  can be corrupted by user noise from social interactions, we partitioned out a sub-corpus of MVSO images based on tag-restricted queries for higher fidelity labels. We show that as a result of these higher fidelity labels, higher performing AlexNet-styled  ANP detectors can be trained using the tag-restricted image subset as compared to the models in full corpus. We release all these newly trained models for public research use along with the list of tag-restricted images from the MVSO dataset. 
 Attention mechanisms have recently been introduced in deep learning for various tasks in natural language processing and computer vision. But despite their popularity, the ``correctness'' of the implicitly-learned attention maps has only been assessed qualitatively by visualization of several examples. In this paper we focus on evaluating and improving the correctness of attention in neural image captioning models. Specifically, we propose a quantitative evaluation metric for the consistency between the generated attention maps and human annotations, using recently released datasets with alignment between regions in images and entities in captions. We then propose novel models with different levels of explicit supervision for learning attention maps during training. The supervision can be strong when alignment between regions and caption entities are available, or weak when only object segments and categories are provided. We show on the popular Flickr30k and COCO datasets that introducing supervision of attention maps during training solidly improves both attention correctness and caption quality, showing the promise of making machine perception more human-like.  
  Memory networks are neural networks with an explicit memory component that can be both read and written to by the network. The memory is often addressed in a soft way using a softmax function, making end-to-end training with backpropagation possible. However, this is not computationally scalable for applications which require the network to read from extremely large memories. On the other hand, it is well known that hard attention mechanisms based on reinforcement learning are challenging to train successfully. In this paper, we explore a form of hierarchical memory network, which can be considered as a hybrid between hard and soft attention memory networks. The memory is organized in a hierarchical structure such that reading from it is done with less computation than soft attention over a flat memory, while also being easier to train than hard attention over a flat memory. Specifically, we propose to incorporate Maximum Inner Product Search  in the training and inference procedures for our hierarchical memory network. We explore the use of various state-of-the art approximate MIPS techniques and report results on SimpleQuestions, a challenging large scale factoid question answering task. 
 Phrase-based statistical machine translation  systems have previously been used for the task of grammatical error correction  to achieve state-of-the-art accuracy. The superiority of SMT systems comes from their ability to learn text transformations from erroneous to corrected text, without explicitly modeling error types. However, phrase-based SMT systems suffer from limitations of discrete word representation, linear mapping, and lack of global context. In this paper, we address these limitations by using two different yet complementary neural network models, namely a neural network global lexicon model and a neural network joint model. These neural networks can generalize better by using continuous space representation of words and learn non-linear mappings. Moreover, they can leverage contextual information from the source sentence more effectively. By adding these two components, we achieve statistically significant improvement in accuracy for grammatical error correction over a state-of-the-art GEC system. 
 Language models  are statistical models that calculate probabilities over sequences of words or other discrete symbols. Currently two major paradigms for language modeling exist: count-based $n$-gram models, which have advantages of scalability and test-time speed, and neural LMs, which often achieve superior modeling performance. We demonstrate how both varieties of models can be unified in a single modeling framework that defines a set of probability distributions over the vocabulary of words, and then dynamically calculates mixture weights over these distributions. This formulation allows us to create novel hybrid models that combine the desirable features of count-based and neural LMs, and experiments demonstrate the advantages of these approaches.% \footnote{   Work was performed while GN was at the Nara Institute of Science and Technology and CD was at Carnegie Mellon University.   Code and data to reproduce experiments is available at \url{http://github.com/neubig/modlm} } 
 We present a method for generating synthetic versions of Twitter data using neural generative models. The goal is protecting individuals in the source data from stylometric re-identification attacks while still releasing data that carries research value. Specifically, we generate tweet corpora that maintain user-level word distributions by augmenting the neural language models with user-specific components. We compare our approach to two standard text data protection methods: redaction and iterative translation. We evaluate the three methods on measures of risk and utility. We define risk following the stylometric models of re-identification, and we define utility based on two general word distribution measures and two common text analysis research tasks. We find that neural models are able to significantly lower risk over previous methods with little cost to utility. We also demonstrate that the neural models allow data providers to actively control the risk-utility trade-off through model tuning parameters. This work presents promising results for a new tool addressing the problem of privacy for free text and sharing social media data in a way that respects privacy and is ethically responsible.  
 		 		Various treebanks have been released for dependency parsing. 		Despite that treebanks may belong to different languages or have different annotation schemes, they contain syntactic knowledge that is potential to benefit each other. 		This paper presents an universal framework for exploiting these multi-typed treebanks to improve parsing with deep multi-task learning. 		We consider two kinds of treebanks as source: the multilingual universal treebanks and the monolingual heterogeneous treebanks. 		Multiple treebanks are trained jointly and interacted with multi-level parameter sharing. 		Experiments on several benchmark datasets in various languages demonstrate that our approach can make effective use of arbitrary source treebanks to improve target parsing models. 		 	
 Authorship analysis  is the study of unveiling the hidden properties of authors from a body of exponentially exploding textual data. It extracts an author's identity and sociolinguistic characteristics based on the reflected writing styles in the text. It is an essential process for various areas, such as cybercrime investigation, psycholinguistics, political socialization, etc. However, most of the previous techniques critically depend on the manual feature engineering process. Consequently, the choice of feature set has been shown to be scenario- or dataset-dependent. In this paper, to mimic the human sentence composition process using a neural network approach, we propose to incorporate different categories of linguistic features into distributed representation of words in order to learn simultaneously the writing style representations based on unlabeled texts for authorship analysis. In particular, the proposed models allow topical, lexical, syntactical, and character-level feature vectors of each document to be extracted as stylometrics. We evaluate the performance of our approach on the problems of authorship characterization and authorship verification with the Twitter, novel, and essay datasets. The experiments suggest that our proposed text representation outperforms the bag-of-lexical-$n$-grams, Latent Dirichlet Allocation, Latent Semantic Analysis, PVDM, PVDBOW, and word2vec representations.                  
 This paper presents a model for end-to-end learning of task-oriented dialog systems.  The main component of the model is a recurrent neural network , which maps from raw dialog history directly to a distribution over system actions.  The LSTM automatically infers a representation of dialog history, which relieves the system developer of much of the manual feature engineering of dialog state.  In addition, the developer can provide software that expresses business rules and provides access to programmatic APIs, enabling the LSTM to take actions in the real world on behalf of the user.  The LSTM can be optimized using supervised learning , where a domain expert provides example dialogs which the LSTM should imitate; or using reinforcement learning , where the system improves by interacting directly with end users.  Experiments show that SL and RL are complementary: SL alone can derive a reasonable initial policy from a small number of training dialogs; and starting RL optimization with a policy trained with SL substantially accelerates the learning rate of RL. 
 In this paper we propose a neural conversation model for conducting dialogues. We demonstrate the use of this model to generate help desk responses, where users are asking questions about PC applications. Our model is distinguished by two characteristics. First, it models intention across turns with a recurrent network, and incorporates an attention model that is conditioned on the representation of intention.  Secondly, it avoids generating non-specific responses by incorporating an IDF term in the objective function. The model is evaluated both as a pure generation model in which a help-desk response is generated from scratch, and as a retrieval model with performance measured using recall rates of the correct response. Experimental results indicate that the model outperforms previously proposed neural conversation architectures, and that using specificity in the objective function significantly improves performances for both generation and retrieval.  
 { A long-standing challenge in coreference resolution has been the incorporation of entity-level information -- features defined over clusters of mentions instead of mention pairs. We present a neural network based coreference system that produces high-dimensional vector representations for pairs of coreference clusters. Using these representations, our system learns when combining clusters is desirable. We train the system with a learning-to-search algorithm that teaches it which local decisions  will lead to a high-scoring final coreference partition. The system substantially outperforms the current state-of-the-art on the English and Chinese portions of the  CoNLL 2012 Shared Task dataset despite using few hand-engineered features. } 
   This document contains the instructions for preparing a camera-ready   manuscript for the proceedings of EACL-2017. The document itself   conforms to its own specifications, and is therefore an example of   what your manuscript should look like. These instructions should be   used for both papers submitted for review and for final versions of   accepted papers.  Authors are asked to conform to all the directions   reported in this document. 
 We submitted two systems to the SemEval-2016 Task 12: Clinical TempEval challenge, participating in Phase 1, where we identified text spans of time and event expressions in clinical notes and Phase 2, where we predicted a relation between an event and its parent document creation time.   For temporal entity extraction, we find that a joint inference-based approach using structured prediction outperforms a vanilla recurrent neural network that incorporates word embeddings trained on a variety of large clinical document sets. For document creation time relations, we find that a combination of date canonicalization and distant supervision rules for predicting relations on both events and time expressions improves classification, though gains are limited, likely due to the small scale of training data.  
 Recent neural models of dialogue generation offer great promise for generating responses for conversational agents, but tend to be shortsighted, predicting utterances one at a time while ignoring their influence on future outcomes. Modeling the future direction of a dialogue is crucial to generating coherent, interesting dialogues, a need which led traditional NLP models of dialogue to draw on reinforcement learning. In this paper, we show how to integrate these goals, applying deep reinforcement learning to model future reward in chatbot dialogue. The model simulates dialogues between two virtual agents, using policy gradient methods to reward sequences that display three useful conversational properties: informativity, coherence, and ease of answering . We evaluate our model on diversity, length as well as with human judges,  showing that the proposed algorithm generates more interactive responses and manages to foster a more sustained conversation in dialogue simulation.  This work marks a first step towards learning a neural conversational model based on the long-term success of dialogues. %\footnote{Code to be released upon publication.}. 
 Discourse coherence is strongly associated with text quality, making it important to natural language generation and understanding. Yet existing models of coherence focus on measuring individual aspects of coherence  in narrow domains.  In this paper, we describe domain-independent neural models of discourse coherence that are capable of measuring multiple aspects of coherence  in existing sentences and can maintain coherence while generating new sentences. We study both discriminative models that learn to distinguish coherent from incoherent discourse, and generative models that produce coherent text, including a novel neural latent-variable Markovian generative model that  captures the latent discourse dependencies between sentences in a text.  Our work achieves state-of-the-art performance on multiple coherence evaluations, and marks an initial step in generating coherent texts given discourse contexts.  
 We introduce a recurrent neural network language model  with long short-term memory  units that utilizes both character-level and word-level inputs. Our model has a gate that adaptively finds the optimal mixture of the  character-level and word-level inputs.  The gate creates the final vector representation of a word by combining two distinct representations of the word. The character-level inputs are converted into vector representations of words using a bidirectional LSTM. The word-level inputs are projected into another high-dimensional space by a word lookup table.  The final vector representations of words are used in the LSTM language model which predicts the next word given all the preceding words. Our model with the gating mechanism effectively utilizes the character-level inputs for rare and out-of-vocabulary words and outperforms word-level language models on several English corpora.  
 The dominant approach for many NLP tasks are recurrent neural networks, in particular LSTMs, and convolutional neural networks. However, these architectures are rather shallow in comparison to the deep convolutional networks which have pushed the state-of-the-art in computer vision.  We present a new architecture  for text processing which operates directly at the character level and uses only small convolutions and pooling operations. % We are able to show that the performance of this model increases with the depth: using up to 29 convolutional layers, we report improvements over the state-of-the-art on several public text classification tasks.  To the best of our knowledge, this is the first time that very deep convolutional nets have been applied to text processing. 
 In this paper, we propose phraseNet, a neural machine translator with a phrase memory which stores phrase pairs in symbolic form, mined from corpus or specified by human experts. For any given source sentence, phraseNet scans the phrase memory to determine the candidate phrase pairs and integrates tagging information in the representation of source sentence accordingly. The decoder utilizes a mixture of word-generating component and phrase-generating component, with a specifically designed strategy to generate a sequence of multiple words all at once. The phraseNet not only approaches one step towards incorporating external knowledge into neural machine translation, but also makes an effort to extend the word-by-word generation mechanism of recurrent neural network. Our empirical study on Chinese-to-English translation shows that, with carefully-chosen phrase table in memory,  phraseNet yields 3.45 BLEU improvement over the generic neural machine translator.  
 Inferring implicit discourse relations in natural language text is the most difficult subtask in discourse parsing. Surface features achieve good performance, but they are not readily applicable to other languages without semantic lexicons. Previous neural models require parses, surface features, or a small label set to work well. Here, we propose neural network models that are based on feedforward and long-short term memory architecture without any surface features. To our surprise, our best configured feedforward architecture outperforms LSTM-based model in most cases despite thorough tuning. Under various fine-grained label sets and a cross-linguistic setting, our feedforward models perform consistently better or at least just as well as systems that require hand-crafted surface features. Our models present the first neural Chinese discourse parser in the style of Chinese Discourse Treebank, showing that our results hold cross-linguistically.  
 We propose to enhance the RNN decoder in a neural machine translator  with external memory, as a natural but powerful extension to the state in the decoding RNN. This memory-enhanced RNN decoder is called MemDec. At each time during decoding, MemDec will read from this memory and write to this memory once, both with content-based addressing. Unlike the unbounded memory in previous work to store the representation of source sentence, the memory in MemDec is a matrix with pre-determined size designed to better capture the information important for the decoding process at each time step. Our empirical study on Chinese-English translation shows that it can improve by $4.8$ BLEU upon Groundhog and $5.3$ BLEU upon on Moses, yielding the best performance achieved with the same training set.  
 	Neural machine translation  often makes mistakes in translating low-frequency content words that are essential to understanding the meaning of the sentence.     We propose a method to alleviate this problem by augmenting NMT systems with discrete translation lexicons that efficiently encode translations of these low-frequency words.     We describe a method to calculate the lexicon probability of the next word in the translation candidate by using the attention vector of the NMT model to select which source word lexical probabilities the model should focus on.      We test two methods to combine this probability with the standard NMT probability:  using it as a bias, and  linear interpolation.     Experiments on two corpora show an improvement of 2.0-2.3 BLEU and 0.13-0.44 NIST score, and faster convergence time.\footnote{Tools to replicate our experiments can be found at {http://isw3.naist.jp/\texttildelow philip-a/emnlp2016/index.html}} 
     We investigate the potential of attention-based neural machine translation     in simultaneous translation. We introduce a novel decoding algorithm, called     simultaneous greedy decoding, that allows an existing neural machine     translation model to begin translating before a full source sentence is     received. This approach is unique from previous works on     simultaneous translation in that segmentation and translation are done     jointly to maximize the translation quality and that translating each     segment is strongly conditioned on all the previous segments.  This paper     presents     a first step toward building a full simultaneous translation system based on     neural machine translation. 
 We propose a novel neural attention architecture to tackle machine comprehension tasks, such as answering Cloze-style queries with respect to a document. Unlike previous models, we do not collapse the query into a single vector, instead we deploy an iterative alternating attention mechanism that allows a fine-grained exploration of both the query and the document. Our model outperforms state-of-the-art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children's Book Test  dataset.  
 We describe a search algorithm for optimizing the number of latent states when estimating latent-variable PCFGs with spectral methods. Our results show that contrary to the common belief that the number of latent states for each nonterminal in an L-PCFG can be decided in isolation with spectral methods, parsing results significantly improve if the number of latent states for each nonterminal is globally optimized, while taking into account interactions between the different nonterminals. In addition, we contribute an empirical analysis of spectral algorithms on eight morphologically rich languages: Basque, French, German, Hebrew, Hungarian, Korean, Polish and Swedish. Our results show that our estimation consistently performs better or close to coarse-to-fine expectation-maximization techniques for these languages. %% consisting of nine treebanks, mostly taken from the workshop on %% Statistical Parsing of Morphologically Rich Languages.  
 This paper connects a vector-based composition model to a formal semantics, the Dependency-based Compositional Semantics . We show theoretical evidence  that the vector compositions in our model conform to the logic of DCS. Experimentally,  we show that vector-based composition brings a strong ability to calculate similar phrases as similar  vectors, achieving near state-of-the-art on a wide range of phrase similarity tasks and relation classification;  meanwhile, DCS can guide building vectors for structured queries that can be directly executed.  We evaluate this utility on sentence  completion task and report a new state-of-the-art.  
 In this paper we study different types of Recurrent Neural Networks  for sequence labeling tasks. We propose two new variants of RNNs integrating improvements for sequence labeling,  and we compare them to the more traditional Elman and Jordan RNNs. We compare all models, either traditional or new, on four distinct tasks of sequence labeling: two on Spoken Language Understanding ; and two of POS tagging for the French Treebank  and the Penn Treebank  corpora. The results show that our new variants of RNNs are always more effective than the others. 
     Neural machine translation has become a major alternative to widely used     phrase-based statistical machine translation. We notice however that much of     research on neural machine translation has focused on European languages     despite its language agnostic nature. In this paper, we apply neural machine     translation to the task of Arabic translation      and compare it against a standard phrase-based     translation system. We run extensive comparison using various configurations     in preprocessing Arabic script and show that the phrase-based and neural     translation systems perform comparably to each other and that proper     preprocessing of Arabic script has a similar effect on both of the systems.     We however observe that the neural machine translation significantly     outperform the phrase-based system on an out-of-domain test set, making it     attractive for real-world deployment.  
 We describe a two-step approach for dialogue management in task-oriented spoken dialogue systems. A unified neural network framework is proposed to enable the system to first learn by supervision from a set of dialogue data and then continuously improve its behaviour via reinforcement learning, all using gradient-based algorithms on one single model. The experiments demonstrate the supervised model's effectiveness in the corpus-based evaluation, with user simulation, and with paid human subjects. The use of reinforcement learning further improves the model's performance in both interactive settings, especially under higher-noise conditions.   
 %\fontsize{10}{11}\selectfont We study the problem of generating abstractive summaries for opinionated text. We propose an attention-based neural network model that is able to absorb information from multiple text units to construct informative, concise, and fluent summaries. An importance-based sampling method is designed to allow the encoder to integrate information from an important subset of input. Automatic evaluation indicates that our system outperforms state-of-the-art abstractive and extractive summarization systems on two newly collected datasets of movie reviews and arguments. Our system summaries are also rated as more informative and grammatical in human evaluation. %We study the problem of generating abstractive summaries for opinionated text. We propose to use a attention-based neural network model which is able to interpret information from multiple input text to construct informative, concise, and fluent summaries. Automatic evaluation on movie review summarization and claim generation for arguments indicates that our system can outperform state-of-the-art abstractive and extractive summarization systems. Human judges also rated our system summaries to be more informative and grammatical.  
  We participated in the WMT 2016 shared news translation task by building neural translation systems for four language pairs, each trained in both directions: English$\leftrightarrow$Czech, English$\leftrightarrow$German, English$\leftrightarrow$Romanian and English$\leftrightarrow$Russian. Our systems are based on an attentional encoder-decoder, using BPE subword segmentation for open-vocabulary translation with a fixed vocabulary. We experimented with using automatic back-translations of the monolingual News corpus as additional training data, pervasive dropout, and target-bidirectional models. All reported methods give substantial improvements, and we see improvements of 4.3--11.2 {}\footnote{We have released scripts, sample configs, synthetic training data and trained models: \url{https://github.com/rsennrich/wmt16-scripts}}  
   Sequence-to-Sequence  modeling has rapidly become an   important general-purpose NLP tool that has proven effective for   many text-generation and sequence-labeling tasks. Seq2seq builds on deep neural language modeling and inherits its   remarkable accuracy in estimating local, next-word   distributions. In this work, we introduce a model and beam-search training   scheme, based on the work of , that extends   seq2seq to learn global sequence scores. This   structured approach avoids classical biases associated with local   training and unifies the training loss with the test-time usage,   while preserving the proven model architecture of seq2seq and   its efficient training approach. We show that our system outperforms a   highly-optimized attention-based seq2seq system and other baselines   on three different sequence to sequence tasks: word ordering,   parsing, and machine translation. 
 Directly reading documents and being able to answer questions from them is an unsolved challenge. To avoid its inherent difficulty, question answering  has been directed towards using Knowledge Bases  instead, which has proven effective. Unfortunately KBs often suffer from being too restrictive, as the schema cannot support certain types of answers, and too sparse, e.g. Wikipedia contains much more information than Freebase. In this work we introduce a new method, Key-Value Memory Networks, that makes reading documents more viable by utilizing different encodings in the addressing  and output stages of the memory read operation. To compare using  KBs, information extraction or Wikipedia documents directly in a single framework we construct  an analysis tool, { benchmark. 
 We investigate the task of assessing sentence-level prompt relevance in learner essays. Various systems using word overlap, neural embeddings and neural compositional models are evaluated on two datasets of learner writing. We propose a new method for sentence-level similarity calculation, which learns to adjust the weights of pre-trained word embeddings for a specific task, achieving substantially higher accuracy compared to other relevant baselines.  
 In this paper, we propose to use deep policy networks which are trained with an advantage actor-critic method for statistically optimised dialogue systems. First, we show that, on summary state and action spaces, deep Reinforcement Learning  outperforms Gaussian Processes methods. Summary state and action spaces lead to good performance but require pre-engineering effort, RL knowledge, and domain expertise. In order to remove the need to define such summary spaces, we show that deep RL can also be trained efficiently on the original state and action spaces. Dialogue systems based on partially observable Markov decision processes are known to require many dialogues to train, which makes them unappealing for practical deployment. We show that a deep RL method based on an actor-critic architecture can exploit a small amount of data very efficiently. Indeed, with only a few hundred dialogues collected with a handcrafted policy, the actor-critic deep learner is considerably bootstrapped from a combination of supervised and batch RL. In addition, convergence to an optimal policy is significantly sped up compared to other deep RL methods initialized on the data with batch RL. All experiments are performed on a restaurant domain derived from the Dialogue State Tracking Challenge 2  dataset.  %Reinforcement learning is widely used to optimize statistical dialogue systems but it usually limited by a small manually defined state and action space. In addition, it usually requires a large amount of training data to get acceptable performance. In this paper, we first propose to use deep reinforcement learning to alleviate the limitation of small state and action space. Deep reinforcement learning such as deep Q network and deep actor-critic network provides a nice way to learn features directly from the raw feature space. In our experiment, it shows that they can achieve comparable performance as the GPSARSA method that works on a manually predefined summary space. To address the training efficiency problem, we propose to augment the deep reinforcement learning with supervised learning pretraining. In particular, a feedforward network based classifier is first trained to predict the machine's action from a dataset, and then this network is fine tuned with the reinforcement learning. It shows that the supervised learned policy can start with the similar performance as that achieved by 5000 dialogues of training from the reinforcement learning algorithm. And it can converge to a better policy overall.   %In this paper, we demonstrate how to optimize dialogue systems using policy networks and two stages of learning: supervised learning followed by reinforcement learning. First, to justify the use of deep reinforcement learning methodologies in general, we provide a comparison between the extensively used method of GPSARSA and DQN, demonstrating that DQN performs significantly better than GPSARSA with much lower wall-clock training time. In addition, deep reinforcement learning alleviates the need for summary state and action spaces, reducing the engineering effort. Then, we focus on policy approximation and apply supervised learning to initialize a policy network. The resulting policy is a considerably better starting point than a random policy. Besides, we show that using two-stage policy gradient reinforcement learning with an Advantage-Actor-Critic  method considerably speeds up convergence to an optimal policy compared to other deep reinforcement learning techniques such as Deep Q Networks  and double DQN.  All experiments are made on a restaurant domain derived from the dialogue state tracking challenge 2  dataset. 
 Convolutional neural networks  with convolutional and pooling operations along the frequency axis have been proposed to attain invariance to frequency shifts of features. However, this is inappropriate with regard to the fact that acoustic features vary in frequency. In this paper, we contend that convolution along the time axis is more effective. We also propose the addition of an intermap pooling  layer to deep CNNs. In this layer, filters in each group extract common but spectrally variant features, then the layer pools the feature maps of each group. As a result, the proposed IMP CNN can achieve insensitivity to spectral variations characteristic of different speakers and utterances. The effectiveness of the IMP CNN architecture is demonstrated on several LVCSR tasks. Even without speaker adaptation techniques, the architecture achieved a WER of 12.7\% on the SWB part of the Hub5\rq2000 evaluation test set, which is competitive with other state-of-the-art methods. 
  Recently a variety of LSTM-based conditional language models  have been applied across a range of language generation tasks. %However, less effort has been put in studying and interpreting the effectiveness of the various proposed architectures. In this work we study various model architectures and different ways to represent and aggregate the source information in an end-to-end neural dialogue system framework. A method called snapshot learning is also proposed to facilitate learning from supervised sequential signals by applying a companion cross-entropy objective function to the conditioning vector. The experimental and analytical results demonstrate firstly that competition occurs between the conditioning vector and the LM, and the differing architectures provide different trade-offs between the two. Secondly, the discriminative power and transparency of the conditioning vector is key to providing both model interpretability and better performance.  Thirdly, snapshot learning leads to consistent performance improvements independent of which architecture is used.  
 We propose a framework to improve performance of distantly-supervised relation extraction, by jointly learning to solve two related tasks: concept-instance extraction and relation extraction.  We combine this with a novel use of document structure: in some small, well-structured corpora, sections can be identified that correspond to relation arguments, and distantly-labeled examples from such sections tend to have good precision.  Using these as seeds we extract additional relation examples by applying label propagation on a graph composed of noisy examples extracted from a large unstructured testing corpus. Combined with the soft constraint that concept examples should have the same type as the second argument of the relation, we get significant improvements over several state-of-the-art approaches to distantly-supervised relation extraction.  
 We introduce an online popularity prediction and tracking task as a benchmark task for reinforcement learning with a combinatorial, natural language action space. A specified number of discussion threads predicted to be popular are recommended, chosen from a fixed window of recent comments to track. Novel deep reinforcement learning architectures are studied for effective modeling of the value function associated with actions comprised of  sub-actions. The proposed model, which represents dependence between sub-actions through a bi-directional LSTM, gives the best performance across different experimental configurations and domains, and it also generalizes well with varying numbers of recommendation requests. %We propose a Reddit popularity prediction and tracking task as a benchmark task for reinforcement learning with a natural language action space. Popular discussion threads are predicted and recommended, and recommending multiple new comments is a combinatorial action. Novel architectures are studied for better modeling value function associated with actions combined with interdependent sub-actions. The proposed DRRN-BiLSTM can not only perform better across different experimental configurations and various domains, but also generalize well when user request changes. % This paper presents a novel architecture for handling a combinatorial action space in reinforcement learning setting, with states and actions represented in natural language. In situations where a complex action is a combination of multiple sub-actions, we use a bi-directional Long Short-Term Memory  to model the action Q-value functions. The exponentially complexity of a combinatorial action space also poses challenges such as enumerating over all possible action choices. The proposed DRRN-BiLstm is able to capture the structure in order to learn an efficient function approximation. Experiments with Reddit popularity tracking task show that the model outperforms significantly over baseline Per-action DQN and simple addition of DRRN Q-values. 
  One of the core components of modern spoken dialogue systems is the belief tracker, which estimates the user's goal at every step of the dialogue. However, most current approaches have difficulty scaling to larger, more complex dialogue domains. This is due to their dependency on either: a) Spoken Language Understanding models that require large amounts of annotated training data; or b) hand-crafted lexicons for capturing some of the linguistic variation in users' language. We propose a novel Neural Belief Tracking  framework which overcomes these problems by building on recent advances in representation learning. NBT models reason over pre-trained word vectors, learning to compose them into distributed representations of user utterances and dialogue context. Our evaluation on two datasets shows that this approach surpasses past limitations, matching the performance of state-of-the-art models which rely on hand-crafted semantic lexicons and outperforming them when such lexicons are not provided.  % Abstract non-Latex: % One of the core components of modern spoken dialogue systems is the belief tracker, which estimates the user's goal at every step of the dialogue. However, most current approaches have difficulty scaling to larger, more complex dialogue domains. This is due to their dependency on either: a) Spoken Language Understanding models that require large amounts of annotated training data; or b) hand-crafted lexicons for capturing some of the linguistic variation in users' language. We propose a novel Neural Belief Tracking  framework which overcomes these problems by building on recent advances in representation learning. NBT models reason over pre-trained word vectors, learning to compose them into distributed representations of user utterances and dialogue context. Our evaluation on two datasets shows that this approach surpasses past limitations, matching the performance of state-of-the-art models which rely on hand-crafted semantic lexicons and outperforming them when such lexicons are not provided.  
 % The production of color language is essential for  grounded language generation. Color descriptions have many challenging properties: they can be vague, compositionally complex, and denotationally rich. We present an effective approach to generating color descriptions using recurrent neural networks and a Fourier-transformed color representation. Our model outperforms previous work on a conditional language modeling task over a large corpus of naturalistic color descriptions. In addition, probing the model's output reveals that it can accurately produce not only basic color terms but also descriptors with non-convex denotations , bare modifiers , and compositional phrases  not seen in training. 
 In an end-to-end dialog system, the aim of dialog state tracking is to accurately estimate a compact representation of the current dialog status from a sequence of noisy observations produced by the speech recognition and the natural language understanding modules. This paper introduces a novel method of dialog state tracking based on the general paradigm of machine reading and proposes to solve it using an End-to-End Memory Network, \memnn, a memory-enhanced neural network architecture. We evaluate the proposed approach on the second Dialog State Tracking Challenge  dataset. The corpus has been converted for the occasion in order to frame the hidden state variable inference as a question-answering task based on a sequence of utterances extracted from a dialog. We show that the proposed tracker gives encouraging results. Then, we propose to extend the DSTC-2 dataset and the definition of this dialog state task with specific reasoning capabilities like counting, list maintenance, yes-no question answering and indefinite knowledge management. Finally, we present encouraging results using our proposed \memnn based tracking model. 
 Prediction without justification has limited applicability. As a remedy, we learn to extract pieces of input text as justifications -- rationales -- that are tailored to be short and coherent, yet sufficient for making the same prediction. Our approach combines two modular components, generator and encoder, which are trained to operate well together. The generator specifies a distribution over text fragments as candidate rationales and these are passed through the encoder for prediction. Rationales are never given during training. Instead, the model is regularized by desiderata for rationales. We evaluate the approach on multi-aspect sentiment analysis against manually annotated test cases. Our approach outperforms attention-based baseline by a significant margin. We also successfully illustrate the method on the question retrieval task.\footnote{Our code and data are available at \url{https://github.com/taolei87/rcnn}.} 
     In this paper, we propose a novel finetuning algorithm for the recently     introduced multi-way, mulitlingual neural machine translate that enables     zero-resource machine translation. When used together with novel many-to-one     translation strategies, we empirically show that this finetuning algorithm     allows the multi-way, multilingual model to translate a zero-resource     language pair  as well as a single-pair neural translation model trained     with up to 1M direct parallel sentences of the same language pair and      better than pivot-based translation strategy, while keeping only one     additional copy of attention-related parameters. 
   Neural machine translation  aims at solving machine translation  \mbox{problems} using neural networks and has exhibited   promising results in recent years.  \mbox{However}, most of the existing NMT models are shallow and there is still a performance gap   between a \mbox{single} NMT model and the best \mbox{conventional} MT system.   In this work, we introduce a new type of linear   connections, named fast-forward connections, based on deep Long Short-Term Memory  networks, and an interleaved   bi-directional architecture for stacking the LSTM layers.  Fast-forward connections play an essential role in propagating the   gradients and building a deep topology of depth $16$. On the WMT'14 English-to-French task, we \mbox{achieve} BLEU=$37.7$ with a   single attention model, which outperforms the corresponding single shallow model by $6.2$ BLEU points. This is the first time that a   single NMT model achieves state-of-the-art performance and  outperforms the best conventional model by $0.7$ BLEU points. We can still   achieve BLEU=$36.3$ even without using an attention mechanism. After  special handling of unknown words and  model   ensembling,  we obtain the best score reported to date  on this task with BLEU=$40.4$. Our models are also  validated  on the more difficult WMT'14 English-to-German task.   %Moreover, when compared with conventional system, our model is especially good at long sequences. After considering a sub test set which   %doesn't have unknow words and includes more than half sequences of the whole set, our model gives BLEU$=41.4$. 
  We propose a new   method for text classification with convolutional neural networks . In AL, one selects the instances to be manually labeled with the aim of maximizing model performance with minimal effort. Neural models capitalize on word embeddings as representations , tuning these to the task at hand. We argue that AL strategies for multi-layered neural models should focus on selecting instances that most affect the embedding space . This is in contrast to traditional AL approaches , which specify higher level objectives.   We propose a simple approach for sentence classification that selects instances containing words whose embeddings are likely to be updated with the greatest magnitude, thereby rapidly learning discriminative, task-specific embeddings. We extend this approach to document classification by jointly considering:  the expected changes to the constituent word representations; and  the model's current overall uncertainty regarding the instance. The relative emphasis placed on these criteria is governed by a stochastic process that favors selecting instances likely to improve representations at the outset of learning, and then shifts toward general uncertainty sampling as AL progresses. Empirical results show that our method outperforms baseline AL approaches on both sentence and document classification tasks. We also show that, as expected, the method quickly learns discriminative word embeddings. To the best of our knowledge, this is the first work on AL addressing neural models for text classification.  %by introducing a stochastic interpolation between   %We also propose a stochastic method for document classification that firstly considers both the embedding layer and the final representation , but later will focus more on final representation. Empirical results show that our method outperforms baseline AL approaches on three sentence datasets and three document datasets. 
    Automated Text Scoring  provides a cost-effective and   consistent alternative to human marking. However, in order to   achieve good performance, the predictive features of the system need   to be manually engineered by human experts. We introduce a model   that forms word representations by learning the extent to which   specific words contribute to the text's score.  Using Long-Short   Term Memory networks to represent the meaning of texts, we   demonstrate that a fully automated framework is able to achieve   excellent results over similar approaches. In an attempt to make our   results more interpretable, and inspired by recent advances in   visualizing neural networks, we introduce a novel method for   identifying the regions of the text that the model has found more   discriminative.  
 Most previous approaches to Chinese word segmentation formalize this problem as a character-based sequence labeling task where only contextual information within fixed sized local windows and simple interactions between adjacent tags can be captured. In this paper, we propose a novel neural framework which thoroughly eliminates context windows and can utilize complete segmentation history. Our model employs a gated combination neural network over characters to produce distributed representations of word candidates, which are then given to a long short-term memory  language scoring model. Experiments on the benchmark datasets show that without the help of feature engineering as most existing approaches, our models achieve competitive or better performances with previous state-of-the-art methods. 
 % Recent successful approaches to end-to-end machine comprehension and question answering use multiple layers of sequential models, memory modules and attention mechanisms. What is common among most of these methods is summarizing the state of the entire world in each layer to guide the inference in the next layer. However, encoding inter-dependencies among a large set of facts into a single vector imposes a challenge. In this paper we introduce Query-ion Networks  that aim at logical ion of the query question as they observe facts across time. % QRNs are highly parallelizable and encode the inter-dependencies between facts without using recurrent neural networks. Our experimental results show that QRNs produce the state of the art results in bAbI question answering and are general enough to extend to other NLP tasks such as sentiment analysis.   %We present Query-Reduction Network , a variant of Recurrent Neural Network  that is suitable for question answering that requires multi-hop reasoning over a large context such as a story or a dialog. %While previous work largely relied on external memory and global softmax attention mechanism, QRN is a single recurrent unit with internal memory and local sigmoid attention. %Unlike most RNN-based models, QRN is able to effectively handle long-term dependencies and is highly parallelizable. %In our experiments we show that QRN obtains the state-of-the-art results in synthetic bAbI QA and dialog tasks, as well as in a real goal-oriented dialog dataset.  % %, and it is general enough to extend to other NLP tasks such as sentiment analysis.  In this paper, we study the problem of question answering when reasoning over multiple facts is required.  We propose Query-Reduction Network , a variant of Recurrent Neural Network  that effectively handles both short-term  and long-term  sequential dependencies to reason over multiple facts.  QRN considers the context sentences as a sequence of state-changing triggers, and  the original query to a more informed query as it observes each trigger  through time.  Our experiments show that QRN produces the state-of-the-art results in  bAbI QA and dialog tasks, and in a real goal-oriented dialog dataset. In addition, QRN formulation allows parallelization on RNN's time axis, saving an order of magnitude in time complexity for training and inference.    
   While end-to-end neural machine translation  has made remarkable progress recently, NMT systems only rely on parallel corpora for parameter estimation. Since parallel corpora are usually limited in quantity, quality, and coverage, especially for low-resource languages, it is appealing to exploit monolingual corpora to improve NMT. We propose a semi-supervised approach for training NMT models on the concatenation of labeled  and unlabeled  data. The central idea is to reconstruct the monolingual corpora using an autoencoder, in which the source-to-target and target-to-source translation models serve as the encoder and decoder, respectively. Our approach can not only exploit the monolingual corpora of the target language, but also of the source language. Experiments on the Chinese-English dataset show that our approach achieves significant improvements over state-of-the-art SMT and NMT systems. 
 We introduce an agreement-based approach to learning parallel lexicons and phrases from non-parallel corpora. The basic idea is to encourage two asymmetric latent-variable translation models  to agree on identifying latent phrase and word alignments. The agreement is defined at both word and phrase levels. We develop a Viterbi EM algorithm for jointly training the two unidirectional models efficiently. Experiments on the Chinese-English dataset show that agreement-based learning significantly improves both alignment and translation performance. 
 We present and evaluate a new model for Natural Language Generation  in Spoken Dialogue Systems, based on statistical planning, given noisy feedback from the current generation context .  We study its use in a standard NLG problem: how to present information  to users, given the complex trade-offs between utterance length, amount of information conveyed, and cognitive load. We set these trade-offs by analysing existing \match{} data.  We then train a NLG policy using Reinforcement Learning , which adapts its behaviour to noisy feedback from the current generation context. This policy is compared to several baselines derived from previous work in this area. The learned policy significantly outperforms all the prior approaches. 
  Word embeddings play a significant role in many modern NLP systems. Since learning one representation per word is problematic for polysemous words and homonymous words, researchers propose to use one embedding per word sense. Their approaches mainly train word sense embeddings on a corpus. In this paper, we propose to use word sense definitions to learn one embedding per word sense. Experimental results on word similarity tasks and a word sense disambiguation task show that word sense embeddings produced by our approach are of high quality.  
     Phonemic or phonetic sub-word units are the most commonly used atomic elements to represent speech signals in modern ASRs.     However they are not the optimal choice due to several reasons such as: large amount of effort required to handcraft a pronunciation dictionary, pronunciation variations, human mistakes and under-resourced dialects and languages.  Here, we propose a data-driven pronunciation estimation and acoustic modeling method which only takes the orthographic  transcription to jointly estimate a set of sub-word units and a reliable dictionary. Experimental results show that the proposed method     which is based on semi-supervised training of a deep neural network largely outperforms phoneme based continuous speech recognition on the TIMIT dataset.     % approach on TIMIT dataset on the continues speech recognition task.          % In speech recognition systems words are typically represented by smaller sub-word units such as phones. For both training and recognition phase the system normally use handcrafted pronunciation dictionary which map words into sequence of sub-word units. Building a dictionary however require linguistic experts and time consuming process. Moreover, the dictionary and sub-word units build on canonical utterance are usually not optimal for directed speech or specific environment for which speech recognition system is trained. This paper address automatically and jointly learning a sub-ward unit and pronunciation dictionary from a data.     % We assume the availability of word level transcription and present a methods to estimate a reliable pronunciation dictionary and sub-ward unit from multiple utterances of the word by means of maximum likelihood.     % Experimental results on TIMIT dataset shows that proposed method outperform commonly used phone based approach trained with manually designed dictionary and transcription. Modeling sub-ward unit with deep neural network, proposed method achieve more than 21\% relative improvement on continuous speech recognition task.   
 First-order factoid question answering assumes that the question can be answered by a single fact in a knowledge base . While this does not seem like a challenging task, many recent attempts that apply  either complex linguistic reasoning or deep neural networks achieve 65\%--76\% accuracy on benchmark sets. Our approach formulates the task as two machine learning problems:\ detecting the entities in the question, and classifying the question as one of the relation types in the KB. We train a recurrent neural network to solve each problem. On the SimpleQuestions dataset, our approach yields substantial improvements over previously published results --- even neural networks based on much more complex architectures. The simplicity of our approach also has practical advantages, such as efficiency and modularity, that are valuable especially in an industry setting. In fact, we present a preliminary analysis of the performance of our model on real queries from Comcast's X1 entertainment platform with millions of users every day. 
 Conventional word sense induction  methods usually represent each instance with discrete linguistic features or co-occurrence features, and train a model for each polysemous word individually.  In this work, we propose to learn sense embeddings for the WSI task.  %Here we learn sense embeddings for WSI task. In the training stage, our method induces several sense centroids  for each polysemous word.  In the testing stage, our method represents each instance as a contextual vector, and induces its sense by finding the nearest sense centroid in the embedding space.  The advantages of our method are   distributed sense vectors are taken as the knowledge representations which are trained discriminatively, and usually have better performance than traditional count-based distributional models, and  a general model for the whole vocabulary is jointly trained to induce sense centroids under the mutli-task learning framework.  % each instance is represented as a distributed context vector, which captures both lexical semantics and syntactic, and  % a general model is trained to induce sense centroids for all polysemous words under the mutli-task learning framework.   Evaluated on SemEval-2010 WSI dataset, our method outperforms all participants and most of the recent state-of-the-art methods. We further verify the two advantages by comparing with carefully designed baselines. 
 We present a natural language generator based on the sequence-to-sequence approach that can be trained to produce natural language strings as well as deep syntax dependency trees from input dialogue acts, and we use it to directly compare two-step generation with separate sentence planning and surface realization stages to a joint, one-step approach.  We were able to train both setups successfully using very little training data. The joint setup offers better performance, surpassing state-of-the-art with regards to $n$-gram-based scores while providing more relevant outputs.  
 Recruiters usually spend less than a minute looking at each r\'esum\'e when deciding whether it's worth continuing the recruitment process with the candidate. Recruiters focus on keywords, and it's almost impossible to guarantee a fair process of candidate selection. The main scope of this paper is to tackle this issue by introducing a data-driven approach that shows how to process r\'esum\'es automatically and give recruiters more time to only examine promising candidates. Furthermore, we show how to leverage Machine Learning and Natural Language Processing in order to extract all required information from the r\'esum\'es. Once the information is extracted, a ranking score is calculated. The score describes how well the candidates fit based on their education, work experience and skills. Later this paper illustrates a prototype application that shows how this novel approach can increase the productivity of recruiters. The application enables them to filter and rank candidates based on predefined job descriptions. Guided by the ranking, recruiters can get deeper insights from candidate profiles and validate why and how the application ranked them. This application shows how to improve the hiring process by giving an unbiased hiring decision support. 
 %This paper describes our approach for the  task . This effort placed us eighth out of 19 teams with macro-average precision, recall and F1-scores of $0.67$, $0.61$ and $0.635$ respectively. %We utilized recent advances in short text categorization using deep learning to create word-level and character-level models. The choice between word-level and character-level models in each particular case was informed through validation performance. Our final system is a combination of classifiers using word-level or character-level models. We also employed novel data augmentation techniques to expand and diversify our training dataset, thus making our system more robust.   This paper describes our approach for the  task . We utilized recent advances in short text categorization using deep learning to create word-level and character-level models. The choice between word-level and character-level models in each particular case was informed through validation performance. Our final system is a combination of classifiers using word-level or character-level models. We also employed novel data augmentation techniques to expand and diversify our training dataset, thus making our system more robust. Our system achieved a macro-average precision, recall and F1-scores of $0.67$, $0.61$ and $0.635$ respectively.    
 Word embedding, specially with its recent developments, promises a quantification of the similarity between terms. However, it is not clear to which extent this similarity value can be genuinely meaningful and useful for subsequent tasks. We explore how the similarity score obtained from the models is really indicative of term relatedness. We first observe and quantify the uncertainty factor of the word embedding models regarding to the similarity value. Based on this factor, we introduce a general threshold on various dimensions which effectively filters the highly related terms. Our evaluation on four information retrieval collections supports the effectiveness of our approach as the results of the introduced threshold are significantly better than the baseline while being equal to  or statistically indistinguishable from the optimal results.  
 %% no longer than 200 words This paper investigates neural character-based morphological tagging for languages with complex morphology and large tag sets.  We systematically explore a variety of neural architectures  to obtain character-based word vectors combined with bidirectional LSTMs to model across-word context in an end-to-end setting.  We explore supplementary use of word-based vectors trained on large amounts of unlabeled data.  Our experiments for morphological tagging suggest that for "simple" model configurations, the choice of the network architecture  or the augmentation with pre-trained word embeddings can be important and clearly impact the accuracy. Increasing the model capacity by adding depth, for example, and carefully optimizing the neural networks can lead to substantial improvements, and the differences in accuracy  become much smaller or even negligible.  Overall, our best morphological taggers for German and Czech outperform the best results reported in the literature by a large margin. 
 The performance of  systems under noisy environments still leaves room for improvement. Speech enhancement or feature enhancement techniques for increasing noise robustness of these systems usually add components to the recognition system that need careful optimization. In this work, we propose the use of a relatively simple curriculum training strategy called . It uses a multi-stage training schedule where samples at  values as low as 0dB are first added and samples at increasing higher  values are gradually added up to an  value of 50dB. We also use a method called  that generates noisy training samples online during training and thus enables dynamically changing the  of our training data. Both the  and the  methods are evaluated on a end-to-end speech recognition pipeline on the Wall Street Journal corpus.  decreases the average  on the 20dB to -10dB  range by up to 31.4\% when compared to a conventional multi-condition training method.   
 Recently, the rapid development of word embedding and neural networks has brought new inspiration to various NLP and IR tasks. In this paper, we describe a staged hybrid model combining Recurrent Convolutional Neural Networks  with highway layers. The highway network module is incorporated in the middle takes the output of the bidirectional Recurrent Neural Network  module in the first stage and provides the Convolutional Neural Network  module in the last stage with the input. The experiment shows that our model outperforms common neural network models  on a sentiment analysis task. Besides, the analysis of how sequence length influences the RCNN with highway layers shows that our model could learn good representation for the long text. 
 Recurrent neural networks, and in particular long short-term   memory  networks, are a remarkably effective tool for   sequence modeling that learn a dense black-box hidden   representation of their sequential input. Researchers interested in   better understanding these models have studied the changes in hidden   state representations over time and noticed some interpretable   patterns but also significant noise.  In this work, we present   \textsc{LSTMVis
     We present NN-grams, a novel, hybrid language model integrating n-grams and neural networks  for speech recognition.  The model takes as input both word histories as well as n-gram counts. Thus, it combines the memorization capacity and scalability of an n-gram model with the generalization ability of neural networks. We report experiments where the model is trained on 26B words. NN-grams are efficient at run-time since they do not include an output soft-max layer. The model is trained using noise contrastive estimation , an approach that transforms the estimation problem of neural networks into one of binary classification between data samples and noise samples. We present results with noise samples derived from either an n-gram distribution or from speech recognition lattices. NN-grams outperforms an n-gram model on an Italian speech recognition dictation task.   
 We investigate the usage of convolutional neural networks  for the slot filling task in spoken language understanding. We propose a novel CNN architecture for sequence labeling which takes into account the previous context words with preserved order information and pays special attention to the current word with its surrounding context. Moreover, it combines the information from the past and the future words for classification. Our proposed CNN architecture outperforms even the previously best ensembling recurrent neural network model and achieves state-of-the-art results with an F1-score of 95.61\% on the ATIS benchmark dataset without using any additional linguistic knowledge and resources.   
 The word sense disambiguation  task aims at identifying the meaning of words in a given context for specific words conveying multiple meanings. This task plays a prominent role in a myriad of real world applications, such as machine translation, word processing and information retrieval. Recently, concepts and methods of complex networks have been employed to tackle this task by representing words as nodes, which are connected if they are semantically similar. % Despite the increasingly number of studies carried out with such models, most of them use networks just to represent the data, while the pattern recognition performed on the attribute space is performed using traditional learning techniques. In other words, the structural relationship between words have not been explicitly used in the pattern recognition process. In addition, only a few investigations have probed the suitability of representations based on bipartite networks and graphs  for the problem, as many approaches consider all possible links between words. % In this context, we assess the relevance of a bipartite network model representing both feature words  and target  words to solve ambiguities in written texts. Here, we focus on the semantical relationships between these two type of words, disregarding the relationships between feature words. % In special, the proposed method not only serves to represent texts as graphs, but also constructs a structure on which the discrimination of senses is accomplished. %More specifically, we evaluate the performance of the WSD task by considering two types of attributes:  and  features. Our results revealed that the proposed learning algorithm in such bipartite networks provides excellent results mostly when  features are employed to characterize the context. Surprisingly, our method even outperformed the support vector machine algorithm in particular cases, with the advantage of being robust even if a small training dataset is available. Taken together, the results obtained here show that the proposed representation/classification method might be useful to improve the semantical characterization of written texts. 
 %Sequence labelling for extraction of medical events and their attributes from unstructured text in Electronic Health Record  notes is a key step towards semantic understanding of EHRs. It has varied applications in health informatics including pharma-co-vigilance and drug surveillance. Most of the current supervised machine learning models in this domain use Conditional Random Fields . In this application, we explored recurrent neural network frameworks and show that they significantly outperformed Conditional Random Field based labeling models. %
 We consider incorporating topic information into the sequence-to-sequence framework to generate informative and interesting responses for chatbots. To this end, we propose a topic aware sequence-to-sequence  model.  The model utilizes topics to simulate prior knowledge of human that guides them to form informative and interesting responses in conversation, and leverages the topic information in generation by a joint attention mechanism and a biased generation probability. The joint attention mechanism summarizes the hidden vectors of an input message as context vectors by message attention, synthesizes topic vectors by topic attention from the topic words of the message obtained from a pre-trained LDA model, and let these vectors jointly affect the generation of words in decoding. To increase the possibility of topic words appearing in responses, the model modifies the generation probability of topic words by adding an extra probability item to bias the overall distribution. Empirical study on both automatic evaluation metrics and human annotations shows that TA-Seq2Seq can generate more informative and interesting responses, and significantly outperform the-state-of-the-art response generation models. 
 Word2vec is a popular family of algorithms for unsupervised training of dense vector representations of words on large text corpuses. The resulting vectors have been shown to capture semantic relationships among their corresponding words, and have shown promise in reducing a number of natural language processing  tasks to mathematical operations on these vectors. While heretofore applications of word2vec have centered around vocabularies with a few million words, wherein the vocabulary is the set of words for which vectors are simultaneously trained, novel applications are emerging in areas outside of NLP with vocabularies comprising several 100 million words. Existing word2vec training systems are impractical for training such large vocabularies as they either require that the vectors of all vocabulary words be stored in the memory of a single server or suffer unacceptable training latency due to massive network data transfer. In this paper, we present a novel distributed, parallel training system that enables unprecedented practical training of vectors for vocabularies with several 100 million words on a shared cluster of commodity servers, using far less network traffic than the existing solutions. We evaluate the proposed system on a benchmark dataset, showing that the quality of vectors does not degrade relative to non-distributed training. Finally, for several quarters, the system has been deployed for the purpose of matching queries to ads in Gemini, the sponsored search advertising platform at Yahoo, resulting in significant improvement of business metrics.  
 We consider the problem of learning distributed representations for documents in data streams. The documents are represented as low-dimensional vectors and are jointly learned with distributed vector representations of word tokens using a hierarchical framework with two embedded neural language models. In particular, we exploit the context of documents in streams and use one of the language models to model the document sequences, and the other to model word sequences within them. The models learn continuous vector representations for both word tokens and documents such that semantically similar documents and words are close in a common vector space. We discuss extensions to our model, which can be applied to personalized recommendation and social relationship mining by adding further user layers to the hierarchy, thus learning user-specific vectors to represent individual preferences. We validated the learned representations on a public movie rating data set from MovieLens, as well as on a large-scale Yahoo News data comprising three months of user activity logs collected on Yahoo servers. The results indicate that the proposed model can learn useful representations of both documents and word tokens, outperforming the current state-of-the-art by a large margin. 
 This paper discusses models for dialogue state tracking using recurrent neural networks . We present experiments on the standard dialogue state tracking  dataset, DSTC2~. On the one hand, RNN models became the state of the art models in DST, on the other hand, most state-of-the-art DST models are only turn-based and require dataset-specific preprocessing  in order to achieve such results. We implemented two architectures which can be used in an incremental setting and require almost no preprocessing. We compare their performance to the benchmarks on DSTC2 and discuss their properties. With only trivial preprocessing, the performance of our models is close to the state-of-the-art results.\footnote{     {\bf Acknowledgment:} We thank Mirek Vodolç’‹ï¹ and Ondé‘¹î“«j DuéŽ·îœ«k for useful comments.     This research was partly funded by the Ministry of Education, Youth and Sports of the Czech Republic under the grant agreement LK11221, core research funding, grant GAUK 1915/2015, and also partially supported by SVV project number 260 333.      We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Tesla K40c GPU used for this research.     Computational resources were provided by the CESNET LM2015042 and the CERIT Scientific Cloud LM2015085, provided under the programme ``Projects of Large Research, Development, and Innovations Infrastructures''.     } 
   In the present paper we show that distributional information is   particularly important when considering concept availability under   implicit language learning conditions. Based on results from   different behavioural experiments we argue that the implicit   learnability of semantic regularities depends on the degree to which   the relevant concept is reflected in language use. In our   simulations, we train a Vector-Space model on either an English or a   Chinese corpus and then feed the resulting representations to a   feed-forward neural network. The task of the neural network was to   find a mapping between the word representations and the novel   words. Using datasets from four behavioural experiments, which used   different semantic manipulations, we were able to obtain learning   patterns very similar to those obtained by humans. 
 %Hierarchical label relations has proven to be useful in a variety of NLP tasks, such as question answering , textual entailment  and text generation . However, currently available taxonomies such as WordNet  are incomplete in coverage, unavailable in many domains, and time intensive to create or extend manually. %In this paper, w We study the problem of automatically building hypernym taxonomies from textual and visual data. Previous works in taxonomy induction generally ignore the increasingly prominent visual data, which encode important perceptual semantics. Instead, we propose a probabilistic model for taxonomy induction by jointly leveraging text and images. To avoid hand-crafted feature engineering, we design end-to-end features based on distributed representations of images and words. The model is discriminatively trained given a small set of existing ontologies and is capable of building full taxonomies from scratch for a collection of unseen conceptual label items with associated images. We evaluate our model and features on the WordNet hierarchies, where our system outperforms previous approaches by a large gap. %Extensive comparisons have been performed to demonstrate the effectiveness of our model and features. We further distinguish the relative importance of different features in constructing various parts of a taxonomy, while our empirical results provide insights into the model and taxonomies induced. 
 In recent years extracting relevant information from biomedical and clinical texts such as research articles, discharge summaries, or electronic health records have been a subject of many research efforts and shared challenges. Relation extraction is the process of detecting and classifying the semantic relation among entities in a given piece of texts. Existing models for this task in biomedical domain use either manually engineered features or kernel methods to create feature vector. These features are then fed to classifier for the prediction of the correct class. It turns out that the results of these methods are highly dependent on quality of user designed features and also suffer from curse of dimensionality. In this work we focus on extracting relations from clinical discharge summaries. Our main objective is to exploit the power of convolution neural network  to learn features automatically and thus reduce the dependency on manual feature engineering. We evaluate performance of the proposed model on i2b2-2010 clinical relation extraction challenge dataset. Our results indicate that convolution neural network can be a good model for relation exaction in clinical text without being dependent on expert's knowledge on defining quality features. 
  Hand-crafted features based on linguistic and domain-knowledge play crucial role in determining the performance of disease name recognition systems. Such methods are further limited by the scope of these features or in other words, their ability to cover the contexts or word dependencies within a sentence. In this work, we focus on reducing such dependencies and propose a domain-invariant framework for the disease name recognition task. In particular, we propose various end-to-end recurrent neural network  models for the tasks of disease name recognition and their classification into four pre-defined categories. We also utilize convolution neural network  in cascade of RNN to get character-based embedded features and employ it with word-embedded features in our model. We compare our models with the state-of-the-art results for the two tasks on NCBI disease dataset. Our results for the disease mention recognition task indicate that state-of-the-art performance can be obtained without relying on feature engineering. Further the proposed models obtained improved performance on the classification task of disease names.   
 Crosslingual word embeddings represent lexical items from different languages in the same vector space, enabling transfer of NLP tools.  However, previous attempts had %low-performance,  expensive resource requirements, difficulty incorporating monolingual data or were unable to handle polysemy. We address these drawbacks in our method which takes advantage of a high coverage dictionary in an EM style training algorithm over monolingual corpora in two languages.  Our model achieves state-of-the-art performance on bilingual lexicon induction task exceeding models using large bilingual corpora, and competitive results on the monolingual word similarity and cross-lingual document classification task.   
 We present a simple neural network for word alignment that builds source and target word window representations to compute alignment scores for sentence pairs. To enable unsupervised training, we use an aggregation operation that summarizes the alignment scores for a given target word. A soft-margin objective increases scores for true target words while decreasing scores for target words that are not present. Compared to the popular Fast Align model, our approach improves alignment accuracy by 7 AER on English-Czech, by 6 AER on Romanian-English and by 1.7 AER on English-French alignment. %On English-French data we match  model and we outperform it on Romanian-English by 2.8 AER. 
 Statistical techniques that analyze texts, referred to as text analytics, have departed from the use of simple word count statistics towards a new paradigm. Text mining now hinges on a more sophisticated set of methods, including the representations in terms of complex networks. While well-established word-adjacency  methods successfully grasp syntactical features of written texts, they are unable to represent important aspects of textual data, such as its topical structure, i.e. the sequence of subjects developing at a mesoscopic level along the text. Such aspects are often overlooked by current methodologies. In order to grasp the mesoscopic characteristics of semantical content in written texts, we devised a network model which is able to analyze documents in a multi-scale fashion. In the proposed model, a limited amount of adjacent paragraphs are represented as nodes, which are connected whenever they share a minimum semantical content.  To illustrate the capabilities of our model, we present, as a case example, a qualitative analysis of ``Alice's Adventures in Wonderland''. We show that the mesoscopic structure of a document, modeled as a network, reveals many semantic traits of texts.  Such an approach paves the way to a myriad of semantic-based applications.  In addition, our approach is illustrated in a machine learning context, in which texts are classified among real texts and randomized instances. 
   With the rapid growth of knowledge bases  on the web, how to take full advantage of them becomes increasingly important. Knowledge base-based question answering  is one of the most promising approaches to access the substantial knowledge. Meantime, as the neural network-based  methods develop, NN-based KB-QA has already achieved impressive results. However, previous work did not put emphasis on question representation, and the question is converted into a fixed vector regardless of its candidate answers. This simple representation strategy is unable to express the proper information of the question. Hence, we present a neural attention-based model to represent the questions dynamically according to the different focuses of various candidate answer aspects. In addition, we leverage the global knowledge inside the underlying KB, aiming at integrating the rich KB information into the representation of the answers. And it also alleviates the out of vocabulary  problem, which helps the attention model to represent the question more precisely. The experimental results on WEBQUESTIONS demonstrate the effectiveness of the proposed approach. 
   This paper presents an end-to-end framework for task-oriented dialog systems using a variant of Deep Recurrent Q-Networks . The model is able to interface with a relational database and jointly learn policies for both language understanding and dialog strategy. Moreover, we propose a hybrid algorithm that combines the strength of reinforcement learning and supervised learning to achieve faster learning speed. We evaluated the proposed model on a 20 Question Game conversational game simulator. Results show that the proposed method outperforms the modular-based baseline and learns a distributed representation of the latent dialog state.  
 Recurrent neural networks such as the GRU and LSTM found wide adoption in natural language processing and achieve state-of-the-art results for many tasks.  These models are characterized by a memory state that can be written to and read from by applying gated composition operations to the current input and the previous state.  However, they only cover a small subset of potentially useful compositions.  We propose Multi-Function Recurrent Units  that allow for arbitrary differentiable functions as composition operations.  Furthermore, MuFuRUs allow for an input- and state-dependent choice of these composition operations that is learned. Our experiments demonstrate that the additional functionality helps in different sequence modeling tasks, including the evaluation of propositional logic formulae, language modeling and sentiment analysis. 
 We describe MITRE's submission to the SemEval-2016 Task 6, Detecting Stance in Tweets. This effort achieved the top score in Task A on supervised stance detection, producing an average F1 score of 67.8 when assessing whether a tweet author was in favor or against a topic. We employed a recurrent neural network initialized with features learned via distant supervision on two large unlabeled datasets. We trained embeddings of words and phrases with the word2vec skip-gram method, then used those features to learn sentence representations via a hashtag prediction auxiliary task. These sentence vectors were then fine-tuned for stance detection on several hundred labeled examples. The result was a high performing system that used transfer learning to maximize the value of the available training data. 
 Many important NLP problems can be posed as dual-sequence or sequence-to-sequence modeling tasks. Recent advances in building end-to-end neural architectures have been highly successful in solving such tasks. In this work we propose a new architecture for dual-sequence modeling that is based on associative memory. We derive AM-RNNs, a recurrent associative memory  which augments generic recurrent neural networks . This architecture is extended to the Dual AM-RNN which operates on two AMs at once. Our models achieve very competitive results on textual entailment. A qualitative analysis demonstrates that long range dependencies between source and target-sequence can be bridged effectively using Dual AM-RNNs. However, an initial experiment on auto-encoding reveals that these benefits are not exploited by the system when learning to solve sequence-to-sequence tasks which indicates that additional supervision or regularization is needed. 
 Recent experiments show that deep bidirectional long short-term memory  recurrent neural network acoustic models outperform feedforward neural networks for automatic speech recognition . However, their training requires a lot of tuning and experience. In this work, we provide a comprehensive overview over various BLSTM training aspects and their interplay within ASR, which has been missing so far in the literature. We investigate on different variants of optimization methods, batching, truncated backpropagation, and regularization techniques such as dropout, and we study the effect of size and depth, training models of up to 10 layers. This includes a comparison of computation times vs.\ recognition performance. Furthermore, we introduce a pretraining scheme for LSTMs with layer-wise construction of the network showing good improvements especially for deep networks. The experimental analysis mainly was performed on the Quaero task, with additional results on Switchboard. The best BLSTM model gave a relative improvement in word error rate of over 15\% compared to our best feed-forward baseline on our Quaero 50h task. All experiments were done using RETURNN and RASR, RWTH's extensible training framework for universal recurrent neural networks and ASR toolkit. The training configuration files are publicly available. % %Deep bidirectional %long short-term memory  recurrent neural network  %acoustic models outperform feed-forward neural networks  %for automatic speech recognition  %but training them requires a lot of tuning and experience. %A comprehensive overview over the various aspects of training BLSTMs for ASR %is missing so far. %We study the effect of size and depth and train models of up to 10 layers. %We investigate on different variants %of optimization methods, batching, truncated backpropagation, %and regularization techniques such as dropout and $L_2$. %We also compare the computation times needed %in relation with recognition performance. %% %In addition, we introduce a pretraining scheme for LSTMs %with layer-wise construction of the network %and show that this improves the performance on deep networks. %% %The major part of the experimental analysis was performed %on the Quaero corpus and some %additional experiments were performed %on the Switchboard corpus. %Our best BLSTM model has a relative improvement in word error rate %of over 15\% compared to %our best feed-forward neural network  baseline on our Quaero 50h task. %% %All the experiments were done with %RETURNN, %the RWTH extensible training framework for universal recurrent neural networks %in combination with RASR, the RWTH ASR toolkit, %and our training configuration files are publicly available. 
 Conversational agents  are beginning to be widely used in conversational interfaces. To design a system that is capable of emulating human-like interactions, a conversational layer that can serve as a fabric for chat-like interaction with the agent is needed. In this paper, we introduce a model that employs Information Retrieval by utilizing convolutional deep structured semantic neural network-based features in the ranker to present human-like responses in ongoing conversation with a user. In conversations, accounting for context is critical to the retrieval model; we show that our context-sensitive approach using a Convolutional Deep Structured Semantic Model  with character trigrams significantly outperforms several conventional baselines in terms of the relevance of responses retrieved. 
 This paper tackles the problem of the semantic gap between a document and a query within an ad-hoc information retrieval task. In this context, knowledge bases  have already been acknowledged as valuable means since they allow the representation of explicit relations between entities. However, they do not necessarily represent implicit relations that could be hidden in a corpora. This latter issue is tackled by  recent works dealing with deep representation learning of texts. With this in mind, we argue that embedding KBs within  deep neural architectures supporting document-query matching would give rise to fine-grained latent representations of both words and their semantic relations.\\ In this paper, we  review the  main approaches of neural-based document ranking as well as  those approaches for latent representation of entities and relations via  KBs. We then propose some avenues to incorporate KBs in deep neural approaches for document ranking. More particularly, this paper advocates that KBs can be used either to support enhanced latent representations of queries and documents based on both distributional and relational semantics or to serve as a semantic translator between their latent distributional representations. 
 As deep neural networks continue to revolutionize various application domains, there is increasing interest in making these powerful models more understandable and interpretable, and narrowing down the causes of good and bad predictions. We focus on recurrent neural networks , state of the art models in speech recognition and translation. Our approach to increasing interpretability is by combining an RNN with a hidden Markov model , a simpler and more transparent model.  We explore various combinations of RNNs and HMMs: an HMM trained on LSTM states; a hybrid model where an HMM is trained first, then a small LSTM is given HMM state distributions and trained to fill in gaps in the HMM's performance; and a jointly trained hybrid model. We find that the LSTM and HMM learn complementary information about the features in the text. 
 % abstract Deep neural networks  have been successfully applied to a wide variety of acoustic modeling tasks in recent years. These include the applications of DNNs either in a discriminative feature extraction or in a hybrid acoustic modeling scenario. Despite the rapid progress in this area, a number of challenges remain in training DNNs. This paper presents an effective way of training DNNs using a manifold learning based regularization framework. In this framework, the parameters of the network are optimized  to preserve underlying manifold based relationships between speech feature vectors while minimizing a measure of loss between network outputs and targets. This is achieved by incorporating manifold based locality  %and geometrical relationships preserving   constraints in the objective criterion of DNNs. Empirical evidence is provided to demonstrate that training a network with manifold constraints  % strengthens the learning of manifold based neighborhood preservation and   preserves structural compactness in the hidden layers of the network. Manifold regularization is applied to train bottleneck DNNs for feature extraction in hidden Markov model  based speech recognition. The experiments in this work are conducted on the Aurora-2 spoken digits and the Aurora-4 read news large vocabulary continuous speech recognition tasks. The performance is measured in terms of word error rate  on these tasks. It is shown that the manifold regularized DNNs result in up to 37\%  reduction in WER relative to standard DNNs. 
 Most state of the art approaches for Named Entity Recognition rely on hand crafted features and annotated corpora. Recently Neural network based models have been proposed which do not require handcrafted features but still require annotated corpora. However, such annotated corpora may not be available for many languages. In this paper, we propose a neural network based model which allows sharing the decoder as well as word and character level parameters between two languages thereby allowing a resource fortunate language to aid a resource deprived language. Specifically, we focus on the case when limited annotated corpora is available in one language  and abundant annotated corpora is available in another language . Sharing the network architecture and parameters between $L_1$ and $L_2$ leads to improved performance in $L_1$. Further, our approach does not require any hand crafted features but instead directly learns meaningful feature representations from the training data itself. We experiment with 4 language pairs and show that indeed in a resource constrained setup , a model jointly trained with data from another language performs better than a model trained only on the limited corpora in one language.          
 We propose a novel deep learning training criterion, named permutation invariant training , for speaker independent multi-talker speech separation, commonly known as the cocktail-party problem. Different from the multi-class regression technique and the deep clustering  technique, our novel approach minimizes the separation error directly. This strategy effectively solves the long-lasting label permutation problem, that has prevented progress on deep learning based techniques for speech separation. We evaluated PIT on the WSJ0 and Danish mixed-speech separation tasks and found that it compares favorably to non-negative matrix factorization , computational auditory scene analysis , and DPCL and generalizes well over unseen speakers and languages. Since PIT is simple to implement and can be easily integrated and combined with other advanced techniques, we believe improvements built upon PIT can eventually solve the cocktail-party problem. 
 We propose a simple domain adaptation method for neural networks in a supervised setting. Supervised domain adaptation is a way of improving the generalization performance on the target domain by using the source domain dataset, assuming that both of the datasets are labeled. Recently, recurrent neural networks have been shown to be successful on a variety of NLP tasks such as caption generation; however, the existing domain adaptation techniques are limited to  tune the model parameters by the target dataset after the training by the source dataset, or  design the network to have dual output, one for the source domain and the other for the target domain. Reformulating the idea of the domain adaptation technique proposed by , %Daum{\'e}~, we propose a simple domain adaptation method, which can be applied to neural networks trained with a cross-entropy loss. On captioning datasets, we show performance improvements over other domain adaptation methods. 
     We first observe a potential weakness of continuous vector representations     of symbols in neural machine translation. That is, the continuous vector     representation, or a word embedding vector, of a symbol encodes multiple     dimensions of similarity, equivalent to encoding more than one meaning of     the word. This has the consequence that the encoder and decoder recurrent     networks in neural machine translation need to spend substantial amount of     their capacity in disambiguating source and target words based on the     context which is defined by a source sentence. Based on this observation, in     this paper we propose to contextualize the word embedding vectors using a     nonlinear bag-of-words representation of the source sentence. Additionally,     we propose to represent special tokens  with typed symbols to facilitate translating those words that are     not well-suited to be translated via continuous vectors. The experiments on     En-Fr and En-De reveal that the proposed approaches of contextualization and     symbolization improves the translation quality of neural machine translation     systems significantly. 
 In this work, we introduce temporal hierarchies to the sequence to sequence  model to tackle the problem of abstractive summarization of scientific articles. The proposed Multiple Timescale model of the Gated Recurrent Unit  is implemented in the encoder-decoder setting to better deal with the presence of multiple compositionalities in larger texts. The proposed model is compared to the conventional RNN encoder-decoder, and the results demonstrate that our model trains faster and shows significant performance gains. The results also show that the temporal hierarchies help improve the ability of seq2seq models to capture compositionalities better without the presence of highly complex architectural hierarchies.   
 Cross lingual projection of linguistic annotation suffers from many sources of bias and noise, leading to unreliable annotations that cannot be used directly. In this paper, we introduce a novel approach to sequence tagging that learns to correct the errors from cross-lingual projection using an explicit debiasing layer. This is framed as joint learning over two corpora, one tagged with gold standard and the other with projected tags. We evaluated with only 1,000 tokens tagged with gold standard tags, along with more plentiful parallel data. Our system equals or exceeds the state-of-the-art on eight simulated low-resource settings, as well as two real low-resource languages, Malagasy and Kinyarwanda. 
 %   Our goal is to combine the rich multi-step inference of symbolic logical reasoning with the generalization capabilities of vector embeddings and neural networks. We are particularly interested in complex reasoning about the entities and relations in knowledge bases . Recently  proposed a compelling methodology to compose relations occurring in a path in KBs using recurrent neural networks . We enhance the capabilities of this modeling technique by incorporating multiple paths  between entities during reasoning. We also learn entity  representations jointly with relation embeddings. On a large dataset of Freebase and ClueWeb, our method provide a relative improvement of 13.7\% in a KB completion task. We further explore multi-task training objectives to address the sparsity of KBs by handling infrequent relation types. Lastly, we achieve competitive results in a recently proposed task of answering compositional path queries on WordNet. % 
   This document contains the instructions for preparing a camera-ready   manuscript for the proceedings of ACL-2016. The document itself   conforms to its own specifications, and is therefore an example of   what your manuscript should look like. These instructions should be   used for both papers submitted for review and for final versions of   accepted papers.  Authors are asked to conform to all the directions   reported in this document. 
 In order to control computational complexity, neural machine translation  systems convert all rare words outside the vocabulary into a single unk symbol. Previous solution  resorts to use multiple numbered unks to learn the correspondence between source and target rare words. However, testing words unseen in the training corpus cannot be handled by this method. And it also suffers from the noisy word alignment. In this paper, we focus on a major type of rare words -- named entity , and propose to translate them with character level sequence to sequence model. The NE translation model is further used to derive high quality NE alignment in the bilingual training corpus. With the integration of NE translation and alignment modules, our NMT system is able to surpass the baseline system by 2.9 BLEU points on the Chinese to English task.  
 Highway deep neural network  is a type of depth-gated feedforward neural network, which has shown to be easier to train with more hidden layers and also generalise better compared to conventional plain deep neural networks . Previously, we investigated a structured HDNN architecture for speech recognition, in which the two gate functions were tied across all the hidden layers, and we were able to train a much smaller model without sacrificing the recognition accuracy. In this paper, we carry on the study of this architecture with sequence-discriminative training criterion and speaker adaptation techniques on the AMI meeting speech recognition corpus. We show that these two techniques improve speech recognition accuracy on top of the model trained with the cross entropy criterion. Furthermore, we demonstrate that the two gate functions that are tied across all the hidden layers are able to control the information flow over the whole network, and we can achieve considerable improvements by only updating these gate functions in both sequence training and adaptation experiments.   
 Reading comprehension has embraced a booming in recent NLP research. Several institutes have released the Cloze-style reading comprehension data, and these have greatly accelerated the research of machine comprehension. In this work, we firstly present Chinese reading comprehension datasets, which consist of People Daily news dataset and Children's Fairy Tale  dataset. Also, we propose a consensus attention-based neural network architecture to tackle the  Cloze-style reading comprehension problem, which aims to induce a consensus attention over every words in the query. Experimental results show that the proposed neural network significantly outperforms the state-of-the-art baselines in several public datasets.  Furthermore, we setup a baseline for Chinese reading comprehension task,  and hopefully this would speed up the process for future research. 
 We present a novel neural architecture for answering queries, designed to optimally leverage explicit support in the form of query-answer memories. Our model is able to refine and update a given query while separately accumulating evidence for predicting the answer. Its architecture reflects this separation with dedicated embedding matrices and loosely connected information pathways  for updating the query and accumulating evidence. This separation of responsibilities effectively decouples the search for query related support and the prediction of the answer. On recent benchmark datasets for reading comprehension, our model achieves state-of-the-art results. A qualitative analysis reveals that the model effectively accumulates weighted evidence from the query and over multiple support retrieval cycles which results in a robust answer prediction. 
 Recently, Neural Networks have been proven extremely effective in many natural language processing tasks such as sentiment analysis, question answering, or machine translation.  Aiming to exploit such advantages in the Ontology Learning process, in this technical report we present a detailed description of a Recurrent Neural Network based system to be used to pursue such goal.  
 Cloze-style reading comprehension is a representative problem in mining relationship between document and query. In this paper, we present a simple but novel model called attention-over-attention reader for better solving cloze-style reading comprehension task. The proposed model aims to place another attention mechanism over the document-level attention and induces ``attended attention'' for final answer predictions. One advantage of our model is that it is simpler than related works while giving excellent performance. In addition to the primary model, we also propose an N-best re-ranking strategy to double check the validity of the candidates and further improve the performance. Experimental results show that the proposed methods significantly outperform various state-of-the-art systems by a large margin in public datasets, such as CNN and Children's Book Test. 
 % Neural networks with recurrent or recursive architectures have shown promising results on NLP tasks.  % The recurrent and recursive architectures have their own strength and limitations.  Recurrent neural networks  process input text sequentially and model the conditional transition between word tokens. %%which have shown improvement in NLP tasks including machine translation and language modeling.  In contrast, the advantages of recursive networks include that they explicitly model the compositionality and the recursive structure of natural language. However, the current recursive architecture is limited by its dependence on syntactic tree. In this paper, we introduce a robust syntactic parsing-independent tree structured model, Neural Tree Indexers  that provides a middle ground between the sequential RNNs and the syntactic tree-based recursive models. NTI constructs a full n-ary tree by processing the input text with its node function in a bottom-up fashion. Attention mechanism can then be applied to both structure and node function. We implemented and evaluated a binary-tree model of NTI, showing the model achieved the state-of-the-art performance on three different NLP tasks: natural language inference, answer sentence selection, and sentence classification, outperforming state-of-the-art recurrent and recursive neural networks \footnote{Code for the experiments and NTI is available at https://bitbucket.org/tsendeemts/nti}.   %%where our models achieved the state-of-the-art performance when evaluated on publicly available benchmarks. Code for the experiments and NTI will be available at http://anonymized upon publication. 
   Deep neural networks have shown recent promise in many language-related tasks such as the modeling of conversations. We extend RNN-based sequence to sequence models to capture the long range discourse across many turns of conversation.  We perform a sensitivity analysis on how much additional context affects performance,  and provide quantitative and qualitative evidence that these models  are able to capture discourse relationships across multiple utterances. Our results quantifies how adding an additional RNN layer for modeling discourse improves the quality of output utterances and providing more of the previous conversation as input also improves performance. By searching the generated outputs for specific discourse markers  we show how neural discourse models can exhibit increased coherence and cohesion in conversations.    
   The rapidly expanding corpus of medical research literature presents major challenges in the understanding of previous work, the extraction of maximum information from collected data, and the identification of promising research directions. We present a case for the use of advanced machine learning techniques as an aide in this task and introduce a novel methodology that is shown to be capable of extracting meaningful information from large longitudinal corpora, and of tracking complex temporal changes within it. 
 Several tasks in argumentation mining and debating, question-answering, and natural language inference involve classifying a sequence in the context of another sequence . For several single sequence classification tasks, the current state-of-the-art approaches are based on recurrent and convolutional neural networks. On the other hand, for bi-sequence classification problems, there is not much understanding as to the best deep learning architecture. In this paper, we attempt to get an understanding of this category of problems by extensive empirical evaluation of 19 different deep learning architectures  for various problems originating in natural language processing like debating, textual entailment and question-answering. Following the empirical evaluation, we offer our insights and conclusions regarding the architectures we have considered. We also establish the first deep learning baselines for three argumentation mining tasks. 
   We present a novel view that unifies two frameworks that aim to solve sequential prediction problems: learning to search  and recurrent neural networks . We point out equivalences between elements of the two frameworks. By complementing what is missing from one framework comparing to the other, we introduce a more advanced imitation learning framework that, on one hand, augments L2S's notion of search space and, on the other hand, enhances RNNs' training procedure to be more robust to compounding errors arising from training on highly correlated examples.   
 Neural conversational models tend to produce generic or safe responses in different contexts, e.g., reply ``Of course'' to narrative statements or ``I don't know'' to questions.  In this paper, we propose an end-to-end approach to avoid such problem in neural generative models. Additional memory mechanisms have been introduced to standard sequence-to-sequence  models, so that context can be considered while generating sentences. Three seq2seq models, which memorize a fix-sized contextual vector from hidden input, hidden input/output and a gated contextual attention structure respectively, have been trained and tested on a dataset of labeled question-answering pairs in Chinese. The model with contextual attention outperforms others including the state-of-the-art seq2seq models on perplexity test. The novel contextual model generates diverse and robust responses, and is able to carry out conversations on a wide range of topics appropriately. 
    We present a novel incremental learning approach for unsupervised word   segmentation that combines features from probabilistic modeling and model   selection.  This includes super-additive penalties for addressing the   cognitive burden imposed by long word formation, and new model selection   criteria based on higher-order generative assumptions.  Our approach is fully   unsupervised; it relies on a small number of parameters that permits flexible   modeling and a mechanism that automatically learns parameters from the data.   Through experimentation, we show that this intricate design has led to   top-tier performance in both phonemic and orthographic word segmentation.  
 In this paper, we present the first experiments using neural network models for the task of error detection in learner writing. We perform a systematic comparison of alternative compositional architectures and propose a framework for error detection based on bidirectional LSTMs. Experiments on the CoNLL-14 shared task dataset show the model is able to outperform other participants on detecting errors in learner writing. Finally, the model is integrated with a publicly deployed self-assessment system, leading to performance comparable to human annotators.   
 	While question answering  with neural network, i.e. neural QA, has achieved promising results in recent years, lacking of large scale real-word QA dataset is still a challenge for developing and evaluating neural QA system. 	To alleviate this problem, we propose a large scale human annotated real-world QA dataset WebQA with more than 42k questions and 556k evidences. 	As existing neural QA methods resolve QA either as sequence generation or classification/ranking problem, they face challenges of expensive $\mathrm{softmax}$ computation, unseen answers handling or separate candidate answer generation component. 	In this work, we cast neural QA as a sequence labeling problem and propose an end-to-end sequence labeling model, which overcomes all the above challenges. 	Experimental results on WebQA show that our model outperforms the baselines significantly with an F1 score of 74.69\% with word-based input, and the performance drops only 3.72 F1 points with more challenging character-based input. 
   Dialogue systems have become commonplace in service applications, but general conversational systems still leave a lot to be desired.  In this paper we present a Natural Language Understanding framework for mapping from text utterances to a semantically tagged grammar that can be understood by a dialogue manager using Sequence-to-Sequence learning.  
 Sentence ordering is a general and critical task for natural language generation applications. Previous works have focused on improving its performance in an external, downstream task, such as multi-document summarization. Given its importance, we propose to study it as an isolated task. We collect a large corpus of academic texts, and derive a data driven approach to learn pairwise ordering of sentences, and validate the efficacy with extensive experiments. Source codes\footnote{https://github.com/fudannlp} and dataset\footnote{http://nlp.fudan.edu.cn/data/} of this paper will be made publicly available. 
 %\boldmath Concepts and methods of complex networks can be used to analyse texts at their different complexity levels. Examples of natural language processing  tasks studied via topological analysis of networks are keyword identification, automatic extractive summarization and authorship attribution. Even though a myriad of network measurements have been applied to study the authorship attribution problem, the use of motifs for text analysis has been restricted to a few works. The goal of this paper is to apply the concept of motifs, recurrent interconnection patterns, in the authorship attribution task. The absolute frequencies of all thirteen directed motifs with three nodes were extracted from the co-occurrence networks and used as classification features. The effectiveness of these features was verified with four machine learning methods. The results show that motifs are able to distinguish the writing style of different authors. In our best scenario, 57.5\% of the books were correctly classified. The chance baseline for this problem is 12.5\%. In addition, we have found that function words play an important role in these recurrent patterns. Taken together, our findings suggest that motifs should be further explored in other related linguistic tasks. 
 We present , a novel method for generating general-purpose vector representation of tweets. The model learns tweet embeddings using character-level CNN-LSTM encoder-decoder. We trained our model on 3 million, randomly selected English-language tweets. The model was evaluated using two methods: tweet semantic similarity and tweet sentiment categorization, outperforming the previous state-of-the-art in both tasks. The evaluations demonstrate the power of the tweet embeddings generated by our model for various tweet categorization tasks. The vector representations generated by our model are generic, and hence can be applied to a variety of tasks. Though the model presented in this paper is trained on English-language tweets, the method presented can be used to learn tweet embeddings for different languages. 
 Job search through online matching engines nowadays are very prominent and beneficial to both job seekers and employers. But the solutions of traditional engines without understanding the semantic meanings of different resumes have not kept pace with the incredible changes in machine learning techniques and computing capability. These solutions are usually driven by manual rules and predefined weights of keywords which lead to an inefficient and frustrating search experience. To this end, we present a machine learned solution with rich features and deep learning methods. Our solution includes three configurable modules that can be plugged with little restrictions. Namely, unsupervised feature extraction, base classifiers training and ensemble method learning. In our solution, rather than using manual rules, machine learned methods to automatically detect the semantic similarity of positions are proposed. Then four competitive ``shallow" estimators and ``deep" estimators are selected. Finally, ensemble methods to bag these estimators and aggregate their individual predictions to form a final prediction are verified. Experimental results of over 47 thousand resumes show that our solution can significantly improve the predication precision current position, salary, educational background and company scale.     
  The vanilla sequence-to-sequence learning  reads and encodes a source sequence into a fixed-length vector only once, suffering from its insufficiency in modeling structural correspondence between the source and target sequence. Instead of handling this insufficiency with a linearly weighted attention mechanism, in this paper, we propose to use a recurrent neural network  as an alternative . During decoding, Cseq2seq-I cyclically feeds the previous decoding state back to the encoder as the initial state of the RNN, and reencodes source representations to produce context vectors. We surprisingly find that the introduced RNN succeeds in dynamically detecting translation-related source tokens according to the partial target sequence. Based on this finding, we further hypothesize that the partial target sequence can act as a feedback to improve the understanding of the source sequence. To test this hypothesis, we propose cyclic sequence-to-sequence learning  which differs from the seq2seq only in the reintroduction of previous decoding state into the same encoder. We further perform parameter sharing on Cseq2seq-II to reduce parameter redundancy and enhance regularization. In particular, we share the weights of the encoder and decoder, and two target-side word embeddings, making Cseq2seq-II equivalent to a single conditional RNN model, with 31\% parameters pruned but even better performance. Cseq2seq-II not only preserves the simplicity of seq2seq but also yields comparable and promising results on machine translation tasks. Experiments on Chinese-English and English-German translation show that Cseq2seq achieves significant and consistent improvements over seq2seq and is as competitive as the attention-based seq2seq model.  
   We consider the task of KBP slot filling -- extracting relation information from newswire documents for knowledge base construction.  We present our pipeline, which employs Relational Dependency Networks  to learn linguistic patterns for relation extraction.   Additionally, we demonstrate how several components  such as weak supervision, word2vec features, joint learning and the use of human advice, can be incorporated in this relational framework. We evaluate the different components in the benchmark KBP 2015 task and show that RDNs effectively model a diverse set of features and perform competitively with current state-of-the-art relation extraction. %Some of the results are surprising - weak supervision and word2vec do not appear to significantly improve the performance. Human advice, however, improves the recall of the system allowing the humans to be more than mere labelers. Finally, joint learning is useful when the target relations are correlated.    
 Short text messages such as tweets are very noisy and sparse in their use of vocabulary. Traditional textual representations, such as tf-idf, have difficulty grasping the semantic meaning of such texts, which is important in applications such as event detection, opinion mining, news recommendation, etc. We constructed a method based on semantic word embeddings and frequency information to arrive at low-dimensional representations for short texts designed to capture semantic similarity. For this purpose we designed a weight-based model and a learning procedure based on a novel median-based loss function. This paper discusses the details of our model and the optimization methods, together with the experimental results on both Wikipedia and Twitter data. We find that our method outperforms the baseline approaches in the experiments, and that it generalizes well on different word embeddings without retraining. Our method is therefore capable of retaining most of the semantic information in the text, and is applicable out-of-the-box. 
  We introduce  , an extension of Recurrent Neural Networks that replaces the softmax output layer by a log-linear output layer, of which the softmax is a special case. This conceptually simple move has two main advantages. First, it allows the learner to combat training data sparsity by allowing it to model words  as complex combinations of attributes without requiring that each combination is directly observed in the training data . Second, it permits the inclusion of flexible prior knowledge in the form of  specified modular features, where the neural network component learns to dynamically control the weights of a log-linear distribution exploiting these features.\par  We conduct experiments in the domain of language modelling of French, that exploit morphological prior knowledge and show an important decrease in perplexity relative to a baseline RNN.\par  We provide other motivating iillustrations, and finally argue that the log-linear and the neural-network components contribute complementary strengths to the LL-RNN: the LL aspect allows the model to incorporate  rich prior knowledge, while the NN aspect, according to the ``representation learning'' paradigm,  allows the model to discover novel combination of characteristics.   \medskip  This is an updated version of the e-print arXiv:1607.02467, in particular now including experiments. 
 We present a memory augmented neural network for natural language understanding: Neural Semantic Encoders. NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves over time and maintains the understanding of input sequences  through read, compose and write operations. NSE can also access\footnote{By access we mean changing the memory states by the read, compose and write operations.} multiple and shared memories.  %depending on the complexity of a task. %Unlike existing memory-based neural network models, NSE is easy to train.  In this paper, we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks: natural language inference, question answering, sentence classification, document sentiment analysis and machine translation where NSE achieved state-of-the-art performance when evaluated on publically available benchmarks. For example, our shared-memory model showed an encouraging result on neural machine translation, improving an attention-based baseline by approximately 1.0 BLEU. 
   Knowing which words have been attended to in previous time steps while   generating a translation is a rich source of information for predicting what   words will be attended to in the future. We improve upon the attention model   of Bahdanau et al.  by explicitly modeling the relationship between   previous and subsequent attention levels for each word using one recurrent   network per input word. This architecture easily captures informative   features, such as fertility and regularities in relative distortion. In   experiments, we show our parameterization of attention improves translation   quality. 
 Natural Language Inference is an important task for Natural Language Understanding. It is concerned with classifying the logical relation between two sentences.  In this paper, we propose several text generative neural networks for generating text hypothesis, which allows construction of new  Natural Language Inference datasets.   To evaluate the models, we propose a new metric -- the accuracy of the classifier trained on the generated dataset. The accuracy obtained by our best generative model is only 2.7\% lower than the accuracy of the classifier trained on the original, human crafted dataset. Furthermore, the best generated dataset combined with the original dataset achieves the highest accuracy.  The best model learns a mapping embedding for each training example.  By comparing various metrics we show that datasets that obtain higher ROUGE or METEOR scores do not necessarily yield higher classification accuracies.   We also provide analysis of what are the characteristics of a good dataset including the distinguishability of the generated datasets from the original one. 
 In this paper, we improve the attention or alignment accuracy of neural machine translation by utilizing the alignments of training sentence pairs. We simply compute the distance between the machine attentions and the ``true'' alignments, and minimize this cost in the training procedure.  Our experiments on large-scale  Chinese-to-English task show that our model improves both translation and alignment  qualities significantly over the large-vocabulary neural machine translation system, and even beats a state-of-the-art traditional syntax-based system. 
     Current language models have significant limitation in the ability to encode and decode factual knowledge. This is mainly because they acquire such knowledge from statistical co-occurrences although most of the knowledge words are rarely observed.~In this paper, we propose a Neural Knowledge Language Model  which combines symbolic knowledge provided by the knowledge graph with the RNN language model.~By predicting whether the word to generate has an underlying fact or not, the model can generate such knowledge-related words by copying from the description of the predicted fact.~In experiments, we show that the NKLM significantly improves the performance while generating a much smaller number of unknown words.     %~We also demonstrate that the sampled descriptions include named entities which used to be unknown words in RNN language models. 
 %We consider the problem of learning convolution neural network models for sentence classification tasks.  %The state-of-the-art CNN models give good performance on sentence classification tasks. The purpose of this work is to empirically study desirable properties such as semantic coherence, attention mechanism and reusability of CNNs in these tasks. Semantically coherent kernels are preferable as they are a lot more interpretable for explaining the decision of the learned CNN model. We observe that the learned kernels do not have semantic coherence. Motivated by this observation, we propose to learn kernels with semantic coherence using clustering scheme combined with Word2Vec representation and domain knowledge such as SentiWordNet. We suggest a technique to visualize attention mechanism of CNNs for decision explanation purpose. Reusable property enables kernels learned on one problem to be used in another problem. This helps in efficient learning as only a few additional domain specific filters may have to be learned. We demonstrate the efficacy of our core ideas of learning semantically coherent kernels and leveraging reusable kernels for efficient learning on several benchmark datasets. Experimental results show the usefulness of our approach by achieving performance close to the state-of-the-art methods but with semantic and reusable properties.  The purpose of this work is to empirically study desirable properties such as semantic coherence, attention mechanism and kernel reusability in Convolution Neural Networks  for learning sentence classification tasks.  We propose to learn semantically coherent kernels using clustering scheme combined with Word2Vec representation and domain knowledge such as SentiWordNet. We also suggest a technique to visualize attention mechanism of CNNs. These ideas are useful for decision explanation purpose. Reusable property enables kernels learned on one problem to be used in another problem. This helps in efficient learning as only a few additional domain specific kernels may have to be learned. Experimental results demonstrate the usefulness of our approach. The performance of the proposed approach, which uses semantic and re-usability properties, is close to that of the state-of-the-art approaches on many real-world datasets. %The performance of the proposed approach, which used semantic and re-usability properties, shows its usefulness and was close to that of the state-of-the-art approaches on many real-world datasets. %Experimental results show the usefulness of our approach by achieving performance close to the state-of-the-art methods but with semantic and reusable properties.  
 Topics generated by topic models are usually represented by lists of $t$ terms or alternatively using short phrases or images. The current state-of-the-art work on labeling topics using images selects images by re-ranking a small set of candidates for a given topic. In this paper, we present a more generic method that can estimate the degree of association between any arbitrary pair of an unseen topic and image using a deep neural network. Our method achieves better runtime performance $O$ compared to $O$ for the current state-of-the-art method, and is also significantly more accurate.  
 Deep learning has significantly advanced state-of-the-art of speech recognition in the past few years. However, compared to conventional Gaussian mixture acoustic models, neural network models are usually much larger, and are therefore not very deployable in embedded  devices. Previously, we investigated a compact highway deep neural network  for acoustic modelling, which is a type of depth-gated feedforward neural network. We have shown that HDNN-based acoustic models can achieve comparable recognition accuracy with much smaller number of model parameters compared to plain deep neural network  acoustic models. In this paper, we push the boundary further by leveraging on the knowledge distillation technique that is also known as teacher-student training, i.e., we train the compact HDNN model with the supervision of a high accuracy cumbersome model. Furthermore, we also investigate sequence training and adaptation in the context of teacher-student training. Our experiments were performed on the AMI meeting speech recognition corpus. With this technique, we significantly improved the recognition accuracy of the HDNN acoustic model with less than 0.8 million parameters,  and narrowed the gap between this model and the plain DNN with 30 million parameters.  
  The identification of authorship in disputed documents still requires human expertise, which is now unfeasible for many tasks owing to the large volumes of text and authors in practical applications. In this study, we introduce a methodology based on the dynamics of word co-occurrence networks representing written texts to classify a corpus of 80 texts by 8 authors. The texts were divided into sections with equal number of linguistic tokens, from which time series were created for 12 topological metrics. The series were proven to be stationary , which permits to use distribution moments as learning attributes. With an optimized supervised learning procedure using a Radial Basis Function Network, 68 out of 80 texts were correctly classified, i.e. a remarkable 85\% author matching success rate. Therefore, fluctuations in purely dynamic network metrics were found to characterize authorship, thus opening the way for the description of texts in terms of small evolving networks. Moreover, the approach introduced allows for comparison of texts with diverse characteristics in a simple, fast fashion. 
 Language processing mechanism by humans is generally more robust than computers.  The \cambridge  effect from the psycholinguistics literature has demonstrated such a robust word processing mechanism, where jumbled words  are recognized with little cost. On the other hand, computational models for word recognition  perform poorly on data with such noise.  Inspired by the findings from the \cambridge effect, we propose a word recognition model based on a semi-character level recurrent neural network . In our experiments, we demonstrate that \jlm has significantly more robust performance in word spelling correction  compared to existing spelling checkers and character-based convolutional neural network. Furthermore, we demonstrate that the model is cognitively plausible by replicating a psycholinguistics experiment about human reading difficulty using our model. 
 This paper explores the task of translating natural language queries into regular expressions which embody their meaning. In contrast to prior work, the proposed neural model does not utilize domain-specific crafting, learning to translate directly from a parallel corpus. To fully explore the potential of neural models, we propose a methodology for collecting a large corpus\footnote{Code and data are submitted as supplementary material.} of regular expression, natural language pairs. Our resulting model achieves a performance gain of 19.6\% over previous state-of-the-art models. 
 The role of social media, in particular microblogging platforms such as Twitter, as a conduit for actionable and tactical information during disasters is increasingly acknowledged. However, time-critical analysis of big crisis data on social media streams brings challenges to machine learning techniques, especially the ones that use supervised learning. Scarcity of labeled data, particularly in the early hours of a crisis, delays the machine learning process. The current state-of-the-art classification methods require a significant amount of labeled data specific to a particular event for training plus a lot of feature engineering to achieve best results. In this work, we introduce neural network based classification methods for binary and multi-class tweet classification task. %Our classification methods  We show that neural network based models do not require any feature engineering and perform better than state-of-the-art methods. In the early hours of a disaster when no labeled data is available, our proposed method makes the best use of the out-of-event data and achieves good results.  
  Within the field of Statistical Machine Translation , the neural approach  has recently emerged as the first technology able to challenge the long-standing dominance of phrase-based approaches .  % ORIGINAL: Neural machine translation  has recently emerged as the first technology able to challenge the long-standing dominance of statistical machine translation .   % In particular, at the IWSLT 2015 evaluation campaign, NMT outperformed well established state-of-the-art PBMT systems on English-German, a language pair known to be particularly hard because of morphology and syntactic differences. %In this paper, we set out to understand in what respects NMT provides %better translation quality than SMT.  To this end,  To understand in what respects NMT provides better translation quality than PBMT, we perform a detailed analysis of neural vs. phrase-based SMT outputs, leveraging high quality post-edits performed by professional translators on the IWSLT data.   %For the first time, our analysis provides useful insights on what linguistic phenomena are best modeled by NMT, while pointing out some other aspects that remain to be improved. For the first time, our analysis provides useful insights on what linguistic phenomena are best modeled by neural models -- such as the reordering of verbs -- while pointing out other aspects that remain to be improved.  %for example, %we will see that NMT output contains less word reordering errors than %PBMT , with a quality peak in the placement of verbs . On the other hand, we will show that some aspects of NMT %should be improved, like the placement of articles.  
 Neural machine translation aims at building a single large neural network that  can be  trained to maximize translation performance. The encoder-decoder  architecture with an attention mechanism achieves a translation performance  comparable to the existing state-of-the-art phrase-based systems on the task of  English-to-French translation.  However, the use of large vocabulary becomes the bottleneck  in both training and improving the performance. In this paper, we propose an efficient architecture to train a deep character-level neural machine translation by introducing  a   and an .  The decimator is used to sample the source  sequence before encoding while the interpolator is used to resample  after decoding.  Such a deep model has  two major advantages.  It avoids the large vocabulary issue radically; at the same time, it is much faster and  more memory-efficient in training than conventional character-based models. %We achieve a higher BLEU score in fewer epochs.  More interestingly, our model is able to  translate the misspelled word like human beings.  % Another achievement of the deep character-level neural machine translation  %is translating the misspelled word like human beings .   %never before possible. %in word-level neural machine translation. 
 Distant speech recognition is a challenge, particularly due to the corruption of speech signals by reverberation caused by large distances between the speaker and microphone. In order to cope with a wide range of reverberations in real-world situations, we present novel approaches for acoustic modeling including an ensemble of deep neural networks  and an ensemble of jointly trained DNNs. First, multiple DNNs are established, each of which corresponds to a different reverberation time 60  in a setup step. Also, each model in the ensemble of DNN acoustic models is further jointly trained, including both feature mapping and acoustic modeling, where the feature mapping is designed for the dereverberation as a front-end. In a testing phase, the two most likely DNNs are chosen from the DNN ensemble using maximum a posteriori  probabilities, computed in an online fashion by using maximum likelihood -based blind $\rm{RT_{60}}$ estimation and then the posterior probability outputs from two DNNs are combined using the ML-based weights as a simple average. Extensive experiments demonstrate that the proposed approach leads to substantial improvements in speech recognition accuracy over the conventional DNN baseline systems under diverse reverberant conditions. 
 While cross-lingual word embeddings have been studied extensively in recent years, the qualitative differences between the different algorithms remain vague. We observe that whether or not an algorithm uses a particular feature set  accounts for a significant performance gap among these algorithms. This feature set is also used by traditional alignment algorithms, such as IBM Model-1, which demonstrate similar performance to state-of-the-art embedding algorithms on a variety of benchmarks. Overall, we observe that different algorithmic approaches for utilizing the sentence ID feature space result in similar performance. This paper draws both empirical and theoretical parallels between the embedding and alignment literature, and suggests that adding additional sources of information, which go beyond the traditional signal of bilingual sentence-aligned corpora, may substantially improve cross-lingual word embeddings, and that future baselines should at least take such features into account. 
 The sequence to sequence architecture is widely used in the response generation and neural machine translation to model the potential relationship between two sentences. It typically consists of two parts: an encoder that reads from the source sentence and a decoder that generates the target sentence word by word according to the encoder's output and the last generated word. However, it faces to the ``cold start'' problem when generating the first word as there is no previous word to refer. Existing work mainly use a special start symbol ``$<$/s$>$'' to generate the first word. An obvious drawback of these work is that there is not a learnable relationship between words and the start symbol. Furthermore, it may lead to the error accumulation for decoding when the first word is incorrectly generated. In this paper, we proposed a novel approach to learning to generate the first word in the sequence to sequence architecture rather than using the start symbol. Experimental results on the task of response generation of short text conversation show that the proposed approach outperforms the state-of-the-art approach in both of the automatic and manual evaluations. 
   When humans read text, they fixate some words and skip others.   However, there have been few attempts to explain skipping behavior   with computational models, as most existing work has focused on   predicting reading times . In this paper, we   propose a novel approach that models both skipping and reading,   using an unsupervised architecture that combines a neural attention   with autoencoding, trained on raw text using reinforcement learning.   Our model explains human reading behavior as a tradeoff between   precision of language understanding    and economy of attention . We   evaluate the model on the Dundee eye-tracking corpus, showing that   it accurately predicts skipping behavior and reading times, is   competitive with surprisal, and captures known qualitative features   of human reading. 
 While word embeddings are currently predominant for natural language processing, most of existing models learn them solely from their contexts. However, these context-based word embeddings are limited since not all words' meaning can be learned based on only context. Moreover, it is also difficult to learn the representation of the rare words due to data sparsity problem. In this work, we address these issues by learning the representations of words by integrating their intrinsic  and extrinsic  information. To prove the effectiveness of our model, we evaluate it on four tasks, including , ,, and . Experiment results show that our model is powerful in both word and document modeling. 
   In this work we release our extensible and easily configurable neural network training software.   It provides a rich set of functional layers with a particular focus on efficient training of   recurrent neural network topologies on multiple GPUs.   The source of the software package is public and freely available for academic research purposes and   can be used as a framework or as a standalone tool which supports a flexible configuration.   The software allows to train state-of-the-art deep bidirectional long short-term memory  models   on both one dimensional data like speech or two dimensional data like handwritten text and was    used to develop successful submission systems in several evaluation campaigns.   %It can be   %applied to a variety of natural language processing tasks and also supports more exotic   %components such as attention-based end-to-end networks or associative LSTMs. 
 Sequence-to-sequence models with soft attention had significant success in machine translation, speech recognition, and question answering.  Though capable and easy to use, they require that the entirety of the input sequence is available at the beginning of inference, an assumption that is not valid for instantaneous translation and speech recognition.  To address this problem, we present a new method for solving sequence-to-sequence problems using hard online alignments instead of soft offline alignments.  The online alignments model is able to start producing outputs without the need to first process the entire input sequence.  A highly accurate online sequence-to-sequence model is useful because it can be used to build an accurate voice-based instantaneous translator. Our model uses hard binary stochastic decisions to select the timesteps at which outputs will be produced. The model is trained to produce these stochastic decisions using a standard policy gradient method.  In our experiments, we show that this model achieves encouraging performance on TIMIT and Wall Street Journal  speech recognition datasets. 
 We present a self-contained system for constructing natural language models for use in text compression. Our system improves upon previous neural network based models by utilizing recent advances in syntactic parsing -- Google's SyntaxNet -- to augment character-level recurrent neural networks. RNNs have proven exceptional in modeling sequence data such as text, as their architecture allows for modeling of long-term contextual information. Modeling and coding are the backbone of modern compression schemes. While coding is considered a solved problem, generating effective, domain-specific models remains a critical step in the process of improving compression ratios.  
 	Many social media platforms offer a mechanism for readers to react to 	comments, both positively and negatively, which in aggregate can be thought of 	as community endorsement. 	This paper addresses the problem of predicting community endorsement in online 	discussions, leveraging both the participant response structure and 	the text of the comment. 	The different types of features are integrated in a neural network that         uses a novel architecture to learn latent modes 	of discussion structure that perform as well as deep neural networks but are 	more interpretable. 	In addition, the latent modes can be used to weight text features thereby 	improving prediction accuracy. 
 We introduce a novel latent vector space model that jointly learns the latent representations of words, e-commerce products and a mapping between the two without the need for explicit annotations. The power of the model lies in its ability to directly model the discriminative relation between products and a particular word. We compare our method to existing latent vector space models  and evaluate it as a feature in a learning to rank setting. Our latent vector space model achieves its enhanced performance as it learns better product representations. Furthermore, the mapping from words to products and the representations of words benefit directly from the errors propagated back from the product representations during parameter estimation. We provide an in-depth analysis of the performance of our model and analyze the structure of the learned representations. 
 This paper proposes  --- a multi-turn dialogue agent which helps users search Knowledge Bases  without composing complicated queries. Such goal-oriented dialogue agents typically need to interact with an external database to access real-world knowledge. Previous systems achieved this by issuing a symbolic query to the KB to retrieve entries based on their attributes. However, such symbolic operations break the differentiability of the system and prevent end-to-end training of neural dialogue agents. In this paper, we address this limitation by replacing symbolic queries with an induced ``soft'' posterior distribution over the KB that indicates which entities the user is interested in. Integrating the soft retrieval process with a reinforcement learner leads to higher task success rate and reward in both simulations and against real users.  We also present a fully neural end-to-end agent, trained entirely from user feedback, and discuss its application towards personalized dialogue agents. % that provides users with an entity from a knowledge base  by interactively asking for its attributes.  % All components of the KB-InfoBot are trained in an end-to-end fashion using reinforcement learning. % Goal-oriented dialogue systems typically need to interact with an external database to access real-world knowledge . % Previous systems achieved this by issuing a symbolic query to the database and adding retrieved results to the dialogue state. % However, such symbolic operations break the differentiability of the system and prevent end-to-end training of neural dialogue agents. % In this paper, we address this limitation by replacing symbolic queries with an induced ``soft'' posterior distribution over the KB that indicates which entities the user is interested in. % We also provide a modified version of the episodic REINFORCE algorithm, which allows the KB-InfoBot to explore and learn both the policy for selecting dialogue acts and the posterior over the KB for retrieving the correct entities. % Experimental results show that the end-to-end trained KB-InfoBot outperforms competitive rule-based baselines, as well as agents which are not end-to-end trainable. 
 The negative sampling  objective function, used in , is a simplification of the Noise Contrastive Estimation  method. NEG was found to be highly effective in learning continuous word representations. However, unlike NCE, it was considered inapplicable for the purpose of learning the parameters of a language model. In this study, we refute this assertion by providing a principled derivation for NEG-based language modeling, founded on a novel analysis of a low-dimensional approximation of the matrix of pointwise mutual information between the contexts and the predicted words.   The obtained  language modeling is closely related to NCE language models but is based on a simplified objective function.     We thus provide a unified formulation for two main language processing tasks, namely word embedding and language modeling, based on the NEG objective function.   Experimental results on two popular language modeling benchmarks show comparable perplexity results, with a small advantage to NEG over NCE. 
     Attention-based encoder-decoder neural network models have recently shown promising results in machine translation and speech recognition. In this work, we propose an attention-based neural network model for joint intent detection and slot filling, both of which are critical steps for many speech understanding and dialog systems. Unlike in machine translation and speech recognition, alignment is explicit in slot filling. We explore different strategies in incorporating this alignment information to the encoder-decoder framework. Learning from the attention mechanism in encoder-decoder model, we further propose introducing attention to the alignment-based RNN models. Such attentions provide additional information to the intent classification and slot label prediction. Our independent task models achieve state-of-the-art intent detection error rate and slot filling F1 score on the benchmark ATIS task. Our joint training model further obtains 0.56\% absolute  error reduction on intent detection and 0.23\% absolute gain on slot filling over the independent task models.   
     Speaker intent detection and semantic slot filling are two critical tasks in spoken language understanding  for dialogue systems. In this paper, we describe a recurrent neural network  model that jointly performs intent detection, slot filling, and language modeling. The neural network model keeps updating the intent prediction as word in the transcribed utterance arrives and uses it as contextual features in the joint model. Evaluation of the language model and online SLU model is made on the ATIS benchmarking data set. On language modeling task, our joint model achieves 11.8\% relative reduction on perplexity comparing to the independent training language model. On SLU tasks, our joint model outperforms the independent task training model by 22.3\% on intent detection error rate, with slight degradation on slot filling F1 score. The joint model also shows advantageous performance in the realistic ASR settings with noisy speech input. 
 Language change is a complex social phenomenon, revealing pathways of communication and sociocultural influence.  But, while language change has long been a topic of study in sociolinguistics, traditional linguistic research methods rely on circumstantial evidence, estimating the direction of change from differences between older and younger speakers.  In this paper, we use a data set of several million Twitter users to track language changes in progress.  First, we show that language change can be viewed as a form of social influence: we observe complex contagion for phonetic spellings and ``netspeak'' abbreviations , but not for older dialect markers from spoken language. Next, we test whether specific types of social network connections are more influential than others, using a parametric Hawkes process model. We find that tie strength plays an important role: densely embedded social ties are significantly better conduits of linguistic influence. Geographic locality appears to play a more limited role: we find relatively little evidence to support the hypothesis that individuals are more influenced by geographically local social ties, even in their usage of geographical dialect markers. 
 Robotic commands in natural language usually contain various spatial descriptions that are semantically similar but syntactically different.  Mapping such syntactic variants into semantic concepts that can be understood by robots is challenging due to the high flexibility of natural language expressions. To tackle this problem, we collect robotic commands for navigation and manipulation tasks using crowdsourcing.  We further define a robot language and use a generative machine translation model to translate robotic commands from natural language to robot language.  The main purpose of this paper is to simulate the interaction process between human and robots using crowdsourcing platforms, and investigate the possibility of translating natural language to robot language with paraphrases.  %By jointly learning the spatial relationship across two different but similar task domains, we demonstrate the effectiveness of our method in improving the grounding performance. %Learning a semantic lexicon is often a fundamental first step in building a human machine interaction system that can interpret the meaning of natural language. It is especially important in machine teaching where a human partner teaches an unseen semantic lexicon to an intelligent machine.  To interactively learn an unseen semantic lexicon, we propose an active learning approach for semantic lexicon learning, which allows a machine to perform semantic reasoning over a sentence and actively select ambiguous lexicon to enquiry its human partner.  By exploiting the extracted semantic grammars during the human-machine interaction, our designed system can gradually mimic its human partner and expressively generate natural language description with rich semantic meaning.  
 This paper describes our deep learning-based approach to sentiment analysis in Twitter as part of SemEval-2016 Task 4. We use a convolutional neural network to determine sentiment and participate in all subtasks, i.e. two-point, three-point, and five-point scale sentiment classification and two-point and five-point scale sentiment quantification. We achieve competitive results for two-point scale sentiment classification and quantification, ranking fifth and a close fourth  respectively despite using only pre-trained embeddings that contain no sentiment information. We achieve good performance on three-point scale sentiment classification, ranking eighth out of 35, while performing poorly on five-point scale sentiment classification and quantification. An error analysis reveals that this is due to low expressiveness of the model to capture negative sentiment as well as an inability to take into account ordinal information. We propose improvements in order to address these and other issues. 
 This paper describes our deep learning-based approach to multilingual aspect-based sentiment analysis as part of SemEval 2016 Task 5. We use a convolutional neural network  for both aspect extraction and aspect-based sentiment analysis. We cast aspect extraction as a multi-label classification problem, outputting probabilities over aspects parameterized by a threshold. To determine the sentiment towards an aspect, we concatenate an aspect vector with every word embedding and apply a convolution over it. Our constrained system  achieves competitive results across all languages and domains, placing first or second in 5 and 7 out of 11 language-domain pairs for aspect category detection  and sentiment polarity  respectively, thereby demonstrating the viability of a deep learning-based approach for multilingual aspect-based sentiment analysis. 
 Spoken dialogue systems allow humans to interact with machines using natural speech. As such, they have many benefits. By using speech as the primary communication medium, a computer interface can facilitate swift, human-like acquisition of information. In recent years, speech interfaces have become ever more popular, as is evident from the rise of personal assistants such as Siri, Google Now, Cortana and Amazon Alexa. Recently, data-driven machine learning methods have been applied to dialogue modelling and the results achieved for limited-domain applications are comparable to or out-perform traditional approaches. Methods based on Gaussian processes are particularly effective as they enable good models to be estimated from limited training data.  Furthermore, they provide an explicit estimate of the uncertainty which is particularly useful for reinforcement learning. This article explores the additional steps that are necessary to extend these methods to model multiple dialogue domains. We show that Gaussian process reinforcement learning is an elegant framework that naturally supports a range of methods, including prior knowledge,  Bayesian committee machines and multi-agent learning, for facilitating extensible and adaptable dialogue systems.  
  The purpose of this document is to provide both the basic paper template and submission guidelines. Abstracts should be a single paragraph, between 4--6 sentences long, ideally.  Gross violations will trigger corrections at the camera-ready phase. 
 The attention mechanism is an important part of the neural machine translation  where it was reported to produce richer source representation compared to fixed-length encoding sequence-to-sequence models. Recently, the effectiveness of attention has also been explored in the context of image captioning. In this work, we assess the feasibility of a multimodal attention mechanism that simultaneously focus over an image and its natural language description for generating a description in another language. We train several variants of our proposed attention mechanism on the Multi30k multilingual image captioning dataset. We show that a dedicated attention for each modality achieves up to 1.6 points in BLEU and METEOR compared to a textual NMT baseline. 
 The attention mechanisim is appealing for neural machine translation, since it is able to dynamically encode a source sentence by generating a alignment between a target word and source words. Unfortunately, it has been proved to be worse than conventional alignment models in aligment accuracy. In this paper, we analyze and explain this issue from the point view of reordering, and propose a supervised attention which is  learned with guidance from conventional alignment models. Experiments on two Chinese-to-English translation tasks  show that the supervised attention mechanism yields better alignments leading to substantial gains over the standard attention based NMT. 
 Machine transliteration is the process of automatically transforming the script of a word from a source language to a target language, while preserving pronunciation. Sequence to sequence learning has recently emerged as a new paradigm in supervised learning. In this paper a character-based encoder-decoder model has been proposed that consists of two Recurrent Neural Networks. The encoder is a Bidirectional recurrent neural network that encodes a sequence of symbols into a fixed-length vector representation, and the decoder generates the target sequence using an attention-based recurrent neural network. The encoder, the decoder and the attention mechanism are jointly trained to maximize the conditional probability of a target sequence given a source sequence. Our experiments on different datasets show that the proposed encoder-decoder model is able to achieve significantly higher transliteration quality over traditional statistical models.  
 We present a new approach for neural machine translation  using the morphological and grammatical decomposition of the words  in the output side of the neural network.  This architecture addresses two main problems occurring in MT, namely dealing with a large target language vocabulary and the out of vocabulary  words. By the means of factors, we are able to handle larger vocabulary and reduce the training time .  In addition, we can produce new words that are not in the vocabulary. We use a morphological analyser to get a factored representation of each word .  We have extended the NMT approach with attention mechanism~ in order to have two different outputs  % , one for the lemmas and the other for the rest of the factors. The final translation is built using some a priori linguistic information.  We compare our extension with a word-based NMT system.  The experiments, performed on the IWSLT'15 dataset translating from English to French, show that while the performance do not always increase, the system can manage a much larger vocabulary and consistently reduce the OOV rate.  We observe up to 2\% BLEU point improvement in a simulated out of domain translation setup. %We modified the standard NMT architecture to enable the generation of two different streams of symbols: the lemmas and the factors. %Those factors are used to train only with these outputs in the target language instead of training with the full vocabulary with the completed words.  %This idea could perform better in morphologically richer languages like French compared to English.  
 User-machine interaction is important for spoken content retrieval. For text content retrieval, the user can easily scan through and select on a list of retrieved item. This is impossible for spoken content retrieval, because the retrieved items are difficult to show on screen. Besides, due to the high degree of uncertainty for speech recognition, the retrieval results can be very noisy. One way to counter such difficulties is through user-machine interaction. The machine can take different actions to interact with the user to obtain better retrieval results before showing to the user. The suitable actions depend on the retrieval status, for example requesting for extra information from the user, returning a list of topics for user to select, etc. In our previous work, some hand-crafted states estimated from the present retrieval results are used to determine the proper actions. In this paper, we propose to use Deep-Q-Learning techniques instead to determine the machine actions for interactive spoken content retrieval. Deep-Q-Learning bypasses the need for estimation of the hand-crafted states, and directly determine the best action base on the present retrieval status even without any human knowledge. It is shown to achieve significantly better performance compared with the previous hand-crafted states.    
 This paper advances the design of CTC-based all-neural  speech recognizers. We propose a novel symbol inventory, and a novel iterated-CTC method in which a second system is used to transform a noisy initial output into a cleaner version.  We present a number of stabilization and initialization methods we have found useful in training these networks.     We evaluate our system on the commonly used NIST 2000 conversational telephony test set, and significantly  exceed the previously published performance of similar systems, both with and without the use of an external language model and decoding technology.  
 % Deep neural networks have achieved remarkable results across many tasks, but have been shown to be vulnerable to adversarial attacks, which can lead to counter-intuitive outputs.   Deep neural networks have achieved remarkable results across many   language processing tasks, however these methods are highly sensitive to noise and adversarial attacks. % This paper reports a Convolutional Neural Network  model experimented on a sentence-level sentiment analysis task. % Sentiment analysis of short text such as Twitter posts is a challenging task because of the limited contextual information and noise-accompanied such as informal usage of words like abbreviations and oral expressions. % We show that using the proposed regularization method can help improve the robustness of neural model.   We present a regularization based method for limiting network sensitivity to its   inputs, inspired by ideas from computer vision, thus learning models   that are more robust.   Empirical evaluation over a range of sentiment datasets with a   convolutional neural network shows that, compared to a baseline   model and the dropout method, our method achieves superior   performance over noisy inputs and out-of-domain   data.\footnote{Implementation     available at \url{https://github.com/lrank/Robust-Representation}.} 
  Recognizing implicit discourse relations is a challenging but important task in the field of Natural Language Processing. For such a complex text processing task, different from previous studies, we argue that it is necessary to repeatedly read the arguments  and dynamically exploit the efficient features useful for recognizing discourse relations. To mimic the repeated reading strategy, we propose the neural networks with multi-level attention , combining the attention mechanism and  external memories to gradually fix the attention on some specific words  helpful to judging the discourse relations. Experiments on the PDTB dataset show that our proposed method achieves the state-of-art results. The visualization of the attention weights also illustrates the progress that our model  observes the arguments on  each level and progressively locates the important words. 
 Neural machine translation  becomes a new state-of-the-art and achieves promising translation results using a simple encoder-decoder neural network. This neural network is trained once on the parallel corpus and the fixed network is used to translate all the test sentences. We argue that the general fixed network cannot best fit the specific test sentences. In this paper, we propose the dynamic NMT which learns a general network as usual, and then fine-tunes the network for each test sentence. The fine-tune work is done on a small set of the bilingual training data that is obtained through similarity search according to the test sentence. Extensive experiments demonstrate that this method can significantly improve the translation performance, especially when highly similar sentences are available.   
 We compare policy differences across institutions by embedding representations of the entire legal corpus of each institution and the vocabulary shared across all corpora into a continuous vector space. We apply our method, Gov2Vec, to Supreme Court opinions, Presidential actions, and official summaries of Congressional bills. The model discerns meaningful differences between government branches. We also learn representations for more fine-grained word sources: individual Presidents and  Congresses. The similarities between learned representations of Congresses over time and sitting Presidents are negatively correlated with the bill veto rate, and the temporal ordering of Presidents and Congresses was implicitly learned from only text. With the resulting vectors we answer questions such as: how does Obama and the 113th House differ in addressing climate change and how does this vary from environmental or economic perspectives? Our work illustrates vector-arithmetic-based investigations of complex relationships between word sources based on their texts. We are extending this to create a more comprehensive legal semantic map. 
 Convolutional neural networks  have demonstrated superior capability for extracting information from raw signals in computer vision. Recently, character-level and multi-channel CNNs have exhibited excellent performance for sentence classification tasks. % We apply CNNs to large-scale authorship attribution, which aims to determine an unknown text's author among many candidate authors, motivated by their ability to process character-level signals and to differentiate between a large number of classes, while making fast predictions in comparison to state-of-the-art approaches. % We extensively evaluate CNN-based approaches that leverage word and character channels and compare them against state-of-the-art methods for a large range of author numbers, shedding new light on traditional approaches. We show that character-level CNNs outperform the state-of-the-art on four out of five datasets in different domains. Additionally, we present the first application of authorship attribution to reddit. Finally, we release our new reddit and Twitter datasets for further research. 
 Recently, there has been an increasing interest in end-to-end speech recognition that directly transcribes speech to text without any predefined alignments. One approach is the attention-based encoder-decoder framework that learns a mapping between variable-length input and output sequences in one step using a purely data-driven method. The attention model has often been shown to improve the performance over another end-to-end approach, the Connectionist Temporal Classification , mainly because it explicitly uses the history of the target character without any conditional independence assumptions. However, we observed that the performance of the attention has shown poor results in noisy condition and is hard to learn in the initial training stage with long input sequences. This is because the attention model is too flexible to predict proper alignments in such cases due to the lack of left-to-right constraints as used in CTC. This paper presents a novel method for end-to-end speech recognition to improve robustness and achieve fast convergence by using a joint CTC-attention model within the multi-task learning framework, thereby mitigating the alignment issue. An experiment on the WSJ and CHiME-4 tasks demonstrates its advantages over both the CTC and attention-based encoder-decoder baselines, showing 5.4-14.6\% relative improvements in Character Error Rate .  
 Twitter data is extremely noisy -- each tweet is short, unstructured and with informal  language, a challenge for current topic modeling. On the other hand, tweets are  accompanied by extra information such as authorship, hashtags and the user-follower  network. Exploiting this additional information, we propose the Twitter-Network   topic model to jointly model the text and the social network in a full Bayesian  nonparametric way. The TN topic model employs the hierarchical Poisson-Dirichlet processes   for text modeling and a Gaussian process random function model for social network  modeling. We show that the TN topic model significantly outperforms several existing  nonparametric models due to its flexibility. % including the author topic model. Moreover, the TN topic model enables additional informative inference such as authors'  interests, hashtag analysis, as well as leading to further applications such as  author recommendation, automatic topic labeling and hashtag suggestion. Note our  general inference framework can readily be applied to other topic models with embedded  PDP nodes.  %  % The results provided by modeling authorship,  %hashtags and the social networks are shown to be informative and useful. %Additionally, we develop an inference framework in which Gibbs sampling can be  %performed on arbitrary topic models with any network of Poisson-Dirichlet  %process nodes, allowing fast development of topic models. % % [The model works generally on any data that contains the relevant information, not just tweets.] % Experiment results show promising model performances compared to related models... 
  %BP: I would downplay the POS "not fitting well" issue in the abstract  %The problem of transferring semantic information across languages is central to multilingual semantic processing. %Many semantic processing systems rely on sources of information such as Part-of-Speech  tags. %However, such tagsets contain both insufficient and irrelevant information for multilingual semantic processing. %We propose a semantic tagset which is tailored for the purposes of multilingual semantic parsing. We propose a novel semantic tagging task, sem-tagging, tailored for the purpose of multilingual semantic parsing, and present the first tagger using deep residual networks . Our tagger uses both word and character representations, and includes a novel residual bypass architecture. We evaluate the tagset both intrinsically on the new task of semantic tagging, as well as on Part-of-Speech  tagging. %, showing that we obtain state-of-the-art results for POS on the Universal Dependencies v1.2  and v1.3  datasets, using a system consisting of a ResNet and an auxiliary loss function based on our novel semantic tagset. Our system, consisting of a ResNet and an auxiliary loss function predicting our semantic tags, significantly outperforms prior results on English Universal Dependencies POS tagging . %Furthermore, we are the first to develop a tagger using deep residual networks , and propose a novel residual bypass architecture. %, a recently emerged convolutional neural network architecture. %We also introduce a novel residual bypass function inspired by ResNets. %This %Our tagger, which uses both word and character representations, is evaluated on both semantic tagging and POS tagging. %We evaluate our tagger on both semantic tagging and English POS tagging, obtaining state-of-the-art results for POS on the Universal Dependencies v1.2  and v1.3  datasets, using a system consisting of a ResNet with our residual bypass function, and an auxiliary loss function based on our semantic tagset. 
  Textual information is considered as significant supplement to knowledge representation learning . There are two main challenges for constructing knowledge representations from plain texts:  How to take full advantages of sequential contexts of entities in plain texts for KRL.  How to dynamically select those informative sentences of the corresponding entities for KRL. In this paper, we propose the Sequential Text-embodied Knowledge Representation Learning to build knowledge representations from multiple sentences. Given each reference sentence of an entity, we first utilize recurrent neural network with pooling or long short-term memory network to encode the semantic information of the sentence with respect to the entity. Then we further design an attention model to measure the informativeness of each sentence, and build text-based representations of entities. We evaluate our method on two tasks, including triple classification and link prediction. Experimental results demonstrate that our method outperforms other baselines on both tasks, which indicates that our method is capable of selecting informative sentences and encoding the textual information well into knowledge representations. 
 Neural network based models have achieved impressive results on various specific tasks. However, in previous works, most models are learned separately based on single-task supervised objectives, which often suffer from insufficient training data. In this paper, we propose two deep architectures which can be trained jointly on multiple related tasks. More specifically, we augment neural model with an external memory, which is shared by several tasks. Experiments on two groups of text classification tasks show that our proposed architectures can improve the performance of a task with the help of other related tasks. 
 Distantly supervised relation extraction has been widely used to find novel relational facts from plain text. To predict the relation between a pair of two target entities, existing methods solely rely on those direct sentences containing both entities. In fact, there are also many sentences containing only one of the target entities, which also provide rich useful information but not yet employed by relation extraction. To address this issue, we build inference chains between two target entities via intermediate entities, and propose a path-based neural relation extraction model to encode the relational semantics from both direct sentences and inference chains. Experimental results on real-world datasets show that, our model can make full use of those sentences containing only one target entity, and achieves significant and consistent improvements on relation extraction as compared with strong baselines. The source code of this paper can be obtained from \url{https://github.com/thunlp/PathNRE}.  %{https://github.com/thunlp/PathNRE.}   %\footnote{The code and dataset will be released in the future.} 
 Discriminating between closely-related language varieties is considered a challenging and important task. This paper describes our submission to the DSL 2016 shared-task, which included two sub-tasks: one on discriminating similar languages and one on identifying Arabic dialects.  We developed a character-level neural network for this task. Given a sequence of characters, our model embeds each character in vector space, runs the sequence through multiple convolutions with different filter widths, and pools the convolutional representations to obtain a hidden vector representation of the text that is used for predicting the language or dialect. We primarily focused on the Arabic dialect identification task and obtained an F1 score of 0.4834, ranking 6th out of 18 participants.  We also analyze errors made by our system on the Arabic data in some detail, and point to challenges such an approach is faced with.\footnote{The code for this work is available at \url{https://github.com/boknilev/dsl-char-cnn}.} 
 Drug name recognition  is an essential step in the Pharmacovigilance  pipeline. DNR aims to find drug name mentions in unstructured biomedical texts and classify them into predefined categories. State-of-the-art DNR approaches heavily rely on hand-crafted features and domain-specific resources which are difficult to collect and tune. For this reason, this paper investigates the effectiveness of contemporary recurrent neural architectures - the Elman and Jordan networks and the bidirectional LSTM with CRF decoding - at performing DNR straight from the text. The experimental results achieved on the authoritative SemEval-2013 Task 9.1 benchmarks show that the bidirectional LSTM-CRF ranks closely to highly-dedicated, hand-crafted systems. 
 Neural machine translation  heavily relies on word-level modelling to learn semantic representations of input sentences. However, for languages without natural word delimiters  where input sentences have to be tokenized first, conventional NMT is confronted with two issues: 1) it is difficult to find an optimal tokenization granularity for source sentence modelling, and 2) errors in 1-best tokenizations may propagate to the encoder of NMT. To handle these issues, we propose word-lattice based Recurrent Neural Network  encoders for NMT, which generalize the standard RNN to word lattice topology. The proposed encoders take as input a word lattice that compactly encodes multiple tokenizations, and learn to generate new hidden states from arbitrarily many inputs and hidden states in preceding time steps. As such, the word-lattice based encoders not only alleviate the negative impact of tokenization errors but also are more expressive and flexible to embed input sentences. Experiment results on Chinese-English translation demonstrate the superiorities of the proposed encoders over the conventional encoder. 
   This document contains the instructions for preparing a camera-ready   manuscript for the proceedings of ACL-2015. The document itself   conforms to its own specifications, and is therefore an example of   what your manuscript should look like. These instructions should be   used for both papers submitted for review and for final versions of   accepted papers.  Authors are asked to conform to all the directions   reported in this document. 
  %We exploit machine learning to optimize translation %in multilingual question answering .  In multilingual question answering, either  the question needs to be translated into the document language, or vice versa. In addition to , there are multiple  to perform the translation, four of which we  explore in this paper:\ word-based, 10-best, context-based, and grammar-based. We  build a feature for each combination of translation  and ,  and train a model that learns optimal feature weights. %Using this model to rank answers is statistically  %significantly better translating all text into English.  On a large forum dataset consisting of posts in English, Arabic, and Chinese,  our novel  approach was more effective than a strong baseline :\ translating all text into English, then training  a classifier based only on English  text. %\red{Baseline = 1-best translation of question into doc language. Cosine similarity between tf-idf vectors representing %question and document, both in doc language.}   %%The World Wide Web is expanding and evolving into a vast network of less structured, more informal, multilingual, and diverse data.  %This paper presents an approach for question answering  in multilingual collections. %%that attempts to address the multilingual and informal aspect of the Web.  %We incorporated the multilingual aspect into our answer ranking models in two ways:\ %First, we introduced novel bilingual features based on a two-way probabilistic translation of  %both questions and answers. Second, we filtered available training data based on language-related criteria. %In addition to improvements in modeling, we also explored a more language-aware approach,  %by building multiple language-specific answer ranking models, and merging the answers. %%also implemented language-specific answer ranking and compared two approaches for applying these classifiers to multilingual QA. %%One approach uses a single classifier to score all question-answer pairs , while the other applies  %%a separate custom classifier for each language in the collection -- in the latter case, answers are merged in the final stage.  %%Experiments were conducted on the DARPA BOLT task, consisting of a collection of Arabic, Chinese, and English Web forum  %%posts, and a set of complex non-factoid questions expressed in English.  %Experiments on a large collection of forum threads revealed that %%we present valuable empirical results on the design and implementation of multilingual QA  %%systems. In two out of the three tasks that retrieve non-English answers, results show that  %%we show the effectiveness of our answer ranking model:\  %our novel answer ranking model  %%including bilingual features and  %%selecting data based on language  %yields statistically significant improvements.  %While using multiple classifiers did not yield the expected increase in overall effectiveness, we present  %arguments for further exploration of this framework. %% for ranking answers in multilingual collections %%that deserves further exploration.  % % %{:\ Information Search and Retrieval % % % %{ multilingual data, question answering, clqa 
  %It is an important challenge to rapidly acquire information from low-resource languages.  %The content analysis of low-resource languages is a challenge for natural language processing and how to rapidly acquire information from low-resource languages has always been a research focus.  %need to emphasize the importance of streaming data   Aligning coordinated text streams from multiple sources and multiple languages has opened many new research venues on cross-lingual knowledge discovery. In this paper we aim to advance state-of-the-art by: . extending coarse-grained topic-level knowledge mining to fine-grained information units such as entities and events; . following a novel ``'' paradigm to construct and utilize network structures to capture and propagate reliable evidence.  We introduce a novel Burst Information Network  representation that can display the most important information and illustrate the connections among bursty entities, events and keywords in the corpus. We propose an effective approach to construct and decipher BINets, incorporating novel criteria based on multi-dimensional clues from pronunciation, translation, burst, neighbor and graph topological structure. The experimental results on Chinese and English coordinated text streams show that our approach can accurately decipher the nodes with high confidence in the BINets and that the algorithm can be efficiently run in parallel, which makes it possible to apply it to huge amounts of streaming data for never-ending language and information decipherment.   {Database Management}{Database Applications}[Data mining] {Artificial Intelligence}{Natural Language Processing}[Text analysis]     %  for constructing and deciphering the BINet of a low resource language using comparable corpora.  %Experimental results show the effectiveness of our approach and potential values of this task.   %The content analysis of low-resource languages is a challenge for natural language processing and how to rapidly acquire information from low-resource languages has always been a research focus. In this paper, we introduce a bursty information network  that can display the most important information and illustrate connections of keywords in the corpus. We propose the methodology for constructing and deciphering the BINet of a low resource language using comparable corpora.  %Experimental results show the effectiveness of our approach and potential values of this task.   
    Research on multilingual speech recognition remains attractive yet challenging. %  A key point of this research is how the utilize the commonality and diversity among different languages.   Recent studies focus on learning shared structures under the multi-task paradigm,   in particular a feature sharing structure.   This approach has been found effective to improve performance on each  language.   However, this approach is only useful when the deployed system supports   just one language. In a true multilingual scenario where multiple languages are   allowed, performance will be significantly reduced due to the competition among   languages in the decoding space.    This paper presents a multi-task recurrent model   that involves a multilingual speech recognition  component and a language recognition    component, and the ASR component is informed of the language information by the   LR component, leading to a language-aware recognition.  %  presents a multi-task recurrent model for bilingual  speech recognition, %  employing both the commonality in two languages via a shared network for feature extraction %  and simultaneously the diversity between two languages provided %  by a language identification network which interacts recurrently with the former one.   We tested the approach on an English-Chinese bilingual recognition task.   The results show that the proposed multi-task recurrent model can improve   performance of multilingual recognition systems.    
 Many current natural language processing applications for social media rely on representation learning and utilize pre-trained word embeddings. There currently exist several publicly-available, pre-trained sets of word embeddings, but they contain few or no emoji representations even as emoji usage in social media has increased. In this paper we release \verb~emoji2vec~, pre-trained embeddings for all Unicode emojis which are learned from their description in the Unicode emoji standard.\footnote{\url{http://www.unicode.org/emoji/charts/full-emoji-list.html}} The resulting emoji embeddings can be readily used in downstream social natural language processing applications alongside \verb~word2vec~. We demonstrate, for the downstream task of sentiment analysis, that emoji embeddings learned from short descriptions outperforms a skip-gram model trained on a large collection of tweets, while avoiding the need for contexts in which emojis need to appear frequently in order to estimate a representation. 
 Motivated by the need to automate medical information extraction from free-text radiological reports, we present a bi-directional long short-term memory  neural network architecture for modelling radiological language. The model has been used to address two NLP tasks: medical named-entity recognition  and negation detection. We investigate whether learning several types of word embeddings improves BiLSTM's performance on those tasks. Using a large dataset of chest x-ray reports, we compare the proposed model to a baseline dictionary-based NER system and a negation detection system that leverages the hand-crafted rules of the NegEx algorithm and the grammatical relations obtained from the Stanford Dependency Parser. Compared to these more traditional rule-based systems, we argue that BiLSTM offers a strong alternative for both our tasks.  
 Coreference resolution systems are typically trained with heuristic loss functions that require careful tuning. In this paper we instead apply reinforcement learning to directly optimize a neural mention-ranking model for coreference evaluation metrics. We experiment with two approaches: the {\sc reinforce} policy gradient algorithm and a reward-rescaled max-margin objective. We find the latter to be more effective, resulting in significant improvements over the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task. 
 We report on our system for the shared task on discrimination of similar languages . The system uses only byte representations in a deep residual network . The system, named ResIdent, is trained only on the data released with the task . We obtain 84.88\% accuracy on subtask A, 68.80\% accuracy on subtask B1, and 69.80\% accuracy on subtask B2. %Although the performance exceeds baseline levels, our system is outperformed by most competing systems. A large difference in accuracy on development data can be observed with relatively minor changes in our network's architecture and hyperparameters. We therefore expect fine-tuning of these parameters to yield higher accuracies. 
 In this work, we present the first results for neuralizing an Unsupervised Hidden Markov Model.  We evaluate our approach on tag induction.  Our approach outperforms existing generative models and is competitive with the state-of-the-art though with a simpler model easily extended to include additional context. 
 Recently, much progress has been made in learning general-purpose sentence representations that can be used across domains. However, most of the existing models typically treat each word in a sentence equally. In contrast, extensive studies have proven that human read sentences efficiently by making a sequence of fixation and saccades. This motivates us to improve sentence representations by assigning different weights to the vectors of the component words, which can be treated as an attention mechanism on single sentences. To that end, we propose two novel attention models, in which the attention weights are derived using significant predictors of human reading time, i.e., Surprisal, POS tags and CCG supertags. The extensive experiments demonstrate that the proposed methods significantly improve upon the state-of-the-art sentence representation models. 
 % This work focuses on the rapid development of linguistic annotation tools for resource-poor languages. We experiment several cross-lingual annotation projection methods using Recurrent Neural Networks  models. The distinctive feature of our approach is that our multilingual word representation requires only a parallel corpus between the source and target language. More precisely, our method has the following characteristics:  it does not use word alignment information,  it does not assume any knowledge about foreign languages, which makes it applicable to a wide range of resource-poor languages,  it provides truly multilingual taggers. %. Our method is based on Recurrent Neural Networks .  %In a previous study, we proposed a method based on Simple RNN to automatically induce a Part-Of-Speech  tagger. In this paper, we propose an improvement of our neural model.  We investigate both uni- and bi-directional RNN models and propose a method to include external information  in the RNN to train higher level taggers . We demonstrate the validity and genericity of our model by using parallel corpora . Our experiments are conducted to induce cross-lingual POS and super sense taggers. 
   Neural encoder-decoder models have shown great success in many sequence generation tasks.   However, previous work has not investigated situations in which we would   like to control the length of encoder-decoder outputs.   %   This capability is crucial for applications such as text summarization,   in which we have to generate concise summaries with a desired length.   In this paper, we propose methods for controlling the output sequence length   for neural encoder-decoder models: two decoding-based methods and two   learning-based methods.\footnote{Available at https://github.com/kiyukuta/lencon.}   %   Results show that our learning-based methods have the capability to control   length without degrading summary quality in a summarization task. 
 This paper discusses lexicon word learning in high-dimensional meaning spaces from the viewpoint of referential uncertainty. We investigate  various state-of-the-art Machine Learning algorithms and discuss the impact  of scaling, representation and meaning space structure. We demonstrate that current Machine Learning techniques successfully deal with high-dimensional meaning spaces. In particular, we show that exponentially  increasing dimensions linearly impact learner performance and that referential uncertainty from word sensitivity has no impact. 
   Computation is classically studied in terms of automata, formal languages and algorithms; yet, the relation between neural dynamics and symbolic representations and operations is still unclear in traditional eliminative connectionism. Therefore, we suggest a unique perspective on this central issue, to which we would like to refer as to transparent connectionism, by proposing accounts of how symbolic computation can be implemented in neural substrates. In this study we first introduce a new model of dynamics on a symbolic space, the versatile shift, showing that it supports the real-time simulation of a range of automata. We then show that the G\"odelization of versatile shifts defines nonlinear dynamical automata, dynamical systems evolving on a vectorial space. Finally, we present a mapping between nonlinear dynamical automata and recurrent artificial neural networks. The mapping defines an architecture characterized by its granular modularity, where data, symbolic operations and their control are not only distinguishable in activation space, but also spatially localizable in the network itself, while maintaining a distributed encoding of symbolic representations. The resulting networks simulate automata in real-time and are programmed directly, in absence of network training. To discuss the unique characteristics of the architecture and their consequences, we present two examples: i) the design of a Central Pattern Generator from a finite-state locomotive controller, and ii) the creation of a network simulating a system of interactive automata that supports the parsing of garden-path sentences as investigated in psycholinguistics experiments. 
 Natural language understanding  is a core component of a spoken dialogue system. Recently recurrent neural networks  obtained strong results on NLU due to their superior ability of preserving sequential information over time. Traditionally, the NLU module tags semantic slots for utterances considering their flat structures, as the underlying RNN structure is a linear chain. However, natural language exhibits linguistic properties that provide rich, structured information for better understanding. This paper introduces a novel model, knowledge-guided structural attention networks , a generalization of RNN to additionally incorporate non-flat network topologies guided by prior knowledge. There are two characteristics: 1) important substructures can be captured from small training data, allowing the model to generalize to previously unseen test data; 2) the model automatically figures out the salient substructures that are essential to predict the semantic tags of the given sentences, so that the understanding performance can be improved. The experiments on the benchmark Air Travel Information System  data show that the proposed K-SAN architecture can effectively extract salient knowledge from substructures with an attention mechanism, and outperform the performance of the state-of-the-art neural network based frameworks. %\footnote{The code and pre-trained model will be released.}. 
 Recurrent neural network  based character-level language models  are extremely useful for modeling out-of-vocabulary words by nature. However, their performance is generally much worse than the word-level language models , since CLMs need to consider longer history of tokens to properly predict the next one. We address this problem by proposing hierarchical RNN architectures, which consist of multiple modules with different timescales. Despite the multi-timescale structures, the input and output layers operate with the character-level clock, which allows the existing RNN CLM training approaches to be directly applicable without any modifications. Our CLM models show better perplexity than Kneser-Ney  5-gram WLMs on the One Billion Word Benchmark with only 2\% of parameters. Also, we present real-time character-level end-to-end speech recognition examples on the Wall Street Journal  corpus, where replacing traditional mono-clock RNN CLMs with the proposed models results in better recognition accuracies even though the number of parameters are reduced to 30\%. 
   Entity images could provide significant visual information for knowledge representation learning. Most conventional methods learn knowledge representations merely from structured triples, ignoring rich visual information extracted from entity images. In this paper, we propose a novel Image-embodied Knowledge Representation Learning model , where knowledge representations are learned with both triple facts and images. More specifically, we first construct representations for all images of an entity with a neural image encoder. These image representations are then integrated into an aggregated image-based representation via an attention-based method. We evaluate our IKRL models on knowledge graph completion and triple classification. Experimental results demonstrate that our models outperform all baselines on both tasks, which indicates the significance of visual information for knowledge representations and the capability of our models in learning knowledge representations with images. 
    This paper presents a unified model to perform language and speaker recognition simultaneously and together.  This model is based on a multi-task recurrent neural network, where the output of one task is fed in as the input of the other, leading  to a collaborative learning framework that can improve both language and speaker recognition by sharing information between the tasks. The preliminary experiments presented in this paper demonstrate that the multi-task model outperforms similar task-specific models on both language and speaker tasks. The language recognition improvement is especially remarkable, which we believe is due to the speaker normalization effect caused by using the information from the speaker recognition component.    %[1] Replaced "borrowing from each other" with "sharing information between the tasks"   
 Recurrent neural networks  have shown clear superiority in sequence modeling, particularly the ones with gated units, such as long short-term memory  and gated recurrent unit . However, the dynamic properties behind the remarkable performance remain unclear in many applications, e.g., automatic speech recognition . This paper employs visualization techniques to study the behavior of LSTM and GRU when performing speech recognition tasks. Our experiments show some interesting patterns in the gated memory, and some of them have inspired simple yet effective modifications on the network structure. We report two of such modifications:  lazy cell update in LSTM, and  shortcut connections for residual learning. Both modifications lead to more comprehensible and powerful networks.  
 Recently, end-to-end memory networks have shown promising results on Question Answering task, which encode the past facts into an explicit memory and perform reasoning ability by making multiple computational steps on the memory. However, memory networks conduct the reasoning on sentence-level memory to output coarse semantic vectors and do not further take any attention mechanism to focus on words, which may lead to the model lose some detail information, especially when the answers are rare or unknown words. In this paper, we propose a novel Hierarchical Memory Networks, dubbed HMN. First, we encode the past facts into sentence-level memory and word-level memory respectively. Then, \-max pooling is exploited following reasoning module on the sentence-level memory to sample the \ most relevant sentences to a question and feed these sentences into attention mechanism on the word-level memory to focus the words in the selected sentences. Finally, the prediction is jointly learned over the outputs of the sentence-level reasoning module and the word-level attention mechanism. The experimental results demonstrate that our approach successfully conducts answer selection on unknown words and achieves a better performance than memory networks. 
   This document contains the instructions for preparing a camera-ready   manuscript for the proceedings of EACL-2017. The document itself   conforms to its own specifications, and is therefore an example of   what your manuscript should look like. These instructions should be   used for both papers submitted for review and for final versions of   accepted papers.  Authors are asked to conform to all the directions   reported in this document. 
 Automated discourse analysis tools based on Natural Language Processing  aiming at the diagnosis of language-impairing dementias generally extract several textual metrics of narrative transcripts. However, the absence of sentence boundary segmentation in the transcripts prevents the direct application of NLP methods which rely on these marks to function properly, such as taggers and parsers. We present the first steps taken towards automatic neuropsychological evaluation based on narrative discourse analysis, presenting a new automatic sentence segmentation method for impaired speech. Our model uses recurrent convolutional neural networks with prosodic, Part of Speech  features, and word embeddings. It was evaluated intrinsically on impaired, spontaneous speech, as well as, normal, prepared speech, and presents better results for healthy elderly   and Mild Cognitive Impairment  patients  than the Conditional Random Fields method  used in the same context of our study. The results suggest that our model is robust for impaired speech and can be used in automated discourse analysis tools to differentiate narratives produced by MCI and CTL. 
 %Although great progress has been made in automatic speech recognition , significant performance degradation still exists in noisy environments. Based on our previous work on very deep CNNs, this architecture is further developed in detail and proposed to improve the recognition accuracy for noise robust speech recognition. In the proposed very deep CNN architecture, the sizes of filters, poolings and input feature maps are adapted: the sizes of filters and poolings are reduced and dimensions of input feature maps are extended to allow adding more convolutional layers. Then the appropriate pooling, padding and input feature map selection strategies are developed on the very deep CNN to make it more robust for noisy scenario. In addition, an adaptation framework using joint training of very deep CNN with auxiliary features, and a system combination scheme using joint decoding with RNN are further explored. The proposed new model is evaluated on Aurora4 and the experiments show that the very deep CNN achieves a WER of 8.81\%, further 7.99\% with auxiliary feature joint training, and 7.09\% with RNN joint decoding, which is to our knowledge also the best published result on Aurora4, even without feature enhancement and sequence training. This paper describes the extension and optimisation of our previous work on very deep convolutional neural networks  for effective recognition of noisy speech in the Aurora 4 task. The appropriate number of convolutional layers, the sizes of the filters,  pooling operations and input feature maps are all modified: the filter and pooling sizes are reduced and dimensions of input feature maps are extended to allow adding more convolutional layers. Furthermore appropriate input padding and input feature map selection strategies are developed. In addition, an adaptation framework using joint training of very deep CNN with auxiliary features i-vector and fMLLR features is developed. These modifications give substantial word error rate reductions over the standard CNN used as baseline. Finally the very deep CNN is combined with an LSTM-RNN acoustic model and it is shown that state-level weighted log likelihood score combination in a joint acoustic model decoding scheme is very effective. On the  Aurora 4 task, the very deep CNN achieves a WER of 8.81\%, further 7.99\% with auxiliary feature joint training, and 7.09\% with LSTM-RNN joint decoding. 
 %  Simultaneous Machine Translation is an important problem.  %  Simultaneous Machine Translation is an important problem. %  Simultaneous Machine Translation is an important problem. %  Simultaneous Machine Translation is an important problem. %  Simultaneous Machine Translation is an important problem. %  Simultaneous Machine Translation is an important problem. %  Simultaneous Machine Translation is an important problem. %  Simultaneous Machine Translation is an important problem. %  Simultaneous Machine Translation is an important problem. % Different from usual machine translation  requirement, Translating in real-time, a.k.a.~simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods.  We propose a neural machine translation  framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively.% \footnote{Code and data can be found at \url{https://github.com/nyu-dl/dl4mt-simul-trans}.}   %output the translation words before the input sentence ended %-> output the translated words before the input sentence ends %Based on neural machine translation , we refine the problem as sequential decision making, and propose an unified framework %-> We propose a unified neural machine translation  framework for simultaneous translation %make decisions %-> make decisions on when to translate %we explore variant targets for delay to approach %-> we explore various targets for delay %design a simultaneous beam-search method to boost quality %-> design a method for beam-search applicable in the simultaneous MT setting  %   
 In this paper, a neural network based real-time speech recognition  system is developed using an FPGA for very low-power operation. The implemented system employs two recurrent neural networks ; one is a speech-to-character RNN for acoustic modeling  and the other is for character-level language modeling . The system also employs a statistical word-level LM to improve the recognition accuracy. The results of the AM, the character-level LM, and the word-level LM are combined using a fairly simple $N$-best search algorithm instead of the hidden Markov model  based network. The RNNs are implemented using massively parallel processing elements  for low latency and high throughput. The weights are quantized to 6 bits to store all of them in the on-chip memory of an FPGA. The proposed algorithm is implemented on a Xilinx XC7Z045, and the system can operate much faster than real-time.  
 A lot of prior work on event extraction has exploited a variety of features to represent events. Such methods have several drawbacks: 1) the features are often specific for a particular domain and do not generalize well;  2) the features are derived from various linguistic analyses and are error-prone; and 3) some features may be expensive and require domain expert. In this paper, we develop a Chinese event extraction system that uses word embedding vectors to represent language, and deep neural networks to learn the abstract feature representation  in order to greatly reduce the effort of feature engineering. In addition, in this framework, we leverage large amount of unlabeled data, which can address the problem of limited labeled corpus for this task. Our experiments show that our proposed method performs better compared to the system using rich language features, and using unlabeled data benefits the word embeddings. This study suggests the potential of DNN and word embedding for the event extraction task. 
  We will demonstrate a conversational products recommendation agent. This system shows how we combine research in personalized recommendation systems with research in dialogue systems to build a virtual sales agent. Based on new deep learning technologies we developed, the virtual agent is capable of learning how to interact with users, how to answer user questions, what is the next question to ask, and what to recommend when chatting with a human user.   Normally a descent conversational agent for a particular domain requires tens of thousands of hand labeled conversational data or hand written rules. This is a major barrier when launching a conversation agent for a new domain. We will explore and demonstrate the effectiveness of the learning solution even when there is no hand written rules or hand labeled training data.   
    %     Supervised algorithms excel on Natural Language Processing  tasks where their training  %and test data are taken from similar domains. Unfortunately, annotation costs and the large variety of linguistic  %domains deem the preparation of labeled training sets for a large number of domains impractical.  %Adaptation of models between training and test domains is hence a fundemental NLP challenge.             %, adapting models from domains rich in labeled training data to domains poor in such data,     %  Domain adaptation is a fundamental NLP challenge.        We introduce a neural network model that marries together ideas       from two prominent strands of research on domain adaptation through representation learning:  structural correspondence learning  and autoencoder neural       networks .  Our model is a three-layer NN that learns to encode the  non-pivot features of an input example into a low-dimensional representation, so that  the existence of pivot features  in the example can be decoded from that representation. The low-dimensional representation is then employed in a learning algorithm for the task. Moreover, we show how to inject pre-trained word embeddings into our model in order to improve generalization across examples with similar pivot features.  We experiment with the task of cross-domain sentiment classification on 16 domain pairs and show substantial improvements over strong baselines.\footnote{Our code is at: https://github.com/yftah89/Neural-SCL-Domain-Adaptation.} %both the SCL and the marginalized stacked denoising autoencoder .  %On the task of cross-domain product sentiment classification , consisting of 12 domain pairs, our model outperforms both the SCL and the marginalized stacked denoising autoencoder  methods by 3.77\% and  2.17\% respectively, on average across domain pairs.    
 Word embeddings have been demonstrated to benefit NLP tasks impressively. Yet, there is room for improvement in the vector representations, because current word embeddings typically contain unnecessary information, i.e., noise. We propose two novel models to improve word embeddings by unsupervised learning, in order to yield word denoising embeddings. The word denoising embeddings are obtained by strengthening salient information and weakening noise in the original word embeddings, based on a deep feed-forward neural network filter. Results from benchmark tasks show that the filtered word denoising embeddings outperform the original word embeddings.  
 Morphology in unbalanced languages remains a big challenge in the context of machine translation. In this paper, we propose to de-couple machine translation from morphology generation in order to better deal with the problem. We investigate the morphology simplification with a reasonable trade-off between expected gain and generation complexity. For the Chinese-Spanish task, optimum morphological simplification is in gender and number. For this purpose, we design a new classification architecture which, compared to other standard machine learning techniques, obtains the best results. This proposed neural-based architecture consists of several layers: an embedding, a convolutional followed by a recurrent neural network and, finally, ends with sigmoid and softmax layers. We obtain classification results over 98\% accuracy in gender classification, over 93\% in number classification, and an overall translation improvement of 0.7 METEOR.  
 Combinatory Category Grammar  supertagging is a task to assign lexical categories to each word in a sentence. Almost all previous methods use fixed context window sizes as input features. However, it is obvious that different tags usually rely on different context window sizes. These motivate us to build a supertagger with a dynamic window approach, which can be treated as an attention mechanism on the local contexts. Applying dropout on the dynamic filters can be seen as drop on words directly, which is superior to the regular dropout on word embeddings. We use this approach to demonstrate the state-of-the-art CCG supertagging performance on the standard test set.  
     Most existing machine translation systems operate at the level of words, relying on explicit segmentation to extract tokens. We introduce a neural machine translation  model that maps a source character sequence to a target character sequence without any segmentation. We employ a character-level convolutional network with max-pooling at the encoder to reduce the length of source representation, allowing the model to be trained at a speed comparable to subword-level models while capturing local regularities. Our character-to-character model outperforms a recently proposed baseline with a subword-level encoder on WMT'15 DE-EN and CS-EN, and gives comparable performance on FI-EN and RU-EN. We then demonstrate that it is possible to share a single character-level encoder across multiple languages by training a model on a many-to-one translation task. In this multilingual setting, the character-level encoder significantly outperforms the subword-level encoder on all the language pairs. We observe that on CS-EN, FI-EN and RU-EN, the quality of the multilingual character-level translation even surpasses the models specifically trained on that language pair alone, both in terms of BLEU score and human judgment. 
 Sequence-to-sequence models have shown success in end-to-end speech recognition. However these models have only used shallow acoustic encoder networks. In our work, we successively train very deep convolutional networks to add more expressive power and better generalization for end-to-end ASR models. We apply network-in-network principles, batch normalization, residual connections and convolutional LSTMs to build very deep recurrent and convolutional structures. Our models exploit the spectral structure in the feature space and add computational depth without overfitting issues.  We experiment with the WSJ ASR task and achieve 10.5\% word error rate without any dictionary or language using a 15 layer deep network. 
 In this paper, we propose a novel neural approach for paraphrase generation. Conventional paraphrase generation methods either leverage hand-written rules and thesauri-based alignments, or use statistical machine learning principles. To the best of our knowledge, this work is the first to explore deep learning models for paraphrase generation. Our primary contribution is a stacked residual LSTM network, where we add residual connections between LSTM layers. This allows for efficient training of deep LSTMs. We evaluate our model and other state-of-the-art deep learning models on three different datasets: PPDB, WikiAnswers, and MSCOCO. Evaluation results demonstrate that our model outperforms sequence to sequence, attention-based, and bi-directional LSTM models on BLEU, METEOR, TER, and an -based sentence similarity metric.  
 Social norms are shared rules that govern and facilitate social interaction. Violating such social norms via teasing and insults may serve to upend power imbalances or, on the contrary reinforce solidarity and rapport in conversation, rapport which is highly situated and context-dependent. In this work, we investigate the task of automatically identifying the phenomena of social norm violation in discourse. Towards this goal, we leverage the power of recurrent neural networks and multimodal information present in the interaction, and propose a predictive model to recognize social norm violation. Using long-term temporal and contextual information, our model achieves an F1 score of 0.705. Implications of our work regarding developing a social-aware agent are discussed.     
     Long short-term memory  recurrent neural networks  have been shown to give state-of-the-art performance on many speech recognition tasks, as they are able to provide the learned dynamically changing contextual window of all sequence history. On the other hand, the convolutional neural networks  have brought significant improvements to deep feed-forward neural networks , as they are able to better reduce spectral variation in the input signal. In this paper, a network architecture called as convolutional recurrent neural network  is proposed by combining the CNN and LSTM RNN. In the proposed CRNNs, each speech frame, without adjacent context frames, is organized as a number of local feature patches along the frequency axis, and then a LSTM network is performed on each feature patch along the time axis. We train and compare FFNNs, LSTM RNNs and the proposed LSTM CRNNs at various number of configurations. Experimental results show that the LSTM CRNNs can exceed state-of-the-art speech recognition performance.   
   This document contains the instructions for preparing a camera-ready   manuscript for the proceedings of ACL-2016. The document itself   conforms to its own specifications, and is therefore an example of   what your manuscript should look like. These instructions should be   used for both papers submitted for review and for final versions of   accepted papers.  Authors are asked to conform to all the directions   reported in this document. 
 Neural networks are among the state-of-the-art techniques for language modeling. Existing neural language models typically map discrete words to distributed, dense vector representations. After information processing of the preceding context words by hidden layers, an output layer estimates the probability of the next word. Such approaches are time- and memory-intensive because of the large numbers of parameters for word embeddings and the output layer. In this paper, we propose to compress neural language models by sparse word representations. In the experiments, the number of parameters in our model increases very slowly with the growth of the vocabulary size, which is almost imperceptible. Moreover, our approach not only reduces the parameter space to a large extent, but also improves the performance in terms of the perplexity measure.\footnote{Code released on https://github.com/chenych11/lm} 
 Machine reading using differentiable reasoning models has recently shown remarkable progress. In this context, End-to-End trainable Memory Networks  have demonstrated promising performance on simple natural language based reasoning tasks such as factual reasoning and basic deduction. However, other tasks, namely multi-fact question-answering, positional reasoning or dialog related tasks, remain challenging particularly due to the necessity of more complex interactions between the memory and controller modules composing this family of models. In this paper, we introduce a novel end-to-end memory access regulation mechanism inspired by the current progress on the connection short-cutting principle in the field of computer vision. Concretely, we develop a Gated End-to-End trainable Memory Network architecture . From the machine learning perspective, this new capability is learned in an end-to-end fashion without the use of any additional supervision signal which is, as far as our knowledge goes, the first of its kind. Our experiments show significant improvements on the most challenging tasks in the 20 \babi dataset, without the use of any domain knowledge. Then, we show improvements on the \dialogbabi tasks including the real human-bot conversion-based Dialog State Tracking Challenge  dataset. On these two datasets, our model sets the new state of the art. 
 This paper describes our submission to the shared task on word/phrase level Quality Estimation  in the First Conference on Statistical Machine Translation . The objective of the shared task was to predict if the given word/phrase is a correct/incorrect  translation in the given sentence. In this paper, we propose a novel approach for word level Quality Estimation using Recurrent Neural Network Language Model  architecture. RNN-LMs have been found very effective in different Natural Language Processing  applications. RNN-LM is mainly used for vector space language modeling for different NLP problems. For this task, we modify the architecture of RNN-LM. The modified system predicts a label  in the slot rather than predicting the word. The input to the system is a word sequence, similar to the standard RNN-LM. The approach is language independent and requires only the translated text for QE. To estimate the phrase level quality, we use the output of the word level QE system. 
 Recently, neural networks have achieved great success on sentiment classification due to their ability to alleviate feature engineering. However, one of the remaining challenges is to model long texts in document-level sentiment classification under a recurrent architecture because of the deficiency of the memory unit. To address this problem, we present a Cached Long Short-Term Memory neural networks  to capture the overall semantic information in long texts. CLSTM introduces a cache mechanism, which divides memory into several groups with different forgetting rates and thus enables the network to keep sentiment information better within a recurrent unit. The proposed CLSTM outperforms the state-of-the-art models on three publicly available document-level sentiment analysis datasets.  
 Conventional attention-based Neural Machine Translation  conducts dynamic alignment in generating the target sentence. By repeatedly reading the representation of source sentence, which keeps fixed after generated by the encoder~, the attention mechanism has greatly enhanced state-of-the-art NMT. In this paper, we propose a new attention mechanism, called~Interactive Attention, which models the interaction between the decoder and the representation of source sentence during translation by both reading and writing operations. Interactive Attention can keep track of the interaction history and therefore improve the translation performance. Experiments on NIST Chinese-English translation task show that Interactive Attention can achieve significant improvements over both the previous attention-based NMT baseline and some state-of-the-art variants of attention-based NMT . And neural machine translator with our Interactive Attention can outperform the open source attention-based NMT system Groundhog by 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU points averagely on multiple test sets. 
 Neural Machine Translation  is a new approach to machine translation that has made great progress in recent years. However, recent studies show that NMT generally produces fluent but inadequate translations . This is in contrast to conventional Statistical Machine Translation , which usually yields adequate but non-fluent translations. It is natural, therefore, to leverage the advantages of both models for better translations, and in this work we propose to incorporate SMT model into NMT framework. More specifically, at each decoding step, SMT offers additional recommendations of generated words based on the decoding information from NMT . Then we employ an auxiliary classifier to score the SMT recommendations and a gating function to combine the SMT recommendations with NMT generations, both of which are jointly trained within the NMT architecture in an end-to-end manner. Experimental results on Chinese-English translation show that the proposed approach achieves significant and consistent improvements over state-of-the-art NMT and SMT systems on multiple NIST test sets.  
  Recently, the development of neural machine translation  has significantly improved the translation quality of automatic machine translation. While most sentences are more accurate and fluent than translations by statistical machine translation -based systems, in some cases, the NMT system produces translations that have a completely different meaning. This is especially the case when rare words occur.  When using statistical machine translation, it has already been shown that significant gains can be achieved by simplifying the input in a preprocessing step. A commonly used example is the pre-reordering approach.  In this work, we used phrase-based machine translation to pre-translate the input into the target language. Then a neural machine translation system generates the final hypothesis using the pre-translation. Thereby, we use either only the output of the phrase-based machine translation  system or a combination of the PBMT output and the source sentence.   We evaluate the technique on the English to German translation task. Using this approach we are able to outperform the PBMT system as well as the baseline neural MT system by up to 2 BLEU points. We analyzed the influence of the quality of the initial system on the final result.    
 \label{abstract}  Since the first online demonstration of Neural Machine Translation  by LISA , NMT development has recently moved from laboratory to production systems as demonstrated by several entities announcing roll-out of NMT engines to replace their existing technologies. NMT systems have a large number of training configurations and the training process of such systems is usually very long, often a few weeks, so role of experimentation is critical and important to share. In this work, we present our approach to production-ready systems simultaneously with release of online demonstrators covering a large variety of languages . We explore different practical choices: an efficient and evolutive open-source framework; data preparation; network architecture; additional implemented features; tuning for production; {" translation. We aim at contributing to set up a collaborative framework to speed-up adoption of the technology, foster further research efforts and enable the delivery and adoption to/by industry of use-case specific engines integrated in real production workflows. Mastering of the technology would allow us to build translation engines suited for particular needs, outperforming current simplest/uniform systems.  %\marginpar{one step further, analyse how to go on commercial paper?} %\marginpar{missing idea that more important is the experimentation and mastering/controlling of the technology than the actual best performing/simplest/uniform systems. not comparing on state of the art research systems but state of the art production system}  
  % 100 - 150 words Conventional deep neural networks  for speech acoustic modeling rely on Gaussian mixture models  and hidden Markov model  to obtain binary class labels as the targets for DNN training. Subword classes in speech recognition systems correspond to context-dependent tied states or senones. The present work addresses some limitations of GMM-HMM senone alignments for DNN training. We hypothesize that the senone probabilities obtained from a DNN trained with binary labels can provide more accurate targets to learn better acoustic models. However, DNN outputs bear inaccuracies which are exhibited as high dimensional unstructured noise, whereas the informative components are structured and low-dimensional. We exploit principle component analysis  and sparse coding to characterize the senone subspaces. Enhanced probabilities obtained from low-rank and sparse reconstructions are used as soft-targets for DNN acoustic modeling, that also enables training with untranscribed data. Experiments conducted on AMI corpus shows 4.6\% relative reduction in word error rate.  
 State-of-the-art speech recognition systems typically employ neural network acoustic models. However, compared to Gaussian mixture models, deep neural network  based acoustic models often have many more model parameters, making it challenging for them to be deployed on resource-constrained platforms, such as mobile devices. In this paper, we study the application of the recently proposed highway deep neural network  for training small-footprint acoustic models. HDNNs are a depth-gated feedforward neural network, which include two types of gate functions to facilitate the information flow through different layers.  Our study demonstrates that HDNNs are more compact than regular DNNs for acoustic modeling, i.e., they can achieve comparable recognition accuracy with many fewer model parameters. Furthermore, HDNNs are more controllable than DNNs: the gate functions of an HDNN can control the behavior of the whole network using a very small number of model parameters. Finally, we show that HDNNs are more adaptable than DNNs. For example, simply updating the gate functions using  adaptation data can result in considerable gains in accuracy. We demonstrate these aspects by experiments using the publicly available AMI corpus, which has around 80 hours of training data.   %Moreover, we also investigate a knowledge distillation technique to further improve the small-footprint HDNN acoustic models.     
 Hypothesis testing is an important cognitive process that supports human reasoning. In this paper, we introduce a new computational hypothesis testing framework that is based on memory augmented neural networks. Our approach involves a hypothesis testing loop that reconsiders and progressively refines a previously formed hypothesis in order to generate new hypotheses to test. We applied the proposed approach to language comprehension task by using Neural Semantic Encoders . Our %%\MakeLowercase{h} NSE models achieved the state-of-the-art results showing an absolute improvement of 1.2\% to 2.6\% accuracy over previous results obtained by single and ensemble systems on standard machine comprehension benchmarks such as the Children's Book Test  and Who-Did-What  news article datasets. 
 The authorship attribution is a problem of considerable practical and technical interest. Several methods have been designed to infer the authorship of disputed documents in multiple contexts. While traditional statistical methods based solely on word counts and related measurements have provided a simple, yet effective solution in particular cases; they are prone to manipulation. Recently, texts have been successfully modeled as networks, where words are represented by nodes linked according to textual similarity measurements. Such models are useful to identify informative topological patterns for the authorship recognition task. However, there is no consensus on which measurements should be used. Thus, we proposed a novel method to characterize text networks, by considering both topological and dynamical aspects of networks. Using concepts and methods from cellular automata theory, we devised a strategy to grasp informative spatio-temporal patterns from this model. Our experiments revealed an outperformance over traditional analysis relying only on topological measurements. Remarkably, we have found a dependence of pre-processing steps  on the obtained results, a feature that has mostly been disregarded in related works. The optimized results obtained here pave the way for a better characterization of textual networks. 
 We explore the use of segments learnt using Byte Pair Encoding  as basic units for statistical machine translation between related languages and compare it with orthographic syllables, which are currently the best performing basic units for this translation task. BPE identifies the most frequent character sequences as basic units, while orthographic syllables are linguistically motivated pseudo-syllables. We show that BPE units modestly outperform orthographic syllables as units of translation, showing up to 11\% increase in BLEU score. While orthographic syllables can be used only for languages whose writing systems use vowel representations, BPE is writing system independent and we show that BPE outperforms other units for non-vowel writing systems too. Our results are supported by extensive experimentation spanning multiple language families and writing systems.  
 This year, the Nara Institute of Science and Technology /Carnegie Mellon University  submission to the Japanese-English translation track of the 2016 Workshop on Asian Translation was based on attentional neural machine translation  models. In addition to the standard NMT model, we make a number of improvements, most notably the use of discrete translation lexicons to improve probability estimates, and the use of minimum risk training to optimize the MT system for BLEU score. As a result, our system achieved the highest translation evaluation scores for the task. 
 % Most existing Neural Machine Translation models use groups of characters or whole words as their unit of input and output. We propose a model with a hierarchical 	exttt{char2word encoder, that takes individual characters both as input and output. We first argue that this hierarchical representation of the character encoder reduces computational complexity, and show that it improves translation performance. Secondly, by qualitatively studying attention plots from the decoder we find that the model learns to compress common words into a single embedding whereas rare words, such as names and places, are represented character by character.} 
 %NMT has to learn translation knowledge from large-scale parallel sentence pairs. Neural Machine Translation  has become the new state-of-the-art in several language pairs. However, it remains a challenging problem how to integrate NMT with a bilingual dictionary which mainly contains words rarely or never seen in the bilingual training data. In this paper, we propose two methods to bridge NMT and the bilingual dictionaries. The core idea behind is to design novel models that transform the bilingual dictionaries into adequate sentence pairs, so that NMT can distil latent bilingual mappings from the ample and repetitive phenomena. One method leverages a mixed word/character model and the other attempts at synthesizing parallel sentences guaranteeing massive occurrence of the translation lexicon. Extensive experiments demonstrate that the proposed methods can remarkably improve the translation quality, and most of the rare words in the test sentences can obtain correct translations if they are covered by the dictionary.  
  Breaking news leads to situations of fast-paced reporting in social media, producing all kinds of updates related to news stories, albeit with the caveat that some of those early updates tend to be rumours, i.e., information with an unverified status at the time of posting. Flagging information that is unverified can be helpful to avoid the spread of information that may turn out to be false. Detection of rumours can also feed a rumour tracking system that ultimately determines their veracity. In this paper we introduce a novel approach to rumour detection that learns from the sequential dynamics of reporting during breaking news in social media to detect rumours in new stories. Using Twitter datasets collected during five breaking news stories, we experiment with Conditional Random Fields as a sequential classifier that leverages context learnt during an event for rumour detection, which we compare with the state-of-the-art rumour detection system as well as other baselines. In contrast to existing work, our classifier does not need to observe tweets querying a piece of information to deem it a rumour, but instead we detect rumours from the tweet alone by exploiting context learnt during the event. Our classifier achieves competitive performance, beating the state-of-the-art classifier that relies on querying tweets with improved precision and recall, as well as outperforming our best baseline with nearly 40\% improvement in terms of F1 score. The scale and diversity of our experiments reinforces the generalisability of our classifier. 
     Multi-hop inference is necessary for machine learning systems to successfully solve tasks such as Recognising Textual Entailment and Machine Reading.     In this work, we demonstrate the effectiveness of adaptive computation for learning the number of inference steps required for examples of different complexity and that learning the correct number of inference steps is difficult.     We introduce the first model involving Adaptive Computation Time which provides a small performance benefit on top of a similar model without an adaptive component as well as enabling considerable insight into the reasoning process of the model. 
 % as the test set. Our methods involve weak supervision  -- we do not use any hand-labeled examples for WSD to build our prediction models; however, we employ an existing concept mapping program, MetaMap, to obtain our concept vectors. Over the MSH WSD dataset, our linear time  method achieves an accuracy of 92.24\% which is a $3\%$ improvement over the best known results~ obtained via unsupervised means. A more expensive approach that we developed relies on a nearest neighbor framework and achieves accuracy of 94.34\%, essentially cutting the error rate in half. Employing dense vector representations learned from unlabeled free text has been shown to benefit many language processing tasks recently and our efforts show that biomedical WSD is no exception to this trend.  For a complex and rapidly evolving domain such as biomedicine, building labeled datasets for larger sets of ambiguous terms may be impractical.  Here, we show that weak supervision that leverages recent advances in representation learning can rival supervised approaches in biomedical WSD. However, external knowledge bases  play a key role in the improvements achieved.  
 Sarcasm detection is a key task for many natural language processing tasks. In sentiment analysis, for example, sarcasm can flip the polarity of an ``apparently positive'' sentence and, hence, negatively affect polarity detection performance.  To date, most approaches to sarcasm detection have treated the task primarily as a text categorization problem. Sarcasm, however, can be expressed in very subtle ways and requires a deeper understanding of natural language that standard text categorization techniques cannot grasp. In this work, we develop models based on a pre-trained convolutional neural network for extracting sentiment, emotion and personality features for sarcasm detection. Such features, along with the network's baseline features, allow the proposed models to outperform the state of the art on benchmark datasets. We also address the often ignored generalizability issue of classifying data that have not been seen by the models at learning phase. 
  Transliteration is a key component of machine translation systems and software internationalization. This paper demonstrates that neural sequence-to-sequence models obtain state of the art or close to state of the art results on existing datasets. In an effort to make machine transliteration accessible, we open source a new Arabic to English transliteration dataset and our trained models. 
 Patient notes contain a wealth of information of potentially great interest to medical investigators. However, to protect patients' privacy, Protected Health Information  must be removed from the patient notes before they can be legally released, a process known as patient note de-identification. The main objective for a de-identification system is to have the highest possible recall. Recently, the first neural-network-based de-identification system has been proposed, yielding state-of-the-art results. Unlike other systems, it does not rely on human-engineered features, which allows it to be quickly deployed, but does not leverage knowledge from human experts or from electronic health records . In this work, we explore a method to incorporate human-engineered features as well as features derived from EHRs to a neural-network-based de-identification system. Our results show that the addition of features, especially the EHR-derived features, further improves the state-of-the-art in patient note de-identification, including for some of the most sensitive PHI types such as patient names. Since in a real-life setting patient notes typically come with EHRs, we recommend developers of de-identification systems to leverage the information EHRs contain. 
   In this paper we describe an end to end    Neural Model for Named Entity Recognition  which is based on  Bi-Directional RNN-LSTM. Almost all NER systems for Hindi use Language Specific  features and handcrafted rules with gazetteers.  Our model is language independent and uses no domain specific features or any handcrafted rules. Our models rely on semantic  information in the form of word vectors   which are learnt by an unsupervised learning algorithm on an unannotated corpus. Our model attained state of the art  performance in both English and Hindi without the use of any morphological analysis or without using gazetteers of any sort. 
 		Recurrent neural networks  have achieved state-of-the-art performances in many natural language processing tasks, such as language modeling and machine translation. However, when the vocabulary is large, the RNN model will become very big  and its training will become very inefficient. In this work, we propose a novel technique to tackle this challenge. The key idea is to use 2-Component  shared embedding for word representations. We allocate every word in the vocabulary into a table, each row of which is associated with a vector, and each column associated with another vector. Depending on its position in the table, a word is jointly represented by two components: a row vector and a column vector. Since the words in the same row share the row vector and the words in the same column share the column vector, we only need $2 $ vectors to represent a vocabulary of $|V|$ unique words, which are far less than the $|V|$ vectors required by existing approaches. Based on the 2-Component shared embedding, we design a new RNN algorithm and evaluate it using the language modeling task on several benchmark datasets. The results show that our algorithm significantly reduces the model size and speeds up the training process, without sacrifice of accuracy . Remarkably, on the One-Billion-Word benchmark Dataset, our algorithm achieves comparable perplexity to previous language models, whilst reducing the model size by a factor of 40-100, and speeding up the training process by a factor of 2. We name our proposed algorithm  to reflect its very small model size and very high training speed.  	
 %-------------------------------------------------------------------------------  We present results that show it is possible to build a competitive, greatly simplified, large vocabulary continuous speech recognition system with whole words as acoustic units. We model the output vocabulary of about 100,000 words directly using deep bi-directional LSTM RNNs with CTC loss. The model is trained on 125,000 hours of semi-supervised acoustic training data, which enables us to alleviate the data sparsity problem for word models. We show that the CTC word models work very well as an end-to-end all-neural speech recognition model without the use of traditional context-dependent sub-word phone units that require a pronunciation lexicon, and without any language model removing the need to decode. We demonstrate that the CTC word models perform better than a strong, more complex, state-of-the-art baseline with sub-word units.  
 A text network refers to a data type that each vertex is associated with a text document and the relationship between documents is represented by edges. The proliferation of text networks such as hyperlinked webpages and academic citation networks has led to an increasing demand for quickly developing a general sense of a new text network, namely . In this paper, we address the problem of text network exploration through constructing a heterogeneous web of topics, which allows people to investigate a text network associating word level with document level. To achieve this, a probabilistic generative model for text and links is proposed, where three different relationships in the heterogeneous topic web are quantified. We also develop a prototype demo system named  to exhibit such heterogeneous topic web, and demonstrate how this system can facilitate the task of text network exploration. Extensive qualitative analyses are included to verify the effectiveness of this heterogeneous topic web. Besides, we validate our model on real-life text networks, showing that it preserves good performance on objective evaluation metrics.   
 Neural sequence models are widely used to model time-series data. % in many fields.  Equally ubiquitous is the usage of beam search  as an approximate inference algorithm to decode output sequences from these models.  BS explores the search space in a greedy left-right fashion retaining only the top-$B$ candidates -- resulting in sequences that differ only slightly from each other.  Producing lists of nearly identical sequences is not only computationally wasteful but also typically fails to capture the inherent ambiguity of complex AI tasks. To overcome this problem, we propose  , an alternative to BS that decodes a list of diverse outputs by optimizing for a diversity-augmented objective. We observe that our method finds better top-1 solutions by controlling for the exploration and exploitation of the search space -- implying that DBS is a . Moreover, these gains are achieved with minimal computational or memory overhead as compared to beam search. To demonstrate the broad applicability of our method, we present results on image captioning, machine translation and visual question generation using both standard quantitative metrics and qualitative human studies.  \ar{Further, we study the role of diversity for image-grounded language generation tasks as the complexity of the image changes.} \ar{We observe that} our method consistently outperforms BS and previously proposed techniques for diverse decoding from neural sequence models. 
 % Motivation It is difficult to train a personalized task-oriented dialogue system because the data collected from each individual is often insufficient. %Data from a single user could not cover all possible situation in general dialogue. Personalized dialogue systems trained on a small dataset is likely to overfit and make it difficult to adapt to different user needs. One way to solve this problem is to consider a collection of multiple users as a source domain and an individual user as a target domain, and to perform transfer learning from the source to the target domain. % Proposed method By following this idea, we propose a PErsonalized Task-oriented diALogue  system, a transfer learning framework based on POMDP to construct a personalized dialogue system. The PETAL system first learns common dialogue knowledge from the source domain and then adapts this knowledge to the target domain. The proposed PETAL system can avoid the negative transfer problem by considering differences between source and target users in a personalized Q-function.  %The policy in the personalized POMDP can learn to choose different actions appropriately for different users. % Result Experimental results on a real-world coffee-shopping data and simulation data show that the proposed PETAL system can learn different optimal policies for different users, and thus effectively improve the dialogue quality under the personalized setting. 
     Modern robotics applications that involve human-robot interaction     require robots to be able to communicate with humans seamlessly     and effectively. Natural language provides a flexible and     efficient medium through which robots can exchange information     with their human partners. Significant advancements have been made     in developing robots capable of interpreting free-form     instructions, but less attention has been devoted to endowing     robots with the ability to generate natural language. We propose a     navigational guide model that enables robots to generate natural     language instructions that allow humans to navigate a priori     unknown environments. We first decide which information to share     with the user according to their preferences, using a policy     trained from human demonstrations via inverse reinforcement     learning. We then ``translate'' this information into a natural     language instruction using a neural sequence-to-sequence model     that learns to generate free-form instructions from natural     language corpora. We evaluate our method on a benchmark route     instruction dataset and achieve a BLEU score of 72.18\% when     compared to human-generated reference instructions. We     additionally conduct navigation experiments with human     participants that demonstrate that our method generates     instructions that people follow as accurately and easily as those     produced by humans. 
 This paper presents a deep learning architecture for the semantic decoder component of a Statistical Spoken Dialogue System. In a slot-filling dialogue, the semantic decoder predicts the dialogue act and a set of slot-value pairs from a set of n-best hypotheses returned by the Automatic Speech Recognition.  Most current  models for spoken language understanding assume  word-aligned semantic annotations as in sequence taggers and  delexicalisation, or a mapping of input words to domain-specific concepts using heuristics that  try to capture morphological variation but that do not scale to other domains nor to language variation . In this work the semantic decoder is trained using unaligned semantic annotations and it uses distributed semantic representation learning to overcome the limitations of explicit delexicalisation.  The proposed  architecture uses a convolutional neural network for the  sentence representation and a long-short term memory network for the context representation. Results are presented for the publicly available DSTC2 corpus and an In-car corpus which is similar to DSTC2 but has a significantly higher word error rate . 
 We consider multi-class classification where the predictor has a hierarchical structure that allows for a very large number of labels both at train and test time. The predictive power of such models can heavily depend on the structure of the tree, and although past work showed how to learn the tree structure, it expected that the feature vectors remained static. We provide a novel algorithm to simultaneously perform representation learning for the input data and learning of the hierarchical predictor. Our approach optimizes an objective function which favors balanced and easily-separable multi-way node partitions. We theoretically analyze this objective, showing that it gives rise to a boosting style property and a bound on classification error. We next show how to extend the algorithm to conditional density estimation. We empirically validate both variants of the algorithm on text classification and language modeling, respectively, and show that they compare favorably to common baselines in terms of accuracy and running time. 
  %translating natural language to executable logical form  %is difficult when the network must interact with a large symbolic knowledge-base. %Extending the success of deep neural networks to   %Harnessing the statistical power of neural networks for natural language understanding and symbolic reasoning requires complex operations and scalable memory, such as interacting with a large symbolic knowledge-base, which, however, are hard to implement in neural networks.  Harnessing the statistical power of neural networks to perform language understanding and symbolic reasoning is difficult, when it requires executing efficient discrete %non-differentiable  operations against a large knowledge-base. %complex non-differentiable operations and scalabe memory. %complex non-differentiable operations and interaction with scalable memory, such as a large knowledge-base.  % %scalable operations such as interacting with a large knowledge-base. % % Recent approaches are typically limited to differentiable operations and memory  and consequently cannot scale beyond small synthetic tasks. % %In this work, we propose the Manager-Programmer-Computer framework, which integrates neural networks with non-differentiable memory to support abstract, scalable and precise operations through a friendly neural computer interface.  %Specifically,  In this work, we introduce a Neural Symbolic Machine , which contains   a neural ``programmer", i.e., a sequence-to-sequence model that maps language utterances to programs and utilizes a key-variable memory to handle compositionality   a symbolic ``computer", i.e., a Lisp interpreter that performs program execution, and helps find good programs by pruning the search space.  %and prunes the space of possible programs. %JB: in my opinion at the abstract this is not  %and prunes search spaces.  %interpreter with code assist for scalability.   %In this paper, we introduce a Neural Symbolic Machine, which contains a sequence-to-sequence neural "programmer", and a non-differentiable "computer" that is a Lisp interpreter with code assist.   %%In this paper, we present the Neural Symbolic Machine , in which a differentiable sequence-to-sequence model maps natural language to logical form, while interacting with a non-differentiable interpreter that provides the outcome of logical form execution. %To overcome the challenging search problem of finding the right logical form when training from question-answer pairs only, we use a maximum likelihood approach to find pseudo-gold logical forms, which are then used as an oracle that guides training with the REINFORCE algorithm. % % %To overcome the search problem of finding good programs, and train NSM with REINFORCE, %we augment REINFORCE with pseudo-gold programs that are found by an iterative maximum-likelihood training process. %To train NSM with REINFORCE we need to overcome the search problem of finding good programs, which we solve by an iterative maximum-likelihood training process. We apply REINFORCE to directly optimize the task reward of this structured prediction problem. To train with weak supervision and improve the stability of REINFORCE  we augment it with an  iterative maximum-likelihood training process.  %, which searches for good programs. % We demonstrate that  NSM outperforms the state-of-the-art on the WebQuestionsSP dataset when trained from question-answer pairs only, without requiring any feature engineering or domain-specific knowledge.  %NSM is able to learn a semantic parser from weak supervision over a large knowledge base. It achieves new state-of-the-art performance on WebQuestionsSP, a challenging semantic parsing dataset. %, with weak supervision.  %Compared to previous approaches, NSM is end-to-end, therefore does not rely on feature engineering or domain specific knowledge.    
 While neural machine translation  is making good progress in the past two years, tens of millions of bilingual sentence pairs are needed for its training. However, human labeling is very costly. To tackle this training data bottleneck, we develop a dual-learning mechanism, which can enable an NMT system to automatically learn from unlabeled data through a dual-learning game. This mechanism is inspired by the following observation: any machine translation task has a dual task, e.g., English-to-French translation  versus French-to-English translation ; the primal and dual tasks can form a closed loop, and generate informative feedback signals to train the translation models, even if without the involvement of a human labeler. In the dual-learning mechanism, we use one agent to represent the model for the primal task and the other agent to represent the model for the dual task, then ask them to teach each other through a reinforcement learning process. Based on the feedback signals generated during this process , we can iteratively update the two models until convergence . We call the corresponding approach to neural machine translation . Experiments show that dual-NMT works very well on English$\leftrightarrow$French translation; especially, by learning from monolingual data , it achieves a comparable accuracy to NMT trained from the full bilingual data for the French-to-English translation task. 
 The success of long short-term memory  neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. Linguistic regularities are often sensitive to syntactic structure; can such  dependencies be captured by LSTMs, which do not have explicit structural representations? We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture's grammatical competence both using training objectives with  an explicit grammatical target  and using language models. In the strongly supervised settings, the LSTM achieved very high overall accuracy , but errors increased when sequential and structural information conflicted. The frequency of such errors rose sharply in the language-modeling setting. We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured. 
 Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. Higher layers include shortcut connections to lower-level task predictions to reflect linguistic hierarchies. We use a simple regularization term to allow for optimizing all model weights to improve one task's loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end model obtains state-of-the-art or competitive results on five different tasks from tagging, parsing, relatedness, and entailment tasks. 
 Several deep learning models have been proposed for question answering. However, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the \oursfull  for question answering. The \ours first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointing decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single \ours model improves the previous state of the art from 71.0\% F1 to \testfsingle\%, while a \ours ensemble obtains \testf\% F1. 
   In this paper, we propose , a \gls{RNN}-based    language model designed to directly capture        the global semantic meaning relating words in a document via latent topics.   Because of their sequential nature, s are good at capturing    the local structure of a word sequence -- both    semantic and syntactic -- but might face   difficulty remembering long-range dependencies. Intuitively,      these long-range dependencies      are of semantic nature.   In contrast, latent topic models   are able to capture   the global semantic   structure of a document       but do not account for word ordering.   The proposed  model integrates the    merits of s and   latent topic models:        it captures local  dependencies using an    and   global  dependencies using latent topics.   Unlike previous work    on contextual  language modeling,   our model is learned end-to-end.    Empirical results on   word prediction   show that     outperforms existing contextual  baselines.     In addition,  can be used as an unsupervised   feature extractor for documents. We do this for sentiment analysis   on the IMDB movie review dataset and report an error rate of   $6.28\%$. This is comparable to the state-of-the-art $5.91\%$   resulting from a semi-supervised approach. Finally,    also yields sensible topics, making it a useful   alternative to document models such as latent Dirichlet allocation.        
   This paper builds off recent work from  using neural attention in a simple graph-based dependency parser. We use a larger but more thoroughly regularized parser than other recent BiLSTM-based approaches, with biaffine classifiers to predict arcs and labels. Our parser gets state of the art or near state of the art performance on standard treebanks for six different languages, achieving 95.7\%{} UAS and 94.1\%{} LAS on the most popular English PTB dataset. This makes it the highest-performing graph-based parser on this benchmark---outperforming  by 1.8\%{} and 2.2\%{}---and comparable to the highest performing transition-based parser , which achieves 95.8\%{} UAS and 94.6\%{} LAS. We also show which hyperparameter choices had a significant effect on parsing accuracy, allowing us to achieve large gains over other graph-based approaches. 
 In this paper we present a domain adaptation technique for formant estimation using a deep network. We first train a deep learning network on a small read speech dataset. We then freeze the parameters of the trained network and use several different datasets to train an adaptation layer that makes the obtained  network universal in the sense that it works well for a variety of speakers and speech domains with very different characteristics. We evaluated our adapted network on three datasets, each of which has different speaker characteristics and speech styles. The performance of our method compares favorably with alternative methods for formant estimation. 
 Truth discovery is to resolve conflicts and find the truth from multiple-source statements. Conventional methods mostly research based on the mutual effect between the reliability of sources and the credibility of statements, however, pay no attention to the mutual effect among the credibility of statements about the same object. We propose memory network based models to incorporate these two ideas to do the truth discovery. We use feedforward memory network and feedback memory network to learn the representation of the credibility of statements which are about the same object. Specially, we adopt memory mechanism to learn source reliability and use it through truth prediction. During learning models, we use multiple types of data  by assigning different weights automatically in the loss function based on their own effect on truth discovery prediction. The experiment results show that the memory network based models much outperform the state-of-the-art method and other baseline methods.  
 Although end-to-end Neural Machine Translation  has achieved remarkable progress in the past two years, it suffers from a major drawback: translations generated by NMT systems often lack of adequacy. It has been widely observed that NMT tends to repeatedly translate some source words while mistakenly ignoring other words.  To alleviate this problem, we propose a novel encoder-decoder-reconstructor framework for NMT.  The reconstructor, incorporated into the NMT model, manages to reconstruct the input source sentence from the hidden layer of the output target sentence, to ensure that the information in the source side is transformed to the target side as much as possible. Experiments show that the proposed framework significantly improves the adequacy of NMT output and achieves superior translation result over state-of-the-art NMT and statistical MT systems. 
 Recently deeplearning models have been shown to be capable of making remarkable performance in sentences and documents classification tasks. In this work, we propose a novel framework called AC-BLSTM for modeling sentences and documents, which combines the asymmetric convolution neural network  with the Bidirectional Long Short-Term Memory network . Experiment results demonstrate that our model achieves state-of-the-art results on five tasks, including sentiment analysis, question type classification, and subjectivity classification. In order to further improve the performance of AC-BLSTM, we propose a semi-supervised learning framework called G-AC-BLSTM for text classification by combining the generative model with AC-BLSTM. 
 The goal of sentence and document modeling is to accurately represent the meaning of sentences and documents for various Natural Language Processing tasks. In this work, we present Dependency Sensitive Convolutional Neural Networks  as a general-purpose classification system for both sentences and documents. DSCNN hierarchically builds textual representations by processing pretrained word embeddings via Long Short-Term Memory networks and subsequently extracting features with convolution operators. Compared with existing recursive neural models with tree structures, DSCNN does not rely on parsers and expensive phrase labeling, and thus is not restricted to sentence-level tasks. Moreover, unlike other CNN-based models that analyze sentences locally by sliding windows, our system captures both the dependency information within each sentence and relationships across sentences in the same document. Experiment results demonstrate that our approach is achieving state-of-the-art performance on several tasks, including sentiment analysis, question type classification, and subjectivity classification. 
 Acoustic word embeddings --- fixed-dimensional vector representations of variable-length spoken word segments --- have begun to be considered for tasks such as speech recognition and query-by-example search.  Such embeddings can be learned discriminatively so that they are similar for speech segments corresponding to the same word, while being dissimilar for segments corresponding to different words.  Recent work has found that acoustic word embeddings can outperform dynamic time warping on query-by-example search and related word discrimination tasks.  However, the space of embedding models and training approaches is still relatively unexplored.  In this paper we present new discriminative embedding models based on recurrent neural networks .  We consider training losses that have been successful in prior work, in particular a cross entropy loss for word classification and a contrastive loss that explicitly aims to separate same-word and different-word pairs in a "Siamese network" training setting. We find that both classifier-based and Siamese RNN embeddings improve over previously reported results on a word discrimination task, with Siamese RNNs outperforming classification models.  In addition, we present analyses of the learned embeddings and the effects of variables such as dimensionality and network structure.   
 We formulate sequence to sequence transduction as a noisy channel decoding problem and use recurrent neural networks to parameterise the source and channel models. Unlike direct models which can suffer from explaining-away effects during training, noisy channel models must produce outputs that explain their inputs, and their component models can be trained with not only paired training samples but also unpaired samples from the marginal output distribution. Using a latent variable to control how much of the conditioning sequence the channel model needs to read in order to generate a subsequent symbol, we obtain a tractable and effective beam search decoder. Experimental results on abstractive sentence summarisation, morphological inflection, and machine translation show that noisy channel models  outperform direct models, and that they significantly benefit from increased amounts of unpaired output data that direct models cannot easily use. 
  Modeling the structure of coherent texts is a key NLP problem. The task of coherently organizing a given set of sentences has been commonly used to build and evaluate models that understand such structure. We propose an end-to-end unsupervised deep learning approach based on the set-to-sequence framework to address this problem. Our model strongly outperforms prior methods in the order discrimination task and a novel task of ordering abstracts from scientific articles. Furthermore, our work shows that useful text representations can be obtained by learning to order sentences. Visualizing the learned sentence representations shows that the model captures high-level logical structure in paragraphs. Our representations perform comparably to state-of-the-art pre-training methods on sentence similarity and paraphrase detection tasks. 
 This work presents a general unsupervised learning method to improve the accuracy of sequence to sequence  models. In our method, the weights of the encoder and decoder of a seq2seq model are initialized with the pretrained weights of two language models and then  fine-tuned with labeled data. We apply this method to challenging benchmarks in machine translation and abstractive summarization and find that it significantly improves the subsequent supervised models.  Our main result is that pretraining improves the generalization of seq2seq models. We achieve state-of-the-art results on the WMT English$\rightarrow$German task, surpassing a range of methods using both phrase-based machine translation and neural machine translation. Our method achieves a significant improvement of 1.3 BLEU from the previous best models on both WMT'14 and WMT'15 English$\rightarrow$German. We also conduct human evaluations on abstractive summarization and find that our method outperforms a purely supervised learning baseline in a statistically significant manner. 
 This paper describes the USTC\_NELSLIP systems submitted to the Trilingual Entity Detection and Linking  track in  2016 TAC Knowledge Base Population  contests. We have built two systems for entity discovery and mention detection : one uses the conditional RNNLM and the other one uses the attention-based encoder-decoder framework. The entity linking  system consists of two modules: a rule based candidate generation and a neural networks probability ranking model. Moreover, some simple string matching rules are used for NIL clustering. At the end, our best system has achieved an F1 score of 0.624 in the end-to-end typed mention ceaf plus metric. 
 Most neural network models for document classification on social media focus on text information to the neglect of other information on these platforms. In this paper, we classify post stance on social media channels and develop UTCNN, a neural network model that incorporates user tastes, topic tastes, and user comments on posts. UTCNN not only works on social media texts, but also analyzes texts in forums and message boards. Experiments performed on Chinese Facebook data and English online debate forum data show that UTCNN achieves a 0.755 macro-average f-score for supportive, neutral, and unsupportive stance classes on Facebook data, which is significantly better than models in which either user, topic, or comment information is withheld. This %%% penguin 1010 å¨£å›¶å–—é model design greatly mitigates the lack of data for the minor class without the use of oversampling.  % penguin: é–¸æ—‚å§µå¨“è·ºî‡¥å®€å¨ˆ without .... %%% penguin 1010 å¨£å›¶å–—é In addition, UTCNN yields a 0.842 accuracy on English online debate forum data, which %%% penguin 1010 å¨£å›¶å–—é also significantly outperforms results from previous work as well as other deep learning models, showing that UTCNN % penguin: é–¸æ—‚å§´é™ ...and other deep learning models %%% penguin 1010 å¨£å›¶å–—é performs well regardless of language or platform. 
   Joint representation learning of text and knowledge within a unified semantic space enables us to perform knowledge graph completion more accurately. In this work, we propose a novel framework to embed words, entities and relations into the same continuous vector space. In this model, both entity and relation embeddings are learned by taking knowledge graph and plain text into consideration. In experiments, we evaluate the joint learning model on three tasks including entity prediction, relation prediction and relation classification from text. The experiment results show that our model can significantly and consistently improve the performance on the three tasks as compared with other baselines. 
 Conditional Random Field  and recurrent neural models have achieved success in structured prediction. More recently, there is a marriage of CRF and recurrent neural models, so that we can gain from both non-linear dense features and globally normalized CRF objective. These recurrent neural CRF models mainly focus on encode node features in CRF undirected graphs. However, edge features prove important to CRF in structured prediction. In this work, we introduce a new recurrent neural CRF model, which learns non-linear edge features, and thus makes non-linear features encoded completely. We compare our model with different neural models in well-known structured prediction tasks. Experiments show that our model outperforms state-of-the-art methods in NP chunking, shallow parsing, Chinese word segmentation and POS tagging. 
 		We focus on named entity recognition  for Chinese social media. With massive unlabeled text and quite limited labelled corpus, we propose a semi-supervised learning model based on B-LSTM neural network. To take advantage of traditional methods in NER such as CRF, we combine transition probability with deep learning in our model. To bridge the gap between label accuracy and F-score of NER, we construct a model which can be directly trained on F-score. When considering the instability of F-score driven method and meaningful information provided by label accuracy, we propose an integrated method to train on both F-score and label accuracy. Our integrated model yields substantial improvement over previous state-of-the-art result. 		 	
 % 300 word limit Compared with word-level and sentence-level convolutional neural networks , the character-level ConvNets has a better applicability for misspellings and typos input. Due to this, recent researches for text classification mainly focus on character-level ConvNets. However, while the majority of these researches employ English corpus for the character-level text classification, few researches have been done using Chinese corpus. This research hopes to bridge this gap, exploring character-level ConvNets for Chinese corpus test classification. We have constructed a large-scale Chinese dataset, and the result shows that character-level ConvNets works better on Chinese character dataset than its corresponding pinyin format dataset, which is the general solution in previous researches. This is the first time that character-level ConvNets has been applied to Chinese character dataset for text classification problem. 
  Sequence labeling architectures use word embeddings for capturing similarity, but suffer when handling previously unseen or rare words. We investigate character-level extensions to such models and propose a novel architecture for combining alternative word representations. By using an attention mechanism, the model is able to dynamically decide how much information to use from a word- or character-level component.  We evaluated different architectures on a range of sequence labeling datasets, and character-level extensions were found to improve performance on every benchmark. In addition, the proposed attention-based architecture delivered the best results even with a smaller number of trainable parameters.   %  In this work, we explore deep learning-based approaches to sequence %  labeling tasks such as part-of-speech tagging and named entity %  recognition, with an emphasis on modeling character-level information %  in a domain- and task-independent way. %  Evaluation on a selection of established reference datasets shows %  \ldots %  We make all tools applied in this work available under open source %  licenses at [redacted].   
 We propose an approach to build a neural machine translation system with no supervised resources  using multimodal embedded representation over texts and images. Based on the assumption that text documents are often likely to be described with other multimedia information  somewhat related to the content, we try to indirectly estimate the relevance between two languages. Using multimedia as the "pivot", we project all modalities into one common hidden space where samples belonging to similar semantic concepts should come close to each other, whatever the observed space of each sample is.  This modality-agnostic representation is the key to bridging the gap between different modalities.  Putting a decoder on top of it, our network can flexibly draw the outputs from any input modality.  Notably, in the testing phase, we need only source language texts as the input for translation.   In experiments, we tested our method on two benchmarks to show that it can achieve reasonable translation performance. We compared and investigated several possible implementations and found that an end-to-end model that simultaneously optimized both rank loss in multimodal encoders and cross-entropy loss in decoders performed the best.  % \PACS{PACS code1 \and PACS code2 \and more} %  
 		Long text brings a big challenge to semantic matching due to their complicated semantic and syntactic structures. To tackle the challenge, we consider using prior knowledge to help identify useful information and filter out noise to matching in long text. To this end, we propose a knowledge enhanced hybrid neural network . The model fuses prior knowledge into word representations by knowledge gates and establishes three matching channels with words, sequential structures of sentences given by Gated Recurrent Units , and knowledge enhanced representations. The three channels are processed by a convolutional neural network to generate high level features for matching, and the features are synthesized as a matching score by a multilayer perceptron. The model extends the existing methods by conducting matching on words, local structures of sentences, and global context of sentences. Evaluation results from extensive experiments on public data sets for question answering and conversation show that KEHNN can significantly outperform the-state-of-the-art matching models and particularly improve the performance on pairs with long text. 	
  %Natural language inference is the problem of identifying whether a %hypothesis can be inferred or contradicted given a premise, all in %natural language. In this work we use the recent advances in representation learning to propose a neural architecture for the problem of natural language inference. Our approach is aligned to mimic how a human does the natural language inference process given two statements. The model uses variants of Long Short Term Memory , attention mechanism and composable neural networks, to carry out the task. Each part of our model can be mapped to a clear functionality humans do for carrying out the overall task of natural language inference. The model is end-to-end differentiable enabling training by stochastic gradient descent. On Stanford Natural Language Inference dataset, the proposed model achieves better accuracy numbers than all published models in literature. %and Sentences Involving Compositional Knowledge dataset.% 
 In this paper, we present our first attempts in building a multilingual Neural Machine Translation framework under a unified approach. We are then able to employ attention-based NMT for many-to-many multilingual translation tasks. Our approach does not require any special treatment on the network architecture and it allows us to learn minimal number of free parameters in a standard way of training. Our approach has shown its effectiveness in an under-resourced translation scenario with considerable improvements up to 2.6 BLEU points. In addition, the approach has achieved interesting and promising results when applied in the translation task that there is no direct parallel corpus between source and target languages.   
 While recent neural machine translation approaches have delivered state-of-the-art performance for resource-rich language pairs, they suffer from the data scarcity problem for resource-scarce language pairs. Although this problem can be alleviated by exploiting a pivot language to bridge the source and target languages, the source-to-pivot and pivot-to-target translation models are usually independently trained. In this work, we introduce a joint training algorithm for pivot-based neural machine translation. We propose three methods to connect the two models and enable them to interact with each other during training. Experiments on Europarl and WMT corpora show that joint training of source-to-pivot and pivot-to-target models leads to significant improvements over independent training across various languages.  %Neural machine translation systems typically rely on the size of parallel corpora. Nevertheless, high-quality parallel corpora %are scarce resources for specific language pairs and domains. %For a source-to-target language pair with a small or even no parallel corpus, we introduce the pivot language to ``bridge'' source language and target language %under the existence of large source-to-pivot and pivot-to-target parallel corpora. %We propose three kinds of connection terms to jointly train source-to-pivot and pivot-to-target translation models to enhance the interaction between %two sets of model parameters. Experiments on German-French %and Spanish-French translation tasks with English as the pivot language show that our joint training approach improves the translation quality %significantly than independent training on source-to-pivot, pivot-to-target and source-to-target directions. 
 Sentence ordering is one of important tasks in NLP. Previous works mainly focused on improving its performance by using pair-wise strategy. However, it is nontrivial for pair-wise models to incorporate the contextual sentence information. In addition, error prorogation could be introduced by using the pipeline strategy in pair-wise models. In this paper, we propose an end-to-end neural approach to address the sentence ordering problem, which uses the pointer network  to alleviate the error propagation problem and utilize the whole contextual information. Experimental results show the effectiveness of the proposed model. Source codes\footnote{https://github.com/fudannlp} and dataset\footnote{http://nlp.fudan.edu.cn/data/} of this paper are available. \renewcommand*{\thefootnote}{\fnsymbol{footnote}} \footnotetext[1]{Jingjing Gong and Xinchi Chen contributed equally to this work.} \renewcommand*{\thefootnote}{\arabic{footnote}} 
 Recently, neural network models for natural language processing tasks have been increasingly focused on for their ability of alleviating the burden of manual feature engineering. However, the previous neural models cannot extract the complicated feature compositions as the traditional methods with discrete features. In this work, we propose a feature-enriched neural model for joint Chinese word segmentation and part-of-speech tagging task. Specifically, to simulate the feature templates of traditional discrete feature based models, we use different filters to model the complex compositional features with convolutional and pooling layer, and then utilize long distance dependency information with recurrent layer. Experimental results on five different datasets show the effectiveness of our proposed model. 
 % We examine the effect of the Group Lasso  regularizer in selecting the salient nodes of Deep Neural Network  hidden layers by applying a DNN-HMM hybrid speech recognizer to TED Talks speech data. We test two types of gLasso regularization, one for outgoing weight vectors and another for incoming weight vectors, as well as two sizes of DNNs: 2048 hidden layer nodes and 4096 nodes. Furthermore, we compare gLasso and L2 regularizers. Our experiment results demonstrate that our DNN training, in which the gLasso regularizer was embedded, successfully selected the hidden layer nodes that are necessary and sufficient for achieving high classification power.  
 Recurrent neural network grammars  are a recently proposed probabilistic generative modeling family for natural language. They show state-of-the-art language modeling and parsing performance. We investigate what information they learn, from a linguistic perspective, through various ablations to the model and the data, and by augmenting the model with an attention mechanism  to enable closer inspection. We find that explicit modeling of composition is crucial for achieving the best performance. Through the attention mechanism, we find that headedness plays a central role in phrasal representation . By training grammars without nonterminal labels, we find that phrasal representations depend minimally on nonterminals, providing support for the endocentricity hypothesis.  
 Researchers have recently started investigating deep neural networks for dialogue applications. In particular, generative sequence-to-sequence  models have shown promising results for unstructured tasks, such as word-level dialogue response generation. The hope is that such models will be able to leverage massive amounts of data to learn meaningful natural language representations and response generation strategies, while requiring a minimum amount of domain knowledge and hand-crafting. An important challenge is to develop models that can effectively incorporate dialogue context and generate meaningful and diverse responses. In support of this goal, we review recently proposed models based on generative encoder-decoder neural network architectures, and show that these models have better ability to incorporate long-term dialogue history, to model uncertainty and ambiguity in dialogue, and to generate responses with high-level compositional structure. 
  Speech is one of the most effective ways of communication among humans. Even though audio is the most common way of transmitting speech, very important information can be found in other modalities, such as vision. Vision is particularly useful when the acoustic signal is corrupted. Multi-modal speech recognition however has not yet found wide-spread use, mostly because the temporal alignment and fusion of the different information sources is challenging.  This paper presents an end-to-end audiovisual speech recognizer , based on recurrent neural networks  with a connectionist temporal classification ~ loss function. CTC creates sparse ``peaky'' output activations, and we analyze the differences in the alignments of output targets  between audio-only, video-only, and audio-visual feature representations. We present the first such experiments on the large vocabulary IBM ViaVoice database, which outperform previously published approaches on phone accuracy in clean and noisy conditions.  %Even though no other comparable pipelines for AVSR have been found, experiments show that the system presented in this paper outperforms traditional pipelines for AVSR such as  as well as deep learning state-of-the-art approaches , which actually use frame level labels to train their systems.  
 A deep learning approach has been widely applied in sequence modeling problems. In terms of automatic speech recognition , its performance has significantly been improved by increasing large speech corpus and deeper neural network. Especially, recurrent neural network and deep convolutional neural network have been applied in ASR successfully. Given the arising problem of training speed, we build a novel deep recurrent convolutional network for acoustic modeling and then apply deep residual learning to it. Our experiments show that it has not only faster convergence speed but better recognition accuracy over traditional deep convolutional recurrent network. In the experiments, we compare the convergence speed of our novel deep recurrent convolutional networks and traditional deep convolutional recurrent networks. With faster convergence speed, our novel deep recurrent convolutional networks can reach the comparable performance. We further show that applying deep residual learning can boost the convergence speed of our novel deep recurret convolutional networks. Finally, we evaluate all our experimental networks by phoneme error rate  with our proposed bidirectional statistical n-gram language model. Our evaluation results show that our newly proposed deep recurrent convolutional network applied with deep residual learning can reach the best PER of 17.33\% with the fastest convergence speed on TIMIT database. The outstanding performance of our novel deep recurrent convolutional neural network with deep residual learning indicates that it can be potentially adopted in other sequential problems.  
   We propose a new encoder-decoder approach to learn distributed sentence representations that are applicable to multiple purposes. The model is learned by using a   convolutional neural network as an encoder to map an input sentence into a continuous vector, and using a long short-term memory recurrent neural network as a decoder. Several tasks are considered, including sentence reconstruction and future sentence prediction. Further, a hierarchical encoder-decoder model is proposed to encode a sentence to   predict multiple future sentences.   By training our models on a large collection of novels, we obtain a highly generic convolutional sentence encoder that performs well in practice. Experimental results on several benchmark datasets, and across a broad range of applications, demonstrate the superiority of the proposed model over competing methods. 
    A significant number of neural architectures for reading comprehension have recently been developed and evaluated on large cloze-style datasets.   We present experiments supporting the emergence of  ``predication structure'' in the hidden state vectors of these readers.  More specifically, we provide evidence that   the hidden state vectors represent atomic formulas $\Phi[c]$ where $\Phi$ is a semantic property  and $c$ is a constant symbol entity identifier.   %We also report the best single-model performance to date on the Who-did-What dataset.   %   %\dmcomment{I think I agree with Kevnin here.}  
 Recurrent neural networks  have shown promising performance for language modeling. However, traditional training of RNNs, using back-propagation through time, often suffers from overfitting. One reason for this is that stochastic optimization  does not provide good estimates of model uncertainty. This paper leverages recent advances in stochastic gradient Markov Chain Monte Carlo  to learn weight uncertainty in RNNs. It yields a principled Bayesian learning algorithm, adding gradient noise during training  and model averaging when testing. Extensive experiments on various RNN models and across a broad range of applications demonstrate the superiority of the proposed approach relative to stochastic optimization.  
 Although attention-based Neural Machine Translation have achieved great success, attention-mechanism cannot capture the entire meaning of the source sentence because the attention mechanism generates a target word depending heavily on the relevant parts of the source sentence. The report of earlier studies has introduced a latent variable to capture the entire meaning of sentence and achieved improvement on attention-based Neural Machine Translation. We follow this approach and we believe that the capturing meaning of sentence benefits from image information because human beings understand the meaning of language not only from textual information but also from perceptual information such as that gained from vision. As described herein, we propose a neural machine translation model that introduces a continuous latent variable containing an underlying semantic extracted from texts and images. Our model, which can be trained end-to-end, requires image information only when training. Experiments conducted with an English--German translation task show that our model outperforms over the baseline. 
 We propose a simple, fast  decoding algorithm that fosters diversity in neural generation.  The algorithm modifies the standard beam search algorithm by penalizing hypotheses that are siblings---expansions of the same parent node in the search---thus favoring including hypotheses from diverse parents.  We evaluate the model on three neural generation tasks: dialogue response generation, abstractive summarization, and machine translation.  We also describe an extended model that uses reinforcement learning to automatically choose the appropriate level of beam diversity for different inputs or tasks. Simple diverse decoding helps across all three tasks,  especially those needing reranking or having diverse ground truth outputs; reinforcement learning offers an additional boost.   \footnote{This paper  includes material from the unpublished manuscript  ``Mutual Information and Diverse Decoding Improve Neural Machine Translation'' .}  \footnote{Code available upon publication.}  %We  %introduce a simple, straightforward re-ranking method to fulfill such a propose .  
  Recurrent neural networks  have achieved great success in language modeling. However, since the RNNs have fixed size of memory, their memory cannot store all the information about the words it have seen before in the sentence, and thus the useful long-term information may be ignored when predicting the next words. In this paper, we propose Attention-based Memory Selection Recurrent Network , in which the model can review the information stored in the memory at each previous time step and select the relevant information to help generate the outputs. In AMSRN, the attention mechanism finds the time steps storing the relevant information in the memory, and memory selection determines which dimensions of the memory are involved in computing the attention weights and from which the information is extracted. In the experiments, AMSRN outperformed long short-term memory  based language models on both English and Chinese corpora. Moreover, we investigate using entropy as a regularizer for attention weights and visualize how the attention mechanism helps language modeling.  
 Learning a natural language interface for database tables is a challenging task that involves deep language understanding and multi-step reasoning. The task is often approached by mapping natural language queries to { that provide the desired response when executed on the database. To our knowledge, this paper presents the first  weakly supervised, end-to-end neural network model to  induce such programs   on a real-world dataset. We enhance the objective function of Neural Programmer, a neural network with built-in discrete operations, and apply it on WikiTableQuestions, a natural language question-answering dataset. The model is trained end-to-end with weak supervision of question-answer pairs, and does not require domain-specific grammars, rules, or annotations that are key elements in previous approaches to program induction.   The main experimental result in this paper is that a single Neural Programmer model achieves 34.2\% accuracy using only 10,000 examples with weak supervision. An ensemble of 15 models, with a trivial combination technique, achieves 37.7\% accuracy, which is competitive to the current state-of-the-art accuracy of 37.1\% obtained by a traditional natural language  semantic parser.  %We plan to open-source our code in a few weeks from now. 
 We use reinforcement learning to learn tree-structured neural networks for computing representations of natural language sentences. In contrast with prior work on tree-structured models in which the trees are either provided as input or predicted using supervision from explicit treebank annotations, the tree structures in this work are optimized to improve performance on a downstream task. Experiments demonstrate the benefit of learning task-specific composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations. We analyze the induced trees and show that while they discover some linguistically intuitive structures , they are different than conventional English syntactic structures. 
   Developers of text-to-speech synthesizers  often make use of   human raters to assess the quality of synthesized speech. We   demonstrate that we can model human raters' mean opinion scores    of synthesized speech using a deep recurrent neural network   whose inputs consist solely of a raw waveform. Our best models   provide utterance-level estimates of MOS only moderately inferior to   sampled human ratings, as shown by Pearson and Spearman   correlations. When multiple utterances are scored and averaged,   a scenario common in synthesizer quality assessment,   AutoMOS achieves correlations approaching those of human raters.   The AutoMOS model has a number of applications, such as the   ability to explore the parameter space of a speech   synthesizer without requiring a human-in-the-loop. 
   We study methods for automated parsing of informal mathematical   expressions into formal ones, a main prerequisite for deep computer   understanding of informal mathematical texts. We propose a   context-based parsing approach that combines efficient statistical   learning of deep parse trees with their semantic pruning by type   checking and large-theory automated theorem proving. We show that   the methods very significantly improve on previous results in   parsing theorems from the Flyspeck corpus. % In this work we improve algorithms targeted at % automated translation of informal mathematical expressions into formal ones. % We first describe the first version of our combined statistical/semantic parsing % method based on the CYK chart-parsing algorithm augmented with limited % internal typechecking and external ATP filtering. This method was % previously evaluated on parsing ambiguous mathematical expressions % over the informalized Flyspeck corpus of about 22000 theorems. We  % discuss the motivation and drawbacks of the first version of the % CYK-based component of the algorithm, and then we propose, % implement and evaluate a more sophisticated approach based on better statistical % model of mathematical data structures. The overall improvement achieved is 37\% more correctly parsed  % Flyspeck theorems among the top 20 parses produced by the trained parsing system. 
 	%Natural language generation is potentially useful in a variety of applications such as natural language understanding and response generation in dialogue systems. In real-world, natural languages are usually generated at particular contexts or situations . Therefore, this paper studied how to generate meaningful natural languages at particular contexts.  	This paper studied generating natural languages at particular contexts or situations. We proposed two novel approaches which encode the contexts into a continuous semantic representation and then decode the semantic representation into text sequences with recurrent neural networks. During decoding, the context information are attended through a gating mechanism, addressing the problem of long-range dependency caused by lengthy sequences. We evaluate the effectiveness of the proposed approaches on user review data, in which rich contexts are available and two informative contexts, sentiments and products, are selected for evaluation. Experiments show that the fake reviews generated by our approaches are very natural. Results of fake review detection with human judges show that more than 50\% of the fake reviews are misclassified as the real reviews, and more than 90\% are misclassified by existing state-of-the-art fake review detection algorithm. 
 Sentiment prediction of contemporary music can have a wide-range of applications in modern society, for instance, selecting music for public institutions such as hospitals or restaurants to potentially improve the emotional well-being of personnel, patients, and customers, respectively. In this project, music recommendation system built upon on a naive Bayes classifier, trained to predict the sentiment of songs based on song lyrics alone. The experimental results show that music corresponding to a happy mood can be detected with high precision based on text features obtained from song lyrics. 
 Hybrid methods that utilize both content and rating information are commonly used in many recommender systems. However, most of them use either handcrafted features or the bag-of-words representation as a surrogate for the content information but they are neither effective nor natural enough. To address this problem, we develop a collaborative recurrent autoencoder  which is a denoising recurrent autoencoder  that models the generation of content sequences in the collaborative filtering  setting. The model generalizes recent advances in recurrent deep learning from i.i.d.\ input to non-i.i.d.\  input and provides a new denoising scheme along with a novel learnable pooling scheme for the recurrent autoencoder. To do this, we first develop a hierarchical Bayesian model for the DRAE and then generalize it to the CF setting. The synergy between denoising and CF enables CRAE to make accurate recommendations while learning to fill in the blanks in sequences. Experiments on real-world datasets from different domains  show that, by jointly modeling the order-aware generation of sequences for the content information and performing CF for the ratings, CRAE is able to significantly outperform the state of the art on both the recommendation task based on ratings and the sequence generation task based on content information. 
  Motivation: Finding related published articles is an important task in any science, but with the explosion of new work in the biomedical domain it has become especially challenging. Most existing methodologies use text similarity metrics to identify whether two articles are related or not. However biomedical knowledge discovery is hypothesis-driven. The most related articles may not be ones with the highest text similarities. In this study, we first develop an innovative crowd-sourcing approach to build an expert-annotated document-ranking corpus. Using this corpus as the gold standard, we then evaluate the approaches of using text similarity to rank the relatedness of articles. Finally, we develop and evaluate a new supervised model to automatically rank related scientific articles. \\ Results: Our results show that authors' ranking differ significantly from rankings by text-similarity-based models.  By training a learning-to-rank model on a subset of the annotated corpus, we found the best supervised learning-to-rank model  significantly surpassed state-of-the-art baseline systems.\\ Availability: Code and data are both publicly available.  \thanks{http://github.com/umassbionlp/crowd-ranking.git}\\ 
 Recurrent neural networks are a powerful tool for modeling sequential data, but the dependence of each timestep's computation on the previous timestep's output limits parallelism and makes RNNs unwieldy for very long sequences. We introduce quasi-recurrent neural networks , an approach to neural sequence modeling that alternates convolutional layers, which apply in parallel across timesteps, and a minimalist recurrent pooling function that applies in parallel across channels. Despite lacking trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size. Due to their increased parallelism, they are up to 16 times faster at train and test time. Experiments on language modeling, sentiment classification, and character-level neural machine translation demonstrate these advantages and underline the viability of QRNNs as a basic building block for a variety of sequence tasks. 
  In this paper we present a technique to train neural network models on small amounts of data. Current methods for training neural networks on small amounts of rich data typically rely on strategies such as fine-tuning a pre-trained neural network or the use of  domain-specific hand-engineered features. Here we take the  approach of treating network layers, or entire networks, as modules and combine pre-trained modules with untrained modules, to learn the shift in distributions between data sets. The central impact of using a modular approach comes from adding new representations to a network, as opposed to replacing representations via fine-tuning. Using this technique, we are able surpass results using standard fine-tuning transfer learning approaches, and we are also able to significantly increase performance over such approaches when using smaller amounts of data.    
 In this work, we propose a training algorithm for an audio-visual automatic speech recognition  system using deep recurrent neural network .First, we train a deep RNN acoustic model with a Connectionist Temporal Classification  objective function. The frame labels obtained from the acoustic model are then used to perform a non-linear dimensionality reduction of the visual features using a deep bottleneck network. Audio and visual features are fused and used to train a fusion RNN. The use of bottleneck features for visual modality helps the model to converge properly during training. Our system is evaluated on GRID corpus. Our results show that presence of visual modality gives significant improvement in character error rate  at various levels of noise even when the model is trained without noisy data. We also provide a comparison of two fusion methods: feature fusion and decision fusion.\footnote[1]{Version  accepted in 4th International Workshop on Multimodal pattern recognition of social signals in human computer interaction, a satellite event of the International Conference on Pattern Recognition }    
   Acquiring your first language is an incredible feat and not easily duplicated. Learning to communicate using nothing but a few pictureless books, a corpus, would likely be impossible even for humans. Nevertheless, this is the dominating approach in most  today. As an alternative, we propose the use of situated interactions between agents as a driving force for communication, and the framework of  for evolving a shared language grounded in the provided environment. We task the agents with interactive image search in the form of the game . The images from the game provide a non trivial environment for the agents to discuss and a natural grounding for the concepts they decide to encode in their communication. Our experiments show that the agents learn not only to encode physical concepts in their words, i.e. grounding, but also that the agents learn to hold a multi-step dialogue remembering the state of the dialogue from step to step.   %Learning your first language is an incredible feat and not easily duplicated. Doing this using nothing but a few pictureless books, a corpus, would likely be impossible even for humans. As an alternative we propose to use situated interactions between agents as a driving force for communication  , and the framework of   for creating a shared language grounded in the provided environment. We task the agents with interactive image search in the form of the game . The images from the game provide a non trivial environment for the agents to discuss and a natural grounding for the concepts they decide to encode in their communication. Our experiments show that it is possible to learn this task using DRQN and even more importantly that the words the agents use correspond to physical attributes present in the images that make up the agents environment. 
 %Change the middle lines of 2nd para %Deep Neural Network architectures with external memory components allow the model to perform inference and capture long term dependencies, by storing information explicitly. The advantages of the framework are demonstrated on the task of video captioning, i.e generating natural language descriptions for videos.   In this paper, we introduce Key-Value Memory Networks to a multimodal setting and a novel key-addressing mechanism to deal with sequence-to-sequence models. The proposed model naturally decomposes the problem of video captioning into vision and language segments, dealing with them as key-value pairs. More specifically, we learn a semantic embedding  corresponding to each frame  in the video, thereby creating  memory slots. We propose to find the next step attention weights conditioned on the previous attention distributions for the key-value memory slots in the memory addressing schema. Exploiting this flexibility of the framework, we additionally capture spatial dependencies while mapping from the visual to semantic embedding. Experiments done on the Youtube2Text dataset demonstrate usefulness of recurrent key-addressing, while achieving competitive scores on BLEU@4, METEOR metrics against state-of-the-art models. %We introduce a temporal structure in the memory addressing schema. This allows us to exploit the temporal dependencies in the recurrent key-addressing and in the language decoder. Exploiting this flexibility of the framework, we additionally capture spatial dependencies while mapping from the visual to semantic embedding. 
 In lexicon-based classification, documents are assigned labels by  comparing the number of words that appear from two opposed lexicons, such as positive and negative sentiment. Creating such words lists is often easier than labeling instances, and they can be debugged by non-experts if classification performance is unsatisfactory. However, there is little analysis or justification of this classification heuristic. This paper describes a set of assumptions that can be used to derive a probabilistic justification for lexicon-based classification, as well as an analysis of its expected accuracy. One key assumption behind lexicon-based classification is that all words in each lexicon are equally predictive. This is rarely true in practice, which is why lexicon-based approaches are usually outperformed by supervised classifiers that learn distinct weights on each word from labeled instances. This paper shows that it is possible to learn such weights without labeled data, by leveraging co-occurrence statistics across the lexicons. This offers the best of both worlds: light supervision in the form of lexicons, and data-driven classification with higher accuracy than traditional word-counting heuristics. 
 	% A Semantic Compositional Network  is developed for image captioning, in which semantic concepts  are detected from the image, and the probability of each tag is used to compose the parameters in a long short-term memory  network. The SCN extends each weight matrix of the LSTM to an ensemble of tag-dependent weight matrices. The degree to which each member of the ensemble is used to generate an image caption is tied to the image-dependent probability of the corresponding tag. In addition to captioning images, we also extend the SCN to generate captions for video clips. We qualitatively analyze semantic composition in SCNs, and quantitatively evaluate the algorithm on three benchmark datasets: COCO, Flickr30k, and Youtube2Text. Experimental results show that the proposed method significantly outperforms prior state-of-the-art approaches, across multiple evaluation metrics. % %This paper presents Semantic Compositional Network  for automatically generating captions for images. When captioning an image, we first detect key semantic concepts in the image, and then the caption is composed of these semantic concepts. In order to achieve this, we develop the SCN by extending each weight matrix of long short-term memory  recurrent networks to be a three-way matrix product, with one of these matrices dependent on the inferred semantic concepts.  Consequently, the SCN can be viewed as an ensemble of semantic-concept-dependent LSTM bases, with the contribution of each LSTM basis unit proportional to the likelihood that the semantic concept is present in the image. % %A neural-network-based image-captioning framework is developed, in which the composition of the network parameters is dependent on semantic information inferred from the image under test. We specifically consider long-short-term memory  networks. The model relies on first detecting high-level semantic concepts associated with in an image, and based on these the LSTM parameters are factored via three-way weight matrices. Consequently, the network parameters  are dependent on the specific image under test.  %In addition to captioning images, the approach is extended to generate captions for videos. We qualitatively analyze semantic composition, and quantitatively evaluate the algorithm on three benchmarks: COCO, Flickr30k, and Youtube2Text. Experimental results show that the proposed method significantly and consistently outperforms prior state-of-the-art approaches, across multiple evaluation metrics.  	% 
 Community-based question answering services have arisen as a popular knowledge sharing pattern for netizens. With abundant interactions among users, individuals are capable of obtaining satisfactory information. However, it is not effective for users to attain answers within minutes. Users have to check the progress over time until the satisfying answers submitted. We address this problem as a user personalized satisfaction prediction task. Existing methods usually exploit manual feature selection. It is not desirable as it requires careful design and is labor intensive. In this paper,  we settle this issue by developing a new multiple instance deep learning framework. Specifically, in our settings, each question follows a weakly supervised learning  assumption, where its obtained answers can be regarded as instance sets and we define the question resolved with at least one satisfactory answer. We thus design an efficient framework exploiting multiple instance learning property with deep learning tactic to model the question-answer pairs relevance and rank the asker's satisfaction possibility. Extensive experiments on large-scale datasets from Stack Exchange demonstrate the feasibility of our proposed framework in predicting askers personalized satisfaction. Our framework can be extended to numerous applications such as UI satisfaction Prediction, multi-armed bandit problem, expert finding and so on.   
 Community-based question answering platforms have attracted substantial users to share knowledge and learn from each other. As the rapid enlargement of CQA platforms, quantities of overlapped questions emerge, which makes users confounded to select a proper reference. It is urgent for us to take effective automated algorithms to reuse historical questions with corresponding answers. In this paper we focus on the problem with question retrieval, which aims to match historical questions that are relevant or semantically equivalent to resolve one's query directly. The challenges in this task are the lexical gaps between questions for the word ambiguity and word mismatch problem. Furthermore, limited words in queried sentences cause sparsity of word features. To alleviate these challenges, we propose a novel framework named HNIL which encodes not only the question contents but also the asker's social interactions to enhance the question embedding performance. More specifically, we apply random walk based learning method with recurrent neural network to match the similarities between asker's question and historical questions proposed by other users. Extensive experiments on a large-scale dataset from a real world CQA site Quora show that employing the heterogeneous social network information outperforms the other state-of-the-art solutions in this task.  
 To enhance developer productivity, all modern integrated development environments  include  functionality that proposes likely next tokens at the cursor. While current IDEs work well for statically-typed languages, their reliance on type annotations means that they do not provide the same level of support for dynamic programming languages as for statically-typed languages.  Moreover, suggestion engines in modern IDEs do not propose expressions or multi-statement idiomatic code.  Recent work has shown that language models can improve code suggestion systems by learning from software repositories.  This paper introduces a neural language model with a sparse pointer network aimed at capturing very long-range dependencies. We release a large-scale code suggestion corpus of $41$M lines of Python code crawled from GitHub. On this corpus, we found standard neural language models to perform well at suggesting local phenomena, but struggle to refer to identifiers that are introduced many tokens in the past.  By augmenting a neural language model with a pointer network specialized in referring to predefined classes of identifiers, we obtain a much lower perplexity and a $5$ percentage points increase in accuracy for code suggestion compared to an LSTM baseline.  In fact, this increase in code suggestion accuracy is due to a $13$ times more accurate prediction of identifiers. Furthermore, a qualitative analysis shows this model indeed captures interesting long-range dependencies, like referring to a class member defined over $60$ tokens in the past. 
 Standard deep reinforcement learning methods such as Deep Q-Networks  for multiple tasks  face scalability problems. We propose a method for multi-domain dialogue policy learning---termed NDQN, and apply it to an information-seeking spoken dialogue system in the domains of restaurants and hotels. Experimental results comparing  DQN  versus NDQN  using simulations report that our proposed method exhibits better scalability and is promising for optimising the behaviour of multi-domain dialogue systems. 
 Structural correspondence learning  is an effective method for cross-lingual sentiment classification. This approach uses unlabeled documents along with a word translation oracle to automatically induce task specific, cross-lingual correspondences. It transfers knowledge through identifying important features, i.e., pivot features. For simplicity, however, it assumes that the word translation oracle maps each pivot feature in source language to exactly only one word in target language. This one-to-one mapping between words in different languages is too strict. Also the context is not considered at all. In this paper, we propose a cross-lingual SCL based on distributed representation of words; it can learn meaningful one-to-many mappings for pivot words using large amounts of monolingual data and a small dictionary. We conduct experiments on NLP\&CC 2013 cross-lingual sentiment analysis dataset, employing English as source language, and Chinese as target language. Our method does not rely on the parallel corpora and the experimental results show that our approach is more competitive than the state-of-the-art methods in cross-lingual sentiment classification. 
 The computational mechanisms by which nonlinear recurrent neural networks  achieve their goals remains an open question.  There exist many problem domains where intelligibility of the network model is crucial for deployment. Here we introduce a recurrent architecture composed of input-switched affine transformations, in other words an RNN without any nonlinearity and with one set of weights per input. We show that this architecture achieves near identical performance to traditional architectures on language modeling of Wikipedia text, for the same number of model parameters.  It can obtain this performance with the potential for computational speedup compared to existing methods, by precomputing the composed affine transformations corresponding to longer input sequences.  As our architecture is affine, we are able to understand the mechanisms by which it functions using linear methods. For example, we show how the network linearly combines contributions from the past to make predictions at the current time step. We show how representations for words can be combined in order to understand how context is transferred across word boundaries. Finally, we demonstrate how the system can be executed and analyzed in arbitrary bases to aid understanding. 
 Classifying products into categories precisely and efficiently is a major challenge in modern e-commerce. The high traffic of new products uploaded daily and the dynamic nature of the categories raise the need for machine learning models that can reduce the cost and time of human editors. In this paper, we propose a decision level fusion approach for multi-modal product classification using text and image inputs. We train input specific state-of-the-art deep neural networks for each input source, show the potential of forging them together into a multi-modal architecture and train a novel policy network that learns to choose between them. Finally, we demonstrate that our multi-modal network improves the top-1 accuracy $\%$ over both networks on a real-world large-scale product classification dataset that we collected from Walmart.com. While we focus on image-text fusion that characterizes e-commerce domains, our algorithms can be easily applied to other modalities such as audio, video, physical sensors, etc. 
  With the advent of semantic web, various tools and techniques have been introduced for presenting and organizing knowledge. Concept hierarchies are one such technique which gained significant attention due to its usefulness in creating domain ontologies that are considered as an integral part of semantic web. Automated concept hierarchy learning algorithms focus on extracting relevant concepts from unstructured text corpus and connect them together by identifying some potential relations exist between them. In this paper, we propose a novel approach for identifying relevant concepts from plain text and then learns hierarchy of concepts by exploiting subsumption relation between them. To start with, we model topics using a probabilistic topic model and then make use of some lightweight linguistic process to extract semantically rich concepts. Then we connect concepts by identifying an "is-a" relationship between pair of concepts. The proposed method is completely unsupervised and there is no need for a domain specific training corpus for concept extraction and learning. Experiments on large and real-world text corpora such as BBC News dataset and Reuters News corpus shows that the proposed method outperforms some of the existing methods for concept extraction and efficient concept hierarchy learning is possible if the overall task is guided by a probabilistic topic modeling algorithm.  \\\\ {\bf Keywords :} Probabilistic Topic Models, Concept Extraction, Subsumption Hierarchy Learning, Natural Language Processing, Semantic Web, Text Mining. 
 An important aspect of developing conversational agents is  to give a bot the  ability to improve  through communicating with humans and to learn from the mistakes that it makes.   Most research has focused on learning from fixed training sets of labeled data rather than interacting with a dialogue partner  in an online fashion. In this paper we explore this direction in a reinforcement learning setting where the bot improves its question-answering ability from feedback a teacher gives following its generated responses. We build a simulator that tests various aspects of such learning in a synthetic environment, and introduce models that work in this regime.  Finally, real experiments with Mechanical Turk validate the approach. 
 %OL will rewrite this:\\   We present a method for inducing    new   dialogue systems from very small amounts of %  unannotated  dialogue data, showing how word-level exploration using Reinforcement Learning , combined with an incremental and semantic grammar - Dynamic Syntax  - allows systems to discover, generate, and understand many new dialogue variants.  The method avoids the use of expensive and time-consuming dialogue act annotations, and supports more natural  dialogues than turn-based systems.   Here, language generation and dialogue management are treated as a joint decision/optimisation problem, and  the  MDP model for RL is constructed automatically. With an implemented system, we show that   this method   enables a wide range of dialogue  variations to be automatically captured, even when the system is trained from only a single  dialogue. The variants include question-answer pairs, over- and under-answering, self- and other-corrections, clarification interaction, split-utterances, and ellipsis.  This generalisation property results   from the  structural knowledge and constraints present within the DS grammar, and highlights some   limitations of recent  systems   built using machine learning techniques only. 
 Advances in neural variational inference have facilitated the learning of powerful directed graphical models with continuous latent variables, such as variational autoencoders. %The hope is that such models will learn to represent latent factors in a wide range of real-world data. The hope is that such models will learn to represent rich, multi-modal latent factors in real-world data, such as natural language text. %However, latent factors in real-world data --- such as natural language text -- often posses latent factors, which follow highly non-linear, multi-modal distributions. %, such as natural language text. %, audio, and images. %However, latent factors in real-world data are often highly complex; for example, topics in newswire text and responses in conversational dialogue often posses latent factors, which follow non-linear , multi-modal distributions. % with many statistical outliers. However, current models often assume simplistic priors on the latent variables --- such as the uni-modal Gaussian distribution --- which are incapable of representing complex latent factors efficiently. To overcome this restriction, we propose the simple, but highly flexible, piecewise constant distribution. This distribution has the capacity to represent an exponential number of modes of a latent target distribution, while remaining mathematically tractable. Our results demonstrate that incorporating this new latent distribution into different models yields substantial improvements in natural language processing tasks such as document modeling and natural language generation for dialogue. 
 Distributed representations of words have been shown to capture lexical semantics, as demonstrated by their effectiveness in word similarity and analogical relation tasks.  But, these tasks only evaluate lexical semantics indirectly. In this paper, we study whether it is possible to utilize distributed representations to generate dictionary definitions of words, as a more direct and transparent representation of the embeddings' semantics. We introduce definition modeling, the task of generating a definition for a given word and its embedding.  We present several definition model architectures based on recurrent neural networks, and experiment with the models over multiple data sets. Our results show that a model that controls dependencies between the word being defined and the definition words performs significantly better, and that a character-level convolution layer designed to leverage morphology can complement word-level embeddings. Finally, an error analysis suggests that the errors made by a definition model may provide insight into the shortcomings of word embeddings. 
 We present an automatic mortality prediction scheme based on the unstructured textual content of clinical notes. Proposing a convolutional document embedding approach, our empirical investigation using the MIMIC-III intensive care database shows significant performance gains compared to previously employed methods such as latent topic distributions or generic doc2vec embeddings. These improvements are especially pronounced for the difficult problem of post-discharge mortality prediction. 
 Transition-based models can be fast and accurate for constituent parsing. Compared with chart-based models, they leverage richer features by extracting history information from a parser stack, which spans over non-local constituents. On the other hand, during incremental parsing, constituent information on the right hand side of the current word is not utilized, which is a relative weakness of shift-reduce parsing. To address this limitation, we leverage a fast neural model to extract lookahead features.  In particular, we build a bidirectional LSTM model, which leverages the full sentence information to predict the hierarchy of constituents that each word starts and ends. The results are then passed to a strong transition-based constituent parser as lookahead features. The resulting parser gives 1.3\% absolute improvement in WSJ and 2.3\% in CTB compared to the baseline, given the highest reported accuracies for fully-supervised parsing. 
 Natural language understanding and dialogue policy learning are both essential in conversational systems that predict the next system actions in response to a current user utterance. Conventional approaches aggregate separate models of natural language understanding  and system action prediction  as a pipeline that is sensitive to noisy outputs of error-prone NLU\@. To address the issues, we propose an end-to-end deep recurrent neural network with limited contextual dialogue memory by jointly training NLU and SAP on DSTC4 multi-domain human-human dialogues. Experiments show that our proposed model significantly outperforms the state-of-the-art pipeline models for both NLU and SAP, which indicates that our joint model is capable of mitigating the affects of noisy NLU outputs, and NLU model can be refined by error flows backpropagating from the extra supervised signals of system actions. 
 % % somehow it does not work Extending the success of deep neural networks to natural language understanding and symbolic reasoning requires complex operations and external memory.  Recent neural program induction approaches have attempted to address this problem, but are typically limited to differentiable  memory, and consequently cannot scale beyond small synthetic tasks. In this work, we propose the Manager-Programmer-Computer framework, which integrates neural networks with non-differentiable memory to support abstract, scalable and precise operations through a friendly neural computer interface.  Specifically, we introduce a Neural Symbolic Machine, which contains a sequence-to-sequence neural "programmer",  and a non-differentiable "computer" that is a Lisp interpreter with code assist.  To successfully apply REINFORCE for training, we augment it with approximate gold programs found by an iterative maximum likelihood training process. NSM is able to learn a semantic parser from weak supervision over a large knowledge base. It achieves new state-of-the-art performance on WebQuestionsSP, a challenging semantic parsing dataset. Compared to previous approaches, NSM is end-to-end, therefore does not rely on feature engineering or domain specific knowledge.  %that takes in natural language input and outputs a program as a sequence of tokens 
 Online content publishers often use catchy headlines for their articles in order to attract users to their websites. These headlines, popularly known as { for detecting  clickbaits. Our model relies on distributed word representations learned from a large unannotated corpora, and character embeddings learned via Convolutional Neural Networks. Experimental results on a dataset of news headlines show that our model outperforms existing techniques for clickbait detection with an accuracy of 0.98 with F1-score of  0.98  and ROC-AUC of 0.99.   
     Diagnosis of a clinical condition is a challenging task, which often requires significant medical investigation. Previous work related to diagnostic inferencing problems mostly consider multivariate observational data . In contrast, we explore the problem using free-text medical notes recorded in an electronic health record . Complex tasks like these can benefit from structured knowledge bases, but those are not scalable. We instead exploit raw text from Wikipedia as a knowledge source. Memory networks have been demonstrated to be effective in tasks which require comprehension of free-form text. They use the final iteration of the learned representation to predict probable classes. We introduce condensed memory neural networks , a novel model with iterative condensation of memory representations that preserves the hierarchy of features in the memory. Experiments on the MIMIC-III dataset show that the proposed model outperforms other variants of memory networks to predict the most probable diagnoses given a complex clinical scenario.   
 Multitask learning has been applied successfully to a range of tasks, mostly morphosyntactic.  However, little is known on when MTL works and whether there are data characteristics that help to determine its success. In this paper we evaluate a range of semantic sequence labeling tasks in a MTL setup. We examine different auxiliary tasks, amongst which a novel setup, and correlate their impact to data-dependent conditions. Our results show that MTL is not always effective, significant improvements are obtained only for 1 out of 5 tasks. When successful, auxiliary tasks with compact and more uniform label distributions are preferable. 
 The advent of the attention mechanism in neural machine translation models has improved the performance of machine translation systems by enabling selective lookup into the source sentence. In this paper, the efficiencies of translation using bidirectional encoder attention decoder models were studied with respect to translation involving morphologically rich languages. The English--Tamil language pair was selected for this analysis. First, the use of Word2Vec embedding for both the English and Tamil words improved the translation results by 0.73 BLEU points over the baseline RNNSearch model with 4.84 BLEU score. The use of morphological segmentation before word vectorization to split the morphologically rich Tamil words into their respective morphemes before the translation caused a reduction in the target vocabulary size by a factor of 8. Also, this model  improved the performance of neural machine translation by 7.05 BLEU points over the RNNSearch model used over the same corpus. Since the BLEU evaluation of the RNNMorph model might be unreliable due to an increase in number of matching tokens per sentence, the performances of the translations were also compared by means of human evaluation metrics of adequacy, fluency and relative ranking. Further, the use of morphological segmentation also improved the efficacy of the attention mechanism. 
 In this work, we present a new dataset for computational humor, specifically comparative humor ranking, which attempts to eschew the ubiquitous binary approach to humor detection. The dataset consists of tweets that are humorous responses to a given hashtag. We describe the motivation for this new dataset, as well as the collection process, which includes a description of our semi-automated system for data collection. We also present initial experiments for this dataset using both unsupervised and supervised approaches. %Our predictive models focus primarily on the lexical diversity and  sentiment.  % %Our best unsupervised system achieves 0.51\% accuracy, revealing how much more difficult this dataset is than comparable datasets. Our best supervised system achieves 0.55\% accuracy.  % % % Our best supervised system achieves 0.55\% accuracy, with best unsupervised results 0.51\%, suggesting that this task is much more difficult than comparable humor detection tasks.  % % Initial experiments indicate that a recurrent neural network model can learn representations more suitable to the task than basic embedding averaging. % Lesha TACL - commented two previous paragraphs Our best supervised system achieved 63.7\% accuracy, suggesting that this task is much more difficult than comparable humor detection tasks.  Initial experiments indicate that a character-level model is more suitable for this task than a token-level model, likely due to a large amount of puns that can be captured by a character-level model.    %These results strongly suggest that this task cannot be solved with a straightforward approach. The performance of a recurrent neural network model dictates that it can learn representations more suitable to the task than basic embedding averaging. 
 In training speech recognition systems, labeling audio clips can be expensive, and not all data is equally valuable. Active learning aims to label only the most informative samples to reduce cost. For speech recognition, confidence scores and other likelihood-based active learning methods have been shown to be effective. Gradient-based active learning methods, however, are still not well-understood.  This work investigates the Expected Gradient Length  approach in active learning for end-to-end speech recognition. We justify  from a variance reduction perspective, and observe that 's measure of informativeness picks novel samples uncorrelated with confidence scores. Experimentally, we show that  can reduce word errors by 11\%, or alternatively, reduce the number of samples to label by 50\%, when compared to random sampling. 
 Inspired by recent research, we explore ways to model the highly morphological Finnish language at the level of characters while maintaining the performance of word-level models. We propose a new Character-to-Word-to-Character  compositional language model that uses characters as input and output while still internally processing word level embeddings. Our preliminary experiments, using the Finnish Europarl V7 corpus, indicate that C2W2C can respond well to the challenges of morphologically rich languages such as high out of vocabulary rates, the prediction of novel words, and growing vocabulary size. Notably, the model is able to correctly score inflectional forms that are not present in the training data and sample grammatically and semantically correct Finnish sentences character by character.    
 %Many natural language processing tasks can be formulated as questions answering problems. This paper introduces a novel neural network model for question answering, the . It enhances neural networks' ability of representing and calculating information over a long period by keeping records of entities contained in text. The core component is a memory pool which comprises entities' states. These entities' states are continuously updated according to the input text. Questions with regard to the input text are used to search the memory pool for related entities and answers are further predicted based on the states of retrieved entities. %Entities in this model are regard as the basic units that carry information and construct text. Information carried by text are encoded in the states of entities. %Hence text can be best understood by analyzing its containing entities. Compared with previous memory network models, the proposed model is capable of handling  fine-grained information and more sophisticated relations based on entities. We formulated several different tasks as question answering problems and tested the proposed model. Experiments reported satisfying results.  %include machine comprehension test, sentiment classification    
   We present a novel scheme to combine neural machine translation  with traditional statistical machine translation . Our approach borrows ideas from linearised lattice minimum Bayes-risk decoding for SMT. The NMT score is combined with the Bayes-risk of the translation according the SMT lattice. This makes our approach much more flexible than $n$-best list or lattice rescoring as the neural decoder is not restricted to the SMT search space. We show an efficient and simple way to integrate risk estimation into the NMT decoder which is suitable for word-level as well as subword-unit-level NMT. We test our method on English-German and Japanese-English and report significant gains over lattice rescoring on several data sets for both single and ensembled NMT. The MBR decoder produces entirely new hypotheses far beyond simply rescoring the SMT search space or fixing UNKs in the NMT output. 
 We propose an online, end-to-end, neural generative conversational model for open-domain dialogue. %Our model augments, in multiple ways, the standard sequence-to-sequence dialog generation framework which often generates short, dull and repetitive responses.  It is trained using a unique combination of offline two-phase supervised learning and online human-in-the-loop active learning. %During the online phase, the agent interacts with human users and incrementally improves its conversational skills via a policy-gradient method.  While most existing research proposes offline supervision or hand-crafted reward functions for online reinforcement, we devise a novel interactive learning mechanism based on hamming-diverse beam search for response generation and one-character user-feedback at each step. Experiments show that our model inherently promotes the generation of semantically relevant and interesting responses, and can be used to train agents with customized personas, moods and conversational styles. 
 We introduce a new model, the \modelname~.  It is equipped with a dynamic long-term memory which allows it to maintain and update a representation of the state of the world as it receives new data. For language understanding tasks, it can reason on-the-fly as it reads text, not just when it is required to answer a question or respond as is the case for a Memory Network . Like a Neural Turing Machine or Differentiable Neural Computer  it maintains a fixed size memory and can learn to perform location and content-based read and write operations.  However, unlike those models it has a simple parallel  architecture in which several memory locations can be updated simultaneously. The \modelabbrev~sets a new state-of-the-art on the bAbI tasks, and is the first method to solve all the tasks in the 10k training examples setting.   We also demonstrate that it can solve a reasoning task which requires a large number of supporting facts, which other methods are not able to solve, and can generalize past its training horizon. It can also be practically used on large scale datasets such as  Children's Book Test, where it obtains competitive performance, reading the story in a single pass. 
     Mismatched transcriptions have been proposed as a mean to acquire probabilistic transcriptions from non-native speakers of a language. Prior work has demonstrated the value of these transcriptions by successfully adapting cross-lingual ASR systems for different target languages. In this work, we describe two techniques to refine these probabilistic     transcriptions: a noisy-channel model of non-native phone misperception     is trained using a recurrent neural network, and decoded using     minimally-resourced language-dependent pronunciation constraints.     Both innovations improve quality of the transcript, and both innovations     reduce phone error rate of a trained ASR, by 7\% and 9\% respectively.   
 We present an architecture for information extraction from text that augments an existing parser with a character-level neural network. The network is trained using a measure of consistency of extracted data with existing databases as a form of noisy supervision.  Our architecture combines the ability of constraint-based information extraction systems to easily incorporate domain knowledge and constraints with the ability of deep neural networks to leverage large amounts of data to learn complex features.  Boosting the existing parser's precision, the system led to large improvements over a mature and highly tuned constraint-based production information extraction system used at Bloomberg for financial language text.  
 Emoji is an essential component in dialogues which has been broadly utilized on almost all social platforms. It could express more delicate feelings beyond plain texts and thus smooth the communications between users, making dialogue systems more anthropomorphic and vivid. In this paper, we focus on automatically recommending appropriate emojis given the contextual information in multi-turn dialogue systems, where the challenges locate in understanding the whole conversations. More specifically, we propose the hierarchical long short-term memory model  to construct dialogue representations, followed by a softmax classifier for emoji classification. We evaluate our models on the task of emoji classification in a real-world dataset, with some further explorations on parameter sensitivity and case study. Experimental results demonstrate that our method achieves the best performances on all evaluation metrics. It indicates that our method could well capture the contextual information and emotion flow in dialogues, which is significant for emoji recommendation. 
 Analysing translation quality in regards to specific linguistic phenomena has historically been difficult and time-consuming. Neural machine translation has the attractive property that it can produce scores for arbitrary translations, and we propose a novel method to assess how well NMT systems model specific linguistic phenomena such as agreement over long distances, the production of novel words, and the faithful translation of polarity. The core idea is that we measure whether a reference translation is more probable under a NMT model than a contrastive translation which introduces a specific type of error. We present \lingeval\footnote{Test set and evaluation script are available at \url{https://github.com/rsennrich/lingeval97}}, a large-scale data set of \lingevalsize contrastive translation pairs based on the WMT English$\to$German translation task, with errors automatically created with simple rules. We report results for a number of systems, and find that recently introduced character-level NMT systems perform better at transliteration than models with byte-pair encoding  segmentation, but perform more poorly at morphosyntactic agreement, and translating discontiguous units of meaning. 
 A good dialogue agent should have the ability to interact with users by both responding to questions and by asking questions, and importantly to learn from both types of interaction. %asking and answering questions. In this work, we explore this direction by  designing a simulator and a set of synthetic tasks in the movie  domain that allow such interactions between a learner and a teacher. We investigate how a learner can benefit from asking questions in both offline and online reinforcement learning settings, and  demonstrate that the learner improves when asking questions. Finally, real experiments with Mechanical Turk validate the approach. Our work represents a first step in developing such  end-to-end learned interactive dialogue agents. %\footnote{Data, code and simulator to be released upon publication. % %A realistic dialogue agent should have the ability of interacting with its %users and learning from those interactions. In this work we explore  %this direction by proposing a training framework in which the dialogue  %agent is trained to not only answer questions from its users, but to also  %ask questions when it is not sure how to respond. We accomplish this  %by designing a simulator and a set of synthetic tasks in the Movie  %Question Answering domain, which enables the agent to interact with  %a teacher by both answering and asking questions to it.  %We investigate how the agent can benefit from asking questions in an  %offline and an online reinforcement learning setting. Our experiments %demonstrate that indeed, when an agent is allowed to ask questions, %its learning ability increases significantly. To the best of  %our knowledge this work is a first step in the direction of developing an  %end-to-end dialogue agent which not only learns from static supervised  %labeled data, but also learns via interacting with a teacher.  % %A good dialogue agent should have the ability to interact with users. %In this work, we explore this direction by % designing a simulator and a set of synthetic tasks in the movie % domain that allow the learner to interact with a teacher by both asking and answering questions. %We investigate how a learner can benefit from asking questions in %both an offline and online reinforcement learning setting. %We demonstrate that the learner improves when asking questions. %Our work represents a first step in developing end-to-end learned interactive dialogue agent\footnote{Data, code and simulator to be released upon publication.}. 
  Existing models based on artificial neural networks  for sentence classification often do not incorporate the context in which sentences appear, and classify sentences individually.  However, traditional sentence classification approaches have been shown to greatly benefit from jointly classifying subsequent sentences, such as with conditional random fields. In this work, we present an ANN architecture that combines the effectiveness of typical ANN models to classify sentences in isolation, with the strength of structured prediction. Our model achieves state-of-the-art results on two different datasets for sequential sentence classification in medical abstracts.    
  We explore the task of multi-source morphological reinflection, which generalizes the standard, single-source version. The input consists of  a target tag and  multiple pairs of source form and source tag for a lemma. The motivation is that it is beneficial to have access to more than one source form since different source forms can provide complementary information, e.g., different stems.  We further present a novel extension to the encoder-decoder recurrent neural architecture, consisting of multiple encoders, to better solve the task. We show that our new architecture outperforms single-source reinflection models and publish our dataset for multi-source morphological reinflection to facilitate future research. 
  Recently, the attention mechanism plays a key role to achieve high performance for Neural Machine Translation models. However, as it computes a score function for the encoder states in all positions at each decoding step, the attention model greatly increases the computational complexity. In this paper, we investigate the adequate vision span of attention models in the context of machine translation, by proposing a novel attention framework that is capable of reducing redundant score computation dynamically. The term ``vision span'' means a window of the encoder states considered by the attention model in one step. In our experiments, we found that the average window size of vision span can be reduced by over 50\% with modest loss in accuracy on English-Japanese and German-English translation tasks.% This results indicate that the conventional attention mechanism performs a significant amount of redundant computation. 
 Training efficiency is one of the main problems for Neural Machine Translation . Deep networks need for very large data as well as many training iterations to achieve state-of-the-art performance. This results in very high computation cost, slowing down research and industrialisation. %In this paper, we propose an efficient training method based on data boosting and bootstrap with no modifications to the neural network. %We experiment on an English-French translation task showing accuracy improvements of up to $1.63$ BLEU points while saving $20\%$ of training time. In this paper, we propose to alleviate this problem with several training methods based on data boosting and bootstrap with no modifications to the neural network. It imitates the learning process of humans, which typically spend more time when learning ``difficult'' concepts than easier ones. We experiment on an English-French translation task showing accuracy improvements of up to $1.63$ BLEU while saving $20\%$ of training time. 
   Machine translation systems are very sensitive to the domains they were trained on. Several domain adaptation techniques have been deeply studied. We propose a new technique for neural machine translation  that we call domain control which is performed at runtime using a unique neural network covering multiple domains. The presented approach shows quality improvements when compared to dedicated domains translating on any of the covered domains and even on out-of-domain data. In addition, model parameters do not need to be re-estimated for each domain, making this effective to real use cases. Evaluation is carried out on English-to-French translation for two different testing scenarios. We first consider the case where an end-user performs translations on a known domain. Secondly, we consider the scenario where the domain is not known and predicted at the sentence level before translating. Results show consistent accuracy improvements for both conditions. 
 Domain adaptation is a key feature in Machine Translation.  It generally encompasses terminology, domain and style adaptation, especially for human post-editing workflows in Computer Assisted Translation .  With Neural Machine Translation , we introduce a new notion of domain adaptation that we call ``specialization'' and which is showing promising results both in the learning speed and in adaptation accuracy. In this paper, we propose to explore this approach under several perspectives. 
 Neural Machine Translation  is a new approach for automatic translation of text from one human language into another. The basic concept in NMT is to train a large Neural Network that maximizes the translation performance on a given parallel corpus. NMT is gaining popularity in the research community because it outperformed traditional SMT approaches in several translation tasks at WMT and other evaluation tasks/benchmarks at least for some language pairs. However, many of the enhancements in SMT over the years have not been incorporated into the NMT framework.  In this paper, we focus on one such enhancement namely domain adaptation. We propose an approach for adapting a NMT system to a new domain. The main idea behind domain adaptation is that the availability of large out-of-domain training data and a small in-domain training data. We report significant gains with our proposed method in both automatic metrics and a human subjective evaluation metric on two language pairs. With our adaptation method, we show large improvement on the new domain while the performance of our general domain only degrades slightly. In addition, our approach is fast enough to adapt an already trained system to a new domain within few hours without the need to retrain the NMT model on the combined data which usually takes several days/weeks depending on the volume of the data. 
 In this paper we propose and carefully evaluate a sequence labeling framework which solely utilizes sparse indicator features derived from dense distributed word representations. The proposed model obtains  state-of-the art performance for both part-of-speech tagging and named entity recognition for a variety of languages. Our model relies only on a few thousand sparse coding-derived features, without applying any modification of the word representations employed for the different tasks. The proposed model has favorable generalization properties as it retains over 89.8\% of its average POS tagging accuracy when trained at 1.2\% of the total available training data, i.e.~150 sentences per language.  
 Neural network based sequence-to-sequence models in an encoder-decoder framework have been successfully applied to solve Question Answering  problems, predicting answers from statements and questions. However, almost all previous models have failed to consider detailed context information and unknown states under which systems  do not have enough information to answer given questions. These scenarios with incomplete or ambiguous information are very common in the setting of Interactive Question Answering . To address this challenge, we develop a novel model, employing context-dependent word-level attention for more accurate statement representations and question-guided sentence-level attention for better context modeling. We also generate unique IQA datasets to test our model, which will be made publicly available. Employing these attention mechanisms, our model accurately understands when it can output an answer or when it requires generating a supplementary question for additional input depending on different contexts. When available, user's feedback is encoded and directly applied to update sentence-level attention to infer an answer. Extensive experiments on QA and IQA datasets quantitatively demonstrate the effectiveness of our model with significant improvement over state-of-the-art conventional QA models. 
 % ================================================================= Text documents can be described by a number of abstract concepts such as semantic category, writing style, or sentiment. Machine learning  models have been trained to automatically map documents to these abstract concepts, allowing to annotate very large text collections, more than could be processed by a human in a lifetime.  Besides predicting the text's category very accurately, it is also highly desirable to understand { the categorization process takes place.  In this paper, we demonstrate that such understanding can be achieved by tracing the classification decision back to individual words using layer-wise relevance propagation , a recently developed technique for explaining predictions of complex non-linear classifiers. We train two word-based ML models, a convolutional neural network  and a bag-of-words SVM classifier, on a topic categorization task and adapt the LRP method to decompose the predictions of these models onto words. Resulting scores indicate how much individual words contribute to the overall classification decision. This enables one to distill relevant information from text documents without an explicit semantic information extraction step. We further use the word-wise relevance scores for generating novel vector-based document representations which capture semantic information. Based on these document vectors, we introduce a measure of model explanatory power and show that, although the SVM and CNN models perform similarly in terms of classification accuracy, the latter exhibits a higher level of explainability which makes it more comprehensible for humans and potentially more useful for other applications.   
 The pre-dominant approach to language modeling to date is based on recurrent neural networks. Their success on this task is often linked to their ability to capture unbounded context. In this paper we develop a finite context approach through stacked convolutions, which can be more efficient since they allow parallelization over sequential tokens. We propose a novel simplified gating mechanism that outperforms  and investigate the impact of key architectural decisions. The proposed approach achieves state-of-the-art on the WikiText-103 benchmark, even though it features long-term dependencies, as well as competitive results on the Google Billion Words benchmark. Our model reduces the latency to score a sentence by an order of magnitude compared to a recurrent baseline. To our knowledge, this is the first time a non-recurrent approach is competitive with strong recurrent models on these large scale language tasks. 
 While neural networks have been successfully applied to many natural language processing tasks,  they come at the cost of interpretability.  In this paper, we propose a general methodology to analyze and interpret  decisions from a neural model by observing the effects on the model of erasing various parts of the representation, such as  input word-vector dimensions, intermediate hidden units, or input words.  We present several approaches to analyzing the effects of such erasure, from computing its impact on evaluation metrics, to using reinforcement learning to erase the minimum set of input words in  order to flip a neural model's decision.  In a comprehensive analysis of multiple NLP tasks from lexical  to sentence-level  to document level , we show that the proposed methodology not only offers clear explanations about neural model decisions, but also provides a way to conduct error analysis on neural models.  
 Headline generation for spoken content is important since spoken content is difficult to be shown on the screen and browsed by the user. It is a special type of abstractive summarization, for which the summaries are generated word by word from scratch without using any part of the original content. Many deep learning approaches for headline generation from text document have been proposed recently, all requiring huge quantities of training data, which is difficult for spoken document summarization. In this paper, we propose an ASR error modeling approach to learn the underlying structure of ASR error patterns and incorporate this model in an Attentive Recurrent Neural Network  architecture. In this way, the model for abstractive headline generation for spoken content can be learned from abundant text data and the ASR data for some recognizers. Experiments showed very encouraging results and verified that the proposed ASR error model works well even when the input spoken content is recognized by a recognizer very different from the one the model learned from.  
     In this paper we present a novel Neural Network algorithm for conducting semi-supervised learning for sequence labeling tasks arranged in a linguistically motivated hierarchy.     This relationship is exploited to regularise the representations of supervised tasks by backpropagating the error of the unsupervised task through the supervised tasks.     We introduce a neural network where lower layers are supervised by downstream tasks and the final layer task is an auxiliary unsupervised task.     The architecture shows improvements of up to two percentage points $F_{\beta=1}$ for Chunking compared to a plausible baseline. 
 		Along with the prosperity of recurrent neural network in modelling sequential data and the power of attention mechanism in automatically identify salient information, image captioning, a.k.a., image description, has been remarkably advanced in recent years. Nonetheless, most existing paradigms may suffer from the deficiency of invariance to images with different scaling, rotation, etc.; and effective integration of standalone attention to form a holistic end-to-end system. In this paper, we propose a novel image captioning architecture, termed Recurrent Image Captioner , which allows visual encoder and language decoder to coherently cooperate in a recurrent manner. Specifically, we first equip CNN-based visual encoder with a differentiable layer to enable spatially invariant transformation of visual signals. Moreover, we deploy an attention filter module  between encoder and decoder to dynamically determine salient visual parts. We also employ bidirectional LSTM to preprocess sentences for generating better textual representations. Besides, we propose to exploit variational inference to optimize the whole architecture. Extensive experimental results on three benchmark datasets  demonstrate the superiority of our proposed architecture as compared to most of the state-of-the-art methods. 	
 We introduce an exceptionally simple  gated recurrent neural network   that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task.  We prove that our model has simple, predicable and non-chaotic dynamics.  This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior. %We provide experiments to show that our network performs comparably to chaotic RNNs on word-level language modeling. %  which indicates that chaos in and of itself is not a necessary ingredient for learning in this setting. 
 %In distant supervised relation extraction, the connection between relations of one entity tuple, which we call , is common. Connections between relations in relation extraction, which we call , are common. In distantly supervised  scenario, one entity tuple may have multiple relation facts.  Exploiting class ties between relations of one entity tuple will be promising for distantly supervised relation extraction. However, previous models are not effective or ignore to model this property. In this work, to effectively leverage class ties, we propose to make joint relation extraction with a unified model that integrates convolutional neural network  with a general pairwise ranking framework, in which three novel ranking loss functions are introduced. Additionally, an effective method is presented to relieve the severe class imbalance problem from NR  for model training. %Besides, an effective method is proposed to relieve the impact of relation NR  for model training and test.e %Experimental results on a widely used dataset show that:  Our model is much more superior than the baselines, achieving state-of-the-art performance;  Leveraging class ties by joint extraction will enhance relation extraction. %;  Relieving the impact of NR  for model training will significantly boost our model performance. % Our model can primely deal with wrong labeling problem. %Experiments on a widely used dataset show that leveraging class ties will enhance extraction and demonstrate that our model is effective to learn class ties. Our model outperforms baselines significantly, achieving state-of-the-art performance.  Experiments on a widely used dataset show that leveraging class ties will enhance extraction and demonstrate the effectiveness of our model to learn class ties. Our model outperforms the baselines significantly, achieving state-of-the-art performance. 
  We present novel method for image-text multi-modal representation learning. In our knowledge, this work is the first approach of applying adversarial learning concept to multi-modal learning and not exploiting image-text pair information to learn multi-modal feature. We only use category information in contrast with most previous methods using image-text pair information for multi-modal embedding.  In this paper, we show that multi-modal feature can be achieved without image-text pair information and our method makes more similar distribution with image and text in multi-modal feature space than other methods which use image-text pair information. And we show our multi-modal feature has universal semantic information, even though it was trained for category prediction. Our model is end-to-end backpropagation, intuitive and easily extended to other multi-modal learning work. 
  This paper tackles the reduction of redundant repeating generation that is often observed in RNN-based encoder-decoder models.  Our basic idea is to jointly estimate the upper-bound frequency of each target vocabulary in the encoder and control the output words based on the estimation in the decoder.  Our method shows significant improvement over a strong RNN-based encoder-decoder baseline and achieved its best results on an abstractive summarization benchmark.  \footnote{This is a draft version of EACL-2017} 
 In this paper, we propose a probabilistic parsing model that defines a proper conditional probability distribution over non-projective dependency trees for a given sentence,  using neural representations as inputs. The neural network architecture is based on bi-directional  LSTM-CNNs, which automatically benefits from both word- and character-level representations,  by using a combination of bidirectional LSTMs and CNNs. On top of the neural network, we introduce a  probabilistic structured layer, defining a conditional log-linear model over non-projective trees. By exploiting Kirchhoff's Matrix-Tree Theorem~, the partition functions and  marginals can be computed efficiently, leading to a straight-forward end-to-end model training procedure  via back-propagation. We evaluate our model on 17 different datasets, across 14 different languages.  Our parser achieves state-of-the-art parsing performance on nine datasets. 
 	We introduce a tree-structured attention neural network for sentences and small phrases and apply it to the problem of sentiment classification. Our model expands the current recursive models by incorporating structural information around a node of a syntactic tree using both  bottom-up and top-down information propagation. Also, the model utilizes structural attention to identify the most salient representations during the construction of the syntactic tree. To our knowledge, the proposed models achieve state of the art performance on the Stanford Sentiment Treebank dataset.  
 In this paper, we focus on the personalized response generation for conversational systems. Based on the sequence to sequence learning, especially the encoder-decoder framework, we propose a two-phase approach, namely initialization then adaptation, to model the responding style of human and then generate personalized responses. For evaluation, we propose a novel human aided method to evaluate the performance of the personalized response generation models by online real-time conversation and offline human judgement. Moreover, the lexical divergence of the responses generated by the 5 personalized models indicates that the proposed two-phase approach achieves good results on modeling the responding style of human and generating personalized responses for the conversational systems. 
 Multi-task learning  involves the simultaneous training of two or more related tasks over shared representations. In this work, we apply MTL to audio-visual automatic speech recognition.  Our primary task is to learn a mapping between audio-visual fused features and frame labels obtained from acoustic GMM/HMM model. This is combined with an auxiliary task  which maps visual features to frame labels obtained from a separate visual GMM/HMM model. The MTL model is tested at various levels of babble noise and the results are compared with a base-line hybrid DNN-HMM AV-ASR model. Our results indicate that MTL is especially useful at higher level of noise. Compared to base-line, upto 7\% relative improvement in WER is reported at -3 SNR dB\footnote{\label{version} September 12, 2016 version, submitted to ICASSP 2017}.  
 We introduce a simple and accurate neural model for dependency-based semantic role labeling. Our model predicts predicate-argument dependencies relying on states of a bidirectional LSTM encoder.  The semantic role labeler achieves competitive performance on  English, even without any kind of syntactic information and only using local inference.  However, when automatically predicted part-of-speech tags are provided as input, it substantially outperforms all previous local models and approaches the best reported results on the English CoNLL-2009 dataset.  We also consider Chinese, Czech and Spanish where our approach also achieves competitive results. Syntactic parsers are unreliable on out-of-domain data, so standard  SRL models are hindered when tested in this setting.  Our syntax-agnostic model appears more robust, resulting in the best reported results on standard out-of-domain test sets. 
     Convolutional Neural Networks  are effective models for reducing spectral     variations and modeling spectral correlations in acoustic features     for automatic speech recognition . Hybrid speech recognition systems     incorporating CNNs with Hidden Markov Models/Gaussian Mixture Models      have achieved the state-of-the-art in various benchmarks. Meanwhile,     Connectionist Temporal     Classification  with Recurrent Neural Networks , which is proposed     for labeling unsegmented sequences, makes it feasible to train an `end-to-end'     speech recognition system instead of hybrid settings. However, RNNs are      computationally expensive and sometimes difficult to train. In this paper, inspired by     the advantages of both CNNs and the CTC approach, we propose an end-to-end speech     framework for sequence labeling, by combining hierarchical CNNs with CTC     directly without recurrent connections. By evaluating the approach on the TIMIT     phoneme recognition task, we show that     the proposed model is not only computationally efficient, but also     competitive with the existing baseline systems. Moreover, we argue that CNNs     have the capability to model temporal correlations with appropriate context     information.   
    We describe an open-source toolkit for neural machine translation   .  The toolkit prioritizes efficiency, modularity, and   extensibility with the goal of supporting NMT research into model   architectures, feature representations, and source modalities, while   maintaining competitive performance and reasonable training   requirements. The toolkit consists of modeling and translation support,   as well as detailed pedagogical documentation about the underlying   techniques.  
 We aim to shed light on the strengths and weaknesses of the newly introduced neural machine translation paradigm. To that end, we conduct a multifaceted evaluation in which we compare outputs produced by state-of-the-art neural machine translation and phrase-based machine translation systems for 9 language directions across a number of dimensions. Specifically, we measure the similarity of the outputs, their fluency and amount of reordering, the effect of sentence length and performance across different error categories.  We find out that translations produced by neural machine translation systems are  considerably different, more fluent and more accurate in terms of word order  compared to those produced by phrase-based systems. Neural machine translation systems are also more accurate at producing inflected forms, but they perform poorly when translating very long sentences. 
   Distinguishing between antonyms and synonyms is a key task to   achieve high performance in NLP systems. While they are notoriously   difficult to distinguish by distributional co-occurrence models,   pattern-based methods have proven effective to differentiate between   the relations. In this paper, we present a novel neural network   model AntSynNET that exploits lexico-syntactic patterns   from syntactic parse trees. In addition to the lexical and syntactic   information, we successfully integrate the distance between the   related words along the syntactic path as a new pattern feature. The   results from classification experiments show that AntSynNET improves    the performance over prior pattern-based   methods. 
 In this paper, we propose a novel domain adaptation method named ``{ and multi domain NMT. We first train an NMT model on an out-of-domain parallel corpus, and then fine tune it on a parallel corpus which is a mix of the in-domain and out-of-domain corpora. All corpora are augmented with artificial tags to indicate specific domains. We empirically compare our proposed method against fine tuning and multi domain methods and discuss its benefits and shortcomings. 
 In this paper, we propose an efficient transfer leaning methods for training a personalized language model using a recurrent neural network with long short-term memory architecture. With our proposed fast transfer learning schemes, a general language model is updated to a personalized language model with a small amount of user data and a limited computing resource. These methods are especially useful for a mobile device environment while the data is prevented from transferring out of the device for privacy purposes. Through experiments on dialogue data in a drama, it is verified that our transfer learning methods have successfully generated the personalized language model, whose output is more similar to the personal language style in both qualitative and quantitative aspects. 
 This paper is focused on automatic multi-label document classification of Czech text documents. The current approaches usually use some pre-processing which can have negative impact . Therefore, we would like to omit it and use deep neural networks that learn from simple features.  This choice was motivated by their successful usage in many other machine learning fields. Two different networks are compared: the first one is a~standard multi-layer perceptron, while the second one is a~popular convolutional network. The experiments on a~Czech newspaper corpus show that both networks significantly outperform baseline method which uses a rich set of features with maximum entropy classifier.  We have also shown that convolutional network gives the best results.   
 Many natural language understanding  tasks, such  as shallow parsing  and semantic slot  filling, require the assignment of representative labels to the  meaningful chunks in a sentence. Most of the current deep  neural network  based methods consider these tasks as a sequence labeling problem, in which a word, rather than a chunk,  is treated as the basic unit for labeling. These chunks are then inferred  by the standard IOB  labels.  In this paper, we propose an alternative approach  by investigating the use of DNN for sequence chunking,  and propose three neural models  so that each chunk can be treated as a complete unit for labeling. Experimental results show that the proposed neural sequence chunking models can achieve start-of-the-art performance on both the text chunking and  slot filling tasks.   
 In this work, we propose contextual language models that incorporate dialog level discourse information into language modeling. Previous works on contextual language model treat preceding utterances as a sequence of inputs, without considering dialog interactions. We design recurrent neural network  based contextual language models that specially track the interactions between speakers in a dialog. Experiment results on Switchboard Dialog Act Corpus show that the proposed model outperforms conventional single turn based RNN language model by 3.3\% on perplexity. The proposed models also demonstrate advantageous performance over other competitive contextual language models.     
 The fifth Dialog State Tracking Challenge  introduces a new cross-language dialog state tracking scenario, where the participants are asked to build their trackers based on the English training corpus, while evaluating them with the unlabeled Chinese corpus. Although the computer-generated translations for both English and Chinese corpus are provided in the dataset, these translations contain errors and careless use of them can easily hurt the performance of the built trackers. To address this problem, we propose a multichannel Convolutional Neural Networks  architecture, in which we treat English and Chinese language as different input channels of one single CNN model. In the evaluation of DSTC5, we found that such multichannel architecture can effectively improve the robustness against translation errors. Additionally, our method for DSTC5 is purely machine learning based and requires no prior knowledge about the target language. We consider this a desirable property for building a tracker in the cross-language context, as not every developer will be familiar with both languages. 
  We introduce multi-modal, attention-based neural machine translation  models  which incorporate visual features into different parts of both the encoder and the decoder.  We utilise global image features extracted using a pre-trained convolutional neural network and incorporate them  as words in the source sentence,  to initialise the encoder hidden state, and  as additional data to initialise the decoder hidden state.  %To that end, we use the Multi30k data set,  %recently released as part of the WMT 2016  %shared task on multimodal machine translation.  In our experiments, we evaluate how these different strategies to incorporate global image features compare and which ones perform best.  We also study the impact that adding synthetic multi-modal, multilingual data brings and find that the additional data have a positive impact on multi-modal models.  %Our proposed models learn how to exploit visual features and improve translations by a large margin.  We report new state-of-the-art results and our best models also significantly improve on a comparable phrase-based Statistical MT  model trained on the Multi30k data set according to all metrics evaluated.  To the best of our knowledge, it is the first time a purely neural model significantly improves over a PBSMT model on all metrics evaluated on this data set. 
 In this paper, drawing intuition from the Turing test, we propose using adversarial training for open-domain dialogue generation: the system is trained to produce sequences that are  indistinguishable from human-generated dialogue utterances. We cast the task as a reinforcement learning  problem where we jointly train two systems, a generative model to produce response sequences, and a discriminator---analagous to the human evaluator in the Turing test--- to distinguish between   the   human-generated dialogues and the machine-generated ones. The outputs from the discriminator are   then  used as rewards for the generative model, pushing the system to generate dialogues that mostly resemble human dialogues.  In addition to adversarial training we describe a model for adversarial evaluation that  uses success in fooling an adversary as a dialogue evaluation metric, while avoiding a number of potential pitfalls. Experimental results on several metrics, including adversarial evaluation, demonstrate that the adversarially-trained system generates higher-quality responses than previous baselines.  
 We introduce a simple, general strategy to manipulate the behavior of a neural decoder that enables it to generate outputs that have specific properties of interest . The model can be thought of as a simple version of the actor-critic model that uses an interpolation of the actor  and the critic  for decision making.  We demonstrate that  the approach is  able to incorporate a variety of properties that cannot be handled by  standard neural sequence decoders, such as sequence length and backward probability , in addition to yielding consistent improvements in abstractive summarization and machine translation when the property to be optimized is BLEU or ROUGE scores.  
 		We study multi-turn response generation in chatbots where a response is generated according to a conversation context.   	Existing work has modeled the hierarchy of the context, but does not pay enough attention to the fact that words and utterances in the context are differentially important. As a result, they may lose important information in context and generate irrelevant responses. We propose a hierarchical recurrent attention network  to model both aspects in a unified framework. In HRAN, a hierarchical attention mechanism attends to important parts within and among utterances with word level attention and utterance level attention respectively. With the word level attention, hidden vectors of a word level encoder are synthesized as utterance vectors and fed to an utterance level encoder to construct hidden representations of the context. The hidden vectors of the context are then processed by the utterance level attention and formed as context vectors for decoding the response.  Empirical studies on both automatic evaluation and human judgment show that HRAN can significantly outperform state-of-the-art models for multi-turn response generation. 	
 Given a collection of images and spoken audio captions, we present a method for discovering word-like acoustic units in the continuous speech signal and grounding them to semantically relevant image regions. For example, our model is able to detect spoken instances of the words ``lighthouse'' within an utterance and associate them with image regions containing lighthouses. We do not use any form of conventional automatic speech recognition, nor do we use any text transcriptions or conventional linguistic annotations. Our model effectively implements a form of spoken language acquisition, in which the computer learns not only to recognize word categories by sound, but also to enrich the words it learns with semantics by grounding them in images. 
 In this paper the task of emotion recognition from speech is considered. Proposed approach uses deep recurrent neural network trained on a sequence of acoustic features calculated over small speech intervals. At the same time special probabilistic-nature CTC loss function allows to consider long utterances containing both emotional and neutral parts. The effectiveness of such an approach is shown in two ways. Firstly, the comparison with recent advances in this field is carried out. Secondly, human performance on the same task is measured. Both criteria show the high quality of the proposed method. 
 Simultaneous administration of multiple drugs can have synergistic or antagonistic effects as one drug can affect activities of other drugs. Synergistic effects lead to improved therapeutic outcomes, whereas, antagonistic effects can be life-threatening, may lead to increased healthcare cost, or may even cause death. Thus identification of unknown drug-drug interaction  is an important concern for efficient and effective healthcare. Although multiple resources for DDI exist, they are often unable to keep pace with rich amount of information available in fast growing biomedical texts. Most existing methods model DDI extraction from text as a classification problem and mainly rely on handcrafted features. Some of these features further depend on domain specific tools. Recently neural network models using latent features have been shown to give similar or better performance than the other existing models dependent on handcrafted features. In this paper, we present three models namely, { and { and { model outperforms all the existing methods, including those relying on handcrafted features. The other two proposed LSTM models also perform competitively with state-of-the-art methods. 
 In Chinese societies, superstition is of paramount importance, and vehicle license plates with desirable numbers can fetch very high prices in auctions.  Unlike other valuable items, license plates are not allocated an estimated price before auction.  I propose that the task of predicting plate prices can be viewed as a natural language processing  task,  as the value depends on the meaning of each individual character on the plate and its semantics. I construct a deep recurrent neural network  to predict the prices of vehicle license plates in Hong Kong, based on the characters on a plate.  I demonstrate the importance of having a deep network and of retraining. Evaluated on 13 years of historical auction prices, the deep RNN's predictions can explain over 80 percent of price variations, outperforming previous models by a significant margin. I also demonstrate how the model can be extended to become  a search engine for plates and to provide estimates of the expected price distribution. 
   Short text clustering is a challenging problem due to its sparseness of text representation. Here we propose a flexible Self-Taught Convolutional neural network framework for Short Text Clustering , which can flexibly and successfully incorporate more useful semantic features and learn non-biased deep text representation in an unsupervised manner. In our framework, the original raw text features are firstly embedded into compact binary codes by using one existing unsupervised dimensionality reduction methods. Then, word embeddings are explored and fed into convolutional neural networks to learn deep feature representations, meanwhile the output units are used to fit the pre-trained binary codes in the training process. Finally, we get the optimal clusters by employing K-means to cluster the learned representations. Extensive experimental results demonstrate that the proposed framework is effective, flexible and outperform several popular clustering methods when tested on three public short text datasets. 
   The capacity of a neural network to absorb information is limited by its number of parameters.  Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation.  In practice, however, there are significant algorithmic and performance challenges.  In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters.  We introduce a Sparsely-Gated Mixture-of-Experts layer , consisting of up to thousands of feed-forward sub-networks.  A trainable gating network determines a sparse combination of these experts to use for each example.  We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora.  We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers.  On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.    
 The application of Deep Neural Networks for ranking in search engines may obviate the need for the extensive feature engineering common to current learning-to-rank methods. However,  we show that combining simple relevance matching features like BM25 with existing Deep Neural Net models often substantially improves the accuracy of these models, indicating that they do not capture essential local relevance matching signals. We describe a novel deep Recurrent Neural Net-based model that we call Match-Tensor. The architecture of the Match-Tensor model simultaneously accounts for both local relevance matching and global topicality signals allowing for a rich interplay between them when computing the relevance of a document to a query.  On a large held-out test set consisting of social media documents, we demonstrate not only that Match-Tensor outperforms BM25 and other classes of DNNs but also that it largely subsumes signals present in these models. 
  %%% Leave the Abstract empty if your article does not require one, please see the Summary Table for full details.  Natural language is inherently a discrete symbolic representation of human knowledge.  Recent advances in machine learning  and in natural language processing  seem to contradict the above intuition: discrete symbols are fading away, erased by vectors or tensors called  and .  However, there is a strict link between distributed/distributional representations and discrete symbols, being the first an approximation of the second.   A clearer understanding of the strict link between distributed/distributional representations and symbols may certainly lead to radically new deep learning networks.  In this paper we make a survey that aims to renew the link between symbolic representations and distributed/distributional representations. This is the right time to revitalize the area of interpreting how discrete symbols are represented inside neural networks.   \tiny   %All article types: you may provide up to 8 keywords; at least 5 are mandatory. 
   Attention networks have proven to be an effective approach for   embedding categorical inference within a deep neural network.   However, for many tasks we may want to model richer structural   dependencies without abandoning end-to-end training. In this work,   we experiment with incorporating richer structural distributions,   encoded using graphical models, within deep networks. We   show that these structured attention networks are simple extensions   of the basic attention procedure, and that they allow for extending   attention  beyond the standard soft-selection approach, such   as attending to partial segmentations or to subtrees. We experiment   with two different classes of structured attention networks: a   linear-chain conditional random field and a graph-based parsing   model, and describe how these models can be practically implemented   as neural network layers. Experiments show that this approach is   effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic  and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn  interesting unsupervised hidden representations that generalize simple attention.  
    Neural machine translation  models are able to partially learn syntactic information from sequential lexical information. Still, some complex syntactic phenomena such as prepositional phrase attachment are poorly modeled.  This work aims to answer two questions: 1) Does explicitly modeling target language syntax help NMT? 2) Is tight integration of words and syntax better than multitask training? We introduce syntactic information in the form of CCG supertags in the decoder, by interleaving the target supertags with the word sequence. Our results on WMT data show that explicitly modeling target-syntax improves machine translation quality for German$\rightarrow$English, a high-resource pair, and for Romanian$\rightarrow$English, a low-resource pair and also several syntactic phenomena including prepositional phrase attachment. Furthermore, a tight coupling of words and syntax improves translation quality more than multitask training. By combining target-syntax with adding source-side dependency labels in the embedding layer, we obtain a total improvement of  0.9 \bleu for German$\rightarrow$English and 1.2 \bleu for Romanian$\rightarrow$English. 
  We introduce a Multi-modal Neural Machine Translation model in which a doubly-attentive decoder naturally incorporates  visual features obtained using pre-trained convolutional neural networks, bridging the gap between image description and translation.  Our decoder learns to attend to source-language words and parts of an image independently by means of two  as it generates words in the target language.  We find that our model can efficiently exploit not just back-translated in-domain multi-modal data but also large general-domain text-only MT corpora.  We also report state-of-the-art results on the Multi30k data set. 
   We present opinion recommendation, a novel task of jointly predicting a custom review with a rating score that a certain user would give to a certain product or service, given existing reviews and rating scores to the product or service by other users, and the reviews that the user has given to other products and services.   A characteristic of opinion recommendation is the reliance of multiple data sources for multi-task joint learning, which is the strength of neural models. We use a single neural network to model users and products, capturing their correlation and generating customised product representations using a deep memory network, from which customised ratings and reviews are constructed jointly. Results show that our opinion recommendation system gives ratings that are closer to real user ratings on Yelp.com data compared with Yelp's own ratings, and our methods give better results compared to several pipelines baselines using state-of-the-art sentiment rating and summarization systems. 
   A fundamental challenge in developing semantic parsers is the paucity of strong   supervision in the form of language utterances annotated with logical   form. In this paper, we propose to exploit structural regularities in language   in   different domains, and train semantic parsers over multiple knowledge-bases   , while sharing information   across datasets. We find that we can substantially improve parsing   accuracy by training a single sequence-to-sequence model over multiple KBs,   when providing an encoding of the domain at decoding time. Our model   achieves state-of-the-art performance on the Overnight dataset   , improves performance over a single KB baseline   from 75.6\% to 79.6\%, while obtaining a 7x reduction in the number of model   parameters. 
 In aspect-based sentiment analysis, most existing methods focus on either identifying aspect/opinion terms or categorizing pre-extracted aspect terms. Each task by itself only provides partial information to end users. To generate more detailed and structured opinion analysis, we study a finer-grained problem, which we call category-specific aspect and opinion terms extraction. This problem involves identification of aspect and opinion terms within each sentence, as well as categorization of the identified terms at the same time. To this end, we propose an end-to-end multi-task memory network, where aspect/opinion terms extraction for a specific category is considered as a task, and all the tasks are learned jointly by exploring commonalities and relationships among them. We demonstrate state-of-the-art performance of our proposed model on three benchmark datasets. 
 Knowledge distillation describes a method for training a student network to  perform better by learning from a stronger teacher network. Translating a sentence with an Neural Machine Translation  engine is time expensive and having a smaller model speeds up this process. We demonstrate how to transfer the translation quality of an ensemble and an oracle \BLEU teacher network into a single NMT system. Further, we present translation improvements  from a teacher network that has the same architecture and dimensions of the student network. As the training of the student model is still expensive, we introduce a data filtering method based on the knowledge of the teacher model that not only speeds up the training, but also leads to better translation quality. Our techniques need no code change and can be easily reproduced with any NMT architecture to speed up the decoding process.  
 The basic concept in Neural Machine Translation  is to train a large Neural Network that maximizes the translation performance on a given parallel corpus. NMT is then using a simple left-to-right beam-search decoder to generate new translations that approximately maximize the trained conditional probability. The current beam search strategy generates the target sentence word by word from left-to-right while keeping a fixed amount of active candidates at each time step.  First, this simple search is less adaptive as it also expands candidates whose scores are much worse than the current best. Secondly, it does not expand hypotheses if they are not within the best scoring candidates, even if their scores are close to the best one. The latter one can be avoided by increasing the beam size until no performance improvement can be observed.  While you can reach better performance, this has the drawback of a slower decoding speed. In this paper, we concentrate on speeding up the decoder by applying a more flexible beam search strategy whose candidate size may vary at each time step depending on the candidate scores. We speed up the original decoder by up to 43\% for the two language pairs German$\to$English and Chinese$\to$English without losing any translation quality. 
 Neural network models are capable of generating extremely natural sounding conversational interactions.  However, these models have been mostly applied to casual scenarios  and have yet to demonstrate they can serve in more useful conversational applications.  This paper presents a novel, fully data-driven, and knowledge-grounded neural conversation model aimed at producing more contentful responses.  We generalize the widely-used Sequence-to-Sequence  approach by conditioning responses on both conversation history and external ``facts'', allowing the model to be versatile and applicable in an open-domain setting.  Our approach yields significant improvements over a competitive \sts baseline. Human judges found that our outputs are significantly more informative. 
   We show that the task of question answering  can significantly benefit from the transfer learning of models trained on a different large, fine-grained QA dataset.   We achieve the state of the art in two well-studied QA datasets, WikiQA and SemEval-2016 , through a basic transfer learning technique from SQuAD.   For WikiQA, our model outperforms the previous best model by more than 8\%.   We demonstrate that finer supervision provides better guidance for learning lexical and syntactic information than coarser supervision, through quantitative results and visual analysis.   We also show that a similar transfer learning procedure  achieves  the state of the art on an entailment task.         % Transfer learning is often believed to perform better when the tasks of the source and the target datasets are more similar.   % We present a counter example to this premise in context-based question answering  domain , where a more dissimilar source task could be more effective for transfer learning.   % We specifically show that span-supervised QA provides a better guidance for learning lexical and syntactic representations than multiple-choice QA.    % This suggests that pretraining with span-supervised QA dataset can be more beneficial than with multiple-choice QA dataset even when the target task is multiple-choice QA.   % We carefully design our experiment to support this claim, and furthermore we obtain new state-of-the-art results on WikiQA and SemEval 2016 with transfer learning from SQuAD.   % Our analysis also leads to an ironic implication that it could be more effective to focus on span-supervised QA models to improve one's performance on multiple-choice QA, or even other natural language tasks.   % singularity, last sentence could be removed from abstract      % Recently, numerous question answering datasets with different ways of supervising and evaluating the answers have become available in natural language processing community.   % We examine and compare the two popular metrics for supervising and evaluating context-based question answering  tasks: span selection within the context and multiple-choice.    % In particular, we show that models supervised with answer span are more capable of learning the lexical and syntactic information of and the interaction between natural language sentences.   % We also present possible difficulties of training models supervised by multiple-choice answers.   % Our analysis suggests   % a general direction for and the benefits of transfer learning from large QA datasets such as SQuAD and NewsQA to other datasets, as we obtain the state-of-the-art results in WikiQA and SemEval 2016 with minimal effort.   % We demonstrate our claims empirically and through numerical results on the several datasets, and qualitatively with the visual analysis of the latent representations of and the interactions between sentences. 
 This paper presents a novel neural machine translation model which jointly learns translation and source-side latent graph representations of sentences. Unlike existing pipelined approaches using syntactic parsers, our end-to-end model learns a latent graph parser as part of the encoder of an attention-based neural machine translation model, and thus the parser is optimized according to the translation objective. In experiments, we first show that our model compares favorably with state-of-the-art sequential and pipelined syntax-based NMT models. We also show that the performance of our model can be further improved by pre-training it with a small amount of treebank annotations. Our final ensemble model significantly outperforms the previous best models on the standard English-to-Japanese translation dataset. 
 People have information needs of varying complexity, which can be solved by an intelligent agent able to answer questions formulated in a proper way, eventually considering user context and preferences. In a scenario in which the user profile can be considered as a question, intelligent agents able to answer questions can be used to find the most relevant answers for a given user. In this work we propose a novel model based on Artificial Neural Networks to answer questions with multiple answers by exploiting multiple facts retrieved from a knowledge base. The model is evaluated on the factoid Question Answering and top-n recommendation tasks of the bAbI Movie Dialog dataset. After assessing the performance of the model on both tasks, we try to define the long-term goal of a conversational recommender system able to interact using natural language and to support users in their information seeking processes in a personalized way. 
  Recent research in neural machine translation has largely focused on two aspects; neural network architectures and end-to-end learning algorithms. The problem of decoding, however, has received relatively little attention from the research community. In this paper, we solely focus on the problem of decoding given a trained neural machine translation model. Instead of trying to build a new decoding algorithm for any specific decoding objective, we propose the idea of {   
 Although deep learning models have proven effective at solving problems in natural language processing, the mechanism by which they come to their conclusions is often unclear.   As a result, these models are generally treated as black boxes, yielding no insight of the underlying learned patterns.  In this paper we consider Long Short Term Memory networks  and demonstrate a new approach for tracking the importance of a given input to the LSTM for a given output. By identifying consistently important patterns of words, we are able to distill state of the art LSTMs on sentiment analysis and question answering into a set of representative phrases. This representation is then quantitatively validated by using the extracted phrases to construct a simple, rule-based classifier which approximates the output of the LSTM. 
 %Public speakings play important roles in schools and work places and properly using humor contributes to effective presentations. For the purpose of automatically evaluating speakers' humor usage, we build a presentation corpus containing humorous utterances based on TED talks. Compared to previous data resources supporting humor recognition research, ours has several advantages, including  both positive and negative instances coming from a homogeneous data set,  containing a large number of speakers, and  being open. Focusing on using lexical cues for humor recognition, we systematically compare a newly emerging text classification method based on Convolutional Neural Networks  with a well-established conventional method using linguistic knowledge. The advantages of the CNN method are both getting higher detection accuracies and being able to learn essential features automatically.  
  Connecting different text attributes associated with the same entity  is important in business data analytics since it could help merge two different tables in a database to provide a more comprehensive profile of an entity. However, the conflation task is challenging because two text strings that describe the same entity could be quite different from each other for reasons such as misspelling. It is therefore critical to develop a conflation model that is able to truly understand the semantic meaning of the strings and match them at the semantic level. To this end, we develop a character-level deep conflation model that encodes the input text strings from character level into finite dimension feature vectors, which are then used to compute the cosine similarity between the text strings. The model is trained in an end-to-end manner using back propagation and stochastic gradient descent to maximize the likelihood of the correct association. Specifically, we propose two variants of the deep conflation model, based on long-short-term memory  recurrent neural network  and convolutional neural network , respectively. Both models perform well on a real-world business analytics dataset and significantly outperform the baseline bag-of-character  model.  
 	%Explicit concept space models have proven efficacy for text representation in many natural language and text mining applications. The idea is to embed textual structures into a semantic space of concepts which captures the main topics of these structures. That so called bag-of-concepts representation suffer from sparsity causing low similarity scores between similar texts due to low concept overlap. In this paper we propose applying neural network embedding models to learn dense "genuine" concept representations. We propose three different mechanisms for generating concept concept representations. Our initial qualitative analysis show that the learned embeddings can produce high quality semantically similar/related concepts to a given concept reflecting the soundness of the these representation. 	 	%Empirical results show the efficacy of the learned representations over existing sparse concept space models.  	Explicit concept space models have proven efficacy for text representation in many natural language and text mining applications. The idea is to embed textual structures into a semantic space of concepts which captures the main ideas, objects, and the characteristics of these structures. The so called Bag of Concepts  representation suffers from data sparsity causing low similarity scores between similar texts due to low concept overlap. To address this problem, we propose two neural embedding models to learn continuous concept vectors. Once they are learned, we propose an efficient vector aggregation method to generate fully continuous BoC representations. We evaluate our concept embedding models on three tasks: 1) measuring entity semantic relatedness and ranking where we achieve 1.6\% improvement in correlation scores, 2) dataless concept categorization where we achieve state-of-the-art performance and reduce the categorization error rate by more than 5\% compared to five prior word and entity embedding models, and 3) dataless document classification where our models outperform the sparse BoC representations. In addition, by exploiting our efficient linear time vector aggregation method, we achieve better accuracy scores with much less concept dimensions compared to previous BoC densification methods which operate in polynomial time and require hundreds of dimensions in the BoC representation. %  
 There has been relatively little attention to incorporating linguistic prior to neural machine translation. Much of the previous work was further constrained to considering linguistic prior on the source side. In this paper, we propose a hybrid model, called NMT+RNNG, that learns to parse and translate by combining the recurrent neural network grammar into the attention-based neural machine translation. Our approach encourages the neural machine translation model to incorporate linguistic prior during training, and lets it translate on its own afterward. Extensive experiments with four language pairs show the effectiveness of the proposed NMT+RNNG. 
  Agglutinative languages such as Turkish, Finnish and Hungarian require morphological disambiguation before further processing due to the complex morphology of words. A morphological disambiguator is used to select the correct morphological analysis of a word.  Morphological disambiguation is important because it generally is one of the first steps of natural language processing and its performance affects subsequent analyses.  In this paper, we propose a system that uses deep learning techniques for morphological disambiguation.  Many of the state-of-the-art results in computer vision, speech recognition and natural language processing have been obtained through deep learning models.  However, applying deep learning techniques to morphologically rich languages is not well studied.  In this work, while we focus on Turkish morphological disambiguation we also present results for French and German in order to show that the proposed architecture achieves high accuracy with no language-specific feature engineering or additional resource.  In the experiments, we achieve \accuracyturkish, \accuracygerman and \accuracyfrench morphological disambiguation accuracy among the ambiguous words for Turkish, German and French respectively.  
  %This paper describes a single convolutional neural network architecture that,   .... % websites allow  users to ask questions about a given topic to a community of experts and receive an answer from other users. Despite their popularity, these websites suffer from the fact that many answers given by the users are not relevant to the input question. Thus, systems able to select good answers to new questions posed on cQA websites are highly desirable.  In this paper, we developed a deep neural network  that learns to solve simultaneously the three tasks of the cQA challenge proposed by the SemEval-2016 Task 3, i.e., question-comment similarity, question-question similarity and new question-comment similarity. The latter is the main  task, which can exploit the previous two for achieving better results. Our DNN is trained jointly on all the three cQA tasks and learns to encode questions and comments into a single vector representation shared across the multiple tasks. The results on the official challenge test set show that our approach produces higher accuracy and faster convergence rates than the individual neural networks. Additionally, our method, which does not use any manual feature engineering, approaches the state of the art established with methods that make heavy use of it.  %This document contains the instructions for preparing a camera-ready  % manuscript for the proceedings of ACL-2016. The document itself   %conforms to its own specifications, and is therefore an example of   %what your manuscript should look like. These instructions should be  
  Previous studies support the idea of merging auditory-based Gabor features with deep learning architectures to achieve robust automatic speech recognition, however, the cause behind the gain of such combination is still unknown. We believe these representations provide the deep learning decoder with more discriminable cues. Our aim with this paper is to validate this hypothesis by performing experiments with three different recognition tasks  and assess the discriminability of the information encoded by Gabor filterbank features. Additionally, to identify the contribution of low, medium and high temporal modulation frequencies subsets of the Gabor filterbank were used as features . With temporal modulation frequencies between $16$ and $25$\,Hz, HTM consistently outperformed the remaining ones in every condition, highlighting the robustness of these representations against channel distortions, low signal-to-noise ratios and acoustically challenging real-life scenarios with relative improvements from $11$ to $56$\% against a Mel-filterbank-DNN baseline. To explain the results, a measure of similarity between phoneme classes from DNN activations is proposed and linked to their acoustic properties. We find this measure to be consistent with the observed error rates and highlight specific differences on phoneme level to pinpoint the benefit of the proposed features.   
 Recent studies have shown effectiveness in using neural networks for Chinese word segmentation. However, these models rely on large-scale data and are less effective for low-resource datasets because of insufficient training data. We propose a transfer learning method to improve low-resource word segmentation by leveraging high-resource corpora. First, we train a teacher model on high-resource corpora and then use the  learned knowledge to initialize a student model. Second, a weighted data similarity method is proposed to train the student model on low-resource data. Experiment results show that our work significantly improves the performance on low-resource datasets: 2.3\% and 1.5\% F-score on PKU and CTB datasets. Furthermore, this paper achieves  state-of-the-art results: 96.1\%, and 96.2\% F-score on PKU and CTB datasets. %Besides, we explore an asynchronous parallel method on neural word segmentation to speed up training. The parallel method accelerates training substantially and is almost five times faster than a serial mode.  
 In machine translation  that involves translating between two languages with significant differences in word order, determining the correct word order of translated words is a major challenge. The dependency parse tree of a source sentence can help to determine the correct word order of the translated words.  In this paper, we present a novel reordering approach utilizing a neural network and dependency-based embeddings to predict whether the translations of two source words linked by a dependency relation should remain in the same order or should be swapped in the translated sentence. Experiments on Chinese-to-English translation show that our approach yields a statistically significant improvement of 0.57 BLEU point on benchmark NIST test sets, compared to our prior state-of-the-art statistical MT system that uses sparse dependency-based reordering features. 
 Neural language models predict the next token using a latent representation of the immediate token history. Recently, various methods for augmenting neural language models with an attention mechanism over a differentiable memory have been proposed. For predicting the next token, these models query information from a memory of the recent history which can facilitate learning mid- and long-range dependencies. However, conventional attention mechanisms used in memory-augmented neural language models produce a single output vector per time step. This vector is used both for predicting the next token  for the key and value of a differentiable memory of a token history.  In this paper, we propose a neural language model with a key-value attention mechanism that outputs separate representations for the key and value of a differentiable memory, as well as for encoding the next-word distribution. This model outperforms existing memory-augmented neural language models on two corpora. Yet, we found that our method mainly utilizes a memory of the five most recent output representations.  This led to the unexpected main finding that a much simpler model based only on the concatenation of recent output representations from previous time steps is on par with more sophisticated memory-augmented neural language models. 
   Interpreting the performance of deep learning models beyond test set accuracy is challenging.   Characteristics of individual data points are often not considered during evaluation, and each data point is treated equally.   We examine the impact of a test set question's difficulty to determine if there is a relationship between difficulty and performance.   We model difficulty using well-studied psychometric methods on human response patterns.   Experiments on Natural Language Inference  and Sentiment Analysis  show that the likelihood of answering a question correctly is impacted by the question's difficulty.    As DNNs are trained with more data, easy examples are learned more quickly than hard examples. 
 Neural attention models have achieved great success in different NLP tasks. However, they have not fulfilled their promise on the AMR parsing task due to  the data sparsity issue. In this paper, we describe a sequence-to-sequence model for AMR parsing and present different ways to tackle the data sparsity problem. We show that our methods achieve significant improvement over a baseline neural attention model and our results are also competitive against state-of-the-art systems that do not use extra linguistic resources. 
  We propose a deep learning model for identifying structure within experiment narratives in scientific literature. We take a sequence labeling approach to this problem, and label clauses within experiment narratives to identify the different parts of the experiment. Our dataset consists of paragraphs taken from open access PubMed papers labeled with rhetorical information as a result of our  pilot annotation. Our model is a Recurrent Neural Network  with Long Short-Term Memory    cells that labels clauses. The clause representations are computed by combining word representations  using a novel attention mechanism that involves a separate RNN.  We compare this model against LSTMs where the input layer has simple or no attention   and a feature rich CRF model. Furthermore, we describe how our work could be useful for information   extraction from scientific literature.  
   Word-vector representations associate a high dimensional real-vector to every word from a corpus. Recently, neural-network based methods have been proposed for learning this representation from large corpora. This type of word-to-vector embedding is able to keep, in the learned vector space, some of the syntactic and semantic relationships present in the original word corpus. This, in turn, serves to address different types of language classification tasks by doing algebraic operations defined on the vectors. The general practice is to assume that the semantic relationships between the words can be  inferred by the application of a-priori specified algebraic operations. Our general goal in this paper is to show that it is possible to learn methods for word composition in semantic spaces. Instead of expressing the compositional method as an algebraic operation, we will encode it as a program, which can be linear, nonlinear, or involve more intricate expressions. More remarkably, this program will be evolved from a set of initial random programs by means of genetic programming .  We show that our method is able to reproduce the same behavior as human-designed algebraic operators. Using a word analogy task as benchmark, we also show that GP-generated programs are able to obtain accuracy values above those produced by the commonly used human-designed rule for algebraic manipulation of word vectors. Finally, we show the robustness of our approach by executing the evolved programs on the word2vec GoogleNews vectors, learned over 3 billion running words, and assessing their accuracy in the same word analogy task.  \\    {}: semantic spaces, compositional methods, word2vec, genetic programming, word vectors 
This work presents a systematic theoretical and empirical comparison of the major algorithms that have been proposed for learning Harmonic and Optimality Theory grammars . By comparing learning algorithms, we are also able to compare the closely related OT and HG frameworks themselves. Experimental results show that the additional expressivity of the HG framework over OT affords performance gains in the task of predicting the surface word order of Czech sentences. We compare the perceptron with the classic Gradual Learning Algorithm , which learns OT grammars, as well as the popular Maximum Entropy model. In addition to showing that the perceptron is theoretically appealing, our work shows that the performance of the HG model it learns approaches that of the upper bound in prediction accuracy on a held out test set  and that it is capable of accurately modeling observed variation. 
   In this paper, we explore a simple solution to ``Multi-Source Neural Machine Translation"  which only relies on preprocessing a N-way multilingual corpus without modifying the Neural Machine Translation  architecture or training procedure. We simply concatenate the source sentences to form a single long multi-source input sentence while keeping the target side sentence as it is and train an NMT system using this preprocessed corpus. We evaluate our method in resource poor as well as resource rich settings and show its effectiveness . We also compare against existing methods for MSNMT and show that our solution gives competitive results despite its simplicity. We also provide some insights on how the NMT system leverages multilingual information in such a scenario by visualizing attention. 
 We investigate the generation of one-sentence Wikipedia biographies from facts derived from Wikidata slot-value pairs. We train a recurrent neural network sequence-to-sequence model with attention to select facts and generate textual summaries. Our model incorporates a novel secondary objective that helps ensure it generates sentences that contain the input facts. The model achieves a \bleu score of 41, improving significantly upon the vanilla sequence-to-sequence model and scoring roughly twice that of a simple template baseline. Human preference evaluation suggests the model is nearly as good as the Wikipedia reference. Manual analysis explores content selection, suggesting the model can trade the ability to infer knowledge against the risk of hallucinating incorrect information. 
    is an important sub-task in argumentation mining.  ACD aims at  detecting and classifying different argument  components in natural language texts.   are important  features the human annotators consider when they manually perform the ACD task. However, HAs are largely ignored by  existing automatic ACD techniques.   has proven to be an effective method for using  HAs in some natural language processing tasks. In this work, we propose a RL-based ACD technique, and evaluate its performance  on two well-annotated corpora. %.  Results suggest that, in terms of classification accuracy, HAs-augmented RL outperforms plain RL by at most 17.85\%, and  outperforms the state-of-the-art supervised learning algorithm  by at most 11.94\%. 
 Segmental conditional random fields  and connectionist temporal classification  are two  sequence labeling methods used for end-to-end training of speech recognition models. Both models define a transcription probability by marginalizing decisions about latent segmentation alternatives to derive a sequence probability: the former uses a globally normalized joint model of segment labels and durations, and the latter classifies each frame as either an output symbol or a ``continuation'' of the previous label.  In this paper, we train a recognition model by optimizing an interpolation between the SCRF and CTC losses, where the same recurrent neural network  encoder is used for feature extraction for both outputs. We find that this multitask objective improves recognition accuracy when decoding with either the SCRF or CTC models. Additionally, we show that CTC can also be used to pretrain the RNN encoder, which improves the convergence rate when learning the joint model.  %Segmental Conditional Random Field  and Connectionist Temporal Classification  have been well studied for acoustic modeling. The former defines the loss function at the sequence level, while the latter defines the loss at the frame level. When combined with neural networks for feature extraction, both models can be trained end-to-end. In this paper, we study the joint training approach of the two models using an interpolated loss functions for speech recognition, where both SCRF and CTC share the same recurrent neural network  encoder used for feature extraction. We show that the joint training approach improves the recognition accuracy of both SCRF and CTC models due to the regularization effect, which is similar to the observation of HMMs where sequence-discriminative training using a sequence-level loss can benefit from the cross entropy  based regularization. Moreover, we show that CTC can also be used to pretrain the RNN encoder, which improves the convergence speed of the joint model.  
 We explore neural network models for answering multi-step reasoning questions that operate on semi-structured tables. Challenges arise from deep logical compositionality and domain openness. Our approach is weakly supervised, trained on question-answer-table triples. It generates human readable logical forms from natural language questions, which are then ranked based on word and character convolutional neural networks. A model ensemble achieved at the moment of publication state-of-the-art score on the WikiTableQuestions dataset. 
 Real-time monitoring and responses to emerging public health threats  rely on the availability of timely surveillance data. During the  early stages of an epidemic, the ready availability of line lists with  detailed tabular information about laboratory-confirmed cases can assist  epidemiologists  in making reliable inferences and forecasts. Such inferences are crucial to understand the epidemiology of a specific disease early enough to stop or control the outbreak. However, construction of such line lists requires considerable human supervision and therefore, difficult to generate in real-time. In this paper, we motivate {\fullmodel}, the first tool for building automated line lists  from open source reports of emerging disease outbreaks. Specifically, we focus on deriving epidemiological characteristics of an  emerging disease and the affected population from reports of illness. {\fullmodel} uses distributed vector representations  to discover a set of indicators for each line list feature. This discovery of indicators is followed by the use of dependency parsing based techniques for final extraction in tabular form. We evaluate the performance of {\fullmodel} against a human annotated line list provided by HealthMap  corresponding to MERS outbreaks in Saudi Arabia. We demonstrate that {\fullmodel}  extracts line list features with increased accuracy compared to a baseline method. We further show how these automatically extracted line list features can be used for making epidemiological inferences, such as inferring demographics and symptoms-to-hospitalization period of affected individuals. 
   Fine-grained entity type classification  is the task of classifying an entity mention to a broad set of types. Distant supervision paradigm is extensively used to generate training data for this task. However, generated training data assigns same set of labels to every mention of an entity without considering its local context. Existing FETC systems have two major drawbacks: assuming training data to be noise free and use of hand crafted features. Our work overcomes both drawbacks. We propose a neural network model that jointly learns entity mentions and their context representation to eliminate use of hand crafted features. Our model treats training data as noisy and uses non-parametric variant of hinge loss function. Experiments show that the proposed model outperforms previous state-of-the-art methods on two publicly available datasets, namely Figer and bbn with an average relative improvement of 2.69\% in micro-F1 score. Knowledge learnt by our model on one dataset can be transferred to other datasets while using same model or other FETC systems. These approaches of transferring knowledge further improve the performance of respective models.         
 While dependency parsers reach very high overall accuracy, some dependency relations are much harder than others. In particular, dependency parsers perform poorly in coordination construction . We extend a state-of-the-art dependency parser with conjunction-specific features, focusing on the similarity between the conjuncts head words. Training the extended parser yields an improvement in conj attachment as well as in overall dependency parsing accuracy on the Stanford dependency conversion of the Penn TreeBank. 
   This document contains the instructions for preparing a camera-ready   manuscript for the proceedings of COLING-2016. The document itself   conforms to its own specifications, and is therefore an example of   what your manuscript should look like. These instructions should be   used for both papers submitted for review and for final versions of   accepted papers.  Authors are asked to conform to all the directions   reported in this document. 
 This paper focuses on unsupervised modeling of morphological families, collectively comprising a forest over the language vocabulary. This formulation enables us to capture edge-wise properties reflecting single-step morphological derivations, along with global distributional properties of the entire forest. These global properties constrain the size of the affix set and encourage formation of tight morphological families. The resulting  objective is solved using Integer Linear Programming  paired with contrastive estimation. We train the model by alternating between optimizing the local log-linear model and the global ILP objective. We evaluate our system on three tasks: root detection, clustering of morphological families and segmentation. Our experiments demonstrate that our model yields consistent gains in all three tasks compared with the best published results.\footnote{Code is available at \url{https://github.com/j-luo93/MorphForest}.} 
 Medical errors are leading causes of death in the US and as such, prevention of these errors is paramount to promoting healthcare. Patient Safety Event reports are narratives describing potential adverse events to the patients and are important in identifying, and preventing medical errors. We present a neural network architecture for identifying the type of safety events which is the first step in understanding these narratives. Our proposed model is based on a soft neural attention model to improve the effectiveness of encoding long sequences. Empirical results on two large-scale real-world datasets of patient safety reports demonstrate the effectiveness of our method with significant improvements over existing methods.  
 Topic models have been widely used in discovering latent topics which are shared across documents in text mining. Vector representations, word embeddings and topic embeddings, map words and topics into a low-dimensional and dense real-value vector space, which have obtained high performance in NLP tasks. However, most of the existing models assume the result trained by one of them are perfect correct and used as prior knowledge for improving the other model. Some other models use the information trained from external large corpus to help improving smaller corpus. In this paper, we aim to build such an algorithm framework that makes topic models and vector representations mutually improve each other within the same corpus. An EM-style algorithm framework is employed to iteratively optimize both topic model and vector representations. Experimental results show that our model outperforms state-of-art methods on various NLP tasks. 
  A recurrent neural network model of phonological pattern learning is proposed. The model is a relatively simple neural network with one recurrent layer, and displays biases in learning that mimic observed biases in human learning. Single-feature patterns are learned faster than two-feature patterns, and vowel or consonant-only patterns are learned faster than patterns involving vowels and consonants, mimicking the results of laboratory learning experiments. In non-recurrent models, capturing these biases requires the use of alpha features or some other representation of repeated features, but with a recurrent neural network, these elaborations are not necessary.  
  Deep learning approaches have been widely used in Automatic Speech Recognition  and they have achieved a significant accuracy improvement. Especially, Convolutional Neural Networks  have been revisited in ASR recently. However, most CNNs used in existing work have less than 10 layers which may not be deep enough to capture all human speech signal information. In this paper, we propose a novel deep and wide CNN architecture denoted as RCNN-CTC, which has residual connections and Connectionist Temporal Classification  loss function. RCNN-CTC is an end-to-end system which can exploit temporal and spectral structures of speech signals simultaneously. Furthermore, we introduce a CTC-based system combination, which is different from the conventional frame-wise senone-based one. The basic subsystems adopted in the combination are different types and thus mutually complementary to each other. Experimental results show that our proposed single system RCNN-CTC can achieve the lowest word error rate  on WSJ and Tencent Chat data sets, compared to several widely used neural network systems in ASR. In addition, the proposed system combination can offer a further error reduction on these two data sets, resulting in relative WER reductions of $14.91\%$ and $6.52\%$ on WSJ dev93 and Tencent Chat data sets respectively.  %greater representation capacity.  %we propose a system-level combination to promote the performance of Automatic Speech Recognition . To be specific, we incorperate Connectionist Temporal Classification  output into ResNet, where CTC is currently widely used in acoustic model to reduce the difficulty of frame-wise labeling. Unlike the Cross Entropy loss , however, combining CTC output maybe challenging: firstly, the softmax output of CTC is not frame alignment, and averaging output will introduce more peak noise; secondly, the output likelihood of subsystem is hard to scale to the same standard, which will disturb the results of decoder, i.e., Weighted Finite-State Transducer . To address the above issues, we implement an improved method called Rover. The key to our systemé–³ãƒ¦ç¨ enhanced performance is the systematic use of residual convolutional and LSTM neural networks, combined with a system combination instead of ensemble model of CTC. 
     We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The system comprises five major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-to-phoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For the segmentation model, we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classification  loss. For the audio synthesis model, we implement a variant of WaveNet that requires fewer parameters and trains faster than the original. By using a neural network for each component, our system is simpler and more flexible than traditional text-to-speech systems, where each component requires laborious feature engineering and extensive domain expertise. Finally, we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations. 
 Multi-task learning  in deep neural networks for NLP has recently received increasing interest due to some compelling benefits, including its potential to efficiently regularize models and to reduce the need for labeled data. While it has brought significant improvements in a number of NLP tasks, mixed results have been reported, and little is known about the conditions under which MTL leads to gains in NLP. This paper sheds light on the specific task relations that can lead to gains from MTL models over single-task setups. 
 %%%%%%%%%%%%%%%% We introduce a new paradigm of learning for reasoning, understanding, and prediction, as well as the scaffolding network to implement this paradigm. The scaffolding network embodies an incremental learning approach that is formulated as a teacher-student network architecture to teach machines how to understand text and do reasoning. The key to our computational scaffolding approach is the interactions between the teacher and the student through sequential questioning. The student observes each sentence in the text incrementally, and it uses an attention-based neural net to discover and register the key information in relation to its current memory. Meanwhile, the teacher asks questions about the observed text, and the student network gets rewarded by correctly answering these questions.  The entire network is updated continually using reinforcement learning.  Our experimental results on synthetic and real datasets show that the scaffolding network not only outperforms state-of-the-art methods but also learns to do reasoning in a scalable way even with little human generated input. %, demonstrating the effectiveness of the proposed incremental learning and teaching paradigm. %  We evaluate our network on synthetic and real datasets of reasoning tasks that require recognizing series of concepts and tracking their states. % We introduce the scaffolding network, an incremental learning approach posed as a teacher-student network to teach machines understand text and do reasoning.  % The key to scaffolding networks is the interaction between the teacher and the student through sequential questioning. % The student observes each sentence incrementally and uses an attention-based neural network to discover and register the key information in relation to its current memory. % Meanwhile, the teacher asks questions about the observed text, and the student gets rewarded by correctly answering these questions.  % We evaluate our network on synthetic and real datasets of reasoning tasks that require recognizing series of concepts and tracking their states, and demonstrate that it outperforms state-of-the-art methods. % Our network learns to do reasoning in a scalable way even with less human generated input.  
  In real-world, our DNA is unique but many people share names. This phenomenon often causes erroneous aggregation of documents of multiple persons who are namesake of one another. Such mistakes deteriorate the performance of document retrieval, web search, and more seriously, cause improper attribution of credit or blame in digital forensic. To resolve this issue, the name disambiguation task is designed which aims to partition the documents associated with a name reference such that each partition contains documents pertaining to a unique real-life person. Existing solutions to this task substantially rely on feature engineering, such as biographical feature extraction, or construction of auxiliary features from Wikipedia. However, for many scenarios, such features may be costly to obtain or unavailable due to the risk of privacy violation. In this work, we propose a novel name disambiguation method. Our proposed method is non-intrusive of privacy because instead of using attributes pertaining to a real-life person, our method leverages only relational data in the form of anonymized graphs.  In the methodological aspect, the proposed method uses a novel representation learning model to embed each document in a low dimensional vector space where name disambiguation can be solved by a hierarchical agglomerative clustering algorithm. Our experimental results demonstrate that the proposed method is significantly better than the existing name disambiguation methods working in a similar setting.    
 End-to-end learning of recurrent neural networks  is an attractive solution for dialog systems; however, current techniques are data-intensive and require thousands of dialogs to learn simple behaviors.  We introduce Hybrid Code Networks , which combine an RNN with  and .  Compared to existing end-to-end approaches, HCNs considerably reduce the amount of training data required, while retaining the key benefit of inferring a latent representation of dialog state.  In addition, HCNs can be optimized with supervised learning, reinforcement learning, or a mixture of both.  HCNs attain state-of-the-art performance on the bAbI dialog dataset , and outperform two commercially deployed customer-facing dialog systems. 
  Despite the successes in capturing continuous distributions, the application of generative adversarial networks  to discrete settings, like natural language tasks, is rather restricted. The fundamental reason is the difficulty of back-propagation through discrete random variables combined with the inherent instability of the GAN training objective. To address these problems, we propose Maximum-Likelihood Augmented Discrete Generative Adversarial Networks. Instead of directly optimizing the GAN objective, we derive a novel and low-variance objective using the discriminator's output that follows corresponds to the log-likelihood. Compared with the original, the new objective is proved to be consistent in theory and beneficial in practice. The experimental results on various discrete datasets demonstrate the effectiveness of the proposed approach. 
 Recent work on generative text modeling has found that variational autoencoders  with LSTM decoders perform worse than simpler LSTM language models . This negative result is so far poorly understood, but has been attributed to the propensity of LSTM decoders to ignore conditioning information from the encoder. In this paper, we experiment with a new type of decoder for VAE: a dilated CNN. By changing the decoder's dilation architecture, we control the size of context from previously generated words. In experiments, we find that there is a trade-off between contextual capacity of the decoder and effective use of encoding information. We show that when carefully managed, VAEs can outperform LSTM language models. We demonstrate perplexity gains on two datasets, representing the first positive language modeling result with VAE. % Further, we conduct an in-depth investigation of the use of VAE  for semi-supervised tasks, demonstrating gains over several strong baselines. Further, we conduct an in-depth investigation of the use of VAE  for semi-supervised and unsupervised labeling tasks, demonstrating gains over several strong baselines. %With the resulting models, %  we are able generate text conditioning on label, i.e., generate text conditioning %  on topic and generate review based on rating. 
 Task-oriented dialog systems have been applied in various tasks, such as automated personal assistants, customer service providers and tutors. These systems work well when users have clear and explicit intentions that are well-aligned to the systems' capabilities. However, they fail if users intentions are not explicit. To address this shortcoming, we propose a framework to interleave non-task content  into task conversations. When the task content fails, the system can still keep the user engaged with the non-task content. We trained a policy using reinforcement learning algorithms to promote  long-turn conversation coherence and consistency, so that the system can have smooth transitions between task and non-task content. To test the effectiveness of the proposed framework, we developed a movie promotion dialog system. Experiments with human users indicate that a system that interleaves social and task content achieves a better task success rate and is also rated as more engaging compared to a pure task-oriented system. %\dk{Out of curiosity, is there a reason you didn't just incorporate your stuff into an established task-oriented dialog system, instead of creating the movie one?}   %This framework is especially useful for situations where users lack a clear calibrated goal with the system. 
 To speed up the training process, many existing systems use parallel technology for online learning algorithms. However, most research mainly focus on stochastic gradient descent  instead of other algorithms. We propose a generic online parallel learning framework for large margin models, and also analyze our framework on popular large margin algorithms, including MIRA and Structured Perceptron. Our framework is lock-free and easy to implement on existing systems. Experiments show that systems with our framework can gain near linear speed up by increasing running threads, and with no loss in accuracy. 
 One of the major drawbacks of modularized task-completion dialogue systems is that each module is trained individually, which presents several challenges. For example, downstream modules are affected by earlier modules, and the performance of the entire system is not robust to the accumulated errors. This paper presents a novel end-to-end learning framework for task-completion dialogue systems to tackle such issues. Our neural dialogue system can directly interact with a structured database to assist users in accessing information and accomplishing certain tasks. The reinforcement learning based dialogue manager offers robust capabilities to handle noises caused by other components of the dialogue system. Our experiments in a movie-ticket booking domain show that our end-to-end system not only outperforms modularized dialogue system baselines for both objective and subjective evaluation, but also is robust to noises as demonstrated by several systematic experiments with different error granularity and rates specific to the language understanding module\footnote{The source code is available at: \url{https://github/com/MiuLab/TC-Bot}.}. %Our results suggest that slot-level errors may have a greater impact on the overall performance of a dialogue system compared to intent-level errors, providing the future direction of system improvement. %achieving significant improvement when interacting with human users for both objective and subjective evaluation.  %This paper presents an end-to-end learning framework for task-completion neural dialogue systems, which leverages supervised and reinforcement learning with various deep learning models. the system is able to interface with a structured database, and interact with users for assisting them to access information and accomplish certain tasks. Our experiments in a movie-ticket booking task show that the proposed system outperforms a modular-based dialogue system. In addition, our experiments demonstrate that the reinforcement learning based dialogue system is more robust to noises produced by other components in the system. To further understand the influence of the language understanding errors to the dialogue system, we conduct a series of systematic experiments with different error granularity and rate, and the empirical study shows that among different types of language understanding errors, slot-level errors can have more impact to the overall performance of a dialogue system compared to intent-level errors. %the reinforcement learning based dialogue system is able to learn when and what to confirm in order to achieve better performance and greater robustness.  % [asli]: It is not clear from the abstract above what is novel in this paper. I think we have to bring that to the surface in the abstract.  % Abstract presents a fusion of experiment results which does not really support the proposed method imo. I mean the motivation should be clear. I tried to re-write it, you're welcome to revise it":  % One of major obstacles in building and end-to-end goal oriented dialog systems is that most of the components that make up the dialog pipeline are modular, in which each module are trained individually. This presents several challenges such as the performance of downstream modules of the dialog system are affected by the current module, or the database calls made by the system to retrieve the information requested by the user is not differentiable. In this paper we present a novel end-to-end deep learning framework for task-completion dialogue systems to tackle with such issues. Our neural dialog system can directly interact with the structured database so as to assist users in accessing information and accomplishing certain tasks. The reinforcement learning based dialog manager in our end-to-end trained dialog system provides robust setting for noises caused by other components of our dialog system as well errors caused by the language understanding module. Our experiments in a movie-ticket booking domain show that our end-to-end system not only outperforms modular dialogue system baselines but also robust to noises as demonstrated by several ablation experiments. We have conducted several systematic experiments with different error granulation and rates for the language understanding errors. The results suggests that among different types of language understanding errors, slot-level errors can have more impact to the overall performance of a dialogue system compared to intent-level errors. 
 	To be able to interact better with humans, it is crucial for machines to understand sound -- a primary modality of human perception.     Previous works have used sound to learn embeddings for improved generic semantic similarity assessment.     In this work, we treat sound as a first-class citizen, studying  6textual tasks which require aural grounding.     To this end, we propose  -- a new embedding scheme that learns  grounded in sounds.     For example, we learn that two seemingly  unrelated concepts, like  and  are similar due to the similar  sounds they make.     Our embeddings prove useful in textual tasks requiring aural reasoning like text-based sound retrieval and discovering Foley sound effects .     Moreover, our embedding space captures interesting dependencies between words and onomatopoeia and outperforms prior work on aurally-relevant word relatedness datasets such as AMEN and ASLex. 
 %\boldmath Convolutional Neural Network  are typically associated with Computer Vision. CNNs are responsible for major breakthroughs in Image Classification and are the core of most Computer Vision systems today. More recently CNNs have been applied to problems in Natural Language Processing and gotten some interesting results. In this paper, we will try to explain the basics of CNNs, its different variations and how they have been applied to NLP. 
     Neural Machine Translation  has shown remarkable progress over the      past few years with production systems now being deployed      to end-users. One major drawback of current architectures     is that they are expensive to train, typically requiring days to weeks of GPU     time to converge. This makes exhaustive hyperparameter search,     as is commonly done with other neural network architectures, prohibitively expensive. In this work, we present the first large-scale analysis of     NMT architecture hyperparameters. We report empirical     results and variance numbers for several hundred experimental runs, corresponding to over 250,000 GPU hours on the standard WMT English to German translation task. Our experiments lead to novel insights and practical advice for building and extending NMT architectures. As part of this contribution, we release an open-source NMT framework\footnote{https://github.com/google/seq2seq/} that enables researchers to easily experiment with novel techniques and reproduce state of the art results. 
 We present Nematus, a toolkit for Neural Machine Translation. The toolkit prioritizes high translation accuracy, usability, and extensibility. Nematus has been used to build top-performing submissions to shared translation tasks at WMT and IWSLT, and has been used to train systems for production environments. 
 Recent development of large-scale question answering  datasets triggered a substantial amount of research into end-to-end neural architectures for QA. Increasingly complex systems have been conceived without comparison to simpler neural baseline systems that would justify their complexity. In this work, we propose a simple heuristic that guides the development of neural baseline systems for the extractive QA task. We find that there are two ingredients necessary for building a high-performing neural QA system: first, the awareness of question words while processing the context and second, a composition function that goes beyond simple bag-of-words modeling, such as recurrent neural networks. Our results show that FastQA, a system that meets these two requirements, can achieve very competitive performance compared with existing models. We argue that this surprising finding puts results of previous systems and the complexity of recent QA datasets into perspective. 
 Semantic role labeling  is the task of identifying the predicate-argument structure of a sentence.  It is typically regarded as an important step in the standard NLP pipeline. As the semantic representations are closely related to syntactic ones, we exploit syntactic information in our model.   We propose a version of graph convolutional networks , a recent class of   neural networks operating on graphs, suited to model syntactic dependency graphs.  GCNs over syntactic dependency trees are used as sentence encoders, producing latent feature representations of words in a sentence.  We observe that GCN layers are complementary to LSTM ones: when we stack both GCN and LSTM layers, we obtain a substantial improvement over an already state-of-the-art LSTM SRL model, resulting in the best reported scores on the standard benchmark  both for Chinese and English. 
 This paper proposes an approach for applying GANs to NMT. We build a conditional sequence generative adversarial net which comprises of two adversarial sub models, a generator and a discriminator. The generator aims to generate sentences which are hard to be discriminated from human-translated sentences ; And the discriminator makes efforts to discriminate the machine-generated sentences from human-translated ones. The two sub models play a mini-max game and achieve the win-win situation when they reach a Nash Equilibrium. Additionally, the static sentence-level BLEU is utilized as the reinforced objective for the generator, which biases the generation towards high BLEU points. During training, both the dynamic discriminator and the static BLEU objective are employed to evaluate the generated sentences and feedback the evaluations to guide the learning of the generator. Experimental results show that the proposed model consistently outperforms the traditional RNNSearch and the newly emerged state-of-the-art Transformer on English-German and Chinese-English translation tasks. 
 This paper presents a study of employing Ranking SVM and Convolutional Neural Network for two missions: legal information retrieval and question answering in the Competition on Legal Information Extraction/Entailment. For the first task, our proposed model used a triple of features , and is based on paragraph level instead of article level as in previous studies. In fact, each single-paragraph article corresponds to a particular paragraph in a huge multiple-paragraph article. For the legal question answering task, additional statistical features from information retrieval task integrated into Convolutional Neural Network contribute to higher accuracy.    %Despite obtaining promising performance in the training dataset, our model did not release good result when it comes to formal run in COLIEE 2016 competition.  %From our perspective, a simple expansion of an article with the content of referential one is not always beneficial, so in our experiment, to simplify, we just ignore the references and citations. Additionally, only single-paragraph articles are used instead of multiple-paragraph ones. %The result demonstrates that our model Ranking SVM outperforms other methods with respect to legal information retrieval task. For the legal question answering task, we found that additional statistical features integrated into Convolutional Neural Network results higher accuracy.   
  Recurrent neural networks , especially long short-term memory  RNNs, are effective network for sequential task like speech recognition. Deeper LSTM models perform well on large vocabulary continuous speech recognition, because of their impressive learning ability. However, it is more difficult to train a deeper network. We introduce a training framework with layer-wise training and exponential moving average methods for deeper LSTM models.  It is a competitive framework that LSTM models of more than 7 layers are successfully trained on Shenma voice search data in Mandarin and they outperform the deep LSTM models trained by conventional approach. Moreover, in order for online streaming speech recognition applications, the shallow model with low real time factor is distilled from the very deep model. The recognition accuracy have little loss in the distillation process.  Therefore, the model trained with the proposed training framework reduces relative 14\% character error rate, compared to original model which has the similar real-time capability. Furthermore, the novel transfer learning strategy with segmental Minimum Bayes-Risk is also introduced in the framework. The strategy makes it possible that  training with only a small part of dataset could outperform full dataset training from the beginning.  
 Despite the remarkable progress recently made in distant speech recognition, state-of-the-art technology still suffers from a lack of robustness, especially when adverse acoustic conditions characterized by non-stationary noises and reverberation are met.  A prominent limitation of current systems lies in the lack of matching and communication between the various technologies involved in the distant speech recognition process. The speech enhancement and speech recognition modules are, for instance, often trained independently. Moreover, the speech enhancement normally helps the speech recognizer, but the output of the latter is not commonly used, in turn, to improve the speech enhancement.  To address both concerns, we propose a novel architecture based on a network of deep neural networks, where all the components are jointly trained and better cooperate with each other thanks to a full communication scheme between them. Experiments, conducted using different datasets, tasks and acoustic conditions, revealed that the proposed framework can overtake other competitive solutions, including recent joint training approaches. 
 % Neural networks have become the state-of-the-art techniques for language modeling tasks.  Feedforward Neural Network -based language models estimate the probability of the next word based on the history of the last N words, whereas Recurrent Neural Networks  perform the same task based only on the last word and some context information that cycles in the network. This paper presents a novel approach, which bridges the gap between these two categories of networks. In particular, we propose an architecture which takes advantage of the explicit, sequential enumeration of the word history in FNN structure while enhancing each word representation at the projection layer through recurrent context information that evolves in the network. The context integration is performed using an additional word-dependent weight matrix that is also learned during the training. Extensive experiments conducted on the Penn Treebank  and the Large Text Compression Benchmark  corpus showed a significant reduction of the perplexity when compared to state-of-the-art feedforward as well as recurrent neural network architectures. %Applying the proposed neural network on the Penn Treebank  and the Large Text Compression Benchmark  corpora %obtains a significant reduction of the perplexity when compared to state-of-the-art neural network architectures, including FNN and RNN. 
 In state-of-the-art Neural Machine Translation, an attention mechanism is used during decoding to enhance the translation. At every step, the decoder uses this mechanism to focus on different parts of the source sentence to gather the most useful information before outputting its target word. Recently, the effectiveness of the attention mechanism has also been explored for multimodal tasks, where it becomes possible to focus both on sentence parts and image regions. Approaches to pool two modalities usually include element-wise product, sum or concatenation. In this paper, we evaluate the more advanced Multimodal Compact Bilinear pooling method, which takes the outer product of two vectors to combine the attention features for the two modalities. This has been previously investigated for visual question answering. We try out this approach for multimodal image caption translation and show improvements compared to basic combination methods. 
 We propose a series of recurrent and contextual neural network models for multiple choice visual question answering on the Visual7W dataset. Motivated by divergent trends in model complexities in the literature, we explore the balance between model expressiveness and simplicity by studying incrementally more complex architectures. We start with LSTM-encoding of input questions and answers; build on this with context generation by LSTM-encodings of neural image and question representations and attention over images; and evaluate the diversity and predictive power of our models and the ensemble thereof. All models are evaluated against a simple baseline inspired by the current state-of-the-art, consisting of involving simple concatenation of bag-of-words and CNN representations for the text and images, respectively. Generally, we observe marked variation in image-reasoning performance between our models not obvious from their overall performance, as well as evidence of dataset bias. Our standalone models achieve accuracies up to $64.6\%$, while the ensemble of all models achieves the best accuracy of $66.67\%$, within $0.5\%$ of the current state-of-the-art for Visual7W.   
 During language acquisition, infants have the benefit of visual cues to ground spoken language. Robots similarly have access to audio and visual sensors. Recent work has shown that images and spoken captions can be mapped into a meaningful common space, allowing images to be retrieved using speech and vice versa. In this setting of images paired with untranscribed spoken captions, we consider whether computer vision systems can be used to obtain textual labels for the speech. Concretely, we use an image-to-words multi-label visual classifier to tag images with soft textual labels, and then train a neural network to map from the speech to these soft targets. We show that the resulting speech system is able to predict which words occur in an \mbox{utterance---acting} as a spoken bag-of-words classifier---without seeing any parallel speech and text.  We find that the model often confuses semantically related words, e.g. ``man'' and ``person'', making it even more effective as a semantic keyword spotter. 
 Learning useful information across long time lags is a critical and difficult problem for temporal neural models in tasks such as language modeling. Existing architectures that address the issue are often complex and costly to train.  The Differential State Framework  is a simple and high-performing design that unifies previously introduced gated neural models.  DSF models maintain longer-term memory by learning to interpolate between a fast-changing data-driven representation and a slowly changing, implicitly stable state. This requires hardly any more parameters than a classical, simple recurrent network. Within the DSF framework, a new architecture is presented, the Delta-RNN.  In language modeling at the word and character levels, the Delta-RNN outperforms popular complex architectures, such as the Long Short Term Memory  and the Gated Recurrent Unit , and, when regularized, performs comparably to several state-of-the-art baselines. At the subword level, the Delta-RNN's performance is comparable to that of complex gated architectures.  
 A significant source of errors in Automatic Speech Recognition  systems is due to pronunciation variations which occur in spontaneous and conversational speech. Usually ASR systems use a finite lexicon that provides one or more pronunciations for each word. In this paper, we focus on learning a similarity function between two pronunciations. The pronunciations can be the canonical and the surface pronunciations of the same word or they can be two surface pronunciations of different words. This task generalizes problems such as lexical access , and defining word neighborhoods. It can also be used to dynamically increase the size of the pronunciation lexicon, or in predicting ASR errors. We propose two methods, which are based on recurrent neural networks, to learn the similarity function. The first is based on binary classification, and the second is based on learning the ranking of the pronunciations. We demonstrate the efficiency of our approach on the task of lexical access using a subset of the Switchboard conversational speech corpus. Results suggest that on this task our methods are superior to previous methods which are based on graphical Bayesian methods. 
   %   We tackle a task where an  learns to navigate in a 2D   maze-like environment called xworld.   %   In each session, the agent perceives a sequence of raw-pixel   frames, a natural language command issued by a , and a   set of rewards.   %   The agent learns the teacher's language from scratch in a grounded   and compositional manner, such that after training it is able to   correctly execute  commands: 1) the combination of   words in the command never appeared before, and/or 2) the command   contains  that are learned from another   task but never learned from navigation.   %   Our deep framework for the agent is trained end to end: it learns   simultaneously the visual representations of the environment, the   syntax and semantics of the language, and the action module that   outputs actions.   %   The zero-shot learning capability of our framework results from   its compositionality and modularity with parameter tying.   %   We visualize the intermediate outputs of the framework,   demonstrating that the agent truly understands how to solve the   problem.   %   We believe that our results provide some preliminary insights on   how to train an agent with similar abilities in a 3D environment.   % 
   We present a model of pragmatic referring expression interpretation  in a grounded communication task  that draws upon predictions from two recurrent neural network classifiers, a speaker and a listener, unified by a recursive pragmatic reasoning framework. Experiments show that this combined pragmatic model interprets color descriptions more accurately than the classifiers from which it is built, and that much of this improvement results from combining the speaker and listener perspectives. We observe that pragmatic reasoning helps primarily in the hardest cases: when the model must distinguish very similar colors, or when few utterances adequately express the target color. Our findings make use of a newly-collected corpus of human utterances in color reference games, which exhibit a variety of pragmatic behaviors. We also show that the embedded speaker model reproduces many of these  pragmatic behaviors.  
 We present two simple ways of reducing the number of parameters and accelerating the training of large Long Short-Term Memory  networks: the first one is "matrix factorization by design" of LSTM matrix into the product of two smaller matrices, and the second one is partitioning of LSTM matrix, its inputs and states into the independent groups. Both approaches allow us to train large LSTM networks significantly faster to the near state-of the art perplexity while using significantly less RNN parameters. 
    We investigate the effective memory depth of RNN models by using them for $n$-gram language model  smoothing.    Experiments on a small corpus  have found the LSTM cell with dropout to be the best model for encoding the $n$-gram state when compared with feed-forward and vanilla RNN models. When preserving the sentence independence assumption the LSTM $n$-gram matches the LSTM LM performance for $n=9$ and slightly outperforms it for $n=13$. When allowing dependencies across sentence boundaries, the LSTM $13$-gram almost matches the perplexity of the unlimited history LSTM LM.    LSTM $n$-gram smoothing also has the desirable property of improving with increasing $n$-gram order, unlike the Katz or Kneser-Ney back-off estimators. Using multinomial distributions as targets in training instead of the usual one-hot target is only slightly beneficial for low $n$-gram orders.    Experiments on the One Billion Words benchmark show that the results hold at larger scale: while LSTM smoothing for short $n$-gram contexts does not provide significant advantages over classic N-gram models, it becomes effective with long contexts ; depending on the task and amount of data it can match fully recurrent LSTM models at about $n=13$. This may have implications when modeling short-format text, e.g. voice search/query LMs.    Building LSTM $n$-gram LMs may be appealing for some practical situations: the state in a $n$-gram LM can be succinctly represented with $*4$ bytes storing the identity of the words in the context and batches of $n$-gram contexts can be processed in parallel. On the downside, the $n$-gram context encoding computed by the LSTM is discarded, making the model more expensive than a regular recurrent LSTM LM.  
   Sentence simplification aims to make sentences easier to read and   understand. Most recent approaches draw on insights from machine   translation to learn simplification rewrites from monolingual   corpora of complex and simple sentences. We address the   simplification problem with an encoder-decoder model coupled with a   deep reinforcement learning framework. Our model, which we call {.} 	 	%  We propose a deep reinforcement learning framework for text simplification tasks, which models the simplicity, meaning preserving and grammaticality. We achieved state-of-the-art performance on two simplification datasets. 
  While recent neural encoder-decoder models have shown great promise in modeling open-domain conversations, they often generate dull and generic responses. Unlike past work that has focused on diversifying the output of the decoder at word-level to alleviate this problem, we present a novel framework based on conditional variational autoencoders that captures the discourse-level diversity in the encoder. Our model uses latent variables to learn a distribution over potential conversational intents and generates diverse responses using only greedy decoders. We have further developed a novel variant that is integrated with linguistic prior knowledge for better performance. Finally, the training procedure is improved by introducing a bag-of-word loss. Our proposed models have been validated to generate significantly more diverse responses than baseline approaches and exhibit competence in discourse-level decision-making.\footnote{Data and an implementation of our model is avalaible at https://github.com/snakeztc/NeuralDialog-CVAE} 
 We propose a general approach to modeling semi-supervised learning  algorithms.  Specifically, we present a declarative language for modeling both traditional supervised classification tasks and many SSL heuristics, including both well-known heuristics such as co-training and novel domain-specific heuristics. In addition to representing individual SSL heuristics, we show that multiple heuristics can be automatically combined using Bayesian optimization methods. We experiment with two classes of tasks, link-based text classification and relation extraction.  We show modest improvements on well-studied link-based classification benchmarks, and state-of-the-art results on relation-extraction tasks for two realistic domains. 
 Data noising is an effective technique for regularizing neural network models. While noising is widely adopted in application domains such as vision and speech, commonly used noising primitives have not been developed for discrete sequence-level settings such as language modeling. In this paper, we derive a connection between input noising in neural network language models and smoothing in $n$-gram models. Using this connection, we draw upon ideas from smoothing to develop effective noising schemes. We demonstrate performance gains when applying the proposed schemes to language modeling and machine translation. Finally, we provide empirical analysis validating the relationship between noising and smoothing. 
 Nowadays, a big part of people rely on available content in social media in their decisions . The possibility that anybody can leave a review provide a golden opportunity for spammers to write spam reviews about products and services for different interests. Identifying these spammers and the spam content is a hot topic of research and although a considerable number of studies have been done recently toward this end, but so far the methodologies put forth still barely detect spam reviews, and none of them show the importance of each extracted feature type. In this study, we propose a novel framework, named \textsl{NetSpam}, which utilizes spam features for modeling review datasets as heterogeneous information networks to map spam detection procedure into a classification problem in such networks. Using the importance of spam features help us to obtain better results in terms of different metrics experimented on real-world review datasets from Yelp and Amazon websites. The results show that NetSpam outperforms the existing methods and among four categories of features; including review-behavioral, user-behavioral, review-linguistic, user-linguistic, the first type of features performs better than the other categories. 
 Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. %For instance, as we observe in a mainstream apps market, people tend to download apps for food delivery at meal-time, which indicates that the interaction of app category and time-stamp is highly predictive of CTR. %In general, CTR can be affected by profound feature interactions that are hard to engineering and have to be learned automatically from data. Despite great progress, existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and high-order feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide \& Deep model from Google, DeepFM has a shared input to its ``wide'' and ``deep'' parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data.  % %CTR prediction is a crucial task in recommender systems. The key of CTR prediction model is the ability of learning feature interactions, which is the weakness of the widely used linear model. Factorization machine  is desined to learn feature interactions automatically. Unfortunately, FM needs specify the order of feature interactions and is complicated to capture high-order feature interactions. Different with linear model and FM, for the purpose of seizing high-order feature interactions, several deep models are proposed recently. However, almost existing CTR models lack the ability to learn both low- and high-order feature interactions. Although Wide \& Deep has this capability, it need two input and the input of the ``wide'' part requires additional feature engineering. Therefore, in this paper, taking the advantages of FM and DNN, we propose Factorization-Machine based Neural network, namely DeepFM, to capture both low- and high-order feature interactions automatically. Moreover, DeepFM does not need any pre-training for the network, as needed in some other networks. We compare the network structure of DeepFM and the others in detail. Comprehensive experiments are conducted to empirically demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction.  %CTR prediction is a crucial task in recommender systems.Factorization Machines  are designed to learn pairwise feature interactions automatically, however, it is complicated to learn high-order feature interactions with FM. Deep Neural Networks  utilize network structures and non-linear activation functions to learn high-order feature interactions, while low-order feature interactions are ignored.  As a very similar work, proposed by Google combines logistic regression  and DNN to achieve the similar goal as ours, however, expertise feature engineering is still needed in the ``wide" part of their framework. In contrast, our proposed DeepFM needs no feature engineering for the input. Moreover, DeepFM does not need any pre-training for the network, as needed in some other networks. We compare the network structure of DeepFM and the others in detail. Comprehensive experiments are conducted to empirically demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction. 
 In this paper we show how the performance of tweet clustering can be improved by leveraging character-based neural networks. The proposed approach overcomes the limitations related to the vocabulary explosion in the word-based models and allows for the seamless processing of the multilingual content. Our evaluation results and code are available on-line\footnote{\url{https://github.com/vendi12/tweet2vec_clustering}}. 
 The number of documents available into Internet moves each day up.  For this reason, processing this amount of information effectively and expressibly becomes a major concern for companies and scientists.  Methods that represent a textual document by a topic representation are widely used in Information Retrieval  to process big data such as Wikipedia articles.  One of the main difficulty in using topic model on huge data collection is related to the material resources  required for model estimate.  To deal with this issue, we propose to build topic spaces from summarized documents.  In this paper, we present a study of topic space representation in the context of big data. The topic space representation behavior is analyzed on different languages.  Experiments show that topic spaces estimated from text summaries are as relevant as those estimated from the complete documents. The real advantage of such an approach is the processing time gain: we showed that the processing time can be drastically reduced using summarized documents .  This study finally points out the differences between thematic representations of documents depending on the targeted languages such as English or  latin languages.\footnote{Preprint of \textsl{International Journal of Computational Linguistics and Applications}, 7:87-109, 2016.} 
 In this paper we analyze the gate activation signals inside the gated recurrent neural networks, and find the temporal structure of such signals is highly correlated with the phoneme boundaries. This correlation is further verified by a set of experiments for phoneme segmentation, in which better results compared to standard approaches were obtained.     \newline     
 We empirically characterize the performance of discriminative and generative LSTM models for text classification. We find that although RNN-based generative models are more powerful than their bag-of-words ancestors , they have higher asymptotic error rates than discriminatively trained RNN models. However we also find that generative models approach their asymptotic error rate more rapidly than their discriminative counterparts---the same pattern that  proved holds for linear classification models that make more na\"{\i}ve conditional independence assumptions.  Building on this finding, we hypothesize that RNN-based generative classification models will be more robust to shifts in the data distribution. This hypothesis is confirmed in a series of experiments in zero-shot and continual learning settings that show that generative models substantially outperform discriminative models. 
 We present a novel cross-lingual transfer method for paradigm completion, the task of mapping a lemma to its inflected forms, using a neural encoder-decoder model, the state of the art for the monolingual task. We use labeled data from a high-resource language to increase performance on a low-resource language. In experiments on 21 language pairs from four different language families, we obtain up to 58\% higher accuracy than without transfer and show that even zero-shot and one-shot learning are possible.  We further find that the degree of language relatedness strongly influences the ability to transfer morphological knowledge. 
 Implicit discourse relation classification is of great challenge due to the lack of connectives as strong linguistic cues, which motivates the use of annotated implicit connectives to improve the recognition. We propose a feature imitation framework in which an implicit relation network is driven to learn from another neural network with access to connectives, and thus encouraged to extract similarly salient features for accurate classification. We develop an adversarial model to enable an adaptive imitation scheme through competition between the implicit network and a rival feature discriminator. Our method effectively transfers discriminability of connectives to the implicit features, and achieves state-of-the-art performance on the PDTB benchmark.  
 Recent works have shown that synthetic parallel data automatically generated by translation models can be effective for various neural machine translation  issues. In this study, we build NMT systems using only synthetic parallel data. As an efficient alternative to real parallel data, we also present a new type of synthetic parallel corpus. The proposed pseudo parallel data are distinct from previous works in that ground truth and synthetic examples are mixed on both sides of sentence pairs. Experiments on Czech-German and French-German translations demonstrate the efficacy of the proposed pseudo parallel corpus, which shows not only enhanced results for bidirectional translation tasks but also substantial improvement with the aid of a ground truth real parallel corpus. 
 Keyphrase boundary classification  is the task of detecting keyphrases in scientific articles and labelling them with respect to predefined types. Although important in practice, this task is so far underexplored, partly due to the lack of labelled data.  To overcome this, we explore several auxiliary tasks, including semantic super-sense tagging and identification of multi-word expressions, and cast the task as a multi-task learning problem with deep recurrent neural networks. Our multi-task models perform significantly better than previous state of the art approaches on two scientific KBC datasets, particularly for long keyphrases.  %We present a novel approach to keyphrase boundary classification  based on multi-task recurrent neural networks. KBC is the task of detecting keyphrases in scientific articles and labeling them with respect to predefined concept classes. We explore several auxiliary tasks, including semantic supersense tagging and identification of multi-word expressions. Our multi-task models perform significantly better than previous state of the art approaches on two scientific KBC datasets, with error reductions of up to 14.9\%~on the SemEval 2017 Task 10 corpus and of up to 5.1\%~on the ACL RD-TEC corpus.  
 The input to a neural sequence-to-sequence model is often determined by an up-stream system, e.g.\ a word segmenter, part of speech tagger, or speech recognizer. These up-stream models are potentially error-prone. Representing inputs through word lattices allows making this uncertainty explicit by capturing alternative sequences and their posterior probabilities in a compact form. In this work, we extend the TreeLSTM  into a LatticeLSTM that is able to consume word lattices, and can be used as encoder in an attentional encoder-decoder model. We integrate lattice posterior scores into this architecture by extending the TreeLSTM's child-sum and forget gates and introducing a bias term into the attention mechanism. We experiment with speech translation lattices and report consistent improvements over baselines that translate either the 1-best hypothesis or the lattice without posterior scores. 
 Building a voice conversion  system from non-parallel speech corpora is challenging  but highly valuable in real application scenarios. In most situations, the source and the target speakers do not repeat the same texts or they may even speak different languages. In this case, one possible, although indirect, solution is to build a generative model for speech. Generative models focus on explaining the observations with latent variables instead of learning a pairwise transformation function, thereby bypassing the requirement of speech frame alignment. In this paper, we propose a non-parallel VC framework with a variational autoencoding Wasserstein generative adversarial network  that explicitly considers a VC objective when building the speech model. Experimental results corroborate the capability of our framework for building a VC system from unaligned data, and demonstrate improved conversion quality.   
 Over 50 million scholarly articles have been published: they constitute a unique repository of knowledge. In particular, one may infer from them relations between scientific concepts, such as synonyms and hyponyms. Artificial neural networks have been recently explored for relation extraction. In this work, we continue this line of work and present a system based on a convolutional neural network to extract relations. Our model ranked first in the SemEval-2017 task 10  for relation extraction in scientific articles . 
   % Code generation, or the general task of semantic parsing, aims to transform natural language descriptions to structured programming code.   % Code generation aims to transform natural language descriptions into source code written in a programming language.   % existing works treat code generation as a language generation task and employ recurrent neural networks to implicitly capture the underlying syntax of the target programming language.   We consider the problem of parsing natural language descriptions into source code written in a general-purpose programming language like Python.   Existing data-driven methods treat this problem as a language generation task without considering the underlying syntax of the target programming language.   Informed by previous work in semantic parsing, in this paper we propose a novel neural architecture powered by a grammar model to explicitly capture the target syntax as prior knowledge.   % In this paper we propose a novel neural architecture which explicitly models the syntax of the target language.   % Guided by an external base of syntax rules as prior knowledge, the model learns to construct an abstract syntax tree by applying production rules at multiple time steps.   % Guided by an external grammar model as prior knowledge, the model learns to construct an abstract syntax tree by applying production rules at multiple time steps.   Experiments find this an effective way to scale up to generation of complex programs from natural language descriptions, achieving state-of-the-art results that well outperform previous code generation and semantic parsing approaches.   % Experiments show that our model scales up to generation of complex programs from natural language descriptions, achieving state-of-the-art results that well outperform previous code generation and semantic parsing approaches.  % \py{Can we still claim that since our model has zero accuracy when the number of nodes grows over 100?}  % \gn{Good point, I modified this.} 
 We evaluate a semantic parser based on a character-based sequence-to-sequence model in the context of the SemEval-2017 shared task on semantic parsing for AMRs. With data augmentation, super characters, and POS-tagging we gain major improvements in performance compared to a baseline character-level model. Although we improve on previous character-based neural semantic parsing models, the overall accuracy is still lower than a state-of-the-art AMR parser. An ensemble combining our neural semantic parser with an existing, traditional parser, yields a small gain in performance. 
 This paper describes our approach to the SemEval 2017 Task 10: , specifically to Subtask : .  %Instead of extensive feature engineering  We explored three  different deep learning approaches: a character-level convolutional neural network , %CNN, a stacked learner with an MLP meta-classifier, and an attention based Bi-LSTM. %We create an ensemble %system From these %three  approaches, we created an ensemble  of differently hyper-parameterized systems, achieving a micro-$F_1$-score of 0.63 on the test data. Our approach ranks 2nd  out of four according to this official score. However, %due to an error,  we  erroneously trained 2 out of 3 neural nets  on only roughly 15\% of the full data, namely, the original development set. When trained on the full data , our ensemble has a micro-$F_{1}$-score of 0.69.  %and 1st under a -$F_{1}$-score. %The code for our experiments  Our code is available from \url{https://github.com/UKPLab/semeval2017-scienceie}.%\url{https://github.com/UKPLab/scienceie2017}. 
 	\renewcommand{\thefootnote}{$\dagger$} 	\footnotetext{Equal contribution.} 	Sentence simplification reduces semantic complexity to benefit people with language impairments. Previous simplification studies on the sentence level and word level have achieved promising results but also meet great challenges. For sentence-level studies, sentences after simplification are fluent but sometimes are not really simplified. For word-level studies, words are simplified but also have potential grammar errors due to different usages of words before and after simplification. In this paper, we propose a two-step simplification framework by combining both the word-level and the sentence-level simplifications, making use of their corresponding advantages. Based on the two-step framework, we implement a novel constrained neural generation model to simplify sentences given simplified words. The final results on Wikipedia and Simple Wikipedia aligned datasets indicate that our method yields better performance than various baselines. 
 Building a dialogue agent to fulfill complex tasks, such as travel planning, is challenging because the agent has to learn to  complete multiple subtasks. For example, the agent needs to reserve a hotel and book a flight so that there leaves enough time for commute between arrival and hotel check-in. This paper addresses this challenge by formulating the task in the mathematical framework of  over Markov Decision Processes , and proposing a hierarchical deep reinforcement learning approach to learning a dialogue manager that operates at different temporal scales. The dialogue manager consists of:  a top-level dialogue policy that selects among subtasks or options,  a low-level dialogue policy that selects primitive actions to complete the subtask given by the top-level policy, and  a global state tracker that helps ensure all cross-subtask constraints be satisfied. Experiments on a travel planning task with simulated and real users show that our approach leads to significant improvements over three baselines, two based on handcrafted rules and the other based on flat deep reinforcement learning.   %In a composite-domain task-completion dialogue system, a conversation agent often switches among multiple sub-domains before it successfully completes the task. Given such a scenario, a standard deep reinforcement learning based dialogue agent may suffer to find a good policy due to the issues such as: increased state and action spaces, sparse reward, long horizon and high sample complexity demands etc.. In this paper, we propose to use a hierarchical deep reinforcement learning approach which can operate at different temporal scales and is intrinsically motivated to tackle these problems. Our hierarchical policy network consists of two levels: the top-level meta-controller for subgoal selection and the low-level controller for dialogue policy learning. Subgoals selected by meta-controller and intrinsic rewards can guide the controller to effectively explore in the state-action space and mitigate the spare reward and long horizon problems. Experiments on both simulations and human evaluation show that our model significantly outperforms flat deep reinforcement learning agents in terms of success rate, rewards and user rating.  % We present a hierarchical deep reinforcement learning agent for an end-to-end, composite-domain task-completion dialogue system. In a composite-domain dialogue, a conversation often switching among multiple sub-domains before successful completion. Standard deep reinforcement learning based dialogue agents may suffer several problems in a complex, multi-domain scenario: increased state and action spaces, high sample complexity demands, sparse reward and long horizon etc., these issues will hinder the agents to find a good policy. Hence, we propose to use hierarchical deep reinforcement learning models which can operate at different temporal scales and are intrinsically motivated to attack these problems. It consists of two levels of network: the top-level meta-controller for subgoal selection and the low-level controller for dialogue policy learning. Subgoals selected by meta-controller and intrinsic rewards can guide the controller to effectively explore in the state-action space and mitigate the spare reward and long horizon problems. Experiments on both simulations and human evaluation show that our model significantly outperforms flat deep reinforcement learning agents in terms of success rate, rewards and user rating.  
 %% Recently, many approaches are explored to enhance the decoding algorithm. For extended periods of time, sequence generation models rely on beam search as the decoding algorithm. However, the performance of beam search degrades when the model is over-confident about a suboptimal prediction. In this work, we enhance beam search by performing minimum Bayes-risk  decoding for some extra steps at a later stage. In our experiments, we found that the conventional MBR reranking is only effective with a large beam size. In contrast, later-stage MBR decoding is shown to work regardless of the choice of beam size, and outperform simple MBR reranking. Additionally, we found that the computation of Bayes risks can be much faster by calculating the discrepancies on GPU in batch mode.
 Ensembling is a well-known technique in neural machine translation  to improve system performance. Instead of a single neural net, multiple neural nets with the same topology are trained separately, and the decoder generates predictions by averaging over the individual models. Ensembling often improves the quality of the generated translations drastically. However, it is not suitable for production systems because it is cumbersome and slow. This work aims to reduce the runtime to be on par with a single system without compromising the translation quality. First, we show that the ensemble can be unfolded into a single large neural network which imitates the output of the ensemble system. We show that unfolding can already improve the runtime in practice since more work can be done on the GPU. We proceed by describing a set of techniques to shrink the unfolded network by reducing the dimensionality of layers. On Japanese-English we report that the resulting network has the size and decoding speed of a single NMT network but performs on the level of a 3-ensemble system. 
  Although neural networks are well suited for sequential transfer learning tasks, the catastrophic forgetting problem hinders proper integration of prior knowledge. In this work, we propose a solution to this problem by using a multi-task objective based on the idea of distillation and a mechanism that directly penalizes forgetting at the shared representation layer during the knowledge integration phase of training. We demonstrate our approach on a Twitter domain sentiment analysis task with sequential knowledge transfer from four related tasks.  We show that our technique outperforms networks fine-tuned to the target task. Additionally, we show both through empirical evidence and examples that it does not forget useful knowledge from the source task that is forgotten during standard fine-tuning. Surprisingly, we find that first distilling a human made rule based sentiment engine into a recurrent neural network and then integrating the knowledge with the target task data leads to a substantial gain in generalization performance. Our experiments demonstrate the power of multi-source transfer techniques in practical text analytics problems when paired with distillation. In particular, for the SemEval 2016 Task 4 Subtask A  dataset we surpass the state of the art established during the competition with a comparatively simple model architecture that is not even competitive when trained on only the labeled task specific data.  
  Neural machine translation , a new approach to machine  translation, has achieved promising results comparable to those of traditional  approaches such as statistical machine translation . Despite its recent success, NMT  cannot handle a larger vocabulary because training complexity and  decoding complexity proportionally increase with the number of target words. This problem  becomes even more serious when translating patent documents,   which contain many technical terms that are observed infrequently.  In NMTs, words that are out of vocabulary are  represented by a single unknown token.  In this paper, we propose a method that enables NMT to translate patent sentences comprising a  large vocabulary of technical terms. We train an NMT system on  bilingual data wherein technical terms are replaced with  technical term tokens; this allows it to translate most of the source  sentences except technical terms. Further, we use it as a decoder to  translate source sentences with technical term tokens and replace the  tokens with technical term translations using SMT. We also use it to  rerank the 1,000-best SMT translations on the basis of the average of the SMT  score and that of the NMT rescoring of the translated sentences  with technical term tokens. Our experiments on Japanese-Chinese patent  sentences show that the proposed NMT system achieves a substantial  improvement of up to 3.1 BLEU points and 2.3 RIBES points over traditional SMT systems and an  improvement of approximately 0.6 BLEU points and 0.8 RIBES points over an equivalent  NMT system without our proposed technique. 
 Most extractive summarization methods focus on the main body of the document from which sentences need to be extracted. However, the gist of the document may lie in side information, such as the title and image captions which are often available for newswire articles. We propose to explore side information in the context of single-document extractive summarization. We develop a framework for single-document summarization composed of a hierarchical document encoder and an attention-based extractor with attention over side information. We evaluate our model on a large scale news dataset. We show that extractive summarization with side information consistently outperforms its counterpart that does not use any side information, in terms of both informativeness and fluency. 
 We present a solution to the problem of paraphrase identification of questions. We focus on a recent dataset of question pairs annotated with binary paraphrase labels and show that a variant of the decomposable attention model  results in accurate performance on this task, while being far simpler than many competing neural architectures. Furthermore, when the model is pretrained on a noisy dataset of automatically collected question paraphrases, it obtains the best reported performance on the dataset. 
   This document contains the instructions for preparing a camera-ready   manuscript for the proceedings of ACL-2017. The document itself   conforms to its own specifications, and is therefore an example of   what your manuscript should look like. These instructions should be   used for both papers submitted for review and for final versions of   accepted papers.  Authors are asked to conform to all the directions   reported in this document. 
 We propose a model to automatically describe changes introduced in the source code of a program using natural language. Our method receives as input a set of code commits, which contains both the modifications and  message introduced by an user. These two modalities are used to train  an encoder-decoder architecture. We evaluated our approach on twelve real world open source projects from four different programming languages. Quantitative and qualitative  results showed that the proposed approach can generate feasible and semantically sound descriptions not only in standard in-project settings, but also in a cross-project setting.  
 We propose a novel deep learning model for joint document-level entity disambiguation, which leverages learned neural representations. Key components are entity embeddings, a neural attention mechanism over local context windows, and a differentiable joint inference stage for disambiguation. Our approach thereby combines benefits of deep learning with more traditional approaches such as graphical models and probabilistic mention-entity maps. Extensive experiments show that we are able to obtain competitive or state-of-the-art accuracy at moderate computational costs. 
 Prominent applications of sentiment analysis are countless, covering areas such as marketing, customer service and communication. The conventional bag-of-words approach for measuring sentiment merely counts term frequencies; however, it neglects the position of the terms within the discourse. As a remedy, we develop a discourse-aware method that builds upon the discourse structure of documents. For this purpose, we utilize rhetorical structure theory to label \mbox{clauses} according to their hierarchical relationships and then assign polarity scores to individual leaves. To learn from the resulting rhetorical structure, we propose a tensor-based, tree-structured deep neural network  in order to process the complete discourse tree. The underlying tensors infer the salient passages of narrative materials. In addition, we suggest two algorithms for data augmentation  that increase our training set and reduce overfitting. Our benchmarks demonstrate the superior performance of our approach. Moreover, our tensor structure reveals the salient text passages and thereby provides explanatory insights. 
 Neural network models have shown their promising opportunities for multi-task learning, which focus on learning the shared layers to extract the common and task-invariant features. However, in most existing approaches, the extracted shared features are prone to be contaminated by task-specific features or the noise brought by other tasks. In this paper, we propose an adversarial multi-task learning framework, alleviating the shared and private latent feature spaces from interfering with each other. We conduct extensive experiments on 16 different text classification tasks, which demonstrates the benefits of our approach. Besides, we show that the shared knowledge learned by our proposed model can be regarded as off-the-shelf knowledge and easily transferred to new tasks. The datasets of all 16 tasks are publicly available at \url{http://nlp.fudan.edu.cn/data/} 
 We propose a multi-view  network for text classification. Our method automatically creates various views of its input text, each taking the form of soft attention weights that distribute the classifier's focus among a set of base features. For a bag-of-words representation, each view focuses on a different subset of the text's words. Aggregating many such views results in a more discriminative and robust representation.  Through a novel architecture that both stacks and concatenates views, we produce a network that emphasizes both depth and width, allowing training to converge quickly. Using our multi-view architecture, we establish new state-of-the-art accuracies on two benchmark tasks. 
 The proliferation of social media in communication and information dissemination has made it an ideal platform for spreading rumors. Automatically debunking rumors at their stage of diffusion is known as early rumor detection, which refers to dealing with sequential posts regarding disputed factual claims with certain variations and highly textual duplication over time. Thus, identifying trending rumors demands an efficient yet flexible model that is able to capture long-range dependencies among postings and produce distinct representations for the accurate early detection. However, it is a challenging task to apply conventional classification algorithms to rumor detection in earliness since they rely on hand-crafted features which require intensive manual efforts in the case of large amount of posts. This paper presents a deep attention model on the basis of recurrent neural networks  to learn selectively temporal hidden representations of sequential posts for identifying rumors. The proposed model delves soft-attention into the recurrence to simultaneously pool out distinct features with particular focus and produce hidden representations that capture contextual variations of relevant posts over time. Extensive experiments on real datasets collected from social media websites demonstrate that  the deep attention based RNN model outperforms state-of-the-arts that rely on hand-crafted features;  the introduction of soft attention mechanism can effectively distill relevant parts to rumors from original posts in advance;  the proposed method detects rumors more quickly and accurately than competitors. 
   We investigate neural techniques for end-to-end computational argumentation mining .    We frame AM both as a token-based dependency parsing and as a token-based sequence tagging problem, including a multi-task learning setup.    Contrary to models that operate on the argument component level, we find that framing AM as dependency parsing %does not   leads to subpar performance results. % probably due to model complexity and entaile data sparsity issues.   In contrast, less complex  tagging models based on BiLSTMs perform robustly across classification scenarios, being able to catch long-range dependencies inherent to the AM problem.    Moreover, we find that jointly learning `natural' subtasks, in a %joint modeling   multi-task learning setup, improves performance.  %  Contrary to what is done in other models, dependency parser does not %  work. In contrast, local models perform decently, and they need the %  long-range dependencies that neural networks offer. Why specialized %  models are not helpful: argumentation mining definition may change, %  optimized models would have to be re-invented each time. In %  contrast, neural nets may be  applied to each of %  provided that the general properties  %  remain.  
 This paper addresses the problem of predicting popularity of comments in an online discussion forum using reinforcement learning, particularly addressing two challenges that arise from having natural language state and action spaces. First, the state representation, which characterizes the history of comments tracked in a discussion at a particular point, is augmented to incorporate the global context represented by discussions on world events available in an external knowledge source. Second, a two-stage Q-learning framework is introduced, making it feasible to search the combinatorial action space while also accounting for redundancy among sub-actions. We experiment with five Reddit communities, showing that the two methods improve over previous reported results on this task. 
   %%question; what we do; hao we do; great results   Neural machine translation  becomes a new approach to machine translation and generates much more fluent   results compared to statistical machine translation .   However, SMT is usually better than NMT in translation adequacy.   It is therefore a promising direction to combine the advantages of both NMT and SMT.   In this paper, we propose a neural system combination framework leveraging multi-source NMT,   which takes as input the outputs of NMT and SMT systems and produces the final translation.   Extensive experiments on the Chinese-to-English translation task show that our model archives significant   improvement by 5.3 BLEU points over the best single system output and 3.4 BLEU points over the state-of-the-art   traditional system combination methods.  
   Modeling attention in neural multi-source sequence-to-sequence learning remains a relatively unexplored area, despite its usefulness in tasks that incorporate multiple source languages or modalities. % We propose two novel approaches to combine the outputs of attention mechanisms over each source sequence,  and . % We compare the proposed methods with existing techniques and present results of systematic evaluation of those methods on the WMT16 Multimodal Translation and Automatic Post-editing tasks. % We show that the proposed methods achieve competitive results on both tasks.  
  Human verbal communication includes affective messages which are conveyed through use of emotionally colored words. There has been a lot of research in this direction but the problem of integrating state-of-the-art neural language models with affective information remains an area ripe for exploration. In this paper, we propose an extension to an LSTM  language model for generating conversational text, conditioned on affect categories. Our proposed model, Affect-LM enables us to customize the degree of emotional content in generated sentences through an additional design parameter. Perception studies conducted using Amazon Mechanical Turk show that Affect-LM generates naturally looking emotional sentences without sacrificing grammatical correctness. Affect-LM also learns affect-discriminative word representations, and perplexity experiments show that additional affective information in conversational text can improve language model prediction.   
 Recurrent Neural Networks are showing much promise in many sub-areas of natural language processing, ranging from document classification to machine translation to automatic question answering. Despite their promise, many recurrent models have to read the whole text word by word, making it slow to handle long documents. For example, it is difficult to use a recurrent network to read a book and answer questions about it. In this paper, we present an approach of reading text while skipping irrelevant information if needed. The underlying model is a recurrent network that learns how far to jump after reading a few words of the input text. We employ a standard policy gradient method to train the model to make discrete jumping decisions. In our benchmarks on four different tasks, including number prediction, sentiment analysis, news article classification and automatic Q\&A, our proposed model, a modified LSTM with jumping, is up to 6 times faster than the standard sequential LSTM, while maintaining the same or even better accuracy.    %As the model also skips irrelevant information, the non-sequential LSTM also generalizes better. 
 Keyphrase provides highly-summative information that can be effectively used for understanding, organizing and retrieving text content. Though previous studies have provided many workable solutions for automated keyphrase extraction, they commonly divided the to-be-summarized content into multiple text chunks, then ranked and selected the most meaningful ones. These approaches could neither identify keyphrases that do not appear in the text, nor capture the real semantic meaning behind the text. We propose a generative model for keyphrase prediction with an encoder-decoder framework, which can effectively overcome the above drawbacks.  We name it as deep keyphrase generation since it attempts to capture the deep semantic meaning of the content with a deep learning method. Empirical analysis on six datasets demonstrates that our proposed model not only achieves a significant performance boost on extracting keyphrases that appear in the source text, but also can generate absent keyphrases based on the semantic meaning of the text. Code and dataset are available at {https://github.com/memray/seq2seq-keyphrase}. 
 Recent works have explored deep architectures for learning multimodal speech representation  in a supervised way. Here we investigate the role of combining different speech modalities, i.e. audio and visual information representing the lipsé–³ movements, in a weakly supervised way using Siamese networks and lexical same-different side information. In particular, we ask whether one modality can benefit from the other to provide a richer representation for phone recognition in a weakly supervised setting. We introduce mono-task and multi-task methods for merging speech and visual modalities for phone recognition. The mono-task learning consists in applying a Siamese network on the concatenation of the two modalities, while the multi-task learning receives several different combinations of modalities at train time. We show that multi-task learning enhances discriminability for visual and multimodal inputs while minimally impacting auditory inputs. Furthermore, we present a qualitative analysis of the obtained phone embeddings, and show that cross-modal visual input can improve the discriminability of phonological features which are visually discernable , resulting in representations that are closer to abstract linguistic features than those based on audio only. 
   In this paper, we propose a new method for calculating the output layer in neural machine translation systems.   The method is based on predicting a binary code for each word   and can reduce computation time/memory requirements of   the output layer to be logarithmic in vocabulary size in the best case.   In addition, we also introduce two advanced approaches to improve the robustness of the   proposed model: using error-correcting codes and combining softmax and binary codes.   Experiments on two English $\leftrightarrow$ Japanese bidirectional translation tasks show proposed models achieve    BLEU scores that approach the softmax,   while reducing memory usage to the order of less than 1/10   and improving decoding speed on CPUs by x5 to x10. 
 In this paper, we study a new learning paradigm for Neural Machine Translation . Instead of maximizing the likelihood of the human translation as in previous works, we minimize the distinction between human translation and the translation given by an NMT model. To achieve this goal, inspired by the recent success of Generative Adversarial Networks , we employ an adversarial training architecture and name it as Adversarial-NMT. In Adversarial-NMT, the training of the NMT model is assisted by an adversary, which is an elaborately designed Convolutional Neural Network . The goal of the adversary is to differentiate the translation result generated by the NMT model from that by human. The goal of the NMT model is to produce high quality translations so as to cheat the adversary. A policy gradient method is leveraged to co-train the NMT model and the adversary. Experimental results on English$\rightarrow$French and German$\rightarrow$English translation tasks show that Adversarial-NMT can achieve significantly better translation quality than several strong baselines. 
   This document contains the instructions for preparing a camera-ready   manuscript for the proceedings of ACL-2017. The document itself   conforms to its own specifications, and is therefore an example of   what your manuscript should look like. These instructions should be   used for both papers submitted for review and for final versions of   accepted papers.  Authors are asked to conform to all the directions   reported in this document. 
   Several approaches have recently been proposed for learning decentralized deep   multiagent policies that coordinate via a differentiable communication channel.   While these policies are effective for many tasks, interpretation of   their induced communication strategies has remained a challenge. Here we   propose to interpret agents' messages by translating them.     Unlike in typical machine translation problems, we have no parallel data   to learn from. Instead we develop a translation model based on the insight   that agent messages and natural language strings mean the same thing .   We present theoretical guarantees and empirical evidence that our approach   preserves both the semantics and pragmatics of messages by ensuring that   players communicating through a translation layer do not suffer a substantial   loss in reward relative to players with a common language.\footnote{   We have released code and data at \url{http://github.com/jacobandreas/neuralese}.   } 
 Fixed-vocabulary language models fail to account for one of the most characteristic statistical facts of natural language: the frequent creation and reuse of new word types. Although character-level language models offer a partial solution in that they can create word types not attested in the training corpus, they do not capture the ``bursty'' distribution of such words. In this paper, we augment a hierarchical $\mathrm{LSTM}$  language model that generates sequences of word tokens character by character with a caching mechanism that learns to reuse previously generated words.  To validate our model we construct a new open-vocabulary language modeling corpus  from comparable Wikipedia articles in 7 typologically diverse languages and demonstrate the effectiveness of our model across this range of languages. 
 	Neural models with minimal feature engineering have achieved competitive performance against traditional methods for the task of Chinese word segmentation. However, both training and working procedures of the current neural models are computationally inefficient. This paper presents a greedy neural word segmenter with balanced word and character embedding inputs to alleviate the existing drawbacks. Our segmenter is truly end-to-end, capable of performing segmentation much faster and even more accurate than state-of-the-art neural models on Chinese benchmark datasets.  	
  Parsing sentences to linguistically-expressive  semantic representations is a key goal of Natural Language Processing. Yet statistical parsing has focussed almost exclusively on bilexical  dependencies or domain-specific logical forms. We propose a neural encoder-decoder transition-based parser which is  the first full-coverage semantic graph parser for  Minimal Recursion Semantics . The model architecture uses stack-based embedding features, predicting graphs jointly with  unlexicalized predicates and their token alignments. Our parser is more accurate than attention-based baselines on MRS, and on an additional Abstract Meaning Representation  benchmark, and GPU batch processing makes it an order of magnitude faster than a  high-precision grammar-based parser. Further, the $86.69\%$ Smatch score of our MRS parser is higher than the upper-bound  on AMR parsing, making MRS an attractive choice as a semantic   representation.\footnote{Code, models and data preparation scripts are available at \url{https://github.com/janmbuys/DeepDeepParser}.}  
 Visual question answering  has attracted a lot of attention lately, seen essentially as a form of  Turing test that artificial intelligence should strive to achieve. In this paper, we study a crucial component of this task: how can we design good datasets for the task?  We focus on the design of multiple-choice based datasets where the learner has to select the right answer from a set of candidate ones including the target  and the decoys . Through careful analysis of the results attained by state-of-the-art learning models and human annotators on existing datasets, we show that the design of the decoy answers has a significant impact on how and what the learning models learn from the datasets. In particular, the resulting learner can ignore the visual information, the question, or both while still doing well on the task. Inspired by this, we propose automatic procedures  to remedy such design deficiencies. We apply the procedures to re-construct decoy answers for two popular Visual QA datasets as well as to create a new Visual QA dataset from the Visual Genome project, resulting in the largest dataset for this task. Extensive empirical studies show that the design deficiencies have been alleviated in the remedied datasets and the performance on them is likely a more faithful indicator of the difference among learning models. The datasets are released and publicly available via \url{http://www.teds.usc.edu/website_vqa/}. 
 We propose a sequence labeling framework with a secondary training objective, learning to predict surrounding words for every word in the dataset. This language modeling objective incentivises the system to learn general-purpose patterns of semantic and syntactic composition, which are also useful for improving accuracy on different sequence labeling tasks. The architecture was evaluated on a range of datasets, covering the tasks of error detection in learner texts, named entity recognition, chunking and POS-tagging. The novel language modeling objective provided consistent performance improvements on every benchmark, without requiring any additional annotated or unannotated data. 
  Tasks like code generation and semantic parsing require mapping unstructured  inputs to well-formed, executable outputs. We introduce abstract syntax networks, a modeling framework for these problems. The outputs are represented as abstract syntax trees  and constructed by a decoder with a dynamically-determined modular structure paralleling the structure of the output tree. On the benchmark \HS dataset for code generation, our model obtains 79.2 BLEU and 22.7\% exact match accuracy, compared to previous state-of-the-art values of 67.1 and 6.1\%. Furthermore, we perform competitively on the \Atis, \Jobs, and \Geo semantic parsing datasets with no task-specific engineering.  %we achieve state-of-the-art results on the \Atis and \Jobs semantic parsing datasets and competitive performance on the \Geo dataset.  %Code generation based on natural language specifications has long been a topic of interest in NLP. Recent sequence-to-sequence models for this task, while promising, do not account for programming language syntax and as a result miss out on crucial modeling and well-formedness constraints. In this work, we introduce a neural  translation architecture that allows for structure on both inputs and outputs and show  how to apply it to the problem of code generation. On the benchmark Hearthstone dataset, our model achieves 82 BLEU, compared to a previous 68 BLEU state-of-the-art. Furthermore, we show that an off-the-shelf specialization of our approach achieves state-of-the-art semantic parsing performance on the \Jobs and \Atis datasets, and is competitive on the \Geo dataset. 
    Different linguistic perspectives causes many diverse segmentation criteria for Chinese word segmentation . Most existing methods focus on improve the performance for each single criterion. However, it is interesting to exploit these different criteria and mining their common underlying knowledge. In this paper, we propose adversarial multi-criteria learning for CWS by integrating shared knowledge from multiple heterogeneous segmentation criteria.  Experiments on eight corpora with heterogeneous segmentation criteria show that the performance of each corpus obtains a significant improvement, compared to single-criterion learning. Source codes of this paper are available on Github\footnote{\url{https://github.com/FudanNLP}}. 
 While part-of-speech  tagging and dependency parsing are observed to be closely related, existing work on joint modeling with manually crafted feature templates suffers from the feature sparsity and incompleteness problems. In this paper, we propose an approach to joint POS tagging and dependency parsing using transition-based neural networks. Three neural network based classifiers are designed to resolve shift/reduce, tagging, and labeling conflicts. Experiments show that our approach significantly outperforms previous methods for joint POS tagging and dependency parsing across a variety of natural languages. 
   Language models are typically applied at the sentence level, without   access to the broader document context.  We present a neural language   model that incorporates document context in the form of a topic    model-like architecture, thus providing a succinct representation of  the   broader document context outside of the current sentence.  Experiments   over a range of datasets demonstrate that our model outperforms a pure   sentence-based model in terms of language model perplexity, and leads   to topics that are potentially more coherent than those produced by a    standard LDA topic model.  Our model also has the ability to generate  related sentences for a topic, providing another way to interpret  topics.  % Traditionally, language model focuses on modelling sentences and ignores  % the larger context surrounding the sentence of interest. We present a  % topically driven language model --- the core idea is to assimilate  % topical information from the document context so the language model has  % access to the larger narrative which is absent in a sentence. As a  % language model, our model outperforms state-of-the-art language models  % that leverage additional context, and as a topic model it generates  % topics that are at least as coherent as LDA topics. Our model provides  % another interpretable aspect for the topics, as it has the ability to  % generate sentences related to a topic.  Our findings suggest that  % language models benefit from topical information, and have implications  % for downstream tasks such as document summarisation.  
 	Mild Cognitive Impairment  is a mental disorder difficult to diagnose. Linguistic features, mainly from parsers, have been used to detect MCI, but this is not suitable for large-scale assessments. MCI disfluencies produce non-grammatical speech that requires manual or high precision automatic correction of transcripts.  In this paper, we modeled transcripts into complex networks and enriched them with word embedding  to better represent short texts produced in neuropsychological assessments. The network measurements were applied with well-known classifiers to automatically identify MCI in transcripts, in a binary classification task. A comparison was made with the performance of traditional approaches using Bag of Words  and linguistic features for three datasets: DementiaBank in English, and Cinderella and Arizona-Battery in Portuguese. Overall, CNE provided higher accuracy than using only complex networks, while Support Vector Machine was superior to other classifiers. CNE provided the highest accuracies for DementiaBank and Cinderella, but BoW was more efficient for the Arizona-Battery dataset probably owing to its short narratives. The approach using linguistic features yielded higher accuracy if the transcriptions of the Cinderella dataset were manually revised. Taken together, the results indicate that complex networks enriched with embedding is promising for detecting MCI in large-scale assessments. 
 We introduce an attention-based Bi-LSTM for Chinese implicit discourse relations and demonstrate that modeling argument pairs as a joint sequence can outperform word order-agnostic approaches.  Our model benefits from a partial sampling scheme and    % sequential modeling %By visualization of attention weights, our model provides additional means to illustrate the most distinctive features learned during relation classification.   is conceptually simple, yet achieves state-of-the-art performance on the Chinese Discourse Treebank. We also visualize its attention activity to illustrate the model's ability to selectively focus on the relevant parts of an input sequence. %We visualize its attention activity and illustrate the model's ability to distinguish between characteristic differences among relations. %A visualization of attention activities provides additional insight into the decisions made during classification.  %Implicit discourse relation recognition is a highly challenging task. The best systems to date are based on feedforward neural network architectures. These systems, however, are completely word order-agnostic. In this work, we introduce an attention-based recurrent neural network, specifically designed for Chinese implicit discourse relations. Our model is structurally simple in design, yet powerful, and achieves state-of-the-art performance on the Chinese Discourse Treebank. We demonstrate that recurrent architectures can shed a light on bla.. ignore word order. 
 Existing question answering methods infer answers either from a knowledge base or from raw text.  While knowledge base  methods are good at answering compositional questions, their performance is often affected by the incompleteness of the KB. Au contraire, %On the contrary,  web text contains millions of facts that are absent in the KB, however in an unstructured form. { to attend to the large body of facts in the combination of text and KB. Our models can be trained in an end-to-end fashion on question-answer pairs. %In this paper, we view text and KB as a unified resource for question answering.  Evaluation results on } 
   We introduce a neural semantic parser which converts natural language   utterances to intermediate   representations in the form of predicate-argument structures, which   are induced with a   transition system and subsequently mapped to target domains.    The semantic parser is trained end-to-end using annotated   logical forms or their denotations. We achieve   the state of the art on .}  
 Neural machine translation  heavily relies on an attention network to produce a context vector for each target word prediction. In practice, we find that context vectors for different target words are quite similar to one another and therefore are insufficient in discriminatively predicting target words. The reason for this might be that context vectors produced by the vanilla attention network are just a weighted sum of source representations that are invariant to decoder states. In this paper, we propose a novel GRU-gated attention model  for NMT which enhances the degree of discrimination of context vectors by enabling source representations to be sensitive to the partial translation generated by the decoder. GAtt uses a gated recurrent unit  to combine two types of information: treating a source annotation vector originally produced by the bidirectional encoder as the history state while the corresponding previous decoder state as the input to the GRU. The GRU-combined information forms a new source annotation vector. In this way, we can obtain translation-sensitive source representations which are then feed into the attention network to generate discriminative context vectors. We further propose a variant that regards a source annotation vector as the current input while the previous decoder state as the history. Experiments on NIST Chinese-English translation tasks show that both GAtt-based models achieve significant improvements over the vanilla attention-based NMT. Further analyses on attention weights and context vectors demonstrate the effectiveness of GAtt in improving the discrimination power of representations and handling the challenging issue of over-translation. 
   This paper aims to catalyze research discussions about text feature extraction techniques using neural network architectures.   The research questions discussed here focus on the state-of-the-art neural network techniques that have proven to be useful tools for language processing, language generation, text classification and other computational linguistics tasks. 
 $^{\diamond}$Work done partly during an internship at the Allen Institute for Artificial Intelligence.}   
 Neural word segmentation research has benefited from large-scale raw texts by leveraging them for pretraining character and word embeddings. On the other hand, statistical segmentation research has exploited richer sources of external information, such as punctuation, automatic segmentation and POS. We investigate the effectiveness of a range of external training sources for neural word segmentation by building a modular segmentation model, pretraining the most important submodule using rich external sources. Results show that such pretraining significantly improves the model, leading to accuracies competitive to the best methods on six benchmarks. 
 We explore the properties of byte-level recurrent language models. When given sufficient amounts of capacity, training data, and compute time, the representations learned by these models include disentangled features corresponding to high-level concepts. Specifically, we find a single unit which performs sentiment analysis. These representations, learned in an unsupervised manner, achieve state of the art on the binary subset of the Stanford Sentiment Treebank. They are also very data efficient. When using only a handful of labeled examples, our approach matches the performance of strong baselines trained on full datasets. We also demonstrate the sentiment unit has a direct influence on the generative process of the model. Simply fixing its value to be positive or negative generates samples with the corresponding positive or negative sentiment. 
 Recently, deep learning methods have been shown to improve the performance of recommender systems over traditional methods, especially when review text is available.  For example, a recent model, DeepCoNN, uses neural nets to learn one latent representation for the text of all reviews written by a target user, and a second latent representation for the text of all reviews for a target item, and then combines these latent representations to obtain state-of-the-art performance on recommendation tasks.  We show that  much of the predictive value of review text comes from reviews of the target user for the target item. We then introduce a way in which this information can be used in recommendation, even when the target user's review for the target item is not available.  Our model, called TransNets, extends the DeepCoNN model by introducing an additional latent layer representing the target user-target item pair.  We then regularize this layer, at training time, to be similar to another latent representation of the target user's review of the target item.  We show that TransNets and extensions of it improve substantially over the previous state-of-the-art. 
 Voice conversion  using sequence-to-sequence learning of context posterior probabilities is proposed. Conventional VC using shared context posterior probabilities predicts target speech parameters from the context posterior probabilities estimated from the source speech parameters. Although conventional VC can be built from non-parallel data, it is difficult to convert speaker individuality such as phonetic property and speaking rate contained in the posterior probabilities because the source posterior probabilities  are directly used for predicting target speech parameters.  In this work, we assume that the training data partly include parallel speech data and  propose sequence-to-sequence learning between the source and target posterior probabilities.   The conversion models perform non-linear and variable-length   transformation from the source probability sequence to the target one.   Further, we propose a joint training algorithm for the modules.   In contrast to conventional VC, which separately trains the speech recognition that estimates   posterior probabilities and the speech synthesis that predicts target speech parameters,   our proposed method jointly trains these modules along with the proposed probability conversion modules. Experimental results demonstrate that our approach outperforms the conventional VC. 
 We present a new model for singing synthesis based on a modified version of the WaveNet architecture. Instead of modeling raw waveform, we model features produced by a parametric vocoder that separates the influence of pitch and timbre. This allows conveniently modifying pitch to match any target melody, facilitates training on more modest dataset sizes, and significantly reduces training and generation times. Our model makes frame-wise predictions using mixture density outputs rather than categorical outputs in order to reduce the required parameter count. As we found overfitting to be an issue with the relatively small datasets used in our experiments, we propose a method to regularize the model and make the autoregressive generation process more robust to prediction errors. Using a simple multi-stream architecture, harmonic, aperiodic and voiced/unvoiced components can all be predicted in a coherent manner. We compare our method to existing parametric statistical and state-of-the-art concatenative methods using quantitative metrics and a listening test. While naive implementations of the autoregressive generation algorithm tend to be inefficient, using a smart algorithm we can greatly speed up the process and obtain a system that's competitive in both speed and quality. 
 In order to adopt deep learning for information retrieval, models are needed that can capture all relevant information required to assess the relevance of a document to a given user query. While previous works have successfully captured unigram term matches, how to fully employ  position-dependent information such as proximity and term dependencies has been insufficiently explored.  In this work, we propose a novel neural IR model named PACRR aiming at better modeling position-dependent interactions between a  query and a document. Extensive experiments on six years' Trec Web Track data confirm that the proposed model yields better results under multiple benchmarks. 
 Recurrent Neural Networks  are widely used to solve a variety of problems and as the quantity of data and the amount of available compute have increased, so have model sizes. The number of parameters in recent state-of-the-art networks makes them hard to deploy, especially on mobile phones and embedded devices. The challenge is due to both the size of the model and the time it takes to evaluate it.  In order to deploy these RNNs efficiently, we propose a technique to reduce the parameters of a network by pruning weights during the initial training of the network. At the end of training, the parameters of the network are sparse while accuracy is still close to the original dense neural network. The network size is reduced by 8$\times$ and the time required to train the model remains constant. Additionally, we can prune a larger dense network to achieve better than baseline performance while still reducing the total number of parameters significantly. Pruning RNNs reduces the size of the model and can also help achieve significant inference time speed-up using sparse matrix multiply. Benchmarks show that using our technique model size can be reduced by 90\% and speed-up is around 2$\times$ to 7$\times$.  
 Automatic affect recognition is a challenging task due to the various modalities emotions can be expressed with. Applications can be found in many domains including multimedia retrieval and human computer interaction. In recent years, deep neural networks have been used with great success in determining emotional states. Inspired by this success, we propose an emotion recognition system using auditory and visual modalities. To capture the emotional content for various styles of speaking, robust features need to be extracted. To this purpose, we utilize a Convolutional Neural Network  to extract features from the speech, while for the visual modality a deep residual network  of 50 layers. In addition to the importance of feature extraction, a machine learning algorithm needs also to be insensitive to outliers while being able to model the context. To tackle this problem, Long Short-Term Memory  networks are utilized. The system is then trained in an end-to-end fashion where é–³ by also taking advantage of the correlations of the each of the streams é–³ we manage to significantly outperform the traditional approaches based on auditory and visual handcrafted features for the prediction of spontaneous and natural emotions on the RECOLA database of the AVEC 2016 research challenge on emotion recognition. 
 Despite the impressive improvements achieved by  deep neural networks in computer vision and NLP tasks, such improvements have not yet been observed in ranking for information retrieval. The reason may be the complexity of the ranking problem, as it is not obvious how to learn from queries and documents when no supervised signal is available. Hence, in this paper, we propose to train a neural ranking model using , where labels are obtained automatically without human annotators or any external resources .  To this aim, we use the output of an unsupervised ranking model, such as BM25, as a weak supervision signal. We further train a set of simple yet effective ranking models based on feed-forward neural networks. We study their effectiveness under various learning scenarios  and using different input representations .  We train our networks using tens of millions of training instances and evaluate it on two standard collections: a homogeneous news collection   and a heterogeneous large-scale web collection . Our experiments indicate that employing proper objective functions and letting the networks to learn the input representation based on weakly supervised data leads to impressive performance, with over 13\% and 35\% MAP improvements over the BM25 model on the Robust and the ClueWeb collections.  Our findings also suggest that supervised neural ranking models can greatly benefit from pre-training on large amounts of weakly labeled data that can be easily obtained from unsupervised IR models.  % ~~~ Ranking model, weak supervision, deep neural network, deep learning, ad-hoc retrieval 
 %\alert{KC: TODO}  We propose a neural machine translation architecture that models the surrounding text in addition to the source sentence. These models lead to better performance, both in terms of general translation quality and pronoun prediction, when trained on small corpora, although this improvement largely disappears when trained with a larger corpus. We also discover that attention-based neural machine translation is well suited for pronoun prediction and compares favorably with other approaches that were specifically designed for this task.  
  We study automatic question generation for sentences from text passages in reading comprehension. We introduce an attention-based sequence learning model for the task and investigate the effect of encoding sentence- vs.\ paragraph-level information. In contrast to all previous work, our model does not rely on hand-crafted rules or a sophisticated NLP pipeline;  it is instead trainable end-to-end via sequence-to-sequence learning. Automatic evaluation results show that our system significantly outperforms the state-of-the-art rule-based system. In human evaluations, questions generated by our system are also rated as being more natural  and as more difficult to answer . 
 % One of the key tasks of sentiment analysis of product reviews is to extract product aspects or features that users have expressed opinions on. In this work,  This paper makes a focused contribution to supervised aspect extraction. It shows that if the system has performed aspect extraction from many past domains and retained their results as knowledge, Conditional Random Fields  can leverage this knowledge in a lifelong learning manner to extract in a new domain markedly better than the traditional CRF without using this prior knowledge. The key innovation is that even after CRF training, the model can still improve its extraction with experiences in its applications.  % with unlabeled data.  % The proposed method by exploiting sharing of aspects in multiple tasks using Conditional Random Fields. Although CRF and Hidden Markov Models  have been used for aspect extraction before, we show that by exploiting multiple tasks in CRF, significantly better results can be achieved.  % much better results. proposed, we show that this supervised approach can be significantly improved by exploiting the idea of concept sharing across multiple domains. For example, ``screen'' is an aspect in iPhone, but not only iPhone has a screen, many electronic devices have screens too. When ``screen'' appears in a review of a new domain , it is likely to be an aspect too. Knowing this information enables us to do much better extraction in the new domain. This paper proposes a novel extraction method exploiting this idea in the context of supervised sequence labeling. Experimental results show that it produces markedly better results than without using the past information. 
 %Yet preventing and diagnosing mental health problems is difficult because it requires patients to recognize the associated symptoms and actively seek help. Moreover,    Mental illnesses adversely affect a significant proportion of the population worldwide.  However, the methods traditionally used for estimating and characterizing the prevalence of mental health conditions are time-consuming and expensive. Consequently, best-available estimates concerning the prevalence of mental health conditions are often years out of date. Automated approaches to supplement these survey methods with broad, aggregated information derived from social media content provides a potential means for near real-time estimates at scale. These may, in turn, provide grist for supporting, evaluating and iteratively improving upon public health programs and interventions.    %At the same time, web-based social networks provide a comfortable medium for people to anonymously connect with others affected by similar issues. This provides an opportunity to develop automated tools for identifying at-risk individuals early on, thus affording the opportunity for effective prevention and treatment. In aggregate, such automated monitoring at scale may allow more accurate estimates concerning the extent of population-level mental illness incidence.    % bcw 4/23 -- would be interested in hearing Glen's take, but I find the epidemiological view more convincing here rather than diagnosing/detecting individuals    We propose a novel model for automated mental health status quantification that incorporates . This builds upon recent work exploring  methods that induce embeddings by leveraging social media post histories. Such embeddings capture latent characteristics of individuals  and encode a soft notion of homophily. In this paper, we investigate whether user embeddings learned from twitter post histories encode information that correlates with mental health statuses. To this end, we estimated user embeddings for a set of users known to be affected by depression and post-traumatic stress disorder , and for a set of demographically matched `control' users. We then evaluated these embeddings with respect to:  their ability to capture homophilic relations with respect to mental health status; and  the performance of downstream mental health prediction models based on these features. Our experimental results demonstrate that the user embeddings capture similarities between users with respect to mental conditions, and are predictive of mental health.     
 We consider the problem of learning general-purpose, paraphrastic sentence embeddings, revisiting the setting of . While they found LSTM recurrent networks to underperform word averaging,  we present several developments that together produce the opposite conclusion.  These include  training on sentence pairs rather than phrase pairs,  averaging states to represent sequences, and regularizing aggressively.  These improve LSTMs in both transfer learning and supervised settings. We also introduce a new recurrent architecture, the \ganlong, that is inspired by averaging and LSTMs while outperforming them both.  We analyze our learned models, finding evidence of preferences for particular parts of speech and dependency relations.  \footnote{Trained models and code are available at \url{http://ttic.uchicago.edu/~wieting}.} 
 The quality of a Neural Machine Translation system depends substantially on the availability of sizable parallel corpora. For low-resource language pairs this is not the case, resulting in poor translation quality.  Inspired by work in computer vision, we propose a novel data augmentation approach that targets low-frequency words by generating new sentence pairs containing rare words in new, synthetically created contexts. Experimental results on simulated low-resource settings show that our method improves translation quality by up to 2.9 BLEU points over the baseline and up to 3.2 BLEU over back-translation. 
 Distributed word representations are widely used for modeling words in NLP tasks. Most of the existing models generate one representation per word and do not consider different meanings of a word.    % We present two approaches to learn multiple topic-sensitive representations per word by using Hierarchical Dirichlet Process. We observe that by modeling topics and integrating topic distributions for each document  we obtain representations that are able to distinguish between different meanings of a given word. Our models yield statistically significant improvements for the lexical substitution task  indicating that commonly used single word representations, even when combined with contextual information, are insufficient for this task.  
  The vast amount of data and increase of computational capacity have allowed the analysis of texts from several perspectives, including the representation of texts as complex networks. Nodes of the network represent the words, and edges represent some relationship, usually word co-occurrence. Even though networked representations have been applied to study some tasks, such approaches are not usually combined with traditional models relying upon statistical paradigms. Because networked models are able to grasp textual patterns, we devised a hybrid classifier, called , that combines the frequency of common words with small structures found in the topology of the network, known as motifs. Our approach is illustrated in two contexts, authorship attribution and translationese identification. In the former, a set of novels written by different authors is analyzed. To identify translationese, texts from the Canadian Hansard and the European parliament were classified as to original and translated instances. Our results suggest that labelled motifs are able to represent texts and it should be further explored in other tasks, such as the analysis of text complexity, language proficiency, and machine translation. 
 This work presents a novel objective function for the unsupervised training of neural network sentence encoders. It exploits signals from paragraph-level discourse coherence  to train these models to understand text. Our objective is purely discriminative, allowing us to train models many times faster than was possible under prior methods, and it yields models which perform well in extrinsic evaluations.  
 While end-to-end neural machine translation  has made remarkable progress recently, it still suffers from the data scarcity problem for low-resource language pairs and domains. In this paper, we propose a method for zero-resource NMT by assuming that parallel sentences have close probabilities of generating a sentence in a third language. Based on this assumption, our method is able to train a source-to-target NMT model  without parallel corpora available, guided by an existing pivot-to-target NMT model  on a source-pivot parallel corpus. Experimental results show that the proposed method significantly improves over a baseline pivot-based model by +3.0 BLEU points across various language pairs. 
 Deep Neural Networks  have provably enhanced the state-of-the-art Neural  Machine Translation  with  their capability in modeling complex functions and capturing   complex linguistic structures.   However NMT systems with deep architecture in their encoder or   decoder RNNs often suffer from severe gradient diffusion   due to the non-linear recurrent activations, which often   make the optimization much more difficult.    To address this problem we propose  novel linear    associative units   to reduce the gradient     propagation length inside the recurrent unit.     Different from conventional     approaches ,    LAUs utilizes linear associative connections    between input and    output of the recurrent unit,    which allows unimpeded information flow through both     space and time direction.  The model is quite simple,      but it is surprisingly effective. Our empirical      study on Chinese-English translation shows that our      model with proper configuration can improve       by 11.7 BLEU upon Groundhog and the best       reported  results in the same setting.       On WMT14 English-German task and a larger WMT14        English-French task, our  model achieves comparable results with the state-of-the-art. 
 Even though a linguistics-free sequence to sequence model in neural machine translation  has certain capability of implicitly learning syntactic information of source sentences, this paper shows that source syntax can be explicitly incorporated into NMT effectively to provide further improvements. Specifically, we linearize parse trees of source sentences to obtain structural label sequences. On the basis, we propose three different sorts of encoders to incorporate source syntax into NMT: 1) Parallel RNN encoder that learns word and label annotation vectors parallelly; 2) Hierarchical RNN encoder that learns word and label annotation vectors in a two-level hierarchy; and 3) Mixed RNN encoder that stitchingly learns word and label annotation vectors over sequences where words and labels are mixed. Experimentation on Chinese-to-English translation demonstrates that all the three proposed syntactic encoders are able to improve translation accuracy. It is interesting to note that the simplest RNN encoder, i.e., Mixed RNN encoder yields the best performance with an significant improvement of 1.4 BLEU points. Moreover, an in-depth analysis from several perspectives is provided to reveal how source syntax benefits NMT.  
 This paper describes the Amobee sentiment analysis system, adapted to compete in SemEval 2017 task 4. The system consists of two parts: a supervised training of RNN models based on a Twitter sentiment treebank, and the use of feedforward NN, Naive Bayes and logistic regression classifiers to produce predictions for the different sub-tasks. The algorithm reached the 3rd place on the 5-label classification task . 
    Linguistic typology studies the range of structures present in human   language. The main goal of the field is to discover which sets of   possible phenomena are universal, and which are merely frequent. For   example, all languages have vowels, while most---but not   all---languages have an \phone{u} sound. In this paper we present the first   probabilistic treatment of a basic question in phonological   typology: What makes a natural vowel inventory?  We introduce a   series of deep stochastic point processes, and contrast them with   previous computational, simulation-based approaches.  We provide a comprehensive suite of experiments on over 200 distinct languages. 
 Attentional sequence-to-sequence models have become the new standard for machine translation, but one challenge of such models is a significant increase in training and decoding cost compared to phrase-based systems. Here, we focus on efficient decoding, with a goal of achieving accuracy close the state-of-the-art in neural machine translation , while achieving CPU decoding speed/throughput close to that of a phrasal decoder.  We approach this problem from two angles: First, we describe several techniques for speeding up an NMT beam search decoder, which obtain a 4.4x speedup over a very efficient baseline decoder without changing the decoder output. Second, we propose a simple but powerful network architecture which uses an RNN  layer at bottom, followed by a series of stacked fully-connected layers applied at every timestep. This architecture achieves similar accuracy to a deep recurrent model, at a small fraction of the training and decoding cost. By combining these techniques, our best system achieves a very competitive accuracy of 38.3 BLEU on WMT English-French NewsTest2014, while decoding at 100 words/sec on single-threaded CPU. We believe this is the best published accuracy/speed trade-off of an NMT system. 
     We propose a recurrent neural model that generates natural-language questions from documents, conditioned on answers. We show how to train the model using a combination of supervised and reinforcement learning. After teacher forcing for standard maximum likelihood training, we fine-tune the model using policy gradient techniques to maximize several rewards that measure question quality. Most notably, one of these rewards is the performance of a question-answering system. Our model is trained and evaluated on the recent question-answering dataset . 
 This paper presents Senti17 system which uses ten convolutional neural networks  to assign a sentiment label to a tweet. The network consists of a convolutional layer followed by a fully-connected layer and a Soft- max on top. Ten instances of this network are initialized with the same word embeddings  as inputs but with different initializations for the network weights. We combine the results of all instances by selecting the sentiment label given by the majority of the ten voters. This system is ranked fourth in SemEval-2017 Task4 over 38 systems with 67.4\% average recall. 
 We present Deep Speaker, a neural speaker embedding system that maps utterances to a hypersphere where speaker similarity is measured by cosine similarity. The embeddings generated by Deep Speaker can be used for many tasks, including speaker identification, verification, and clustering. We experiment with ResCNN and GRU architectures to extract the acoustic features, then mean pool to produce utterance-level speaker embeddings, and train using triplet loss based on cosine similarity. Experiments on three distinct datasets suggest that Deep Speaker outperforms a DNN-based i-vector baseline. For example, Deep Speaker reduces the verification equal error rate by 50\%  and improves the identification accuracy by 60\%  on a text-independent dataset. We also present results that suggest adapting from a model trained with Mandarin can improve accuracy for English speaker recognition. 
 Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. % Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. %  Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. % In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors  on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available\footnote{\url{https://www.github.com/facebookresearch/InferSent}}. %  
 Automatically assessing emotional valence in human speech has historically been a difficult task for machine learning algorithms. The subtle changes in the voice of the speaker that are indicative of positive or negative emotional states are often "overshadowed" by voice characteristics relating to emotional intensity or emotional activation. In this work we explore a representation learning approach that automatically derives discriminative representations of emotional speech. In particular, we investigate two machine learning strategies to improve classifier performance:  utilization of unlabeled data using a deep convolutional generative adversarial network , and  multitask learning. Within our extensive experiments we leverage a multitask annotated emotional corpus as well as a large unlabeled meeting corpus . Our speaker-independent classification experiments show that in particular the use of unlabeled data in our investigations improves performance of the classifiers and both fully supervised baseline approaches are outperformed considerably. We improve the classification of emotional valence on a discrete 5-point scale to 43.88\% and on a 3-point scale to 49.80\%, which is competitive to state-of-the-art performance. 
 Abundant data is the key to successful machine learning. However, supervised learning requires annotated data that are often hard to obtain.  In a classification task with limited resources, Active Learning  promises to guide annotators to examples that bring the most value for a classifier. AL can be successfully combined with self-training, i.e., extending a training set with the unlabelled examples for which a classifier is the most certain. We report our experiences on using AL in a systematic manner to train an SVM classifier for Stack Overflow posts discussing performance of software components.  We show that the training examples deemed as the most valuable to the classifier are also the most difficult for humans to annotate. Despite carefully evolved annotation criteria, we report low inter-rater  agreement, but we also propose mitigation strategies. Finally, based on one annotator's work, we show that self-training can improve the classification accuracy. We conclude the paper by discussing implication for future text miners aspiring to use AL and self-training. 
 We propose a max-pooling based loss function for training Long Short-Term Memory  networks for small-footprint keyword spotting , with low CPU, memory, and latency requirements. The max-pooling loss training can be further guided by initializing with a cross-entropy loss trained network. A posterior smoothing based evaluation approach is employed to measure keyword spotting performance. Our experimental results show that LSTM models trained using cross-entropy loss or max-pooling loss outperform a cross-entropy loss trained baseline feed-forward Deep Neural Network . In addition, max-pooling loss trained LSTM with randomly initialized network performs better compared to cross-entropy loss trained LSTM. Finally, the max-pooling loss trained LSTM  initialized with a cross-entropy pre-trained network shows the best performance, which yields $67.6\%$ relative reduction compared to baseline feed-forward DNN in Area Under the Curve  measure. 
 Nowadays, geographic information related to Twitter is crucially important for fine-grained applications. However, the amount of geographic information avail- able on Twitter is low, which makes the pursuit of many applications challenging. Under such circumstances, estimating the location of a tweet is an important goal of the study. Unlike most previous studies that estimate the pre-defined district as the classification task, this study employs a probability distribution to represent richer information of the tweet, not only the location but also its ambiguity. To realize this modeling, we propose the convolutional mixture density network , which uses text data to estimate the mixture model parameters. Experimentally obtained results reveal that CMDN achieved the highest prediction performance among the method for predicting the exact coordinates. It also provides a quantitative representation of the location ambiguity for each tweet that properly works for extracting the reliable location estimations.  
 		 		 		Deep neural models, particularly the LSTM-RNN model, have shown great potential for language identification . However, the use of phonetic information has been largely overlooked by most existing neural LID methods, although this information has been used very successfully in conventional phonetic LID systems.  We present a phonetic temporal neural model for LID, which is an LSTM-RNN LID system that accepts phonetic features produced by a phone-discriminative DNN as the input, rather than raw acoustic features.  This new model is similar to traditional phonetic LID methods, but the phonetic knowledge here is much richer: it is at the frame level and involves compacted information of all phones. Our experiments conducted on the Babel database and the AP16-OLR database demonstrate that the temporal phonetic neural approach is very effective, and significantly outperforms existing acoustic neural models. It also outperforms the conventional i-vector approach on short utterances and in noisy conditions. 		 		 		 		 	
   Knowledge graphs , which could provide essential relational information between entities, have been widely utilized in various knowledge-driven applications. Since the overall human knowledge is innumerable that still grows explosively and changes frequently, knowledge construction and update inevitably involve automatic mechanisms with less human supervision, which usually bring in plenty of noises and conflicts to KGs. However, most conventional knowledge representation learning methods assume that all triple facts in existing KGs share the same significance without any noises. To address this problem, we propose a novel confidence-aware knowledge representation learning framework , which detects possible noises in KGs while learning knowledge representations with confidence simultaneously. Specifically, we introduce the triple confidence to conventional translation-based methods for knowledge representation learning. To make triple confidence more flexible and universal, we only utilize the internal structural information in KGs, and propose three kinds of triple confidences considering both local and global structural information. In experiments, We evaluate our models on knowledge graph noise detection, knowledge graph completion and triple classification. Experimental results demonstrate that our confidence-aware models achieve significant and consistent improvements on all tasks, which confirms the capability of CKRL modeling confidence with structural information in both KG noise detection and knowledge representation learning. 
  Drug-drug interaction  is a vital information when physicians and pharmacists intend to co-administer two or more drugs. Thus, several DDI databases are constructed to avoid mistakenly combined use. In recent years, automatically extracting DDIs from biomedical text has drawn researchers' attention. However, the existing work utilize either complex feature engineering or NLP tools, both of which are insufficient for sentence comprehension. Inspired by the deep learning approaches in natural language processing, we propose a recurrent neural network model with multiple attention layers for DDI classification. We evaluate our model on 2013 SemEval DDIExtraction dataset. The experiments show that our model classifies most of the drug pairs into correct DDI categories, which outperforms the existing NLP or deep learning methods. 
 Semantic parsing has emerged as a significant and powerful paradigm for natural language interface and question answering systems. Traditional methods of building a semantic parser rely on high-quality lexicons, hand-crafted grammars and linguistic features which are limited by applied domain or representation. In this paper, we propose a general approach to learn from denotations based on Seq2Seq model augmented with attention mechanism. We encode input sequence into vectors and use dynamic programming to infer candidate logical forms. We utilize the fact that similar utterances should have similar logical forms to help reduce the searching space. Under our learning policy, the Seq2Seq model can learn mappings gradually with noises. Curriculum learning is adopted to make the learning smoother. We test our method on the arithmetic domain which shows our model can successfully infer the correct logical forms and learn the word meanings, compositionality and operation orders simultaneously. 
  DeepTingle is a text prediction and classification system trained on the collected works of the renowned fantastic gay erotica author Chuck Tingle. Whereas the writing assistance tools you use everyday  are trained on generic, purportedly ``neutral'' datasets, DeepTingle is trained on a very specific, internally consistent but externally arguably eccentric dataset. This allows us to foreground and confront the norms embedded in data-driven creativity and productivity assistance tools. As such tools effectively function as extensions of our cognition into technology, it is important to identify the norms they embed within themselves and, by extension, us. DeepTingle is realized as a web application based on LSTM networks and the GloVe word embedding, implemented in JavaScript with Keras-JS.  
 Relation Extraction is an important subtask of Information Extraction which has the potential of employing deep learning  models with the creation of large datasets using distant supervision. In this review, we compare the contributions and pitfalls of the various DL models that have been used for the task, to help guide the path ahead. 
 In this work, we present a minimal neural model for constituency parsing based on independent scoring of labels and spans. We show that this model is not only compatible with classical dynamic programming techniques, but also admits a novel greedy top-down inference algorithm based on recursive partitioning of the input. We demonstrate empirically that both prediction schemes are competitive with recent work, and when combined with basic extensions to the scoring model are capable of achieving state-of-the-art single-model performance on the Penn Treebank  and strong performance on the French Treebank . 
   This paper demonstrates end-to-end neural network architectures for   Vietnamese named entity recognition. Our best model is a   combination of bidirectional Long Short-Term Memory ,   Convolutional Neural Network , Conditional Random Field ,   using pre-trained word embeddings as input, which achieves an   $F_{1}$ score of 88.59\% on a standard test set.  Our system is able   to achieve a comparable performance to the first-rank system of the   VLSP campaign without using any syntactic or hand-crafted   features. We also give an extensive empirical study on using common   deep learning models for Vietnamese NER, at both word and character   level.   
 Tree-structured neural networks have proven to be effective in learning semantic representations by exploiting syntactic information. In spite of their success, most existing models suffer from the underfitting problem: they recursively use the same shared compositional function throughout the whole compositional process and lack expressive power due to inability to capture the richness of compositionality. In this paper, we address this issue by introducing the dynamic compositional neural networks over tree structure , in which the compositional function is dynamically generated by a meta network. The role of meta-network is to capture the metaknowledge across the different compositional rules and formulate them. Experimental results on two typical tasks show the effectiveness of the proposed models. 
 Well-established automatic analyses of texts mainly consider frequencies of linguistic units, e.g. letters, words and bigrams, while methods based on co-occurrence networks consider the structure of texts regardless of the nodes label . In this paper, we reconcile these distinct viewpoints by introducing a generalized similarity measure to compare texts which accounts for both the network structure of texts and the role of individual words in the networks. We use the similarity measure for authorship attribution of three collections of books, each composed of $8$ authors and $10$ books per author. High accuracy rates were obtained with typical values from $90\%$ to $98.75\%$, much higher than with the traditional the TF-IDF approach for the same collections. These accuracies are also higher than taking only the topology of networks into account. We conclude that the different properties of specific words on the macroscopic scale structure of a whole text are as relevant as their frequency of appearance; conversely, considering the identity of nodes brings further knowledge about a piece of text represented as a network. 
 Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning .  Models trained only with supervised learning often exhibit ``exposure bias'' -- they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries.  
 We consider the problem of translating high-level textual descriptions to formal representations in technical documentation as part of an effort to model the meaning of such documentation.  We focus specifically on the problem of learning translational correspondences between text descriptions and grounded representations in the target documentation, such as formal representation of functions or code templates.  Our approach exploits the parallel nature of such documentation, or the tight coupling between high-level text and the low-level representations we aim to learn. Data is collected by mining technical documents for such parallel text-representation pairs, which we use to train a simple semantic parsing model. We report new baseline results on sixteen novel datasets, including the standard library documentation for nine popular programming languages across seven natural languages, and a small collection of Unix utility manuals. %\footnote{All datasets and associated code will be distributed upon publication.}  %%  representations grounded in the target documentation collections, such as formal representations of functions and short code templates.   %, as a first step towards modeling the meaning of such documentation.   %, and a small collection of Unix utility manuals. \footnote{All datasets and associated code will be distributed upon publication.}   
 Drug repositioning  refers to identification of novel indications for the approved drugs. The requirement of huge investment of time as well as money and risk of failure in clinical trials have led to surge in interest in drug repositioning. DR exploits two major aspects associated with drugs and diseases:  existence of similarity among drugs and among diseases due to their shared involved genes or pathways or common biological effects. Existing methods of identifying drug-disease association majorly rely  on the information available in the structured  databases only. On the other hand, abundant information available in form of free texts in biomedical  research articles are not being fully exploited. Word-embedding or obtaining vector representation of words from a large corpora of free texts using neural network methods have been shown to give significant performance  for several natural language processing tasks. In this work we propose a novel way of representation learning to obtain features of drugs and diseases by combining complementary information available in unstructured texts  and structured datasets. Next we use matrix completion approach on these feature vectors to learn projection matrix between  drug and disease vector spaces. The proposed method has shown  competitive performance with state-of-the-art methods. Further, the case  studies on Alzheimer's and Hypertension diseases have shown that the predicted associations are matching with the existing  knowledge.  %\boldmath %\blindtext[1] 
   Neural task-oriented dialogue systems often struggle to smoothly interface with a knowledge base. In this work, we seek to address this problem by proposing a new neural dialogue agent that is able to effectively sustain grounded, multi-domain discourse through a novel key-value retrieval mechanism. The model is end-to-end differentiable and does not need to explicitly model dialogue state or belief trackers. We also release a new dataset of 3,031 dialogues that are grounded through underlying knowledge bases and span three distinct tasks in the in-car personal assistant space: calendar scheduling, weather information retrieval, and point-of-interest navigation. Our architecture is simultaneously trained on data from all domains and significantly outperforms a competitive rule-based system and other existing neural dialogue architectures on the provided domains according to both automatic and human evaluation metrics.    
 Named-entity recognition  aims at identifying entities of interest in a text. Artificial neural networks  have recently been shown to outperform existing NER systems. However, ANNs remain challenging to use for non-expert users. In this paper, we present NeuroNER, an easy-to-use named-entity recognition tool based on ANNs.  Users can annotate entities using a graphical web-based user interface : the annotations are then used to train an ANN, which in turn predict entities' locations and categories in new texts. NeuroNER  makes this annotation-training-prediction flow smooth and accessible to anyone. 
 % We present a novel neural network model that learns POS tagging and graph-based dependency parsing jointly.  Our model uses bidirectional LSTMs to learn  feature representations shared for both POS tagging and   dependency parsing tasks, thus handling the feature-engineering problem.  Our extensive experiments, on 19 languages from the Universal Dependencies project, show that our model outperforms the state-of-the-art neural network-based Stack-propagation model for joint POS tagging and transition-based dependency parsing, resulting in a new state of the art.  Our code is open-source and available together with pre-trained models at: \url{https://github.com/datquocnguyen/jPTDP}.  \medskip  Keywords: Neural network, POS tagging, Dependency parsing, Bidirectional LSTM, Universal Dependencies, Multilingual parsing. 
 Frame stacking is broadly applied in end-to-end neural network training like connectionist temporal classification , and it leads to more accurate models and faster decoding. However, it is not well-suited to conventional neural network based on context-dependent state acoustic model, if the decoder is unchanged. In this paper, we propose a novel frame retaining method which is applied in decoding. The system which combined frame retaining with frame stacking could reduces the time consumption of both training and decoding. Long short-term memory  recurrent neural networks  using it achieve almost linear training speedup and reduces relative 41\% real time factor . At the same time, recognition performance is no degradation or improves sightly on Shenma voice search dataset in Mandarin.    
 Accuracy is one of the basic principles of journalism. However, it is increasingly hard to manage due to the diversity of news media. Some editors of online news tend to use catchy headlines which trick readers into clicking. These headlines are either ambiguous or misleading, degrading the reading experience of the audience. Thus, identifying inaccurate news headlines is a task worth studying. Previous work names these headlines ``clickbaits'' and mainly focus on the features extracted from the headlines, which limits the performance since the consistency between headlines and news bodies is underappreciated. In this paper, we clearly redefine the problem and identify ambiguous and misleading headlines separately. We utilize class sequential rules to exploit structure information when detecting ambiguous headlines. For the identification of misleading headlines, we extract features based on the congruence between headlines and bodies. To make use of the large unlabeled data set, we apply a co-training method and gain an increase in performance. The experiment results show the effectiveness of our methods. Then we use our classifiers to detect inaccurate headlines crawled from different sources and conduct a data analysis. 
 Recent approaches based on artificial neural networks  have shown promising results for named-entity recognition . In order to achieve high performances, ANNs need to be trained on a large labeled dataset. However, labels might be difficult to obtain for the dataset on which the user wants to perform NER: label scarcity  is particularly pronounced for patient note de-identification, which is an instance  of NER. In this work, we analyze to what extent transfer learning may address this issue. In particular, we demonstrate that transferring an ANN model trained on a large labeled dataset to another dataset with a limited number of labels improves upon the state-of-the-art results on two different datasets for patient note de-identification. 
 In this paper, we extend an attention-based neural machine translation  model by allowing it to access an entire training set of parallel sentence pairs even after training. The proposed approach consists of two stages. In the first stage--retrieval stage--, an off-the-shelf, black-box search engine is used to retrieve a small subset of sentence pairs from a training set given a source sentence. These pairs are further filtered based on a fuzzy matching score based on edit distance. In the second stage--translation stage--, a novel translation model, called search engine guided NMT , seamlessly uses both the source sentence and a set of retrieved sentence pairs to perform the translation. Empirical evaluation on three language pairs  shows that the proposed approach significantly outperforms the baseline approach and the improvement is more significant when more relevant sentence pairs were retrieved.  
 We introduce  , a new gated RNN which is distinguished by the use of purely additive latent state updates. At every time step, the new state is computed as a gated component-wise sum of the input and the previous state, without any of the non-linearities commonly used in RNN transition dynamics. We formally show that RAN states are weighted sums of the input vectors, and that the gates only contribute to computing the weights of these sums. Despite this relatively simple functional form, experiments demonstrate that RANs perform on par with LSTMs on benchmark language modeling problems. This result shows that many of the non-linear computations in LSTMs and related networks are not essential, at least for the problems we consider, and suggests that the gates are doing more of the computational work than previously understood. %that is much simpler than existing approaches , produces highly interpretable outputs, and improves performance significantly on benchmark language modeling  tasks. 
   We frame Question Answering  as a Reinforcement Learning task, an approach that  we call . We propose an agent that sits between  the user and a black box QA system and learns to reformulate questions to elicit the best possible answers. The agent probes the system with, potentially many, natural language reformulations of an initial question and aggregates the returned evidence to yield the best answer. The reformulation system is trained end-to-end to maximize answer quality using policy gradient. We evaluate on SearchQA, a dataset of complex questions extracted from . The agent outperforms a state-of-the-art base model, playing the role of the environment, and other benchmarks. We also analyze the language that the agent has learned while interacting with the question answering system. We find that successful question reformulations look quite different from natural language paraphrases. The agent is able to discover non-trivial reformulation strategies that resemble classic information retrieval techniques such as term re-weighting  and stemming.   
     We introduce a technique for augmenting neural text-to-speech~ with low-dimensional trainable speaker embeddings to generate different voices from a single model. As a starting point, we show improvements over the two state-of-the-art approaches for single-speaker neural TTS: Deep Voice~1 and Tacotron. We introduce Deep Voice 2, which is based on a similar pipeline with Deep Voice~1, but constructed with higher performance building blocks and demonstrates a significant audio quality improvement over Deep Voice~1. We improve Tacotron by introducing a post-processing neural vocoder, and demonstrate a significant audio quality improvement. We then demonstrate our technique for multi-speaker speech synthesis for both Deep Voice 2 and Tacotron on two multi-speaker TTS datasets. We show that a single neural TTS system can learn hundreds of unique voices from less than half an hour of data per speaker, while achieving high audio quality synthesis and preserving the speaker identities almost perfectly. 
   We introduce a neural network that represents sentences by composing their words according to induced binary parse trees. We use Tree-LSTM as our composition function, applied along a tree structure found by a fully differentiable natural language chart parser. Our model simultaneously optimises both the composition function and the parser, thus eliminating the need for externally-provided parse trees which are normally required for Tree-LSTM. It can therefore be seen as a tree-based RNN that is unsupervised with respect to the parse trees. As it is fully differentiable, our model is easily trained with an off-the-shelf gradient descent method and backpropagation. We demonstrate that it achieves better performance compared to various supervised Tree-LSTM architectures on a textual entailment task and a reverse dictionary task. 
   In this paper, we focus on learning structure-aware document   representations from data without recourse to a discourse parser or   additional annotations. Drawing inspiration from recent efforts to   empower neural networks with a structural bias   , we propose a model that can   encode a document while automatically inducing rich structural   dependencies. Specifically, we embed a differentiable non-projective   parsing algorithm into a neural model and use attention mechanisms   to incorporate the structural biases. Experimental evaluations across   different tasks and datasets show that the proposed model achieves   state-of-the-art results on document modeling tasks while inducing   intermediate structures which are both interpretable and meaningful.   %   The syntactic structure can provide inductive bias for machine %   learning system to better model human languages, and for many tasks, %   incorporating structure information can lead to a better %   representation of the text.  Though obtaining syntactic structure %   usually requires an external parser, recent research found it is %   possible to implicitly model the it without abandoning the %   end-to-end training.  However, this method is not suitable for %   modeling documents due to its non-projectivity and unparallelizable %   problem.  In this paper, we proposed a model that can encode a %   document while automatically learning richer structural %   dependencies.  We embed a differentiable non-projective parsing %   algorithm into a neural model and use self-attention mechanisms to %   incorporate the structural biases.  %  %  %  Experiments on both %   sentence-level and document level tasks show that this approach is %   effective for learning structural information and can achieve %   state-of-the results on document modeling tasks. 
 Biomedical events describe complex interactions between various biomedical entities. Event trigger is a word or a phrase which typically signifies the occurrence of an event. Event trigger identification is an important first step in all event extraction methods. However many of the current approaches either rely on complex hand-crafted features or consider features only within a window. In this paper we propose a method that takes the advantage of recurrent neural network  to extract higher level features present across the sentence. Thus hidden state representation of RNN along with word and entity type embedding as features avoid relying on the complex hand-crafted features generated using various NLP toolkits. Our experiments have shown to achieve state-of-art F1-score on Multi Level Event Extraction  corpus. We have also performed category-wise analysis of the result and discussed the importance of various features in trigger identification task. 
 We evaluate the character-level translation method for neural semantic parsing on a large corpus of sentences annotated with Abstract Meaning Representations . Using a sequence-to-sequence model, and some trivial preprocessing and postprocessing of AMRs, we obtain a baseline accuracy of 53.1 . We examine five different approaches to improve this baseline result:  reordering AMR branches to match the word order of the input sentence increases performance to 58.3;  adding part-of-speech tags  to the input shows improvement as well ;  So does the introduction of super characters , reaching 57.4;  optimizing the training process by using pre-training and averaging a set of models increases performance to 58.7;  adding silver-standard training data obtained by an off-the-shelf parser yields the biggest improvement, resulting in an F-score of 64.0. Combining all five techniques leads to an F-score of 71.0 on holdout data, which is state-of-the-art in AMR parsing. This is remarkable because of the relative simplicity of the approach. 
 % TSD 2017:   We show that a recently proposed neural dependency parser can be   improved by joint training on multiple languages from the same   family. The parser is implemented as a deep neural network whose   only input is orthographic representations of words. In   order to successfully parse, the network has to discover how linguistically   relevant concepts can be inferred from word spellings. We analyze   the representations of characters and words that are learned by the network to establish   which properties of languages were accounted for. In   particular we show that the parser has approximately learned to   associate Latin characters with their Cyrillic counterparts and that   it can group Polish and Russian words that have a similar   grammatical function. Finally, we evaluate the parser on selected   languages from the Universal Dependencies dataset and show that it   is competitive with other recently proposed state-of-the art   methods, while having a simple structure.  % TSD 2017: keywords, comma-separated  
 Humor is a defining characteristic of human beings.  Our goal is to develop methods that automatically  detect humorous statements and rank them on a  continuous scale. In this paper we report on results  using a Language Model approach, and outline our plans for using methods from Deep Learning. 
   We present a transition-based dependency parser that uses a convolutional neural network to compose word representations from characters. The character composition model shows great improvement over the word-lookup model, especially for parsing agglutinative languages. These improvements are even better than using pre-trained word embeddings from extra data. On the SPMRL data sets, our system outperforms the previous best greedy parser  by a margin of 3\% on average.\footnote{The parser is available at }  
   In the past few years, attention mechanisms have become an indispensable component of end-to-end neural machine translation models. However, previous attention models always refer to some source words when predicting a target word, which contradicts with the fact that some target words have no corresponding source words. Motivated by this observation, we propose a novel attention model that has the capability of determining when a decoder should attend to source words and when it should not. Experimental results on NIST Chinese-English translation tasks show that the new model achieves an improvement of 0.8 BLEU score over a state-of-the-art baseline. 
 		Different from other sequential data, sentences in natural language are structured by linguistic grammars. Previous generative conversational models with chain-structured decoder ignore this structure in human language and might generate plausible responses with less satisfactory relevance and fluency. In this study, we aim to incorporate the results from linguistic analysis into the process of sentence generation for high-quality conversation generation. Specifically, we use a dependency parser to transform each response sentence into a dependency tree and construct a training corpus of sentence-tree pairs. A tree-structured decoder is developed to learn the mapping from a sentence to its tree, where different types of hidden states are used to depict the local dependencies from an internal tree node to its children. For training acceleration, we propose a tree canonicalization method, which transforms trees into equivalent ternary trees. 		Then, with a proposed tree-structured search method, the model is able to generate the most probable responses in the form of dependency trees, which are finally flattened into sequences as the system output. 		Experimental results demonstrate that the proposed X2Tree framework outperforms baseline methods over 11.15\% increase of acceptance ratio. 	
 We propose a novel system which can transform a recipe into any selected regional style . This system has two characteristics. First the system can identify the degree of regional cuisine style mixture of any selected recipe and visualize such regional cuisine style mixtures using barycentric Newton diagrams. Second, the system can suggest ingredient substitutions through an extended word2vec model, such that a recipe becomes more authentic for any selected regional cuisine style. Drawing on a large number of recipes from Yummly, an example shows how the proposed system can transform a traditional Japanese recipe, Sukiyaki, into French style.    Keywords: food, big data, regional cuisine style, newton diagram, neural network, word2vec 
    Recently deep neural networks  have been used to learn speaker features.   However, the quality of the learned features   is not sufficiently good, so a complex back-end model, either   neural or probabilistic, has to be used to address the residual uncertainty when   applied to speaker verification, just as with raw features. This paper presents   a convolutional time-delay deep neural network structure  for speaker feature learning.   Our experimental results on the  database demonstrated that this CT-DNN can   produce high-quality speaker features: even with a single feature ,   the EER can be as low as 7.68\%. This effectively confirmed that the speaker trait   is largely a deterministic short-time property rather than a long-time distributional pattern,   and therefore can be extracted from just dozens of frames.  
   It has been shown that Chinese poems can be successfully generated by \mbox{sequence-to-sequence} neural models, particularly with the attention mechanism.  A potential problem of this approach, however, is that neural models can only learn abstract rules, while poem generation is a highly creative process that involves not only rules but also innovations for which pure statistical models are not appropriate in principle.  This work proposes a memory-augmented neural model for Chinese poem generation, where the neural model and the augmented memory work together to balance the requirements of linguistic accordance and aesthetic innovation, leading to innovative generations that are still rule-compliant. In addition, it is found that the memory mechanism provides interesting flexibility that can be used to generate poems with different styles.  
 Solving algebraic word problems requires executing a series of arithmetic operations---a program---to obtain a final answer. However, since programs can be arbitrarily complicated, inducing them directly from question-answer pairs is a formidable challenge. To make this task more feasible, we solve these problems by generating , sequences of natural language and human-readable mathematical expressions that derive the final answer through a series of small steps. Although rationales do not explicitly specify programs, they provide a scaffolding for their structure via intermediate milestones. To evaluate our approach, we have created a new 100,000-sample dataset of questions, answers and rationales. Experimental results show that indirect supervision of program learning via answer rationales is a promising strategy for inducing arithmetic programs. 
  %Knowledge Graphs are traditionally considered to be static in nature. However,  The availability of large scale event data with time stamps has given rise to  knowledge graphs that contain temporal information for each edge. Reasoning over time in such dynamic knowledge graphs is not yet well understood. To this end, we present {\bf Know-Evolve}, a novel deep evolutionary knowledge network that learns non-linearly evolving entity representations over time. The occurrence of a fact  is modeled as a multivariate point process whose intensity function is modulated by the score for that fact computed based on the learned entity embeddings. We demonstrate significantly improved performance over various relational learning approaches on two large scale real-world datasets. Further, our method effectively predicts occurrence or recurrence time of a fact which is novel compared to prior reasoning approaches in multi-relational setting.    %To facilitate temporal reasoning, propose a multivariate point process framework to model the occurrence of a fact  in continuous time and uses the learned embeddings are used to compute relationship score that further parametrizes intensity function of the point process.  %and incomplete and most relational learning methods on knowledge graphs have focussed on the task of reasoning over structure in these graphs.  %As relationships form and reform over time, the features of involved entities evolve and co-evolve in non-linearly. In this paper,  % and thereby allowing to reason over time  % %An event forges relationships between involved entities. Such relationships can be represented as $fact$ triplet $$ which signifies a relationship $r$ between subject $s$ and an object $o$.  %and have recently gained attraction for providing useful knowledge to application like recommendation, question answering and conversational dialogue settings. % also does not have time associated with each available fact. . 
 Linking human whole-body motion and natural language is of great interest for the generation of semantic representations of observed human behaviors as well as for the generation of robot behaviors based on natural language input. While there has been a large body of research in this area, most approaches that exist today require a symbolic representation of motions , which have to be defined a-priori or require complex segmentation algorithms. In contrast, recent advances in the field of neural networks and especially deep learning have demonstrated that sub-symbolic representations that can be learned end-to-end usually outperform more traditional approaches, for applications such as machine translation. In this paper we propose a generative model that learns a bidirectional mapping between human whole-body motion and natural language using   and . Our approach does not require any segmentation or manual feature engineering and learns a distributed representation, which is shared for all motions and descriptions. We evaluate our approach on 2\,846~human whole-body motions and 6\,187~natural language descriptions thereof from the . Our results clearly demonstrate the effectiveness of the proposed model: We show that our model generates a wide variety of realistic motions only from descriptions thereof in form of a single sentence. Conversely, our model is also capable of generating correct and detailed  natural language descriptions from human motions. 
 \small\baselineskip=9pt Visual question answering  is a recently proposed artificial intelligence task that requires a deep understanding of both images and texts. In deep learning, images are typically modeled through convolutional neural networks  while texts are typically modeled through recurrent neural networks . In this work, we perform a detailed analysis on the natural language questions in VQA, which raises a different need for text representations as compared to other natural language processing tasks. Based on the analysis, we propose to rely on CNNs for learning text representations. By exploring various properties of CNNs specialized for text data, we present our ``CNN Inception + Gate'' model for text feature extraction in VQA. The experimental results show that simply replacing RNNs with our CNN-based model improves question representations and thus the overall accuracy of VQA models. In addition, our model has much fewer parameters and the computation is much faster. We also prove that the text representation requirement in VQA is more complicated and comprehensive than that in conventional natural language processing tasks. Shallow models like the fastText model, which can obtain comparable results with deep learning models in simple tasks like text classification, have poor performances in VQA. 
 The design of neural architectures for structured objects is typically guided by experimental insights rather than a formal process. In this work, we appeal to kernels over combinatorial structures, such as sequences and graphs, to derive appropriate neural operations. We introduce a class of deep recurrent neural operations and formally characterize their associated kernel spaces. Our recurrent modules compare the input to virtual reference objects  via the kernels.  Similar to traditional neural operations, these reference objects are parameterized and directly optimized in end-to-end training. We empirically evaluate the proposed class of neural architectures on standard applications such as language modeling and molecular graph regression, achieving state-of-the-art results across these applications. %We also draw connections to existing architectures such as LSTMs. 
 % Textual analysis of short texts such as single sentences  % and Twitter Messages is challenging because of the limited contextual  % information that they normally contain.  Cities have been a thriving place for citizens over the centuries due to their complex infrastructure. The emergence of the Cyber-Physical-Social Systems  and context-aware technologies boost a growing interest in analysing, extracting  and eventually understanding city events which subsequently can be utilised to  leverage the citizen observations of their cities. In this paper, we  investigate the feasibility of using Twitter textual streams for extracting city events. We propose a hierarchical multi-view deep  learning approach to contextualise citizen observations of various city systems and services such as traffic, public transport, weather, sociocultural activities  and public safety as a source of city events. Our goal has been to build a flexible  architecture that can learn representations useful for tasks, thus avoiding excessive  task-specific feature engineering. We apply our approach on a real-world dataset  consisting of event reports and tweets collected by~ over four  months from San Francisco Bay Area dataset and additional datasets collected from Greater London.  The results of our evaluations show that our proposed solution outperforms the existing models and  can be used for extracting city related events with an averaged accuracy of $81\%$ over all classes. To further evaluate the impact of our Twitter event extraction model, we have used two sources of authorised reports through collecting road traffic disruptions data from Transport for London API, and parsing the Time Out London website for sociocultural events. The analysis showed that 49.5\% of the Twitter traffic comments are reported approximately five hours prior to the authorities official records. Moreover, we discovered that amongst the scheduled sociocultural event topics; tweets reporting transportation, cultural and social events are 31.75\% more likely to influence the distribution of the Twitter comments than sport, weather and crime topics. 
  Eliminating the negative effect of non-stationary environmental noise is a long-standing research topic for automatic speech recognition that stills remains an important challenge.  Data-driven supervised approaches, including ones based on deep neural networks, have recently emerged as potential alternatives to traditional unsupervised approaches and with sufficient training, can alleviate the shortcomings of the unsupervised methods in various real-life acoustic environments.  In this light, we review recently developed, representative deep learning approaches for tackling non-stationary additive and convolutional degradation of speech with the aim of providing guidelines for those involved in the development of environmentally robust speech recognition systems.  We separately discuss single- and multi-channel techniques developed for the front-end and back-end of speech recognition systems, as well as joint front-end and back-end training frameworks. 
 In this paper, we propose a novel ranking framework for collaborative filtering with the overall aim of learning user preferences over items by minimizing a pairwise ranking loss. We show the minimization problem involves dependent random variables and provide a theoretical analysis by proving the consistency of the empirical risk minimization in the worst case where all users choose a minimal number of positive and negative items. We further derive a Neural-Network model that jointly learns a new representation of users and items in an embedded space as well as the preference relation of users over the pairs of items. The learning objective is based on three scenarios of ranking losses that control the ability of the model to maintain the ordering over the items induced from the users' preferences, as well as, the capacity of the dot-product defined in the learned embedded space to produce the ordering. The proposed model is by nature suitable for implicit feedback and involves the estimation of only very few parameters. Through extensive experiments on several real-world benchmarks on implicit data, we show the interest of learning the preference and the embedding simultaneously when compared to learning those separately. We also demonstrate that our approach is very competitive with the best state-of-the-art collaborative filtering techniques proposed for implicit feedback. 
 Multi-task learning  allows deep neural networks to learn from related tasks by sharing parameters with other networks. In practice, however, MTL involves searching an enormous space of possible parameter sharing architectures to find  the layers or subspaces that benefit from sharing,  the appropriate amount of sharing, and  the appropriate relative weights of the different task losses. Recent work has addressed each of the above problems in isolation. In this work we present an approach that learns a  multi-task architecture that jointly addresses --. We present experiments on synthetic data and data from OntoNotes~5.0, including four different tasks and seven different domains. Our extension consistently outperforms previous approaches to learning latent architectures for multi-task problems and achieves up to 15\%~average error reductions over common approaches to MTL. 
   In this work, we present the Grounded Recurrent Neural Network , a recurrent neural network architecture for multi-label prediction which explicitly ties labels to specific dimensions of the recurrent hidden state . The approach is particularly well-suited for extracting large numbers of concepts from text. We apply the new model to address an important problem in healthcare of understanding what medical concepts are discussed in clinical text. Using a publicly available dataset derived from Intensive Care Units, we learn to label a patient's diagnoses and procedures from their discharge summary. Our evaluation shows a clear advantage to using our proposed architecture over a variety of strong baselines. 
   Most real-world document collections involve various types of metadata, such as author, source, and date, and yet the most commonly-used approaches to modeling text corpora ignore this information. While specialized models have been developed for particular applications, few are widely used in practice, as customization typically requires derivation of a custom inference algorithm. In this paper,  we build on recent advances in variational inference methods and propose a general neural framework, based on topic models, to enable flexible incorporation of metadata and allow for rapid exploration of alternative models. Our approach achieves strong performance, with a manageable tradeoff between perplexity, coherence, and sparsity.  Finally, we demonstrate the potential of our framework through an exploration of a corpus of articles about US immigration. 
 Natural language generation  plays a critical role in spoken dialogue systems. This paper presents a new approach to NLG by using recurrent neural networks , in which a gating mechanism is applied before RNN computation. This allows the proposed model to generate appropriate sentences. The RNN-based generator can be learned from unaligned data by jointly training sentence planning and surface realization to produce natural language responses. The model was extensively evaluated on four different NLG domains. The results show that the proposed generator achieved better performance on all the NLG domains compared to previous generators. 
 Natural language generation  is a critical component in a spoken dialogue system.  This paper presents a Recurrent Neural Network based Encoder-Decoder architecture, in which an LSTM-based decoder is introduced to select, aggregate semantic elements produced by an attention mechanism over the input elements, and to produce the required utterances. The proposed generator can be jointly trained both sentence planning and surface realization to produce natural language sentences. The proposed model was extensively evaluated on four different NLG datasets. The experimental results showed that the proposed generators not only consistently outperform the previous methods across all the NLG domains but also show an ability to generalize from a new, unseen domain and learn from multi-domain datasets.  
 	Hate speech detection on Twitter is critical for applications like controversial event extraction, building AI chatterbots, content recommendation, and sentiment analysis. We define this task as being able to classify a tweet as racist, sexist or neither. The complexity of the natural language constructs makes this task very challenging. We perform extensive experiments with multiple deep learning architectures to learn semantic word embeddings to handle this complexity. Our experiments on a benchmark dataset of 16K annotated tweets show that such deep learning methods outperform state-of-the-art char/word n-gram methods by $\sim$18 F1 points. 
 Topic models have been widely explored as probabilistic generative models of documents. Traditional inference methods have sought closed-form derivations for updating the models, however as the expressiveness of these models grows, so does the difficulty of performing fast and accurate inference over their parameters. This paper presents alternative neural approaches to topic modelling by providing parameterisable distributions over topics which permit training by backpropagation in the framework of neural variational inference. In addition, with the help of a stick-breaking construction, we propose a   recurrent network that is able to discover a notionally unbounded number of   topics, analogous to Bayesian non-parametric topic models. Experimental results on the MXM Song Lyrics, 20NewsGroups and Reuters News datasets demonstrate the effectiveness and efficiency of these neural topic models.  
 Speech emotion recognition is an important and challenging task in the realm of human-computer interaction.  Prior work proposed a variety of models and feature sets for training a system. In this work, we conduct extensive experiments using an attentive convolutional neural network with multi-view learning objective function.  We compare system performance using different lengths of the input signal, different types of acoustic features and different types of emotion speech . Our experimental results on the Interactive Emotional Motion Capture  database reveal that the recognition performance strongly depends on the type of speech data independent of the choice of input features.  Furthermore, we achieved state-of-the-art results on the improvised speech data of IEMOCAP.    
 This paper demonstrates the potential of convolutional neural networks  for detecting and classifying prosodic events on words, specifically pitch accents and phrase boundary tones, from frame-based acoustic features. Typical approaches use not only feature representations of the word in question but also its surrounding context. We show that adding position features indicating the current word benefits the CNN. In addition, this paper discusses the generalization from a speaker-dependent modelling approach to a speaker-independent setup. The proposed method is simple and efficient and yields strong results not only in speaker-dependent but also speaker-independent cases. 
 Task-specific word identification aims to choose the task-related words that best describe a short text. Existing approaches require well-defined seed words or lexical dictionaries , which are often unavailable for many applications such as social discrimination detection and fake review detection. However, we often have a set of labeled short texts where each short text has a task-related class label, e.g., discriminatory or non-discriminatory, specified by users or learned by classification algorithms. In this paper, we focus on identifying task-specific words and phrases from short texts by exploiting their class labels rather than using seed words or lexical dictionaries. We consider the task-specific word and phrase identification as feature learning. We train a convolutional neural network over a set of labeled texts and use score vectors to localize the task-specific words and phrases. Experimental results on sentiment word identification show that our approach significantly outperforms existing methods. We further conduct two case studies to show the effectiveness of our approach. One case study on a crawled tweets dataset demonstrates that our approach can successfully capture the discrimination-related words/phrases. The other case study on fake review detection shows that our approach can identify the fake-review words/phrases.  {\bf Keywords:} Task-specific word identification, convolutional neural network, deep learning 
 %Transfer learning is an important problem of language understanding , which focuses on how the recognition pattern of a semantic concept benefits other, associated ones. In this paper, we exploit substructure in semantic slots to explore their relations. We propose a new semantic representation based on hierarchical concepts in which a semantic slot is represented as a composition of atomic concepts. Meanwhile, we propose concept transfer learning methods for modelling slot relations and extending slots in LU. Our methods are applied to two adaptive LU problems: value set mismatch and domain adaptation, and evaluated on two LU benchmarks respectively: ATIS and DSTC 2\&3. The experiment results show the concept transfer learning is very efficient to solve those two problems. In particular, we achieve state-of-the-art performance  on ATIS by only using lexicon features. %Concept definition is the is the key issue in language understanding  adaptation since different concept definitions can easily lead to data sparsity even if different data sets are actually semantically correlated. In this paper, a novel concept transfer learning approach is proposed to address this issue. Here, substructures within literal concept definition are investigated to reveal the relationship between concepts. A hierarchical semantic representation for concepts is proposed, where a semantic slot is represented as a composition of atomic concepts. Based on this new hierarchical representation, transfer learning approaches are developed for adaptive LU. The approaches are applied to two adaptive LU problems: value set mismatch and domain adaptation, and evaluated on two LU benchmarks respectively: ATIS and DSTC 2\&3. The experiment results show that the proposed concept transfer learning is very efficient and effective. In particular, we achieve state-of-the-art performance  on ATIS by only using lexicon features. Concept definition is important in language understanding  adaptation since literal definition difference can easily lead to data sparsity even if different data sets are actually semantically correlated. To address this issue, in this paper, a novel concept transfer learning approach is proposed. Here, substructures within literal concept definition are investigated to reveal the relationship between concepts. A hierarchical semantic representation for concepts is proposed, where a semantic slot is represented as a composition of atomic concepts. Based on this new hierarchical representation, transfer learning approaches are developed for adaptive LU. The approaches are applied to two tasks: value set mismatch and domain adaptation, and evaluated on two LU benchmarks: ATIS and DSTC 2\&3.  Thorough empirical studies validate both the efficiency and effectiveness of the proposed method. In particular, we achieve state-of-the-art performance  on ATIS by only using lexicon features. 
 Automated story generation is the problem of automatically selecting a sequence of events, actions, or words that can be told as a story. We seek to develop a system that can generate stories by learning everything it needs to know from textual story corpora. To date, recurrent neural networks that learn language models at character, word, or sentence levels have had little success generating coherent stories. We explore the question of event representations that provide a mid-level of abstraction between words and sentences in order to retain the semantic information of the original data while minimizing event sparsity. We present a technique for preprocessing textual story data into event sequences. We then present a technique for automated story generation whereby we decompose the problem into the generation of successive events  and the generation of natural language sentences from events . We give empirical results comparing different event representations and their effects on event successor generation and the translation of events to natural language. 
 Using supporting backchannel  cues can make human-computer interaction more social. BCs provide a feedback from the listener to the speaker indicating to the speaker that he is still listened to. BCs can be expressed in different ways, depending on the modality of the interaction, for example as gestures or acoustic cues. In this work, we only considered acoustic cues. We are proposing an approach towards detecting BC opportunities based on acoustic input features like power and pitch. While other works in the field rely on the use of a hand-written rule set or specialized features, we made use of artificial neural networks. They are capable of deriving higher order features from input features themselves. In our setup, we first used a fully connected feed-forward network to establish an updated baseline in comparison to our previously proposed setup. We also extended this setup by the use of Long Short-Term Memory  networks which have shown to outperform feed-forward based setups on various tasks. %In contrast to our previous work, we increased the F1-Score from 0.11 to 0.17. Our best system achieved an F1-Score of 0.37 using power and pitch features. Adding linguistic information using word2vec, the score increased to 0.39. 
  Generative Adversarial Networks  have shown great promise recently in image generation. Training GANs for language generation has proven to be more difficult, because of the non-differentiable nature of generating text with recurrent neural networks. Consequently, past work has either resorted to pre-training with maximum-likelihood or used convolutional networks for generation. In this work, we show that recurrent neural networks can be trained to generate text with GANs from scratch using curriculum learning,  by slowly teaching the model to generate sequences of increasing and variable length. We empirically show that our approach vastly improves the quality of generated sequences compared to a convolutional baseline. \footnote{} 
 Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks  as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Our work shows how a deep learning architecture equipped with an RN module can implicitly discover and learn to reason about entities and their relations. 
 State-of-the-art methods for protein-protein interaction  extraction are primarily feature-based or kernel-based by leveraging lexical and syntactic information. But how to incorporate such knowledge in the recent deep learning methods remains an open question. In this paper, we propose a multichannel dependency-based convolutional neural network model . It applies one channel to the embedding vector of each word in the sentence, and another channel to the embedding vector of the head of the corresponding word. Therefore, the model can use richer information obtained from different channels. Experiments on two public benchmarking datasets, AIMed and BioInfer, demonstrate that \mdcnn compares favorably to the state-of-the-art rich-feature and single-kernel based methods. In addition, \mdcnn achieves 24.4\% relative improvement in F1-score over the state-of-the-art methods on cross-corpus evaluation and 12\% improvement in F1-score over kernel-based methods on ``difficult'' instances. These results suggest that \mdcnn generalizes more easily over different corpora, and is capable of capturing long distance features in the sentences. 
   We present a general-purpose tagger based on convolutional neural networks , used for both composing word vectors and encoding context information. The CNN tagger is robust across different tagging tasks: without task-specific tuning of hyper-parameters, it achieves state-of-the-art results in part-of-speech tagging, morphological tagging and supertagging. The CNN tagger is also robust against the out-of-vocabulary problem, it performs well on artificially unnormalized texts. 
 In the last few years, Recurrent Neural Networks  have proved effective on several NLP tasks. Despite such great success, their ability to model  is still limited. This lead research toward solutions where RNNs are combined with models which already proved effective in this domain, such as CRFs. In this work we propose a solution far simpler but very effective: an evolution of the simple Jordan RNN, where labels are re-injected as input into the network, and converted into embeddings, in the same way as words. We compare this RNN variant to all the other RNN models, Elman and Jordan RNN, LSTM and GRU, on two well-known tasks of Spoken Language Understanding . Thanks to label embeddings and their combination at the hidden layer, the proposed variant, which uses more parameters than Elman and Jordan RNNs, but far fewer than LSTM and GRU, is more effective than other RNNs, but also outperforms sophisticated CRF models. 
  Increasingly, cognitive scientists have demonstrated interest in applying tools from deep learning. One use for deep learning is in language acquisition where it is useful to know if a linguistic phenomenon can be learned through domain-general means. To assess whether unsupervised deep learning is appropriate, we first pose a smaller question: Can unsupervised neural networks apply linguistic rules productively, using them in novel situations?  We draw from the literature on determiner/noun productivity by training an unsupervised, autoencoder network measuring its ability to combine nouns with determiners.  Our simple autoencoder creates combinations it has not previously encountered and produces a degree of overlap matching adults. While this preliminary work does not provide conclusive evidence for productivity, it warrants further investigation with more complex models. Further, this work helps lay the foundations for future collaboration between the deep learning and cognitive science communities.  Keywords:  Deep Learning; Language Acquisition; Linguistic Productivity; Unsupervised Learning; Determiners 
 We consider the problem of learning general-purpose, paraphrastic sentence embeddings in the setting of .  We use  neural machine translation to generate sentential paraphrases via back-translation of bilingual sentence pairs. We evaluate the paraphrase pairs by their ability to serve as training data for learning paraphrastic sentence embeddings.  We find that the data quality is stronger than prior work based on bitext and on par with  manually-written English paraphrase pairs, with the advantage that our approach can scale up to generate large training sets for many languages and domains.  We experiment with several language pairs and data sources, and develop  a variety of data filtering techniques.  In the process, we explore how  neural machine translation output differs from human-written sentences, finding clear differences in length,  the amount of repetition, and the use of rare words.\footnote{Generated paraphrases and code are  available at \url{http://ttic.uchicago.edu/~wieting}.} 
 % 97 Words: Ladder networks are a notable new concept in the field of semi-supervised learning by showing state-of-the-art results in image recognition tasks while being compatible with many existing neural architectures. We present the recurrent ladder network, a novel modification of the ladder network, for semi-supervised learning of recurrent neural networks which we evaluate with a phoneme recognition task on the TIMIT corpus. Our results show that the model is able to consistently outperform the baseline and achieve fully-supervised baseline performance with only 75\% of all labels which demonstrates that the model is capable of using unsupervised data as an effective regulariser.  
 Current Chinese social media text summarization models are based on an encoder-decoder framework. Although its generated summaries are similar to source texts literally, they have low semantic relevance. In this work, our goal is to improve semantic relevance between source texts and summaries for Chinese social media summarization. We introduce a Semantic Relevance Based neural model to encourage high semantic similarity between texts and summaries. In our model, the source text is represented by a gated attention encoder, while the summary representation is produced by a decoder. Besides, the similarity score between the representations is maximized during training. Our experiments show that the proposed model outperforms baseline systems on a social media corpus. 
 Common-sense and background knowledge is required to understand natural language, but in most neural natural language understanding  systems, this knowledge must be acquired from training corpora during learning, and then it is static at test time. We introduce a new architecture for the dynamic integration of explicit background knowledge in NLU models. A general-purpose reading module reads background knowledge in the form of free-text statements  and yields refined word representations to a task-specific NLU architecture that reprocesses the task inputs with these representations. Experiments on document question answering  and recognizing textual entailment  demonstrate the effectiveness and flexibility of the approach. Analysis shows that our model learns to exploit knowledge in a semantically appropriate way. 
   We present a state-of-the-art end-to-end Automatic Speech Recognition  model.   We learn to listen and write characters with a joint Connectionist Temporal Classification  and attention-based encoder-decoder network.   The encoder is a deep Convolutional Neural Network  based on the VGG network.   The CTC network sits on top of the encoder and is jointly trained with the attention-based decoder.   During the beam search process, we combine the CTC predictions, the attention-based decoder predictions and a separately trained LSTM language model.   We achieve a 5-10\% error reduction compared to prior systems on spontaneous Japanese and Chinese speech, and our end-to-end model beats out traditional hybrid ASR systems.  %The joint CTC-attention-based end-to-end automatic speech recognition  effectively utilizes both advantages of connectionist temporal classification  and attention-based encoder-decoders,é–µå“¸nd performs better than the individual approaches.  %In our previous work, we designed the joint CTC-attention model as a combination of an encoder network with stacked bidirectional long short-term memory  layers, a decoder network with attention-based LSTM layers, and a CTC network that shared the encoder network.    %In this paper, we extend our model by incorporating a VGGNet, a very deep convolutional neural network , into the encoder and a pre-trained large-scale LSTM language model into the decoder.  %We further improves the beam search process by using CTC prefix probabilities during decoding, while our previous implementation used the CTC probabilities in a rescoring step.  %We have applied the extended end-to-end system to two ASR benchmarks  and obtained 5-10\% error reduction from the prior system. Finally, our system has achieved better performance than state-of-the-art hybrid ASR systems without linguistic resources. 
 Depthwise separable convolutions reduce the number of parameters and computation used in convolutional operations while increasing representational efficiency. They have been shown to be successful in image classification models, both in obtaining better models than previously possible for a given parameter count  and considerably reducing the number of parameters required to perform at a given level . Recently, convolutional sequence-to-sequence networks have been applied to machine translation tasks with good results. In this work, we study how depthwise separable convolutions can be applied to neural machine translation. We introduce a new architecture inspired by Xception and ByteNet, called SliceNet, which enables a significant reduction of the parameter count and amount of computation needed to obtain results like ByteNet, and, with a similar parameter count, achieves new state-of-the-art results. In addition to showing that depthwise separable convolutions perform well for machine translation, we investigate the architectural changes that they enable: we observe that thanks to depthwise separability, we can increase the length of convolution windows, removing the need for filter dilation. We also introduce a new "super-separable" convolution operation that further reduces the number of parameters and computational cost for obtaining state-of-the-art results. } 
We present a framework and its implementation relying on Natural Language Processing methods, which aims at the identification of exercise item candidates from corpora. The hybrid system combining heuristics and machine learning methods includes a number of relevant selection criteria. We focus on two fundamental aspects: linguistic complexity and the dependence of the extracted sentences on their original context. Previous work on exercise generation addressed these two criteria only to a limited extent, and a refined overall candidate sentence selection framework appears also to be lacking. In addition to a detailed description of the system, we present the results of an empirical evaluation conducted with language teachers and learners which indicate the usefulness of the system for educational purposes. We have integrated our system into a freely available online learning platform. 
 Recent work has explored the syntactic abilities of RNNs using the subject-verb agreement task, which diagnoses sensitivity to sentence structure. RNNs performed this task well in common cases, but faltered in complex sentences . We test whether these errors are due to inherent limitations of the architecture or to the relatively indirect supervision provided by most agreement dependencies in a corpus. We trained a single RNN to perform both the agreement task and an additional task, either CCG supertagging or language modeling. Multi-task training led to significantly lower error rates, in particular on complex sentences, suggesting that RNNs have the ability to evolve more sophisticated syntactic representations than shown before. We also show that easily available agreement training data can improve performance on other syntactic tasks, in particular when only a limited amount of training data is available for those tasks. The multi-task paradigm can also be leveraged to inject grammatical knowledge into language models. 
   Factoid question answering  has recently benefited from the development of deep learning  systems.   Neural network models outperform traditional approaches in domains where large datasets exist, such as SQuAD  for Wikipedia articles.   However, these systems have not yet been applied to QA in more specific domains, such as biomedicine, because datasets are generally too small to train a DL system from scratch.   For example, the BioASQ dataset for biomedical QA comprises less then $900$ factoid  and list  QA instances.   In this work, we adapt a neural QA system trained on a large open-domain dataset  to a biomedical dataset  by employing various transfer learning techniques.   Our network architecture is based on a state-of-the-art QA system, extended with biomedical word embeddings and a novel mechanism to answer list questions.   In contrast to existing biomedical QA systems, our system does not rely on domain-specific ontologies, parsers or entity taggers, which are expensive to create.   Despite this fact, our systems achieve state-of-the-art results on factoid questions and competitive results on list questions. 
 Speech recognition systems for irregularly-spelled languages like English normally require hand-written pronunciations. In this paper, we describe a system for automatically  obtaining pronunciations of words for which pronunciations are not available, but for which transcribed data exists.  Our method integrates information from the letter sequence and from the acoustic evidence. The novel aspect of the problem that we address is the problem of how to prune entries from such a lexicon . Experiments on various ASR tasks show that, with the proposed framework, starting with an initial lexicon of several thousand words, we are able to learn a lexicon which performs close to a full expert lexicon in terms of WER performance on test data, and is better than lexicons built using G2P alone or with a pruning criterion based on pronunciation probability. 
 Query-by-example search often uses dynamic time warping  for comparing queries and proposed matching segments.  Recent work has shown that comparing speech segments by representing them as fixed-dimensional vectors --- acoustic word embeddings --- and measuring their vector distance  can discriminate between words more accurately than DTW-based approaches. We consider an approach to query-by-example search that embeds both the query and database segments according to a neural model, followed by nearest-neighbor search to find the matching segments.  Earlier work on embedding-based query-by-example, using template-based acoustic word embeddings, achieved competitive performance. We find that our embeddings, based on recurrent neural networks trained to optimize word discrimination, achieve substantial improvements in performance and run-time efficiency over the previous approaches. 
 We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality of phrase-based statistical machine translation. 
  The goal of semantic parsing is to map natural language to a machine interpretable meaning representation language . One of the constraints that limits full exploration of deep learning technologies for semantic parsing is the lack of sufficient annotation training data. In this paper, we propose using sequence-to-sequence in a multi-task setup for semantic parsing with a focus on transfer learning. We explore three multi-task architectures for sequence-to-sequence modeling and compare their performance with an independently trained model. Our experiments show that the multi-task setup aids transfer learning from an auxiliary task with large labeled data to a target task with smaller labeled data. We see absolute accuracy gains ranging from 1.0\% to 4.4\% in our in-house data set, and we also see good gains ranging from 2.5\% to 7.0\% on the ATIS semantic parsing tasks with syntactic and semantic auxiliary tasks.   %  We explore encoder-decoder architectures for semantic parsing. The %  goal of semantic parsing is to map natural language to a machine %  interpretable meaning representation language . We focus on %  attention \& copy-based sequence-to-sequence methods for this task, which %  obviate the need for explicitly hand-engineering features or %  manually defining the state space for parsing. Furthermore, we %  explore a shared encoder and multi-encoder setup for parsing %  simultaneously into multiple semantic formalisms. Experimental %  results show that this multi-task setup helps improve the %  performance of the overall system.  
  We compare three approaches to statistical machine translation  by performing a fine-grained manual evaluation via error annotation of the systems' outputs. The error types in our annotation are compliant with the multidimensional quality metrics , and the annotation is performed by two annotators. Inter-annotator agreement is high for such a task, and results show that the best performing system  reduces the errors produced by the worst system  by 54\%.  
 We propose a two-stage neural model to tackle question generation from documents. First, our model estimates the probability that word sequences in a document are ones that a human would pick when selecting candidate answers by training a neural key-phrase extractor on the answers in a question-answering corpus. Predicted key phrases then act as target answers and condition a sequence-to-sequence question-generation model with a copy mechanism. Empirically, our key-phrase extraction model significantly outperforms an entity-tagging baseline and existing rule-based approaches. We further demonstrate that our question generation system formulates fluent, answerable questions from key phrases. This two-stage system could be used to augment or generate reading comprehension datasets, which may be leveraged to improve machine reading systems or in educational settings. 
    This work presents a novel approach to Automatic Post-Editing  and Word-Level Quality Estimation  using ensembles of specialized Neural Machine Translation  systems. Word-level features that have proven effective for QE are included as input factors, expanding the representation of the original source and the machine translation hypothesis, which are used to generate an automatically post-edited hypothesis. We train a suite of NMT models that use different input representations, but share the same output space. These models are then ensembled together, and tuned for both the APE and the QE task. We thus attempt to connect the state-of-the-art approaches to APE and QE within a single framework. Our models achieve state-of-the-art results in both tasks, with the only difference in the tuning step which learns weights for each component of the ensemble.  
  Word embeddings are now a standard technique for inducing meaning representations for words. For getting good representations, it is important to take into account different senses of a word. In this paper, we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks.   
    In this paper, we present Neural Phrase-based Machine Translation .\footnote{The source code is available at \url{https://github.com/posenhuang/NPMT}.} Our   method explicitly models the phrase structures in output sequences using   Sleep-WAke Networks , a recently proposed segmentation-based sequence   modeling method. To mitigate the monotonic alignment requirement of SWAN, we   introduce a new layer to perform  local reordering of input sequences.   Different from existing neural machine translation  approaches, NPMT does   not use attention-based decoding mechanisms.    Instead, it directly outputs phrases in a sequential order and can decode in linear time.    Our experiments show that NPMT achieves superior performances on IWSLT 2014 German-English/English-German and IWSLT   2015 English-Vietnamese machine translation tasks compared with strong NMT baselines.   We also observe that our method produces meaningful phrases in output   languages. 
 	Knowledge base completion  aims to predict missing information in a knowledge base. 	In this paper, we address the out-of-knowledge-base  entity problem in KBC: 	how to answer queries concerning test entities not observed at training time. 	Existing embedding-based KBC models assume that all test entities are available at training time, 		making it unclear how to obtain embeddings for new entities without costly retraining. 	To solve the OOKB entity problem without retraining, 		we use graph neural networks  		to compute the embeddings of OOKB entities, 		exploiting the limited auxiliary knowledge provided at test time. 	The experimental results show the effectiveness of our proposed model in the OOKB setting. 	Additionally, in the standard KBC setting in which OOKB entities are not involved, 		our model achieves state-of-the-art performance on the WordNet dataset. 	The code and dataset are available at \protect{\url{https://github.com/takuo-h/GNN-for-OOKB}}. 
 Training of neural machine translation  models usually uses mini-batches for efficiency purposes. During the mini-batched training process, it is necessary to pad shorter sentences in a mini-batch to be equal in length to the longest sentence therein for efficient computation. Previous work has noted that sorting the corpus based on the sentence length before making mini-batches reduces the amount of padding and increases the processing speed. However, despite the fact that mini-batch creation is an essential step in NMT training, widely used NMT toolkits implement disparate strategies for doing so, which have not been empirically validated or compared. This work investigates mini-batch creation strategies with experiments over two different datasets. Our results suggest that the choice of a mini-batch creation strategy has a large effect on NMT training and some length-based sorting strategies do not always work well compared with simple shuffling. 
 Human conversation is inherently complex, often spanning many different topics/domains. This makes policy learning for dialogue systems very challenging. Standard flat reinforcement learning methods do not provide an efficient framework for modelling such dialogues. In this paper, we focus on the under-explored problem of multi-domain dialogue management. First, we propose a new method for hierarchical reinforcement learning using the  framework. Next, we show that the proposed architecture learns faster and arrives at a better policy than the existing flat ones do. Moreover, we show how pretrained policies can be adapted to more complex systems with an additional set of new actions. In doing that, we show that our approach has the potential to facilitate policy optimisation for more sophisticated multi-domain dialogue systems. 
  This paper introduces THUMT, an open-source toolkit for neural machine translation  developed by the Natural Language Processing Group at Tsinghua University. THUMT implements the standard attention-based encoder-decoder framework on top of Theano and supports three training criteria: maximum likelihood estimation, minimum risk training, and semi-supervised training. It features a visualization tool for displaying the relevance between hidden states in neural networks and contextual words, which helps to analyze the internal workings of NMT. Experiments on Chinese-English datasets show that THUMT using minimum risk training significantly outperforms GroundHog, a state-of-the-art toolkit for NMT.   
 We are increasingly surrounded by artificially intelligent technology that takes decisions and executes actions on our behalf. This creates a pressing need for general means to communicate with, instruct and guide artificial agents, with human language the most compelling means for such communication. To achieve this in a scalable fashion, agents must be able to relate language to the world and to actions; that is, their understanding of language must be grounded and embodied. However, learning grounded language is a notoriously challenging problem in artificial intelligence research. Here we present an agent that learns to interpret language in a simulated 3D environment where it is rewarded for the successful execution of written instructions. Trained via a combination of reinforcement and unsupervised learning, and beginning with minimal prior knowledge, the agent learns to relate linguistic symbols to emergent perceptual representations of its physical surroundings and to pertinent sequences of actions. The agent's comprehension of language extends beyond its prior experience, enabling it to apply familiar language to unfamiliar situations and to interpret entirely novel instructions. Moreover, the speed with which this agent learns new words increases as its semantic knowledge grows. This facility for generalising and bootstrapping semantic knowledge indicates the potential of the present approach for reconciling ambiguous natural language with the complexity of the physical world.  
 We propose a neural multi-document summarization  system that incorporates sentence relation graphs. We employ a Graph Convolutional Network  on the relation graphs, with sentence embeddings obtained from Recurrent Neural Networks as input node features. Through multiple layer-wise propagation, the GCN generates high-level hidden sentence features for salience estimation. We then use a greedy heuristic to extract salient sentences while avoiding redundancy. In our experiments on DUC 2004, we consider three types of sentence relation graphs and demonstrate the advantage of combining sentence relations in graphs with the representation power of deep neural networks. Our model improves upon traditional graph-based extractive approaches and the vanilla GRU sequence model with no graph, and it achieves competitive results against other state-of-the-art multi-document summarization systems. 
  We address the problem of cross-language adaptation for question-question similarity reranking in community question answering, with the objective to port a system trained on one input language to another input language given labeled training data for the first language and only unlabeled data for the second language. In particular, we propose to use adversarial training of neural networks to learn high-level features that are discriminative for the main learning task, and at the same time are invariant across the input languages. The evaluation results show sizable improvements for our cross-language adversarial neural network  model over a strong non-adversarial system. %based on projection using cross-language word embeddings.    %with discriminative neural networks. %, which is novel in a cross-language setting.   %for a cross-lingual task. To the best of our knowledge, this is the first attempt to learn %application of adversarial training for learning  %task-specific representations in a cross-lingual setting.        %a general neural network architecture that combines:  %similar   %a traditional deep feed-forward neural network optimized to minimize the loss of  paired with an adversarial network used  the in a unified   
 Understanding spoken language is a highly complex problem, which can be decomposed into several simpler tasks. In this paper, we focus on Spoken Language Understanding , the module of spoken dialog systems responsible for extracting a semantic interpretation from the user utterance. The task is treated as a labeling problem. In the past, SLU has been performed with a wide variety of probabilistic models. The rise of neural networks, in the last couple of years, has opened new interesting research directions in this domain. Recurrent Neural Networks  in particular are able not only  to represent several pieces of information  as embeddings but also, thanks to their recurrent architecture, to encode as embeddings relatively long contexts. Such long contexts are in general out of reach for models previously used for SLU. In this paper we propose novel RNNs architectures for SLU which outperform previous ones. Starting from a published idea as base block, we design new deep RNNs achieving state-of-the-art results on two widely used corpora for SLU: ATIS , in English, and MEDIA , in French. 
 Recently, a technique called Layer-wise Relevance Propagation  was shown to deliver insightful explanations in the form of input space relevances for understanding feed-forward neural network classification decisions. In the present work, we extend the usage of LRP to recurrent neural networks. We propose a specific propagation rule applicable to multiplicative connections as they arise in recurrent network architectures such as LSTMs and GRUs. We apply our technique to a word-based bi-directional LSTM model on a five-class sentiment prediction task, and evaluate the resulting LRP relevances both qualitatively and quantitatively, obtaining better results than a gradient-based related method which was used in previous work. 
   Word embedding models such as Skip-gram learn a vector-space   representation for each word, based on the local word collocation   patterns that are observed in a text corpus. Latent topic models, on   the other hand, take a more global view, looking at the word   distributions across the corpus to assign a topic to each word   occurrence. These two paradigms are complementary in how they   represent the meaning of word occurrences. While some previous   works have already looked at using word embeddings for improving the   quality of latent topics, and conversely, at using latent topics for improving word embeddings, such ``two-step'' methods cannot capture the mutual interaction between the two paradigms. In this paper, we propose STE, a framework which can learn word embeddings and latent topics in a unified manner. STE naturally obtains topic-specific word embeddings, and thus addresses the issue of polysemy. At the same time, it also learns the term distributions of the topics, and the topic distributions of the documents. Our experimental results demonstrate that the STE model can indeed generate useful topic-specific word embeddings and coherent latent topics in an effective and efficient way. 
 % % A central problem that remains in NMT is : once the model is trained, the most probable output which maximizes the log-likelihood as we trained cannot be properly found at test time.  % Previous neural machine translation models have   Previous neural machine translation models used some heuristic search algorithms  in order to avoid solving the maximum a posteriori problem over translation sentences at test time. In this paper, we propose the Gumbel-Greedy Decoding which trains a generative network to predict translation under a trained  model. We solve such a problem using the Gumbel-Softmax reparameterization, which makes our generative network differentiable and trainable through standard stochastic gradient methods. We empirically demonstrate that our proposed model is effective for generating sequences of discrete words.   % 
 This paper introduces a novel deep learning framework including a lexicon-based approach for sentence-level prediction of sentiment label distribution. We propose to first apply semantic rules and then use a Deep Convolutional Neural Network  for character-level embeddings in order to increase information for word-level embedding. After that, a Bidirectional Long Short-Term Memory network  produces a sentence-wide feature representation from the word-level embedding. We evaluate our approach on three twitter sentiment classification datasets. Experimental results show that our model can improve the classification accuracy of sentence-level sentiment analysis in Twitter social networking. 
   Neural machine translation  has recently become popular in the field of machine translation.  However, NMT suffers from the problem of repeating or missing words in the translation. To address this problem,  proposed an encoder-decoder-reconstructor framework for NMT using back-translation. In this method, they selected the best forward translation model in the same manner as , and then trained a bi-directional translation model as fine-tuning. Their experiments show that it offers significant improvement in BLEU scores in Chinese-English translation task. We confirm that our re-implementation also shows the same tendency and alleviates the problem of repeating and missing words in the translation on a English-Japanese task too. In addition, we evaluate the effectiveness of pre-training by comparing it with a jointly-trained model of forward translation and back-translation. 
   This paper describes our submission to the 2017 BioASQ challenge.   We participated in Task B, Phase B which is concerned with biomedical question answering .   We focus on factoid and list question, using an  QA model, that is, we restrict our system to output substrings of the provided text snippets.   At the core of our system, we use FastQA, a state-of-the-art neural QA system.   We extended it with biomedical word embeddings and changed its answer layer to be able to answer list questions in addition to factoid questions.   We pre-trained the model on a large-scale open-domain QA dataset, SQuAD, and then fine-tuned the parameters on the BioASQ training set.   With our approach, we achieve state-of-the-art results on factoid questions and competitive results on list questions. 
 Neural network acoustic models have significantly advanced state of the art speech recognition over the past few years. However, they are usually computationally expensive due to the large number of matrix-vector multiplications and nonlinearity operations. Neural network models also require significant amounts of memory for inference because of the large model size. For these two reasons, it is challenging to deploy neural network based speech recognizers on resource-constrained platforms such as embedded devices. This paper investigates the use of binary weights and activations for computation and memory efficient neural network acoustic models. Compared to real-valued weight matrices, binary weights require much fewer bits for storage, thereby cutting down the memory footprint. Furthermore, with binary weights or activations, the matrix-vector multiplications are turned into addition and subtraction operations, which are computationally much faster and more energy efficient for hardware platforms. In this paper, we study the applications of binary weights and activations for neural network acoustic modeling, reporting encouraging results on the WSJ and AMI corpora.  
 %% Text of abstract Background. Previous state-of-the-art systems on Drug Name Recognition  and Clinical Concept Extraction  have focused on a combination of text ``feature engineering" and conventional machine learning algorithms such as conditional random fields and support vector machines. However, developing good features is inherently heavily time-consuming. Conversely, more modern machine learning approaches such as recurrent neural networks  have proved capable of automatically learning effective features from either random assignments or automated word ``embeddings''.\\ Objectives.  To create a highly accurate DNR and CCE system that avoids conventional, time-consuming feature engineering.  To create richer, more specialized word embeddings by using health domain datasets such as MIMIC-III.  To evaluate our systems over three contemporary datasets.\\ Methods. Two deep learning methods, namely the Bidirectional LSTM and the Bidirectional LSTM-CRF, are evaluated. A CRF model is set as the baseline to compare the deep learning systems to a traditional machine learning approach. The same features are used for all the models.\\ Results. We have obtained the best results with the Bidirectional LSTM-CRF model, which has outperformed all previously proposed systems. The specialized embeddings have helped to cover unusual words in DrugBank and MedLine, but not in the i2b2/VA dataset.\\ Conclusion. We present a state-of-the-art system for DNR and CCE. Automated word embeddings has allowed us to avoid costly feature engineering and achieve higher accuracy. Nevertheless, the embeddings need to be retrained over datasets that are adequate for the domain, in order to adequately cover the domain-specific vocabulary. 
   Interest in neural machine translation has grown rapidly as its effectiveness has been demonstrated across language and data scenarios.  New research regularly introduces architectural and algorithmic improvements that lead to significant gains over ``vanilla'' NMT implementations.  However, these new techniques are rarely evaluated in the context of previously published techniques, specifically those that are widely used in state-of-the-art production and shared-task systems.  As a result, it is often difficult to determine whether improvements from research will carry over to systems deployed for real-world use.  In this work, we recommend three specific methods that are relatively easy to implement and result in much stronger experimental systems.  Beyond reporting significantly higher BLEU scores, we conduct an in-depth analysis of where improvements originate and what inherent weaknesses of basic NMT models are being addressed.  We then compare the relative gains afforded by several other techniques proposed in the literature when starting with vanilla systems versus our stronger baselines, showing that experimental conclusions may change depending on the baseline chosen.  This indicates that choosing a strong baseline is crucial for reporting reliable experimental results. 
 %TODO rewrite We develop a technique for transfer learning in machine comprehension  using a novel two-stage synthesis network . Given a high-performing MC model in one domain, our technique aims to answer questions about documents in another domain, where we use no labeled data of question-answer pairs. Using the proposed SynNet with a pretrained model from the SQuAD dataset on the challenging NewsQA dataset, we achieve an F1 measure of 44.3\% with a single model and 46.6\% with an ensemble, approaching performance of in-domain models  and outperforming the out-of-domain baseline of 7.6\%, without use of provided annotations.\footnote{} 
 Words in natural language follow a Zipfian distribution whereby some words are frequent but most are rare. Learning representations for words in the ``long tail'' of this distribution requires enormous amounts of data.  Representations of rare words trained directly on end tasks are usually poor, requiring us to pre-train embeddings on external data, or treat all rare words as out-of-vocabulary words with a unique representation. We provide a method for predicting embeddings of rare words on the fly from small amounts of auxiliary data with a network trained end-to-end for the downstream task. % % DIMA: I crammed end-to-end in the text, although in a different way We show that this improves results against baselines where embeddings are trained on the end task for reading comprehension, recognizing textual entailment and language modeling. % % DIMA: unfortunately, we don't have experiment to support that.. 
 End-to-end training of automated speech recognition  systems requires massive data and compute resources. We explore transfer learning based on model adaptation as an approach for training ASR models under constrained GPU memory, throughput and training data.  We conduct several systematic experiments adapting a Wav2Letter convolutional neural network originally trained for English ASR to the German language. We show that this technique allows faster training on consumer-grade resources while requiring less training data in order to achieve the same accuracy, thereby lowering the cost of training ASR models in other languages.  Model introspection revealed that small adaptations to the network's weights were sufficient for good performance, especially for inner layers. 
 We present a novel training framework for neural sequence models, particularly for grounded dialog generation.  The standard training paradigm  for these models is maximum likelihood estimation , or minimizing the cross-entropy of the human responses. Across a variety of domains, a recurring problem with MLE trained generative neural dialog models  is that  they tend to produce  `safe' and generic responses .  In contrast, discriminative dialog models  that are trained to rank a list of candidate human responses outperform their generative counterparts;  in terms of automatic metrics, diversity,  informativeness of the responses.  However, $D$ is not useful in practice since it can not be deployed to have real conversations with users.   Our work aims to achieve the best of both worlds -- the practical usefulness of $G$ and the strong performance of $D$ -- via knowledge transfer from $D$ to $G$. Our primary contribution is  an end-to-end trainable generative visual dialog model, where $G$ receives gradients from $D$ as a   loss of the sequence sampled from $G$.  We leverage the recently proposed Gumbel-Softmax  approximation to the discrete distribution --  specifically, a RNN augmented with a sequence of GS samplers, coupled with the straight-through gradient estimator to enable end-to-end differentiability. We also introduce a stronger encoder for visual dialog, and employ a self-attention mechanism for answer encoding along with a metric learning loss to aid $D$ in better capturing semantic similarities in answer responses.  Overall, our proposed model outperforms state-of-the-art on the VisDial dataset by a significant margin . The source code can be downloaded from {\textcolor{blue}{https://github.com/jiasenlu/visDial.pytorch}}  % ------------------------------- 
 Recurrent Neural Networks , which are a powerful scheme for modeling temporal and sequential data need to capture long-term dependencies on datasets and represent them in hidden layers with a powerful model to capture more information from inputs. For modeling long-term dependencies in a dataset, the gating mechanism concept can help RNNs remember and forget previous information. Representing the hidden layers of an RNN with more expressive operations  helps it learn a more complex relationship between the current input and the previous hidden layer information. These ideas can generally improve RNN performances. In this paper, we proposed a novel RNN architecture that combine the concepts of gating mechanism and the tensor product into a single model. By combining these two concepts into a single RNN, our proposed models learn long-term dependencies by modeling with gating units and obtain more expressive and direct interaction between input and hidden layers using a tensor product on 3-dimensional array  weight parameters. We use Long Short Term Memory  RNN and Gated Recurrent Unit  RNN and combine them with a tensor product inside their formulations. Our proposed RNNs, which are called a Long-Short Term Memory Recurrent Neural Tensor Network  and Gated Recurrent Unit Recurrent Neural Tensor Network , are made by combining the LSTM and GRU RNN models with the tensor product. We conducted experiments with our proposed models on word-level and character-level language modeling tasks and revealed that our proposed models significantly improved their performance compared to our baseline models. 
 Deep convolutional neural networks are being actively investigated in a wide range of speech and audio processing applications including speech recognition, audio event detection and computational paralinguistics, owing to their ability to reduce factors of variations, for learning from speech. However, studies have suggested to favor a certain type of convolutional operations when building a deep convolutional neural network for speech applications although there has been promising results using different types of convolutional operations. In this work, we study four types of convolutional operations on different input features for speech emotion recognition under noisy and clean conditions in order to derive a comprehensive understanding.  %Overall, we present eight deep neural networks in addition to a support vector machine baseline. %Our proposed models reflect the growing trend in combining the convolutional neural network and the recurrent neural network into one architecture and train it in an end-to-end fashion.  Since affective behavioral information has been shown to reflect temporally varying of mental state and convolutional operation are applied locally in time, all deep neural networks share a deep recurrent sub-network architecture for further temporal modeling. We present detailed quantitative module-wise performance analysis to gain insights into information flows within the proposed architectures. In particular, we demonstrate the interplay of affective information and the other irrelevant information during the progression from one module to another. Finally we show that all of our deep neural networks provide state-of-the-art performance on the eNTERFACE'05 corpus.  
 The state-of-the-art solutions  to the vocabulary mismatch  in information retrieval    mainly aim at    leveraging either the relational semantics provided by external resources or the distributional semantics,  recently  investigated by deep neural approaches.  Guided by the intuition that the relational semantics might improve the effectiveness of deep neural approaches, we propose the Deep Semantic Resource Inference Model  that relies on: 1) a representation of raw-data that models the relational semantics of text by jointly considering objects and relations expressed in a knowledge resource, and 2) an end-to-end neural architecture that learns the query-document relevance by leveraging the distributional and relational semantics of documents and queries. The experimental evaluation carried out on two TREC  datasets from TREC Terabyte and TREC CDS tracks relying respectively on WordNet and MeSH resources, indicates that our model outperforms state-of-the-art semantic and deep neural IR models. 
Elections unleash strong political views on Twitter, but what do people really think about politics? Opinion and trend mining on micro blogs dealing with politics has recently attracted researchers in several fields including Information Retrieval and Machine Learning . Since the performance of ML and Natural Language Processing  approaches are limited by the amount and quality of data available, one promising alternative for some tasks is the automatic propagation of expert annotations. This paper intends to develop a so-called active learning process for automatically annotating French language tweets that deal with the image  of politicians. Our main focus is on the methodology followed to build an original annotated dataset expressing opinion from two French politicians over time. We therefore review state of the art NLP-based ML algorithms to automatically annotate tweets using a manual initiation step as bootstrap. This paper focuses on key issues about active learning while building a large annotated data set from noise. This will be introduced by human annotators, abundance of data and the label distribution across data and entities. In turn, we show that Twitter characteristics such as the author's name or hashtags can be considered as the bearing point to not only improve automatic systems for Opinion Mining  and Topic Classification but also to reduce noise in human annotations. However, a later thorough analysis shows that reducing noise might induce the loss of crucial information.
  We propose a simple yet effective technique for neural network learning. The forward propagation is computed as usual. In back propagation, only a small subset of the full gradient is computed to update the model parameters. The gradient vectors are sparsified in such a way that only the top-$k$ elements  are kept. As a result, only $k$ rows or columns  of the weight matrix are modified, leading to a linear reduction  in the computational cost. Surprisingly, experimental results demonstrate that we can update only 1--4\% of the weights at each back propagation pass. This does not result in a larger number of training iterations. More interestingly, the accuracy of the resulting models is actually improved rather than degraded, and a detailed analysis is given. The code is available at \url{https://github.com/lancopku/meProp}.  
  This paper proposes K-NRM, a kernel based neural model for document ranking. Given a query and a set of documents,  K-NRM uses a  translation matrix that models word-level similarities via word embeddings,  a new kernel-pooling technique that uses kernels to extract multi-level soft match features, and a learning-to-rank layer that combines those features into the final ranking score.  The whole model is trained end-to-end. The ranking layer learns desired feature patterns from the pairwise ranking loss.  The kernels transfer the feature patterns into soft-match targets at each similarity level and enforce them on the translation matrix. The word embeddings are tuned accordingly so that they can produce the desired soft matches. Experiments on a commercial search engine's query log demonstrate the improvements of K-NRM over prior feature-based and neural-based states-of-the-art, and explain the source of K-NRM's advantage:  Its kernel-guided embedding encodes a similarity metric tailored for matching query words to document words, and provides effective multi-level soft matches. 
  End-to-end learning treats the entire system as a whole adaptable black box, which, if sufficient data are available, may learn a system that works very well for the target task. This principle has recently been applied to several prototype research on speaker verification , where the feature learning and classifier are learned together with an objective function that is consistent with the evaluation metric. An opposite approach to end-to-end is feature learning, which firstly trains a feature learning model, and then constructs a back-end classifier separately to perform SV.  Recently, both approaches achieved significant performance gains on SV, mainly attributed to the smart utilization of deep neural networks. However, the two approaches have not been carefully compared, and their respective advantages have not been well discussed. In this paper, we compare the end-to-end and feature learning approaches on a text-independent SV task. Our experiments on a dataset sampled from the Fisher database and involving 5,000 speakers demonstrated that the feature learning approach outperformed the end-to-end approach. This is a strong support for the feature learning approach, at least with data and computation resources similar to ours.  
  Existing speaker verification  systems often suffer from performance degradation if there is any language mismatch between model training, speaker enrollment, and test. A major cause of this degradation is that most existing SV methods rely on a probabilistic model to infer the speaker factor, so any significant change on the distribution of the speech signal will impact the inference. Recently, we proposed a deep learning model that can learn how to extract the speaker factor by a deep neural network . By this feature learning, an SV system can be constructed with a very simple back-end model. In this paper, we investigate the robustness of the feature-based SV system in situations with language mismatch. Our experiments were conducted on a complex cross-lingual scenario, where the model training was in English, and the enrollment and test were in Chinese or Uyghur. The experiments demonstrated that the feature-based system outperformed the i-vector system with a large margin, particularly with language mismatch between enrollment and test.    %Recently deep neural networks  have been used to learn speaker features. %By the layer-by-layer processing, speaker-discriminative information can be preserved and strengthened, %while speaker-irrelevant variations, such as phonetic content, are diminished and removed. With a large %training data, the learned speaker feature is assumed to be generalizable. From another perspective, only %if the feature is generalizable, it can be said that the speaker characteristics have been extracted by the feature. %In this paper, we investigate the generalizability of deep speaker features with a cross-lingual speaker %verification  task, where the model is trained with an English database, while the test is on a Chinese-Uyghur %bi-lingual database. Due to the complex data profile and language setting, this task can be used as a good %benchmark to test the generalizability of SV systems. Our experimental results showed that on this task, the deep feature %system outperforms the state-of-the-art i-vector/PLDA model with a large margin, thus demonstrating the %generalizability of the learned features.   
 We present the first approach to automated audio captioning. We employ an encoder-decoder scheme with an alignment model in between. The input to the encoder is a sequence of log mel-band energies calculated from an audio file, while the output is a sequence of words, i.e. a caption. The encoder is a multi-layered, bi-directional gated recurrent unit  and the decoder a multi-layered GRU with a classification layer connected to the last GRU of the decoder. The classification layer and the alignment model are fully connected layers with shared weights between timesteps. The proposed method is evaluated using data drawn from a commercial sound effects library, ProSound Effects. The resulting captions were rated through metrics utilized in machine translation and image captioning fields. Results from metrics show that the proposed method can predict words appearing in the original caption, but not always correctly ordered. 
 Neural IR models, such as DRMM and PACRR, have achieved strong results by successfully capturing relevance matching signals. We argue that the context of these matching signals is also important. Intuitively, when extracting, modeling, and combining matching signals, one would like to consider the surrounding text  as well as other signals from the same document that can contribute to the overall relevance score. In this work, we highlight three potential shortcomings caused by not considering context information and propose three neural ingredients to address them: a disambiguation component, cascade k-max pooling, and a shuffling combination layer. Incorporating these components into the PACRR model yields Co-PACRR, a novel context-aware neural IR model. Extensive comparisons with established models   on Trec Web Track data   confirm that the proposed model can achieve superior search results.   In addition, an ablation analysis is conducted to gain insights   into the impact of and interactions between different components. We release our code to enable future comparisons\footnote{\url{https://github.com/khui/repacrr}}. 
    	 	In this paper,  we introduce a novel approach to generate synthetic data for  training  Neural Machine Translation systems. The proposed approach supports language variants and dialects with very limited parallel training data. This is achieved using a seed data to project words  from a closely-related resource-rich language to an under-resourced language variant  via word embedding representations. The proposed approach is based on localized  embedding projection of distributed representations which utilizes monolingual embeddings and approximate nearest neighbors queries to transform parallel data across language variants.   	 	Our approach is language independent and can be used to generate data for any variant of the source language such as slang or  spoken dialect  or even for a different language that is  related to the source language. We report experimental results on Levantine  to English translation using Neural Machine Translation. We show that the synthetic data can provide significant improvements over  a very large scale system by more than 2.8 Bleu points and it can be used to provide a reliable translation system for a spoken dialect which  does not have sufficient parallel data.    
 Deep reinforcement learning  methods have significant potential for dialogue policy optimisation.  However, they  suffer from a poor performance in the early stages of learning. This is especially problematic for on-line learning with real users. Two approaches are introduced to tackle this problem. Firstly, to speed up the learning process, two sample-efficient neural networks algorithms: trust region actor-critic with experience replay   and episodic natural actor-critic with experience replay  are presented.  For TRACER, the trust region helps to control the learning step size and avoid catastrophic model changes.  For eNACER, the natural gradient identifies the steepest ascent direction in policy space to speed up the convergence. Both models employ  off-policy learning with experience replay to improve sample-efficiency.  Secondly, to mitigate the cold start issue, a corpus of demonstration data is utilised to pre-train the models prior to  on-line reinforcement learning.  Combining these two approaches, we demonstrate a practical approach to learn deep RL-based dialogue policies and demonstrate their effectiveness in a task-oriented information seeking domain.  
 Neural word segmentation has attracted more and more research interests for its ability to alleviate the effort of feature engineering and utilize the external resource by the pre-trained character or word embeddings. In this paper, we propose a new neural model to incorporate the word-level information for Chinese word segmentation. Unlike the previous word-based models, our model still adopts the framework of character-based sequence labeling, which has advantages on both effectiveness and efficiency at the inference stage. To utilize the word-level information, we also propose a new long short-term memory  architecture over directed acyclic graph . Experimental results demonstrate that our model leads to better performances than the baseline models. 
 		In state-of-the-art Neural Machine Translation , an attention mechanism is used during decoding to enhance the translation. At every step, the decoder uses this mechanism to focus on different parts of the source sentence to gather the most useful information before outputting its target word. Recently, the effectiveness of the attention mechanism has also been explored for multimodal tasks, where it becomes possible to focus both on sentence parts and image regions that they describe. In this paper, we compare several attention mechanism on the multimodal translation task  and evaluate the ability of the model to make use of images to improve translation. We surpass state-of-the-art scores on the Multi30k data set, we nevertheless identify and report different misbehavior of the machine while translating.  	
 In Multimodal Neural Machine Translation , a neural model generates a translated sentence that describes an image, given the image itself and one source descriptions in English. This is considered as the multimodal image caption translation task. The images are processed with Convolutional Neural Network  to extract visual features exploitable by the translation model. So far, the CNNs used are pre-trained on object detection and localization task. We hypothesize that richer architecture, such as dense captioning models, may be more suitable for MNMT and could lead to improved translations. We extend this intuition to the word-embeddings, where we compute both linguistic and visual representation for our corpus vocabulary. We combine and compare different configurations and show state-of-the-art results according to previous work.   
  %  %   %  %  %limiting their performance on new    Most previous event extraction studies have relied heavily on features derived from annotated event mentions, thus cannot be applied to new event types without annotation effort. In this work, we take a fresh look at event extraction and model it as a grounding problem. We design a transferable neural architecture, mapping event mentions and types jointly into a shared semantic space using structural and compositional neural networks, where the type of each event mention can be determined by the closest of all candidate types % . By leveraging ~available manual annotations for a small set of existing event types and ~existing event ontologies, our framework applies to new event types without requiring additional annotation. Experiments on both existing event types  and new event types  demonstrate the effectiveness of our approach. Without any manual annotations for 23 new event types, our zero-shot framework achieved performance comparable to a state-of-the-art supervised model which is trained from the annotations of 500 event mentions.  %corpora with  %  %Most previous event extraction studies heavily rely on features encoded from annotated event mentions, thus cannot be applied to new event types without annotation effort. In this work, we take a fresh look at event extraction and model it as a ``grounding'' problem. We design a transferable neural architecture, mapping event mentions and types into a shared semantic space using structural and compositional neural networks, where the type of each event mention can be determined by comparing its similarity with all candidate types. Taking advantage of available manual annotations for a small set of existing old event types, our framework can be applied to any new event types and event ontologies without any annotation effort. Experiments on both the existing event types  and new event types demonstrate the effectiveness of our transferable neural architecture. Without using any manual annotations for new event types, our framework achieves comparable performance with a state-of-the-art supervised model trained from about 500 event annotations for 23 event types. % * <cbonial@me.com> 2017-04-12T21:22:08.261Z: %  % manually annotations --> manual annotations %  % ^.   % Inspired by the success of zero-shot transfer learning on visual object classification, we apply it to event extraction.    % We leverage existing human-constructed event ontologies and manual annotations for a small set of pre-defined event types, and transfer the knowledge of these existing types to extract events of new types.   % Our basic hypothesis is that all event mentions and types can be mapped to a shared semantic space using structural and compositional neural networks, where the type of each event mention can be determined by comparing the similarity with all candidate types.   % XXX grounding problem, mapping/ranking function is universal...  % Experiments on both the existing ACE event types and new event types demonstrate the effectiveness of our transferable neural architecture. Without using any manual annotations on new event types, our framework achieves comparable performance as supervised model trained from about 400 event mentions.   %A major obstacle to progress on event extraction is the poor portability of traditional supervised methods - manual annotation for any new event type is costly; and every time when we receive  %one major reason for slow progress on event extraction is on its poor portability.... annotation is costly ... detailed numbers... and it's difficult to re-use annotations...  %Traditional supervised event extraction methods heavily relied on costly manual annotations.  % provide detailed dollar no  % old types annotations cannot be used for new types  % features encoded from annotated event mentions, thus cannot be applied to new event types without annotation effort.    % Inspired by the success of zero-shot transfer learning on visual object classification, we apply it to event extraction.    % We leverage existing human-constructed event ontologies and manual annotations for a small set of pre-defined event types, and transfer the knowledge of these existing types to extract events of new types.   % Our basic hypothesis is that all event mentions and types can be mapped to a shared semantic space using structural and compositional neural networks, where the type of each event mention can be determined by comparing the similarity with all candidate types.   % XXX grounding problem, mapping/ranking function is universal...  % Experiments on both the existing ACE event types and new event types demonstrate the effectiveness of our transferable neural architecture. Without using any manual annotations on new event types, our framework achieves comparable performance as supervised model trained from about 400 event mentions.     %  %There are two general types of Information Extraction  paradigms: Traditional domain-specific supervised IE that relies on human defined schemas and human annotations and thus yields poor portability and Open IE which is able to extract information for open domains without human supervision but heavily relies on information redundancy. Inspired by the great success of zero-shot transfer learning on image classification, we apply it to combine the merits of these two IE paradigms. Using event extraction as a case study, we leverage existing human constructed schema and manual annotations for a small set of pre-defined event types, and transfer the knowledge of these existing types to the extraction of new event types. Our basic hypothesis is that all event mentions can be grounded to event type space by shared structural and compositional neural networks, thus new event mentions can be mapped to correct event types based on the consistency of their structural semantics. Experiments on both the existing ACE event schema and new event types demonstrate the effectiveness of our transferable neural architecture. Without using any manual annotations on new event types, our framework achieves comparable performance as state-of-the-art supervised model trained from about 400 event mentions.  %There are two general types of Information Extraction  paradigms: Traditional domain-specific supervised IE that relies on human defined schemas and human annotations and thus yields poor portability and Open IE which is able to extract information for open domains without human supervision but heavily relies on information redundancy.       %If we can generate rich semantic representation for each candidate event mention structure EM , and also generate semantic representation for each event type structure ET  in a target event ontology, then we can try to link/map EM to the best ET by comparing their semantic similarity. This nice nature about this new view is that the mapping function will be ``universal'', independent of event types. In other words, if we can learn this mapping function from any old seen event types, we can apply it to new unseen event types.   %We learn the semantic representations of event mention and event type structures by a transferable Neural Network architecture and directly map them into a shared semantic space, as illustrated in Figure 2. Given a rich event ontology, which contains some é–³ãƒ¦ç¬©eené–³ types  with annotated events and é–³ãƒ¦ç”«nseené–³ types with predefined roles, we will leverage this architecture to minimize the distance between each event %mention and its corresponding type. The event mentions from unseen types will be projected into the same semantic space with the same structural composition framework and assigned a top-ranked event type with the highest similarity.   %\lifu{add one sentence for motivation}   %Our basic hypothesis is that all event mentions can be grounded in an event type space using shared structural and compositional neural networks, thus new event mentions can be mapped to correct event types based on the consistency of their structural semantics.  %  %to combine the merits of these two IE paradigms. Using event extraction as a case study,   %Our basic hypothesis is that all event mentions and types can be mapped to a shared semantic space using structural and compositional neural networks, where the type of each event mention can be determined by comparing the similarity with all candidate types.  %\lifu{correct?} %suffers from the long-tail problem and is not easily to be mapped with appointed types. %large amount of labeled data.%than three thousand sentences. %\lifu{whether new event types equal to ``unseen'' types?}   %\todo{problem: open IE programs suffer from quality; traditional supervised IE suffer from portability, is there any way to combine these two, namely to leverage existing human constructed schema and human annotations for existing types, and transfer knowledge to new event types. inspired from the success of zero-shot transfer learning XXX cite in image classification, we propose to apply it for .....}  %\todo{hypotheses: what's sharing between old event types and new event types? what's the intuition for transfer learning to work for this task?}  %\todo{results: better quality on existing types, and also better on new event types than liberal ie, open IE}  %\todo{change to acl17 template; fix citation format}  % \lifu{I think here we want to claim compositional method for mention structure representation can be shared from seen types to unseen types, so new event mentions can also be grounded to event types as the training objective}    
  Slot Filling  aims to extract the values of certain types of attributes  for a given entity from a large collection of source documents.  In this paper we propose an effective DNN architecture for SF with the following new strategies: . Take a regularized dependency graph instead of a raw sentence as input to DNN, to compress the wide contexts between query and candidate filler; . Incorporate two attention mechanisms: local attention learned from query and candidate filler, and global attention learned from external knowledge bases, to guide the model to better select indicative contexts to determine slot type. Experiments show that this framework outperforms state-of-the-art on both relation extraction  and slot filling validation for each individual system .   
  Most of neural approaches to relation classification have focused on finding short patterns that represent the semantic relation using Convolutional Neural Networks  and those approaches have generally achieved better performances than using Recurrent Neural Networks . In a similar intuition to the CNN models, we propose a novel RNN-based model that strongly focuses on only important parts of a sentence using multiple range-restricted bidirectional layers and attention for relation classification. Experimental results on the SemEval-2010 relation classification task show that our model is comparable to the state-of-the-art CNN-based and RNN-based models that use additional linguistic information. 
 While natural languages are compositional, how  state-of-the-art  neural models achieve compositionality  is still unclear.  We propose a  deep   network, which  not only achieves competitive accuracy for text classification, but also  exhibits  compositional behavior. That is, while creating hierarchical  representations of  a piece of text, such as a sentence, the lower layers of the network distribute their  layer-specific  attention weights to individual words. In contrast, the higher layers compose meaningful phrases and clauses, whose lengths increase as the networks get deeper until fully composing the  sentence.  
 Recent developments in deep learning with application to language modeling have led to success in tasks of text processing, summarizing and machine translation.  However, deploying huge language models for mobile device such as on-device keyboards poses computation as a bottle-neck due to their puny computation capacities.  In this work we propose an embedded deep learning based word prediction method that optimizes run-time memory and also provides a real time prediction environment. Our model size is 7.40MB and has average prediction time of 6.47 ms. We improve over the existing methods for word prediction in terms of key stroke savings and word prediction rate. 
  Text preprocessing is often the first step in the pipeline of a Natural Language Processing  system, with potential impact in its final performance.  Despite its importance, text preprocessing has not received much attention in the deep learning literature.  In this paper we investigate the impact of simple text preprocessing decisions  on the performance of a standard neural text classifier.   We perform an extensive evaluation on standard benchmarks from text categorization and sentiment analysis. %While this aspect may be relevant in the final performance of the model, it has not received a substantial interest in the literature.   %In particular, we evaluate the performance of a CNN-based classification model on topic categorization and polarity detection tasks exploiting four simple preprocessing techniques  both in the evaluation sets and on the corpus utilized to train the word embeddings used for initializing the embedding layer.   While our experiments show that a simple tokenization of input text is generally adequate, they also highlight significant degrees of variability across preprocessing techniques. This reveals the importance of paying attention to this usually-overlooked step in the pipeline, particularly when comparing different models. Finally, our evaluation provides insights into the best preprocessing practices for training word embeddings.% , partially explaining the success of the popular pre-trained Word2vec embeddings. 
 Many NLP applications require disambiguating polysemous words.  Existing methods that learn polysemous word vector representations involve first detecting various senses and optimizing the sense-specific embeddings separately, which are invariably more involved than single sense learning methods such as word2vec.  Evaluating these methods is also problematic, as rigorous quantitative evaluations in this space is limited, especially when compared with single-sense embeddings. In this paper, we propose a simple method to learn a word representation, given {}  
 Neural machine translation models rely on the beam search algorithm for decoding. In practice, we found that the quality of hypotheses in the search space is negatively affected owing to the fixed beam size. To mitigate this problem, we store all hypotheses in a single priority queue and use a universal score function for hypothesis selection. The proposed algorithm is more flexible as the discarded hypotheses can be revisited in a later step. We further design a penalty function to punish the hypotheses that tend to produce a final translation that is much longer or shorter than expected. Despite its simplicity, we show that the proposed decoding algorithm is able to select hypotheses with better qualities and improve the translation performance.   
 Question answering is an important and difficult task in the natural language  processing domain, because many basic natural language processing tasks can be cast  into a question answering task.  Several deep neural network architectures have been developed recently,  which employ memory and inference components to memorize and reason over text information, and generate answers to questions.   However, a major drawback of many such models is that they are capable of only generating single-word answers. In addition, they require large amount of training data to generate accurate answers.   In this paper, we introduce the Long-Term  Memory Network , which incorporates both an external memory module and a Long Short-Term Memory   module to comprehend the input data and generate multi-word answers. The LTMN model can be trained end-to-end using back-propagation and requires minimal supervision. We test our model on two synthetic data sets  and the real-world Stanford question  answering data set, and show that it can achieve  state-of-the-art performance. 
  Grammatical error correction  systems strive to correct both global errors in word order and usage, and local errors in spelling and inflection. Further developing upon recent work on neural machine translation, we propose a new hybrid neural model with nested attention layers for GEC. Experiments show that the new model can effectively correct errors of both types by  incorporating word and character-level information, and that the model significantly outperforms previous neural models for GEC as measured on the standard CoNLL-14 benchmark dataset. Further analysis also shows that the superiority of the proposed model can be largely attributed to the use of the nested attention mechanism, which has proven particularly effective in correcting local errors that involve small edits in orthography.  
 Most work on neural natural language generation  focus on controlling the content of the generated text. We experiment with controlling several stylistic aspects of the generated text, in addition to its content. The method is based on conditioned RNN language model, where the desired content as well as the stylistic parameters serve as conditioning contexts. We demonstrate the approach on the movie reviews domain and show that it is successful in generating coherent sentences corresponding to the required linguistic style and content.  
  Recent work has proposed several generative neural models for constituency parsing that achieve state-of-the-art results. Since direct search in these generative models is difficult, they have primarily been used to rescore candidate outputs from base parsers in which decoding is more straightforward.  We first present an algorithm for direct search in these generative models.  We then demonstrate that the rescoring results are at least partly due to implicit model combination rather than reranking effects.  Finally, we show that explicit model combination can improve performance even further, resulting in new state-of-the-art numbers on the PTB of 94.25 F1 when training only on gold data and 94.66 F1 when using external data.  
    Progress in natural language interfaces to databases  has been slow mainly due to linguistic issues  and domain portability. Moreover, the lack of a large corpus to be used as a standard benchmark has made data-driven approaches difficult to develop and compare. In this paper, we revisit the problem of NLIDBs and recast it as a sequence translation problem. To this end, we introduce a large dataset extracted from the Stack Exchange Data Explorer website, which can be used for training neural natural language interfaces for databases. We also report encouraging baseline results on a smaller manually annotated test corpus, obtained using an attention-based sequence-to-sequence neural network.  
 \footnotesize  Foreign policy analysis has been struggling to find ways to measure policy preferences and paradigm shifts in international political systems. This paper presents a novel, potential solution to this challenge, through the application of a neural word embedding  model on a dataset featuring speeches by heads of state or government in the United Nations General Debate. The paper provides three key contributions based on the output of the Word2vec model. First, it presents a set of policy attention indices, synthesizing the semantic proximity of political speeches to specific policy themes. Second, it introduces country-specific semantic centrality indices, based on topological analyses of countries' semantic positions with respect to each other. Third, it tests the hypothesis that there exists a statistical relation between the semantic content of political speeches and UN voting behavior, falsifying it and suggesting that political speeches contain information of different nature then the one behind voting outcomes. The paper concludes with a discussion of the practical use of its results and consequences for foreign policy analysis, public accountability, and transparency.  
 We present a new approach to extraction of hypernyms based on projection learning and word embeddings. In contrast to classification-based approaches, projection-based methods require no candidate hyponym-hypernym pairs. While it is natural to use both positive and negative training examples in supervised relation extraction, the impact of negative examples on hypernym prediction was not studied so far. In this paper, we show that explicit negative examples used for regularization of the model significantly improve performance compared to the state-of-the-art approach of~ on three datasets from different languages. 
 The interpretation of spatial references is highly contextual, requiring joint inference over both language and the environment. We consider the task of spatial reasoning in a simulated environment, where an agent can act and receive rewards. The proposed model learns a representation of the world steered by instruction text.  This design allows for precise alignment of local neighborhoods with corresponding verbalizations, while also handling global references in the instructions. We train our model with reinforcement learning using a variant of generalized value iteration. The model outperforms state-of-the-art approaches on several metrics, yielding a 45\% reduction in goal localization error. \footnote{Code and dataset are available at \url{https://github.com/JannerM/spatial-reasoning}} % \footnote{Code and dataset are available at \url{anonymized}}   % This paper proposes a novel approach for grounded spatial reasoning in natural language. The interpretation of spatial references is highly contextual, requiring joint reasoning over both language and the environment. Further, the rich variety of linguistic expressions demands an appropriate representation that can generalize well across different verbalizations. With a view to address both challenges, we explore a model that learns a representation of the world steered by instruction text. Our design allows for precise reasoning over local neighborhoods while also handling global references in the instructions. We train our model with reinforcement learning, using only reward-based feedback provided by the environment.  
  We study in this work the importance of depth in convolutional models for text classification, either when character or word inputs are considered. We show on 5 standard text classification and sentiment analysis tasks that  deep models indeed give better performances than shallow networks when the text input is represented as a sequence of characters. However, a simple shallow-and-wide network outperforms deep models such as DenseNet with word inputs. Our shallow word model further establishes new state-of-the-art performances on two datasets: Yelp Binary  and Yelp Full . 
 Tagging news articles or blog posts with relevant tags from a collection of predefined ones is coined as document tagging in this work. Accurate tagging of articles can benefit several downstream applications such as recommendation and search. In this work, we propose a novel yet simple approach called DocTag2Vec to accomplish this task. We substantially extend Word2Vec and Doc2Vec---two popular models for learning  distributed representation of words and documents. In DocTag2Vec, we simultaneously learn the representation of words, documents, and tags in a joint vector space during training, and employ the simple $k$-nearest neighbor search to predict tags for unseen documents. In contrast to previous multi-label learning methods, DocTag2Vec directly deals with raw text instead of provided feature vector, and in addition, enjoys advantages like the learning of tag representation, and the ability of handling newly created tags. To demonstrate the effectiveness of our approach, we conduct experiments on several datasets and show promising results against state-of-the-art methods.  
 		Despite the close relationship between speech perception and production, research in automatic speech recognition  and text-to-speech synthesis  has progressed more or less independently without exerting much mutual influence on each other. In human communication, on the other hand, a closed-loop speech chain mechanism with auditory feedback from the speaker's mouth to her ear is crucial. In this paper, we take a step further and develop a closed-loop speech chain model based on deep learning. The sequence-to-sequence model in close-loop architecture allows us to train our model on the concatenation of both labeled and unlabeled data. While ASR transcribes the unlabeled speech features, TTS attempts to reconstruct the original speech waveform based on the text from ASR. In the opposite direction, ASR also attempts to reconstruct the original text transcription given the synthesized speech. To the best of our knowledge, this is the first deep learning model that integrates human speech perception and production behaviors. Our experimental results show that the proposed approach significantly improved the performance more than separate systems that were only trained with labeled data. 	
 This paper proposes a hierarchical attentional neural translation model which   %incorporates a bidirectional source-side tree-based encoder. Unlike the representations of parent nodes in the existing syntactic-based neural translation models which solely rely on child nodes, the proposed model    focuses on enhancing source-side hierarchical representations by covering both local and global semantic information using a bidirectional tree-based encoder.   To maximize the predictive likelihood of target words, a weighted variant of an attention mechanism is used to balance the attentive information between lexical and phrase vectors.   Using a tree-based rare word encoding, the proposed model is extended to sub-word level to alleviate the out-of-vocabulary  problem.   %In order to alleviate the out-of-vocabulary  problem, the proposed model is also extended to sub-word level to encode rare words following a lexical tree.   Empirical results reveal that the proposed model significantly outperforms sequence-to-sequence attention-based and  %syntactic   {tree}-based neural translation models in English-Chinese translation tasks. 
 We investigate the utility of different auxiliary objectives and training strategies within a neural sequence labeling approach to error detection in learner writing.  Auxiliary costs provide the model with additional linguistic information, allowing it to learn general-purpose compositional features that can then be exploited for other objectives. Our experiments show that a joint learning approach trained with parallel labels on in-domain data improves performance over the previous best error detection system.  While the resulting model has the same number of parameters, the additional objectives allow it to be optimised more efficiently and achieve better performance. 
 Domain similarity measures can be used to gauge adaptability and select suitable data for transfer learning, but existing approaches define ad hoc measures that are deemed suitable for respective tasks. Inspired by work on curriculum learning, we propose to learn data selection measures using Bayesian Optimization and evaluate them across models, domains and tasks. Our learned measures outperform existing domain similarity measures significantly on three tasks: sentiment analysis, part-of-speech tagging, and parsing.  We show the importance of complementing similarity with diversity, and that learned measures are---to some degree---transferable across models, domains, and even tasks. 
 In this paper we aim to automatically discover high quality frame-level speech features and acoustic tokens directly from unlabeled speech data. A Multi-granular Acoustic Tokenizer  was proposed for automatic discovery of multiple sets of acoustic tokens from the given corpus. Each acoustic token set is specified by a set of hyperparameters describing the model configuration. These different sets of acoustic tokens carry different characteristics for the given corpus and the language behind, thus can be mutually reinforced. The multiple sets of token labels are then used as the targets of a Multi-target Deep Neural Network  trained on frame-level acoustic features. Bottleneck features extracted from the MDNN are then used as the feedback input to the MAT and the MDNN itself in the next iteration. The multi-granular acoustic token sets and the frame-level speech features can be iteratively optimized in the iterative deep learning framework. We call this framework the Multi-granular Acoustic Tokenizing Deep Neural Network . The results were evaluated using the metrics and corpora defined in the Zero Resource Speech Challenge organized at Interspeech 2015, and improved performance was obtained with a set of experiments of query-by-example spoken term detection on the same corpora. Visualization for the discovered tokens against the English phonemes was also shown. 
 Most neural machine translation  models are based on the sequential encoder-decoder framework, which makes no use of syntactic information. In this paper, we improve this model by explicitly incorporating source-side syntactic trees. More specifically, we propose  a  which learns both sequential and tree structured representations;  a  that lets the attention depend on the source-side syntax. Experiments on Chinese-English translation demonstrate that our proposed models outperform the sequential attentional model as well as a stronger baseline with a bottom-up tree encoder and word coverage.\footnote{Our code is publicly available at \url{https://github.com/howardchenhd/Syntax-awared-NMT/}} 
 Being a matter of cognition, user interests should be apt to classification independent of the language of users, social network and content of interest itself. To prove it, we analyze a collection of English and Russian Twitter and Vkontakte community pages by interests of their followers. First, we create a model of Major Interests  with the help of expert analysis and then classify a set of pages using machine learning algorithms . We take three interest domains that are typical of both English and Russian-speaking communities: football, rock music, vegetarianism.  The results of classification show a greater correlation between Russian-Vkontakte and Russian-Twitter pages while English-Twitter pages appear to provide the highest score.  
   Ongoing innovations in recurrent neural network   architectures have provided a steady influx of apparently   state-of-the-art results on language modelling benchmarks. However,   these have been evaluated using differing codebases and limited   computational resources, which represent uncontrolled sources of   experimental variation. We reevaluate several popular   architectures and regularisation methods with large-scale automatic   black-box hyperparameter tuning and arrive at the somewhat   surprising conclusion that standard LSTM architectures, when   properly regularised, outperform more recent models. We establish a   new state of the art on the Penn Treebank and Wikitext-2 corpora, as   well as strong baselines on the Hutter Prize dataset. 
 	This paper presents our novel method to encode word confusion networks, which can represent a rich hypothesis space of automatic speech recognition systems, via recurrent neural networks. 	We demonstrate the utility of our approach for the task of dialog state tracking in spoken dialog systems that relies on automatic speech recognition output. 	Encoding confusion networks outperforms encoding the best hypothesis of the automatic speech recognition in a neural system for dialog state tracking on the well-known second Dialog State Tracking Challenge dataset. 	 
 Layer normalization is a recently introduced technique for normalizing the activities of neurons in deep neural networks to improve the training speed and stability. In this paper, we introduce a new layer normalization technique called Dynamic Layer Normalization~ for adaptive neural acoustic modeling in speech recognition. By dynamically generating the scaling and shifting parameters in layer normalization, DLN adapts neural acoustic models to the acoustic variability arising from various factors such as speakers, channel noises, and environments. Unlike other adaptive acoustic models, our proposed approach does not require additional adaptation data or speaker information such as i-vectors. Moreover, the model size is fixed as it dynamically generates adaptation parameters. We apply our proposed DLN to deep bidirectional LSTM acoustic models and evaluate them on two benchmark datasets for large vocabulary ASR experiments: WSJ and TED-LIUM release 2. The experimental results show that our DLN improves neural acoustic models in terms of transcription accuracy by dynamically adapting to various speakers and environments.  
 In this paper, we introduce the novel concept of densely connected layers into recurrent neural networks. We evaluate our proposed architecture on the Penn Treebank language modeling task. We show that we can obtain similar perplexity scores with six times fewer parameters compared to a standard stacked 2-layer LSTM model trained with dropout . In contrast with the current usage of skip connections, we show that densely connecting only a few stacked layers with skip connections already yields significant perplexity reductions.  % While skip connections are mainly used to improve gradient flow in large networks, we show that they are very effective   
 Reinforcement learning is widely used for dialogue policy optimization where the reward function often consists of more than one component, e.g., the dialogue success and the dialogue length. In this work, we propose a structured method for finding a good balance between these components by searching  for the optimal reward component weighting. %through the balance space.  To render this search feasible, we use multi-objective reinforcement learning to significantly reduce the number of training dialogues required. We apply our proposed method to find optimized component weights for six domains and compare them to a default baseline. 
 We investigate grounded sentence representations, where we train a sentence encoder to predict the image features of a given caption---i.e., we try to ``imagine'' how a sentence would be depicted visually---and use the resultant features as sentence representations. We examine the quality of the learned representations on a variety of standard sentence representation quality benchmarks, showing improved performance for grounded models over non-grounded ones. In addition, we thoroughly analyze the extent to which grounding contributes to improved performance, and show that the system also learns improved word embeddings. 
 Syllabification does not seem to improve word-level RNN language modeling quality when compared to character-based segmentation. However, our best syllable-aware language model, achieving performance comparable to the competitive character-aware model, has 18\%--33\% fewer parameters and is trained 1.2--2.2 times faster. 
 %Pioneering works on fixed dimensional vector representations of audio word segments show favorable result such as embedding semantic information or phonetic structure into the vectors. While previous works focus on a single language, this paper examined the transferability of unsupervised Audio Word2Vec and succesfully  transform audio segments from low-resource language into vectors using the seq2seq autoencoder trained with high-resource language. The result eliminates the challenge of training Audio Word2Vec in low-resource langauge and further expand the applications of the Audio Word2Vec model. In the Spoken Term Detection  application, the performance of the cross-lingual seq2seq autoencoder surpasses the cross-lingual naive autoencoder and the seq2seq autoencoder trained with the queried, low-resource language. These experimental result indicates that the proposed model successfully extracts phonetic structure commonly appeared in spoken language. Besides the STD application, this paper also provide a thorough analysis and visualization on the vector representation. Audio Word2Vec offers vector representations of fixed dimensionality for variable-length audio segments using Sequence-to-sequence Autoencoder . These vector representations are shown to describe the sequential phonetic structures of the audio segments to a good degree, with real world applications such as query-by-example Spoken Term Detection . This paper examines the capability of language transfer of Audio Word2Vec. We train $SA$ from one language  and use it to extract the vector representation of the audio segments of another language . We found that $SA$ can still catch phonetic structure from the audio segments of the target language if the source and target languages are similar.  In query-by-example STD, we obtain the vector representations from the $SA$ learned from a large amount of source language data, and found them surpass the representations from naive encoder and $SA$ directly learned from a small amount of target language data. The result shows that it is possible to learn Audio Word2Vec model from high-resource languages and use it on low-resource languages. This further expands the usability of Audio Word2Vec.  
   Distributional semantics models are known to struggle with small data. It is generally accepted that in order to learn `a good vector' for a word, a model must have sufficient examples of its usage.  This contradicts the fact that humans can guess the meaning of a word from a few occurrences only.  In this paper, we show that a neural language model such as Word2Vec only necessitates minor modifications to its standard architecture to learn new terms from tiny data, using background knowledge from a previously learnt semantic space. We test our model on word definitions and on a nonce task involving 2-6 sentences' worth of context, showing a large increase in performance over state-of-the-art models on the definitional task. 
 We study the problem of learning to reason in large scale knowledge graphs . More specifically, we describe a novel reinforcement learning framework for learning multi-hop relational paths: we use a policy-based agent with continuous states based on knowledge graph embeddings, which reasons in a KG vector space by sampling the most promising relation to extend its path. In contrast to prior work, our approach includes a reward function that takes the accuracy, diversity, and efficiency into consideration. Experimentally, we show that our proposed method outperforms a path-ranking based algorithm and knowledge graph embedding methods on Freebase and Never-Ending Language Learning datasets.\footnote{Code and the NELL dataset are available at \url{https://github.com/xwhan/DeepPath}.} 
 With the ever decreasing attention span of contemporary Internet users, the title of online content  can be a major factor in determining its popularity. To take advantage of this phenomenon, we propose a new method based on a bidirectional Long Short-Term Memory  neural network designed to predict the popularity of online content using only its title. We evaluate the proposed architecture on two distinct datasets of news articles and news videos distributed in social media that contain over 40,000 samples in total. On those datasets, our approach improves the performance over traditional shallow approaches by a margin of 15\%. Additionally, we show that using pre-trained word vectors in the embedding layer improves the results of LSTM models, especially when the training set is small.    To our knowledge, this is the first attempt of applying popularity prediction using only textual information from the title. 
 We introduce the first end-to-end coreference resolution model and show that it significantly outperforms all previous work without using a syntactic parser or hand-engineered mention detector. The key idea is to directly consider all spans in a document as potential mentions and learn distributions over possible antecedents for each. The model computes span embeddings that combine context-dependent boundary representations with a head-finding attention mechanism. It is trained to maximize the marginal likelihood of gold antecedent spans from coreference clusters and is factored to enable aggressive pruning of potential mentions. Experiments demonstrate state-of-the-art performance, with a gain of 1.5 F1 on the OntoNotes benchmark and by 3.1 F1 using a 5-model ensemble, despite the fact that this is the first approach to be successfully trained with no external resources. 
 \fontsize{10}{11}\selectfont We study the problem of domain adaptation for neural abstractive summarization. We make initial efforts in investigating what information can be transferred to a new domain. Experimental results on news stories and opinion articles indicate that neural summarization model benefits from pre-training based on extractive summaries. We also find that the combination of in-domain and out-of-domain setup yields better summaries when in-domain data is insufficient. Further analysis shows that, the model is capable to select salient content even trained on out-of-domain data, but requires in-domain data to capture the style for a target domain. 
 	 Trans-dimensional random field language models  have recently been introduced, where sentences are modeled as a collection of random fields. The TRF approach has been shown to have the advantages of being computationally more efficient in inference than LSTM LMs with close performance and being able to flexibly integrate rich features. In this paper we propose neural TRFs, beyond of the previous discrete TRFs that only use linear potentials with discrete features. The idea is to use nonlinear potentials with continuous features, implemented by neural networks , in the TRF framework. Neural TRFs combine the advantages of both NNs and TRFs. The benefits of word embedding, nonlinear feature learning and larger context modeling are inherited from the use of NNs. At the same time, the strength of efficient inference by avoiding expensive softmax is preserved. A number of technical contributions, including employing deep convolutional neural networks  to define the potentials and incorporating the joint stochastic approximation  strategy in the training algorithm, are developed in this work, which enable us to successfully train neural TRF LMs. Various LMs are evaluated in terms of speech recognition WERs by rescoring the 1000-best lists of WSJ'92 test data. The results show that neural TRF LMs not only improve over discrete TRF LMs, but also perform slightly better than LSTM LMs with only one fifth of parameters and 16x faster inference efficiency.  
 Multimodal sentiment analysis is an increasingly popular research area, which extends the conventional language-based definition of sentiment analysis to a multimodal setup where other relevant modalities accompany language. In this paper, we pose the problem of multimodal sentiment analysis as modeling intra-modality and inter-modality dynamics. We introduce a novel model, termed \mnb, which learns both such dynamics end-to-end.  The proposed approach is tailored for the volatile nature of spoken language in online videos as well as accompanying gestures and voice. In the experiments, our model outperforms state-of-the-art approaches for both multimodal and unimodal sentiment analysis. 
 In this work, we perform an empirical comparison among the CTC, RNN-Transducer, and  attention-based Seq2Seq models for end-to-end speech recognition. We show that, without any language model, Seq2Seq and RNN-Transducer models both outperform the best reported CTC models with a language model, on the popular Hub5'00 benchmark.  On our internal diverse dataset, these trends continue - RNN-Transducer models rescored with a language model after beam search outperform our best CTC models. These results simplify the speech recognition pipeline so that decoding can now be expressed purely as neural network operations. We also study how the choice of encoder architecture affects the performance of the three models - when all encoder layers are forward only, and when encoders downsample the input representation aggressively. 
 Natural language inference  is a central problem in language understanding. End-to-end artificial neural networks have reached state-of-the-art performance in NLI field recently.   In this paper, we propose Character-level Intra Attention Network  for the NLI task. In our model, we use the character-level convolutional network to replace the standard word embedding layer, and we use the intra attention to capture the intra-sentence semantics. The proposed CIAN model provides improved results based on a newly published MNLI corpus. 
   We propose a methodology that adapts graph embedding techniques  as well as cross-lingual vector space mapping approaches  in order to merge the corpus and ontological sources of lexical knowledge. We also  perform comparative  analysis of the  used algorithms in order to identify the best combination for the proposed system. We then apply this to the task of enhancing the coverage of an existing word embedding's vocabulary with rare and unseen words. We show that our technique can provide considerable extra coverage , leading to consistent performance gain  on the Rare Word Similarity dataset. 
 In this paper we propose a model to learn multimodal  multilingual representations for matching  images and sentences in different languages,  with the aim of advancing multilingual  versions of image search and image understanding. Our model learns a common representation for images and their descriptions in two different languages  by considering the image  as a pivot between two languages. We introduce a new pairwise ranking loss function  which can handle both symmetric and asymmetric  similarity between the two modalities. We evaluate our models on image-description ranking for  German and English, and on semantic textual similarity of image descriptions in English.  In both cases we achieve state-of-the-art performance.  
  It has been shown that increasing model depth improves the quality of neural machine translation. However, different architectural variants to increase model depth have been proposed, and so far, there has been no thorough comparative study.  In this work, we describe and evaluate several existing approaches to introduce depth in neural machine translation. Additionally, we explore novel architectural variants, including deep transition RNNs, and we vary how attention is used in the deep decoder. We introduce a novel "BiDeep" RNN architecture that combines deep transition RNNs and stacked RNNs.  Our evaluation is carried out on the English to German WMT news translation dataset, using a single-GPU machine for both training and inference. We find that several of our proposed architectures improve upon existing approaches in terms of speed and translation quality. We obtain best improvements with a BiDeep RNN of combined depth 8, obtaining an average improvement of 1.5 {\sc Bleu} over a strong shallow baseline.  We release our code for ease of adoption.  
 We introduce globally normalized convolutional neural networks for %a non-sequence labeling task, namely  joint entity classification and relation extraction. In particular, we propose a way to utilize a linear-chain conditional random field output layer for predicting entity types and relations between entities at the same time. Our experiments show that global normalization outperforms a locally normalized softmax layer on a benchmark dataset. 
 Question Answering is a task which requires building models capable of providing answers to questions expressed in human language. Full question answering involves some form of reasoning ability. We introduce a neural network architecture for this task, which is a form of Memory Network, that recognizes entities and their relations to answers through a focus attention mechanism. Our model is named  and extends  by exploiting aspects of the question during the memorization process. We validate the model on both synthetic and real datasets: the  question answering dataset and the   dataset. In our experiments, the models achieved a State-of-The-Art in the former and competitive results in the latter.  
 Search systems are often focused on providing relevant results for the ``now'', assuming both corpora and user needs that focus on the present. However, many corpora today reflect significant longitudinal collections ranging from 20 years of the Web to hundreds of years of digitized newspapers and books. Understanding the temporal intent of the user and retrieving the most relevant historical content has become a significant challenge. Common search features, such as query expansion, leverage the relationship between terms but cannot function well across all times when relationships vary temporally. In this work, we introduce a temporal relationship model that is extracted from longitudinal data collections. The model supports the task of identifying, given two words, when they relate to each other. We present an algorithmic framework for this task and show its application for the task of query expansion, achieving high gain. 
 We investigate the compositional structure of message vectors computed by a deep network trained on a communication game. By comparing truth-conditional representations of encoder-produced message vectors to human-produced referring expressions, we are able to identify aligned  pairs with the same meaning. We then search for structured relationships among these aligned pairs to discover simple vector space transformations corresponding to negation, conjunction, and disjunction. Our results suggest that neural representations are capable of spontaneously developing a ``syntax'' with functional analogues to qualitative properties of natural language.\footnote{ Code and data are available at \url{http://github.com/jacobandreas/rnn-syn}.} 
 %Rectified Linear Units  are widely used in feed-forward neural networks, and in convolutional neural networks in particular. However, they can be rarely found in recurrent neural networks due to the unboundedness and the positive image of the rectified linear activation function.  %In this paper, we introduce Dual Rectified Linear Units , a novel type of rectified unit that comes with an unbounded positive and negative image and can be used as a drop-in replacement for a  activation function in the recurrent step of Quasi-Recurrent Neural Networks  . Like ReLUs, DReLUs are less prone to the vanishing gradient problem, they are noise robust, and they induce sparse activations.  In this paper, we introduce a novel type of Rectified Linear Unit , called a Dual Rectified Linear Unit . A DReLU, which comes with an unbounded positive and negative image, can be used as a drop-in replacement for a  activation function in the recurrent step of Quasi-Recurrent Neural Networks  . Similar to ReLUs, DReLUs are less prone to the vanishing gradient problem, they are noise robust, and they induce sparse activations.  We independently reproduce the QRNN experiments of  and compare our DReLU-based QRNNs with the original -based QRNNs and Long Short-Term Memory networks  on sentiment classification and word-level language modeling. Additionally, we evaluate on character-level language modeling, showing that we are able to stack up to eight QRNN layers with DReLUs, thus making it possible to improve the current state-of-the-art in character-level language modeling over shallow architectures based on LSTMs.  % Additionally, we introduce an exponential variant, called Dual Exponential Linear Unit , which performs even better when training deep stacks of neural network layers. 
 Practitioners apply neural networks to increasingly complex problems in natural language processing, such as syntactic parsing and semantic role labeling that have rich output structures. Many such structured-prediction problems require deterministic constraints on the output values; for example, in sequence-to-sequence syntactic parsing, we require that the sequential outputs encode valid trees. While hidden units might capture such properties, the network is not always able to learn such constraints from the training data alone, and practitioners must then resort to post-processing.  In this paper, we present an inference method for neural networks that enforces deterministic constraints on outputs without performing rule-based post-processing or expensive discrete search.  Instead, in the spirit of gradient-based training, we enforce constraints with gradient-based inference : for each input at test-time, we nudge continuous model weights until the network's unconstrained inference procedure generates an output that satisfies the constraints.  We study the efficacy of GBI on three tasks with hard constraints: semantic role labeling, syntactic parsing, and sequence transduction.   In each case, the algorithm not only satisfies constraints, but improves accuracy, even when the underlying network is state-of-the-art. 
 Deep residual learning ~ is a new method for training very deep neural networks using identity mapping for shortcut connections. ResNet has won the ImageNet ILSVRC 2015 classification task, and achieved state-of-the-art performances in many computer vision tasks. However, the effect of residual learning on noisy natural language processing tasks is still not well understood. In this paper, we design a novel convolutional neural network  with residual learning, and investigate its impacts on the task of distantly supervised noisy relation extraction. In contradictory to popular beliefs that ResNet only works well for very deep networks, we found that even with 9 layers of CNNs, using identity mapping could significantly improve the performance for distantly-supervised relation extraction. 
 Generative neural models have recently achieved state-of-the-art results for constituency parsing. However, without a feasible search procedure, their use has so far been limited to reranking the output of external parsers in which decoding is more tractable. We describe an alternative to the conventional action-level beam search used for discriminative neural models that enables us to decode directly in these generative models. We then show that by improving our basic candidate selection strategy and using a coarse pruning function, we can improve accuracy while exploring significantly less of the search space. Applied to the model of , our inference procedure obtains 92.56 F1 on section 23 of the Penn Treebank, surpassing prior state-of-the-art results for single-model systems. 
   We introduce and describe the results of a novel shared task on bandit learning for machine translation. The task was organized jointly by Amazon and Heidelberg University for the first time at the Second Conference on Machine Translation . The goal of the task is to encourage research on learning machine translation from weak user feedback instead of human references or post-edits. On each of a sequence of rounds, a machine translation system is required to propose a translation for an input, and receives a real-valued estimate of the quality of the proposed translation for learning. This paper describes the shared task's learning and evaluation setup, using services hosted on Amazon Web Services , the data and evaluation metrics, and the results of various machine translation architectures and learning protocols.    
 \footnotesize  In complex, high dimensional and unstructured data it is often difficult to extract meaningful patterns. This is especially the case when dealing with textual data. Recent studies in machine learning, information theory and network science have developed several novel instruments to extract the semantics of unstructured data, and harness it to build a network of relations. Such approaches serve as an efficient tool for dimensionality reduction and pattern detection. This paper applies semantic network science to extract ideological proximity in the international arena, by focusing on the data from General Debates in the UN General Assembly on the topics of high salience to international community. UN General Debate corpus  covers all high-level debates in the UN General Assembly from 1970 to 2014, covering all UN member states. The research proceeds in three main steps. First, Latent Dirichlet Allocation  is used to extract the topics of the UN speeches, and therefore semantic information. Each country is then assigned a vector specifying the exposure to each of the topics identified. This intermediate output is then used in to construct a network of countries based on information theoretical metrics where the links capture similar vectorial patterns in the topic distributions. Topology of the networks is then analyzed through network properties like density, path length and clustering. Finally, we identify specific topological features of our networks using the map equation framework to detect communities in our networks of countries.       Key Words: Topic modeling, network science, topology, information theory, map equation framework.   
 We examine the effects of particular orderings of sentence pairs on the on-line training of neural machine translation . We focus on two types of such orderings:  ensuring that each minibatch contains sentences similar in some aspect and  gradual inclusion of some sentence types as the training progresses . In our English-to-Czech experiments, the internal homogeneity of minibatches has no effect on the training but some of our ``curricula'' achieve a small improvement over the baseline. 
 	One central mystery of neural NLP is what neural models ``know'' about their subject matter.  When a neural machine translation system learns to translate from one language to another, does it learn the syntax or semantics of the languages?  Can this knowledge be extracted from the system to fill holes in human scientific knowledge? Existing typological databases contain relatively full feature specifications for only a few hundred languages. Exploiting the existence of parallel texts in more than a thousand languages, we build a massive many-to-one neural machine translation  system from 1017 languages into English, and use this to predict information missing from typological databases. Experiments show that the proposed method is able to infer not only syntactic, but also phonological and phonetic inventory features, and improves over a baseline that has access to information about the languages' geographic and phylogenetic neighbors.\footnote{Code and learned vectors are available at \url{http://github.com/chaitanyamalaviya/lang-reps}} 
  Unsupervise learned word embeddings have seen tremendous success in numerous Natural Language Processing  tasks in recent years. The main contribution of this paper is to develop a technique called Skill2vec, which applies machine learning techniques in recruitment to enhance the search strategy to find candidates possessing the appropriate skills. Skill2vec is a neural network architecture inspired by Word2vec, developed by Mikolov et al. in 2013. It transforms skills to new vector space, which has the characteristics of calculation and presents skills relationships. We conducted an experiment evaluation manually by a recruitment company's domain experts to demonstrate the effectiveness of our approach.   
 In this paper we show that reporting a single performance score is insufficient to compare non-deterministic approaches. We demonstrate for common sequence tagging tasks that the seed value for the random number generator can result in statistically significant  differences for state-of-the-art systems. For two recent systems for NER, we observe an absolute difference of one percentage point $F_1$-score depending on the selected seed value, making these systems perceived either as state-of-the-art or mediocre. Instead of publishing and reporting single performance scores, we propose to compare score distributions based on multiple executions.  Based on the evaluation of  LSTM-networks for five sequence tagging tasks, we present network architectures that produce both superior performance as well as are more stable with respect to the remaining hyperparameters. The full experimental results are published in .\footnote{\url{https://arxiv.org/abs/1707.06799}} The implementation of our network is publicly available.\footnote{\url{https://github.com/UKPLab/emnlp2017-bilstm-cnn-crf}}  
  The necessity of using a fixed-size word vocabulary in order to control the model complexity in state-of-the-art neural machine translation  systems is an important bottleneck on performance, especially for morphologically rich languages. Conventional methods that aim to overcome this problem by using sub-word or character-level representations solely rely on statistics and disregard the linguistic properties of words, which leads to interruptions in the word structure and causes semantic and syntactic losses.  In this paper, we propose a new vocabulary reduction method for NMT, which can reduce the vocabulary of a given input corpus at any rate while also considering the morphological properties of the language. Our method is based on unsupervised morphology learning and can be, in principle, used for pre-processing any language pair. We also present an alternative word segmentation method based on supervised morphological analysis, which aids us in measuring the accuracy of our model. We evaluate our method in Turkish-to-English NMT task where the input language is morphologically rich and agglutinative. We analyze different representation methods in terms of translation accuracy as well as  the semantic and syntactic properties of the generated output. Our  method obtains a significant improvement of 2.3 BLEU points over the conventional vocabulary reduction technique, showing that it can provide better accuracy in open vocabulary translation of morphologically rich languages.  
   We investigate techniques for supervised domain adaptation for neural machine translation where an existing model trained on a large out-of-domain dataset is adapted to a small in-domain dataset.    In this scenario, overfitting is a major challenge. We investigate a number of techniques to reduce overfitting and improve transfer learning, including regularization techniques such as dropout and L2-regularization towards an out-of-domain prior. In addition, we introduce , a novel regularization technique inspired by dropout. We apply these techniques, alone and in combination, to neural machine translation, obtaining improvements on IWSLT datasets for English$\rightarrow$German and English$\rightarrow$Russian. We also investigate the amounts of in-domain training data needed for domain adaptation in NMT, and find a logarithmic relationship between the amount of training data and gain in BLEU score. 
 	Achieving artificial visual reasoning --- the ability to answer image-related questions which require a multi-step, high-level process --- is an important step towards artificial general intelligence. This multi-modal task requires learning a question-dependent, structured reasoning process over images from language. Standard deep learning approaches tend to exploit biases in the data rather than learn this underlying structure, while leading methods learn to visually reason successfully but are hand-crafted for reasoning. We show that a general-purpose, Conditional Batch Normalization approach achieves state-of-the-art results on the CLEVR Visual Reasoning benchmark with a 2.4\% error rate. We outperform the next best end-to-end method  and even methods that use extra supervision . We probe our model to shed light on how it reasons, showing it has learned a question-dependent, multi-step process. Previous work has operated under the assumption that visual reasoning calls for a specialized architecture, but we show that a general architecture with proper conditioning can learn to visually reason effectively. 
 % In the framework of Twitter sentiment analysis different studies tackle problems like binary, ternary  or fine-grained  sentiment classification.  % Instead of learning the tasks separately as commonly done, we propose a neural-network, multitask model that jointly learns them.  % The model builds on a shared recurrent neural network, trained on both ternary and fine-grained classification, in a multitask fashion.  % Our study demonstrates the potential of multitask models on the problem of sentiment classification while improving the state-of-the-art results.   % Sentiment analysis of tweets is a challenging problem where given a tweet one needs and an active research field in machine learning.   Traditional sentiment analysis approaches tackle problems like ternary  and fine-grained  classification by learning the tasks separately.  We argue that such classification tasks are correlated and we propose a multitask approach based on a recurrent neural network that benefits by jointly learning them.  % Our approach is based on a recurrent neural network, and learns a joint model for both tasks.  Our study demonstrates the potential of multitask models on this type of problems and improves the state-of-the-art results in the fine-grained sentiment classification problem.  
 Music genre classification, especially using lyrics alone, remains a challenging topic in Music Information Retrieval. In this study we apply recurrent neural network models to classify a large dataset of intact song lyrics. As lyrics exhibit a hierarchical layer structure---in which words combine to form lines, lines form segments, and segments form a complete song---we adapt a hierarchical attention network  to exploit these layers and in addition learn the importance of the words, lines, and segments. We test the model over a 117-genre dataset and a reduced 20-genre dataset. Experimental results show that the HAN outperforms both non-neural models and simpler neural models, whilst also classifying over a higher number of genres than previous research. Through the learning process we can also visualise which words or lines in a song the model believes are important to classifying the genre. As a result the HAN provides insights, from a computational perspective, into lyrical structure and language features that differentiate musical genres. 
 	 	%	Visual question answering aims at answering open-ended questions referring to given visual images. One main challenge is that the distribution of questions and answers tends to be heavy-tailed. Existing approaches train deep neural network with multiple labels to predict answers. Since the gradient-based network learning scheme often requires a huge amount of training data, the heavy-tailed textual information within the questions or answers are always excluded.  	 %	This paper presents using memory-augmented neural networks to predict accurate answers to visual questions, even when those answers occur rarely in the training set.  %	The memory networks incorporate both internal and external memory blocks and selectively pay attention to each training exemplar. We show that memory-augmented neural networks are able to maintain a relatively long-term memory of scarce training exemplars, which are important for visual question answering due to the heavy-tailed distribution of answers in a general VQA setting. %	%. This substantially help answer the questions correctly.  %	Experimental results on two large scale benchmark datasets show the favorable performance of the proposed algorithm with comparison to state of the art.  	 	In this paper, we exploit memory-augmented neural networks to predict accurate answers to visual questions, even 	when those answers rarely occur in the training set. The 	memory network incorporates both internal and external 	memory blocks and selectively pays attention to each training exemplar. We show that memory-augmented neural networks are able to maintain a relatively long-term memory 	of scarce training exemplars, which is important for visual 	question answering due to the heavy-tailed distribution of 	answers in a general VQA setting. Experimental results 	in two large-scale benchmark datasets show the favorable 	performance of the proposed algorithm with the comparison to 	state of the art. 	 
 Video Question Answering is a challenging problem in visual information retrieval, which provides the answer to the referenced video content according to the question. However, the existing visual question answering approaches mainly tackle the problem of static image question, which may be ineffectively for video question answering due to the insufficiency of modeling the temporal dynamics of video contents. In this paper, we study the problem of video question answering by modeling its temporal dynamics with frame-level attention mechanism. We propose the attribute-augmented attention network learning framework that enables the joint frame-level attribute detection and unified video representation learning for video question answering. We then incorporate the multi-step reasoning process for our proposed attention network to further improve the performance. We construct a large-scale video question answering dataset. We conduct the experiments on both multiple-choice and open-ended video question answering tasks to show the effectiveness of the proposed method. 
 In recent years, deep neural models have been widely adopted for text matching tasks, such as question answering and information retrieval, showing improved performance as compared with previous methods. In this paper, we introduce the MatchZoo toolkit that aims to facilitate the designing, comparing and sharing of deep text matching models. Specifically, the toolkit provides a unified data preparation module for different text matching problems, a flexible layer-based model construction process, and a variety of training objectives and evaluation metrics. In addition, the toolkit has implemented two schools of representative deep text matching models, namely representation-focused models and interaction-focused models. Finally, users can easily modify existing models, create and share their own models for text matching in MatchZoo.   %Text matching is a fundamental problem in natural language processing , which can be used in information retrieval, question answering, paraphrase identification and many other NLP tasks. % Both of these tasks can be treated as a question of matching between two pieces of text. %Recently, a number of deep models are designed for text matching, while rear of them provided a complete solution to reproduce the experimental results. %In this paper, we describe the MatchZoo toolkit which provides a unified  process for deep models in text matching. The toolkit firstly defines a standard data format as the input of the deep models; then implements two kinds of text matching models, namely representation-based models and interaction-based models; finally, provides variance of evaluation measures. In addition, users can easily modify existing models or implement their own models due to the well-designed interface.  it is convenient for everyone to reproduce the baseline models and implement their own models in MatchZoo.  % such as MatchPyramid, Match-SRNN, DRMM and so on.  
 Deep neural networks have become a primary tool for solving problems in many fields. They are also used for addressing information retrieval problems and show strong performance in several tasks.   Training these models requires large, representative datasets and for most IR tasks, such data contains sensitive information from users.  Privacy and confidentiality concerns prevent many data owners from sharing the data, thus today the research community can only benefit from research on large-scale datasets in a limited manner.   In this paper, we discuss , i.e., using predictions from a privacy preserving trained model instead of labels from the original sensitive training data as a supervision signal.  We present the results of preliminary experiments in which we apply the idea of mimic learning and privacy preserving mimic learning for the task of document re-ranking as one of the core IR tasks. % This research is a step toward laying the ground for enabling researchers from data-rich environments to share knowledge learned from actual users' data, which should facilitate research collaborations.  
  Discussion forums are an important source of information. They are often used to answer specific questions a user might have and to discover more about a topic of interest. Discussions in these forums may evolve in intricate ways, making it difficult for users to follow the flow of ideas. We propose a novel approach for automatically identifying the underlying thread structure of a forum discussion. Our approach is based on a neural model that computes coherence scores of possible reconstructions and then selects the highest scoring, i.e., the most coherent one. Preliminary experiments demonstrate promising results outperforming a number of strong baseline methods.   
 Time is an important relevance signal when searching streams of social media posts. The distribution of document timestamps from the results of an initial query can be leveraged to infer the distribution of relevant documents, which can then be used to rerank the initial results. Previous experiments have shown that kernel density estimation is a simple yet effective implementation of this idea.  This paper explores an alternative approach to mining temporal signals with recurrent neural networks. Our intuition is that neural networks provide a more expressive framework to capture the temporal coherence of neighboring documents in time. To our knowledge, we are the first to integrate lexical and temporal signals in an end-to-end neural network architecture, in which existing neural ranking models are used to generate query--document similarity vectors that feed into a bidirectional LSTM layer for temporal modeling. Our results are mixed:\ existing neural models for document ranking alone yield limited improvements over simple baselines, but the integration of lexical and temporal signals yield significant improvements over competitive temporal baselines. 
 Most work on natural language question answering today focuses on answer selection:\ given a candidate list of sentences, determine which contains the answer. Although important, answer selection is only one stage in a standard end-to-end question answering pipeline. This paper explores the effectiveness of convolutional neural networks  for answer selection in an end-to-end context using the standard TrecQA dataset.  We observe that a simple {-weighted word overlap. This result suggests that users are sensitive to relatively small differences in answer selection quality. 
  The dominant neural architectures in question answer retrieval are based on recurrent or convolutional encoders configured with complex word matching layers. Given that recent architectural innovations are mostly new word interaction layers or attention-based matching mechanisms, it seems to be a well-established fact that these components are mandatory for good performance. Unfortunately, the memory and computation cost incurred by these complex mechanisms are undesirable for practical applications. As such, this paper tackles the question of whether it is possible to achieve competitive performance with simple neural architectures. We propose a simple but novel deep learning architecture for fast and efficient question-answer ranking and retrieval. More specifically, our proposed model, HyperQA, is a parameter efficient neural network that outperforms other parameter intensive models such as Attentive Pooling BiLSTMs and Multi-Perspective CNNs on multiple QA benchmarks. The novelty behind HyperQA  is a pairwise ranking objective that models the relationship between question and answer embeddings in Hyperbolic space instead of Euclidean space. This empowers our model with a self-organizing ability and enables automatic discovery of latent hierarchies while learning embeddings of questions and answers. Our model requires no feature engineering, no similarity matrix matching, no complicated attention mechanisms nor over-parameterized layers and yet outperforms and remains competitive to many models that have these functionalities on multiple benchmarks.   
  In this work we present a technique to use natural language to help reinforcement learning generalize to unseen environments.  % * <upolehsan@gmail.com> 2017-05-19T13:49:52.134Z: %  % > introduce % or should it be present a technique? %  % ^ <upolehsan@gmail.com> 2017-05-19T14:00:34.019Z. This technique uses neural machine translation, specifically the use of encoder-decoder networks, to learn associations between natural language behavior descriptions and state-action information.  We then use this learned model to guide agent exploration using a modified version of policy shaping to make it more effective at learning in unseen environments.  We evaluate this technique using the popular arcade game, Frogger, under ideal and non-ideal conditions.  This evaluation shows that our modified policy shaping algorithm improves over a Q-learning agent as well as a baseline version of policy shaping.  
  Machine comprehension style question answering is a representative problem in natural language processing. Previous methods rarely spend time on the improvement of encoding layer, especially the embedding of syntactic information and name entity of the words, which are very crucial to the quality of encoding. Moreover, existing attention methods represent each query word as a vector or use a single vector to represent the whole query sentence, neither of them can handle the proper weight of the key words in query sentence. In this paper, we introduce a novel neural network architecture called Multi-layer Embedding with Memory Network for machine reading task. In the encoding layer, we employ classic skip-gram model to the syntactic and semantic information of the words to train a new kind of embedding layer. We also propose a memory network of full-orientation matching of the query and passage to catch more pivotal information. Experiments show that our model has competitive results both from the perspectives of precision and efficiency in Stanford Question Answering Dataset among all published results and achieves the state-of-the-art results on TriviaQA dataset.    
     %In this paper we present the Full-Network Multimodal Embedding, which is a multimodal embedding that successfully incorporates the Full-Network embedding into the standard text/image multimodal generation scheme. Experiments on image annotation and image retrieval compare the use of Full-Network embedding consistently showing superior performance against not using it. Furthermore, Full-Network Multimodal Embedding is capable of obtaining state-of-the-art results on image annotations while achieving competitive results in both tasks.          The current state-of-the-art for image annotation and image retrieval tasks is obtained through deep neural networks, which combine an image representation and a text representation into a shared embedding space. In this paper we evaluate the impact of using the Full-Network embedding in this setting, replacing the original image representation in a competitive multimodal embedding generation scheme. Unlike the one-layer image embeddings typically used by most approaches, the Full-Network embedding provides a multi-scale representation of images, which results in richer characterizations. To measure the influence of the Full-Network embedding, we evaluate its performance on three different datasets, and compare the results with the original multimodal embedding generation scheme when using a one-layer image embedding, and with the rest of the state-of-the-art. Results for image annotation and image retrieval tasks indicate that the Full-Network embedding is consistently superior to the one-layer embedding. These results motivate the integration of the Full-Network embedding on any multimodal embedding generation scheme, something feasible thanks to the flexibility of the approach.          %In this paper we evaluate the impact of using the Full-Network embedding for image annotation and image retrieval tasks. Unlike the popular one-layer embeddings typically used in most multimodal embedding generation schemes, the Full-Network embedding provides a multi-scale representation of images, which results in richer embeddings. To measure the influence of using the Full-Network embedding, we replace the one-layer image embedding originally implemented in a competitive multimodal embedding generation scheme, and evaluate its performance on three different datasets. Results for image annotation and image retrieval tasks indicate that the Full-Network embedding is consistently superior to the one-layer embedding, achieving state-of-the-art results on certain image annotations problems.            %The here proposed Full-Network Multimodal Embedding  takes advantage of the richer image representation provided by a Full-Network embedding successfully integrating it in a multimodal embedding generation scheme. We conduct experiments on image annotation and image retrieval tasks comparing our proposal with the same multimodal scheme using a one-layer convolutional neural network embedding for image representation. Results on 3 different datasets show that the Full-Network embedding is consistently superior to the one-layer baseline. Furthermore, Full-Network Multimodal Embedding is capable of obtaining state-of-the-art results on image annotation task.      
  The goal of counterfactual learning for statistical machine translation  is to optimize a target SMT system from logged data that consist of user feedback to translations that were predicted by another, historic SMT system. A challenge arises by the fact that risk-averse commercial SMT systems deterministically log the most probable translation. The lack of sufficient exploration of the SMT output space seemingly contradicts the theoretical requirements for counterfactual learning. We show that counterfactual learning from deterministic bandit logs is possible nevertheless by smoothing out deterministic components in learning. This can be achieved by additive and multiplicative control variates that avoid degenerate behavior in empirical risk minimization. Our simulation experiments show improvements of up to 2 BLEU points by counterfactual learning from deterministic bandit feedback. 
 Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing   typically sees initialization of only the lowest layer of deep models with pretrained word vectors.  In this paper,  we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation  to contextualize word vectors. We show that adding these context vectors  improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks:  sentiment analysis ,  question classification ,  entailment ,  and question answering .  For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art. 
 In this paper, we explore the utilization of natural language to drive transfer for reinforcement learning . Despite the wide-spread application of deep RL techniques, learning generalized policy representations that work across domains remains a challenging problem. We demonstrate that textual descriptions of environments provide a compact intermediate channel to facilitate effective policy transfer. Specifically, by learning to ground the meaning of text to the dynamics of the environment such as transitions and rewards, an autonomous agent can effectively bootstrap policy learning on a new domain given its description. We employ a model-based RL approach consisting of a differentiable planning module, a model-free component and a factorized state representation to effectively use entity descriptions. Our model outperforms prior work on both transfer and multi-task scenarios in a variety of different environments. For instance, we achieve up to 14\% and 11.5\% absolute improvement over previously existing models in terms of average and initial rewards, respectively. 
 Recently, some E-commerce sites launch a new interaction box called Tips on their mobile apps. Users can express their experience and feelings or provide suggestions using short texts typically several words or one sentence. In essence, writing some tips and giving a numerical rating are two facets of a user's product assessment action, expressing the user experience and feelings. Jointly modeling these two facets is helpful for designing a better recommendation system. While some existing models integrate text information such as item specifications or user reviews into user and item latent factors for improving the rating prediction, no existing works consider tips for improving recommendation quality. We propose a deep learning based framework named NRT which can simultaneously predict precise ratings and generate abstractive tips with good linguistic quality simulating user experience and feelings. For abstractive tips generation, gated recurrent neural networks are employed to ``translate'' user and item latent representations into a concise sentence. %All the neural parameters and the latent factors are learnt by a multi-task learning approach in an end-to-end training paradigm. Extensive experiments on benchmark datasets from different domains show that NRT achieves significant improvements over the state-of-the-art methods. Moreover, the generated tips can vividly predict the user experience and feelings. 
 Coreference resolution is an intermediate step for text understanding. It is used in tasks and domains for which we do not necessarily have coreference annotated corpora. Therefore, generalization is of special importance for coreference resolution. However, while recent coreference resolvers have notable improvements on the CoNLL dataset, they struggle to generalize properly to new domains or datasets. In this paper, we investigate the role of linguistic features in building more generalizable coreference resolvers. We show that generalization improves only slightly by merely using a set of additional linguistic features. However, employing features and subsets of their values that are informative for coreference resolution, considerably improves generalization. Thanks to better generalization, our system achieves state-of-the-art results in out-of-domain evaluations, e.g., on WikiCoref,  our system, which is trained on CoNLL, achieves on-par performance with a system designed for this dataset.  
 We show that small and shallow feed-forward neural networks can achieve near state-of-the-art results on a range of unstructured and structured language processing tasks while being considerably cheaper in memory and computational requirements than deep recurrent models.   Motivated by resource-constrained environments like mobile phones, we showcase simple techniques for obtaining such small neural network models, and investigate different tradeoffs when deciding how to allocate a small memory budget.   
 We present a new topic model that generates documents by sampling a topic for one whole sentence at a time, and generating the words in the sentence using an RNN decoder that is conditioned on the topic of the sentence. We argue that this novel formalism will help us not only visualize and model the topical discourse structure in a document better, but also potentially lead to more interpretable topics since we can now illustrate topics by sampling representative sentences instead of bag of words or phrases. We present a variational auto-encoder approach for learning in which we use a factorized variational encoder that independently models the posterior over topical mixture vectors of documents using a feed-forward network, and the posterior over topic assignments to sentences using an RNN. Our preliminary experiments on two different datasets indicate early promise, but also expose many challenges that remain to be addressed. 
 Segmental models are an alternative to frame-based models for sequence prediction, where hypothesized path weights are based on entire segment scores rather than a single frame  at a time. Neural segmental models are segmental models that use neural network-based weight functions. Neural segmental models have achieved competitive results for speech recognition, and their end-to-end training has been explored in several studies. In this work, we review neural segmental models, which can be viewed as consisting of a neural network-based acoustic encoder and a finite-state transducer decoder. We study end-to-end segmental models with different weight functions, including ones based on frame-level neural classifiers and on segmental recurrent neural networks. We study how reducing the search space size impacts performance under different weight functions. We also compare several  loss functions for end-to-end training. Finally, we explore training approaches, including multi-stage vs.~end-to-end training and multitask training that combines  segmental and frame-level losses. 
 Recent work in learning ontologies  has leveraged the intrinsic geometry of spaces of learned representations to make predictions that automatically obey complex structural constraints. We explore two extensions of one such model, the order-embedding  model for hierarchical relation learning, with an aim towards improved performance on text data for commonsense knowledge representation. Our first model jointly learns ordering relations and non-hierarchical knowledge in the form of raw text. Our second extension exploits the partial order structure of the training data to find long-distance triplet constraints among embeddings which are poorly enforced by the pairwise training procedure. We find that both incorporating free text and augmented training constraints improve over the original order-embedding model and other strong baselines. 
 In this paper, we offer an in-depth analysis about the modeling and search performance. We address the question if a more complex search algorithm is necessary. Furthermore, we investigate the question if more complex models which might only be applicable during rescoring are promising.  By separating the search space and the modeling using $n$-best list reranking, we analyze the influence of both parts of an NMT system independently. By comparing differently performing NMT systems, we show that the better translation is already in the search space of the translation systems with less performance. This results indicate that the current search algorithms are sufficient for the NMT systems. Furthermore, we could show that even a relatively small $n$-best list of $50$ hypotheses already contain notably better translations.  
   We propose a new framework for abstractive text summarization based on a sequence-to-sequence oriented encoder-decoder model equipped with a deep recurrent generative decoder .   Latent structure information implied in the target summaries is learned based on a recurrent latent random model for improving the summarization quality.   Neural variational inference is employed to address the intractable posterior inference for the recurrent latent variables.   Abstractive summaries are generated based on both the generative latent variables and the discriminative deterministic states.   Extensive experiments on some benchmark datasets in different languages show that DRGN achieves improvements over the state-of-the-art methods. 
 Intelligent selection of training data has proven a successful technique to simultaneously increase training efficiency and translation performance for phrase-based machine translation . With the recent increase in popularity of neural machine translation , we explore in this paper  and  NMT can also benefit from data selection.   While state-of-the-art data selection  consistently performs well for PBMT, we show that gains are substantially lower for NMT.  Next, we introduce  for NMT, a method in which we vary the selected subset of training data between different training epochs.  Our experiments show that the best results are achieved when applying a technique we call , with improvements up to +2.6 BLEU over the original data selection approach and up to +3.1 BLEU over a general baseline.  
 This paper describes the University of Edinburgh's submissions to the WMT17 shared news translation and biomedical translation tasks. We participated in 12 translation directions for news, translating between English and Czech, German, Latvian, Russian, Turkish and Chinese. For the biomedical task we submitted systems for English to Czech, German, Polish and Romanian. Our systems are neural machine translation systems trained with Nematus, an attentional encoder-decoder. We follow our setup from last year and build BPE-based models with parallel and back-translated monolingual training data. Novelties this year include the use of deep architectures, layer normalization, and more compact models due to weight tying and  improvements in BPE segmentations. We perform extensive ablative experiments, reporting on the effectivenes of layer normalization, deep architectures, and different ensembling techniques. 
 We study the impact of big models  and big data  on dependency grammar induction. We experimented with L-DMV, a lexicalized version of Dependency Model with Valence  and L-NDMV, our lexicalized extension of the Neural Dependency Model with Valence .  We find that L-DMV only benefits from very small degrees of lexicalization and moderate sizes of training corpora. L-NDMV can benefit from big training data and lexicalization of greater degrees, especially when enhanced with good model initialization, and it achieves a result that is competitive with the current state-of-the-art. 
    We investigate the task of building a domain aware chat system which generates intelligent responses in a conversation comprising of different domains. The domain in this case is the topic or theme of the conversation. To achieve this, we present DOM-Seq2Seq, a domain aware neural network model based on the novel technique of using domain-targeted sequence-to-sequence models~ and a domain classifier. The model captures features from current utterance and domains of the previous utterances to facilitate the formation of relevant responses. We evaluate our model on automatic metrics and compare our performance with the Seq2Seq model.  
  Linguistic resources such as part-of-speech  tags have been extensively used in statistical machine translation  frameworks and have yielded better performances.  However, usage of such linguistic annotations in neural machine translation  systems has been left under-explored.   In this work, we show that multi-task learning is a successful and a easy approach to introduce an additional knowledge into an end-to-end neural attentional model.  By jointly training several natural language processing  tasks in one system, we are able to leverage common information and improve the performance of the individual task.   We analyze the impact of three design decisions in multi-task learning: the tasks used in training, the training schedule, and the degree of parameter sharing across the tasks, which is defined by the network architecture.  The experiments are conducted for an German to English translation task.  As additional linguistic resources, we exploit POS information and named-entities .  Experiments show that the translation quality can be improved by up to 1.5 BLEU points under the low-resource condition.  The performance of the POS tagger is also improved using the multi-task learning scheme.   
 The RepEval 2017 Shared Task aims to evaluate natural language understanding models for sentence representation, in which a sentence is represented as a fixed-length vector with neural networks and the quality of the representation is tested with a natural language inference task. This paper describes our system  that is ranked among the top in the Shared Task, on both the in-domain test set  and on the cross-domain test set , demonstrating that the model generalizes well to the cross-domain data. Our model is equipped with intra-sentence gated-attention composition which helps achieve a better performance. In addition to submitting our model to the Shared Task, we have also tested it on the Stanford Natural Language Inference  dataset. We obtain an accuracy of 85.5\%, which is the best reported result on SNLI when cross-sentence attention is not allowed, the same condition enforced in RepEval 2017.  
 Grapheme-to-phoneme conversion  is necessary for text-to-speech and automatic speech recognition systems. Most g2p systems are monolingual: they require language-specific data or handcrafting of rules. Such systems are difficult to extend to low resource languages, for which data and handcrafted rules are not available. As an alternative, we present a neural sequence-to-sequence approach to g2p which is trained on spelling--pronunciation pairs in hundreds of languages. The system shares a single encoder and decoder across all languages, allowing it to utilize the intrinsic similarities between different writing systems. We show an 11\% improvement in phoneme error rate over an approach based on adapting high-resource monolingual g2p models to low-resource languages. Our model is also much more compact relative to previous approaches. 
 Automatic question-answering is a classical problem in natural language processing, which aims at designing systems that can automatically answer a question, in the same way as human does. In this work, we propose a deep learning based model for automatic question-answering. First the questions and answers are embedded using neural probabilistic modeling. Then a deep similarity neural network is trained to find the similarity score of a pair of answer and question. Then for each question, the best answer is found as the one with the highest similarity score.  We first train this model on a large-scale public question-answering database, and then fine-tune it to transfer to the customer-care chat data.  We have also tested our framework on a public question-answering database and achieved very good performance. 
 Due to the large amount of textual information available on Internet, it is of paramount relevance to use techniques that find relevant and concise content. A typical task devoted to the identification of informative sentences in documents is the so called extractive document summarization task. In this paper, we use complex network concepts to devise an extractive Multi Document Summarization  method, which extracts the most central sentences from several textual sources. In the proposed model, texts are represented as networks, where nodes represent sentences and the edges are established based on the number of shared words. Differently from previous works, the identification of relevant terms is guided by the characterization of nodes via dynamical measurements of complex networks, including symmetry, accessibility and absorption time. The evaluation of the proposed system revealed that excellent results were obtained with particular dynamical measurements, including those based on the exploration of networks via random walks. % 
 In the encoder-decoder architecture for neural machine translation , the hidden states of the recurrent structures in the encoder and decoder carry the crucial information about the sentence.%, which is represented as fixed-length real-value vectors.  These vectors are generated by parameters which are updated by back-propagation of translation errors through time. We argue that propagating errors through the end-to-end recurrent structures are not a direct way of control the hidden vectors. %Without proper control, the hidden vectors may not be the best representation of sentences, which may make the translation process arbitrary.  In this paper, we propose to use word predictions as a mechanism for direct supervision. More specifically, we require these vectors to be able to predict the vocabulary in target sentence. Our simple mechanism ensures better representations in the encoder and decoder without using any extra data or annotation. It is also helpful in reducing the target side vocabulary and improving the decoding efficiency. Experiments on Chinese-English and German-English machine translation tasks show BLEU improvements by 4.53 and 1.3, respectively. 
 We compare several language models for the word-ordering task and propose a new { models. We evaluate the model on a large German WMT data set where it significantly outperforms existing models. We also describe a novel search strategy for LM-based word ordering and report results on the English Penn Treebank. Our best model setup outperforms prior work both in terms of speed and quality. 
 Phrases play an important role in natural language understanding and machine translation . However, it is difficult to integrate them into current neural machine translation  which reads and generates sentences word by word. In this work, we propose a method to translate phrases in NMT by integrating a phrase memory storing target phrases from a phrase-based statistical machine translation  system into the encoder-decoder architecture of NMT. At each decoding step, the phrase memory is first re-written by the SMT model, which dynamically generates relevant target phrases with contextual information provided by the NMT model. Then the proposed model reads the phrase memory to make probability estimations for all phrases in the phrase memory. If phrase generation is carried on, the NMT decoder selects an appropriate phrase from the memory to perform phrase translation and updates its decoding state by consuming the words in the selected phrase. Otherwise, the NMT decoder generates a word from the vocabulary as the general NMT decoder does. Experiment results on the Chinese$\rightarrow$English translation show that the proposed model achieves significant improvements over the baseline on various test sets. 
  Neural machine translation  has achieved notable success in recent times, however it is also widely recognized that this approach has limitations with handling infrequent words and word pairs. This paper presents a novel memory-augmented NMT  architecture, which stores knowledge about how words  should be translated in a memory and then utilizes them to assist the neural model. We use this memory mechanism to combine the knowledge learned from a conventional statistical machine translation system and the rules learned by an NMT system, and also propose a solution for out-of-vocabulary  words based on this framework. Our experiments on two Chinese-English translation tasks demonstrated that the M-NMT architecture outperformed the NMT baseline by $9.0$ and $2.7$ BLEU points on the two tasks, respectively. Additionally, we found this architecture resulted in a much more effective OOV treatment compared to competitive methods.   %[1]  I want to check, is this words and also word pairs?  Or is it just "infrequent word pairings"  Just to make it clear to me. %WD: word and word pairs  
 In neural image captioning systems, a recurrent neural network  is typically viewed as the primary `generation' component. This view suggests that the image features should be `injected' into the RNN. This is in fact the dominant view in the literature. Alternatively, the RNN can instead be viewed as only encoding the previously generated words. This view suggests that the RNN should only be used to encode linguistic features and that only the final representation should be `merged' with the image features at a later stage. This paper compares these two architectures. We find that, in general, late merging outperforms injection, suggesting that RNNs are better viewed as encoders, rather than generators. 
  Transferring knowledge from a source domain to another domain is useful, especially when gathering new data is very expensive and time-consuming. Deep networks have been well-studied for question answering tasks in recent years; however, no prominent research for transfer learning through deep neural networks exists in the question answering field. In this paper, two main methods  in this field are examined. Then, a new method named Intelligent sample selection  is proposed to improve the MULT method for question answering tasks. Different datasets, specificay SQuAD, SelQA, WikiQA, NewWikiQA and InforBoxQA, are used for evaluation. Moreover, two different tasks of question answering - answer selection and answer triggering - are evaluated to examine the effectiveness of transfer learning. The results show that using transfer learning generally improves the performance if the corpora are related and are based on the same policy. In addition, using ISS-MULT could finely improve the MULT method for question answering tasks, and these improvements prove more significant in the answer triggering task.  
 Active learning aims to select a small subset of data for annotation such that a classifier learned on the data is highly accurate. This is usually done using heuristic selection methods, however the effectiveness of such methods is limited and moreover, the performance of heuristics varies between datasets. To address these shortcomings, we introduce a novel formulation by reframing the active learning as a reinforcement learning problem and explicitly learning a data selection policy, where the policy takes the role of the active learning heuristic.  Importantly, our method allows the selection policy learned using simulation on one language to be transferred to other languages. We demonstrate our method using cross-lingual named entity recognition, observing uniform improvements over traditional active learning. 
  Deep learning methods employ multiple processing layers to learn hierarchical representations of data, and have produced state-of-the-art results in many domains. Recently, a variety of model designs and methods have blossomed in the context of natural language processing . In this paper, we review significant deep learning related models and methods that have been employed for numerous NLP tasks and provide a walk-through of their evolution. We also summarize, compare and contrast the various models and put forward a detailed understanding of the past, present and future of deep learning in NLP.  
Neural network-based dialog systems are attracting increasing attention in both academia and  	industry. Recently, researchers have begun to realize the importance of speaker modeling in neural dialog systems, but there lacks established tasks and datasets. 	In this paper, we propose \textit{speaker classification
 In this paper, we discuss different methods which use meta information and richer context that may accompany source language input to improve machine translation quality. We focus on category information of input text as meta information, but the proposed methods can be extended to all textual and non-textual meta information that might be available for the input text or automatically predicted using the text content.  The main novelty of this work is to use state-of-the-art neural network methods to tackle this problem within a statistical machine translation  framework.  We observe translation quality improvements up to 3\% in terms of BLEU score in some text categories. 
 In this paper, we introduce a hybrid search for attention-based neural machine translation . A target phrase learned with statistical MT models extends a hypothesis in the NMT beam search when the attention of the NMT model focuses on the source words translated by this phrase. Phrases added in this way are scored with the NMT model, but also with SMT features including phrase-level translation probabilities and a target language model. Experimental results on German$\rightarrow$English news domain and English$\rightarrow$Russian e-commerce domain  translation tasks show that using phrase-based models in NMT search improves MT quality by up to 2.3\% BLEU absolute as compared to a strong NMT baseline. 
 This paper describes our submission  to the 2016 Discriminating Similar Languages  shared task. We participated in the closed Sub-task 1  with two separate machine learning techniques. The first approach is a character based Convolution Neural Network with a bidirectional long short term memory  layer , which achieved an accuracy of 78.45\% with minimal tuning. The second approach is a character-based n-gram model. This last approach achieved an accuracy of 88.45\% which is close to the accuracy of 89.38\% achieved by the best submission, and allowed us to rank \#7 overall.  
   Argument labeling of explicit discourse relations is a challenging task. The state of the art systems achieve slightly above 55\% F-measure but require hand-crafted features. In this paper, we propose a Long Short Term Memory  based model for argument labeling. We experimented with multiple configurations of our model. Using the PDTB dataset, our best model achieved an F1 measure of 23.05\% without any feature engineering. This is significantly higher than the 20.52\% achieved by the state of the art RNN approach, but significantly lower than the feature based state of the art systems. On the other hand, because our approach learns only from the raw dataset, it is more widely applicable to multiple textual genres and languages.   
 Most existing methods for biomedical entity recognition task rely on explicit feature engineering where many features either are specific to a particular task or depends on output of other existing NLP tools. Neural architectures have been shown across various domains that efforts for explicit feature design can be reduced. In this work we propose an unified framework using bi-directional long short term memory network  for named entity recognition  tasks in biomedical and clinical domains. Three important characteristics of the framework are as follows -  model learns contextual as well as morphological features using two different BLSTM in hierarchy,  model uses first order linear conditional random field  in its output layer in cascade of BLSTM to infer label or tag sequence,  model does not use any domain specific features or dictionary, i.e., in another words, same set of features are used in the three NER tasks, namely, disease name recognition , drug name recognition  and clinical entity recognition . We compare performance of the proposed model with existing state-of-the-art models on the standard benchmark datasets of the three tasks. We show empirically that the proposed framework outperforms all existing models. Further our analysis of CRF layer and word-embedding obtained using character based embedding show their importance. %  Through the experiments done on disease name recognition , drug name recognition  and clinical entity recognition  tasks we demonstrate the importance of proposed method. 
 One of the challenges in Speech Emotion Recognition  ``in the wild'' is the large mismatch between training and test data . In order to improve the generalisation capabilities of the emotion models, we propose to use Multi-Task Learning  and use gender and naturalness as auxiliary tasks in deep neural networks. This method was evaluated in within-corpus and various cross-corpus classification experiments that simulate conditions ``in the wild''. In comparison to Single-Task Learning  based state of the art methods, we found that our MTL method proposed improved performance significantly. Particularly, models using both gender and naturalness achieved more gains than those using either gender or naturalness separately. This benefit was also found in the high-level representations of the feature space, obtained from our method proposed, where discriminative emotional clusters could be observed.  %In comparison to Single-Task Learning  based state of the art methods, we found that our MTL method proposed improved performance significantly. In a feature space, high-level representations obtained from our method showed discriminative emotional clusters of aggregated corpora and resulted in the significant gains.  %In this paper, we investigated explicit modelling of variant contexts to improve generalisation of large emotional speech samples. Particularly, we utilised the most accessible contexts such as gender and naturalness as auxiliary tasks for multi-task learning. We employed various conditions of validation such as within-corpus, cross-corpus, and within-corpora to see how diverse contexts affect performance. We found that the proposed method improved the state of the art methods in more diverse contexts and larger corpora. The contexts, gender and naturalness, were indeed helpful for generalisation of emotional speech corpora.  
 Word embeddings are representations of individual words of a text document in a vector space and they are often useful for performing natural language processing tasks. Current state of the art algorithms for learning word embeddings learn vector representations from large corpora of text documents in an unsupervised fashion. This paper introduces SWESA , an algorithm for sentiment analysis via word embeddings. SWESA leverages document label information to learn vector representations of words from a modest corpus of text documents by solving an optimization problem that minimizes a cost function with respect to both word embeddings as well as classification accuracy. Analysis reveals that SWESA provides an efficient way of estimating the dimension of the word embeddings that are to be learned. Experiments on several real world data sets %including IMDB, Yelp, Amazon, and CHESS,% show that SWESA has superior performance when compared to previously suggested approaches to word embeddings and sentiment analysis tasks. 
 While there have been significant advances in detecting emotions from speech and image recognition, emotion detection on text is still under-explored and remained as an active research field. This paper introduces a corpus for text-based emotion detection on multiparty dialogue as well as deep neural models that outperform the existing approaches for document classification. We first present a new corpus that provides annotation of seven emotions on consecutive utterances in dialogues extracted from the show, Friends. We then suggest four types of sequence-based convolutional neural network models with attention that leverage the sequence information encapsulated in dialogue. Our best model shows the accuracies of 37.9\% and 54\% for fine- and coarse-grained emotions, respectively. Given the difficulty of this task, this is promising. 
 Learning latent representations from long text sequences is an important first step in many natural language processing applications. Recurrent Neural Networks  have become a cornerstone for this challenging task. However, the quality of sentences during RNN-based decoding  decreases with the length of the text. We propose a sequence-to-sequence, purely convolutional and deconvolutional autoencoding framework that is free of the above issue, while also being computationally efficient. The proposed method is simple, easy to implement and can be leveraged as a building block for many applications. We show empirically that compared to RNNs, our framework is better at reconstructing and correcting long paragraphs. Quantitative evaluation on semi-supervised text classification and summarization tasks demonstrate the potential for better utilization of long unlabeled text data. 
   In this paper, we propose new methods to learn Chinese word representations. Chinese characters are composed of graphical components, which carry rich semantics. It is common for a Chinese learner to comprehend the meaning of a word from these graphical components. As a result, we propose models that enhance word representations by character glyphs. The character glyph features are directly learned from the bitmaps of characters by convolutional auto-encoder, and the glyph features improve Chinese word representations which are already enhanced by character embeddings. Another contribution in this paper is that we created several evaluation datasets in traditional Chinese and made them public. 
 Cross-modal data retrieval has been the basis of various creative tasks performed by Artificial Intelligence . One such highly challenging task for AI is to convert a book into its corresponding movie, which most of the creative film makers do as of today. In this research, we take the first step towards it by visualizing the content of a book using its corresponding movie visuals. Given a set of sentences from a book or even a fan-fiction written in the same universe, we employ deep learning models to visualize the input by stitching together relevant frames from the movie. We studied and compared three different types of setting to match the book with the movie content:  Dialog model: using only the dialog from the movie,  Visual model: using only the visual content from the movie, and  Hybrid model: using the dialog and the visual content from the movie. Experiments on the publicly available MovieBook dataset shows the effectiveness of the proposed models. 
 In this paper, we propose to use deep 3-dimensional convolutional networks  in order to address the challenge of modelling spectro-temporal dynamics for speech emotion recognition . Compared to a hybrid of Convolutional Neural Network and Long-Short-Term-Memory , our proposed 3D CNNs simultaneously extract short-term and long-term spectral features with a moderate number of parameters. We evaluated our proposed and other state-of-the-art methods in a speaker-independent manner using aggregated corpora that give a large and diverse set of speakers. We found that 1) shallow temporal and moderately deep spectral kernels of a homogeneous architecture are optimal for the task; and 2) our 3D CNNs are more effective for spectro-temporal feature learning compared to other methods. Finally, we visualised the feature space obtained with our proposed method using t-distributed stochastic neighbour embedding  and could observe distinct clusters of emotions. 
 Language models for agglutinative languages have always been hindered in past due to myriad of agglutinations possible to any given word through various affixes. We propose a method to diminish the problem of out-of-vocabulary words by introducing an embedding derived from syllables and morphemes which leverages the agglutinative property. Our model outperforms character-level embedding in perplexity by 16.87 with 9.50M parameters. Proposed method achieves state of the art performance over existing input prediction methods in terms of Key Stroke Saving and has been commercialized. 
 Recent applications of neural language models have led to an increased interest in the automatic generation of natural language. However impressive, the evaluation of neurally generated text has so far remained rather informal and anecdotal. Here, we present an attempt at the systematic assessment of one aspect of the quality of neurally generated text. We focus on a specific aspect of neural language generation: its ability to reproduce authorial writing styles. Using established models for authorship attribution, we empirically assess the stylistic qualities of neurally generated text. In comparison to conventional language models, neural models generate fuzzier text that is relatively harder to attribute correctly. Nevertheless, our results also suggest that neurally generated text offers more valuable perspectives for the augmentation of training data. 
 Recently, bidirectional recurrent network language models  have been shown to outperform standard, unidirectional, recurrent neural network language models  on a range of speech recognition tasks. This indicates  that future word context information beyond the word history can be useful. However, bi-RNNLMs pose a number of challenges as they make use of the complete previous and future word context information. This impacts both training efficiency and their use within a lattice rescoring framework. In this paper these issues are addressed by proposing a novel neural network structure, succeeding word RNNLMs . Instead of using a recurrent unit to capture the complete future word contexts, a  feedforward unit is used to model a finite  number of succeeding, future, words. This model can be trained much more  efficiently than bi-RNNLMs and can also be used for lattice rescoring.  Experimental results on a meeting transcription task   show the proposed model consistently outperformed uni-RNNLMs and  yield only a slight degradation compared to bi-RNNLMs in N-best rescoring. Additionally, performance improvements can be obtained using lattice rescoring and subsequent confusion network decoding. 
     Neural machine translation  approaches have improved the state of the     art in many machine translation settings over the last couple of years,     but they require large amounts of training data to produce sensible     output.  We demonstrate that NMT can be used for low-resource languages as     well, by introducing more local dependencies and using word alignments to     learn sentence reordering during translation.  In addition to our novel     model, we also present an empirical evaluation of low-resource     phrase-based statistical machine translation  and NMT to investigate     the lower limits of the respective technologies.  We find that while SMT     remains the best option for low-resource settings, our method can produce     acceptable translations with only 70~000 tokens of training data, a level     where the baseline NMT system fails completely. 
 We introduce the Helsinki Neural Machine Translation system  and how it is applied in the news translation task at WMT 2017, where it ranked first in both the human and automatic evaluations for English--Finnish.  We discuss the success of English--Finnish translations and the overall advantage of NMT over a strong SMT baseline. We also discuss our submissions for English--Latvian, English--Chinese and Chinese--English. 
 We investigate the use of extended context in attention-based neural machine translation. We base our experiments on translated movie subtitles and discuss the effect of increasing the segments beyond single translation units. We study the use of extended source language context as well as bilingual context extensions. The models learn to distinguish between information from different segments and are surprisingly robust with respect to translation quality. In this pilot study, we observe interesting cross-sentential attention patterns that improve textual coherence in translation at least in some selected cases. 
  We present a novel end-to-end trainable neural network model for task-oriented dialog systems. The model is able to track dialog state, issue API calls to knowledge base , and incorporate structured KB query results into system responses to successfully complete task-oriented dialogs. The proposed model produces well-structured system responses by jointly learning belief tracking and KB result processing conditioning on the dialog history. We evaluate the model in a restaurant search domain using a dataset that is converted from the second Dialog State Tracking Challenge  corpus. Experiment results show that the proposed model can robustly track dialog state given the dialog history. % Our model also demonstrates promising performance in per-response accuracy comparing to prior end-to-end trainable neural network models, and obtains performance comparable to state-of-the-art systems.  Moreover, our model demonstrates promising results in producing appropriate system responses, outperforming prior end-to-end trainable neural network models using per-response accuracy evaluation metrics.  
 Question answering  systems are sensitive to the many different ways natural language expresses the same information need. In this paper we turn to paraphrases as a means of capturing this knowledge and present a general framework which learns felicitous paraphrases for various QA tasks. Our method is trained end-to-end using question-answer pairs as a supervision signal. A question and its paraphrases serve as input to a neural scoring model which assigns higher weights to linguistic expressions most likely to yield correct answers. We evaluate our approach on QA over Freebase and answer sentence selection. Experimental results on three datasets show that our framework consistently improves performance, achieving competitive results despite the use of simple QA models. 
  This paper addresses the problem of extracting  keyphrases from scientific articles and categorizing them as corresponding to a task, process, or material. We cast the problem as sequence tagging and introduce  semi-supervised methods to a neural tagging model, which builds on recent advances in named entity recognition. Since annotated training data is scarce in this domain, we introduce a graph-based semi-supervised algorithm together  with a data selection scheme to leverage unannotated articles. Both inductive and transductive semi-supervised learning strategies outperform state-of-the-art information extraction performance on the 2017 SemEval Task 10 ScienceIE task.  %Scientific keyphrase extraction is the task of We present a CRF-LSTM neural approach for identifying keywords in scientific literature, in terms of Task, Process and Material.  %In order to improve the performance of neural network based models with limited annotated training data and large unlabeled data, we explore different ways to use unlabeled data. We investigate the performance of word embedding trained on Semantic Scholar Computer Science data and Wikipedia data on different fields of papers . The best model on different sub-fields is selected to do semi-supervised learning. Based on CRF-LSTM structure, we propose a way of reducing the effect of low confidence tokens, which gives state-of-the-art performance over all previous methods.  
 Homographs, words with different meanings but the same surface form, have long caused difficulty for machine translation systems, as it is difficult to select the correct translation based on the context. However, with the advent of neural machine translation  systems, which can theoretically take into account global sentential context, one may hypothesize that this problem has been alleviated. In this paper, we first provide empirical evidence that existing NMT systems in fact still have significant problems in properly translating ambiguous words. We then proceed to describe methods, inspired by the word sense disambiguation literature, that model the context of the input word with context-aware word embeddings that help to differentiate the word sense before feeding it into the encoder. Experiments on three language pairs demonstrate that such models improve the performance of NMT systems both in terms of BLEU score and in the accuracy of translating homographs.% \footnote{Code for our translation models is available at \\ \url{https://goo.gl/oaiqoT}} 
 The goal of language modeling techniques is to capture the statistical and structural properties of natural  languages from training corpora. This task typically involves the learning of short range dependencies,  which generally model the syntactic properties of a language and/or long range dependencies, which are semantic in nature.  We propose in this paper a new multi-span architecture, which separately models the short and long  context information while it dynamically merges them to perform the language modeling task. This is  done through a novel recurrent Long-Short Range Context  network, which  explicitly models the local  and global  context using two separate hidden states  that evolve in time. This new architecture is an adaptation of the Long-Short Term Memory network   to take into account the linguistic properties. Extensive experiments conducted on the Penn Treebank   and the Large Text Compression Benchmark  corpus showed a significant reduction of the perplexity when compared to state-of-the-art language modeling techniques. 
 The electronic health record  contains a large amount of multi-dimensional and unstructured clinical data of significant operational and research value. Distinguished from previous studies, our approach embraces a double-annotated dataset and strays away from obscure ``black-box'' models to comprehensive deep learning models. In this paper, we present a novel neural attention mechanism that not only classifies clinically important findings. Specifically, convolutional neural networks  with attention analysis are used to classify radiology head computed tomography reports based on five categories that radiologists would account for in assessing acute and communicable findings in daily practice. The experiments show that our CNN attention models outperform non-neural models, especially when trained on a larger dataset. Our attention analysis demonstrates the intuition behind the classifier's decision by generating a heatmap that highlights attended terms used by the CNN model; this is valuable when potential downstream medical decisions are to be performed by human experts or the classifier information is to be used in cohort construction such as for epidemiological studies.  
 The performance of Neural Network -based language models is steadily improving  due to the emergence of new architectures, which are able to learn different natural language characteristics. This paper presents a novel framework, which shows that a significant improvement can  be achieved by combining different existing heterogeneous models in a single architecture.  This is done through 1) a feature layer, which separately learns different NN-based models and 2) a mixture layer,  which merges the resulting model features. In doing so,  this architecture benefits from the learning capabilities of each model with no noticeable increase in the number of model parameters or the training time.  Extensive experiments conducted on the Penn Treebank  and the Large Text Compression Benchmark  corpus showed a significant reduction of the perplexity when compared to state-of-the-art feedforward as well as recurrent neural network architectures. 
     Automatically evaluating the quality of dialogue responses for unstructured domains is a challenging problem. Unfortunately, existing automatic evaluation metrics are biased and correlate very poorly with human judgements of response quality. Yet having an accurate automatic evaluation procedure is crucial for dialogue research, as it allows rapid prototyping and testing of new models with fewer expensive human evaluations. In response to this challenge, we formulate automatic dialogue evaluation as a learning problem. We present an evaluation model  that learns to predict human-like scores to input responses, using a new dataset of human response scores. We show that the { can generalize to evaluating dialogue models unseen during training, an important step for automatic dialogue evaluation.\footnote{Code and pre-trained model parameters are available: github.com/mike-n-7/ADEM.}     
 This paper demonstrates neural network-based toolkit namely NNVLP for essential   Vietnamese language processing tasks including part-of-speech  tagging, chunking, named entity recognition . Our toolkit is a   combination of bidirectional Long Short-Term Memory ,   Convolutional Neural Network , Conditional Random Field ,   using pre-trained word embeddings as input, which achieves state-of-the-art results on these three tasks.  We provide both API and web demo\footnote{\url{nnvlp.org}} for this toolkit. 
 An exhaustive study on neural network language modeling  is performed in this paper. Different architectures of basic neural network language models are described and examined. A number of different improvements over basic neural network language models, including importance sampling, word classes, caching and bidirectional recurrent neural network , are studied separately, and the advantages and disadvantages of every technique are evaluated. Then, the limits of neural network language modeling are explored from the aspects of model architecture and knowledge representation. Part of the statistical information from a word sequence will loss when it is processed word by word in a certain order, and the mechanism of training neural network by updating weight matrixes and vectors imposes severe restrictions on any significant enhancement of NNLM. For knowledge representation, the knowledge represented by neural network language models is the approximate probabilistic distribution of word sequences from a certain training data set rather than the knowledge of a language itself or the information conveyed by word sequences in a natural language. Finally, some directions for improving neural network language modeling further is discussed. 
 Neural network models have recently\footnote{This paper was accepted by International Conference on Intelligent Text Processing and Computational Linguistics  2016, April; Konya, Turkey.} received heated research attention in the natural language processing community. Compared with traditional models with discrete features, neural models have two main advantages. First, they take low-dimensional, real-valued embedding vectors as inputs, which can be trained over large raw data, thereby addressing the issue of feature sparsity in discrete models. Second, deep neural networks can be used to automatically combine input features, and including non-local features that capture semantic patterns that cannot be expressed using discrete indicator features. As a result, neural network models have achieved competitive accuracies compared with the best discrete models for a range of NLP tasks.  On the other hand, manual feature templates have been carefully investigated for most NLP tasks over decades and typically cover the most useful indicator pattern for solving the problems. Such information can be complementary the features automatically induced from neural networks, and therefore combining discrete and neural features can potentially lead to better accuracy compared with models that leverage discrete or neural features only.  In this paper, we systematically investigate the effect of discrete and neural feature combination for a range of fundamental NLP tasks based on sequence labeling, including word segmentation, POS tagging and named entity recognition for Chinese and English, respectively. Our results on standard benchmarks show that state-of-the-art neural models can give accuracies comparable to the best discrete models in the literature for most tasks and combing discrete and neural features unanimously yield better results.     
 We present CloudScan; an invoice analysis system that requires zero configuration or upfront annotation.  In contrast to previous work, CloudScan does not rely on templates of invoice layout, instead it learns a single global model of invoices that naturally generalizes to unseen invoice layouts.  The model is trained using data automatically extracted from end-user provided feedback. This automatic training data extraction removes the requirement for users to annotate the data precisely.  We describe a recurrent neural network model that can capture long range context and compare it to a baseline logistic regression model corresponding to the current CloudScan production system.  We train and evaluate the system on 8 important fields using a dataset of 326,471 invoices. The recurrent neural network and baseline model achieve 0.891 and 0.887 average F1 scores respectively on seen invoice layouts. For the harder task of unseen invoice layouts, the recurrent neural network model outperforms the baseline with 0.840 average F1 compared to 0.788.  
  In recent years, many deep-learning based models are proposed for text classification. This kind of models well fits the training set from the statistical point of view.  However, it lacks the capacity of utilizing instance-level information from individual instances in the training set.  In this work, we propose to enhance neural network models by allowing them to leverage information from $k$-nearest neighbor  of the input text.  Our model employs a neural network that encodes texts into text embeddings. Moreover, we also utilize $k$-nearest neighbor of the input text as an external memory, and utilize it to capture instance-level information from the training set.  The final prediction is made based on features from both the neural network encoder and the kNN memory.  Experimental results on several standard benchmark datasets show that our model outperforms the baseline model on all the datasets, and it even beats a very deep neural network model  in several datasets.  Our model also shows superior performance when training instances are scarce, and when the training set is severely unbalanced. Our model also leverages techniques such as   semi-supervised training and transfer learning quite well.  
  In this paper, we explore alternative ways to train a neural machine translation system in a multi-domain scenario. We investigate data concatenation , model stacking , data selection and multi-model ensemble.  %We evaluate these methods based on three criteria: i) translation quality, ii) training time, and iii) robustness towards out-of-domain tests.  Our findings  %on Arabic-English and German-English language pairs  show that the best translation quality can be achieved by building an initial system on a concatenation of available out-of-domain data and then fine-tuning it on in-domain data. Model stacking works best when training begins with the furthest out-of-domain data and the model is incrementally fine-tuned with the next furthest domain and so on. Data selection did not give the best results, but can be considered as a decent compromise between training time and translation quality.  A weighted ensemble of different individual models performed better than data selection. It is beneficial in a scenario when there is no time for fine-tuning an already trained model.  
    In this paper, we present hierarchical relation-based latent Dirichlet allocation , a data-driven hierarchical topic model for extracting terminological ontologies from a large number of heterogeneous documents. In contrast to traditional topic models, hrLDA relies on noun phrases instead of unigrams, considers syntax and document structures, and enriches topic hierarchies with topic relations. Through a series of experiments, we demonstrate the superiority of hrLDA over existing topic models, especially for building hierarchies. Furthermore, we illustrate the robustness of hrLDA in the settings of noisy data sets, which are likely to occur in many practical scenarios. Our ontology evaluation results show that ontologies extracted from hrLDA are very competitive with the ontologies created by domain experts.  
   Even for common NLP tasks, sufficient   supervision is not available in many languages---morphological   tagging is no exception. In the work presented here, we explore   a transfer learning scheme, whereby we   train character-level recurrent neural taggers   to predict morphological taggings for high-resource   languages and low-resource languages together. Learning   joint character representations among multiple related languages   successfully enables knowledge transfer from the high-resource languages to the low-resource   ones, improving accuracy by up to 30\%. 
   %%question; what we do; hao we do; great results   The attention model has become a standard component in neural machine translation  and it guides translation process by selectively focusing on parts of the source sentence when predicting each target word. %   However, we find that the generation of a target word does not only depend on the source sentence, but also rely heavily on the previous generated target words, especially the distant words which are difficult to model by using recurrent neural networks.   To solve this problem, we propose in this paper a novel look-ahead attention mechanism for generation in NMT, which aims at directly capturing the dependency relationship between target words. We further design three patterns to integrate our look-ahead attention into the conventional attention model.   Experiments on NIST Chinese-to-English and WMT English-to-German translation tasks show that our proposed look-ahead attention mechanism achieves substantial improvements over state-of-the-art baselines.  
 Beam search is a desirable choice of test-time decoding algorithm for neural sequence models because it potentially avoids search errors made by simpler greedy methods. However, typical cross entropy training procedures for these models do not directly consider the behaviour of the final decoding method. As a result, for cross-entropy trained models, beam decoding can sometimes yield reduced test performance when compared with greedy decoding. In order to train models that can more effectively make use of beam search, we propose a new training procedure that focuses on the final loss metric  evaluated on the output of beam search. While well-defined, this ``direct loss'' objective is itself discontinuous and thus difficult to optimize. Hence, in our approach, we form a sub-differentiable surrogate objective by introducing a novel continuous approximation of the beam search decoding procedure. In experiments, we show that optimizing this new training objective yields substantially better results on two sequence tasks  when compared with both cross entropy trained greedy decoding and cross entropy trained beam decoding baselines. 
 This paper is the first attempt to learn the policy of an inquiry dialog system  by using deep reinforcement learning .  Most IDS frameworks represent dialog states and dialog acts with logical formulae.  In order to make learning inquiry dialog policies more effective,  we introduce a logical formula embedding framework based on a recursive neural network.  The results of experiments to evaluate the effect of 1) the DRL and 2) the logical formula embedding framework show that the combination of the two are as effective or even better than existing rule-based methods for inquiry dialog policies.  
 We propose the \FullModelName{} , a method that learns representations of documents in an unsupervised manner for news article retrieval. In the \ModelName{} paradigm, we learn low-dimensional representations of words and documents from scratch using gradient descent and rank documents according to their similarity with query representations that are composed from word representations. We show that \ModelName{} performs better at document ranking than existing latent semantic vector space methods. The addition of \ModelName{} to a mixture of lexical language models and a state-of-the-art baseline vector space model yields a statistically significant increase in retrieval effectiveness. Consequently, \ModelName{} adds a complementary relevance signal. Next to semantic matching, we find that \ModelName{} performs well in cases where lexical matching is needed. % \ModelName{} learns a notion of term specificity directly from the document collection without feature engineering. We also show that \ModelName{} learns regularities related to Luhn significance. Finally, we give advice on how to deploy \ModelName{} in situations where model selection  is infeasible. We find that an unsupervised ensemble of multiple models trained with different hyperparameter values performs better than a single cross-validated model. Therefore, \ModelName{} can safely be used for ranking documents without supervised relevance judgments. 
 This paper presents a state-of-the-art model for visual question answering , which won the first place in the 2017 . VQA is a task of significant importance for research in artificial intelligence, given its multimodal nature, clear evaluation protocol, and potential real-world applications. The performance of deep neural networks for VQA is very dependent on choices of architectures and hyperparameters. To help further research in the area, we describe in detail our high-performing, though relatively simple model. Through a massive exploration of architectures and hyperparameters representing more than 3,000 GPU-hours, we identified tips and tricks that lead to its success, namely: sigmoid outputs, soft training targets, image features from bottom-up attention, gated tanh activations, output embeddings initialized using GloVe and Google Images, large mini-batches, and smart shuffling of training data. We provide a detailed analysis of their impact on performance to assist others in making an appropriate selection. 
 Nowadays, cross-modal retrieval plays an indispensable role to flexibly find information across different modalities of data. Effectively measuring the similarity between different modalities of data is the key of cross-modal retrieval.  %Different modalities such as image and text may provide asymmetrical information to describe the same semantics, because some modality-specific characteristics within one modality cannot be exactly aligned with other modalities when performing cross-modal matching.  Different modalities such as image and text have imbalanced and complementary relationships, which contain unequal amount of information when describing the same semantics. For example, images often contain more details that cannot be demonstrated by textual descriptions and vice versa.  Existing works based on Deep Neural Network  mostly construct one common space for different modalities to find the latent alignments between them, which lose their exclusive modality-specific characteristics. % and cannot fully exploit the intrinsic information within each modality. %Thus, treating different modalities equally to find the latent alignments between them may lose their exclusive modality-specific characteristics, which cannot fully exploit the intrinsic information within each modality. %which leads to misalignment when performing cross-modal matching. %Different from existing works based on Deep Neural Network , which mostly construct one common space for different modalities equally,  Different from the existing works, we propose modality-specific cross-modal similarity measurement  approach by constructing independent semantic space for each modality, which adopts end-to-end framework to directly generate modality-specific cross-modal similarity without explicit common representation.  %For each semantic space, modality-specific characteristics within one modality are fully exploit, while the data of another modality is projected into it to capture the imbalanced and complementary relationships between different modalities.  %Specifically, we design recurrent attention network to model the modality-specific characteristics with attention mechanism. An attention based joint embedding loss is further proposed to utilize the learned attention weights for guiding the fine-grained cross-modal correlation learning.  For each semantic space, modality-specific characteristics within one modality are fully exploited by recurrent attention network, while the data of another modality is projected into this space with attention based joint embedding to utilize the learned attention weights for guiding the fine-grained cross-modal correlation learning, which can capture the imbalanced and complementary relationships between different modalities. %For each semantic space, we design recurrent attention network for one modality to model the fine-grained context information with attention mechanism, aiming to exploit its intrinsic modality-specific characteristics. An attention based joint embedding loss is proposed to project the data of another modality into the semantic space, which can guide the fine-grained cross-modal correlation learning to utilize the imbalance relationships contained in different modalities.  Finally, the complementarity between the semantic spaces for different modalities is explored by adaptive fusion of the modality-specific cross-modal similarities to perform cross-modal retrieval.  Experiments on the widely-used Wikipedia and Pascal Sentence datasets as well as our constructed large-scale XMediaNet dataset verify the effectiveness of our proposed approach, outperforming 9 state-of-the-art methods. 
 Image captioning often requires a large set of training image-sentence pairs. In practice, however, acquiring sufficient training pairs is always expensive, making the recent captioning models limited in their ability to describe objects outside of training corpora . In this paper, we present Long Short-Term Memory with Copying Mechanism  --- a new architecture that incorporates copying into the Convolutional Neural Networks  plus Recurrent Neural Networks  image captioning framework, for describing novel objects in captions. Specifically, freely available object recognition datasets are leveraged to develop classifiers for novel objects. Our LSTM-C then nicely integrates the standard word-by-word sentence generation by a decoder RNN with copying mechanism which may instead select words from novel objects at proper places in the output sentence. Extensive experiments are conducted on both MSCOCO image captioning and ImageNet datasets, demonstrating the ability of our proposed LSTM-C architecture to describe novel objects. Furthermore, superior results are reported when compared to state-of-the-art deep models. 
  We investigate task clustering for deep learning-based multi-task and few-shot learning in the settings with large numbers of tasks. Our method measures task similarities using cross-task transfer performance matrix.  Although this matrix provides us critical information regarding similarities between tasks, the uncertain task-pairs, i.e., the ones with extremely asymmetric transfer scores, may collectively mislead clustering algorithms to output an inaccurate task-partition. Moreover, when the number of tasks is large, generating the full transfer performance matrix can be very time consuming. To overcome these limitations, we propose a novel task clustering algorithm to estimate the similarity matrix based on the theory of matrix completion. The proposed algorithm can work on partially-observed similarity matrices based on only sampled task-pairs with reliable scores, ensuring its efficiency and robustness. Our theoretical analysis shows that under mild assumptions, the reconstructed matrix perfectly matches the underlying é–³ãƒ¦æ¶ªrueé–³ similarity matrix with an overwhelming probability. The final task partition is computed by applying an efficient spectral clustering algorithm to the recovered matrix. Our results show that the new task clustering method can discover task clusters that benefit both multi-task learning and few-shot learning setups for sentiment classification and dialog intent classification tasks.  % 
  Recurrent neural networks show state-of-the-art results in many text analysis tasks but often require a lot of memory to store their weights. Recently proposed Sparse Variational Dropout~ eliminates the majority of the weights in a feed-forward neural network without significant loss of quality. We apply this technique to sparsify recurrent neural networks. To account for recurrent specifics we also rely on Binary Variational Dropout for RNN~. We report 99.5\% sparsity level on sentiment analysis task without a quality drop and up to 87\% sparsity level on language modeling task with slight loss of accuracy. 
 One of the main computational and scientific challenges in the modern age is to extract useful information from unstructured texts.  Topic models are one popular machine-learning approach which infers the latent topical structure of a collection of documents.  Despite their success --- in particular of its most widely used variant called Latent Dirichlet Allocation  --- and numerous applications in sociology, history, and linguistics, topic models are known to suffer from severe conceptual and practical problems, e.g. a lack of justification for the Bayesian priors, discrepancies with statistical properties of real texts, and the inability to properly choose the number of topics. Here we obtain a fresh view on the problem of identifying topical structures by relating it to the problem of finding communities in complex networks.  This is achieved by representing text corpora as bipartite networks of documents and words.  By adapting existing community-detection methods -- using a stochastic block model  with non-parametric priors -- we obtain a more versatile and principled framework for topic modeling .  The analysis of artificial and real corpora demonstrates that our SBM approach leads to better topic models than LDA in terms of statistical model selection.   More importantly, our work shows how to formally relate methods from community detection and topic modeling, opening the possibility of cross-fertilization between these two fields. 
   In this paper, we consider several compression techniques for the language modeling problem based on recurrent neural networks . It is known that conventional RNNs, e.g, LSTM-based networks in language modeling, are characterized with either high space complexity or substantial inference time. This problem is especially crucial for mobile applications, in which the constant interaction with the remote server is inappropriate. By using the Penn Treebank  dataset we compare pruning, quantization,  low-rank factorization, tensor train decomposition for LSTM networks in terms of model size and suitability for fast inference.    
 Relational databases store a significant amount of the worldé–³ãƒ¦ç¨ data. However, accessing this data currently requires users to understand a query language such as SQL. We propose \model, a deep neural network for translating natural language questions to corresponding SQL queries. Our model uses rewards from in-the-loop query execution over the database to learn a policy to generate the query, which contains unordered parts that are less suitable for optimization via cross entropy loss. Moreover, \model leverages the structure of SQL to prune the space of generated queries and significantly simplify the generation problem. In addition to the model, we release \dataset, a dataset of \numinstances hand-annotated examples of questions and SQL queries distributed across \numtables tables from Wikipedia that is an order of magnitude larger than comparable datasets. By applying policy-based reinforcement learning with a query execution environment to \dataset, \model outperforms a state-of-the-art semantic parser, improving execution accuracy from \perfeaseqtoseq to \perfeaours and logical form accuracy from \perflaseqtoseq to \perflaours. 
 Generating texts from structured data  is important for various natural language processing tasks such as question answering and dialog systems. In recent studies, researchers use neural language models and encoder-decoder frameworks for table-to-text generation. However, these neural network-based approaches do not model the order of contents during text generation. When a human writes a summary based on a given table, he or she would probably consider the content order before wording. In a biography, for example, the nationality of a person is typically mentioned before occupation in a biography. In this paper, we propose an order-planning text generation model to capture the relationship between different fields and use such relationship to make the generated text more fluent and smooth. We conducted experiments on the WikiBio dataset and achieve significantly higher performance than previous methods in terms of BLEU, ROUGE, and NIST scores. 
 Retrieving spoken content with spoken queries, or query-by-example spoken term detection , is attractive because it makes possible the matching of signals directly on the acoustic level without transcribing them into text. Here, we propose an end-to-end query-by-example STD model based on an attention-based multi-hop network, whose input is a spoken query and an audio segment containing several utterances; the output states whether the audio segment includes the query. The model can be trained in either a supervised scenario using labeled data, or in an unsupervised fashion. In the supervised scenario, we find that the attention mechanism and multiple hops improve performance, and that the attention weights indicate the time span of the detected terms. In the unsupervised setting, the model mimics the behavior of DTW, and it performs as well as DTW but with a lower run-time complexity. 
 Words in some natural languages can have a composite structure. Elements of this structure include the root , prefixes and suffixes with which various nuances and relations to other words can be expressed. Thus, in order to build a proper word representation one must take into account its internal structure. From a corpus of texts we extract a set of frequent subwords and from the latter set we select patterns, i.e. subwords which encapsulate information on character $n$-gram regularities. The selection is made using the pattern-based Conditional Random Field model~ with $l_1$ regularization. Further, for every word we construct a new sequence over an alphabet of patterns. The new alphabet's symbols confine a local statistical context stronger than the characters, therefore they allow better representations in ${\mathbb{R}}^n$ and are better building blocks for word representation.  In the task of subword-aware language modeling, pattern-based models outperform character-based analogues by 2-20 perplexity points. Also, a recurrent neural network in which a word is represented as a sum of embeddings of its patterns is on par with a competitive and significantly more sophisticated character-based convolutional architecture.  
 The ubiquity of metaphor in our everyday communication makes it an important problem for natural language understanding. Yet, the majority of metaphor processing systems to date rely on hand-engineered features and there is still no consensus in the field as to which features are optimal for this task. In this paper, we present the first deep learning architecture designed to capture metaphorical composition. Our results demonstrate that it outperforms the existing approaches in the metaphor identification task.%, as well as learning accurate representations for literal and metaphorical phrases.%\todo{KATIA: This last claim may need to be adapted after the analysis is finished. I guess we don't really evaluate it except qualitatively} %[Luana]: I think it's safer to take this claim out of the abstract... 
 Learning algorithms for natural language processing  tasks traditionally rely on manually defined relevant contextual features. On the other hand, neural network models using an only distributional representation of words have been successfully applied for several NLP tasks. Such models learn features automatically and avoid explicit feature engineering. Across several domains, neural models become a natural choice specifically when limited characteristics of data are known. However, this flexibility comes at the cost of interpretability. In this paper, we define three different methods to investigate ability of bi-directional recurrent neural networks  in capturing contextual features. In particular, we analyze RNNs for sequence tagging tasks. We perform a comprehensive analysis on general as well as biomedical domain datasets. Our experiments focus on important contextual words as features, which can easily be extended to analyze various other feature types. We also investigate positional effects of context words and show how the developed methods can be used for error analysis. 
 This paper describes a preliminary study for producing and distributing a large-scale database of embeddings from the Portuguese Twitter stream. We start by experimenting with a relatively small sample and focusing on three challenges: volume of training data, vocabulary size and intrinsic evaluation metrics. Using a single GPU, we were able to scale up vocabulary size from 2048 words embedded and 500K training examples to 32768 words over 10M training examples while keeping a stable validation loss and approximately linear trend on training time per epoch. We also observed that using less than 50\% of the available training examples for each vocabulary size might result in overfitting. Results on intrinsic evaluation show promising performance for a vocabulary size of 32768 words. Nevertheless, intrinsic evaluation metrics suffer from over-sensitivity to their corresponding cosine similarity thresholds, indicating that a wider range of metrics need to be developed to track progress. 
 Measuring the salience of a word is an essential step in numerous NLP tasks. Heuristic approaches such as tfidf have been used so far to estimate the salience of words.  We propose   scores, unlike heuristics, are learnt from a corpus. Specifically, we learn word salience scores such that, using pre-trained word embeddings as the input, can accurately predict the words that appear in a sentence, given the words that appear in the sentences preceding or succeeding that sentence. Experimental results on sentence similarity prediction show that the learnt word salience scores perform comparably or better than some of the state-of-the-art approaches for representing sentences on benchmark datasets for sentence similarity, while using only a fraction of the training and prediction times required by prior methods. Moreover, our NWS scores positively correlate with psycholinguistic measures such as concreteness, and imageability implying a close connection to the salience as perceived by humans. 
 Co-occurrences between two words provide useful insights into the semantics of those words. Consequently, numerous prior work on word embedding learning have used co-occurrences between two words as the training signal for learning word embeddings. However, in natural language texts it is common for multiple words to be related and co-occurring in the same context. We extend the notion of co-occurrences to cover $k$-way co-occurrences among a set of $k$-words. Specifically, we prove a theoretical relationship between the joint probability of $k$ words, and the sum of $\ell_2$ norms of their embeddings. Next, we propose a learning objective motivated by our theoretical result that utilises $k$-way co-occurrences for learning word embeddings. Our experimental results show that the derived theoretical relationship does indeed hold empirically, and despite data sparsity, for some smaller $k$ values,  $k$-way embeddings perform comparably or better than $2$-way embeddings in a range of tasks. 
 This study addresses the problem of identifying the meaning of unknown words or entities in a discourse with respect to the word embedding approaches used in neural language models. We proposed a method for on-the-fly construction and exploitation of word embeddings in both the input and output layers of a neural model by tracking contexts. This extends the dynamic entity representation used in  and incorporates a copy mechanism proposed independently by  and . In addition, we construct a new task and dataset called Anonymized Language Modeling for evaluating the ability to capture word meanings while reading. Experiments conducted using our novel dataset show that the proposed variant of RNN language model outperformed the baseline model. Furthermore, the experiments also demonstrate that dynamic updates of an output layer help a model predict reappearing entities, whereas those of an input layer are effective to predict words following reappearing entities. 
 %TODO not a big fan of this sentence... unless you have a direct citation from a very well established source such claims are tricky %JAMES half my intro is 10+ citations of work that explicitly aims to do this, and I don't want to say that deep learning isn't yet able to do it because HMRNN etc. exist Building models that take advantage of the hierarchical structure of language without  annotation is a longstanding goal in natural language processing.  %new attempt: %Deep learning models for sequence encoding and generation have proven very powerful but are not yet able to learn grammatical structure as part of their decoder. We introduce such a model for the task of machine translation, pairing a recurrent neural network grammar encoder with a novel attentional RNNG decoder and applying policy gradient reinforcement learning to induce unsupervised tree structures on both the source and target. When trained on character-level datasets with no explicit segmentation or parse annotation, the model learns a plausible segmentation and shallow parse, obtaining performance close to an attentional baseline. % * <nishant.subramani23@gmail.com> 2017-04-14T17:46:32.818Z: %  % Consider shortening these two sentences to make them more digestible. %  % ^. %Because attention takes place directly between hierarchical constituents in the source and target, the modelé–³ãƒ¦ç¨ encoder also produces explicitly compositional phrase representations that may be transferred to other tasks. 
 We present MILABOT: a deep reinforcement learning chatbot developed by the Montreal Institute for Learning Algorithms  for the Amazon Alexa Prize competition. MILABOT is capable of conversing with humans on popular small talk topics through both speech and text. The system consists of an ensemble of natural language generation and retrieval models, including template-based models, bag-of-words models, sequence-to-sequence neural network and latent variable neural network models. By applying reinforcement learning to crowdsourced data and real-world user interactions, the system has been trained to select an appropriate response from the models in its ensemble. The system has been evaluated through A/B testing with real-world users, where it performed significantly better than many competing systems. Due to its machine learning architecture, the system is likely to improve with additional data. 
 We propose simple and flexible training and decoding methods for influencing output style and topic in neural encoder-decoder based language generation. This capability is desirable in a variety of applications, including conversational systems, where successful agents need to produce language in a specific style and generate responses steered by a human puppeteer or external knowledge.  We decompose the neural generation process into empirically easier sub-problems: a faithfulness model and a decoding method based on selective-sampling.  We also describe training and sampling algorithms that bias the generation process with a specific language style restriction, or a topic restriction. Human evaluation results show that our proposed methods are able to restrict style and topic without degrading output quality in conversational tasks. 
 Mainly for the sake of solving the lack of keyword-specific data, we propose one Keyword Spotting  system using Deep Neural Network  and Connectionist Temporal Classifier  on power-constrained small-footprint mobile devices, taking full advantage of general corpus from continuous speech recognition which is of great amount. DNN is to directly predict the posterior of phoneme units of any personally customized key-phrase, and CTC to produce a confidence score of the given phoneme sequence as responsive decision-making mechanism. The CTC-KWS has competitive performance in comparison with purely DNN based keyword specific KWS, but not increasing any computational complexity. 
  This paper describes SYSTRAN's systems submitted to the WMT 2017 shared news translation task for English-German, in both translation directions. Our systems are built using OpenNMT\footnote{\url{http://opennmt.net}}, an open-source neural machine translation system, implementing sequence-to-sequence models with LSTM encoder/decoders and attention. We experimented using monolingual data automatically back-translated. Our resulting models are further hyper-specialised with an adaptation technique that finely tunes models according to the evaluation test sentences.  
   We introduce an open-source toolkit for neural machine translation    to support research into model architectures, feature representations,   and source modalities, while maintaining competitive performance, modularity   and reasonable training requirements. 
 Existing neural conversational models process natural language primarily on a lexico-syntactic level, thereby ignoring one of the most crucial components of human-to-human dialogue: its affective content. We take a step in this direction by proposing three novel ways to incorporate affective/emotional aspects into long short term memory  encoder-decoder neural conversation models:  affective word embeddings, which are cognitively engineered,  affect-based objective functions that augment the standard cross-entropy loss, and  affectively diverse beam search for decoding. Experiments show that these techniques improve the open-domain conversational prowess of encoder-decoder networks by enabling them to produce emotionally rich responses that are more interesting and natural. 
  Linguistic sequence labeling is a general approach encompassing a variety of problems, such as part-of-speech tagging and named entity recognition. Recent advances in neural networks  make it possible to build reliable models without handcrafted features. However, in many cases, it is hard to obtain sufficient annotations to train these models. In this study, we develop a neural framework to extract knowledge from raw texts and empower the sequence labeling task. Besides word-level knowledge contained in pre-trained word embeddings, character-aware neural language models are incorporated to extract character-level knowledge. Transfer learning techniques are further adopted to mediate different components and guide the language model towards the key knowledge. Comparing to previous methods, these task-specific knowledge allows us to adopt a more concise model and conduct more efficient training. Different from most transfer learning methods, the proposed framework does not rely on any additional supervision. It extracts knowledge from self-contained order information of training sequences. Extensive experiments on benchmark datasets demonstrate the effectiveness of leveraging character-level knowledge and the efficiency of co-training. For example, on the CoNLL03 NER task, model training completes in about 6 hours on a single GPU, reaching F$_1$ score of 91.71$\pm$0.10 without using any extra annotations. 
 Recurrent neural nets  and convolutional neural nets  are widely used on NLP tasks to capture the long-term and local dependencies, respectively. Attention mechanisms have recently attracted enormous interest due to their highly parallelizable computation, significantly less training time, and flexibility in modeling dependencies. We propose a novel attention mechanism in which the attention between elements from input sequence is directional and multi-dimensional . A light-weight neural net, ``Directional Self-Attention Network '', is then proposed to learn sentence embedding, based solely on the proposed attention without any RNN/CNN structure. DiSAN is only composed of a directional self-attention with temporal order encoded, followed by a multi-dimensional attention that compresses the sequence into a vector representation. Despite its simple form, DiSAN outperforms complicated RNN models on both prediction quality and time efficiency. It achieves the best test accuracy among all sentence encoding methods and improves the most recent best result by $1.02\%$ on the Stanford Natural Language Inference  dataset, and shows state-of-the-art test accuracy on the Stanford Sentiment Treebank , Multi-Genre natural language inference , Sentences Involving Compositional Knowledge , Customer Review, MPQA, TREC question-type classification and Subjectivity   datasets.  
 Neural sequence-to-sequence networks with attention have achieved remarkable performance for machine translation. One of the reasons for their effectiveness is their ability to capture relevant source-side contextual information at each time-step prediction through an attention mechanism. However, the target-side context is solely based on the sequence model which, in practice, is prone to a recency bias and lacks the ability to capture effectively non-sequential dependencies among words. To address this limitation, we propose a target-side-attentive residual recurrent network for decoding, where attention over previous words contributes directly to the prediction of the next word. The residual learning facilitates  the flow of information from the distant past and is able to emphasize any of the previously translated words, hence it gains access to a wider context. The proposed model outperforms a neural MT baseline as well as a memory and self-attention network on three language pairs.  The analysis of the attention learned by the decoder confirms that it emphasizes a wider context, and that it captures syntactic-like structures. 
 This paper provides a sample of a \LaTeX\ document which conforms, somewhat loosely, to the formatting guidelines for ACM SIG Proceedings.\footnote{This is an abstract footnote} 
 While some remarkable progress has been made in neural machine translation  research, there have not been many reports on its development and evaluation in practice. This paper tries to fill this gap by presenting some of our findings from building an in-house travel domain NMT system in a large scale E-commerce setting. The three major topics that we cover are optimization and training , handling real-world content and evaluating results.   %While neural machine translation  has recently made remarkable progress, not many practical use cases of technology adoption have been presented. In this paper, we try to fill this gap presenting the Booking.com in-house NMT system and highlighting some of the main learnings on our journey towards a custom NMT for travel domain.    %In particular, we cover three main learnings on our journey and should be of interest to anyone looking to deploy practical NMT system. First, we cover the efficient optimization and training of a full-scale MT system, focusing on the optimal choice of optimization techniques and analyzing the NMT system's learning curve to identify the minimum corpus size that can be used to train a high-quality system. Second, we explore some practical considerations for handling real-world content, like named entities and real words. Finally, we look into some aspects of NMT quality evaluation, taking a close look at the correlation between BLEU and results of human evaluation. We also describe techniques to proofread the final translation identifying particularly pernicious errors. %BLABLA about results  %We conclude that NMT in a right configuration can be an enabler of the multilingual scaled content services and might have a significant impact on the user experience in Booking.com environment. 
  Cross-lingual representation learning is an important step in making NLP scale to all the world's languages. Recent work on bilingual lexicon induction suggests that it is possible to learn cross-lingual representations of words based on similarities between images associated with these words. However, that work focused on the translation of selected nouns only. In our work, we investigate whether the meaning of other parts-of-speech, in particular adjectives and verbs, can be learned in the same way. We also experiment with combining the representations learned from visual data with embeddings learned from textual data. Our experiments across five language pairs indicate that previous work does not scale to the problem of learning cross-lingual representations beyond simple nouns.  
  This paper presents an approach to the task of predicting an event description from a preceding sentence in a text. Our approach explores sequence-to-sequence learning using a bidirectional multi-layer recurrent neural network. Our approach substantially outperforms previous work in terms of the BLEU score on two datasets derived from WikiHow and DeScript respectively. Since the BLEU score is not easy to interpret as a measure of event prediction, we complement our study with a second evaluation that exploits the rich linguistic annotation of gold paraphrase sets of events.   %, and show that our model achieves a prediction accuracy of 31\%.  
 In this paper, we present a deep reinforcement learning  framework for iterative dialog policy optimization in end-to-end task-oriented dialog systems. Popular approaches in learning dialog policy with RL include letting a dialog agent to learn against a user simulator. Building a reliable user simulator, however, is not trivial, often as difficult as building a good dialog agent. We address this challenge by jointly optimizing the dialog agent and the user simulator with deep RL by simulating dialogs between the two agents. We first bootstrap a basic dialog agent and a basic user simulator by learning directly from dialog corpora with supervised training. We then improve them further by letting the two agents to conduct task-oriented dialogs and iteratively optimizing their policies with deep RL.  %The dialog agent consists of connected neural network based system components, and the entire system can be trained in an end-to-end fashion.  Both the dialog agent and the user simulator are designed with neural network models that can be trained end-to-end. Our experiment results show that the proposed method leads to promising improvements on task success rate and total task reward comparing to supervised training and single-agent RL training baseline models.  
 The past several years have witnessed the rapid progress of end-to-end Neural Machine Translation .  However, there exists discrepancy between training and inference in NMT when decoding, which may lead to serious problems since the model might be in a part of the state space it has never seen during training. To address the issue, Scheduled Sampling has been proposed. However, there are certain limitations in Scheduled Sampling and we propose two dynamic oracle-based methods to improve it. We manage to mitigate the discrepancy by changing the training process towards a less guided scheme and meanwhile aggregating the oracle's demonstrations. Experimental results show that the proposed approaches improve translation quality over standard NMT system. \\ \newline \Keywords{machine translation, dynamic oracle, language model
 Sentiment analysis can be regarded as a relation extraction problem in which the sentiment of some opinion holder towards a certain aspect of a product, theme or event needs to be extracted. We present a novel neural architecture for sentiment analysis as a relation extraction problem that addresses this problem by dividing it into three subtasks: i) identification of aspect and opinion terms, ii) labeling of opinion terms with a sentiment, and iii) extraction of relations between opinion terms and aspect terms. For each subtask, we propose a neural network based component and combine all of them into a complete system for relational sentiment analysis.  The component for aspect and opinion term extraction is a hybrid architecture consisting of a recurrent neural network stacked on top of a convolutional neural network. This approach outperforms a standard convolutional deep neural architecture as well as a recurrent network architecture and performs competitively compared to other methods on two datasets of annotated customer reviews. To extract sentiments for individual opinion terms, we propose a recurrent architecture in combination with word distance features and achieve promising results, outperforming a majority baseline by 18\% accuracy and providing the first results for the USAGE dataset. Our relation extraction component outperforms the current state-of-the-art in aspect-opinion relation extraction by 15\% F-Measure.  
 The World Wide Web holds a wealth of information in the form of unstructured texts such as customer reviews for products, events and more. By extracting and analyzing the expressed opinions in customer reviews in a fine-grained way, valuable opportunities and insights for customers and businesses can be gained.  We propose a neural network based system to address the task of Aspect-Based Sentiment Analysis to compete in Task 2 of the ESWC-2016 Challenge on Semantic Sentiment Analysis. Our proposed architecture divides the task in two subtasks: aspect term extraction and aspect-specific sentiment extraction. This approach is flexible in that it allows to address each subtask independently. As a first step, a recurrent neural network is used to extract aspects from a text by framing the problem as a sequence labeling task. In a second step, a recurrent network processes each extracted aspect with respect to its context and predicts a sentiment label. The system uses pretrained semantic word embedding features which we experimentally enhance with semantic knowledge extracted from WordNet. Further features extracted from SenticNet prove to be beneficial for the extraction of sentiment labels. As the best performing system in its category, our proposed system proves to be an effective approach for the Aspect-Based Sentiment Analysis. 
 Despite the ubiquity of mobile and wearable text messaging applications, the problem of keyboard text decoding is not tackled sufficiently in the light of the enormous success of the deep learning Recurrent Neural Network  and Convolutional Neural Networks  for natural language understanding. In particular, considering that the keyboard decoders should operate on devices with  memory and processor resource constraints, makes it challenging to deploy industrial scale deep neural network  models. This paper proposes a sequence-to-sequence neural attention network system for automatic text correction and completion. Given an erroneous sequence, our model encodes character level hidden representations and then decodes the revised sequence thus enabling auto-correction and completion.  Further, what makes the problem different from vanilla language modelling is the simultaneous text correction and completion.  We achieve this by a combination of character level CNN and gated recurrent unit  encoder along with and a word level gated recurrent unit  attention decoder. Unlike traditional language models that learn from billions of words, our corpus size is only $12$ million words; an order of magnitude smaller. The memory footprint of our learnt model for inference and prediction is also an order of magnitude smaller than the conventional language model based text decoders. We report baseline performance for neural keyboard decoders in such limited domain. Our models achieve a word level accuracy of $90\%$ and a character error rate CER of $2.4$ over the Twitter typo dataset. We present a novel dataset of noisy to corrected mappings by inducing the noise distribution from the Twitter data over the OpenSubtitles $2009$ dataset; on which our model predicts with a word level accuracy of $98\%$ and sequence accuracy of $0.689$. We have also conducted an user study from $8$ users, with our model predicting with an average CER of $2.6\%$ while being competitive with the state-of-the-art non-neural touch-screen keyboard decoders at CER of $1.6\%$. We observe a CER of $2.1\%$ on physical keyboard based decoding. Further, we plan to release the training dataset and the associated software along with the baselines as an open-source tool-kit. We also propose an alternative smooth evaluation measure over the character error rate  for evaluating model predictions based on the contextual underlying intent of the sentence.  Additionally, we have released our trained decoder as an inference server available at \url{www-edc.eng.cam.ac.uk/shaona}. 
 	% %Determining semantic relationships between text sequences is an important step toward natural language understanding.  %Fixed-length sentence embeddings are a scalable and efficient strategy, due to low matching complexity. A latent-variable model is introduced for text matching, inferring sentence representations by jointly optimizing generative and discriminative objectives. To alleviate typical optimization challenges in latent-variable models for text, we employ deconvolutional networks as the sequence decoder , providing learned latent codes with more semantic information and better generalization. Our model, trained in an unsupervised manner, yields stronger empirical predictive performance than a decoder based on Long Short-Term Memory , with less parameters and considerably faster training. Further, we apply it to text sequence-matching problems. The proposed model significantly outperforms several strong sentence-encoding baselines, especially in the semi-supervised setting. 	% 
 Word embedding is a Natural Language Processing  technique that automatically maps words from a vocabulary to vectors of real numbers in an embedding space. It has been widely used in recent years to boost the performance of a variety of NLP tasks such as Named Entity Recognition, Syntactic Parsing and Sentiment Analysis.  Classic word embedding methods such as Word2Vec and GloVe work well when they are given a large text corpus. When the input texts are sparse as in many specialized domains , these methods often fail to produce high-quality vectors.  In this paper, we describe a novel method to train domain-specific word embeddings from sparse texts. In addition to domain texts, our method also leverages diverse types of domain knowledge such as  domain vocabulary and semantic relations. Specifically, we first propose a general framework to encode diverse types of domain knowledge as text annotations.  Then we  develop a novel Word Annotation Embedding  algorithm to incorporate diverse types of  text annotations in word embedding.  We have evaluated our method on two cybersecurity text corpora: a  malware description corpus and a Common Vulnerability and Exposure  corpus. Our evaluation results have demonstrated the effectiveness  of our method in learning domain-specific word embeddings.  
 Conventional automatic speech recognition  typically performs multi-level pattern recognition tasks that map the acoustic speech waveform into a hierarchy of speech units. But, it is widely known that information loss in the earlier stage can propagate through the later stages. After the resurgence of deep learning, interest has emerged in the possibility of developing a purely end-to-end ASR system from the raw waveform to the transcription without any predefined alignments and hand-engineered models. However, the successful attempts in end-to-end architecture still used spectral-based features, while the successful attempts in using raw waveform were still based on the hybrid deep neural network - Hidden Markov model  framework. In this paper, we construct the first end-to-end attention-based encoder-decoder model to process directly from raw speech waveform to the text transcription. We called the model as é–³ãƒ¦ç©¾ttention-based Wav2Texté–³. To assist the training process of the end-to-end model, we propose to utilize a feature transfer learning. Experimental results also reveal that the proposed Attention-based Wav2Text model directly with raw waveform could achieve a better result in comparison with the attentional encoder-decoder model trained on standard front-end filterbank features. 	
 Convolutional neural networks  have recently emerged as a popular building block for natural language processing . Despite their success, most existing CNN models employed in NLP share the  same learned  set of filters for all input sentences. In this paper, we consider an approach of using a small  to learn context-sensitive convolutional filters for text processing. The role of meta network is to abstract the contextual information of a sentence or document into a set of  filters. We further generalize this framework to model sentence pairs, where a  filter generation mechanism is introduced to encapsulate co-dependent sentence representations.  In our benchmarks on four different tasks,  including ontology classification, sentiment analysis, answer sentence selection, and paraphrase identification, our proposed model, a modified CNN with context-sensitive filters, consistently outperforms the standard CNN and attention-based CNN baselines. By visualizing the learned  filters, we further validate and rationalize the effectiveness of proposed framework.  %we illustrate that the  has managed to generate different filter weights according to the specific inputs   %We further generalize our framework to model question-answer sentence pairs and propose an adaptive question answering  model; a novel two-way feature abstraction mechanism is introduced to encapsulate co-dependent sentence representations.   
   Many real-world applications require automated data annotation, such as identifying tissue origins based on gene expressions and classifying images into semantic categories. Annotation classes are often numerous and subject to changes over time, and annotating examples has become the major bottleneck for supervised learning methods. In science and other high-value domains, large repositories of data samples are often available, together with two sources of organic supervision: a lexicon for the annotation classes, and text descriptions that accompany some data samples. Distant supervision has emerged as a promising paradigm for exploiting such indirect supervision by automatically annotating examples where the text description contains a class mention in the lexicon. However, due to linguistic variations and ambiguities, such training data is inherently noisy, which limits the accuracy of this approach. In this paper, we introduce an auxiliary natural language processing system for the text modality, and incorporate co-training to reduce noise and augment signal in distant supervision. Without using any manually labeled data, our \EZL system learned to accurately annotate data samples in functional genomics and scientific figure comprehension, substantially outperforming state-of-the-art supervised methods trained on tens of thousands of annotated examples.   
 		 		Traditional supervised learning makes the { or { problem. In recent years, this problem was studied in both computer vision and text classification communities. In this paper, we propose a novel deep learning algorithm that can perform open world text classification significantly better than existing state-of-the-art methods from both computer vision and text classification.  		 	
 We describe a data-driven approach for automatically explaining new, non-standard English expressions in a given sentence, building on a large dataset that includes 15 years of crowdsourced examples from . Unlike prior studies that focus on matching keywords from a slang dictionary, we investigate the possibility of learning a neural sequence-to-sequence model that generates explanations of unseen non-standard English expressions given context. We propose a dual encoder approach---a word-level encoder learns the representation of context, and a second character-level encoder to learn the hidden representation of the target non-standard expression. Our model can produce reasonable definitions of new non-standard English expressions given their context with certain confidence. 
   This paper presents an intertemporal bimodal network to analyze the evolution of the semantic content of a scientific field within the framework of topic modeling, namely using the Latent Dirichlet Allocation . The main contribution is the conceptualization of the topic dynamics and its formalization and codification into an algorithm. To benchmark the effectiveness of this approach, we propose  three indexes which track the transformation of topics over time, their rate of birth and death, and the novelty of their content. Applying the LDA, we test the algorithm both on a controlled experiment and on a corpus of several thousands of scientific papers over a period of more than 100 years which account for the history of the economic thought.  Keywords: topic modeling, LDA, bimodal network, topic dynamics, economic thought  
   In processing human produced text using natural language processing  techniques, two fundamental subtasks that arise are  [label=]  of the plain text into meaningful subunits , and , to establish relations between subunits.  Such structural interpretation of text provides essential building blocks for upstream expert system tasks: , comprising the tasks of [label=]  In this work, we propose a new joint model that is able to tackle the two tasks simultaneously and construct the  by [label=]  For this purpose, we perform an extensive comparative study of the pipeline methods and the new proposed joint model, reporting  %a 3.42\%  an improvement of over three percentage points in the overall edge $F_1$ score of the property tree.  Also, we propose attention methods, to encourage our model to focus on salient tokens during the construction of the . Thus we experimentally demonstrate the usefulness of attentive neural architectures for the proposed joint model, showcasing  % 2.13\%  a further improvement of two percentage points in edge $F_1$ score for our application.  %Stacking a second LSTM layer on top of the joint model increases the performance by 0.2\% while requiring more computational time comparing to the one-stack LSTM with the attention layer on top. While the results demonstrated are for the particular real estate setting, the model is generic in nature, and thus could be equally applied to other expert system scenarios requiring the general tasks of both [label=]  
 \fontsize{10pt}{12pt}\selectfont Parallel sentence extraction is a task addressing the data sparsity problem found in multilingual natural language processing applications. We propose an end-to-end deep neural network approach to detect translational equivalence between sentences in two different languages. In contrast to previous approaches, which typically rely on multiples models and various word alignment features, by leveraging continuous vector representation of sentences we remove the need of any domain specific feature engineering. Using a siamese bidirectional recurrent neural networks, our results against a strong baseline based on a state-of-the-art parallel sentence extraction system show a significant improvement in both the quality of the extracted parallel sentences and the translation performance of statistical machine translation systems. We believe this study is the first one to investigate deep learning for the parallel sentence extraction task.  
 In order to maximize the applicability of sentiment analysis results, it is necessary to not only classify the overall sentiment  of a given document but also to identify the main words that contribute to the classification. However, most datasets for sentiment analysis only have the sentiment label for each document or sentence. In other words, there is no information about which words play an important role in sentiment classification. In this paper, we propose a method for identifying key words discriminating positive and negative sentences by using a weakly supervised learning method based on a convolutional neural network . In our model, each word is represented as a continuous-valued vector and each sentence is represented as a matrix whose rows correspond to the word vector used in the sentence. Then, the CNN model is trained using these sentence matrices as inputs and the sentiment labels as the output. Once the CNN model is trained, we implement the word attention mechanism that identifies high-contributing words to classification results with a class activation map, using the weights from the fully connected layer at the end of the learned CNN model. In order to verify the proposed methodology, we evaluated the classification accuracy and inclusion rate of polarity words using two movie review datasets. Experimental result show that the proposed model can not only correctly classify the sentence polarity but also successfully identify the corresponding words with high polarity scores.  Keywords: Weakly Supervised Learning, Word Attention, Convolutional Neural Network, Class Activation Mapping 
 In this paper we investigate the role of the dependency tree in a named entity recognizer upon using a set of .  We perform a comparison among different  architectures and show that the grammar of a sentence positively influences the results.  Experiments on the \ontonotes{} dataset demonstrate consistent performance improvements, without requiring heavy feature engineering nor additional language\--specific knowledge.}. 
 % Sentence-level classification and sequential labeling are two fundamental tasks in language understanding. While these two tasks are usually modeled separately, in reality, they are often correlated, for example in intent classification and slot filling, or in topic classification and named-entity recognition.  In order to utilize the potential benefits from their correlations,  we propose a jointly trained model for learning the two tasks simultaneously  via Long Short-Term Memory  networks.  This model predicts the sentence-level category and the word-level label sequence from the stepwise output hidden representations of LSTM.  We also introduce a novel mechanism of ``sparse attention'' to weigh words differently based on their semantic relevance to sentence-level classification. The proposed method outperforms baseline models on ATIS and TREC datasets. 
  We present a multi-modal dialogue system for interactive learning of perceptually grounded word meanings from a human tutor. The system integrates an incremental, semantic parsing/generation framework - Dynamic Syntax and Type Theory with Records  - with a set of visual classifiers that are learned throughout the interaction and which ground the meaning representations that it produces. We use this system in interaction with a simulated human tutor to study the effects of different dialogue policies and capabilities on accuracy of learned meanings, learning rates, and efforts/costs to the tutor. We show that  the overall performance of the learning agent is affected by  who takes initiative in the dialogues;  the ability to express/use their confidence level about   visual attributes; and  the ability to process elliptical and incrementally constructed dialogue turns.  Ultimately, we train an adaptive dialogue policy which optimises the trade-off between classifier accuracy and tutoring costs.   %Human tutors can correct, question, and confirm the statements of a dialogue agent which is trying to interactively learn the meanings of perceptual words, e.g.\ colours and shapes. %We show that different learner and tutor dialogue strategies lead to different learning rates, accuracy of learned meanings, and effort/costs for   human tutors. For example, we show that a learner which can handle corrections in dialogue can learn meanings that are as accurate as a fully-supervised learner, but with less cost/effort to the human tutor.   %multi-attribute objects through Human-Robot Interaction.  %We design a semantic and visual processing system to support this and illustrate how they can be integrated.  %, where we assume that the system does not know the meanings of words % from human descriptions % before learning.  %Previous work could only learn about one attribute in each dialogue turn, whereas here we allow multiple labels to be learned in each turn .  %An attribute-based label set is built using Type Theory with Records  semantic representations and will then be used to interactively train image classifiers . %  and a `zero-shot' learning method with visual features.  %We then evaluate the effectiveness of dialogue interaction on the task of attribute-based object and word-meaning learning. We also test      
 We motivate and describe a new freely available  human-human dialogue data set for interactive learning of visually grounded word meanings through ostensive  definition by a tutor to a learner. The data has been collected using a novel, character-by-character variant of the DiET chat tool  %AE: Greg, what is the best reference here please? % be careful about anonymity with a novel task, where a Learner needs to learn invented visual attribute words  from a tutor. As such, the text-based interactions closely resemble face-to-face conversation and thus contain many of the linguistic phenomena encountered in natural, spontaneous dialogue. These include self- and other-correction, mid-sentence continuations, interruptions,  overlaps, fillers, and hedges. We also present a generic n-gram framework for building user  simulations from this type of incremental  data, which is freely available to researchers. We show that the simulations produce outputs that are similar to the original data . Finally, we train and evaluate a Reinforcement Learning dialogue control agent for learning visually grounded word meanings, trained from the BURCHAK corpus. The learned policy shows comparable performance to a rule-based system built previously.  %We present work on human-human dialogue data collection and further development in support of multi-modal dialogue system design on interactive learning tasks. We build a generic user simulation framework using the real data. We learn a new dialogue policy using  Reinforcement Learning  in interaction with the simulated human tutor. We then focus on comparing the performance  of the new RL-based policy and the rule-based dialogue system against the user simulation: the RL-based policy shows a comparable performance with the rule-based system.     
 Aspect-level sentiment classification aims at identifying the sentiment polarity of specific target in its context. Previous approaches have realized the importance of targets in sentiment classification and developed various methods with the goal of precisely modeling their contexts via generating  target-specific representations. However, these studies always ignore the separate modeling of targets. In this paper, we argue that both targets and contexts deserve special treatment and need to be learned their own representations via interactive learning. Then, we propose the  interactive attention networks  to interactively learn attentions in the contexts and targets, and generate the representations for targets and contexts separately. With this design, the IAN model can well represent a target and its collocative context, which is helpful to sentiment classification. Experimental results on SemEval 2014 Datasets demonstrate the effectiveness of our model. 
 Weighted finite automata~ can expressively model functions defined over strings but are inherently linear models. Given the recent successes of nonlinear models in machine learning, it is natural to wonder whether extending WFA to the nonlinear setting would be beneficial. In this paper, we propose a novel model of neural network based nonlinear WFA model  along with a learning algorithm. Our learning algorithm is inspired by the  algorithm for WFA and relies on a nonlinear decomposition of the so-called Hankel matrix, by means of an auto-encoder network.  The expressive power of NL-WFA and the proposed learning algorithm are assessed on both synthetic and real world data, showing  that NL-WFA can lead to smaller model sizes and infer complex grammatical structures from data. 
 Model compression is significant for the wide adoption of Recurrent Neural Networks  in both user devices possessing limited resources and business clusters requiring quick responses to large-scale service requests.  This work aims to learn structurally-sparse Long Short-Term Memory  by reducing the sizes of basic structures within LSTM units, including input updates, gates, hidden states, cell states and outputs. %In this work, we focus on reducing the sizes of basic structures  within Long Short-Term Memory  units, so as to learn structurally-sparse LSTMs.  Independently reducing the sizes of basic structures can result in inconsistent dimensions among them, and consequently, end up with invalid LSTM units. %Independently reducing the sizes of those basic structures can result in unmatched dimensions among them, and consequently, end up with invalid LSTM units. To overcome the problem, we propose Intrinsic Sparse Structures  in LSTMs. Removing a component of ISS will simultaneously decrease the sizes of all basic structures by one and thereby always maintain the dimension consistency. %By reducing one component of ISS, the sizes of those basic structures are simultaneously reduced by one such that the consistency of dimensions is maintained. By learning ISS within LSTM units, the obtained LSTMs remain regular while having much smaller basic structures.  %By learning ISS within LSTM units, the eventual LSTMs are still regular LSTMs but have much smaller sizes of basic structures.  Based on group Lasso regularization, our method achieves $10.59 \times$ speedup without losing any perplexity of a language modeling of Penn TreeBank dataset.  It is also successfully evaluated through a compact model with only $2.69$M weights for machine Question Answering of SQuAD dataset. Our approach is successfully extended to non-LSTM RNNs, like Recurrent Highway Networks~. Our source code is available\footnote{\url{https://github.com/wenwei202/iss-rnns}}. 
 We present methodology for using dynamic evaluation to improve neural sequence models. Models are adapted to recent history via a gradient descent based mechanism, causing them to assign higher probabilities to re-occurring sequential patterns. Dynamic evaluation outperforms existing adaptation approaches in our comparisons. Dynamic evaluation improves the state-of-the-art word-level perplexities on the Penn Treebank and WikiText-2 datasets to 51.1 and 44.3 respectively, and the state-of-the-art character-level cross-entropies on the text8 and Hutter Prize datasets to 1.19 bits/char and 1.08 bits/char respectively.    
 We present a factorized hierarchical variational autoencoder, which learns disentangled and interpretable representations from sequential data without supervision.  Specifically, we exploit the multi-scale nature of information in sequential data by formulating it explicitly within a factorized hierarchical graphical model that imposes sequence-dependent priors and sequence-independent priors to different sets of latent variables. The model is evaluated on two speech corpora to demonstrate, qualitatively, its ability to transform speakers or linguistic content by manipulating different sets of latent variables; and quantitatively, its ability to outperform an i-vector baseline for speaker verification  and reduce the word error rate by as much as 35\% in mismatched train/test scenarios for automatic speech recognition tasks. 
 Increasingly large document collections require improved information processing methods for searching, retrieving, and organizing  text. Central to these information processing methods is document classification, which has become an important application for supervised learning. Recently the performance of traditional supervised classifiers has degraded as the number of documents has increased. This is because along with growth in the number of documents has come an increase in the number of categories. This paper approaches this problem differently from current document classification methods that view the problem as multi-class classification. Instead we perform hierarchical classification using an approach we call Hierarchical Deep Learning for Text classification~. HDLTex employs stacks of deep learning architectures to provide specialized understanding at each level of the document hierarchy.\\  The continually increasing number of documents produced each year necessitates ever improving information processing methods for searching, retrieving, and organizing  text. Central to these information processing methods is document classification, which has become an important application for supervised learning. The most commonly used supervised learning techniques for document classification are naive Bayes classifiers and support vector machines. Recently the performance of these traditional classifiers has degraded as the number of documents has increased. This is because along with this growth in the number of documents has come an increase in the number of labels or categories required for classification. This paper approaches this problem differently from current document classification methods which view the problem as multi-class classification. Instead we perform hierarchical classification using an approach we call Hierarchical Deep Learning for Text classification~. HDLTex employs stacks of different deep learning architectures to provide specialized understanding at each level of the document hierarchy. Tests of HDLTex show improved accuracy over traditional document classification methods.   
 We present a new approach to the design of deep networks for natural language processing , based on the general technique of Tensor Product Representations  for encoding and processing symbol structures in distributed neural networks. A network architecture --- the }  --- is proposed which is capable in principle of carrying out TPR computation, but which uses unconstrained deep learning to design its internal representations. Instantiated in a model for image-caption generation, TPGN outperforms LSTM baselines when evaluated on the COCO dataset. The TPR-capable structure enables interpretation of internal representations and operations, which prove to contain considerable grammatical content. Our caption-generation model can be interpreted as generating sequences of grammatical categories and retrieving words by their categories from a plan encoded as a distributed representation. 
 Using machine learning algorithms, including deep learning, we studied the prediction of personal attributes from the text of tweets, such as gender, occupation, and age groups. We applied word2vec to construct word vectors, which were then used to vectorize tweet blocks.  The resulting tweet vectors were used as inputs for training models, and the prediction accuracy of those models was examined as a function of the dimension of the tweet vectors and the size of the tweet blocks.  The results showed that the machine learning algorithms could predict the three personal attributes of interest with 60--70\% accuracy. 
 Language understanding  and dialogue policy learning are two essential components in conversational systems. Human-human dialogues are not well-controlled and often random and unpredictable due to their own goals and speaking habits. This paper proposes a role-based contextual model to consider different speaker roles independently based on the various speaking patterns in the multi-turn dialogues. The experiments on the benchmark dataset show that the proposed role-based model successfully learns role-specific behavioral patterns for contextual encoding and then significantly improves language understanding and dialogue policy learning tasks\footnote{The source code is available at: \url{https://github.com/MiuLab/Spk-Dialogue}.}. %and achieves improvement for understanding and policy learning. %leveraging the role information in dialogue contexts  %to better understand the current sentence and predict the next system action as the policy. 
 Speech recognition is largely taking advantage of deep learning, showing that substantial benefits can be obtained by modern Recurrent Neural Networks . The most popular RNNs are Long Short-Term Memory , which typically reach state-of-the-art performance in many tasks thanks to their ability to learn long-term dependencies and robustness to vanishing gradients. Nevertheless, LSTMs have a rather complex design with three multiplicative gates, that might impair their efficient implementation. An attempt to simplify LSTMs has recently led to Gated Recurrent Units , which are based on just two multiplicative gates.  This paper builds on these efforts by further revising GRUs and proposing a simplified architecture potentially more suitable for speech recognition. The contribution of this work is two-fold. First, we suggest to remove the reset gate in the GRU design, resulting in a more efficient single-gate architecture. Second, we propose to replace tanh with ReLU activations in the state update equations. Results show that, in our implementation, the revised architecture reduces the per-epoch training time with more than 30\% and consistently improves recognition performance across different tasks, input features, and noisy conditions when compared to a standard GRU.    
 Identifying nominals with no head match is a long-standing challenge in coreference resolution with current systems performing significantly worse than humans. In this paper we present a new neural network architecture which outperforms the current state-of-the-art system on the English portion of the CoNLL 2012 Shared Task. This is done by using a logistic regression on features produced by two submodels, one of which is has the architecture proposed in  while the other combines domain specific embeddings of the antecedent and the mention. We also propose some simple additional features which seem to improve performance for all models substantially, increasing $F_1$ by almost 4\% on basic logistic regression and other complex models. 
 Neural machine translation  has recently achieved impressive results. A potential problem of the existing NMT algorithm, however, is that the decoding is conducted from left to right, without considering the right context.  This paper proposes an two-stage approach to solve the problem. In the first stage, a conventional attention-based NMT system is used to produce a draft translation, and in the second stage, a novel double-attention NMT system is used to refine the translation, by looking at the original input as well as the draft translation. This drafting-and-refinement can obtain the right-context information from the draft, hence producing more consistent translations. We evaluated this approach using two Chinese-English translation tasks, one with 44k pairs and 1M pairs respectively. The experiments showed that our approach achieved positive improvements over the conventional NMT system: the improvements are 2.4 and 0.9 BLEU points on the small-scale and large-scale tasks, respectively. 
 Clickbait  make use of misleading titles that hide critical information from or exaggerate the content on  the landing target pages  to entice clicks.  %The target contents are usually of low quality.  As clickbaits often use eye-catching wording to attract viewers, target contents are often of low quality. Clickbaits are especially widespread on social media such as Twitter, adversely impacting user experience by causing immense dissatisfaction. Hence, it has become increasingly important to put forward a widely applicable approach to identify and detect clickbaits. In this paper, we make use of a dataset from the clickbait challenge 2017  comprising of over 21,000 headlines/titles, each of which is annotated by at least five judgments from crowdsourcing on how clickbait it is. We attempt to build an effective computational clickbait detection model on this dataset. We first considered a total of 331 features, filtered out many features to avoid overfitting and improve the running time of learning, and eventually  selected the 60 most important features for our final model. Using these features, Random Forest Regression achieved the following results: MSE=0.035 MSE, Accuracy=0.82, and F1-sore=0.61 on the clickbait class.  
 We present a framework for machine translation evaluation using neural networks in a pairwise setting, where the goal is to select the better translation from a pair of hypotheses, given the reference translation. In this framework, lexical, syntactic and semantic information from the reference and the two hypotheses is embedded into compact distributed vector representations, and fed into a multi-layer neural network that models nonlinear interactions between each of the hypotheses and the reference, as well as between the two hypotheses. We experiment with the benchmark datasets from the WMT Metrics shared task, on which we obtain the best results published so far, with the basic network configuration. We also perform a series of experiments to analyze and understand the contribution of the different components of the network. We evaluate variants and extensions, including fine-tuning of the semantic embeddings, and sentence-based representations modeled . In summary, the proposed framework is flexible and generalizable, allows for efficient learning and scoring, and provides an MT evaluation metric that correlates with human judgments, and is on par with the state of the art. 
 	Text summarization and text simplification are two major ways to simplify the text for poor readers, including children, non-native speakers, and the functionally illiterate. Text summarization is to produce a brief summary of the main ideas of the text, while text simplification aims to reduce the linguistic complexity of the text and retain the original meaning. Recently, most approaches for text summarization and text simplification are based on the sequence-to-sequence model, which achieves much success in many text generation tasks. However, although the generated simplified texts are similar to source texts literally, they have low semantic relevance. In this work, our goal is to improve semantic relevance between source texts and simplified texts for text summarization and text simplification. We  	introduce a Semantic Relevance Based neural model to encourage high semantic similarity between texts and summaries. In our model, the source text is represented by a gated attention encoder, while the summary representation is produced by a decoder. Besides, the similarity score between the representations is maximized during training. Our experiments show that the proposed model outperforms the state-of-the-art systems on two benchmark corpus\footnote{Our code is available at https://github.com/shumingma/SRB}. 
 Lexical entailment, such as hyponymy, is a fundamental issue in the semantics of natural language.  This paper proposes distributional semantic models which efficiently learn word embeddings for entailment, using a recently-proposed framework for modelling entailment in a vector-space.  These models postulate a latent vector for a pseudo-phrase containing two neighbouring word vectors. We investigate both modelling words as the evidence they contribute about this phrase vector, or as the posterior distribution of a one-word phrase vector, and find that the posterior vectors perform better.  The resulting word embeddings outperform the best previous results on predicting hyponymy between words, in unsupervised and semi-supervised experiments. 
 In recent years, digitizing paper-based files became a major undertaking for private and public archives as well as an important task in electronic mailroom applications.  As a first step, the workflow involves scanning and Optical Character Recognition  of documents.  Preservation of document contexts of single page scans is a major requirement in this context.  To facilitate workflows involving very large amounts of paper scans, page stream segmentation  is the task to automatically separate a stream of scanned images into multi-page documents. In a digitization project together with a German federal archive, we developed a novel approach based on convolutional neural networks  combining image and text features to achieve optimal document separation results. Evaluation shows that our PSS architecture achieves an accuracy up to 93~\% which can be regarded as a new state-of-the-art for this task. \\ \newline \Keywords{page stream segmentation, convolutional neural nets, document image classification, document management, text classification
 The surge of social media use brings huge demand of multilingual sentiment analysis  for unveiling cultural difference. So far, traditional methods resorted to machine translation---translating texts in other languages to English, and then adopt the methods once worked in English. However, this paradigm is conditioned by the quality of machine translation. In this paper, we propose a new deep learning paradigm to assimilate the differences between languages for MSA. We first pre-train monolingual word embeddings separately, then map word embeddings in different spaces into a shared embedding space, and then finally train a parameter-sharing deep neural network for MSA. The experimental results show that our paradigm is effective. Especially, our CNN model outperforms a state-of-the-art baseline by around 2.1\% in terms of classification accuracy. 
 In this paper, we propose a novel end-to-end neural architecture for ranking candidate answers, that adapts a hierarchical recurrent neural network and a latent topic clustering module.  With our proposed model, a text is encoded to a vector representation from an word-level to a chunk-level to effectively capture the entire meaning.  In particular, by adapting the hierarchical structure, our model shows very small performance degradations in longer text comprehension while other state-of-the-art recurrent neural network models suffer from it. Additionally, the latent topic clustering module extracts semantic information from target samples. This clustering module is useful for any text related tasks by allowing each data sample to find its nearest topic cluster, thus helping the neural network model analyze the entire data.  We evaluate our models on the Ubuntu Dialogue Corpus and consumer electronic domain question answering dataset, which is related to Samsung products.  The proposed model shows state-of-the-art results for ranking question-answer pairs. 
   We show how to predict the basic word-order facts of a novel language given only a corpus of part-of-speech  sequences.  We predict how often direct objects follow their verbs, how often adjectives follow their nouns, and in general the directionalities of all dependency relations.  Such typological properties could be helpful in grammar induction.  While such a problem is usually regarded as unsupervised learning, our innovation is to treat it as { features of a language's POS sequence  that correlate with the language's deeper structure .  In the experiment, we show: 1) Given a small set of real languages, it helps to add many synthetic languages to the training data. 2) Our system is robust even when the POS sequences include noise. 3) Our system on this task outperforms a grammar  induction baseline by a large margin.  
 Company disclosures greatly aid in the process of financial decision-making; therefore, they are consulted by financial investors and automated traders before exercising ownership in stocks. While humans are usually able to correctly interpret the content, the same is rarely true of computerized decision support systems, which struggle with the complexity and ambiguity of natural language. A possible remedy is represented by deep learning, which overcomes several shortcomings of traditional methods of text mining. For instance, recurrent neural networks, such as long short-term memories, employ hierarchical structures, together with a large number of hidden layers, to automatically extract features from ordered sequences of words and capture highly non-linear relationships such as context-dependent meanings. However, deep learning has only recently started to receive traction, possibly because its performance is largely untested. Hence, this paper studies the use of deep neural networks for financial decision support. We additionally experiment with transfer learning, in which we pre-train the network on a different corpus with a length of 139.1 million words. Our results reveal a higher directional accuracy as compared to traditional machine learning when predicting stock price movements in response to financial disclosures. Our work thereby helps to highlight the business value of deep learning and provides recommendations to practitioners and executives. 
 Learning effective representations of sentences is one of the core missions of natural language understanding. Existing models either train on a vast amount of text, or require costly, manually curated sentence relation datasets. We show that with dependency parsing and rule-based rubrics, we can curate a high quality sentence relation task by leveraging explicit discourse relations. We show that our curated dataset provides an excellent signal for learning vector representations of sentence meaning, representing relations that can only be determined when the meanings of two sentences are combined. We demonstrate that the automatically curated corpus allows a bidirectional LSTM sentence encoder to yield high quality sentence embeddings and can serve as a supervised fine-tuning dataset for larger models such as BERT. Our fixed sentence embeddings achieve high performance on a variety of transfer tasks, including SentEval, and we achieve state-of-the-art results on Penn Discourse Treebank's implicit relation prediction task. 
 Focusing on the task of identifying event temporal status, we find that events directly or indirectly governing the target event in a dependency tree are most important contexts.  Therefore, we extract dependency chains containing context events and use them as input in neural network models, which consistently outperform previous models using local context words as input.  Visualization verifies that the dependency chain representation can effectively capture the context events which are closely related to the target event and play key roles in predicting event temporal status. 
 {2} % abstract reset page number at the start, end of environment \thispagestyle{plain} % empty -> plain  Traditional approach in artificial intelligence  have been solving the problem that is difficult for human but relatively easy for computer if it could be formulated as mathematical rules or formal languages. However, their symbol, rule-based approach failed in the problem where human being solves intuitively like image recognition, natural language understanding and speech recognition.  Therefore the machine learning, which is subfield of AI, have tackled this intuitive problems by making the computer learn from data automatically instead of human efforts of extracting complicated rules. Especially the deep learning which is a particular kind of machine learning as well as central theme of this thesis, have shown great popularity and usefulness recently.  It has been known that the powerful computer, large dataset and algorithmic improvement have made recent success of the deep learning. And this factors have enabled recent research to train deeper network achieving significant performance improvement. Those current research trends motivated me to quest deeper architecture for the end-to-end speech recognition.   In this thesis, I experimentally showed that the proposed deep neural network achieves state-of-the-art results on `TIMIT' speech recognition benchmark dataset. Specifically, the convolutional attention-based sequence-to-sequence model which has the deep stacked convolutional layers in the attention-based seq2seq framework achieved 15.8\% phoneme error rate.  
 %  Analyzing customer feedback is the best way to channelize the data into new marketing strategies that benefit entrepreneurs as well as customers. Therefore an automated system which can analyze the customer behavior is in great demand. Users may write feedbacks in any language, and hence mining appropriate information often becomes intractable. Especially in a traditional feature-based supervised model, it is difficult to build a generic system as one has to understand the concerned language for finding the relevant features.   % as understanding %understanding In a global multilingual environment, feedback may be available in any language. In order to build a feature-oriented supervised model, one has to understand the  %feedback given in various languages, which is often not possible. In order to overcome this, we propose deep Convolutional Neural Network  and Recurrent Neural Network  based approaches that do not require handcrafting of features. We evaluate these techniques for analyzing customer feedback sentences on four languages, namely English, French, Japanese and Spanish. %on four  %that and deep learning based techniques  to analyze customer feedback given in four different languages named as English, French, Japanese and Spanish.  Our empirical analysis shows that our models perform well in all the four languages on the setups of IJCNLP Shared Task on Customer Feedback Analysis. Our model achieved the second rank in French, with an accuracy of 71.75\% and third ranks for all the other languages. 
   We propose an end-to-end neural network to predict the geolocation of a  tweet.  The network takes as input  a number of raw Twitter metadata such as the tweet message and  associated user account information. Our model is language independent,  and despite minimal feature engineering, it is interpretable and capable  of learning location indicative words and timing patterns.  Compared to  state-of-the-art systems, our model outperforms them by 2\%-6\%.   Additionally, we propose extensions to the model to compress  representation learnt by the network into binary codes. Experiments show  that it produces compact codes compared to benchmark hashing algorithms.   An implementation of the model is released  publicly.\footnote{\url{https://github.com/jhlau/twitter-deepgeo}}     
