document,summary,id
"  Social media has become an essential element of our society by which people communicate and exchange information on a daily basis. The strong influence of social media on internet users has been of great benefit to many individuals, businesses, and organizations. Many companies and organizations nowadays use social media to reach customers, promote products, and ensure customer satisfaction. Despite the benefits associated with the widespread use of social media, they remain vulnerable to ill-intentioned activities, as the openness, anonymity, and informal structure of these platforms have contributed to the spread of harmful and violent content. \par  Although social media service providers have policies to control these ill-intentioned behaviors, these rules are rarely followed by users. Social media providers also allow their users to report any inappropriate content, but unreported content may not be discovered due to the huge volume of data on these platforms. Some countries have restricted the use of social media, and others have taken legal action regarding violent or harmful content that might target particular individuals or communities. However, these violations might end up unpunished due to the anonymous nature of these platforms, allowing ill-intentioned users to fearlessly share harmful content by using nicknames or fake identities. One of the most-shared harmful content on social media is hate content, which might take different forms such as text, photos, and/or video. Hate speech is any expression that encourages, promotes, or justifies violence, hatred, or discrimination against a person or group of individuals based on characteristics such as color, gender, race, sexual orientation, nationality, religion, or other attributes. Online hate speech is rapidly increasing over the entire world, as nearly \% of the world閳ユ獨 population  communicates on social media. Studies have shown that nearly \% of Americans have experienced online hate and harassment. This result is \% higher than the results of a comparable questionnaire conducted in  . For younger people, the results show that \% of teenagers frequently encounter hate speech on social media.  \par   One of the most dangerous and influential forms of online hate speech is led and spread by supporters of extreme ideologies who target other racial groups or minorities. White supremacists are one of the ideological groups who believe that people of the white race are superior and should be dominant over people of other races; this is also referred to as white nationalism in more radical ideologies. White supremacists claim that they are undermined by dark skin people, Jews, and multicultural Muslims, and they want to restore white people閳ユ獨 power, violently if necessary. They have also claimed responsibility for many violent incidents that happened in the s, including bank robberies, bombings, and murders. The white supremacist ideology has been adopted by both right-wing and left-wing extremists who combine white supremacy with political movements. \par   White supremacist hate speech has become a significant threat to the community, either by influencing young people with hateful ideas or by creating movements to implement their goals in the real world. A study has also suggested links between hate speech and hate crimes against others . Several recent brutal attacks have also been committed by supporters of radical white supremacists who were very active members on social media. The mass shootings in New Zealand, Texas, and Norway were committed by white supremacists who had shared their opinions and ideologies on social media. The attacker of two mosques in Christchurch, New Zealand, was a 28 year old man who identified himself as a white nationalist hero, and posted a manifesto that discussed his intent to kill people as a way to reinforce the sovereignty of white extremists. From a psychological point of view, any violent attack must be preceded by warning behaviors, which includes any behavior that shows before a violent attack that is associated with it, and can in certain situations predict it. Warning behaviors can be either real-world markers  or linguistic markers or signs  which can happen in real life and/or online.  \par   Automatic detection of white supremacist content on social media can be used to predict hate crimes and violent events. Perpetrators can be caught before attacks happen by examining online posts that give strong indications of an intent to make an attack. Predicting violent attacks based on monitoring online behavior would be helpful in crime prevention, and detecting hateful speech on social media will also help to reduce hatred and incivility among social media users, especially younger generations. \par  Studies have investigated the detection of different kinds of hate speech such as detecting cyberbullying , offensive language  , or targeted hate speech in general by distinguishing between types of hate speech and neutral expressions. Others have dealt with the problem by detecting a specific types of hate speech, such as anti-religion, jihadist, sexist, and racist. However, less attention has been given to detecting white supremacism in particular, with limited studies.   \par  White supremacist extremists tend to use rhetoric   in their language. They also use specific vocabulary, abbreviations, and coded words to express their beliefs and intent to promote hatred or encourage violence to avoid being detected by traditional detection methods. They mostly use hate speech against other races and religions, or claim that other races are undermining them. Figure shows an example of a white supremacist tweet.  \par  \subsection{Research goal and contributions}  In this paper, we aim to detect white supremacist tweets based on textual features by using deep learning techniques. We collected about  tweets from white supremacist accounts and hashtags to extract word embeddings, and then we labeled about  subsets of the data corpus to build a white supremacist dataset. We applied two approaches: the first uses domain-specific word embedding learned from the corpus and then classifies  tweets using a Bidirectional LSTM-based deep model. This approach is evaluated on multiple dataset and achieved different results depending on the datasets that ranged from a \% to a \% F1-score. The second approach uses a pre-trained language model that is fine-tune on the white supremacist dataset using Neural Network dense layer. The BERT language model F1-scores ranged from \% to \%. Thus, the research contribution can be summarized as follow:   \par  The rest of the paper proceeds with the Background Section , which provides information on the methodology used, related studies in the Literature Review section , a detailed description of methods in the Methodology section , details of the used datasets in the Dataset section , specifications of the methodologies and the results of each approach in the Experiments and Results section , observations and analysis of the performance of each approach in the Discussion section , and finally, the Conclusion and Future Work section .    
","  White supremacists embrace a radical ideology that considers white people superior to people of other races. The critical influence of these groups is no longer limited to social media; they also have a significant effect on society in many ways by promoting racial hatred and violence. White supremacist hate speech is one of the most recently observed harmful content on social media. Traditional channels of reporting hate speech have proved inadequate due to the tremendous explosion of information, and therefore, it is necessary to find an automatic way to detect such speech in a timely manner. This research investigates the viability of automatically detecting white supremacist hate speech on Twitter by using deep learning and natural language processing techniques. Through our experiments, we used two approaches, the first approach is by using domain-specific embeddings which are extracted from white supremacist corpus in order to catch the meaning of this white supremacist slang with bidirectional Long Short-Term Memory  deep learning model, this approach reached a 0.74890 F1-score. The second approach is by using the one of the most recent language model which is BERT, BERT model provides the state of the art of most NLP tasks. It reached to a 0.79605 F1-score. Both approaches are tested on a balanced dataset given that our experiments were based on textual data only. The dataset was combined from dataset created from Twitter and a Stormfront dataset compiled from that white supremacist forum.",0
"   Graph Neural Networks  have in recent years been shown to provide a scalable and highly performant means of incorporating linguistic information and other structural biases into NLP models. They have been applied to various kinds of representations  and shown effective on a range of tasks, including relation extraction~, question answering~, syntactic and semantic parsing tasks~, summarization ~, machine translation~ and abusive language detection in social networks~.     While GNNs often yield strong performance, % such models are % complex, and it can be difficult to understand the `reasoning' behind their predictions. For NLP practitioners, it is highly desirable to know which linguistic information a given model encodes and how that encoding happens~. The difficulty in interpreting GNNs represents a barrier to such analysis. %  Furthermore,  this opaqueness decreases user trust% , impedes the discovery of harmful biases, and complicates error analysis% ~,   an issue for GNNs where seemingly small implementation differences can make or break models~.  In this work, we focus on post-hoc analysis of GNNs and formulate some desiderata for an interpretation method:      A simple way to perform interpretation is to use  erasure search~, an approach wherein attribution happens by searching for a maximal subset of features which can be entirely removed without affecting model predictions. % The removal guarantees that all information about the discarded features is ignored by the model. This  contrasts with approaches which use heuristics to define feature importance, for example attention-based methods~ or back-propagation techniques~. They do not guarantee that the model ignores low-scoring features, attracting criticism in recent years . % The trust in erasure search is reflected in the literature through other methods % motivated as approximations of erasure~, or through new attribution techniques % evaluated using erasure search as ground truth~.  Applied to GNNs, erasure search would involve a search for the largest subgraph which can be completely discarded. Besides faithfulness considerations and conceptual simplicity, discrete attributions would also simplify the comparison of relevance between paths; this is in contrast to continuous attribution to edges, where it is not straightforward to extract and visualize important paths. Furthermore, in contrast to techniques based on artificial gradients~, erasure search would provide implementation invariance~. This is important in NLP, as models commonly use highly parametrized decoders on top of GNNs, e.g.~\citet{koncel-kedziorski-etal-2019-text}.   While arguably satisfying criteria  and  in our desiderata, erasure search unfortunately fails on tractability. In practical scenarios, it is infeasible, and even approximations, which remove one feature at a time~ and underestimate their contribution due to saturation~,  remain prohibitively expensive.   Our GraphMask aims at meeting the above desiderata by achieving the same benefits as erasure search in a scalable manner. That is, our method makes easily interpretable hard choices on whether to retain or discard edges such that discarded edges have no relevance to model predictions, while remaining tractable and model-agnostic~. GraphMask  can be understood as a differentiable form of subset erasure, where, instead of finding an optimal subset to erase for every given example, we learn an erasure function which predicts for every edge  at every layer  whether that connection should be retained. Given an example graph , our method returns for each layer  a subgraph  such that we can faithfully claim that no edges outside  influence the predictions of the model. To enable gradient-based optimization for our erasure function, we rely on sparse stochastic gates~.  In erasure search, optimization happens individually for each example. This can result in a form of overfitting where even non-superfluous edges are aggressively pruned, because a similar prediction could be made using an alternative smaller subgraph; we refer to this problem as hindsight bias. % Because our model relies on a parametrized erasure function rather than an individual per-edge choice, we can address this issue by amortizing parameter learning over a training dataset through a process similar to the readout bottleneck introduced in~\citet{schulz2020restricting}. As we demonstrate in Section, this strategy avoids hindsight bias.  \paragraph{Contributions} Our contributions are as follows:   
"," Graph neural networks  have become a popular approach to integrating structural inductive biases into NLP models. However, there has been little work on interpreting them, and specifically on understanding which parts of the graphs  contribute to a prediction. In this work, we introduce a post-hoc method for interpreting the predictions of GNNs which identifies unnecessary edges. % Given a trained GNN model, we learn a simple classifier that, for every edge in every layer, predicts if that edge can be dropped. We demonstrate that such a classifier can be trained in a fully differentiable fashion, employing stochastic gates and encouraging sparsity through the expected  $L_0$ norm. We use our technique as an attribution method to analyze GNN models for two tasks -- question answering and semantic role labeling -- providing insights into the information flow in these models. We show that we can drop a large proportion of edges without deteriorating the performance of the model, while we can analyse the remaining edges for interpreting model predictions.",1
"      Many natural language tasks involve entities, e.g., relation classification, entity typing, named entity recognition , and question answering . Key to solving such entity-related tasks is a model to learn the effective representations of entities. Conventional entity representations assign each entity a fixed embedding vector that stores information regarding the entity in a knowledge base  . Although these models capture the rich information in the KB, they require entity linking to represent entities in a text, and cannot represent entities that do not exist in the KB.  By contrast, contextualized word representations  based on the transformer , such as BERT , and RoBERTa , provide effective general-purpose word representations trained with unsupervised pretraining tasks based on language modeling. Many recent studies have solved entity-related tasks using the contextualized representations of entities computed based on CWRs . However, the architecture of CWRs is not well suited to representing entities for the following two reasons:  Because CWRs do not output the span-level representations of entities, they typically need to learn how to compute such representations based on a downstream dataset that is typically small.  Many entity-related tasks, e.g., relation classification and QA, involve reasoning about the relationships between entities. Although the transformer can capture the complex relationships between words by relating them to each other multiple times using the self-attention mechanism , it is difficult to perform such reasoning between entities because many entities are split into multiple tokens in the model. Furthermore, the word-based pretraining task of CWRs is not suitable for learning the representations of entities because predicting a masked word given other words in the entity, e.g., predicting ``Rings'' given ``The Lord of the [MASK]'', is clearly easier than predicting the entire entity.  In this paper, we propose new pretrained contextualized representations of words and entities by developing LUKE . LUKE is based on a transformer  trained using a large amount of entity-annotated corpus obtained from Wikipedia. An important difference between LUKE and existing CWRs is that it treats not only words, but also entities as independent tokens, and computes intermediate and output representations for all tokens using the transformer . Since entities are treated as tokens, LUKE can directly model the relationships between entities.  LUKE is trained using a new pretraining task, a straightforward extension of BERT's masked language model  . The task involves randomly masking entities by replacing them with  entities, and trains the model by predicting the originals of these masked entities. We use RoBERTa as base pre-trained model, and conduct pretraining of the model by simultaneously optimizing the objectives of the MLM and our proposed task. When applied to downstream tasks, the resulting model can compute representations of arbitrary entities in the text using  entities as inputs. Furthermore, if entity annotation is available in the task, the model can compute entity representations based on the rich entity-centric information encoded in the corresponding entity embeddings.  Another key contribution of this paper is that it extends the transformer using our entity-aware self-attention mechanism. Unlike existing CWRs, our model needs to deal with two types of tokens, i.e., words and entities. Therefore, we assume that it is beneficial to enable the mechanism to easily determine the types of tokens. To this end, we enhance the self-attention mechanism by adopting different query mechanisms based on the attending token and the token attended to.  We validate the effectiveness of our proposed model by conducting extensive experiments on five standard entity-related tasks: entity typing,  relation classification, NER,  cloze-style QA, and extractive QA. Our model outperforms all baseline models, including RoBERTa, in all experiments, and obtains state-of-the-art results on five tasks: entity typing on the Open Entity dataset , relation classification on the TACRED dataset , NER on the CoNLL-2003 dataset , cloze-style QA on the ReCoRD dataset , and extractive QA on the SQuAD 1.1 dataset . We publicize our source code and pretrained representations at \url{https://github.com/studio-ousia/luke}.  The main contributions of this paper are summarized as follows:   
","     Entity representations are useful in natural language tasks involving entities.     In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer \cite{NIPS2017_7181}.     The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them.     Our model is trained using a new pretraining task based on the masked language model of BERT \cite{devlin2018bert}.     The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia.     We also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens  when computing attention scores.     The proposed model achieves impressive empirical performance on a wide range of entity-related tasks.     In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity , TACRED , CoNLL-2003 , ReCoRD , and SQuAD 1.1 .     Our source code and pretrained representations are available at \url{https://github.com/studio-ousia/luke}.",2
"  Modern methods of natural language processing  are based on complex neural network architectures, where language units are represented in a metric space . Such a phenomenon allows us to express linguistic features  mathematically.   The method of obtaining such representation and their interpretations were described in multiple overview works. Almeida and Xex\'eo surveyed different types of static word embeddings , and Liu et al.  focused on contextual representations found in the most recent neural models. Belinkov and Glass  surveyed the strategies of interpreting latent representation. Best to our knowledge, we are the first to focus on the syntactic and morphological abilities of the word representations. We also cover the latest approaches, which go beyond the interpretation of latent vectors and analyze the attentions present in state-of-the-art Transformer models. %analyzed matrix representation of the neural networks. %.    %\tltodo{Maybe use ToC as instroduction to section and remove them from here} %The survey is organized in the following way: %In Section, we introduce several types of NLP models that are going to be analyzed. Section shortly describes the metrics used to evaluate syntactic information captured by the models. The observations and results for static and contextual word embeddings are presented in Section. The observations on attention matrices for different Transformer architectures are described in Section. We summarize our findings in Section. %for attention matrices in Transformer models. %We conclude the survey by mentioning supervised approaches to enhance syntactic signal.   %
","  Neural networks trained on natural language processing tasks capture syntax even though it is not provided as a supervision signal. %The syntax is captured by the natural language processing models even when not provided as a supervision signal. This %This phenomenon  indicates that syntactic analysis is essential to the understating of language in artificial intelligence systems. % This overview paper covers approaches of evaluating the amount of syntactic information included in the representations of words for different neural network architectures. %This overview paper covers approaches to evaluating of syntactic information in the representation of words in neural networks. We compare the spectrum of model architectures and the training data. We mainly summarize research on English monolingual data on language modeling tasks and multilingual data for neural machine translation systems and multilingual language models.   %Particularly we consider corpora in one language, mainly English used for training Language Models, and multilingual data for Machine Translation Systems and Multilingual Language Models. We describe which pre-trained models and representations of language are best suited for transfer to syntactic tasks.  % We hope that our comparison will help in finding pretrained model for transfer   % The survey covers the research on producing representation of language and evaluation of captured syntactic information. I focus on the works that do not use syntactic supervision during training of the representation, and are obtained on large mono or multilingual corpora.  % The aim of this work is to examine to what extent syntactic features can be extracted from plain text and how it can be compared to expert annotations.",3
" Texts represent the main source of knowledge for our society. However, they can be written in various manners, thus creating a barrier between the readers and the ideas they intend to convey. Therefore, document comprehension is the main challenge users have to overcome, by understanding the meaning behind troublesome words and becoming familiar with them. Complex Word Identification  is a task that intends to identify hard-to-understand tokens, highlighting them for further clarification and assisting users to grasping the contents of the document.  Motivation. Each culture includes exclusive ideas, available only for the ones who can pass the obstacle of language. However, properly understanding language can prove to be a difficult task. By identifying complex words, users can make consistent steps towards adapting to the culture and accessing the knowledge it has to offer. As an example, entries like ""mayoritariamente""  or ""gobernatura""  in the Spanish environment can create understanding problems for non-native Spanish speakers, thus requiring users to familiarize themselves with these particular terms.  Challenges. The identification task becomes increasingly more difficult, as proper complex word identification is not guaranteed. For example, if we use human identification techniques, language learners may consider a new word to be complex, while others might not share the same opinion by relying on their prior knowledge in that language. Therefore, universal annotation techniques are required, such that a ground truth can be established and the same set of words is considered complex in any context.  Proposed Approach. We consider state-of-the-art solutions, namely multilingual Transformer-based approaches, to address the CWI challenge. First, we apply a zero-shot learning approach. This was performed by training Recurrent Neural Networks  and Transformer-based models on a source language corpus, followed by validating and testing on a corpus from a target language, different from the source language.  A second experiment consists of a one-shot learning approach that considers training on each of the three languages , but only keeping one entry from the target language, and validating and testing on English, German, Spanish, and French, respectively.   In addition, we performed few-shot learning experiments by validating and testing on a language, and training on the others, but with the addition of a small number of training entries from the target language. The model learns sample structures from the language and, in general, performs better when applied on multiple entries. Furthermore, this training process can help the model adapt to situations in which the number of training inputs is scarce. The dataset provided by the CWI Shared Task 2018  was used to perform all experiments.  This paper is structured as follows. The second section describes related work and its impact on the CWI task. The third section describes the corpus and outlines our method based on multilingual embeddings and Transformer-based models, together with the corresponding experimental setup. The fourth section details the results, alongside a discussion and an error analysis. The fifth section concludes the paper and outlines the main ideas, together with potential extensions.  
"," Complex Word Identification  is a task centered on detecting hard-to-understand words, or groups of words, in texts from different areas of expertise. The purpose of CWI is to highlight problematic structures that non-native speakers would usually find difficult to understand. Our approach uses zero-shot, one-shot, and few-shot learning techniques, alongside state-of-the-art solutions for Natural Language Processing  tasks . Our aim is to provide evidence that the proposed models can learn the characteristics of complex words in a multilingual environment by relying on the CWI shared task 2018 dataset available for four different languages . Our approach surpasses state-of-the-art cross-lingual results in terms of macro F1-score on English , German , and Spanish  languages, for the zero-shot learning scenario. At the same time, our model also outperforms the state-of-the-art monolingual result for German .",4
" Aspect based sentiment analysis   is a fine-grained sentiment analysis task. ABSA contains several subtasks, four of which are aspect category detection  detecting aspect categories mentioned in sentences, aspect category sentiment analysis  predicting the sentiments of the detected aspect categories, aspect term extraction  identifying aspect terms presenting in sentences and aspect term sentiment analysis  classifying the sentiments toward the identified aspect terms. While aspect categories mentioned in a sentence are from a few predefined categories and may not occur in the sentence, aspect terms  explicitly appear in sentences. Fig.  shows an example. ACD detects the two aspect categories food and service and ACSA predicts the positive and negative sentiments toward them. ATE identifies the two aspect terms ``taste'' and ``service'' and ATSA classifies the positive and negative sentiments toward them. In this paper, we concentrate on the ACSA task. The ACD task as a auxiliary is used to find aspect category-related nodes from sentence constituency parse trees for the ACSA task.    Since a sentence usually discusses one or more aspect categories and expresses different sentiments toward them, various attention-based methods have been developed to allocate appropriate sentiment words for given aspect categories. Wang et al.  were the first to explore attention mechanism on the ACSA task and proposed an attention based LSTM . For a given sentence and an aspect category mentioned in the sentence, AT-LSTM first models the sentence via a LSTM model,  then combines the hidden states from the LSTM with the representation of the aspect category to generate aspect category-specific word representations, finally applies an attention mechanism over the word representations to find the aspect category-related sentiment words, that are used to predict the sentiment of the aspect category. The constrained attention networks   handles multiple aspect categories of a sentence simultaneously and introduces orthogonal and sparse regularizations to constrain the attention weight allocation. The aspect-level sentiment capsules model  performs ACD and ACSA simultaneously, which also uses an attention mechanism to find aspect category related sentiment words and achieves state-of-the-art performances on the ACSA task.  However, these models directly use the given aspect category to find the aspect category-related sentiment words, which may cause mismatching between the sentiment words and the aspect categories when an unrelated sentiment word is semantically meaningful for the given aspect category. For the example in Fig., ``Great'' and ``bad'' can be used interchangeably. It is hard for attention-based methods to distinguish which word is associated with aspect category food or service among ``good'' and ``bad''. To solve the problem, The HiErarchical ATtention network  first finds the aspect terms indicating the given aspect cagegory, then finds the aspect category-related sentiment words  depending on the position information and semantics of the aspect terms. Although HEAT obtains good results, to train HEAT, we additionally need to annotate the aspect terms indicating the given aspect category, which can be time-consuming and expensive.  To mitigate the mismatch problem, we propose a Sentence Constituent-Aware Network  for aspect-category sentiment analysis which does not require any additional annotation. SCAN contains two graph attention networks   and an interactive loss function. Given a sentence, we first use the Berkeley Neural Parser  to generate the constituency parse tree. The two GATs generate representations of the nodes in the sentence constituency parse tree for the ACD task and the ACSA task, respectively. The GAT for ACD mainly attends to the words indicating aspect categories, while the GAT for ACSA mainly attends to sentiment words. For a given aspect category, the interactive loss function helps the ACD task to find the nodes that can predict the aspect category but can閳ユ獩 predict other aspect categories. The sentiment words in the nodes then are used to predict the sentiment polarity of the aspect category by the ACSA task. Fig.  shows the constituency parse tree of the sentence ``Greate taste bad service.''. For the aspect category food, SCAN first finds the yellow nodes ``Greate taste'' and ``taste'', then predict the sentiment of food based on the sentiment word ``Great'' in the node ``Great taste''. SCAN excludes the blue node ``Great taste bad service.'' for food, because it can predict not only food but also service.  The main contributions of our work can be summarized as follows:    
"," Aspect category sentiment analysis  aims to predict the sentiment polarities of the aspect categories discussed in sentences. Since a sentence usually discusses one or more aspect categories and expresses different sentiments toward them, various attention-based methods have been developed to allocate the appropriate sentiment words for the given aspect category and obtain promising results. However, most of these methods directly use the given aspect category to find the aspect category-related sentiment words, which may cause mismatching between the sentiment words and the aspect categories when an unrelated sentiment word is semantically meaningful for the given aspect category. To mitigate this problem, we propose a Sentence Constituent-Aware Network  for aspect-category sentiment analysis. SCAN contains two graph attention modules and an interactive loss function. The graph attention modules generate representations of the nodes in sentence constituency parse trees for the aspect category detection  task and the ACSA task, respectively. ACD aims to detect aspect categories discussed in sentences and is a auxiliary task. For a given aspect category, the interactive loss function helps the ACD task to find the nodes which can predict the aspect category but can闁炽儲鐛 predict other aspect categories. The sentiment words in the nodes then are used to predict the sentiment polarity of the aspect category by the ACSA task. The experimental results on five public datasets demonstrate the effectiveness of SCAN. \footnote{Data and code can be found at https://github.com/l294265421/SCAN}  \keywords{Aspect Category Sentiment Analysis  \and Aspect Based Sentiment Analysis \and Graph Attention Network.}",5
"  With the rapid development of e-commerce, online reviews written by  users  have become increasingly important for reflecting real customer experiences. To ease the process of review writing, the task of personalized review generation~ has been proposed to automatically produce review text conditioned on necessary context data, \eg users, items, and ratings.  As a mainstream solution, RNN-based models  have been widely applied to the PRG task. Standard RNN models mainly model sequential dependency among tokens,  which cannot effectively generate high-quality review text. Many efforts have been devoted to improving this kind of architecture for the PRG task, including context utilization,  long text generation, and  writing style enrichment. These studies have improved the performance of the PRG task to some extent. However, two major issues still remain to be solved. First, the generated text is likely  to be uninformative, lacking factual description on product information. Although several studies try to incorporate structural or semantic features ,  they mainly extract such features from the review text.   Using review data alone, it is difficult to fully capture diverse and comprehensive facts from unstructured text. Second, most of these studies focus on word-level generation, which makes it difficult to directly model  user preference at a higher level. For example, given a product, a user may focus on the price, while another user may emphasize the look.  To address these issues, we propose to improve the PRG task with external knowledge graph . By associating online items with KG entities, we are able to obtain rich attribute or feature information for items, which is potentially useful for the PRG task. Although the idea is intuitive, it is not easy to fully utilize the knowledge information for generating review text in our task. KG typically organizes facts as triples, describing the relation between two involved entities. It may not be suitable to simply integrate KG information to enhance text representations or capture user preference due to varying intrinsic characteristics of different data signals.  In order to bridge the semantic gap, we augment the original KG with user and word nodes, and construct a heterogeneous knowledge graph  by adding user-item links and entity-word links. User-item links are formed according to user-item interactions, and entity-word links are formed according to their co-occurrence in review sentences. We seek to learn a unified semantic space that is able to encode different kinds of nodes. Figure presents an illustrative example for the HKG. Given such a graph, we focus on two kinds of useful information for the PRG task. First, the associated facts regarding to an item  can be incorporated to enrich the review content. Second, considering users as target nodes, we can utilize this graph to infer users' preference  on some specific relation or aspect . The two kinds of information reflect word- and aspect-level enrichment, respectively. To utilize the semantics at the two levels, we decompose  the review generation process into two stages, namely aspect sequence generation and sentence generation.  We aim to inject multi-granularity KG information in different generation stages for improving the PRG task.     To this end, in this paper, we propose a KG-enhanced personalized review generation model based on capsule graph neural networks~. Compared with most of existing GNN-based methods representing graphs as individual scalar features, Caps-GNN can extract underlying characteristics of graphs as capsules at the graph level through the dynamic routing mechanism and each capsule reflects the graph properties in different aspects. Based on the constructed HKG, we utilize Caps-GNN to extract graph properties in different aspects as graph capsules, which may be helpful to infer aspect- and word-level user preference. For aspect sequence generation, we propose a novel adaptive learning algorithm that is able to capture personalized user preference at the aspect level, called aspect capsules, from the graph capsules.  We associate an aspect capsule with a unique aspect from unsupervised topic models.   Furthermore, for the generation of sentences, we utilize the learned aspect capsules to capture personalized user preference at the word level. Specially, we design a graph-based copy mechanism to generate related entities or words by copying them from the HKG, which can enrich the review contents.  In this way, KG information has been effectively utilized  at both aspect and word levels in our model.   %To our knowledge, we are the first to utilize knowledge graph to generate personalized review text, which is able to capture both aspect- and word-level KG semantics for learning user preference.  To our knowledge, we are the first to utilize KG to capture both aspect- and word-level user preference for generating personalized review text. For evaluation, we constructed three review datasets by associating items with KG entities. Extensive experiments  demonstrate the effectiveness of KG information and our model. %%Our code and dataset will be released after the review period.       
"," Personalized review generation  aims to automatically produce review text reflecting user preference, which is a challenging natural language generation task. Most of previous studies do not explicitly model  factual description of products, tending to generate uninformative content. Moreover, they mainly focus on word-level generation, but cannot accurately reflect more abstractive  user preference in multiple aspects.  To address the above issues, we propose a novel knowledge-enhanced PRG model  based on capsule graph neural network~. We first  construct a heterogeneous knowledge graph  for utilizing rich item attributes. We adopt  Caps-GNN to learn graph capsules for encoding underlying characteristics from the HKG. Our generation process contains two major steps, namely aspect sequence generation and sentence generation. First, based on graph capsules, we adaptively learn aspect capsules for inferring the aspect sequence.   Then, conditioned on the inferred aspect label, we design a graph-based copy mechanism to generate sentences by incorporating related entities or words from HKG. To our knowledge, we are the first to utilize knowledge graph for the PRG task. The incorporated KG information is able to enhance user preference at both aspect and word levels. Extensive experiments on three real-world datasets have demonstrated the effectiveness of our model on the PRG task.",6
" % significance of sentence functions for dialog Humans express intentions in conversations through sentence functions, such as interrogation for acquiring further information, declaration for making statements, and imperative for making requests and instructions. For machines to interact with humans, it is therefore essential to enable them to make use of sentence functions for dialogue generation. Sentence function is an important linguistic feature indicating the communicative purpose of a sentence in a conversation. There are four major sentence functions: Declarative, Interrogative, Exclamatory and Imperative . Each major sentence function can be further decomposed into fine-grained ones according to different purposes indicated in conversations. For example, Interrogative is divided into Wh-style Interrogative, Yes-no Interrogative and other types. These fine-grained sentence functions have great influences on the structures of utterances in conversations including word orders, syntactic patterns, and other aspects . Figure  presents how sentence functions influence the responses.  Given the same query expressed in Positive Declarative, the responses expressed in Wh-style Interrogative and in Negative Declarative are completely different.    % challenges, quantitatively list some numbers Although the use of sentence functions improves the overall quality of generated responses , it suffers from the data imbalance issue. For example, in the recently released response generation dataset with manually annotated sentence functions STC-SeFun , more than 40\% of utterances are Positive Declarative while utterances annotated with Declarative with Interrogative words account for less than 1\%  of the entire dataset. Therefore, dialogue generation models suffer from data deficiency for these infrequent sentence functions.  % proposed method Recently, model-agnostic meta-learning ~  has shown promising results on several low-resource natural language generation  tasks, including neural machine translation , personalized response generation  and domain-adaptive dialogue generation . They treat languages of translation, personas of dialog and dialog domains as separate tasks in MAML respectively. In the same spirit of previous works, we first treat dialogue generation conditioned on different sentence functions as separate tasks, and meta-train a dialogue generation model using high-resource sentence functions. Moreover, we observe that sentence functions have hierarchical structures: four major sentence functions can be further divided into twenty fine-grained types. Some fine-grained sentence functions may share some similarities while some others are disparate. For example, utterances belong to Wh-style Interrogative and Yes-no Interrogative may share some transferable word patterns while utterances in Wh-style Interrogative and in Exclamatory with interjections totally differ from each other.  Motivated by this observation, we explore a structured meta-learning  considering inherent structures among fine-grained sentence functions. Inspired from recent advances on learning several initializations with a set of meta-learners , we develop our own approach to utilize the underlying structure of sentence functions. More specifically, our proposed SML explicitly tailors transferable knowledge among different sentence functions. It utilizes the learned representations of fine-grained sentence functions as parameter gates to influence the globally shared parameter initialization. Therefore, conversation models for similar sentence functions can share similar parameter initializations and vice versa. As a result, SML enhances meta-learning effectiveness by promoting knowledge customization among different sentence functions but simultaneously preserving knowledge generalization for similar sentence functions.  % experiments The experimental results on STC-SeFun dataset  show that responses generated from our proposed structured meta-learning algorithm are of better quality over several baselines in both human and automatic evaluations.  Moreover, our proposed model can generate responses consistent with the target sentence functions while baseline models may ignore the target sentence functions or generate some generic responses. We further conduct a detailed analysis on our proposed model and show that it indeed can learn word orders and syntactic patterns for different fine-grained sentence functions.     
"," Sentence function is an important linguistic feature indicating the communicative purpose in uttering a sentence. Incorporating sentence functions into conversations has shown improvements in the quality of generated responses. However, the number of utterances for different types of fine-grained sentence functions is extremely imbalanced. Besides a small number of high-resource sentence functions, a large portion of sentence functions is infrequent. Consequently, dialogue generation conditioned on these infrequent sentence functions suffers from data deficiency. In this paper, we investigate a structured meta-learning  approach for dialogue generation on infrequent sentence functions. We treat dialogue generation conditioned on different sentence functions as separate tasks, and apply model-agnostic meta-learning to high-resource sentence functions data.  Furthermore, SML enhances meta-learning effectiveness by promoting knowledge customization among different sentence functions but simultaneously preserving knowledge generalization for similar sentence functions.  Experimental results demonstrate that SML not only improves the informativeness and relevance of generated responses, but also can generate responses consistent with the target sentence functions.",7
" As mentioned in Chapter , models trained simply to obtain a high accuracy on held-out sets can often learn to rely on shallow input statistics, resulting in brittle models. % susceptible to adversarial attacks. For example, \citet{lime} present a document classifier that distinguishes between Christianity and Atheism with a test accuracy of . However, on close inspection, the model spuriously separates classes based on words contained in the headers, such as ``Posting'', ``Host'', and ``Re''.  Spurious correlations in both training and test sets allow for such undesired models to obtain high accuracies. Much more complex hidden correlations may be present in any arbitrarily large and human-annotated dataset . Such correlations may be difficult to spot, and even when one identifies them, it is an open question how to mitigate them .   In this chapter, I investigate a direction that has the potential to both steer neural models away from relying on spurious correlations and provide explanations for the predictions of these models. This direction is that of enhancing neural models with the capability to learn from natural language explanations during training time and to generate such explanations at test time. For humans, it has been shown that explanations play a key role in structuring conceptual representations for categorisation and generalisation . Humans also benefit tremendously from reading explanations before acting in an environment for the first time . Thus, explanations may also be used to set a model in a better initial position to further learn the correct functionality. Meanwhile, at test time, generating correct argumentation in addition to obtaining a high accuracy has the potential to endow a model with a higher level of transparency and trust.     %In this work, we introduce a new dataset and models for exploiting and generating explanations for the task of recognizing textual entailment.  Incorporating external knowledge into a neural model was shown to result in more robust models . % show that models achieving high accuracies on SNLI, such as , show dramatically reduced performance on this simpler dataset, while the model of \citet{kim} is more robust due to incorporating external knowledge.  Free-form natural language explanations are a form of external knowledge that has the following advantages over formal language. First, it is easy for humans to provide free-form language, eliminating the additional effort of learning to produce formal language, thus making it simpler to collect such datasets. Secondly, natural language explanations might potentially be mined from existing large-scale free-form text. Finally, natural language is readily comprehensible to an end-user who needs to assert the reliability of a model.  %Thirdly, the formal languages chosen by researchers may differ from work to work and therefore models constructed over one formal language might not be trivially transferred to another. Meanwhile free-form explanations are generic and applicable to diverse areas of research, such as natural language processing, computer vision, or policy learning.   Despite the potential for natural language explanations to improve both learning and transparency, there is a scarcity of such datasets in the community, as discussed in Section .  To address this deficiency, I collected a large corpus of K human-annotated explanations for the SNLI dataset~. I chose SNLI because it constitutes an influential corpus for natural language understanding that requires deep assimilation of fine-grained nuances of commonsense knowledge. %A plethora of models have been developed on this dataset, including previous state-of-the-art in universal sentence representations , which demonstrates the power of this task and dataset. I call this explanation-augmented dataset e-SNLI, which I release publicly\footnote{The dataset can be found at \url{https://github.com/OanaMariaCamburu/e-SNLI}.} to advance research in the direction of training with and generation of free-form natural language explanations.    %To demonstrate the efficacy of the e-SNLI dataset,  %I show that it is much more difficult for neural models to produce correct natural language explanations based on spurious correlations than it is to produce correct labels. Further, I develop models that predict a label and generate an explanation for their prediction. I also investigate how the presence of natural language explanations at training time can guide neural models into learning better universal sentence representations  and into having better capabilities to solve out-of-domain instances.  Secondly, I show that it is much more difficult for a neural model to produce correct natural language explanations based on spurious correlations than it is for it to produce correct labels based on such correlations.   Thirdly, I develop models that predict a label and generate an explanation for their prediction, and I investigate the correctness of the generated explanations.   Finally, I investigate whether training a neural model with natural language explanations can result in better universal sentence representations produced by this model and in better performance on out-of-domain datasets.   \paragraph{Remark.} In this chapter, I use the concept of correct explanation to refer to the correct argumentation for the ground-truth label on an instance.  This should not be confused with the concept of faithful explanation, which refers to the accuracy with which an explanation describes the decision-making process of a model, as described in Section .  The capability of a neural model to generate correct explanations is an important aspect of the development of such models.  For example, correct argumentation may sometimes be needed in practice, alongside the correct final answer. Hence, in this chapter, I inspect the correctness of the explanations generated by the introduced neural models. In the next chapter, I will take a step towards verifying the faithfulness of these explanations.% is given in Chapter .   
","  Deep neural networks are becoming more and more popular due to their revolutionary success in diverse areas, such as computer vision, natural language processing, and speech recognition. However, the decision-making processes of these models are generally not interpretable to users. In various domains, such as healthcare, finance, or law, it is critical to know the reasons behind a decision made by an artificial intelligence system. Therefore, several directions for explaining neural models have recently been explored.   In this thesis, I investigate two major directions for explaining deep neural networks. The first direction consists of feature-based post-hoc explanatory methods, that is, methods that aim to explain an already trained and fixed model , and that provide explanations in terms of input features, such as tokens for text and superpixels for images . The second direction consists of self-explanatory neural models that generate natural language explanations, that is, models that have a built-in module that generates explanations for the predictions of the model. The contributions in these directions are as follows.   % In this thesis, I investigate the topic of explaining deep neural networks. This topic is crucial nowadays as neural model are becoming more and more employed in real-world applications due to their high performance in diverse areas, such as computer vision, natural language processing, and speech recognition. However, the decision-making processes learned by these models are not generally human-interpretable. In various real-world applications, such as healthcare, finance, or criminal justice, it is critical to know the reasons behind a decision made by an artificial intelligence system. Therefore, several directions for explaining neural models have recently been explored.   % a series of methods have recently been developed to provide explanations for the predictions of neural models. This thesis brings contributions to two major directions for explaining deep neural networks: feature-based post-hoc explanatory methods and self-explanatory neural models that generate natural language explanations for their predictions. The contributions are as follows.   %However, it is still an open question how to verify whether the explanations provided by these methods are faithfully describing the decision-making processes of the models that they aim to explain. Secondly, it is also an open question whether neural networks can learn from human-provided natural language explanations for the ground-truth labels at training time, as well as support their predictions with natural language explanations at test time, just like humans do.    First, I reveal certain difficulties of explaining even trivial models using only input features. I show that, despite the apparent implicit assumption that explanatory methods should look for one specific ground-truth feature-based explanation, there is often more than one such explanation for a prediction. I also show that two prevalent classes of explanatory methods target different types of ground-truth explanations without explicitly mentioning it. Moreover, I show that, sometimes, neither of these explanations is enough to provide a complete view of a decision-making process on an instance. %These findings can have an important impact on how users choose explanatory methods to best suit their needs.    Second, I introduce a framework for automatically verifying the faithfulness with which feature-based post-hoc explanatory methods describe the decision-making processes of the models that they aim to explain. This framework relies on the use of a particular type of model that is expected to provide insight into its decision-making process. I analyse potential limitations of this approach and introduce ways to alleviate them.  % The introduced verification framework is generic and can be instantiated on different tasks and domains to provide off-the-shelf sanity tests that can be used to test feature-based post-hoc explanatory methods. I instantiate this framework on a task of sentiment analysis and provide sanity tests\footnote{The sanity tests are available at \\ \url{https://github.com/OanaMariaCamburu/CanITrustTheExplainer}.} %to test any feature-based post-hoc explanatory method. Furthermore,  on which I present the performances of three popular explanatory methods. %The results show that these methods may provide unfaithful explanations.  %I also discuss ways in which the current limitations of the framework can further be addressed to lead to more robust and flexible verifications.    %In the process of developing this framework, I uncover several ways in which a particular type of model that is expected to provide insight into its decision-making process can provide misleading such insight. I also introduce checks that can be done to account for this misleading insight in order to use this type of model in the proposed framework.  % %%%%%%%% BEFORE %%%%%%%%%%The framework is generic and can be instantiated on different tasks and domains. I instantiate it on a task of sentiment analysis and provide sanity tests that can be used off-the-shelf\footnote{The tests are available at \url{https://github.com/OanaMariaCamburu/CanITrustTheExplainer}.} to test any feature-based post-hoc explanatory method. Furthermore, I present preliminary results of three explanatory methods on these tests, which raise awareness of the unfaithful explanations that these methods may provide. %I discuss ways in which the limitations of this verification framework can further be addressed and open the path towards more robust and flexible verification frameworks that can be adapted to users' needs.  %%%% this framework relies on the use of a particular type of model that is expected to provide insight into its decision-making process. I analyse the potential limitations of this approach and introduce ways to overcome them. By constructions   %In addition, as a step towards addressing the question of verifying if explanatory methods faithfully describe the decision-making processes learned by the models they aim to explain, I investigate a particular type of self-explanatory neural model and I show three ways in which this type of model can provide misleading explanations. % on its decision-making process.    %Secondly, I present a novel verification framework that can generate a multitude of sanity tests for explanatory methods. I instantiate this framework on the task of sentiment analysis and provide three sanity tests, which can be used off-the-shelf.\footnote{The tests are available at \url{https://github.com/OanaMariaCamburu/CanITrustTheExplainer}.} I present the results of three explanatory methods on these tests. I discuss ways in which the limitations of this verification framework can further be addressed and open the path towards more robust and flexible verification frameworks that can be adapted to users' needs.  % improve their behaviour and performance %exhibit improved behaviour  % if they are additionally given natural language explanations for the ground-truth label at training time  Third, to explore the direction of self-explanatory neural models that generate natural language explanations for their predictions, I collected a large dataset of $\sim\!\!570$K human-written natural language explanations on top of the influential Stanford Natural Language Inference  dataset. I call this explanation-augmented dataset e-SNLI.\footnote{The dataset is publicly available at \url{https://github.com/OanaMariaCamburu/e-SNLI}.} %, which I release publicly\footnote{The dataset is available at \url{https://github.com/OanaMariaCamburu/e-SNLI}.} %to advance research in the direction of training with and generation of natural language explanations.  % Further, I provide empirical evidence that models generating correct explanations are more reliable than models that just predict the correct labels.  % I also train different neural models that generate natural language explanations at test time, and I measure the success of these models to generate correct explanations. I also investigate whether the presence of natural language explanations at training time can lead a model to produce better universal sentence representations and to perform better on out-of-domain datasets. I do a series of experiments that investigate both the capabilities of neural models to generate correct natural language explanations at test time, and the benefits of providing natural language explanations at training time.  Fourth, I show that current self-explanatory models that generate natural language explanations for their own predictions may generate inconsistent explanations, such as ``There is a dog in the image.'' and ``There is no dog in the [same] image.''. Inconsistent explanations reveal either that the explanations are not faithfully describing the decision-making process of the model or that the model learned a flawed decision-making process.  I introduce a simple yet effective adversarial framework for sanity checking models against the generation of inconsistent natural language explanations. Moreover, as part of the framework, I address the problem of adversarial attacks with exact target sequences, a scenario that was not previously addressed in sequence-to-sequence attacks, and which can be useful for other tasks in natural language processing. I apply the framework on a state of the art neural model on e-SNLI and show that this model can generate a significant number of inconsistencies.  This work paves the way for obtaining more robust neural models accompanied by faithful explanations for their predictions.  %My hope is that in the future feature-based post-hoc explanatory methods will be superseded  by robust and accurate neural models that faithfully explain themselves to their human users in natural language.",8
"  Aspect-based sentiment analysis , also termed as Target-based Sentiment Analysis in some literature, is a fine-grained sentiment analysis task. It is usually formulated as detecting aspect terms and sentiments expressed in a sentence towards the aspects. This type of formulation is referred to as aspect-sentiment pair extraction.   Meanwhile, there exists another type of approach to ABSA,  referred to as aspect-opinion co-extraction, which focuses on jointly deriving aspect terms  and opinion terms  from sentences, yet without figuring out their sentiment dependencies. The compelling performances of both directions illustrate a strong dependency between aspect terms, opinion terms and the expressed sentiments.    This motivates us to put forward a new perspective for ABSA as joint extraction of aspect terms, opinion terms and sentiment polarities,\footnote{For simplicity, these four concepts are hereafter referred to as aspect, opinion, sentiment, and triplet, respectively.} in short opinion triplet extraction. An illustrative example of differences among aspect-sentiment pair extraction, aspect-opinion co-extraction, and opinion triplet extraction is given in Figure. Opinion triplet extraction can be viewed as an integration of aspect-sentiment pair extraction and aspect-opinion co-extraction, by taking into consideration their complementary nature. It brings in two-fold advantages:  the opinions can boost the expressive power of models and help better determine aspect-oriented sentiments;  the sentiment dependencies between aspects and opinions can bridge the gap of how sentiment decisions are made and further promote interpretability of models.   There is some prior research with a similar viewpoint.  proposes to extract opinion tuples, i.e., s,\footnote{To some extent, opinion triplet extraction aims at solving the same task  as they does regardless of the minor difference.} by first jointly extracting aspect-sentiment pairs and opinions by two sequence taggers, in which sentiments are attached to aspects via unified tags,\footnote{An aspect tag set \{, , \} and a sentiment tag set \{, , \} are unified into the aspect-sentiment tag set \{, , , , , , \}. Here, , , and  indicate begin, inside, and outside of a span. And , , and  are neutral, negative, and positive.} and then pairing the extracted aspect-sentiments and opinions by an additional classifier. Despite of remarkable performance the approach has achieved, two issues need to be addressed.  The first issue arises from the prediction of aspects and sentiments with a set of unified tags thus degrading the sentiment dependency parsing process to a binary classification. As is discussed in prior studies on aspect-sentiment pair extraction, although the concerned framework with unified tagging scheme is theoretically  elegant and mitigates the computational cost, it is insufficient to model the interaction between the aspects and sentiments.  Secondly, the coupled aspect-sentiment formalization disregards the importance of their interaction with opinions. Such interaction has been shown important to handle the overlapping circumstances where different triplet patterns share certain elements, in other triplet extraction-based tasks such as relation extraction. To show why triplet interaction modelling is crucial, we divide triplets into three categories, i.e., aspect overlapped, opinion overlapped, and normal ones. Examples of these three kinds of triplets are shown in Figure. We can observe that two triplets tend to have the same sentiment if they share the same aspect or opinion. Hence, modelling triplet interaction shall benefit the ASBA task, yet it can not be explored with the unified aspect-sentiment tags in which sentiments have been attached to aspects without considering the overlapping cases.    To circumvent the above issues, we propose a multi-task learning framework for opinion triplet extraction, namely OTE-MTL, to jointly detect aspects, opinions, and sentiment dependencies. On one hand, the aspects and opinions can be extracted with two independent heads in the multi-head architecture we propose. On the other hand, we decouple sentiment prediction from aspect extraction. Instead, we employ a sentiment dependency parser as the third head, to predict word-level sentiment dependencies, which will be utilized to further decode span-level\footnote{The aspects and opinions are usually spans over several words in the sentence} dependencies when incorporated with the detected aspects and opinions. In doing so, we expect to alleviate issues brought by the unified tagging scheme. Specifically, we exploit sequence tagging strategies for extraction of aspects and opinions, whilst taking advantage of a biaffine scorer to obtain word-level sentiment dependencies. Additionally, since these task-heads are jointly trained, the learning objectives of aspect and opinion extraction could be considered as regularization applied on the sentiment dependency parser. In this way, the parser is learned with aspect- and opinion-aware constraints, therefore fulfilling the demand of triplet interaction modelling. Intuitively, if we are provided with a sentence containing two aspects but only one opinion , we can identify triplets with overlapped opinion thereby.  Extensive experiments are carried out on four SemEval benckmarking data collections for ABSA. Our framework are compared with a range of state-of-the-art approaches. The results demonstrate the effectiveness of our overall framework and individual components within it. A further case study shows that how our model better handles overlapping cases.
"," The state-of-the-art Aspect-based Sentiment Analysis  approaches are mainly based on either detecting aspect terms and their corresponding sentiment polarities, or co-extracting aspect and opinion terms. However, the extraction of aspect-sentiment pairs lacks opinion terms as a reference, while co-extraction of aspect and opinion terms would not lead to meaningful pairs without determining their sentiment dependencies. To address the issue, we present a novel view of ABSA as an opinion triplet extraction task, and propose a multi-task learning framework to jointly extract aspect terms and opinion terms, and simultaneously parses sentiment dependencies between them with a biaffine scorer. At inference phase, the extraction of triplets is facilitated by a triplet decoding method based on the above outputs. We evaluate the proposed framework on four SemEval benchmarks for ASBA. The results demonstrate that our approach significantly outperforms a range of strong baselines and state-of-the-art approaches.\footnote{Code and datasets for reproduction are available at \href{https://github.com/GeneZC/OTE-MTL}{\texttt{https://github.com/GeneZC/OTE-MTL}}.}",9
"  We use a sequence of vectors to represent a sentence, where each vector consists of  a semantic-role  tag, a part-of-speech  tag, and other syntactic and semantic tags,  and we refer to such a sequence as a \textsl{meta sequence}.  We present an application using meta-sequence learning to generate, on a given article,  adequate QAPs to form multiple-choice questions.  In particular, we develop a scheme called MetaQA to learn meta sequences  of declarative sentences and the corresponding interrogative sentences from a training dataset. % consisting of such sentences. Combining and removing redundant meta sequences yields a set called MSDIP  , with each element being a pair of an MD and corresponding MI, where MD and MI stand for, respectively, a meta sequence for a declarative sentence and for an interrogative sentence. A trained MetaQA model generates QAPs for a given declarative sentence  as follows: Generate a meta sequence for , find a best-matched MD from MSDIP, generates meta sequences for interrogative sentences according to the corresponding MIs and the meta sequence of , identifies the meta-sequence answer to each MI, and coverts them back to text to form a QAP.    \begin{comment} 
"," %Creating multiple-choice questions to assess reading comprehension of a given article %involves generating question-answer pairs  on the main points of the document. We present a meta-sequence representation of sentences and demonstrate how to use meta-sequence learning to generate adequate question-answer pairs  over a given article. %learning scheme to generate adequate QAPs  %via meta-sequence representations of sentences.   %without handcrafted features.  A meta sequence is a sequence of vectors of semantic and syntactic tags. %In particular, %we devise a scheme called MetaQA to %learn meta sequences from training data to form  %pairs of a meta sequence for a declarative sentence   %and a corresponding  interrogative sentences . % indexed for fast retrieval,  On a given declarative sentence, a trained model  converts it to a meta sequence,  finds a matched meta sequence in its learned database,  and   uses the corresponding meta sequence for interrogative sentence to generate QAPs. %We implement MetaQA for the English language using  %semantic-role labeling,  %part-of-speech tagging, and  named-entity recognition, We show that, trained on a small dataset,  our method generates efficiently, on the official SAT practice reading tests, a large number of syntactically and semantically correct QAPs with high accuracy.",10
" In recent years, there has been a revolution in machine learning-based program synthesis techniques for automatically generating programs from high-level expressions of user intent, such as input-output examples~ and natural language~. Many of these techniques use deep neural networks to consume specifications and then perform model-guided search to find a program %---often in some domain-specific language--- that satisfies the user. However, because  %both natural language and input examples  the user's specification  can be inherently ambiguous~, a recent thread of work on multimodal synthesis attempts to combine different types of cues, such as natural language and examples, to allow program synthesis to effectively scale to more complex problems. Critically, this setting introduces a new challenge: how do we efficiently synthesize programs with a combination of hard and soft constraints from distinct sources?  In this paper, we formulate multimodal synthesis as an optimal synthesis task and propose an optimal synthesis algorithm to solve it. % In this paper, we cast multimodal synthesis as a type of optimal synthesis problem where the goal  The goal of optimal synthesis is to generate a program that satisfies any hard constraints provided by the user while also maximizing the score under a learned neural network model that captures noisy information, like that from natural language. % This problem, which we refer to as optimal neural synthesis, is important In practice, there are many programs that satisfy the hard constraints, so this maximization is crucial to finding the program that actually meets the user's expectations: if our neural model is well-calibrated, a program that maximizes the score under the neural model is more likely to be the user's intended program.  % The key technical contribution of this paper is a new optimal neural synthesis algorithm in contexts where the user guidance includes a combination of natural language and examples.  Our optimal neural synthesis algorithm takes as input multimodal user guidance. In our setting, we train a neural model to take natural language input that can be used to guide the search for a program consistent with some user-provided examples. Because our search procedure enumerates programs according to their score, the first enumerated program satisfying the examples is guaranteed to be optimal according to the model. A central feature of our approach is the use of a tree-structured neural model, namely the abstract syntax network ~, for constructing syntactically valid programs in a top-down manner. The structure of the ASN model restricts search to programs that are syntactically correct, thereby avoiding the need to deal with program syntax errors~, and it allows us to search over programs in a flexible way, without constraining a left-to-right generation order like seq2seq models do. More importantly, the use of top-down search allows us to more effectively leverage automated program analysis techniques for proving infeasibility of partial ASTs. As a result, our synthesizer can prune the search space more aggressively than prior work and significantly speed up search. While our network structure and pruning techique are adapted from prior work, we combine them and generalize them to this optimal neural synthesis setting in a new way, and we show that our general approach leads to substantial improvements over previous synthesis approaches.    We  implement our method in a synthesizer called \toolname%\footnote{MultIModal Optimal Synthesis with Asn} and evaluate it on the challenging {\sc StructuredRegex} dataset~ for synthesizing regular expressions from linguistically diverse natural language descriptions and positive/negative examples. We compare our approach against a range of approaches from prior work and ablations of our own method.  \toolname\ achieves substantial gain over past work by solving 59.8\%  of the programs of Test  set in \streg{} by exploring on average 560  states, which surpasses previous state-of-the-art by 11.7\%  with   fewer states.    % 
"," Multimodal program synthesis, which leverages different types of user input to synthesize a desired program, is an attractive way to scale program synthesis to challenging settings; however, it requires integrating noisy signals from the user  with hard constraints on the program's behavior. This paper proposes an optimal neural synthesis approach where the goal is to find a program that satisfies user-provided constraints while also maximizing the program's score with respect to a neural model. Specifically, we focus on multimodal synthesis tasks in which the user intent is expressed using combination of natural language  and input-output examples. At the core of our method is a top-down recurrent neural model that places distributions over abstract syntax trees conditioned on the NL input. This model not only allows for efficient search over the space of syntactically valid programs, but it allows us to leverage automated program analysis techniques for pruning the search space based on infeasibility of partial programs with respect to the user's constraints. The experimental results on a multimodal synthesis dataset  show that our method substantially outperforms prior state-of-the-art techniques in terms of accuracy %, finds model-optimal programs more frequently, and explores fewer states during search.",11
"  The desire for human-like interfaces to technical systems, as evidenced by growing use of intelligent assistants, belies the need for conversational AI systems that can accomplish a wide range of tasks, such as booking restaurants, trains, and flights, IT help desk and accessing financial accounts and transaction records. The wide range of tasks have necessitated the need for a flexible and scalable dialogue system that can support a variety of use cases with minimal development and maintenance effort. Existing dialogue systems are broken into two major categories,  open-domain dialogue systems, which focus on non-task related conversations, and task-oriented dialogue systems, which focus on user task completion. A typical open-domain system uses an end-to-end neural architecture often trained with input and output utterances from human-to-human conversations . While open-domain systems are optimized for engaging in human-like conversation, they lack any inherent ability to interface with any other systems on behalf of their conversation partner. Whereas, a typical task-oriented dialogue system seeks to understand human intents and execute them. This is done by adopting a modularized pipeline architecture with three modules that are sequentially connected as shown in Fig. . A natural language understanding  module that recognizes user intents and extract useful entity information . The dialogue management  module contains two submodules, the dialogue state tracker  and the dialogue action policy  modules. The DST module tracks the mapping of entities to slots that are relevant or required for completing user tasks . The POL module decides which actions to execute via the API. Finally, the natural language generation  module generates the user response based on the user aspects of the system actions . In some cases, multiple modules are combined together, e.g. systems with a composite NLU and DST module , and systems with a composite POL and NLG module that maps previous utterances and dialogue states to the system response .  Despite research advances in modular neural approaches, they are hardly used in practice. Industrial dialogue systems, though modularized, still use expensive expert driven rule-based heuristics implemented with several lines of codes and hand-crafted templates, and therefore difficult to scale as the number of use cases grows. More recently, there has been a renewed effort to apply a single end-to-end neural architecture  to model task-oriented dialogue with the use of autoregressive transformer architecture . This has led to the reformulation of dialogue system design as a text generation or sequence modeling task. While some of these efforts have obtained state-of-the-art performance on publicly available task-oriented dialogue datasets, there is still room for improvement, especially in the areas of generality and practicality. First, their problem formulation fails to reconcile open-domain and task-oriented dialogue in the same model architecture. Also, in many cases, they do not address the complexity of the action policy especially towards the back-end API system. Finally, they don't fully incorporate the control, verification and explanation capabilities that make modularized approaches attractive.  To resolve these shortcomings, we propose DLGNet-Task, an end-to-end neural network that simultaneously handles both open-domain and task-oriented dialogue, in such a way that the model outputs are controllable, verifiable, and explainable at the module level. This system is compatible with both data driven and expert driven rule-based approaches.   That is, our approach is simultaneously modular and end-to-end, and can be a drop-in replacement for traditional modular task-oriented dialogue  systems. To the best of our knowledge, this is the most expressive approach to date in achieving this objective. In summary, we are able to model the individual behavior of NLU, DM and NLG components with a single neural network model trained end-to-end. Still, the model is flexible enough to allow individual modules to be separately trained and validated in line with the traditional TOD system.  % Validation at module level can provide information about where additional training is needed. It could also help in balancing the contribution of each module if the model is finetuned with module-level objectives.  % The DLGNet-Task model is based on the autoregressive transformer architecture similar to DLGNet  and GPT-2/3  models. To evaluate the performance of DLGNet-Task, we trained the model with just the system-level training objective on a modified MultiWoz2.1 dataset. The dataset modification is done mainly to support DLGNet-Task design framework . Based on the widely used TOD metrics, such as inform rate, success rate, and BLEU score , our experiments show that DLGNet-Task produces a comparable performance to the state-of-the-art approaches on the MultiWoz2.1 dataset.  % in addition to the controllable, verifiable, and explainable model's intermediate outputs.    
"," Task oriented dialogue  requires the complex interleaving of a number of individually controllable components with strong guarantees for explainability and verifiability. This has made it difficult to adopt the multi-turn multi-domain dialogue generation capabilities of streamlined end-to-end open-domain dialogue systems. In this paper, we present a new framework, DLGNet-Task, a unified task-oriented dialogue system which employs autoregressive transformer networks such as DLGNet and GPT-2/3 to complete user tasks in multi-turn multi-domain conversations. Our framework enjoys the controllable, verifiable, and explainable outputs of modular approaches, and the low development, deployment and maintenance cost of end-to-end systems. Treating open-domain system components as additional TOD system modules allows DLGNet-Task to learn the joint distribution of the inputs and outputs of all the functional blocks of existing modular approaches such as, natural language understanding , state tracking, action policy, as well as natural language generation . Rather than training the modules individually, as is common in real-world systems, we trained them jointly  with appropriate module separations. When evaluated on the MultiWOZ2.1 dataset, DLGNet-Task shows comparable performance to the existing state-of-the-art approaches. Furthermore, using DLGNet-Task in conversational AI systems reduces the level of effort required for developing, deploying, and maintaining intelligent assistants at scale.  % significant improvement over existing approaches, and achieves state-of-the-art performance at both the module and system levels.",12
"   Knowledge graphs  represent knowledge of the world as relationships between entities, i.e., triples with the form  . Such knowledge resource provides clean and structured evidence for many downstream applications such as question answering. KGs are usually constructed by human experts, which is time-consuming and leads to highly incomplete graphs . Therefore automatic KG completion  is proposed to infer a missing link of relationship  between a head entity  and a tail entity .    Existing KG completion work mainly makes use of two types of information: 1) co-occurrence of entities and relations and 2) deducible reasoning paths of tuples. KG embeddings encode entities and relations, the first type of information, together into continuous vector space with low-rank tensor approximations~.  Ours approach utilizes the second type of information, reasoning path of tuples that can be deduced to the target tuple~. Here a reasoning path starts with the head entity  and ends with the tail entity \e{t}: \e{h \overset{r_1}{\rightarrow} e_1  \overset{r_k}{\rightarrow} e_k \overset{r_N}{\rightarrow} t}, where \e{r_1 \wedge ... \wedge r_N} forms a relation chain that infers the existence of . Therefore these methods are also referred as multi-hop reasoning over KGs, which learns a multi-hop chain as a rule to deduce the target . An example of such a chain is given in Figurea to infer whether an athlete plays in an location. Multi-hop reasoning approaches can usually utilize richer evidence and self-justifiable in terms of  reasoning path rules used in the predictions, making the prediction of missing relations more interpretable.   Despite  advantages and  success of the multi-hop reasoning approach , a target relationship may not be perfectly inferred from a single relation chain. There could exist multiple weak relation chains that correlate with the target relation. Figure gives examples of such cases.  These multiple chains could be leveraged in following ways:  the reasoning process naturally relies on the logic conjunction of multiple chains ;  more commonly, there are instances for which none of the chains is accurate, but aggregating multiple pieces of evidence improves the confidence , as also observed in the case-based study works. Inspired by these observations, we propose the concept of  multi-chain multi-hop rule set.  Here, instead of treating each single multi-hop chain as a rule, we learn rules consisting of a small set of multi-hop chains. Therefore the inference of target relationships becomes a joint scoring of such  a set of chains. {We  treat each set of chains as one rule and, since different query pairs can follow different rules, together we have  a set of rules to reason each relation.}  Learning the generalized multi-hop rule set is a combinatorial search problem.  We address this challenge with a game-theoretic approach inspired by. Our approach consists of two steps:  selecting a generalized multi-hop rule set by employing a Multi-Layer Perceptron  over the candidate chains;   reasoning with the generalized rule set, which uses another MLP to model the conditional probability of the target relationship given the selected relation chains. The nonlinearity of MLP as reasoner provides the potential to model the logic conjunction among the selected chains in the rule set.  We demonstrate the advantage of our method on KG completion tasks in FB15K-237 and NELL-995. Our method outperforms existing single-chain approaches, showing that our defined generalized rules are necessary for many reasoning tasks.  
"," Multi-hop reasoning approaches over knowledge graphs infer a missing relationship between entities with a multi-hop rule, which corresponds to a chain of relationships. We extend existing works to consider a generalized form of multi-hop rules, where each rule is a set of relation chains.  To learn such generalized rules efficiently, we propose a two-step approach that first selects a small set of relation chains as a rule and then evaluates the confidence of the target relationship by jointly scoring the selected chains. A game-theoretical framework is proposed to this end to simultaneously optimize the rule selection and prediction steps. Empirical results show that our multi-chain multi-hop  rules result in superior results compared to the standard single-chain approaches, justifying both our formulation  of  generalized rules  and the effectiveness of the proposed learning framework.",13
"  Generating text that conforms to syntactic or semantic constraints benefits many NLP applications. To name a few, when paired data are limited, \citet{yang-etal-2019-low} build templates from large-scale unpaired data to aid the training of the dialog generation model; \citet{Niu2017ASO} and \citet{liu-etal-2019-rhetorically} apply style constraints to adjust the formality or rhetoric of the utterances; \citet{iyyer2018adversarial} and \citet{li-etal-2019-Insufficient} augment dataset using controlled generation to improve the model performance.  We study the problem of syntactically controlled text generation, which aims to generate target text with pre-defined syntactic guidance. Most recent studies on this topic  use sentences as exemplars to specify syntactic guidance. However, the guidance specified by a sentence can be vague, because its syntactic and semantic factors are tangled. Different from them, we use constituency parse trees as explicit syntactic constraints. As providing full-fledged parse trees of the target text is impractical, we require only a template parse tree that sketches a few top levels of a full tree . Figure shows our pipeline.    \citet{iyyer2018adversarial} adopt the same setting as ours. Their proposed SCPN model uses two LSTM  encoders to respectively encode source text and parse tree, and connects them to one decoder with additional attention  and pointer  structures. Nonetheless, recurrent encoders not only suffer from information loss by compressing a whole sequence into one vector but also are incapable of properly modeling the tree structure of constituency parse as well. Consequently, their network tends to ``translate'' the parse tree, instead of learning the real syntactic structures from it. % \zc{this sentence is still unclear.}  We propose a Transformer-based syntax-guided text generation method, named \ours. It first expands a template constituency parse tree to a full-fledged parse tree tailored for the input source text, and then uses the full tree to guide text generation. To capture the tree structure of the syntax, we apply a path attention mechanism  to our text generation model. It forces one node to attend to only other nodes located in its path  instead of all the nodes in the tree. Such a mechanism limits the information flow among the nodes in the constituency tree that do not have the direct ancestor-descendant relationship, forcing the parent nodes to carry more information than their children. In cooperation with path attention, we linearize the constituency trees to a more compact node-level format . Moreover, to address the challenge of properly integrating the semantic and syntactic information, we design a multi-encoder attention mechanism . It enables the Transformer decoder to accept outputs from multiple encoders simultaneously.  We evaluated our model on the controlled paraphrasing task. The experiment results show that \ours outperforms the state-of-the-art SCPN method by  in syntactic quality and  in semantic quality. % \zc{ use absolute improvements instead of relative ones} Human evaluations prove our method generates  semantically and syntactically superior sentences, with  semantic and  syntactic score improvements. % \zc{also give concrete numbers here, how much improvements?} Further, we find that the multi-encoder attention mechanism enhances the Transformer's ability to deal with multiple inputs, and the path attention mechanism significantly contributes to the model's semantic performance .   Our contributions include: 1) a multi-encoder attention mechanism that allows a Transformer decoder to attend to multiple encoders; 2) a path attention mechanism designed to better incorporate tree-structured syntax guidance with a special tree linearization format; and 3) a syntax-guided text generation method \ours that achieves new state-of-the-art semantic and syntactic performance. 
","   We study the problem of using  constituency parse trees as syntactic guidance for controlled text generation. Existing approaches to this problem use recurrent structures, which not only suffer from the long-term dependency problem but also falls short in modeling the tree structure of the syntactic guidance. We propose to leverage the parallelism of Transformer to better incorporate parse trees. Our method first expands a partial template constituency parse tree to a full-fledged parse tree tailored for the input source text, and then uses the expanded tree to guide text generation. The effectiveness of our model in this process hinges upon two new attention mechanisms: 1) a path attention mechanism that forces one node to attend to only other nodes located in its path in the syntax tree to better incorporate syntax guidance; 2) a multi-encoder attention mechanism that allows the decoder to dynamically attend to information from multiple encoders. Our experiments in the controlled paraphrasing task show that our method outperforms SOTA models both semantically and syntactically, improving the best baseline's BLEU score from $11.83$ to $26.27$.",14
" Recently, there has been great success in automatic text summarization and generation. To better compare and improve the performance of models, evaluation for such systems has been a problem of interest. The selection of evaluation metrics will greatly affect the assessed quality of a generated summary and thus affect the evaluation of summarization models.   The most ideal metric is definitely human judgement, which is often treated as the gold standard. But human evaluation is time-consuming and labor-intensive, an automatic evaluation metric that cannot only save human resources but also simulate the ability of human judgement is of crucial importance.   Most of the existing automatic evaluation methods assess a summary by comparing it with reference texts written by humans. Some of them are model-free and simply use hand-crafted matching functions to calculate the similarity between the candidate summary and the reference  . These methods consider both the reference and the candidate as a sequence of tokens or n-gram blocks. For instance, as the de facto standard evaluation metric, ROUGE  calculates the n-gram overlap between the machine-generated summaries and reference summaries. Although these methods have the advantage of interpretability and efficiency, they are found to correlate poorly with human evaluation.   To reduce the requirement of exact word matching, some recent work tried to match the reference and the candidate summary in the embedding space of words or sentences . For instance, BERTScore  uses contextual word embeddings generated by BERT and performs a greedy matching to obtain the maximum cosine similarity between two texts. %\citeauthor{clarketal2019sentence}  designed a metric that combines sentence-level embeddings with the word mover閳ユ獨 distance   to calculate the distance of moving the candidate sequence into the reference and transforms the distance into a similarity score, while MoverScore  combines n-gram embeddings with WMD.   These methods are proved to correlate better with human judgement than ROUGE on many datasets, which demonstrates the effectiveness of using contextual embeddings.   }  , all the three dimensions focus on evaluating the linguistic quality of summaries.}  \end{table*}  However, the aforementioned methods all have some intrinsic drawbacks: these methods always need at least one human-generated reference to assess a candidate summary. References written by humans are costly to obtain. In addition, most of them only consider the semantic similarities with references, i.e. semantic qualities of the summaries, which ignores the linguistic qualities and other important aspects. In this paper, we propose a new unsupervised contrastive learning framework for automatically evaluating the summary qualities without comparing with reference summaries or training with human ratings. Specifically, we design an evaluator to consider both linguistic and semantic aspects of a summary. Then for each of the aspect we create a set of negative samples by perturbing the training samples. We compare the scores of original training samples and the negative samples to obtain the contrastive loss function and learn the evaluator. The experiments on Newsroom and CNN/Daily Mail demonstrate that our new evaluation method has much higher correlation with human judgement.  We summarize our contributions as follows:    
"," Evaluation of a document summarization system has been a critical factor to impact the success of the summarization task. Previous approaches, such as ROUGE, mainly consider the informativeness of the assessed summary and require human-generated references for each test summary. In this work, we propose to evaluate the summary qualities without reference summaries by unsupervised contrastive learning. Specifically, we design a new metric which covers both linguistic qualities and semantic informativeness based on BERT. To learn the metric, for each summary, we construct different types of negative samples with respect to different aspects of the summary qualities, and train our model with a ranking loss. Experiments on Newsroom and CNN/Daily Mail demonstrate that our new evaluation method outperforms other metrics even without reference summaries. Furthermore, we show that our method is general and transferable across datasets.",15
" Part-of-speech  tags and dependency parsing have formed a long-standing union in NLP. But equally long-standing has been the question of its efficacy. % of this union. %POS tags as features for parsers.  \carlos{Prior to the prevalence of deep learning in NLP, they were shown to be useful for syntactic disambiguation in certain contexts} %Certainly in the nigh-on forgotten pre-deep learning era of NLP, it seemed as if they were useful for syntactic disambiguation in certain contexts  . However, for neural network implementations, especially those which utilise character embeddings, POS tags have been shown to be much less useful .   Others have found that POS tags can still have a positive impact when using character representations given that the accuracy of the predicted POS tags used is sufficiently high . \citet{smith2018investigation} undertook a systematic study of the impact of features for Universal Dependency  parsing and found that using universal POS  tags does still offer a marginal improvement for their transition-based neural parser. The use of fine-grained POS tags still seems to garner noticeable improvements %even for challenging multi-lingual settings  .   %By far and away the most common use of  Latterly, POS tags have been commonly utilised implicitly for neural network parsers in multi-learning frameworks where they can be leveraged without the cost of error-propagation . Beyond multi-learning systems, \citet{strzyz2019viable} introduced dependency parsing as sequence labelling by encoding dependencies using relative positions of UPOS tags, thus explicitly requiring them at runtime. %So even if coarse POS tags, universal or otherwise, prove to be superfluous for graph- or transition-based neural parsers as direct features, there are still many uses for them.% in dependency parsing.   We follow the work of \citet{smith2018investigation} and evaluate the interplay of word embeddings, character embeddings, and POS tags as features for two modern parsers, one a graph-based parser, Biaffine, and the other a transition-based parser, UUParser . Similar to \citet{zhang2020pos}, we focus on the contribution of POS tags but evaluate UPOS tags.  \paragraph{Contribution} We analyse the effect UPOS accuracy has on two dependency parser systems for a number of UD treebanks. Our results suggest that in order to leverage UPOS tags as explicit features for these neural parsers, a prohibitively high tagging accuracy is needed, and that gold tag annotation seems to possess some exceptionality. We also investigate what aspects of predicted UPOS tags have the most impact on parsing accuracy.  
"," We present an analysis %contributing to the discussion  on the effect UPOS accuracy has on parsing performance. Results suggest that leveraging UPOS tags as features for neural parsers requires a prohibitively high tagging accuracy and that the use of gold tags offers a non-linear increase in performance, suggesting some sort of exceptionality. We also investigate what aspects of predicted UPOS tags impact parsing accuracy the most, highlighting some potentially meaningful linguistic facets of the problem.",16
"  Conversational Machine Reading  is challenging because the rule text may not contain the literal answer, but provide a procedure to derive it through interactions . In this case, the machine needs to read the rule text, interpret the user scenario, clarify the unknown user's background by asking questions, and derive the final answer. Taking Figure  as an example, to answer the user whether he is suitable for the loan program, the machine needs to interpret the rule text to know what are the requirements, understand he meets ``American small business'' from the user scenario, ask follow-up clarification questions about ``for-profit business'' and ``not get financing from other resources'', and finally it concludes the answer ``Yes'' to the user's initial question.    Existing approaches  decompose this problem into two sub-tasks.  Given the rule text, user question, user scenario, and dialog history , the first sub-task is to make a decision among ``Yes'', ``No'', ``Inquire'' and ``Irrelevant''. The ``Yes/No'' directly answers the user question and ``Irrelevant'' means the user question is unanswerable by the rule text. If the user-provided information  are not enough to determine his fulfillment or eligibility, an ``Inquire'' decision is made and the second sub-task is activated. The second sub-task is to capture the underspecified condition from the rule text and generate a follow-up question to clarify it. \citet{zhong-zettlemoyer-2019-e3} adopt BERT  to reason out the decision, and propose an entailment-driven extracting and editing framework to extract a span from the rule text and edit it into the follow-up question.  The current \sota model EMT  uses a Recurrent Entity Network  with explicit memory to track the fulfillment of rules at each dialog turn for decision making and question generation.   In this problem, document interpretation requires identification of conditions and determination of logical structures because rules can appear in the format of bullet points, in-line conditions, conjunctions, disjunctions, etc. Hence, correctly interpreting rules is the first step towards decision making. Another challenge is dialog understanding. The model needs to evaluate the user's fulfillment over the conditions, and jointly consider the fulfillment states and the logical structure of rules for decision making. For example, disjunctions and conjunctions of conditions have completely different requirements over the user's fulfillment states. However, existing methods have not considered condition-level understanding and reasoning.   In this work, we propose \modelnameshortnsp: \modelnamecap. To better understand the logical structure of a rule text and to extract conditions from it, we first segment the rule text into clause-like elementary discourse units  using a pre-trained discourse segmentation model. Each EDU is treated as a condition of the rule text, and our model estimates its entailment confidence scores over three states: Entailment, Contradiction or Neutral by reading the user scenario description and existing dialog. Then we map the scores to an entailment vector for each condition, and reason out the decision based on the entailment vectors and the logical structure of rules. Compared to previous methods that do little entailment reasoning  or use it as multi-task learning , \modelnameshort is the first method to explicitly build the dependency between entailment states and decisions at each dialog turn.   \modelnameshort achieves new \sota results on the blind, held out test set of ShARC. In particular, \modelnameshort outperforms the previous best model EMT  by 3.8\% in micro-averaged decision accuracy and 3.5\% in macro-averaged decision accuracy. Specifically, \modelnameshort performs well on simple in-line conditions and conjunctions of rules while still needing improvements on understanding disjunctions. Finally, we conduct comprehensive analyses to unveil the limitation of \modelnameshort and current challenges for the ShARC benchmark. We find one of the biggest bottlenecks is the user scenario interpretation, in which various types of reasoning are required. % Code and models will be released to facilitate research along this line.   
","  Document interpretation and dialog understanding are the two major challenges for conversational machine reading. In this work, we propose \modelnameshortnsp, a discourse-aware entailment reasoning network to strengthen the connection and enhance the understanding for both document and dialog. Specifically, we split the document into clause-like elementary discourse units  using a pre-trained discourse segmentation model, and we train our model in a weakly-supervised manner to predict whether each EDU is entailed by the user feedback in a conversation. Based on the learned EDU and entailment representations, we either reply to the user our final decision ``yes/no/irrelevant"" of the initial question, or generate a follow-up question to inquiry more information. Our experiments on the ShARC benchmark  show that \modelnameshort achieves \sota results of 78.3\% macro-averaged accuracy on decision making and 64.0 BLEU1 on follow-up question generation. Code and models are released at \url{https://github.com/Yifan-Gao/Discern}.",17
"   .     %      % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. }  Neural Language Models  have become a central component in NLP systems over the last few years, showing outstanding performance and improving the state-of-the-art on many tasks . However, the introduction of such systems has come at the cost of interpretability %and explainability and, consequently, at the cost of obtaining meaningful explanations when automated decisions take place. % and, specifically, of understanding how linguistic predictors - that were common as features in earlier systems - are encoded in such models.  Recent work has begun to study these models in order to understand whether they encode %are able to learn  linguistic phenomena even without being explicitly designed %forse meglio trained?  to learn such properties . Much of this work focused on the analysis and interpretation of attention mechanisms  and on the definition of probing models trained to predict simple linguistic properties from unsupervised representations.   Probing models trained  on different contextual representations provided evidences that such models are able to capture a wide range of linguistic phenomena  and even to organize this information in a hierarchical manner . However, the way in which this knowledge affects the decisions they make when solving specific downstream tasks has been less studied.  In this paper, we extended prior work by studying the linguistic properties encoded by one of the most prominent NLM, BERT , and how these properties affect its predictions when solving a specific downstream task. %,  using a suite of more than 80 probing tasks.  % qui vedere se tenere 'several' perch鑼 abbiamo 10 task di classificazione o dire che 鐚 uno solo diviso in 10 ""sotto-task"". We defined three research questions aimed at understanding:  what kind of linguistic properties are already encoded in a pre-trained version of BERT and where across its 12 layers;  how the knowledge of these properties is modified after a fine-tuning process;  whether this implicit knowledge %of these properties  affects the ability of the model to solve a specific downstream task, i.e. Native Language Identification . %With this aim, we firstly perform a very large suite of probing tasks using %on %DOMI: SPOSTIAMO QUESTA PARTE %To answer the first two questions, we firstly perform a very large suite of probing tasks using %on %the sentence representations extracted from the internal layers of BERT. Each of these tasks makes explicit a particular property of the sentence, from very shallow features  to more complex aspects of morpho--syntactic and syntactic structure , thus making them as particularly suitable to assess the implicit linguistic knowledge encoded in a NLM at a deep level of granularity. %with respect to a wide spectrum of phenomena overing lexical, morpho-syntactic and syntactic structure.  To tackle the first two questions, we adopted an approach inspired to the `linguistic profiling' methodology put forth by , which assumes that wide counts of linguistic features automatically extracted from parsed corpora allow modeling a specific language variety and detecting how it changes with respect to other varieties, e.g. complex vs simple language, female vs male--authored texts, texts written in the same L2 language by authors with different L1 languages.  Particularly relevant for our study, is that multi-level linguistic features have been shown to have a highly predictive role in tracking the evolution of learners' linguistic competence across time and developmental levels, both in first and second language acquisition scenarios .  %when leveraged by traditional learning models on a variety of text classification problems, all of which can be successfully tackled using formal, rather than content based aspects of a text: from the assessment of sentence complexity and text readability , to the identification of personal and sociodemographics traits of an author, such as his/her native language, gender, age etc.  and to the prediction of the evolution of learners' linguistic competence across time . %From this perspective, our approach can be considered as a particular implementation of the `linguistic profiling' methodology put forth by , which assumes that wide counts of linguistic features automatically extracted from parsed corpora allow modeling a specific language variety and detecting in what way it changes with respect to other varieties, e.g. complex vs simple language, female vs male--authored texts, texts written in the same L2 language by authors with different L1 languages. Given the strong informative power of these features to encode a variety of language phenomena across stages of acquisition, we assume that they can be also helpful to dig into the issues of interpretability of NLMs. In particular, we would like to investigate whether features successfully exploited to model the evolution of language competence can be similarly helpful in profiling how the implicit linguistic knowledge of a NLM changes across layers and before and after tuning on a specific downstream task. We chose the NLI task, i.e. the task of automatically classifying the L1 of a writer based on his/her language production in a learned language .  %Secondly, we investigate the type and degree of variations of linguistic information before and after fine-tuning the pre-trained model on 10 distinct  datasets used to solve Native Language Identification , i.e. the task of automatically classifying the L1 of a writer based on his/her language production in a learned language .   As shown by , linguistic features play a very important role when NLI is tackled as a sentence--classification task rather than as a traditional document--classification task.  %NLI can be addressed by exploiting only linguistic features extracted at sentence--level reaching comparable performance to those obtained by state--of--the--art models based on word embeddings .  This is the reason why we considered the sentence-level NLI classification as a task particularly suitable for probing the NLM linguistic knowledge. %perch鑼 鐚 un task che per essere risolto 鐚 necessario che il modello codifichi un'ampia gamma di informazioni linguistiche e anche perch鑼 鐚 un task basato sull'info estratta dalla sentence -come dimostrato da Cimino et al  nonostante lo stato dell'arte 鐚 stato definito soltanto usando word embeddings  %vecchia versione: a fine-tuning process based on a Native Language Identification  downstream task.  %vecchia versione: -base and 10 fine-tuned models obtained training BERT on as many Native Language Identification  tasks.  Finally, we investigated whether and which linguistic information encoded by BERT is involved in discriminating the sentences correctly or incorrectly classified by the fine-tuned models. To this end, we tried to understand if the linguistic knowledge that the model has of a sentence affects the ability to solve a specific downstream task involving that sentence.   %vecchia versione: Adopting a suite of more than 80 probing tasks, we firstly perform % We perform our experiments using a suite of more than 80 probing tasks, each of which corresponds to a specific/distinct sentence-level feature. We find that / We show that  %The remainder of the paper is organized as follows. We start by presenting some related works which are more closely related to our study  and in Section  we highlight the main novelties of our approach. We then describe in more details the data , the probing tasks  and the models  we used. Experiments and results are described in Section ,  and . To conclude, in Section  we summarize the main findings of the study.  \paragraph{Contributions} In this paper:  we carried out an in-depth linguistic profiling of BERT's internal representations %deep analysis of the implicit linguistic knowledge stored in BERT's internal representations and how it changes across layers using a wide suite of sentence-level probing tasks, corresponding to a wide spectrum of linguistic phenomena at different level of complexity; % we verify the implicit linguistic knowledge stored in BERT's internal representations using a suite of more than 80 probing tasks corresponding to a wide range of linguistic phenomena at different level of complexity;   we showed that contextualized representations tend to lose their precision in encoding a wide range of linguistic properties %general-purpose linguistic properties  after a fine-tuning process; % RIVEDERE 'GENERAL-PURPOSE' COME TERMINE PER DESCRIVERE LE NOSTRE FEATURES  we showed that the linguistic knowledge stored in the contextualized representations of BERT positively affects its ability to solve NLI downstream tasks: the more BERT stores information about these features% in its embeddings/internal representations , the higher will be its capacity of predicting the correct label.   
"," In this paper we investigate the linguistic knowledge learned by a Neural Language Model  before and after a fine-tuning process and how this knowledge affects its predictions during several classification problems. We use a wide set of probing tasks, each of which corresponds to a distinct sentence-level feature extracted from different levels of linguistic annotation. We show that BERT is able to encode a wide range of linguistic characteristics, but it tends to lose this information when trained on specific downstream tasks. We also find that BERT's capacity to encode different kind of linguistic properties has a positive influence on its predictions: the more it stores readable linguistic information of a sentence, the higher will be its capacity of predicting the expected label assigned to that sentence.",18
"   Recent emergent-communication studies, renewed by the astonishing success of neural networks, are often motivated by a desire to develop neural network agents eventually able to verbally interact with humans . To facilitate such interaction, neural networks' emergent language should possess many natural-language-like properties. However, it has been shown that, even if these emergent languages lead to successful communication, they often do not bear core properties of natural language .  In this work, we focus on one basic property of natural language that resides on the tendency to use messages that are close to the informational optimum. This is illustrated in the Zipf's law of Abbreviation , an empirical law that states that in natural language, the more frequent a word is, the shorter it tends to be . Crucially, ZLA is considered to be an efficient property of our language .  Besides the obvious fact that an efficient code would be easier to process for us, it is also argued to be a core property of natural language, likely to be correlated with other fundamental aspects of human communication, such as regularity and compositionality . Encouraging it might hence lead to emergent languages that are also more likely to develop these other desirable properties.   Despite the importance of such property,  \citet{chaabouni:etal:2019} showed that standard neural network agents, when trained to play a simple signaling game , develop an inefficient code, which even displays an anti-ZLA pattern. That is, counterintuitively, more frequent inputs are coded with longer messages than less frequent ones. This inefficiency was related to  neural networks' ``innate preference'' for long messages. In this work, we aim at understanding which constraints need to be introduced on neural network agents in order to overcome  their innate preferences and communicate efficiently, showing a proper ZLA pattern.  To this end, we %follow \citet{chaabouni:etal:2019} and use a reconstruction game where we have two neural network agents: speaker and listener. For each input, the speaker outputs a sequence of symbols  sent to the listener. The latter needs then to predict the speaker's input based on the given message. Also, similarly to the previous work, inputs are drawn from a power-law distribution.   We first describe the experimental and optimization framework . In particular, we introduce a new communication system called `LazImpa', comprising two different constraints  Laziness on the speaker side and  Impatience on the listener side. The former constraint is inspired by the least-effort principle which is attested to be a ubiquitous pressure in human communication .   However, if such a constraint is applied too early, the system does not learn an efficient system. We show that incrementally penalizing long messages in the cost function enables an early exploration of the message space  and prevents converging to an inefficient local minimum.   The other constraint, on the listener side, relies on the prediction mechanism, argued to be important in language comprehension \citep[e.g.,][]{federmeier2007, altmann2009}, and is achieved by allowing the listener to reconstruct the intended input as soon as possible. We also provide a two-level analytical method: first, metrics quantifying the efficiency of a code; second, a new protocol to measure its informativeness . Applying these metrics, we demonstrate that, contrary to the standard speaker/listener agents, our new communication system `LazImpa' leads to the emergence of an efficient code. The latter follows a ZLA-like distribution, close to natural languages . Besides the plausibility of the introduced constraints, our new communication system is, first, task- and architecture-agnostic , and second allows stable optimization of the speaker/listener. We also show how both listener and speaker constraints are fundamental to the emergence of a ZLA-like distribution, as efficient as natural language .  
"," Previous work has shown that artificial neural agents naturally develop surprisingly non-efficient codes.  This is illustrated by the fact that in a referential game involving a speaker and a listener neural networks optimizing accurate transmission over a discrete channel, the emergent messages fail to achieve an optimal length. Furthermore, frequent messages tend to be longer than infrequent ones, a pattern contrary to the Zipf Law of Abbreviation  observed in all natural languages. Here, we show that near-optimal and ZLA-compatible messages can emerge, but only if both the speaker and the listener are modified. We hence introduce a new communication system, ``LazImpa'', where the speaker is made increasingly lazy, i.e.,~avoids long messages, and the listener impatient, i.e.,~seeks to guess the intended content as soon as possible.",19
" % 1 - What problem are you solving? Entity typing classifies textual mentions of entities, according to their semantic class, within a set of labels  organized in an inventory. %Multi-label text classification is the task of assigning to a sample all the relevant labels from a label  inventory . The task has progressed from recognizing a few coarse classes , to extremely large inventories, with hundreds  or thousands of labels . Therefore, exploiting inter-label correlations has become critical to improve performance.   % 2 - Why is it an interesting/important problem? % es interesante porque son buenos para modelar redes y estructuras jer璋﹔quicas. % Problema: su adopcion en nlp ha sido baja dado que no hay una forma muy intuitiva de modelar texto en ellos. Distintos papers muestran como agregar un peque甯給 cambio pero no una aplicacion real y completa Large inventories tend to exhibit a hierarchical structure, either by an explicit tree-like arrangement of the labels , or implicitly through the label distribution in the dataset . %A natural solution for dealing with large inventories is to organize them in hierarchy ranging from general, coarse labels near the top, to more specific, fine classes at the bottom. Prior work has integrated only explicit hierarchical information by formulating a hierarchy-aware loss  or by representing instances and labels in a joint Euclidean embedding space .  However, the resulting space is hard to interpret, and these methods fail to capture implicit relations in the label inventory. Hyperbolic space is naturally equipped for embedding symbolic data with hierarchical structures . Intuitively, that is because the amount of space grows exponentially as points move away from the origin. This mirrors the exponential growth of the number of nodes in trees with increasing distance from the root  . %Its tree-like properties make it efficient to learn hierarchical representations with low distortion .     % Embeddings  that  are  close  to  the  origin  of  the  disk  will have a relatively small distance to all other points, rep-resenting the root of the hierarchy.  On the other hand,embeddings that are close to the boundary of the disk will have a relatively large distance to all other points and are well suited to represent leaf nodes   % 3 - How are you going to solve it? In this work, we propose a fully hyperbolic neural model for fine-grained entity typing. Noticing a perfect match between hierarchical label inventories in the linguistic task and the benefits of hyperbolic spaces, we endow a classification model with a suitable geometry to capture this fundamental property of the data distribution. By virtue of the hyperbolic representations, the proposed approach automatically infers the latent hierarchy arising from the class distribution and achieves a meaningful and interpretable organization of the label space. This arrangement captures implicit hyponymic relations  in the inventory and enables the model to excel at fine-grained classification. To the best of our knowledge, this work is the first to apply hyperbolic geometry from beginning to end to perform multi-label classification on real NLP datasets.  %NICE PHRASE FROM GULCEHRE: The focus of this work is to endow neural network representations with suitable geometry to capture fundamental properties of data... given the perfect fit between the label distribution in the linguistic task of entity typing and the mathematical properties of hyperbolic spaces.   % esto deberia ser ""hay componentes ya hechos"". Y lo conecto al toque con el parrafo sig.  Recent work has proposed hyperbolic neural components, such as word embeddings , recurrent neural networks  and attention layers . %Advantages of hyperbolic representations are well-established for discrete data such as networks  and graphs . In the realm of Natural Language Processing  components that exploit hyperbolic geometry have been developed as well, such as word embeddings , recurrent neural networks  and attention layers . %or classifiers  Me encanta este paper pero no hace NLP :. We address these issues. Our model encodes textual inputs, applies a novel attention mechanism, and performs multi-class multi-label classification, executing all operations in the Poincar\'e model of hyperbolic space . %By employing the leveraging the geometric properties of hyperbolic space through    %The lack of systems that utilize hyperbolic space from beginning to end is due to three main difficulties: %First, there are different analytic models of hyperbolic space, and not all previous work operates in the same one, which hinders their combination.  %Second, it is not clear how to integrate these components into conventional Euclidean neural models since a mapping of the data from one space onto the other is required. Third, optimization of hyperbolic models is non-trivial.   %We bridge the gaps among previous work by developing the missing connections and adapting different components to employ the Poincar\'e model of hyperbolic space in all layers of the network.  % We bridge the gaps among previous work by developing the missing connections and adapting different components, in order to accomplish a full hyperbolic neural network. This is, a network that extracts features from text, applies attention layers and performs \todo{I am the only one doing this}{multi-class classification}, executing all operations in hyperbolic geometry.   % able to perform multi-label multi-class classification with text as input    %The model is proposed in a generic manner such that it can be applied to classify sequential data . Since hyperbolic geometry is naturally equipped to model hierarchical structures, we hypothesize that the model will excel at tasks that profit from the incorporation of hierarchical information. % \todo{awful}{systems} that operate under this metric space result in superior performance when incorporating hierarchical information.   %We evaluate our model on the task of fine-grained entity type classification , which we consider a suitable testbed due to its connection with textual inputs and hierarchical type inventories.  % Introduce main results % HNN's phrase: ""On a series of experiments and datasets we showcase the effectiveness of our hyperbolic neural network layers compared to their ""classic"" Euclidean variants on"" % \todo[inline]{Forwarding a bit of the results is a good idea . %\todo[inline]{Cambiar esta frase a la idea de que ""imponer the right metric es como imponer the right bias""}  %We impose an inductive bias on the model by means of the geometry of its internal representation. This allows us to operate on very low-dimensional spaces thus substantially reducing the parameter cost. Instead of relying on large pre-trained models, we impose a suitable inductive bias by choosing an adequate metric space to embed the data, which does not introduce extra burden on the parameter footprint. %Phrase from xiong2019inductiveBias: ""Instead of using an explicit graphical model, we enforce a relational bias on model parameters, which does not introduce extra burden on label decoding."" % Misma idea pero yo meto el bias en la representacion, lo cual no introduce un costo adicional y permite operar con MUCHOS menos par璋﹎etros.   %Our components are developed in a modular way which allows them to be seamlessly integrated into NLP architectures.    %\todo{Remove!}{While there now exist several hyperbolic components, a practitioner faced with these options has a simple question: How to integrate them with conventional layers? In this work, we answer this question.}  By means of the exponential and logarithmic maps  we are able to mix hyperbolic and Euclidean components into one model, aiming to exploit their strengths at different levels of the representation. We perform a thorough ablation that allows us to understand the impact of each hyperbolic component in the final performance of the system , and showcases its ease of integration with Euclidean layers.  %In summary, we make the following contributions: %%%%% %      %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
"," Label inventories for fine-grained entity typing have grown in size and complexity. Nonetheless, they exhibit a hierarchical structure. Hyperbolic spaces offer a mathematically appealing approach for learning hierarchical representations of symbolic data. However, it is not clear how to integrate hyperbolic components into downstream tasks. This is the first work that proposes a fully hyperbolic model for multi-class multi-label classification, which performs all operations in hyperbolic space. We evaluate the proposed model on two challenging datasets and compare to different baselines that operate under Euclidean assumptions.  Our hyperbolic model infers the latent hierarchy from the class distribution, captures implicit hyponymic relations in the inventory, and shows performance on par with state-of-the-art methods on fine-grained classification with remarkable reduction of the parameter size. A thorough analysis sheds light on the impact of each component in the final prediction and showcases its ease of integration with Euclidean layers. \footnote{Code available at:\\ \url{https://github.com/nlpAThits/hyfi}}",20
"  Entity Recognition  involves detection  and classification of entities mentioned in unstructured text into pre-defined categories. It is one of the foundational sub-task of several Information Extraction   and Natural Language Processing  pipelines. Hence, errors introduced during the extraction of entities can propagate further and degrade the performance of the complete IE or NLP pipeline. In the domains of experimental biology, the growing complexity of experiments has resulted in a need to automate wet laboratory procedures. Such an automation will be useful in avoiding human errors introduced in the wet lab protocols and thereby will enhance the reproducibility of experimental biological research.   To achieve this reproducibility, some of the previous research works have focussed on defining machine-readable formats for writing wet lab protocols . However, the vast majority of today閳ユ獨 protocols are written in natural language with jargon and colloquial language constructs that emerge as a byproduct of ad-hoc protocol documentation. This motivates the need for machine reading systems that can interpret the meaning of these natural language instructions, to enhance reproducibility via semantic protocols  and enable robotic automation  by mapping natural language instructions to executable actions. In order to enable research on interpreting natural language instructions, with practical applications in biology and life sciences, an annotated database  of wet lab protocols was introduced.   The first step in interpreting natural language lab protocols is to extract entities, followed by identification of relations between them. To address the research focussing on entity recognition over Wet Lab Protocols a shared task  was introduced at EMNLP WNUT-2020 Workshop. The task was based on the annotated database  of wet lab protocols. We tackle this task in two phases. In the first phase, we experiment with various contextualised word embeddings  and a BiLSTM-CRF model to arrive at the best-performing architecture. In the second phase, we create an ensemble composed of eleven BiLSTM-CRF models. The individual models are trained on random train-validation splits of the complete dataset. Here, we also experiment with different output merging schemes, including Majority Voting and SLE.  The rest of the paper is structured as follows: Section 2 states the task definition. Section 3 describes the specifics of our methodology. Section 4 explains the experimental setup and the results, and Section 5 concludes the paper.  
"," In this paper, we describe the approach that we employed to address the task of Entity Recognition over Wet Lab Protocols - a shared task in EMNLP WNUT-2020 Workshop. Our approach is composed of two phases. In the first phase, we experiment with various contextualised word embeddings  and a BiLSTM-CRF model to arrive at the best-performing architecture. In the second phase, we create an ensemble composed of eleven BiLSTM-CRF models. The individual models are trained on random train-validation splits of the complete dataset. Here, we also experiment with different output merging schemes, including Majority Voting and Structured Learning Ensembling . Our final submission achieved a micro F1-score of 0.8175 and 0.7757 for the partial and exact match of the entity spans, respectively. We were ranked first and second, in terms of partial and exact match, respectively.",21
"  We make many decisions as we interact with the world. When we are rewarded , we learn to modify not only the proximal cause of the stimulus but the chain of decisions leading up to it, to encourage  future similar results. This process naturally is the paradigm of Reinforcement Learning . Policy-based learning seeks to find good estimates for , a function that returns the expected cumulative reward  if action  is chosen at state . A desirable property of methodologies to learn  is their ability to generalize such that an appropriate action can be taken when encountering a previously unseen state.   Recent advances have shown strong evidence of generalization in spatiotemporal modalities such as robotic manipulation , video games , and autonomous navigation . However, in the modality of language, there is less work applying generalization approaches to decision making.  Useful applications of sequential decision making language models are personal assistants that proactively anticipate client needs; anti-phishing mediation agents that waste a would-be thief's time with relevant but non-helpful responses; and investigative journalist assistants that determine what to read, whom to contact, and what questions to ask to create a revelatory news report.  Neural reinforcement learning  training approaches, such as those used to play action video games , have potential applicability in language-based decision making due to their ability to learn to navigate adversarial or exploratory scenarios. Naturally, the generalization and background knowledge capability afforded by large contextualized language models such as \bert  may be applicable as well. A useful virtual world proxy in which to explore these approaches' applicability is that of text adventure game playing. In a text adventure game, a player is immersed in an environment by reading textual descriptions of a scene and issuing natural language commands to navigate inside the scene. The player discovers and interacts with entities and accomplishes goals, while receiving explicit rewards for doing so.   Learning to play text games is a useful pursuit because it is a convenient proxy for the real world cases cited above. Unlike these, plentiful data for numerous games exist, an endless supply of games can be constructed, and text games have built-in reward functions,  making them suitable for RL. This class of problems is also useful because it is challenging: after exposure to a family of games that explore the same topic and have similar gameplay , human players perform nearly perfectly on additional games, but computer models struggle.   Why is this? Humans quickly understand the situation they are placed in and can make rational decisions based on trial-and-error and life experience, which we can call commonsense knowledge. Knowing a priori that, e.g.,  a  door should be  or that it is helpful to  in a  allows  players to learn  faster. Even though these games have the complexity of finite-state machines, computer models cannot learn to play them well. The problem appears to be due to a lack of generalization caused by a lack of commonsense. To a computer model, considering whether to  using a  is no more ludicrous than considering whether to  using a  . Both actions can be discouraged by negative reinforcement, but a human only needs to learn not to do the latter.   Furthermore, a computer player learning that one can  with a  may not generalize that one can  the same way, but a human surely will.  There is existing work in learning to play text games with RL   but the standard pattern of incorporating large language models such as \bert  has not yet been seen in current literature. It turns out that this integration is not trivial. Most models that use \bert and its ilk predominantly apply their results to supervised learning tasks that have training data with ground truth  or at least, in the case of generation-based tasks like dialogue and translation, a corpus of desirable output to mimic . For tasks suited to RL such as the exploration of and interaction with a world, there is no true target or even, initially, a corpus, and thus learning can only proceed iteratively via, e.g., exploration-exploitation , which requires millions of training iterations to converge . Integrating this process with the additional overhead of fine-tuning a large model like \bert leads to an impractical slowdown: for the experiments considered in this work, the baseline models that use \cnn require a little more than three weeks to train on an Nvidia P100 GPU-equipped machine. Using the same models on the same tasks run for the same number of iterations on the same hardware while fine-tuning a 12-layer \bert model would take more than two years.      In this work, we compare different previously used representation models for deep RL through an imitation learning method that first trains a light-weight teacher using exploration-exploitation, and then uses that trained model to train a more heavy-weight student model. This dramatically decreases the amount of training time needed to learn.  Moreover, we devise a means of casting an RL problem into a supervised learning paradigm, allowing better exploitation of large contextualized language models. In so doing, we show that agents can benefit from both the imitation learning and the reformulation, converging faster than other models, and exceeding teacher performance by 7\% and 24\% on both in- and out-of-domain problems, despite the limited search space.  The novel contributions of this work are:     
"," We consider problems of making sequences of decisions to accomplish tasks,  interacting via the medium of language. These problems are often tackled with reinforcement learning approaches. We find that these models do not generalize well when applied to novel task domains. However, the large amount of computation necessary to adequately train and explore the search space of sequential decision making, under a reinforcement learning paradigm, precludes the inclusion of large contextualized language models, which might otherwise enable the desired generalization ability. We introduce a teacher-student imitation learning methodology and a means of converting a reinforcement learning model into a natural language understanding model. Together, these methodologies enable the introduction of contextualized language models into the sequential decision making problem space. We show that models can learn faster and generalize more, leveraging both the imitation learning and the reformulation. Our models exceed teacher performance on various held-out decision problems, by up to 7\% on in-domain problems and 24\% on out-of-domain problems.",22
"  %   Reinforcement learning has shown great success in environments with large state spaces. Using neural networks to capture state representations has allowed end-to-end training of agents on domains like Atari  and Go . It is natural to emulate this success in text domains, especially given that the state space in language-based tasks is combinatorially large. A sentence of length  with allowed vocabulary  has  possible states, and tabular methods like learning  will fail unless coupled with powerful function approximators like neural networks.\\  While the current state of RL has multiple challenges, sparse rewards are one that leads to slow, and sometimes no convergence. Consider an agent learning in an environment with a large state space, with only a few states leading to a reward . An agent starting on the far left must take a large number of actions before encountering a reward. In turn, this sparse feedback results in a very noisy gradient for training the neural network. In an extreme scenario, as in Figure , an agent might have to take an exponential number of actions to reach a single leaf that has a reward.      Some early work, such as reward shaping , attempted to solve the sparse reward problem by introducing dense rewards based on heuristics, e.g., how close the agent is to the goal. However, these require complex design choices that might result in unexpected behavior from the agents.\\  Sparse rewards are common because they are the most straightforward way to specify how a task needs to be solved. If a robot is expected to pour water from a jug into a glass, the simplest way is to give a reward of  if it fills the glass, and  otherwise. This type of reward design is common in text-based games, in which the agent is rewarded upon reaching the goal state, and task-oriented dialogue, in which the agent is rewarded based on the successful completion of the task.\\  For this study, we examine text-based games and find that providing dense rewards with the help of sentiment analysis improves performance under some conditions.  
"," While reinforcement learning  has been successful in natural language processing  domains such as dialogue generation and text-based games, it typically faces the problem of sparse rewards that leads to slow or no convergence. Traditional methods that use text descriptions to extract only a state representation ignore the feedback inherently present in them. In text-based games, for example, descriptions like ``Good Job! You ate the food'' indicate progress, and descriptions like ``You entered a new room'' indicate exploration. Positive and negative cues like these can be converted to rewards through sentiment analysis. This technique converts the sparse reward problem into a dense one, which is easier to solve. Furthermore, this can enable reinforcement learning without rewards, in which the agent learns entirely from these intrinsic sentiment rewards. This framework is similar to intrinsic motivation, where the environment does not necessarily provide the rewards, but the agent analyzes and realizes them by itself. We find that providing dense rewards in text-based games using sentiment analysis improves performance under some conditions.",23
"  Natural language data is rich in structure, but most of the structure is not visible at the surface.  Machine learning models tackling high-level language tasks would benefit from uncovering underlying structures such as trees, sequence tags, or segmentations.  Traditionally, practitioners turn to pipeline approaches where an external, pretrained model is used to predict, \eg, syntactic structure.  The benefit of this approach is that the predicted tree is readily available for inspection, but the downside is that the errors  can easily propagate throughout the pipeline and require further attention . In contrast, deep neural architectures tend to eschew such preprocessing, and instead learn soft hidden representations, not easily amenable to visualization and analysis.  The best of both worlds would be to model structure as a latent variable, combining the transparency of the pipeline approach with the end-to-end unsupervised representation learning that makes deep models appealing. Moreover, large-capacity model tend to rediscover structure from scratch , so structured latent variables may reduce the required capacity.  Learning with discrete, combinatorial latent variables is, however, challenging, due to the intersection of large cardinality and null gradient issues. For example, when learning a latent dependency tree, the latent parser must choose among an exponentially large set of possible trees; what's more, the parser may only learn from gradient information from the downstream task. If the highest-scoring tree is selected using an argmax operation, the gradients will be zero, preventing learning.  One strategy for dealing with the null gradient issue is to use a surrogate gradient, explicitly overriding the zero gradient from the chain rule, as if a different computation had been performed. The most commonly known example is the straight-through estimator \citep[STE;][]{bengio2013estimating}, which pretends that the argmax node was instead an identity operator. Such methods lead to a fundamental mismatch between the objective and the learning algorithm. The effect of this mismatch  is still insufficiently understood, and the design of successful new variants is therefore challenging. For example, the recently-proposed SPIGOT method  found it beneficial to use a projection as part of the surrogate gradient.  In this paper, we study surrogate gradient methods for deterministic learning with discrete structured latent variables. Our contributions are:    While the discrete methods do not outperform the relaxed alternatives using the same building \linebreak blocks, we hope that our interpretation and insights would trigger future latent structure research.  The code for the paper is available on \url{https://github.com/deep-spin/understanding-spigot}.      
"," Latent structure models are a powerful tool for modeling language data: they can mitigate the error propagation and annotation bottleneck in pipeline systems, while simultaneously uncovering linguistic insights about the data. One challenge with end-to-end training of these models is the argmax operation, which has null gradient. In this paper, we focus on surrogate gradients, a popular strategy to deal with this problem. We explore latent structure learning through the angle of pulling back the downstream learning objective. In this paradigm, we discover a principled motivation for both the straight-through estimator  as well as the recently-proposed SPIGOT---a variant of STE for structured models. Our perspective leads to new algorithms in the same family. We empirically compare the known and the novel pulled-back estimators against the popular alternatives, yielding new insight for practitioners and revealing intriguing failure cases.",24
"   %% Paragraph 1:  %% * introduce the constructions of interest  %% * give broad impression of the subtlety of grammatical phenomena, %% * emphasize the verb bias problem, since this is one of our unique contributions When we use language, we are often faced with a choice between several possible ways of expressing the same message. For example, in English, to express an event of intended or actual transfer between two animate entities, one option is the double-object  construction, in which two noun phrases follow the verb.  Alternatively, the same content can be expressed using the prepositional dative  construction.  \ex.  \a. Ava gave him something. \hfill DO \b. Ava gave something to him. \hfill PO  Speakers' preferences for one or the other construction depend on multiple factors, including the length and definiteness of the arguments  . % could also cite: Davidse 1996; Givo 铏俷 1984a; Polinsky 1996; Ransom 1979; Snyder 2003; Thompson 1990, 1995;  One particularly subtle factor is the lexical verb bias. While some verbs readily occur in either construction, others have strong preferences for one over the other :  \ex.  \a. ?Ava said him something. \hfill DO \b. Ava said something to him. \hfill PO  %% Paragraph 2:  %% * transition to motivation for why this problem is interesting for NLP %% * briefly mention major previous work on this problem and its gaps   Decades of work in linguistics and psychology has investigated how humans learn these distinctions . Yet, as deep neural networks have achieved state-of-the-art performance across many tasks in natural language processing, little is known about the extent to which they have acquired similarly fine-grained preferences. Although neural language models robustly capture certain types of grammatical constraints, e.g., subject-verb agreement and long distance dependencies , they continue to struggle with other aspects of syntax, including argument structure \cite[e.g.][]{warstadt2019neural}. Verb biases provide a particularly interesting testbed.  Successfully predicting these psycholinguistic phenomena requires the integration of specific lexical information with representations of higher-level grammatical structures, with implications for understanding differential performance between models on other tasks.    %% Paragraph 3: our contribution In the current work, we take an analytic and comparative approach. First, we introduce the DAIS  dataset, containing 50K human preference judgments for 5K sentence pairs, using 200 unique verbs. These empirical judgments indicate that verb bias preferences are highly gradient in practice , rather than belonging to binary ``alternating'' and ``non-alternating'' classes, as commonly assumed. Second, we evaluate the predictions of a variety of neural models, including both recurrent architectures and transformers, and analyze their internal states to understand what drives differences in performance.  \change{Finally, we evaluate our models on natural production data from the Switchboard corpus, finding that transformers achieve similar classification accuracy as prior work using hand-annotated features \cite[;][]{bresnan2007predicting}.}   
"," Languages typically provide more than one grammatical construction to express certain types of messages. A speaker's choice of construction is known to depend on multiple factors, including the choice of main verb -- a phenomenon known as verb bias. Here we introduce DAIS, a large benchmark dataset containing 50K human judgments for 5K distinct sentence pairs in the English dative alternation. This dataset includes 200 unique verbs and systematically varies the definiteness and length of arguments.  We use this dataset, as well as an existing corpus of naturally occurring data, to evaluate how well recent neural language models capture human preferences. Results show that larger models perform better than smaller models, and transformer architectures  tend to out-perform recurrent architectures  even under comparable parameter and training settings.  Additional analyses of internal feature representations suggest that transformers may better integrate specific lexical information with grammatical constructions.",25
"     The core idea behind the predominant pretrain and fine-tune paradigm for transfer learning in NLP is that general language knowledge, gleaned from large quantities of data using unsupervised objectives, can serve as a foundation for more specialized endeavors. Current practice involves taking the full model that has amassed such general knowledge and fine-tuning it with a second objective appropriate to the new task \citep[see][for an overview]{raffelExploringLimitsTransfer2019}. Using these methods, pre-trained transformer-based language models \citep[e.g., BERT, ][]{devlin-etal-2019-bert} have been employed to great effect on a wide variety of NLP problems, thanks, in part, to a fine-grained ability to capture aspects of linguistic context .      However, this paradigm introduces a subtle but insidious limitation that becomes evident when the downstream application is a topic model. A topic model may be cast as a  autoencoder , and we could fine-tune a pretrained transformer with an identical document reconstruction objective. But in replacing the original topic model, we lose the property that makes it desirable: its interpretability. The transformer gains its contextual power from its ability to exploit a huge number of parameters, while the interpretability of a topic model comes from a dramatic dimensionality reduction.      We combine the advantages of these two approaches---the rich contextual language knowledge in pretrained transformers and the intelligibility of topic models---using knowledge distillation . In the original formulation, knowledge distillation involves training a parameter-rich teacher classifier on large swaths of data, then using its high-quality probability estimates over outputs to guide a smaller student model. Since the information contained in these estimates is useful---a picture of an ox will yield higher label probabilities for buffalo than apricot---the student needs less data to train and can generalize better.  We show how this principle can apply equally well to improve unsupervised topic modeling, which to our knowledge has not previously been attempted.  While distillation usually involves two models of the same type, it can also apply to models of differing architectures. Our method is conceptually quite straightforward: we fine-tune a pretrained transformer  on a document reconstruction objective, where it acts in the capacity of an autoencoder. When a document is passed through this BERT autoencoder, it generates a distribution over words that includes unobserved but related terms. We then incorporate this distilled document representation into the loss function for topic model estimation.    To connect this method to the more standard supervised knowledge distillation, observe that the unsupervised ``task'' for both an autoencoder and a topic model is the reconstruction of the original document, i.e. prediction of a distribution over the vocabulary. The BERT autoencoder, as ``teacher'', provides a dense prediction that is richly informed by training on a large corpus. The topic model, as ``student'', is generating its own prediction of that distribution. We use the former to guide the latter, essentially as if predicting word distributions were a multi-class labeling problem. \newcommand{\reffig}[1]{\hl{[FIG: #1]}} \newcommand{\reftable}[1]{\hl{[TABLE: #1]}} \newcommand{\refsec}[1]{\hl{[SECTION: #1]}} \newcommand{\ho}[1]{\textcolor{blue}{}} \newcommand{\pg}[1]{\textcolor{red}{}} \newcommand{\psrcomment}[1]{}  \newcommand{\ignore}[1]{} \newcommand{\ourmodel}{BAT }   \newcommand{\e}[2]{\mathbb{E}_{#1}\left[ #2 \right] } \newcommand{\B}{B} \DeclareMathOperator*{\argmin}{arg\,min}  \aclfinalcopy %   \newcommand\BibTeX{Bib\TeX}  \title{Improving Neural Topic Models using Knowledge Distillation}  \author{Alexander Hoyle\thanks{\, Equal contribution.} \\   Computer Science \\   University of Maryland \\   College Park, MD \\    \\\And   Pranav Goel\footnotemark[1] \\   Computer Science \\   University of Maryland \\   College Park, MD \\    \\\And   Philip Resnik \\   Linguistics / UMIACS \\   University of Maryland \\   College Park, MD \\    \\}  \date{}  \begin{document}                         \bibliography{anthology,refs,zotero} \bibliographystyle{acl_natbib}  \clearpage \appendix 
","     Topic models are often used to identify human-interpretable topics to help make sense of large document collections. We use knowledge distillation to combine the best attributes of probabilistic topic models and pretrained transformers. Our modular method can be straightforwardly applied with any neural topic model to improve topic quality, which we demonstrate using two models having disparate architectures, obtaining state-of-the-art topic coherence. We show that our adaptable framework not only improves performance in the aggregate over all estimated topics, as is commonly reported, but also in head-to-head comparisons of aligned topics.",26
"      Interactive systems capable of understanding natural language and responding in the form of natural language text have high potentials in various applications.  In pursuit of building and evaluating such systems, we study learning agents for Interactive Fiction  games. IF games are world-simulating software in which players use text commands to control the protagonist and influence the world, as illustrated in Figure. IF gameplay agents need to simultaneously understand the game's information from a text display  and generate natural language command  via a text input interface.  Without providing an explicit game strategy, the agents need to identify behaviors that maximize objective-encoded cumulative rewards.    IF games composed of human-written texts  create superb new opportunities for studying and evaluating natural language understanding  techniques due to their unique characteristics.   Game designers elaborately craft on the literariness of the narrative texts to attract players when creating IF games. The resulted texts in IF games are more linguistically diverse and sophisticated than the template-generated ones in synthetic text games.  The language contexts of IF games are more versatile because various designers contribute to enormous domains and genres, such as adventure, fantasy, horror, and sci-fi.  The text commands to control characters are less restricted, having sizes over six orders of magnitude larger than previous text games.  The recently introduced Jericho benchmark provides a collection of such IF games.   The complexity of IF games demands more sophisticated NLU techniques than those used in synthetic text games. Moreover, the task of designing IF game-play agents, intersecting NLU and reinforcement learning , poses several unique challenges on the NLU techniques. The first challenge is the difficulty of exploration in 	extbf{the huge natural language action space}. To make RL agents learn efficiently %via trial-and-error  without prohibitive exhaustive trials, the action estimation must generalize learned knowledge from tried actions to others.  To this end, previous approaches, starting with a single embedding vector of the observation, either predict the elements of actions independently; or embed each valid action as another vector and predict action value based on the vector-space similarities. These methods do not consider the compositionality or role-differences of the action elements, or the interactions among them and the observation. Therefore, their modeling of the action values is less accurate and less data-efficient.   The second challenge is 	extbf{partial observability}.  At each game-playing step, the agent receives a textual observation describing the locations, objects, and characters of the game world.  But the latest observation is often not a sufficient summary of the interaction history and may not provide enough information to determine the long-term effects of actions.  Previous approaches address this problem by building a representation over past observations . These methods treat the historical observations equally and summarize the information into a single vector without focusing on important contexts related to the action prediction for the current observation. Therefore, their usages of history also bring noise, and the improvement is not always significant.  We propose a novel formulation of IF game playing as Multi-Passage Reading Comprehension  and harness MPRC techniques to solve the huge action space and partial observability challenges. The graphical illustration is shown in Figure.  First, the action value prediction  is essentially generating and scoring a compositional action structure by finding supporting evidence from the observation. We base on the fact that each action is an instantiation of a template, i.e., a verb phrase with a few placeholders of object arguments it takes~. Then the action generation process can be viewed as extracting objects for a template's placeholders from the textual observation, based on the interaction between the template verb phrase and the relevant context of the objects in the observation. Our approach addresses the structured prediction and interaction problems with the idea of context-question attention mechanism in RC models.  Specifically, we treat the observation as a passage and each template verb phrase as a question.  The filling of object placeholders in the template thus becomes an extractive QA problem that selects objects from the observation given the template. Simultaneously each action  gets its evaluation value predicted by the RC model. Our formulation and approach better capture the fine-grained interactions between observation texts and structural actions, in contrast to previous approaches that represent the observation as a single vector and ignore the fine-grained dependency among action elements.  Second, alleviating partial observability is essentially enhancing the current observation with potentially relevant history and predicting actions over the enhanced observation. Our approach retrieves potentially relevant historical observations with an object-centric approach  , so that the retrieved ones are more likely to be connected to the current observation as they describe at least one shared interactable object. Our attention mechanisms are then applied across the retrieved multiple observation texts to focus on informative contexts for action value prediction.   We evaluated our approach on the suite of Jericho IF games, compared to all previous approaches. Our approaches achieved or outperformed the state-of-the-art performance on 25 out of 33 games, trained with less than one-tenth of game interaction data used by prior art.  We also provided ablation studies on our models and retrieval strategies.     %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
"," Interactive Fiction  games with real human-written natural language texts provide a new natural evaluation for language understanding techniques.  In contrast to previous text games with mostly synthetic texts, IF games pose language understanding challenges on the human-written textual descriptions of diverse and sophisticated game worlds and language generation challenges on the action command generation from less restricted combinatorial space. We take a novel perspective of IF game solving and re-formulate it as Multi-Passage Reading Comprehension  tasks. Our approaches utilize the context-query attention mechanisms and the structured prediction in MPRC to efficiently generate and evaluate action outputs and apply an object-centric historical observation retrieval strategy to mitigate the partial observability of the textual observations.  Extensive experiments on the recent IF benchmark  demonstrate clear advantages of our approaches achieving high winning rates and low data requirements compared to all previous approaches.\footnote{Source code is available at: \url{https://github.com/XiaoxiaoGuo/rcdqn}. }",27
"   Recent advances in self-supervised pre-training have resulted in impressive downstream performance on several NLP tasks. However, this has led to the development of enormous models, which often require days of training on non-commodity hardware . Furthermore, studies have shown that it is quite challenging to successfully train these large Transformer models, requiring complicated learning schemes and extensive hyperparameter tuning.  Despite these expensive training regimes, recent studies have found that once trained, these bi-directional language models exhibit simple patterns of self-attention without much linguistic backing. For example, 40\% of heads in a pre-trained BERT model simply pay attention to delimiters added by the tokenizer . Since these attention patterns are independent of linguistic phenomena, a natural question arises: can Transformer models be guided towards such attention patterns without requiring extensive training?    In this paper, we propose an attention guidance  mechanism for self-attention modules in Transformer architectures to enable faster, more efficient, and robust self-supervised learning. Our approach is simple and agnostic to the training objective. Specifically, we introduce an auxiliary loss function to guide the self-attention heads in each layer towards a set of pre-determined patterns . These patterns encourage the formation of both  global  and local  structures in the model.   Through several experiments, we show that our approach enables training large Transformer models considerably faster 閳 for example, we can train a 16-layer RoBERTa model with SOTA performance on a low-resource domain in just two days using four GPUs, while excluding our loss leads to slow or no convergence. Our method also achieves competitive performance with BERT on three English natural language understanding tasks, and outperforms the baseline masked language modeling  models on eleven out of twelve settings considered.  Further, we also show that our initialization is agnostic to the training objective by demonstrating gains on the replaced token detection objective proposed by ELECTRA and on machine translation with Transformers. Finally, we provide an analysis of the attention heads learned using our method. Surprisingly, contrary to recent studies, we find that it is possible to train models that perform well on language modeling without learning a single attention head that models coreferences. % . For example, our model fails the co-reference test in  while still performing well on language modeling and downstream tasks.  To summarize, our main contributions are: 
"," % Despite being successful in downstream language understanding tasks, modern language models contain millions of parameters and require multiple days of training on specialized hardware such as TPUs. Training such models on commodity hardware  often means slow convergence, making it practically intractable for many researchers.  In this paper, we propose a simple and effective technique to allow for efficient self-supervised learning with bi-directional Transformers. Our approach is motivated by recent studies demonstrating that self-attention patterns in trained models contain a majority of non-linguistic regularities. We propose a computationally efficient auxiliary loss function to guide attention heads to conform to such patterns. Our method is agnostic to the actual pre-training objective and results in faster convergence of models as well as better performance on downstream tasks compared to the baselines, achieving state of the art results in low-resource settings. Surprisingly, we also find that linguistic properties of attention heads are not necessarily correlated with language modeling performance.\footnote{Code: \href{https://github.com/ameet-1997/AttentionGuidance}{https://github.com/ameet-1997/AttentionGuidance}}",28
" %  % Transformer models  have outperformed previously used RNN based models and traditional statistical MT techniques.  This improvement, though, comes at the cost of higher computation complexity. The decoder computation is sequential and becomes the bottleneck due to the autoregressive nature, large depth and self-attention structure.   % Another recent trend has been making the models larger and ensembling multiple models to achieve the best possible translation quality . Leading solutions on common benchmark  usually use an ensemble of Transformer big models, which combined can have more than 1 billion parameters.   % In this paper, we focus on developing architectures which are faster during inference and have less number of parameters, without sacrificing translation quality.  % Recent work \citet{ludicrously:kim2019} proposed methods to replace self-attention in the decoder with simpler simple recurrent units  and used knowledge distillation to simplify training for the final architecture. \citet{deepencoder} also proposed to make the decoder lightweight by training a deep-encoder, shallow decoder architecture. Another line of effort to make NMT architectures more efficient is pruning different components of the model. \citet{prune_voita-etal-2019-analyzing} and \citet{prune_michel:NIPS2019_9551} show that most of the attention heads in the network learn redundant information and can be pruned away.  % All of the above works use the vanilla Transformer architecture as their baseline, so it is not clear if these approaches can give complimentary results when combined together. In this work, we explore and benchmark combining all of the above techniques, with the goal of maximizing inference speed without hurting in translation quality. % %We adapt the same approach and  extend it with the following ideas. First, we optimized the SSRU to make it more efficient. Second, we removed the feed-forward network in the decoder completely. Then, we kept only 1 layer in the decoder and used very deep encoder. Last we pruned all the redundant heads in the deep encoder.  % After carefully stacking the approaches, our proposed architecture is able to achieve a significant speed improvement of 84\% on GPU and 102\% on CPU architectures without any degradation of translation quality in terms of BLEU.  % %%%%%%%% original Related Work %%%%%%%%% % 
"," Large Transformer models have achieved state-of-the-art results in neural machine translation and have become standard in the field. In this work, we look for the optimal combination of known techniques to optimize inference speed without sacrificing translation quality. We conduct an empirical study that stacks various approaches and demonstrates that combination of replacing decoder self-attention with simplified recurrent units, adopting a deep encoder and a shallow decoder architecture and multi-head attention pruning can achieve up to $109$\% and $84$\% speedup on CPU and GPU respectively and reduce the number of parameters by $25$\% while maintaining the same translation quality in terms of BLEU. %State-of-the-art neural machine translation has become compute and parameter intensive in the last several years, which puts significant pressure on the latency and hardware resources during inference. In this paper, we change the standard Transformer architecture to reduce the number of parameters and increase inference speed without sacrificing translation quality. We demonstrate that combination of replacing decoder self-attention with the simpler simple recurrent units, adopting a deep encoder and shallow decoder architecture, and multi-head attention pruning, we can achieve up to 102\% speedup and reduce the number of parameters by 13\% while maintaining the same translation quality in terms of BLEU.",29
" Intent Detection  is a crucial task in natural language understanding, whose objective is to extract underlying intents behind the given utterances. The extracted intents could provide further contexts for further downstream Natural Language Processing tasks such as dialogue state tracking or question answering. Unlike traditional text classification, ID is challenging for two main reasons  Utterances are usually short and diversely expressed,  Emerging intents occur continuously, especially across different domains .  Despite recent advances, state-of-the-art ID methods  require a large amount of annotated data to achieve competitive performance. This requirement inhibits models' capability in generalizing to newly emerging intents with no or limited annotations during inference. Re-training or fine-tuning large models on few samples of emerging classes could easily lead to overfitting problems.      Motivated by human capability in correctly categorizing new classes with only a few examples , few-shot learning  paradigms are adopted to tackle the scarcity problems of emerging classes. FSL methods take advantage of a small set of labeled examples  to learn how to discriminate unlabeled samples  between classes, even those not seen during training.  Recent works in FSL  focus on learning the matching information between the labeled samples  and the unlabeled samples  to provide additional contextual information for instance-level representations, leading to effective prototype representation. However, these methods only extract similarity based on fine-grained word semantics, failing to capture the diverse expressions of users' utterances. This problem could further lead to overfitting either to seen intents or novel intents, especially in the challenging Generalized Few-shot Intent Detection  setting  where both seen and novel intents are existent in a joint label space during inference. Instead, matching support and query samples on coarser-grained semantic components could provide additional informative contexts beyond word levels. For instance, two utterances ""i need to get a table at a pub with southeastern cuisine"" and ``book a spot for six friends"" share a similar intent label ``Book Restaurant"". While word-level semantics might find similar action words as ``get"" and ``book"", these words do not necessarily contribute to the correct intent findings. Instead, coarser-grained semantics such as ``get a table"" and ``book a spot"" could provide further hints to identify ``Book Restaurant"" intent.      As semantic components  could be effectively extracted from multi-head self-attention, matching these SC between support and query can enhance both query and support representations, leading to improvements in generalization from seen training classes to unseen testing classes. To further enhance the dynamics of extracted SC across various domains and diversely expressed utterances, we introduce additional head regularizations. In addition, to overcome the insufficiency of a single similarity measure for matching sentences with diverse semantics, a more comprehensive matching method is further explored.      Our main contribution is summarized as follows:   
"," Few-shot Intent Detection is challenging due to the scarcity of available annotated utterances. Although recent works demonstrate that multi-level matching plays an important role in transferring learned knowledge from seen training classes to novel testing classes, they rely on a static similarity measure and overly fine-grained matching components. These limitations inhibit generalizing capability towards Generalized Few-shot Learning settings where both seen and novel classes are co-existent. In this paper, we propose a novel Semantic Matching and Aggregation Network where semantic components are distilled from utterances via multi-head self-attention with additional dynamic regularization constraints. These semantic components capture high-level information, resulting in more effective matching between instances. Our multi-perspective matching method provides a comprehensive matching measure to enhance representations of both labeled and unlabeled instances. We also propose a more challenging evaluation setting that considers classification on the joint all-class label space. Extensive experimental results demonstrate the effectiveness of our method. Our code and data are publicly available \footnote{\url{https://github.com/nhhoang96/Semantic\_Matching}} .",30
" Multilingual Neural Machine Translation , which leverages a single NMT model to handle the translation of multiple languages, has drawn research attention in recent years. MNMT is appealing since it greatly reduces the cost of training and serving separate models for different language pairs. It has shown great potential in knowledge transfer among languages, improving the translation quality for low-resource and zero-shot language pairs.  Previous works on MNMT has mostly focused on model architecture design with different strategies of parameter sharing or representation sharing. Existing MNMT systems mainly rely on bitext training data, which is limited and costly to collect. Therefore, effective utilization of monolingual data for different languages is an important research question yet is less studied for MNMT.  Utilizing monolingual data  has been widely explored in various NMT and natural language processing  applications. Back translation , which leverages a target-to-source model to translate the target-side monolingual data into source language and generate pseudo bitext, has been one of the most effective approaches in NMT. However, well trained NMT models are required to generate back translations for each language pair, it is computationally expensive to scale in the multilingual setup. Moreover, it is less applicable to low-resource language pairs without adequate bitext data. Self-supervised pre-training approaches, which train the model with denoising learning objectives on the large-scale monolingual data, have achieved remarkable performances in many NLP applications. However, catastrophic forgetting effect, where finetuning on a task leads to degradation on the main task, limits the success of continuing training NMT on models pre-trained with monolingual data. Furthermore, the separated pre-training and finetuning stages make the framework less flexible to introducing additional monolingual data or new languages into the MNMT system.  In this paper, we propose a multi-task learning  framework to effectively utilize monolingual data for MNMT. Specifically, the model is jointly trained with translation task on multilingual parallel data and two auxiliary tasks: masked language modeling  and denoising auto-encoding  on the source-side and target-side monolingual data respectively. We further present two simple yet effective scheduling strategies for the multilingual and multi-task framework. In particular, we introduce a dynamic temperature-based sampling strategy for the multilingual data. To encourage the model to keep learning from the large-scale monolingual data, we adopt dynamic noising ratio for the denoising objectives to gradually increase the difficulty level of the tasks.   We evaluate the proposed approach on a large-scale multilingual setup with  language pairs from the WMT datasets. We study three English-centric multilingual systems, including many-to-English, English-to-many, and many-to-many. We show that the proposed MTL approach significantly boosts the translation quality for both high-resource and low-resource languages. Furthermore, we demonstrate that MTL can effectively improve the translation quality on zero-shot language pairs with no bitext training data. In particular, MTL achieves even better performance than the pivoting approach for multiple low-resource language pairs. We further show that MTL outperforms pre-training approaches on both NMT tasks as well as cross-lingual transfer learning for NLU tasks, despite being trained on very small amount of data in comparison to pre-training approaches.  The contributions of this paper are as follows. First, we propose a new MTL approach to effectively utilize monolingual data for MNMT. Second, we introduce two simple yet effective scheduling strategies, namely the dynamic temperature-based sampling and dynamic noising ratio strategy. Third, we present detailed ablation studies to analyze various aspects of the proposed approach. Finally, we demonstrate for the first time that MNMT with MTL models can be effectively used for cross-lingual transfer learning for NLU tasks with similar or better performance than the state-of-the-art massive scale pre-trained models using single task.   
"," While monolingual data has been shown to be useful in improving bilingual neural machine translation , effectively and efficiently leveraging monolingual data for Multilingual NMT  systems is a less explored area. In this work, we propose a multi-task learning  framework that jointly trains the model with the translation task on bitext data and two denoising tasks on the monolingual data. We conduct extensive empirical studies on MNMT systems with $10$ language pairs from WMT datasets. We show that the proposed approach can effectively improve the translation quality for both high-resource and low-resource languages with large margin, achieving significantly better results than the individual bilingual models. We also demonstrate the efficacy of the proposed approach in the zero-shot setup for language pairs without bitext training data. Furthermore, we show the effectiveness of MTL over pre-training approaches for both NMT and cross-lingual transfer learning NLU tasks; the proposed approach outperforms massive scale models trained on single task.",31
"  Neural machine translation  is a data-hungry approach, which requires a large amount of data to train a well-performing NMT model. However, the complex patterns and potential noises in the large-scale data make training NMT models difficult.  To relieve this problem, several approaches have been proposed to better exploit the training data, such as curriculum learning, data diversification, and data denoising.  In this paper, we explore an interesting alternative which is to reactivate the inactive examples in the  training data for NMT models. By definition, inactive examples are the training examples that only marginally contribute to or even inversely harm the performance of NMT models.  Concretely, we use sentence-level output probability assigned by a trained NMT model to measure the activeness level of training examples, and regard the examples with the least probabilities as inactive examples . Experimental results show that removing 10\% most inactive examples can marginally improve translation performance. In addition, we observe a high overlapping ratio  of the most inactive and active examples across random seeds, model capacity, and model architectures . These results provide empirical support for our hypothesis of the existence of inactive examples in large-scale datasets, which is invariant to specific NMT models and depends on the data distribution itself.  We further propose data rejuvenation to rejuvenate the inactive examples to improve the performance of NMT models.  Specifically, we train an NMT model on the active examples as the rejuvenation model to re-label the inactive examples, resulting in the rejuvenated examples~. The final NMT model is trained on the combination of the active examples and rejuvenated examples. Experimental results show that the data rejuvenation approach consistently and significantly improves performance on SOTA NMT models  on the benchmark WMT14 English-German and English-French datasets~. Encouragingly, our approach is also complementary to existing data manipulation methods , and combining them can further improve performance.      Finally, we conduct extensive analyses to better understand the inactive examples and the proposed data rejuvenation approach. Quantitative analyses reveal that the inactive examples are more difficult to learn than active ones, and rejuvenation can reduce the learning difficulty~. The rejuvenated examples stabilize and accelerate the training process of NMT models~, resulting in final models with better generalization capability~.  Our contributions of this work are as follows:      
"," Large-scale training datasets lie at the core of the recent success of neural machine translation  models. However, the complex patterns and potential noises in the large-scale data make training NMT models difficult. In this work, we explore to identify the inactive training examples which contribute less to the model performance, and show that the existence of inactive examples depends on the data distribution. We further introduce data rejuvenation to improve the training of NMT models on large-scale datasets by exploiting inactive examples. The proposed framework consists of three phases.  First, we train an identification model on the original training data, and use it to distinguish inactive examples and active examples by their sentence-level output probabilities. Then, we train a rejuvenation model on the active examples, which is used to re-label the inactive examples with forward-translation. Finally, the rejuvenated examples and the active examples are combined to train the final NMT model. Experimental results on WMT14 English-German and English-French datasets show that the proposed data rejuvenation consistently and significantly improves performance for several strong NMT models. Extensive analyses reveal that our approach stabilizes and accelerates the training process of NMT models, resulting in final models with better generalization capability.}  %In this work, we propose to improve the training of NMT models on large-scale datasets by exploiting inactive training examples, which contribute less to the model performance. Specifically, the proposed framework consists of three phases. First, we identify the inactive examples with their sentence-level prediction confidence assigned by an identification model trained on the original training data. Then, we train a rejuvenation model on the active examples, which is used to re-label the inactive examples with forward-translation. Finally, the rejuvenated examples and the active examples are combined to train the final NMT model. Experimental results on WMT14 English-German and English-French datasets show that the proposed data rejuvenation consistently and significantly improves performance for several strong NMT models. Extensive analyses reveal that our approach stabilizes and accelerates the training process of NMT models, resulting in final models with better generalization capability.",32
"  The following instructions are directed to authors of papers submitted to EMNLP 2020 or accepted for publication in its proceedings. All authors are required to adhere to these specifications. Authors are required to provide a Portable Document Format  version of their papers. The proceedings are designed for printing on A4 paper.   
"," This document contains the instructions for preparing a manuscript for the proceedings of EMNLP 2020. The document itself conforms to its own specifications, and is therefore an example of what your manuscript should look like. These instructions should be used for both papers submitted for review and for final versions of accepted papers. Authors are asked to conform to all the directions reported in this document.",33
" Modern neural machine translation~ models employ sufficient capacity to fit the massive data well by utilizing a large number of parameters, and suffer from the widely recognized issue, namely, over-parameterization. For example,  showed that over 40\% of the parameters in an RNN-based NMT model can be pruned with negligible performance loss. However, the low utilization efficiency of parameters results in a waste of computational resources , as well as renders the model stuck in a local optimum.   In response to the over-parameterization issue, network pruning has been widely investigated for both computer vision   and natural language processing  tasks . Recent work has proven that such spare parameters can be reused to maximize the utilization of models in CV tasks such as image classification. The leverage of parameter rejuvenation in sequence-to-sequence learning, however, has received relatively little attention from the research community. In this paper, we empirically study the efficiency issue for NMT models.  Specifically, we first investigate the effects of weight pruning on advanced Transformer models, showing that 20\% parameters can be directly pruned, and by continuously training the sparse networks, we can prune 50\% with no performance loss. Starting from this observation, we then exploit whether these redundant parameters are able to be re-utilized for improving the performance of NMT models. Experiments are systematically conducted on different datasets  and NMT architectures . Results demonstrate that the rejuvenation approach can significantly and consistently improve the translation quality by up to +0.8 BLEU points. Further analyses reveal that the rejuvenated parameters are reallocated to enhance the ability to model the source-side low-level information, lacking of which leads to a number of problems in NMT models.  \paragraph{Contributions} Our key contributions are:   
"," Modern neural machine translation  models employ a large number of parameters, which leads to serious over-parameterization and typically causes the underutilization of computational resources. In response to this problem, we empirically investigate whether the redundant parameters can be reused to achieve better performance. Experiments and analyses are systematically conducted on different datasets and NMT architectures. We show that: 1) the pruned parameters can be rejuvenated to improve the baseline model by up to +0.8 BLEU points; 2) the rejuvenated parameters are reallocated to enhance the ability of modeling low-level lexical information.",34
"  Sentiment analysis  has attracted increasing attention recently. Aspect-based sentiment analysis   is a fine-grained sentiment analysis task and includes many subtasks, two of which are aspect category detection  that detects the aspect categories mentioned in a sentence and aspect-category sentiment analysis  that predicts the sentiment polarities with respect to the detected aspect categories. Figure shows an example. ACD detects the two aspect categories, ambience and food, and ACSA predicts the negative and positive sentiment toward them respectively. In this work, we focus on ACSA, while ACD as an auxiliary task is used to find the words indicating the aspect categories in sentences for ACSA.    Since a sentence usually contains one or more aspect categories, previous studies have developed various methods for generating aspect category-specific sentence representations to detect the sentiment toward a particular aspect category in a sentence. To name a few, attention-based models  allocate the appropriate sentiment words for the given aspect category. \citet{xue2018aspect} proposed to generate aspect category-specific representations based on convolutional neural networks and gating mechanisms. Since aspect-related information may already be discarded and aspect-irrelevant information may be retained in an aspect independent encoder, some existing methods  utilized the given aspect to guide the sentence encoding from scratch. Recently, BERT based models  have obtained promising performance on the ACSA task. However, these models ignored that the sentiment of an aspect category mentioned in a sentence is an aggregation of the sentiments of the words indicating the aspect category. It leads to suboptimal performance of these models. For the example in Figure, both ``drinks'' and ``food'' indicate the aspect category food. The sentiment about food is a combination of the sentiments of ``drinks'' and ``food''. Note that, words indicating aspect categories not only contain aspect terms explicitly indicating an aspect category but also contain other words implicitly indicating an aspect category . In Figure, while ``drinks'' and ``food'' are aspect terms explicitly indicating the aspect category food, ``large'' and ``noisy'' are not aspect terms implicitly indicating the aspect category ambience.  In this paper, we propose a Multi-Instance Multi-label Learning Network for Aspect-Category sentiment analysis . AC-MIMLLN explicitly models the fact that the sentiment of an aspect category mentioned in a sentence is an aggregation of the sentiments of the words indicating the aspect category. Specifically, AC-MIMLLN treats sentences as bags, words as instances, and the words indicating an aspect category as the key instances  of the aspect category. Given a bag and the aspect categories mentioned in the bag, AC-MIMLLN first predicts the instance sentiments, then finds the key instances for the aspect categories, finally aggregates the sentiments of the key instances to get the bag-level sentiments of the aspect categories.  Our main contributions can be summarized as follows:   
"," 	Aspect-category sentiment analysis  aims to predict sentiment polarities of sentences with respect to given aspect categories. To detect the sentiment toward a particular aspect category in a sentence, most previous methods first generate an aspect category-specific sentence representation for the aspect category, then predict the sentiment polarity based on the representation. These methods ignore the fact that the sentiment of an aspect category mentioned in a sentence is an aggregation of the sentiments of the words indicating the aspect category in the sentence, which leads to suboptimal performance. In this paper, we propose a Multi-Instance Multi-Label Learning Network for Aspect-Category sentiment analysis , which treats sentences as bags, words as instances, and the words indicating an aspect category as the key instances of the aspect category. Given a sentence and the aspect categories mentioned in the sentence, AC-MIMLLN first predicts the sentiments of the instances, then finds the key instances for the aspect categories, finally obtains the sentiments of the sentence toward the aspect categories by aggregating the key instance sentiments. Experimental results on three public datasets demonstrate the effectiveness of AC-MIMLLN \footnote{Data and code are available at https://github.com/l294265421/AC-MIMLLN}.",35
" The recent success of the language model pre-training approaches~, which train language models on diverse text corpora with self-supervised or multi-task learning, have brought up huge performance improvements on several natural language understanding  tasks~. The key to this success is their ability to learn generalizable text embeddings that achieve near optimal performance on diverse tasks with only a few additional steps of fine-tuning on each downstream task.    Most of the existing works on language model aim to obtain a universal language model that can address nearly the entire set of available natural language tasks on heterogeneous domains. Although this train-once and use-anywhere approach has been shown to be helpful for various natural language tasks~, there have been considerable needs on adapting the learned language models to domain-specific corpora . Such domains may contain new entities that are not included in the common text corpora, and may contain only a small amount of labeled data as obtaining annotation on them may require expert knowledge.  Some recent works~ suggest to further pre-train the language model with self-supervised tasks on the domain-specific text corpus for adaptation, and show that it yields improved performance on tasks from the target domain.  Masked Language Models  objective in BERT~ has shown to be effective for the language model to learn the knowledge of the language in a bi-directional manner~. In general, masks in MLMs are sampled at random~, which seems reasonable for learning a generic language model pre-trained from scratch, since it needs to learn about as many words in the vocabulary as possible in diverse contexts.  However, in the case of further pre-training of the already pre-trained language model, such a conventional selection method may lead a domain adaptation in an inefficient way, since not all words will be equally important for the target task. Repeatedly learning for uninformative instances thus will be wasteful. Instead, as done with instance selection, it will be more effective if the masks focus on the most important words for the target domain, and for the specific NLU task at hands. How can we then obtain such a masking strategy to train the MLMs?   Several works~ propose rule-based masking strategies which work better than random masking ~ when applied to language model pre-training from scratch. Based on those works, we assume that adaptation of the pre-trained language model can be improved via a learned masking policy which selects the words to mask. Yet, existing models are inevitably suboptimal since they do not consider the target domain and the task. To overcome this limitation, in this work, we propose to adaptively generate mask by learning the optimal masking policy for the given task, for the task-adaptive pre-training~ of the language model.  As described in Figure , we want to further pre-train the language model on a specific task with a task-dependent masking policy, such that it directs the solution to the set of parameters that can better adapt to the target domain, while task-agnostic random policy leads the model to an arbitrary solution.  To tackle this problem, we pose the given learning problem as a meta-learning problem where we learn the task-adaptive mask-generating policy, such that the model learned with the masking strategy obtains high accuracy on the target task.  We refer to this meta-learner as the Neural Mask Generator . Specifically, we formulate mask learning as a bi-level problem where we pre-train and fine-tune a target language model in the inner loop, and learn the NMG at the outer loop, and solve it using renforcement learning. We validate our method on diverse NLU tasks, including question answering and text classification. The results show that the models trained using our NMG outperforms the models pre-trained using rule-based masking strategies, as well as finds a proper adaptive masking strategy for each domain and task.  Our contribution is threefold:   
"," We propose a method to automatically generate a domain- and task-adaptive maskings of the given text for self-supervised pre-training, such that we can effectively adapt the language model to a particular target task . Specifically, we present a novel reinforcement learning-based framework which learns the masking policy, such that using the generated masks for further pre-training of the target language model helps improve task performance on unseen texts. We use off-policy actor-critic with entropy regularization and experience replay for reinforcement learning, and propose a Transformer-based policy network that can consider the relative importance of words in a given text. We validate our Neural Mask Generator  on several question answering and text classification datasets using BERT and DistilBERT as the language models, on which it outperforms rule-based masking strategies, by automatically learning optimal adaptive maskings. \footnote{Code is available at \url{github.com/Nardien/NMG}.}",36
"   Sentiment analysis has become an increasingly popular natural language  processing  task in academia and industry.  It provides real-time  feedback on consumer experience and their needs, which helps  producers to offer better services.  To deal with the presence of  multiple categories in one document,  ACSA tasks, including aspect-category  sentiment analysis  and targeted aspect-category sentiment analysis , were introduced.   The main purpose for ACSA task  is to identify sentiment polarity  of an input sentence upon specific predefined categories . For  example, as shown in Table , giving an input sentence ``Food is  always fresh and hot-ready to eat, but it is too expensive."" and predefined categories \{food, service, price,  ambience and anecdotes/miscellaneous\},  the sentiment of category food is positive, the polarity  regarding to category price is negative, while is none for others.  In this task, the models should  capture both explicit expressions and implicit expressions. For example, the phrase ``too expensive"" indicates the  negative polarity  in the price category, without a direct indication of ``price"".    In order to  deal with ACSA with both multiple categories and multiple targets, TACSA task was introduced  to analyze sentiment polarity on a set of predefined target-category pairs. An example is shown in Table , given targets ``restaurant-1"" and ``restaurant-2"", in the case ``I like  restaurant-1 because it's cheap, but restaurant-2 is too  expansive"", the category price for target ``restaurant-1"" is positive, but is  negative for target ``restaurant-2"", while is none for other target-category pairs. A mathematical definition for ACSA is given  as follows: giving a  sentence  as input, a predefined set of targets  and a predefined set of  aspect categories , a model predicts the sentiment polarity  for  each target-category pair . For ACSA  task, there is only one target  in all  categories. In this paper, in order to simplify the expression in TACSA, we use predefined categories, which is short for predefined target-category pairs.   	} 	 \end{table*}  Multi-task learning, with shared encoders but individual decoders for each category, is an approach to analyze all the categories in one sample simultaneously for ACSA . Compared with single-task ways , multi-task approaches utilize category-specific knowledge in training signals from each task and get better performance. However, current multi-task models still suffer from a lack of  features such as category name . Models with category name features encoded in the model may further improve the performance.  On the other hand, the predefined categories in ACSA task make the application  in new categories inflexible, as for ACSA applications, the number of categories maybe  varied over time.  For example, fuel consumption, price level, engine power, space and so  on are source categories to be analyzed in the gasoline automotive domain. For  electromotive domain, source categories in the automotive domain will still be used, while new target category such as battery duration should also be analyzed.  Incremental learning is a way to solve this problem. Therefore, it is necessary to propose an  incremental learning task and an incremental learning model concerned with new  category for ACSA tasks.  Unfortunately, in the current multi-task learning ACSA models, the encoder is shared but the decoders for each category are individual. This parameter sharing mechanism results in only the shared encoder  and target-category-related decoders are finetuned during the finetuning process, while the decoder of source categories remains unchanged. The finetuned encoder and original decoder of source categories may cause catastrophic forgetting problem in the origin  categories. For real applications, high accuracy is excepted in source  categories and target  categories.  Based on the previous researches that decoders between different tasks are usually modeled by mean regularization   , an idea comes up to further make the decoders the same by sharing the decoders in all categories to decrease the catastrophic forgetting problem. But here raises another question, how to identify each category in the encoder and decoder shared network? In our approach, we  solve the category discrimination problem by the input category name feature.   In this paper,  we proposed a multi-task category name embedding network  .  The multi-task learning  framework makes full use of training signals from all categories. To make it feasible for incremental learning, both encoder and decoders for each category are shared. The category names were applied as another input feature for task discrimination. We also present a new task for ACSA incremental learning. In particular,  our contribution is three-folded:    We proposed a multi-task CNE-net framework with both encoder and decoder shared to weaken catastrophic forgetting problem in multi-task learning ACSA model.     We achieved  state-of-the-art on the two ACSA datasets, SemEval14-Task4  and Sentihood.   We proposed a new task for incremental learning in ACSA. By sharing both encoder layers and decoder layers of all the tasks, we   achieved better results compared with other baselines both in source  categories and in the target category.   
"," ACSA tasks, including aspect-category sentiment analysis  and  targeted  aspect-category sentiment analysis , aims at identifying sentiment  polarity on predefined categories. Incremental learning on new categories is necessary for ACSA real applications. Though current multi-task learning models achieve good performance in ACSA tasks, they suffer from catastrophic forgetting problems in ACSA incremental learning tasks. In this paper, to make multi-task learning feasible for incremental learning, we proposed Category Name  Embedding network  . We set both encoder and decoder shared among all categories to weaken the catastrophic forgetting problem. Besides the origin input sentence, we applied another input feature, i.e., category name, for task discrimination.  Our model achieved state-of-the-art  on two ACSA benchmark datasets. Furthermore, we proposed  a dataset for ACSA incremental learning and achieved the best performance compared with other strong baselines.",37
"   Conditional random fields  have been shown to perform well in various sequence labeling tasks. Recent work uses rich neural network architectures to define the ``unary'' potentials, i.e., terms that only consider a single position's label at a time~. However, ``binary'' potentials, which consider pairs of adjacent labels, are usually quite simple and may consist solely of a parameter or parameter vector for each unique label transition. Models with unary and binary potentials are generally referred to as ``first order'' models.   A major challenge with CRFs is the complexity of training and inference, which are quadratic in the number of output labels for first order models and grow exponentially when higher order dependencies are considered. This explains why the most common type of CRF used in practice is a first order model, also referred to as a ``linear chain'' CRF.   One promising alternative to CRFs is structured prediction energy networks , which use deep neural networks to parameterize arbitrary potential functions for structured prediction. While SPENs also pose challenges for learning and inference, \citet{tu-18} proposed a way to train SPENs jointly with ``inference networks'', neural networks trained to approximate structured  inference.   In this paper, we leverage the frameworks of SPENs and inference networks to explore high-order energy functions for sequence labeling. Naively instantiating high-order energy terms can lead to a very large number of parameters to learn, so we instead develop concise neural parameterizations for high-order terms. In particular, we draw from vectorized Kronecker products, convolutional networks, recurrent networks, and self-attention.  We also consider ``skip-chain'' connections~ with various skip distances and ways of reducing their total parameter count for increased learnability.   Our experimental results on four sequence labeling tasks show that a range of high-order energy functions can yield performance improvements. While the optimal energy function varies by task, we find strong performance from skip-chain terms with short skip distances, convolutional networks with filters that consider label trigrams, and recurrent networks and self-attention networks that consider large subsequences of labels.     We also demonstrate that modeling high-order dependencies can lead to significant performance improvements in the setting of noisy training and test sets.  Visualizations of the high-order energies show various methods capture intuitive structured dependencies among output labels.   Throughout, we use inference networks that share the same architecture as unstructured classifiers for sequence labeling, so test time inference speeds are unchanged between local models and our method.  Enlarging the inference network architecture by adding one layer leads consistently to better results, rivaling or improving over a BiLSTM-CRF baseline,  suggesting that training efficient inference networks with high-order energy terms can make up for errors arising from approximate inference. While we focus on sequence labeling in this paper, our results show the potential of developing high-order structured models for other NLP tasks in the future.   
"," Many tasks in natural language processing involve predicting structured outputs, e.g., sequence labeling, semantic role labeling, parsing, and machine translation. Researchers are increasingly applying deep representation learning to these problems, but the structured component of these approaches is usually quite simplistic. In this work, we propose several high-order energy terms to capture complex dependencies among labels in sequence labeling, including several that consider the entire label sequence. We use neural parameterizations for these energy terms, drawing from convolutional, recurrent, and self-attention networks. We use the framework of learning energy-based inference networks for dealing with the difficulties of training and inference with such models. We empirically demonstrate that this approach achieves substantial improvement using a variety of high-order energy terms on four sequence labeling tasks, while having the same decoding speed as simple, local classifiers.  We also find high-order energies to help in noisy data conditions.\footnote{Code  is available at \url{https://github.com/tyliupku/Arbitrary-Order-Infnet}}",38
" Long document coreference resolution poses runtime and memory challenges. Current best models % for coreference resolution have large memory requirements and quadratic runtime in the document length~, making them impractical for long documents. %  Recent work revisiting the entity-mention paradigm~, which seeks to maintain explicit representations only of entities, rather than all their constituent mentions, has shown practical benefits for memory while being competitive with state-of-the-art models~. In particular, unlike other approaches to coreference resolution which maintain representations of both mentions and their corresponding entity clusters~ , the entity-mention paradigm stores representations only of the entity clusters, which are updated incrementally as coreference predictions are made. While such an approach requires less memory than those that additionally store mention representations, the number of entities can be impractically large when processing long documents, making the storing of all entity representations problematic.  Is it necessary to maintain an unbounded number of mentions or entities?  Psycholinguistic evidence suggests it is not, as human language processing is incremental  and has limited working memory~. In practice, we find that most entities have a small spread , and thus do not need to be kept persistently in memory. This observation suggests that tracking a limited, small number of entities at any time can resolve the computational %  issues, albeit at a potential accuracy tradeoff.  Previous work on bounded memory models for coreference resolution has shown potential, but has been tested only on short documents  % . % Moreover, this previous work makes token-level predictions while standard coreference datasets have span-level annotations.  % We propose a bounded memory model that performs quasi-online coreference resolution,    
"," Long document coreference resolution remains a challenging task	 due to the large memory and runtime requirements of current models. Recent work doing incremental coreference resolution using just the global representation of entities shows practical benefits but requires keeping all entities in memory, which can be impractical for long documents. % We argue that keeping all entities in memory is unnecessary, and we propose a memory-augmented neural network that tracks only a small bounded number of entities at a time, thus guaranteeing a linear runtime in length of document. We show that  the model remains competitive with models with high memory and computational requirements on OntoNotes and LitBank, and  the model learns an efficient memory management strategy easily outperforming a rule-based strategy.",39
" Since the early days of NLP, conversational agents have been designed to interact with humans through language to solve diverse tasks, e.g., remote instructions or booking assistants . In this goal-oriented dialogue setting, the conversational agents are often designed to compose with predefined language utterances. Even if such approaches are efficient, they also tend to narrow down the agent's language diversity.  To remove this restriction, recent work has been exploring interactive word-based training. In this setting, the agents are generally trained through a two-stage process: Firstly, the agent is pretrained on a human-labeled corpus through supervised learning to generate grammatically reasonable sentences. Secondly, the agent is finetuned to maximize the task-completion score by interacting with a user. Due to sample-complexity and reproducibility issues, the user is generally replaced by a game simulator that may evolve with the conversational agent. Unfortunately, this pairing may lead to the language drift phenomenon, where the conversational agents gradually co-adapt, and drift away from the pretrained natural language. The model thus becomes unfit to interact with humans.  While domain-specific methods exist to counter language drift, a simple task-agnostic method consists of combining interactive and supervised training losses on a pretraining corpus, which was later formalized as Supervised SelfPlay  .   Inspired by language evolution and cultural transmission, recent work proposes Seeded Iterated Learning  as another task-agnostic method to counter language drift. SIL modifies the training dynamics by iteratively refining a pretrained student agent by imitating interactive agents, as illustrated in Figure. At each iteration, a teacher agent is created by duplicating the student agent, which is then finetuned towards task completion. A new dataset is then generated by greedily sampling the teacher, and those samples are used to refine the student through supervised learning. The authors empirically show that this iterated learning procedure induces an inductive learning bias that successfully maintains the language grounding while improving task-completion.       \vskip -1em \end{figure*}  As a first contribution, we further examine the performance of these two methods in the setting of a translation game.  We show that S2P is unable to maintain a high grounding score and experiences a late-stage collapse, while SIL has a higher negative likelihood when evaluated on human corpus.  We propose to combine SIL with S2P by applying an S2P loss in the interactive stage of SIL. We show that the resulting Supervised Seeded Iterated Learning  algorithm manages to get the best of both algorithms in the translation game. Finally, we observe that the late-stage collapse of S2P is correlated with conflicting gradients before showing that \algo empirically reduces this gradient discrepancy.    
"," Language drift has been one of the major obstacles to train language models through interaction. When word-based conversational agents are trained towards completing a task, they tend to invent their language rather than leveraging natural language. In recent literature, two general methods partially counter this phenomenon: Supervised Selfplay  and Seeded Iterated Learning .  While S2P jointly trains interactive and supervised losses to counter the drift, SIL changes the training dynamics to prevent language drift from occurring. In this paper, we first highlight their respective weaknesses, i.e., late-stage training collapses and higher negative likelihood when evaluated on human corpus. Given these observations, we introduce \longalgo~ to combine both methods to minimize their respective weaknesses.  We then show the effectiveness of \algo in the language-drift translation game.",40
"  Advances in pretraining language models   as general-purpose representations have pushed  the state of the art on a variety of natural language tasks. However, not all languages enjoy large public datasets for pretraining and/or downstream tasks. Multilingual language models such as mBERT  and XLM   have been proven effective for cross-lingual transfer learning  by pretraining a single shared Transformer model   jointly on multiple languages. The goals of multilingual modeling are not limited  to improving language modeling in low-resource languages ,  but also include zero-shot cross-lingual transfer on downstream tasks---it  has been shown that multilingual models can generalize to target languages  even when labeled training data is only available in the source language   on a wide range of tasks .   However, multilingual models are not equally beneficial for all languages. \citet{conneau2019unsupervised} demonstrated that including more languages in a single model  can improve performance for low-resource languages but hurt performance for high-resource languages. Similarly, recent work   in multilingual neural machine translation  also observed  performance degradation on high-resource language pairs. In multi-task learning , this phenomenon is known as negative interference or negative transfer ,  where training multiple tasks jointly hinders the performance on individual tasks. % In multilingual language modeling, each language is a single task and negative interference during pretraining can hurt the model's generalization on individual languages.  Despite these empirical observations, little prior work analyzed or showed  how to mitigate negative interference in multilingual language models. Particularly, it is natural to ask:  Can negative interference occur for low-resource languages also?  What factors play an important role in causing it?  Can we mitigate negative interference to improve the model's cross-lingual transferability?   In this paper, we take a step towards addressing these questions. We pretrain a set of monolingual and bilingual models and evaluate them  on a range of downstream tasks to analyze negative interference. We seek to individually characterize the underlying factors of negative interference  through a set of ablation studies and glean insights on its causes. Specifically, we examine if training corpus size and language similarity affect negative interference,  and also measure gradient and parameter similarities between languages.  Our results show that negative interference can occur in both high-resource and low-resource languages. In particular, we observe that neither subsampling the training corpus  nor adding typologically similar languages substantially impacts negative interference. On the other hand, we show that gradient conflicts  and language-specific parameters do exist in multilingual models,  suggesting that languages are fighting for model capacity, which potentially causes negative interference. We further test whether explicitly assigning language-specific modules  to each language can alleviate negative interference, and find that the resulting model performs better  within each individual language but worse on zero-shot cross-lingual tasks.  Motivated by these observations, we further propose  to meta-learn these language-specific parameters  to explicitly improve generalization of shared parameters on all languages. Empirically, our method improves not only within-language performance on monolingual tasks  but also cross-lingual transferability on zero-shot transfer benchmarks. To the best of our knowledge, this is the first work to systematically study  and remedy negative interference in multilingual language models.  % Advances in pretraining language models   % as general-purpose representations have pushed the state-of-the-art on a variety of natural language tasks. % However, not all languages have large amounts of training data for pretraining and/or downstream tasks. % Multilingual language models such as mBERT  and XLM  have been proven effective for cross-lingual transfer learning by pretraining a single shared Transformer model  jointly on multiple languages. % The goal is to not only improve language modeling in low-resource languages , but also enable zero-shot cross-lingual transfer on downstream tasks -- it has been shown that multilingual models can generalize to target languages when labeled training data is only available in the source language  on a wide range of tasks .   % However, multilingual models are not equally beneficial for all languages. % \citet{conneau2019unsupervised} demonstrated that including more languages in a single model can improve performance for low-resource languages but hurt performance for high-resource languages. % Similarly, recent work  in multilingual neural machine translation  also observed performance degradation on high-resource language pairs. % In multi-task learning , this phenomenon is known as negative interference or negative transfer ,  % where training multiple tasks jointly hinders the performance on individual tasks. % % In multilingual language modeling, each language is a single task and negative interference during pretraining can hurt the model's generalization on individual languages.  % Despite these empirical observations, little prior work analyzed or showed how to mitigate negative interference in multilingual language models. % Particularly, it is natural to ask: %  Can negative interference occur for low-resource languages also? %  What factors play an important role in causing it? %  Can we mitigate negative interference to improve the model's cross-lingual transferability?   % In this paper, we take a step towards addressing these questions. % We pretrain a set of monolingual and bilingual models, and evaluate them on a range of downstream tasks to analyze negative interference. % We seek to individually characterize the underlying factors of negative interference through a set of ablation studies and glean insights on its causes. % Specifically, we examine if training corpus size and language similarity affect negative interference, and also measure gradient and parameter similarities between languages.  % Our results show that negative interference can occur in both high-resource and low-resource languages. % In particular, we observe that subsampling the training corpus or adding typologically similar languages has little impact on negative interference. % On the other hand, we show that gradient conflicts and language-specific parameters do exist in multilingual models, suggesting that languages are fighting for model capacity which potentially causes negative interference. % Thus, we further test whether explicitly assigning language-specific modules to each language can alleviate negative interference.  % To our surprise, the model performs better within each individual language but worse on zero-shot cross-lingual tasks.  % Motivated by these observations, we further propose to meta-learn these language-specific parameters to explicitly improve generalization of shared parameters on all languages. % Empirically, our method improves not only within-language performance on monolingual tasks but also cross-lingual transferability on zero-shot transfer benchmarks. % To the best of our knowledge, this is the first work to systematically study and treat negative interference in multilingual language models.   
"," Modern multilingual models are trained on concatenated text  from multiple languages in hopes of conferring benefits to each , with the most pronounced benefits accruing to low-resource languages. However, recent work has shown that this approach can degrade  performance on high-resource languages,  a phenomenon known as negative interference. In this paper, we present the first systematic study of negative interference. We show that, contrary to previous belief,  negative interference  also impacts low-resource languages. While parameters are maximally shared to learn language-universal structures,  we demonstrate that language-specific parameters do exist in multilingual models and they are a potential cause of negative interference. Motivated by these observations,  we also present a meta-learning algorithm that obtains  better cross-lingual transferability  and alleviates negative interference,  by adding language-specific layers as meta-parameters  and training them in a manner that explicitly improves  shared layers' generalization on all languages. Overall, our results show that negative interference  is more common than previously known,  suggesting new directions for improving multilingual representations.\footnote{Source code is available at \url{https://github.com/iedwardwangi/MetaAdapter}.} %  % State-of-the-art multilingual models are trained on concatenated text from multiple languages to enable positive cross-lingual transfer, especially from high-resource languages to low-resource languages. % However, recent work found that such a training paradigm can degrade the model's performance on high-resource languages too, a phenomenon known as negative interference. % In this paper, we present the first systematic study of negative interference. % We show that, contrary to what was previously hypothesized, negative interference is not exclusive to high-resource but can also occur in low-resource settings. % In addition, despite that parameters are shared with the goal to learn language-universal structures, we demonstrate that language-specific parameters in multilingual models are a potential cause of negative interference. % Motivated by these observations, we show that we can obtain better cross-lingual transferability and alleviate negative interference through a meta-learning algorithm, which considers language-specific layers as meta parameters and trains them in the manner that explicitly improves the generalization of shared parameters across all languages. % Overall, our results show that negative interference occurs more commonly than previously believed and suggest a new direction towards improving multilingual representations by resolving language conflicts.\footnote{Code will be released upon publication.}",41
"  Event argument extraction  aims to identify the entities that serve as arguments of an event and to classify the specific roles they play. As in Fig., ``two soldiers'' and ``yesterday'' are arguments, where the event triggers are ``attacked''   and ``injured'' . For the trigger ``attacked'', ``two soldiers'' plays the argument role Target while ``yesterday'' plays the argument role Attack\_Time. For the event trigger ``injured'', ``two soldiers'' and ``yesterday'' play the role Victim and INJURY\_Time, respectively. There has been significant work on event extraction  , but the EAE task remains a challenge and has become the bottleneck for improving the overall performance of EE.\footnote{EAE has similarities with semantic role labeling. Event triggers are comparable to predicates in SRL and the roles in most SRL datasets have a standard convention of interpreting who did what to whom. EAE has a custom taxonomy of roles by domain. We also use inspiration from the SRL body of work .}     Supervised data for EAE is expensive and hence scarce. One possible solution is to use other available resources like unlabeled data. For that,  We use  BERT as our model encoder which leverages a much larger unannotated corpus where semantic information is captured. Unlike%previous studies ~ who added a final/prediction layer to BERT for argument extraction, we use BERT as token embedder and build a sequence of EAE task-specific components .  We use  in-domain data to adapt the BERT model parameters in a subsequent pretraining step as in . This makes the encoder domain-aware.  We perform self-training to construct auto-labeled data .  A crucial aspect for EAE is to integrate event trigger information into the learned representations. This is important because arguments are dependent on triggers, i.e., the same argument span plays completely different roles toward different triggers. An example is shown in Fig., where ``two soldiers'' plays the role Target for the event ATTACK and the role Victim for INJURY. Different from existing work that relies on regular sequence encoders, we design a novel trigger-aware encoder which simultaneously learns four different types of trigger-informed sequence representations. %for candidate arguments.   Capturing the long-range dependency is another important factor, e.g., the connection between an event trigger and a distant argument. Syntactic information could be useful in this case, as it could help bridge the gap from a word to another distant but highly related word. We modify a Transformer  by explicitly incorporating syntax via an attention layer driven by the dependency parse of the sequence. % .  %Since arguments of an event are entities, entity mentions are very effective hints.  We design our role-specific argument decoder to seamlessly accommodate both settings . We also tackle the role overlap problem  using a set of classifiers or taggers in our decoder.   Our model achieves the new state-of-the-art on ACE2005 Events data.% for EAE.  % % Motivation 1: data scarcity. Proposed and used solutions:  pretrained model BERT  External embedding   Self-training   BERT MLM  MLM encoder and decoder joint pre-training.  Teacher-Student    %
"," Event argument extraction  aims to identify the arguments of an event and classify the roles that those arguments play. Despite great efforts made in prior work, there remain many challenges:  Data scarcity.  Capturing the long-range dependency, specifically, the connection between an event trigger and a distant event argument.  Integrating event trigger information into candidate argument representation. For , we explore using unlabeled data in different ways. For , we propose to use a syntax-attending Transformer that can utilize dependency parses to guide the attention mechanism. For , we propose a trigger-aware sequence encoder with several types of trigger-dependent sequence representations. We also support argument extraction either from text annotated with gold entities or from plain text. Experiments on the English ACE2005 benchmark show that our approach achieves a new state-of-the-art.",42
" In most current NLP tasks, fixed-length vector representations of words, word embeddings, are used to represent some form of the meaning of the word. In the case of humans, however, oftentimes we will use a sequence of words known as a definition ---a statement of the meaning for a term--- to express meanings of terms . It is with this in mind that the question of ``Can machines define?'' is aimed to be answered with the task of definition modeling .  Definition modeling can be framed as a task of conditional generation, in which the definition  of the word or phrase is generated given a conditioning variable  such as a word's associated word embedding or other representations of context. Current approaches for this task  are mainly encoder-decoder based, in which one encodes a contextual representation for a word/phrase  using a variety of features such as context or character composition, and uses the contextual representation to generate the definition .   % here discuss issues of these approaches including Despite the relative success of existing approaches for definition modelling, their discriminative nature ---where distributional-derived information is at one end of the model and lexical information is at the other--- limits their power as the underlying semantic representations of the distributional and lexical information are learned in an implicit rather than direct way. For example, although \citet{ishiwatari-etal-2019-learning} successfully showed that both local and global contexts are useful to disambiguate meanings of phrases in certain cases, their approach heavily relies on an attention mechanism to identify semantic alignments between the input phrase and the output definition, which may introduce noise and ultimately be insufficient to capture the entire meaning of each phrase-definition pair.  % latent definition space with  To tackle this issue, we propose to explicitly model the underlying semantics of  phrase-definition pairs by introducing a continuous latent variable  over a definition space, which is used in conjunction with  to guide the generation of definition . The introduction of this latent representation enables us to treat it as a global defining signal during the generation process, complementing existing alignment mechanisms such as the attention.   % We specifically incorporate the latent variable directly into the decoder cell, showing that the addition of the latent variable in this way leads to increased performance on our task.   Although the latent definition variable enables us to explicitly model underlying semantics of context-definition pairs, the incorporation of it into the task renders the posterior intractable. In this paper we recur to variational inference to estimate this intractable posterior, effectively making our model a Conditional Variational Autoencoder and evolving the generation process from  to .  %to serve as a global decoding signal allows for the decoder to rely on both the attention, but if the attention is misleading you can rely on the latent variable % and this issue of misleading attentions is exacerbated in noisy datasets, so we can see improvements there as well when the generator learns misleading attention representations.    % EDISON % enables us not only to generate definitions of previously unknown words or phrases, by menas of an example , but also to obtain semantically meaningful vectors of new words by means of providing their definition alongside with an example of their usage. Effectively, our mode is able to mapping both inputs  to the same smooth space/manifold?.  We also note that existing approaches for definition modelling heavily rely on word embeddings, which due to their fixed nature can only capture so much of the semantics, being known to offer limited capabilities when dealing with polysemy. Considering the success of pretrained deep contextualized word representations which by specifically addressing these limitations have been shown to improve performance on a variety of downstream NLP tasks , in this paper we propose a mechanism to integrate deep contextualized word representations in the definition modelling task. Specifically, we successfully leverage BERT  as our contextual encoder and our definition encoder to produce representations for  and  respectively.   %While the inclusion of deep contextual word representations is important to our approach, our resuts show that it is not essential . %As a result, our model is able to  allowing for  a more meaningful continuous latent space, and  . [2-3 more sentences]  Finally, we develop two new datasets for this task, one derived from the Cambridge Dictionary , and the other derived from Le Petit Robert. In summary, our contributions are:  Datasets and pre-trained models will be publicly released to the greater NLP community to help facilitate further advances on this task upon acceptance of this paper. 
","     %Definition modeling, the task of generating word/phrase definitions, is the task that aims to answer ``Can machines define?'' In this paper, we aim to tackle this problem by introducing a continuous latent variable to explicitly model the underlying relationship between a phrase used within a context and its definition. Additionally, we release 2 new datasets, Cambridge and the first non-English dataset Robert. On most datasets, our Variational Contextual Definition Modeler  achieves a new state-of-the-art, outperforming existing systems as well as a new BERT-based baseline. In this paper we tackle the task of definition modeling, where the goal is to learn to generate definitions of words and phrases. Existing approaches for this task are discriminative, combining distributional and lexical semantics in an implicit rather than direct way. To tackle this issue we propose a generative model for the task, introducing a continuous latent variable to explicitly model the underlying relationship between a phrase used within a context and its definition. We rely on variational inference for estimation and leverage contextualized word embeddings for improved performance. Our approach is evaluated on four existing challenging benchmarks with the addition of two new datasets, Cambridge and the first non-English corpus Robert, which we release to complement our empirical study. Our Variational Contextual Definition Modeler  achieves state-of-the-art performance in terms of automatic and human evaluation metrics, demonstrating the effectiveness of our approach.\footnote{We release the code at: \url{https://github.com/machelreid/vcdm}}",43
"  Topic segmentation is a fundamental NLP task that has received considerable attention in recent years .  It can reveal important aspects of a document semantic structure by splitting the document into topical-coherent textual units. Taking the Wikipedia article in Table as an example, without the section marks, a reliable topic segmenter should be able to detect the correct boundaries within the text and chunk this article into the topical-coherent units ,  and . The results of topic segmentation can further benefit other key downstream NLP tasks such as document summarization , question answering , machine reading  and dialogue modeling .   }  A Wikipedia sample article about City Marcus covering three topics: ,  and } \end{table}  A wide variety of techniques have been proposed for topic segmentation. Early unsupervised models exploit word statistic overlaps , Bayesian contexts  or   %the  semantic relatedness graphs  to measure the lexical or semantic cohesion between the sentences or paragraphs and infer the segment boundaries from them. More recently, several works have framed topic segmentation as neural supervised learning, because of the remarkable success achieved by such models in most NLP tasks .  %While one line of research forms topic segmentation as a sequence labeling problem and builds neural models to predict segment boundaries directly ;  %another line of works first trains neural models for other tasks , and then uses these models' outputs to predict boundaries .  Despite %the  minor architectural differences, most of these neural solutions adopt Recurrent Neural Network  and its variants  as their main framework.  On the one hand, RNNs are appropriate because topic segmentation can be modelled as a sequence labeling task where each sentence is either the end of a segment or not. On the other hand, this choice makes these neural models limited in how to model the context. Because some sophisticated RNNs  are able to preserve long-distance information , which can largely help language models. But for topic segmentation, it is critical to supervise the model to focus more on the local context.    %In fact, RNNs are superior on many NLP tasks due to their capability of preserving long-distance information . %However, for topic segmentation, it is also critical to supervise the model to learn the right information from the local context.   As illustrated in Table, the prediction of the segment boundary between  and  hardly depends on the content in . Bringing in excessive long-distance signals may cause unnecessary noise and %further  hurt %model's  performance. Moreover, text coherence has strong relation with topic segmentation .  For instance, in Table, sentence pairs from the same segment  %should be  are more coherent %to put together than sentence pairs across segments .  Arguably, with a proper way of modeling the coherence between adjacent sentences, a topic segmenter can be further enhanced.   %\textcolor{red}{We hypothesize that topic segment prediction should rely on local contextual information in a way that cannot be effectively captured by RNNs.} %\textcolor{red}{In essence, RNNs are able to model long and short-distance dependencies only implicitly.}  %However, with restricted self-attention, our model can pay attention to the local context from the neighboring sentences in a more explicitly constrained way . %In essence, local contextual information is critical in predicting topical boundaries, but simple Recurrent Neural Network  and its variants are arguably not sufficiently powerful to represent the necessary information.  %However, both approaches still face the challenge of insufficient context modeling. Topic segment boundary prediction usually heavily relies on local contextual information. Hence, how to effectively select local contexts and model the relations between contexts becomes important. Neural models like RNN and its variants can represent the state of each timestep by memorizing or forgetting the information from its previous and later contexts. But how these learned contextual information contribute to model's decision is not straightforward and sufficiently transparent.  In this paper, we propose to enhance a state-of-the-art  topic segmenter  based on hierarchical attention BiLSTM network to better model the local context of a sentence in two complementary ways. First, we add a coherence-related auxiliary task to make our model learn more informative hidden states for all the sentences in a document.  %More specifically, we refine the objective of our model to encourage that the coherence of the sentences from different segments is smaller than the coherence of the sentences from the same segment.  More specifically, we refine the objective of our model to encourage smaller coherence for the sentences from different segments and larger coherence for the sentences from the same segment.  Secondly, we enhance context modeling by utilizing restricted self-attention , which enables our model to pay attention to the local context and make better use of the information from the closer neighbors of each sentence .  Our empirical results show  that our proposed context modeling strategy significantly improves the performance of the SOTA neural segmenter on three datasets,  that the enhanced segmenter is more robust in domain transfer setting when applied to four challenging real-world test sets, sampled differently from the training data,  that our context modeling strategy is also effective for the segmenters trained on other challenging languages , rather than just English.   
","      Topic segmentation is critical %, the process of splitting a document into topic-coherent pieces,      %plays a vital role      in key NLP tasks and recent works favor highly effective neural supervised  approaches.     %Due to the high effectiveness of neural models, more recent works have favored framing topic segmentation as a neural-based supervised learning problem.     However, current neural solutions are arguably limited in how they model context.     %topic segmenters proposed so far are still limited by the insufficient context modeling.      In this paper, we enhance a segmenter based on a hierarchical attention BiLSTM network to better model context, by adding a coherence-related auxiliary task and restricted self-attention. Our optimized segmenter\footnote{Our code will be publicly available at \url{www.cs.ubc.ca/cs-research/lci/research-groups/natural-language-processing/}} outperforms SOTA approaches when trained and tested on three datasets. We also the robustness of our proposed model in domain transfer setting by training a model on a large-scale dataset and testing it on four challenging real-world benchmarks. Furthermore, we apply our proposed strategy to two other languages , and show its effectiveness in multilingual scenarios.",44
"  Deep learning techniques, including contextualized word embeddings based on transformers and pretrained on language modelling, have resulted in considerable improvements for many NLP tasks. However, they often require large amounts of labeled training data, and there is also growing evidence that transferring approaches from high to low-resource settings is not straightforward. In , rule-based or linguistically motivated CRFs still outperform RNN-based methods on several tasks for South African languages. For pretraining approaches where labeled data exists in a high-resource language, and the information is transferred to a low-resource language, \citet{data/Xtreme20} find a significant gap between performance on English and the cross-lingually transferred models. In a recent study, \citet{lowresource/Lauscher2020FromZTH} find that the transfer for multilingual transformer models is less effective for resource-lean settings and distant languages. A popular technique to obtain labeled data quickly and cheaply is distant and weak supervision. \citet{lowresource/kann20weakly} recently inspected POS classifiers trained on weak supervision. They found that in contrast to scenarios with simulated low-resource settings of high-resource languages, in truly low-resource settings this is still a difficult problem. These findings also highlight the importance of aiming for realistic experiments when studying low-resource scenarios.   In this work, we analyse multilingual transformer models, namely mBERT  and XLM-RoBERTa . We evaluate both sequence and token classification tasks in the form of news title topic classification and named entity recognition . A variety of approaches have been proposed to improve performance in low-resource settings. In this work, we study  transfer learning from a high-resource language and  distant supervision. We selected these as they are two of the most popular techniques in the recent literature and are rather independent of a specific model architecture. Both need auxiliary data. For transfer learning, this is labeled data in a high-resource language, and for distant supervision, this is expert insight and a mechanism to automatically generate labels. We see them, therefore, as orthogonal and depending on the scenario and the data availability, either one or the other approach might be applicable.  Our study is performed on three, linguistically different African languages: Hausa, isiXhosa and \yoruba. These represent languages with millions of users and active use of digital infrastructure, but with only very limited support for NLP technologies. For this aim, we also collected three new datasets that are made publicly available alongside the code and additional material.  We show both challenges and opportunities when working with multilingual transformer models evaluating trends for different levels of resource scarcity. The paper is structured into the following questions we are interested in:     
"," Multilingual transformer models like mBERT and XLM-RoBERTa have obtained great improvements for many NLP tasks on a variety of languages. However, recent works also showed that results from high-resource languages could not be easily transferred to realistic, low-resource scenarios. In this work, we study trends in performance for different amounts of available resources for the three African languages Hausa, isiXhosa and \yoruba on both NER and topic classification. We show that in combination with transfer learning or distant supervision, these models can achieve with as little as 10 or 100 labeled sentences the same performance as baselines with much more supervised training data. However, we also find settings where this does not hold. Our discussions and additional experiments on assumptions such as time and hardware restrictions highlight challenges and opportunities in low-resource learning.",45
"  In computer vision, zero shot-learning  for image classification is the problem of classifying images given auxiliary information. An image classification model is trained to classify images from a pre-defined set of classes. At test time, images from new classes are given, and the task is to transfer knowledge learned from seen classes during training to unseen test classes.        A common setup for ZSL assumes that the auxiliary information is a set of semantically meaningful properties  describing the class  . A different ZSL setup uses image captions as auxiliary information . Typically, this auxiliary information is manually collected by human raters for each image  and averaged across images. A more realistic approach  relies on available online text descriptions of classes  . It avoids expensive annotation and exposure to test images.  In this work, we classify bird species according to Wikipedia descriptions. This task raises many challenges:   Differences between the birds are very small, which makes it a fine-grained classification task;  This is an expert task,  and the text contains terminology that is unlikely to be familiar to a layman; and, on top of that  The text descriptions of the classes are long, containing few visually relevant sentences.    As opposed to previous work on text-based ZSL employing textual descriptions  that focused on the visual modality, here we focus on the text modality, and address a key question in ZSL: \textit {How can we identify text components that are visual in nature?}   To get an intuition about the task setup and our proposed solution, consider the following situation. Imagine you have never seen a zebra but have seen a horse. What if you were given a text describing a zebra: \enquote{Zebras have hooves, mane, tail, pointed ears, and white and black stripes}. This description would probably be very close to a description of a horse having \enquote{hooves, mane, tail, pointed ears} and you would probably be looking for an image that reminds you of a horse but has \enquote{white and black stripes}. So, even without ever seeing a zebra, using text-descriptions of the zebra and knowledge already acquired about horses, one can correctly classify unknown classes like a zebra.   Our proposed solution has two-phases. First, based on the intuition that similar objects  tend to have similar texts, we encode a similarity feature that enhances text descriptions' separability.  In addition, we leverage the intuition that the differences between text descriptions of species would be their most salient visual features, and extract visually relevant descriptions from the text.   Our experiments empirically demonstrate both the efficacy and generalization capacity of our proposed solution.  On two large ZSL datasets, in both the easy and hard scenarios, the similarity method obtains a ratio improvement of up to 18.3\%. With the addition of extracting visually relevant descriptions, we obtain a ratio improvement of up to 48.16\% over the state-of-the-art. We further show that our visual-summarization method generalizes from the CUB dataset  to the NAB dataset , and we demonstrate its contribution to additional models by a ratio improvement of up to 59.62\%.    The contributions of this paper are threefold. First, to the best of our knowledge, we are the first to showcase the critical importance of the text representation in zero-shot image-recognition scenarios, and we present two concrete text-based processing methods that vastly improve the results. Second, we demonstrate the efficacy and generalizability of our proposed methods by applying them to both the zero-shot and generalized zero-shot tasks, outperforming all previously reported results on the CUB and NAB Benchmarks.   Finally, we show that visual aspects learned from one dataset can be transferred effectively to another dataset without the need to obtain dataset-specific captions.  The efficacy of our proposed solution on these benchmarks illustrates that purposefully exposing the visual features in texts is indispensable for tasks that learn to align the vision-and-language modalities.   
","    We study the problem of recognizing visual entities from the textual descriptions of their classes. Specifically, given birds' images with free-text descriptions of their species, we learn to classify images of previously-unseen species based on specie descriptions. This setup has been studied in the vision community under the name zero-shot learning from text, focusing on learning to transfer knowledge about visual aspects of birds from seen classes to previously-unseen ones. Here, we suggest focusing on the textual description and distilling from the description the most relevant information to effectively match visual features to the parts of the text that discuss them.  Specifically,  we propose to leverage the similarity between species, reflected in the similarity between text descriptions of the species.  we derive visual summaries of the texts, i.e.,  extractive summaries that focus on the visual features that tend to be reflected in images. We propose a simple attention-based model augmented with the similarity and visual summaries components. Our empirical results consistently and significantly outperform the state-of-the-art on the largest benchmarks for text-based zero-shot learning, illustrating the critical importance of texts for zero-shot image-recognition.",46
" Natural Language Understanding  evaluation plays a key role in benchmarking progress in natural language processing  research. With the recent advance in language representative learning, results on previous benchmarks have rapidly saturated. This leads to an explosion of difficult, diverse proposals of tasks/datasets for NLU evaluation, including Natural Language Inference , Grounded Commonsense Inference, Commonsense QA, Social Interactions Reasoning, Abductive Commonsense Reasoning , etc.   One common practice followed by most of these recent works is to simplify the evaluation of various reasoning abilities as a classification task. This is analogous to asking objective questions to a human in educational testing. This simplification not only facilitates the data annotation but also gives interpretable evaluation results, based on which behaviors of the models are studied and then weaknesses are diagnosed.   Despite the straightforwardness of this formalization, one assumption behind most prior benchmark data sourcing is that there exists a single prescriptive ground truth label for each example. The assumption might be true in human educational settings where prescriptivism is preferred over descriptivism because the goal is to test humans with well-defined knowledge or norms. However, it is not true for many NLP tasks due to their pragmatic nature where the meaning of the same sentence might differ depending on the context or background knowledge.   Specifically for the NLI task, \citet{manning2006local} advocate that annotation tasks should be ``natural'' for untrained annotators, and the role of NLP should be to model the inferences that humans make in practical settings. Previous work that uses a graded labeling schema on NLI, showed that there are inherent disagreements in inference tasks. All these discussions challenge the commonly used majority ``gold-label'' practice in most prior data collections and evaluations.  Intuitively, such disagreements among humans should be allowed because different annotators might have different subjective views of the world and might think differently when they encounter the same reasoning task. Thus, from a descriptive perspective, evaluating the capacity of NLP models in predicting not only individual human opinions or the majority human opinion, but also the overall distribution over human judgments provides a more representative comparison between model capabilities and `collective' human intelligence.  Therefore, we collect ChaosNLI, a large set of Collective HumAn OpinionS for examples in several existing  NLI datasets, and comprehensively examine the factor of human agreement  on the state-of-the-art model performances. Specifically, our contributions are:  The ChaosNLI dataset and experimental scripts are available at \url{https://github.com/easonnie/ChaosNLI}   
"," Despite the subjective nature of many NLP tasks, most NLU evaluations have focused on using the majority label with presumably high agreement as the ground truth. Less attention has been paid to the distribution of human opinions. We collect ChaosNLI, a dataset with a total of 464,500 annotations to study Collective HumAn OpinionS in oft-used NLI evaluation sets. This dataset is created by collecting 100 annotations per example for 3,113 examples in SNLI and MNLI and 1,532 examples in \abdnli. Analysis reveals that:  high human disagreement exists in a noticeable amount of examples in these datasets;  the state-of-the-art models lack the ability to recover the distribution over human labels;   models achieve near-perfect accuracy on the subset of data with a high level of human agreement, whereas they can barely beat a random guess on the data with low levels of human agreement, which compose most of the common errors made by state-of-the-art models on the evaluation sets. This questions the validity of improving model performance on old metrics for the low-agreement part of evaluation datasets. Hence, we argue for a detailed examination of human agreement in future data collection efforts, and evaluating model outputs against the distribution over collective human opinions.\footnote{The ChaosNLI dataset and experimental scripts are available at \url{https://github.com/easonnie/ChaosNLI}}",47
" Due to the growing number of Internet users, cyber-violence emerged with offensive language pervasive across social media. With anonymity as a 閳ユ笡rivilege閳, netizens hide behind the screens, behaving in a manner most of them would not otherwise in reality. Thus, government organizations, online communities, and technology companies are all striving for ways to detect aggressive language in social media and help build a more friendly online environment.  Manual filtering is very time consuming and it can cause post-traumatic stress disorder-like symptoms to human annotators. One of the most common strategies  to tackle the problem is to train systems capable of recognizing offensive content, which can then be deleted or set aside human moderation.  SemEval 2020 Task-12  is the second edition of OffensEval . In this competition, organizers offers 5 languages datasets including Arabic , Danish , English , Turkish  and Greek . In Sub-task A, the participants need to predict whether a post uses offensives language. Besides, the organizers provide other two sub-tasks which mainly focus on English, to predict the type and target of offensive language.  Participating in all 3 Sub-tasks, we proposed several methods based on pre-training language models including ERNIE and XLM-R. In Sub-task A, we scored 0.9199, 0.851, 0.8258, 0.802, 0.8989 in English, Greek, Turkish, Danish and Arabic respectively. We ranked first in average F1 scores, and ranked in top three across all languages. In Sub-task B and Sub-task C, we also took the first place with 0.7462 and 0.7145. In the following sections, we will elaborate the methods, dataset and experiments of our system.  
","   This paper describes Galileo闁炽儲鐛 performance in SemEval-2020 Task 12 on detecting and categorizing offensive language in social media. For Offensive Language Identification, we proposed a multi-lingual method using Pre-trained Language Models, ERNIE and XLM-R. For offensive language categorization, we proposed a knowledge distillation method trained on soft labels generated by several supervised models. Our team participated in all three sub-tasks. In Sub-task A - Offensive Language Identification, we ranked first in terms of average F1 scores in all languages. We are also the only team which ranked among the top three across all languages.  We also took the first place in Sub-task B - Automatic Categorization of Offense Types and Sub-task C - Offence Target Identification.",48
" Understanding and reasoning over natural language plays a significant role in artificial intelligence tasks such as Machine Reading Comprehension  and Question Answering . Several QA tasks have been proposed in recent years to evaluate the language understanding capabilities of machines . These tasks are single-hop QA tasks and consider answering a question given only one single paragraph. % The drawback of single-hop QA tasks is the lack of evaluating deep reasoning capability.  % We observe that many existing neural models achieve promising performance without reasoning.  Many existing neural models rely on learning context and type-matching heuristics. Those rarely build reasoning modules but achieve promising performance on single-hop QA tasks. The main reason is that these single-hop QA tasks are lacking a realistic evaluation of reasoning capabilities because they do not require complex reasoning.   Recently multi-hop QA tasks, such as HotpotQA  and WikiHop, have been proposed to assess multi-hop reasoning ability. HotpotQA task provides annotations to evaluate document level question answering and finding supporting facts. Providing supervision for supporting facts improves explainabilty of the predicted answer because they clarify the cross paragraph reasoning path.   Due to the requirement of multi-hop reasoning over multiple documents with strong distraction, multi-hop QA tasks are challenging.  Figure shows an example of HotpotQA. Given a question and 10 paragraphs, only paragraph  and paragraph  are relevant. The second sentence in paragraph  and the first sentence in paragraph  are the supporting facts. The answer is ``Geelong Football Club''.   Primary studies in HotpotQA task prefer to use a reading comprehension neural model. First, they use a neural retriever model to find the relevant paragraphs to the question. After that, a neural reader model is applied to the selected paragraphs for answer prediction. Although these approaches obtain promising results, the performance of evaluating multi-hop reasoning capability is unsatisfactory.   To solve the multi-hop reasoning problem, some models tried to construct an entity graph using Spacy or Stanford CoreNLP and then applied a graph model to infer the entity path from question to the answer. However, these models ignore the importance of the semantic structure of the sentences and the edge information and entity types in the entity graph. To take the in-depth semantic roles and semantic edges between words into account here we use semantic role labeling  graph as the backbone of a graph convolutional network. Semantic role labeling provides the semantic structure of the sentence in terms of argument-predicate relationships.  % such as ``who did what to whom.'' The argument-predicate relationship graph can significantly improve the multi-hop reasoning results. Our experiments show that SRL is effective in finding the cross paragraph reasoning path and answering the question.  Our proposed semantic role labeling graph reasoning network  jointly learns to find cross paragraph reasoning paths and answers questions on multi-hop QA. In SRLGRN model, firstly, we train a paragraph selection module to retrieve gold documents and minimize distractor. Second, we build a heterogeneous document-level graph that contains sentences as nodes ,  % and the sentence nodes include  and SRL sub-graphs including semantic role labeling arguments as nodes and predicates as edges. Third, we train a graph encoder to obtain the graph node representations that incorporate the argument types and the semantics of the predicate edges in the learned representations. Finally, we jointly train a multi-hop supporting fact prediction module that finds the cross paragraph reasoning path, and answer prediction module that obtains the final answer. Notice that both supporting fact prediction and answer prediction are based on contextual semantics graph representations as well as token-level BERT pre-trained representations. The contributions of this work are as follows:   {\bf 1)} We propose the SRLGRN framework that considers the semantic structure of the sentences in building a reasoning graph network. Not only the semantics roles of nodes but also the semantics of edges are exploited in the model.  {\bf 2)} We evaluate and analyse the reasoning capabilities of the semantic role labeling graph compared to usual entity graphs. %We analyze the multi-hop reasoning capacity on HotpotQA task.  The fine-grained semantics of SRL graph help in both finding the answer and the explainability of the reasoning path.  {\bf 3)} Our proposed model obtains competitive results on both HotpotQA  and the SQuAD benchmarks.  
"," This work deals with the challenge of learning and reasoning over multi-hop question answering . We propose a graph reasoning network based on the semantic structure of the sentences to learn cross paragraph reasoning paths and find the supporting facts and the answer jointly. The proposed graph is a heterogeneous document-level graph that contains nodes of type sentence , and semantic role labeling sub-graphs per sentence that contain arguments as nodes and predicates as edges. Incorporating the argument types, the argument phrases, and the semantics of the edges originated from SRL predicates into the graph encoder helps in finding and also the explainability of the reasoning paths. Our proposed approach shows competitive performance on the HotpotQA distractor setting benchmark compared to the recent state-of-the-art models.",49
"   The organizers of the 2020 VarDial Evaluation Campaign  proposed a shared task targeted towards the geolocation of short texts, e.g.~tweets, namely the Social Media Variety Geolocation  task. Typically formulated as a double regression problem, the task is about predicting the location, expressed in latitude and longitude, from where the text received as input was posted on a certain social media platform. Twitter and Jodel are the platforms used for data collection, divided by the language area in three subtasks, namely:   In this paper, we focus only on the second subtask, SMG-CH, proposing a variety of handcrafted and deep learning models, as well as an ensemble model that combines all our previous models through meta-learning. Our first model is a Support Vector Regression  classifier  based on string kernels, which are known to perform well in other dialect identification tasks . Our second model is a character-level convolutional neural network  , which is also known to provide good results in dialect identification . Due to the high popularity and the outstanding results of Bidirectional Encoder Representations from Transformers   in solving mainstream NLP tasks, we decided to try out a Long Short-Term Memory  network  based on German BERT embeddings as our third model. Lastly, we combine our three models into an ensemble that employs Extreme Gradient Boosting   as meta-learner. We conducted experiments on the development set provided by the organizers, in order to decide which models to choose for our three submissions for the SMG-CH subtask. Our results indicate that the ensemble model attains the best results. Perhaps surprisingly, our shallow approach based on string kernels outperforms both deep learning models. Our observations are consistent across the development and the test sets provided by the organizers.  % We experimented with a few Machine Learning algorithms for the second subtask, namely CH,  % Geolocation can be framed as a double regression task, but more sophisticated model architectures have been proposed .  % Jodel is a mobile chat application that lets people anonymously talk to other users within a 10km-radius around them.   % All three subtasks will use the same data format and evaluation methodology, and participants are encouraged to submit their systems for all subtasks.  The rest of this paper is organized as follows. We present related work on dialect identification and geolocation of short texts in Section. Our approaches are described in more detail in Section. We present the experiments and empirical results in Section. Finally, our conclusions are drawn in Section.  
"," In this work, we introduce the methods proposed by the UnibucKernel team in solving the Social Media Variety Geolocation task featured in the 2020 VarDial Evaluation Campaign. We address only the second subtask, which targets a data set composed of nearly 30 thousand Swiss German Jodels. The dialect identification task is about accurately predicting the latitude and longitude of test samples. We frame the task as a double regression problem, employing a variety of machine learning approaches to predict both latitude and longitude. From simple models for regression, such as Support Vector Regression, to deep neural networks, such as Long Short-Term Memory networks and character-level convolutional neural networks, and, finally, to ensemble models based on meta-learners, such as XGBoost, our interest is focused on approaching the problem from a few different perspectives, in an attempt to minimize the prediction error. With the same goal in mind, we also considered many types of features, from high-level features, such as BERT embeddings, to low-level features, such as characters n-grams, which are known to provide good results in dialect identification. Our empirical results indicate that the handcrafted model based on string kernels outperforms the deep learning approaches. Nevertheless, our best performance is given by the ensemble model that combines both handcrafted and deep learning models.",50
"  Comparing and contrasting the meaning of text conveyed in different languages is a fundamental nlp task. It can be used to curate clean parallel corpora for downstream tasks such as machine translation~, cross-lingual transfer learning, or semantic modeling~, and it is also useful to directly analyze multilingual corpora. For instance, detecting the commonalities and divergences between sentences drawn from English and French Wikipedia articles about the same topic would help analyze language bias~, or mitigate differences in coverage and usage across languages~. This requires not only detecting coarse content mismatches, but also fine-grained differences in sentences that overlap in content.  Consider the following English and French sentences, sampled from the WikiMatrix parallel corpus. While they share important content, highlighted words convey meaning missing from the other language:     We show that explicitly considering diverse types of semantic divergences in bilingual text benefits both the annotation and prediction of cross-lingual semantic divergences. We create and release the Rationalized English-French Semantic Divergences corpus , based on a novel divergence annotation protocol that exploits rationales to improve annotator agreement. We introduce \modelname, a  bert-based model that detects fine-grained semantic divergences without supervision by learning to rank synthetic divergences of varying granularity. Experiments on \dataset show that our model distinguishes semantically equivalent from divergent examples much better than a strong sentence similarity baseline and that unsupervised token-level divergence tagging offers promise to refine distinctions among divergent instances. We make our code and data publicly available.\footnote{Implementations of \modelname can be found at: \url{https://github.com/Elbria/xling-SemDiv}; the \dataset dataset is hosted at:   \url{https://github.com/Elbria/xling-SemDiv/tree/master/REFreSD}.}       
","  Detecting fine-grained differences in content conveyed in different languages matters for cross-lingual nlp and multilingual corpora analysis, but it is a challenging machine learning problem since annotation is expensive and hard to scale.~This work improves the prediction and annotation of fine-grained semantic divergences.~We introduce a training strategy for multilingual bert models by learning to rank synthetic divergent examples of varying granularity.~We evaluate our models on the~Rationalized~English-French~Semantic~Divergences, a new dataset released with this work, consisting of English-French sentence-pairs annotated with semantic divergence classes and token-level rationales.~Learning to rank helps detect fine-grained sentence-level divergences more accurately than a strong sentence-level similarity model, while token-level predictions have the potential of further distinguishing between coarse and fine-grained divergences.",51
"  There is a variety of successful summarization applications but few can afford to have a large number of annotated examples that are sufficient to meet the requirement of end-to-end neural abstractive summarization. Examples range from summarizing radiology reports to congressional bills and meeting conversations. The lack of annotated resources suggests that end-to-end systems may not be a ``one-size-fits-all'' solution to neural text summarization. There is an increasing need to develop cascaded architectures to allow for customized content selectors to be combined with general-purpose neural text generators to realize the full potential of neural abstractive summarization.   We advocate for explicit content selection as it allows for a rigorous evaluation and visualization of intermediate  results of such a module, rather than associating it with text generation.  Existing neural abstractive systems can perform content selection implicitly using end-to-end models, or more explicitly, with an external module to select important sentences or words to aid generation. However, content selection concerns not only the selection of important segments from a document, but also the cohesiveness of selected segments and the amount of text to be selected in order for a neural text generator to produce a summary.       In this paper, we aim to investigate the feasibility of a cascade approach to neural text summarization. We explore a constrained summarization task, where an abstract is created one sentence at a time through a cascaded pipeline. Our pipeline architecture chooses one or two sentences from the source document, then highlights their summary-worthy segments and uses those as a basis for composing a summary sentence. When a pair of sentences are selected, it is important to ensure that they are fusible---there exists cohesive devices that tie the two sentences together into a coherent text---to avoid generating nonsensical outputs.  Highlighting sentence segments allows us to perform fine-grained content selection that guides the neural text generator to stitch selected segments into a coherent sentence.  The contributions of this work are summarized as follows.    
","  We present an empirical study in favor of a cascade architecture to neural text summarization. Summarization practices vary widely but few other than news summarization can provide a sufficient amount of training data enough to meet the requirement of end-to-end neural abstractive systems which perform content selection and surface realization jointly to generate abstracts.  Such systems also pose a challenge to summarization evaluation, as they force content selection to be evaluated along with text generation, yet evaluation of the latter remains an unsolved problem. In this paper, we present empirical results showing that the performance of a cascaded pipeline that separately identifies important content pieces and stitches them together into a coherent text is comparable to or outranks that of end-to-end systems, whereas a pipeline architecture allows for flexible content selection. We finally discuss how we can take advantage of a cascaded pipeline in neural text summarization and shed light on important directions for future research.",52
"    A renewed emphasis must be placed on sentence fusion in the context of neural abstractive summarization. A majority of the systems are trained end-to-end, where an abstractive summarizer is rewarded for generating summaries that contain the same words as human abstracts, measured by automatic metrics such as ROUGE. A summarizer, however, is not rewarded for correctly fusing sentences. In fact, when examined more closely, only few sentences in system abstracts are generated by fusion. For instance, 6\% of summary sentences generated by Pointer-Gen are through fusion, whereas human abstracts contain 32\% fusion sentences. Moreover, sentences generated by fusion are prone to errors. They can be ungrammatical, nonsensical, or otherwise ill-formed. There is thus an urgent need to develop neural abstractive summarizers to fuse sentences properly.       The importance of sentence fusion has long been recognized by the community before the era of neural text summarization. The pioneering work of Barzilay et al.~\shortcite{barzilay-etal-1999-information} introduces an information fusion algorithm that combines similar elements across related text to generate a succinct summary. Later work, such as, builds a dependency or word graph by combining syntactic trees of similar sentences, then employs integer linear programming to decode a summary sentence from the graph.  Most of these studies have assumed a set of similar sentences as input, where fusion is necessary to reduce repetition. Nonetheless, humans do not limit themselves to combine similar sentences. In this paper, we pay particular attention to fuse disparate sentences that contain fundamentally different content but remain related to make fusion sensible. In Figure, we provide an example of a sentence fusion instance.   We address the challenge of fusing disparate sentences by enhancing the Transformer architecture with points of correspondence between sentences, which are devices that tie two sentences together into a coherent text. The task of sentence fusion involves choosing content from each sentence and weaving the content pieces together into an output sentence that is linguistically plausible and semantically truthful to the original input. It is distinct from~\citet{geva-etal-2019-discofuse} that connect two sentences with discourse markers. Our contributions are as follows.       
","  The ability to fuse sentences is highly attractive for summarization systems because it is an essential step to produce succinct abstracts. However, to date, summarizers can fail on fusing sentences. They tend to produce few summary sentences by fusion or generate incorrect fusions that lead the summary to fail to retain the original meaning.  In this paper, we explore the ability of Transformers to fuse sentences and propose novel algorithms to enhance their ability to perform sentence fusion by leveraging the knowledge of points of correspondence between sentences. Through extensive experiments, we investigate the effects of different design choices on Transformer's performance. Our findings highlight the importance of modeling points of correspondence between sentences for effective sentence fusion.",53
"   The recent advances in neural machine translation   have provided the research community and the commercial landscape with effective translation models that can at times achieve near-human performance. However, this usually holds at phrase or sentence level. When using these models in larger units of text, such as paragraphs or documents, the quality of the translation may drop considerably in terms of discourse attributes such as lexical and stylistic consistency.  In fact, document-level translation is still a very open and challenging problem. The sentences that make up a document are not unrelated pieces of text that can be predicted independently; rather, a set of sequences linked together by complex underlying linguistics aspects, also known as the discourse . The discourse of a document includes several properties such as grammatical cohesion , lexical cohesion , document coherence  and the use of discourse connectives . Ensuring that the translation retain such linguistic properties is expected to significantly improve its overall readability and flow.  However, due to the limitations of current decoder technology, NMT models are still bound to translate at sentence level. In order to capture the discourse properties of the source document in the translation, researchers have attempted to incorporate more contextual information from surrounding sentences. Most document-level NMT approaches augment the model with multiple encoders, extra attention layers and memory caches to encode the surrounding sentences, and leave the model to implicitly learn the discourse attributes by simply minimizing a conventional NLL objective. The hope is that the model will spontaneously identify and retain the discourse patterns within the source document. Conversely, very little work has attempted to model the discourse attributes explicitly. Even the evaluation metrics typically used in translation such as BLEU  are not designed to assess the discourse quality of the translated documents.  For these reasons, in this paper we propose training an NMT model by directly targeting two specific discourse metrics: lexical cohesion  and coherence . LC is a measure of the frequency of semantically-similar words co-occurring in a document  . For example, car, vehicle, engine or wheels are all semantically-related terms. There is significant empirical evidence that ensuring lexical cohesion in a text eases its understanding . At its turn, COH measures how well adjacent sentences in a text are linked to each other. In the following example from Hobbs \shortcite{hobbs1979coherence}:       the two sentences make little `sense' one after another. An incoherent text, even if grammatically and syntactically perfect, is anecdotally very difficult to understand and therefore coherence should be actively pursued. Relevant to translation, Vasconcellos \shortcite{vasconcellos1989cohesion} has found that a high percentage of the human post-editing changes over machine-generated translations involves the improvement of cohesion and coherence.  Several LC and COH metrics that well correlate with the human judgement have been proposed in the literature. However, like BLEU and most other evaluation metrics, they are discrete, non-differentiable functions of the model's parameters. Hereafter, we propose to overcome this limitation by using the well-established policy gradient approach from reinforcement learning  which allows using any evaluation metric as a reward without having to differentiate it. By combining different types of rewards, the model can be trained to simultaneously achieve more lexically-cohesive and more coherent document translations, while at the same time retaining faithfulness to the reference translation. %the information contained in the source document.  %The rest of the paper is organized as follows. Section  discusses related work. Section  describes the baseline NMT architectures used for the experiments. Section  presents the proposed training approach and the discourse rewards used with it. Section  presents the experiments and, finally, Section  concludes the paper.   
","   Document-level machine translation focuses on the translation of entire documents from a source to a target language. It is widely regarded as a challenging task since the translation of the individual sentences in the document needs to retain aspects of the discourse at document level. However, document-level translation models are usually not trained to explicitly ensure discourse quality. Therefore, in this paper we propose a training approach that explicitly optimizes two established discourse metrics, lexical cohesion  and coherence , by using a reinforcement learning objective. Experiments over four different language pairs and three translation domains have shown that our training approach has been able to achieve more cohesive and coherent document translations than other competitive approaches, yet without compromising the faithfulness to the reference translation. In the case of the Zh-En language pair, our method has achieved an improvement of $2.46$ percentage points  in LC and $1.17$ pp in COH over the runner-up, while at the same time improving $0.63$ pp in BLEU score and $0.47$ pp in $\mathrm{F}_{\mathrm{BERT}}$.      %In fact, in some cases our training approach has even improved translation accuracy metrics such as BLEU and the recently proposed $F_{\text{BERT}}$.",54
" In recent years, neural models have led to state-of-the-art results in machine translation  . Many of these systems can broadly be characterized as following a multi-layer encoder-decoder neural network design: both the encoder and decoder learn representations of word sequences by a stack of layers , building on an interesting line of work in improving such models. The simplest of these increases the model capacity by widening the network, whereas more recent work shows benefits from stacking more layers on the encoder side. For example, for the popular Transformer model , deep systems have shown promising BLEU improvements by either easing the information flow through the network  or constraining the gradient norm across layers . An improved system can even learn a 35-layer encoder, which is  deeper than that of vanilla Transformer .  Although these methods have enabled training deep neural MT  models, questions remain as to the nature of the problem. The main question here is: why and how deep networks help in NMT. Note that previous work evaluates these systems in a black-box manner . It is thus natural to study how much a deep NMT system is able to learn that is different from the shallow counterpart. Beyond this, training an extremely deep model is expensive although a narrow-and-deep network can speed up training . For example, it takes us  longer time to train the model when we deepen the network from 6 layers to 48 layers. This might prevent us from exploiting deeper models in large-scale systems.  In this paper, we explore why deep architectures work to render learning NMT models more effectively. By investigating the change of the hidden states in different layers, we find that new representations are learned by continually stacking layers on top of the base model. More stacked layers lead to a stronger model of representing the sentence. This particularly makes sense in the deep NMT scenario because it has been proven that deep models can benefit from an enriched representation .  In addition, the finding here inspires us to develop a simple yet efficient method to train a deep NMT encoder: we train model parameters from shallow to deep, rather than training the entire model from scratch. To stabilize training, we design a sparse linear combination method of connecting lower-level layers to the top. It makes efficient pass of information through the deep network but does not require large memory footprint as in dense networks. We experiment with the method in a state-of-the-art deep Transformer system. Our encoder consists of 48-54 layers, which is almost the deepest Transformer model used in NMT. On WMT En-De and En-Fr tasks, it yields a  speedup of training, matching the state-of-the-art on the WMT'16 En-De task.  
","    Deep encoders have been proven to be effective in improving neural machine translation  systems, but training an extremely deep encoder is time consuming. Moreover, why deep models help NMT is an open question. In this paper, we investigate the behavior of a well-tuned deep Transformer system. We find that stacking layers is helpful in improving the representation ability of NMT models and adjacent layers perform similarly. This inspires us to develop a shallow-to-deep training method that learns deep models by stacking shallow models. In this way, we successfully train a Transformer system with a 54-layer encoder. Experimental results on WMT'16 English-German and WMT'14 English-French translation tasks show that it is $1.4$ $\times$ faster than training from scratch, and achieves a BLEU score of $30.33$ and $43.29$ on two tasks. The code is publicly available at \href{https://github.com/libeineu/SDT-Training/}{https://github.com/libeineu/SDT-Training}.",55
"   Task-oriented dialogue systems complete tasks for users, such as making a hotel reservation or finding train routes, in a multi-turn conversation . The generated system utterances should not only be naturally sound, but more importantly be informative, i.e., to proceed the dialogue towards task completion. To fulfill this requirement, conditioned response generation is widely adopted based on system actions .  The response generation process is decoupled into two consecutive steps, where an action is first selected and then an utterance is generated conditioned on this action. One can optimize each step towards its goal, i.e., informative and naturally sound, without impinging the other . However, such approaches rely on action annotations , which require domain knowledge and extensive efforts to obtain.     %  \end{threeparttable} \end{table}    To deal with the absence of action annotations, latent action learning has been introduced .  System utterances are represented as low-dimensional latent variables by an auto-encoding task , and utterances with the same representations are considered to convey similar meanings.  Such action representations might be prone to over-dependence on the training data, which restricts the model generalization capability, especially when multiple domains are considered. % This is because the implicit nature of latent variables makes it unable to enforce the desired properties of the latent space, i.e., to capture the intentions of system utterances, without explicit supervision .  This is because, without explicit supervision, the desired property of capturing the intentions of system utterances in the latent space cannot be enforced , which in turn is due to the implicit nature of latent variables. For example, variational auto-encoder , which is often used for latent action learning, tends to produce a balanced distribution over the latent variables , while the true distribution of system actions is highly imbalanced . The resulting misaligned action representations would confuse the model of both steps and degenerate the sample efficiency in training.      % This is because without explicit supervision the desired property of capturing the intentions of system utterances in the latent space cannot be enforced , which in turn is due to the implicit nature of latent variables.   To address the above issues, we propose to learn natural language actions that represent system utterances as a span of words, which explicitly reveal the underlying intentions. % benefits of natural language actions Natural language provides unique compositional structure while retaining the representation flexibility. These properties promote model generalization and thus make natural language a 閾夸骏xible representation for capturing characteristics with minimal assumptions . % the main rationale to obtain such actions % In our scenarios, we aim to use language as the interface by  Motivated by these advantages, we learn natural language actions by identifying salient words of system utterances.  Salient refers to indicative for a prediction task  that takes as input the original utterance. % for the characteristics of utterances. The main rationale is that the principal information that the task concerns can be preserved by just the salient words. For example, the sentiment of sentence ``The movie starts out as competent but turn bland'' can be revealed by the word ``bland'' when it is identified salient by considering the complete context.   In our scenarios, we consider measuring word saliency in terms of state transitions. This is because state transitions reflect how the intentions of a system utterance influence the dialogue progress, and action representations that capture such influences can well reveal the intentions . By considering salient words for state tracking tasks as actions, we obtain action representations that enjoy the merits of natural language and indeed capture the characteristics of interest, i.e., intentions of system utterances. % explainable     % technical contributions Obtaining salient words by applying existing saliency identification approaches  is, however, unable to produce unified action representations. Specifically, system utterances with the same intention might not share similar wordings, and existing attribution approaches can only identify salient words within utterances. We tackle this challenge by proposing a memory-augmented saliency approach that identifies salient words from a broader vocabulary. The vocabulary consists of all the words that could compose natural language actions,~\footnote{We consider content words from state annotations and task descriptions, which will be specified in Sec. } and each word is stored as a slot in the memory component.  By incorporating the memory component into a dialogue state tracking model, we use each system utterance as a query to perform memory retrieval, and the retrieval results are considered as salient words. The retrieval results might contain words that are redundant since we do not have direct supervision for the retrieval operations. For example, the resulting salient words might be ``but turn bland'' in the example shown earlier, which include unnecessary words and may lead to degenerated action results.  To obtain compact action representations, we propose an auxiliary task based on pseudo parallel corpus, i.e., dialogue context and state annotation pairs.  We observe that dialogue states serve as good examples of how compact representation should be. Therefore, we use the encoded dialogue context as query and ask the memory component to reconstruct its text-based dialogue states. In this way, the obtained concise actions generalize better and can be easily interpreted.     Our contributions are summarized as follows:       
","   Response generation for task-oriented dialogues implicitly optimizes two objectives at the same time: task completion and language quality. Conditioned response generation serves as an effective approach to separately and better optimize these two objectives.  Such an approach relies on system action annotations which are expensive to obtain. To alleviate the need of action annotations, latent action learning is introduced to map each utterance to a latent representation. However, this approach is prone to over-dependence on the training data, and the generalization capability is thus restricted.   To address this issue, we propose to learn natural language actions that represent utterances as a span of words.  This explicit action representation promotes generalization via the compositional structure of language. It also enables an explainable generation process. Our proposed unsupervised approach learns a memory component to summarize system utterances into a short span of words. To further promote a compact action representation, we propose an auxiliary task that restores state annotations as the summarized dialogue context using the memory component. Our proposed approach outperforms latent action baselines on MultiWOZ, a benchmark multi-domain dataset.",56
"      Consider helping a friend prepare dinner in an unfamiliar house: when your friend asks you to clean and slice an apple for an appetizer, how would you approach the task? Intuitively, one could reason abstractly:  find an apple  wash the apple in the sink  put the clean apple on the cutting board  find a knife  use the knife to slice the apple  put the slices in a bowl. Even in an unfamiliar setting, abstract reasoning can help accomplish the goal by leveraging semantic priors. Priors like locations of objects --~apples are commonly found in the kitchen along with implements for cleaning and slicing, object affordances --~a sink is useful for washing an apple unlike a refrigerator, pre-conditions --~better to wash an apple before slicing it, rather than the converse. We hypothesize that, learning to solve tasks using abstract language,  unconstrained by the particulars of the physical world, enables agents to complete embodied tasks in novel environments by leveraging the kinds of semantic priors that are exposed by abstraction and interaction.      To test this hypothesis, we have created the novel \env framework, the first interactive, parallel environment that aligns text descriptions and commands with physically embodied robotic simulation. We build \env by extending two prior works: \tw~ - an engine for interactive text-based games, and \alfred~ - a large scale dataset for vision-language instruction following in embodied environments.  \env provides two views of the same underlying world and two modes by which to interact with it: \tw, an abstract, text-based environment, generates textual observations of the world and responds to high-level text actions;  \alfred, the embodied simulator, renders the world in high-dimensional images and responds to low-level physical actions as from a robot .\footnote{Note: Throughout this work, for clarity of exposition, we use \alfred{} to refer to both tasks and the grounded simulation environment, but rendering and physics are provided by \thor{}~.} Unlike prior work on instruction following , which typically uses a static corpus of cross-modal expert demonstrations, we argue that aligned parallel environments like \env offer a distinct advantage: they allow agents to explore, interact, and learn in the abstract environment of language before encountering the complexities of the embodied environment.  While fields such as robotic control use % simulators like MuJoCo~ to provide infinite data through interaction, there has been no analogous mechanism  -- short of hiring a human around the clock --  for providing linguistic feedback and annotations to an embodied agent.  \tw{} addresses this discrepancy by providing programmatic and aligned linguistic signals during agent exploration. This facilitates the first work, to our knowledge, in which an embodied agent learns the meaning of complex multi-step policies, expressed in language, directly through interaction.    Empowered by the \env framework, we introduce \model , an agent that first learns to perform abstract tasks in \tw using Imitation Learning  and then transfers the learned policies to embodied tasks in \alfred.  When operating in the embodied world, \model leverages the abstract understanding gained from \tw to generate text-based actions; these serve as high-level subgoals that facilitate physical action generation by a low-level controller. Broadly, we find that \model is capable of generalizing in a zero-shot manner from \tw to unseen embodied tasks and settings. Our results show that training first in the abstract text-based environment is not only  faster, but also yields better performance than training from scratch in the embodied world. These results lend credibility to the hypothesis that solving abstract language-based tasks can help build priors that enable agents to generalize to unfamiliar embodied environments.   Our contributions are as follows:\\[-15pt]             
","  Given a simple request like Put a washed apple in the kitchen fridge, humans can reason in purely abstract terms by imagining action sequences and scoring their likelihood of success, prototypicality, and efficiency, all without moving a muscle.  Once we see the kitchen in question, we can update our abstract plans to fit the scene. Embodied agents require the same abilities, but existing work does not yet provide the infrastructure necessary for both reasoning abstractly and executing concretely.  We address this limitation by introducing \env{}, a simulator that enables agents to learn abstract, text-based policies in \tw and then execute goals from the ALFRED benchmark in a rich visual environment. \env{} enables the creation of a new \model agent whose abstract knowledge, learned in \tw, corresponds directly to concrete, visually grounded actions. In turn, as we demonstrate empirically, this fosters better agent generalization than training only in the visually grounded environment. \model's simple, modular design factors the problem to allow researchers to focus on models for improving every piece of the pipeline .",57
"  Annual Reports may extend up to 250 pages long as stated above, which contains different sections General Corporate Information, financial and operating cost, CEOs message, Narrative texts, accounting policies, Financial statement including balance sheet and summary of financial data documents. In the Financial narrative summarisation task, only the narrative section is summarised, which is not explicitly marked in the dataset, making it challenging and interesting.  In recent years, previous manual small-scale research in the Accounting and Finance literature has been scaled up with the aid of NLP and ML methods, for example, to examine approaches to retrieving structured content from financial reports, and to study the causes and consequences of corporate disclosure and financial reporting outcomes . \par Companies produce glossy brochures of annual reports with a much looser structure, and this makes automatic summarisation of narratives in UK annual reports a challenging task . Hence we summarize the narrative section of annual reports, particular narrative sentences that are spread loosely across the document need to be first identified and summarise those sentences. The summarisation limit is set to 1000 words, where the actual length of the report may go up to 250 pages long. Hence to summarize these long annual reports using a combination of extractive and abstractive summarisation.\par The text summary method can be classified into two paradigms: extractive and abstractive. The extractive summarisation method extracts the meaningful sentences or a section of text from the original text and combines them  to form a summary . Whereas abstractive summarisation generates words and sentences that are similar in meaning to the given text to form a summary that may not be in actual text . When summarizing long documents such as in our case up to 250 pages long, extractive summarisation may not produce a coherent and readable summary, and abstractive summarisation cannot cover complete information using encoder-decoder architecture. One problem is that typical seq2seq frameworks often generate unnatural summaries consisting of repeated words or phrases . Hence, we come up with a combination of extractive and abstractive summarisation to first select important narrative sentences and concisely convey them. \par Pointer Networks  is used in various combinatorial optimization problems, such as Travelling Salesman Problem , Convex hull optimization. We used pointer networks in our task of financial narrative summarization to extract relevant narrative sentences in a particular order to have a logical flow in summary. These extracted sentences are paraphrased to summarise these sentences in an abstractive way using the T-5 sequence-to-sequence model. We train the complete model by optimizing the ROUGE-LCS evaluation metric through a reinforcement learning objective.   % % The following footnote without marker is nebe fireded for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  .     %      % % final paper: en-us version             % space normally used by the marker     This work is licensed under a Creative Commons      Attribution 4.0 International License.     License details:     \url{http://creativecommons.org/licenses/by/4.0/}. } 
","   Companies provide annual reports to their shareholders at the end of the financial year that describes their operations and financial conditions. The average length of these reports is 80, and it may extend up to 250 pages long. In this paper, we propose our methodology PoinT-5  algorithms) that we used in the Financial Narrative Summarisation  2020 task. The proposed method uses Pointer networks to extract important narrative sentences from the report, and then T-5 is used to paraphrase extracted sentences into a concise yet informative sentence. We evaluate our method using $\operatorname{ROUGE}$-N , L,and SU4. The proposed method achieves the highest precision scores in all the metrics and highest F1 scores in $\operatorname{ROUGE}$ 1,and LCS and only solution to cross MUSE solution baseline in $\operatorname{ROUGE}$-LCS metrics.",58
"   Neural Architecture Search  methods aim to automatically discover neural architectures that perform well on a given task and dataset. These methods search over a space of possible model architectures, looking for ones that perform well on the task and will generalize to unseen data. There has been substantial prior work on how to define the architecture search space, search over that space, and estimate model performance .    Recent works, however, cast doubt on the quality and performance of NAS-optimized architectures , showing that current methods fail to find the best performing architectures for a given task and perform similarly to random architecture search.  In this work, we explore applications of a SOTA NAS algorithm, ENAS , to two sentence-pair tasks, paraphrase detection  and semantic textual similarity . We conduct a large set of experiments testing the effectiveness of ENAS-optimized RNN architectures across multiple models , embeddings  and datasets . We are the first, to our knowledge, to apply ENAS to PD and STS, to explore applications across multiple embeddings and traditionally LSTM-based NLP models, and to conduct extensive SOTA HPT across multiple ENAS-RNN architecture candidates.   Our experiments suggest that baseline LSTM models, with appropriate hyperparameter tuning , can sometimes match or exceed the performance of models with ENAS-RNNs. We also observe that random architectures sampled from the ENAS search space offer a strong baseline, and can sometimes outperform ENAS-RNNs. Given these observations, we recommend that researchers  conduct extensive HPT  across various candidate architectures for the fairest comparisons;  compare the performances of ENAS-RNNs against both standard architectures like LSTMs and RNN cells randomly sampled from the ENAS search space;  examine the computational  requirements of ENAS methods alongside the gains observed.   
","  Neural Architecture Search  methods, which automatically learn entire neural model or individual neural cell architectures, have recently achieved competitive or state-of-the-art  performance on variety of natural language processing and computer vision tasks, including language modeling, natural language inference, and image classification. In this work, we explore the applicability of a SOTA NAS algorithm, Efficient Neural Architecture Search  \cite{Pham2018EfficientNA} to two sentence pair tasks, paraphrase detection and semantic textual similarity. We use ENAS to perform a micro-level search and learn a task-optimized RNN cell architecture as a drop-in replacement for an LSTM. We explore the effectiveness of ENAS through experiments on three datasets , with two different models , and two sets of embeddings . In contrast to prior work applying ENAS to NLP tasks, our results are mixed -- we find that ENAS architectures sometimes, but not always, outperform LSTMs and perform similarly to random architecture search.",59
" Constituency parsing is a well-studied problem in natural language processing, but most state-of-the-art parsers have only been tested on written text, e.g.\ the standard Penn Treebank Wall Street Journal  dataset .  These recent neural parsers are commonly formulated as encoder-decoder systems, where the encoder learns the input sentence representation and the decoder learns to predict a parse tree. While input is often represented by word-level features, representation for the output trees varies:  as a sequence of parse symbols , a set of spans ,  syntactic distances , or per-word structure-rich labels . A key characteristic in many of these neural parsers is the recurrent network structure, particularly Long Short-Term Memory networks ; however, Kitaev and Klein  have shown that a non-recurrent encoder such as the Transformer network introduced in  is also capable of encoding timing information through self-attention mechanisms, achieving state-of-the-art parse results on the Treebank WSJ dataset.  Further, these parsers  %seem to mainly  benefit from contextualized information learned from larger external text data, such as ELMo  and BERT .  It is not clear that these advances will transfer to speech data, particularly for the different styles of speech. Even when perfect transcripts are available, speech poses many challenges to parsers learned from written text due to the lack of punctuation and case, and the presence of disfluencies.  On the other hand, speech signals carry rich information beyond words via variations in timing, intonation, and loudness, i.e. in prosody. Linguistic studies have shown that prosodic cues align with constituent structure , signal disfluencies by marking the interruption point , and help listeners resolve syntactic ambiguities . Empirical evidence, however, has been mixed regarding the utility of prosody for constituency parsing. Most gains have been observed when sentence boundaries are unknown , or with annotated prosodic labels . Most related to our current work, Tran et al.\  recently showed the benefit of using prosody in parsing within a sequence-to-sequence framework, proposing a convolutional neural network  as a mechanism to combine discrete word-level features with frame-level acoustic-prosodic features.  In this study, we extend the work in  and  to explore the utility of recent neural advances on spontaneous speech data, and compare the utility of prosody in read vs.\ spontaneous speech. Specifically, the goal of the current study is to answer the following questions:    % TT: may cut this if space is lacking. But I didn't want to end the intro with questions without saying anything further %The rest of this paper is organized as follows: Section  describes the models used in this work; Section  reviews the datasets and metrics in constituency parsing; Section  presents our experiments, results, and analyses; and Section  summarizes the findings.  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % Moved data table here since it was oddly arranged %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  \end{table*}\documentclass[a4paper]{article}  \usepackage{INTERSPEECH2019} \usepackage{url} \usepackage{multirow} \usepackage{xcolor} \usepackage{subcaption,enumitem} \usepackage{booktabs} \usepackage{comment}  \newcommand{\ttcomment}[1]{\textcolor{red}{\bf \small [#1 --TT]}} \newcommand{\jycomment}[1]{\textcolor{blue}{\bf \small [#1 --JY]}} \newcommand{\ylcomment}[1]{\textcolor{cyan}{\bf \small [#1 --YL]}} \newcommand{\mocomment}[1]{\textcolor{green}{\bf \small [#1 --MO]}}  \title{On the Role of Style in Parsing Speech with Neural Models} \name{Trang Tran, Jiahong Yuan, Yang Liu, Mari Ostendorf} %The maximum number of authors in the author list is twenty. If the number of contributing authors is more than twenty, they should be listed in a footnote or in acknowledgement section, as appropriate. \address{   Electrical \& Computer Engineering, University of Washington\\   LAIX Inc.} \email{\{ttmt001,ostendor\}@uw.edu, \{jiahong.yuan,yang.liu\}@liulishuo.com}   Index Terms: constituency parsing, prosody, spontaneous speech, contextualized embeddings %Index Terms: constituency parsing, prosody, spontaneous speech, read speech, switchboard, ELMo, BERT, contextualized embeddings %\ttcomment{take out some of these?}             \bibliographystyle{IEEEtran}  \bibliography{interspeech19}  \end{document} 
"," The differences in written text and conversational speech are substantial; previous parsers trained on treebanked text have given very poor results on spontaneous speech. For spoken language, the mismatch in style also extends to prosodic cues, though it is less well understood.  This paper re-examines the use of written text in parsing speech in the context of recent advances in neural language processing. We show that neural approaches facilitate using written text to improve  parsing of spontaneous speech, and that prosody further improves over this state-of-the-art result. Further, we find an asymmetric degradation from read vs.\ spontaneous mismatch, with spontaneous speech more generally useful for training parsers.  %  Prosodic information in the speech signal has been shown to correlate with syntactic structure of a sentence; however, the impact of prosody on parsing has been mixed. Recent results show a benefit for conversational speech, particularly in utterances with disfluencies, but there is little recent work on other speaking styles. In this work, we extend recent advances in constituency parsing of spontaneous speech, integrating acoustic-prosodic cues and achieving SOTA results on the Switchboard dataset. We then explore the performance of the parser on mismatched training/testing scenarios. Specifically, we show that training on spontaneous speech results in a small degradation when testing on read speech, while fine-tuning with WSJ read speech substantially degrades the performance on spontaneous speech.",60
" The recent progress in machine translation models has led researchers to question the use of n-gram overlap metrics such as BLEU, which focus solely on surface-level aspects of the generated text, and thus may correlate poorly with human evaluation. This has led to a surge of interest for more flexible metrics that use machine learning to capture semantic-level information. Popular examples of such metrics include YiSi-1, ESIM, BERTscore, the Sentence Mover's Similarity, and \BLEURT{}.  These metrics utilize contextual embeddings from large models such as BERT which have been shown to capture linguistic information beyond surface-level aspects.   The WMT Metrics 2020 Shared Task is the reference benchmark for evaluating these metrics in the context of machine translation. It tests the evaluation of systems that are to-English  and to other languages , which requires a multilingual approach. An additional challenge for learned metrics is that human ratings are not available for all language pairs, and therefore, the models must use unlabeled data and perform zero-shot generalization.   We describe several learned metrics based on \BLEURT{}~, originally developed for English data. We first extend \BLEURT{} to the multilingual setup, and show that our approach achieves competitive results on the WMT Metrics 2019 Shared Task.\footnote{We use the following languages for fine-tuning and/or testing: Chinese, Czech, German, English, Estonian, Finnish, French, Gujarati, Kazakh, Lithuanian, Russian, and Turkish. In addition, we also pre-train on Inuktitut, Japanese, Khmer, Pastho, Polish, Romanian, and Tamil.} We also present several simple BERT-based baselines, which we submit for analysis. Finally, we focus on English to German and enhance \BLEURT{}'s performance by combining its predictions with those of YiSi as well as by using alternative  references.   
"," The quality of machine translation systems has dramatically improved over the last decade, and as a result, evaluation has become an increasingly challenging problem. This paper describes our contribution to the WMT 2020 Metrics Shared Task, the main benchmark for automatic evaluation of translation. We make several submissions based on \BLEURT{}, a previously published metric which uses transfer learning. We extend the metric beyond English and evaluate it on 14 language pairs for which fine-tuning data is available, as well as 4 ``zero-shot'' language pairs, for which we have no labelled examples. Additionally, we focus on English to German and demonstrate how to combine \BLEURT{}'s predictions with those of YiSi and use alternative reference translations to enhance the performance. Empirical results show that the models achieve competitive results on the WMT Metrics 2019 Shared Task, indicating their promise for the 2020 edition.",61
"  There is a growing interest in using formal languages to study fundamental properties of neural architectures, which has led to the extraction of interpretable models .  Recent work has explored the generalized Dyck-n  languages, a subset of context-free languages.  consists of ``well-balanced'' strings of parentheses with  different types of bracket pairs, and it is the canonical formal language to study nested structures.  \citet{weiss2018practical} show that LSTMs  are a variant of the -counter machine and can recognize  languages. The dynamic counting mechanisms, however, are not sufficient for  as it requires emulating a pushdown automata.  \citet{hahn2020theoretical} shows that for a sufficiently large length, Transformers  will fail to transduce the  language.     We empirically show that with the addition of a starting symbol to the vocabulary,  a two-layer multi-headed SA network  is able to learn  languages, and generalize to longer sequences, although not perfectly. As shown in Figure , the network is able to identify the corresponding closing bracket for an opening bracket, in what resembles a stack-based automaton. For example, the symbol ``]'' in the string ``'', will first pop ``['' from the stack, then it attends to `` enables the model to learn the occurrence of the end of a clause or the end of the sequence, which can be regarded as a mechanism to represent an empty stack.   Our work is the first to perform an empirical exploration of SA on formal languages. We present detailed comparison between an SA which incorporates a starting symbol , and one that does not , and demonstrate significant differences in their generalization across the length of sequences and the depth of dependencies.   Recent work has suggested that the ability of self-attention mechanisms to model hierarchical structures is limited. \citet{shen2019ordered} show that the performance of Transformers on tasks such as logical inference and ListOps is either poor or worse than LSTMs. \citet{tran2018importance} have also reported similar results on SA, concluding that recurrence is necessary to model hierarchical structures. In comparison, our results show that SA outperforms LSTM on  languages except for  on longer sequences.  \citet{papadimitriou2020pretraining} posit that the ability of neural models to learn hierarchical structures can be attributed to a ``looking back'' capability, rather than directly encoding hierarchies. Our analysis sheds light on the ability of SA to learn hierarchical structures by elegantly attending to the correct preceding symbol.  
","   We focus on the recognition of Dyck-n  languages with self-attention  networks, which has been deemed to be a difficult task for these networks. We compare the performance of two variants of SA, one with a starting symbol  and one without . Our results show that SA$^+$ is able to generalize to longer sequences and deeper dependencies. For $\mathcal{D}_2$, we find that SA$^-$ completely breaks down on long sequences whereas the accuracy of SA$^+$ is 58.82$\%$. We find attention maps learned by SA$^+$ to be amenable to interpretation and compatible with a stack-based language recognizer. Surprisingly, the performance of SA networks is at par with LSTMs, which provides evidence on the ability of SA to learn hierarchies without recursion.",62
" 	 	Although neural machine translation  has achieved great progress in recent years , when fed an entire document, standard NMT systems translate sentences in isolation without considering the cross-sentence dependencies. Consequently, document-level neural machine translation  methods are proposed to utilize source-side or target-side inter-sentence contextual information to improve translation quality over sentences in a document . 	 	More recently, researchers of DocNMT mainly focus on exploring various attention-based networks to leverage the cross-sentence context efficiently, and evaluate the special discourse phenomena . However, there is still an issue that has received less attention: which context sentences should be used when translating a source sentence? 	 	 			 			 		\end{center} 	\end{table} 	 	 	We conduct an experiment to verify an intuition: the translation of different source sentences requires different context. As shown in Table , we train two DocNMT models and test them using various context settings\footnote{We apply a typical DocNMT method  to train models on ZhEn TED, and select 1,000 sentences to test. The BLEU of sentence-level baseline is 20.06.}. During the test, we obtain dynamic context sentences that achieve the best BLEU scores by traversing all the context combinations for each source sentence. Compared with the fixed size context , dynamic context  can significantly improve translation quality. Although row 2 uses more context, redundant information may hurt the results. Experiments indicate that only the limited context sentences are really useful, and they change with source sentences. 	 	Majority of existing DocNMT models set the context size or scope to be fixed. They utilize all of the 	previous  context sentences , or the full context in the entire document . As a result, the inadequacy or redundancy of contextual information is almost inevitable. From this viewpoint, \citet{maruf2019selective} propose a selective attention approach that uses the sparsemax function  instead of the softmax to normalize the attention weights. The sparsemax assigns the low probability in softmax to zero so that the model can focus on the sentences with high probability. However, the learning of attention weights lacks guidance, and they cannot handle the situation where the source sentences achieve the best translation results without relying on any context, which happens in about 39.4\% of sentences in the experiment. 	 	To address the problem, we propose an effective approach to select contextual sentences {\bf dynamically} for each source sentence in the document-level translation. Specifically, we propose a Context Scorer to score each candidate context sentence according to the currently translated source sentence. Then, we utilize two selection strategies to select useful context sentences for the translation module. The size of selected context is variable for different sentences. A core challenge of our approach is that the selection process is non-differentiable. Therefore, we leverage the reinforcement learning  method to train the selection and DocNMT modules together. We design a novel reward to encourage the model to be aware of different context sentences and select more appropriate context to improve translation quality. 	 	In this paper, we make the following contributions: 	 	 	
"," 		Document-level neural machine translation has yielded attractive improvements. However, majority of existing methods roughly use all context sentences in a fixed scope. They neglect the fact that different source sentences need different sizes of context. To address this problem, we propose an effective approach to select dynamic context so that the document-level translation model can utilize the more useful selected context sentences to produce better translations. Specifically, we introduce a selection module that is independent of the translation module to score each candidate context sentence. Then, we propose two strategies to explicitly select a variable number of context sentences and feed them into the translation module. We train the two modules end-to-end via reinforcement learning. A novel reward is proposed to encourage the selection and utilization of dynamic context sentences. Experiments demonstrate that our approach can select adaptive context sentences for different source sentences, and significantly improves the performance of document-level translation methods.",63
"  \vsec Automatic text summarization\footnote{We refer to abstractive summarization in this paper.} is an attractive technique for helping humans to grasp the content of documents effortlessly. While supervised neural methods have shown good performances, the unsupervised approach is starting to attract interest due to its advantage of not requiring costly parallel corpora. However, the empirical performance of unsupervised methods is currently behind that of state-of-the-art supervised models. Unsupervised text summarization is still developing and is now at the stage where various solutions should be actively explored.     One previous unsupervised approach extends neural encoder-decoder modeling to the zero paired data scenario, where a model is trained with a paradigm called compression-reconstruction  learning. The mechanism is similar to that of the back-translation: the model consists of a compressor  and a reconstructor, and they are co-trained so that the reconstructor can recover the original sentence from the summary generated by the compressor~. Experimental results showed that such an unsupervised encoder-decoder-based summarizer is able to learn the mapping from a sentence to a summary without paired data. % Also, \citealp{zhou-rush-2019-simple} proposes a more straightforward method that mimics the reconstruction part by means of contextual similarity between an original input sentence and a top of a generating summary. % However, the performance of any unsupervised methods is still deficient compared to the latest supervised models.   Reinforcement learning  is also a potential solution for the no paired data situation. In related fields, for example, there are unsupervised methods for text simplification and text compression with policy-gradient learning. Recent RL techniques take a value-based approach  such as DQN or the combination of policy and value-based approaches such as Asynchronous Advantage Actor-Critic. A critical requirement to leverage a value-based method is a value function that represents the goodness of an action on a given state. We can naturally define the value function by utilizing the CR-learning paradigm, and it makes the latest value-based approaches available for unsupervised text summarization. % , and they require to define value-function. % We can leverage the values-based approach  % A crucial requirement for RL is a value function that represents a goodness of action on a given state. % We can satisfy the requirement by leveraging the definition in CR learning paradigm. % One concern is, however, that RL with large action space   generally has difficulty in the training. % In addition, the latest techniques to improve RL are from a value-based approach  such as DQN or the combination of policy-based and value-based approaches such as Asynchronous Advantage Actor-Critic.   In this paper, we propose a new method based on Q-learning and an edit-based summarization~. The edit-based summarization generates a summary by operating an edit action  for each word in the input sentence. Our method implements the editing process with two modules: 1) an {\bf E}ditorial {\bf A}gent that predicts edit actions, and 2) a {\bf L}anguage {\bf M}model  converter that deterministically decodes a sentence on the basis of action signals, which we call \ealm. The CR learning is defined on the Q-learning framework to train the agent to predict edit actions that instruct the LM converter to produce a good summary. Although a vast action space causing sparsity in reward, such as the word generation of an encoder-decoder model, is generally difficult to be learned in RL, our method mitigates this issue thanks to its fewer edit actions and the deterministic decoding of a language model. Moreover, the formulation by Q-learning enables us to incorporate the latest techniques in RL.  The main contribution of this paper is that we provide a new solution in the form of an unsupervised edit-based summarization leveraging Q-learning and a language model. Experimental results show that our method achieved a competitive performance with encoder-decoder-based methods even with truly no paired data , and qualitative analysis brings insights as to what current unsupervised models are missing. Also, the problem formulation on Q-learning enables us to import the latest techniques in RL, which leads to potential improvements in future research.  % 2) We propose the first Q-learning-based method that uses a pre-trained language model. % , which mitigates the issues prevalent among the previous methods. % Empirically, our method shows a competitive performance in the news corpus benchmarks with truly no paired data . % Also, our method requires no parallel data even for validation; therefore, it can be instantly applicable to any situation if there is a language model.  % Our proposed approach brings new insights to the growing field of unsupervised text summarization, and will pave the way to future development.  % This paper is organized as follows: Section defines the problem statement of unsupervised text summarization with the \algoname\ paradigm. % After reviewing the previous methods in Section, we introduce our approach in Section . % Then, we report experimental results in Section . % Discussing insights from the experiment in Section , we conclude the contribution of this paper for future unsupervised text summarization in Section .    % Text Summarization is a task to transform an input sentence into an informative summary . % Although supervised summarization models like encoder-decoder have shown success for these years , it still has an issue to demand us to create massive parallel data. % The question ``how we can model the transformation only from the input sentences?"" attracts research interests, and known as unsupervised text summarization .  % In unsupervised text summarization, only the input sentences are available for training a model. % Instead, it holds a hypothesis: a summary should contain information about its input sentence to some extent that we can guess the original contents. % And, lgoname is an approach to leverage this hypothesis .  % In \algoname, we prepare two modules, the one for compression that produces the summary  from the input sentence, and the other one for reconstruction that re-produces the input sentence from the generated summary. % These two modules are optimized based on the hypothesis, more specifically, minimizing the difference between the input sentence and the reconstructed sentence while the compressed sentence  is satisfying essential properties such as shortness or readability . % In previous studies, they use generative models such as encoder-decoder for compression and reconstruction, and directly train them to output desired sentences . % We illustrate the flow in the left-hand side of Figure .  % Our proposed method is also on top of the same paradigm but uses different modules, {\bf Q-learning agent} and {\bf fixed-language model} .\footnote{A pretrained language model that is not fine-tuned, i.e., fixed, during training.} % As illustrated in the right-hand side of Figure , the agent determines action, whether to remove, keep, or replace each word in the input sentence. % Receiving the action signals, the fixed-LM deterministically produces compressed and reconstructed sentences. % In short, we train the agent to properly control the fixed-LM so that we obtain desired sentences as the results of compression and reconstruction.  % The primary contribution of this paper is to provide a new option leveraging Q-learning with a language model to the growing field of unsupervised text summarization. % Introducing Q-learning, we open the problem to sophisticated techniques on value-based Reinforcement Learning  algorithms , which is not covered only with policy-based RL algorithms employed so far.\footnote{RL algorithms are classified into value-based  and policy-based . To the best of our knowledge, most of the text summarization methods with RL, both in supervised and unsupervised settings, leverages policy-based RL algorithms . Combining such a previous policy-based and our value-based methods for sentence compression will lead to the applicability of more advanced RL algorithms such as Actor-Critic  and Asynchronous Advantage Actor-Critic .} % Also, proposing an approach to fixedly utilize the pre-trained language model, we benefit from its powerful performance capturing sentence semantics along with mitigating issues generative models inherently hold such as complexity in co-training of multiple generators or repetition in decoding. % Experimentally, our approach shows promising results; it achieves competitive performance in standard datasets and outperforms the previous generator models in out-of-domain circumstances. % This paper brings novel insights for unsupervised text summarization and contributes to be flourishing in the future.  % This paper is organized as follows: Section defines the problem statement of unsupervised text summarization with the \algoname\ paradigm. % After reviewing the previous methods in Section, we introduce our approach in Section . % Then, we report experimental results in Section . % Discussing insights from the experiment in Section , we conclude the contribution of this paper for future unsupervised text summarization in Section .  \vsecu 
"," % Unsupervised methods for abstractive text summarization are attractive because they do not require parallel corpora. % However, their performance is still somehow lacking, therefore research on promising solutions is ongoing. % In this paper, we propose a new approach based on Q-learning with an edit-based summarization. % Our method combines two key modules to form an {\bf E}ditorial {\bf A}gent and {\bf L}anguage {\bf M}odel converter~. % The agent predicts edit actions, and then the LM converter deterministically generates a summary on the basis of the action signals. % Q-learning is leveraged to train the agent to output proper edit actions. % Experimental results show that \ealm~has a competitive performance compared with the previous encoder-decoder-based methods, even with truly zero paired data .  % Defining the task as Q-learning enables us not only to develop a competitive method but also to make the latest techniques in reinforcement learning available for unsupervised summarization. % We also conduct qualitative analysis and provide insights on future work for the current unsupervised summarizers.\footnote{Our codes are available at \url{https://github.com/kohilin/ealm}} Unsupervised methods are promising for abstractive textsummarization in that the parallel corpora is not required.  However, their performance is still far from being satisfied, therefore research on promising solutions is on-going.   In this paper, we propose a new approach based on Q-learning with an edit-based summarization.  The method combines two key modules to form an Editorial Agent and Language Model converter .  The agent predicts edit actions , and then the LM converter deterministically generates a summary on the basis of the action signals.  Q-learning is leveraged to train the agent to produce proper edit actions.  Experimental results show that \ealm~delivered competitive performance compared with the previous encoder-decoder-based methods, even with truly zero paired data . Defining the task as Q-learning enables us not only to develop a competitive method but also to make the latest techniques in reinforcement learning available for unsupervised summarization. We also conduct qualitative analysis, providing insights into future study on unsupervised summarizers.\footnote{Our codes are available at \url{https://github.com/kohilin/ealm}}",64
" Neural machine translation  systems are data driven models, which highly depend on the training corpus.  NMT models have a tendency towards over-fitting to frequent observations  while neglecting those low-frequency observations.  Unfortunately, there exists a token imbalance phenomenon in natural languages as different tokens appear with different frequencies, which roughly obey the Zipf's Law.  Table shows that there is a serious imbalance between high-frequency tokens and low-frequency tokens.  NMT models rarely have the opportunity to learn and generate those ground-truth low-frequency tokens in the training process. %It is harder for the NMT model to generate ground-truth low-frequency tokens even in the training process.  %Compared to the reference, the NMT model tends to generate more high-frequency tokens and less low-frequency tokens, which hurts the translation quality.  Some work tries to improve the rare word translation by maintaining phrase tables or back-off vocabulary or adding extra components, which bring in extra training complexity and computing expense.  Some NMT techniques which are based on smaller translation granularity can alleviate this issue, such as hybrid word-character-based model, BPE-based model and word-piece-based model. %For example, the sub-word model adapted byte pair encoding  technique to the task of word segmentation.  These effective work alleviate the token imbalance phenomenon to a certain extent and become the de-facto standard in most NMT models.  Although sub-word based NMT models have achieved significant improvements, they still face the token-level frequency imbalance phenomenon, as Table shows.  %It is obvious that there are always low-frequency tokens no matter what the number of merge operations of BPE is. %As shown in Table, the rare word 'slower' is split into two tokens as 'slow' and 'er', there still exist obvious token-level imbalance between 'slow' and other tokens.   \iffalse            \end{table} \fi    \iffalse  \fi   \iffalse            \end{table} \fi Furthermore, current NMT models generally assign equal training weights to target tokens without considering their frequencies.  It is very likely for NMT models to ignore the loss produced by the low-frequency tokens because of their small proportion in the training sets. The parameters related to them can not be adequately trained, which will, in turn, make NMT models tend to prioritize output fluency over translation adequacy, and ignore the generation of low-frequency tokens during decoding, which is illustrated in Table. It shows that the vanilla NMT model tends to generate more high-frequency tokens and less low-frequency tokens. %This will, in turn, make the model %tend to generate too many high-frequency tokens and too less low-frequency tokens during decoding. However, low-frequency tokens may carry critical semantic information which may affect translation quality once they are neglected.   %It is very likely for NMT models to ignore the loss produced by rare words so that the patterns learned by the encoder, decoder, or attention modules from them can't be adequately updated. What's more, NMT models tend to prioritize output fluency over translation adequacy and ignore the translation of rare words during generation.  %In our experiments, we observed that vanilla NMT models usually produce more frequent words and less rare words than real references. Therefore, some techniques should be adopted to improve the translation of rare words. %distribution.   %It is obvious that there are always rare tokens no matter what the number of merge operations of BPE is and the problem of token distribution imbalance still exists.  %One of the advantages of this technique is that it reduces the number of rare words by splitting them into more frequent subword tokens , which in fact  %relieve the imbalance of word   %The strength is that NMT models can make use of large amounts of parallel training sentences and learn the knowledge and features embodied in the training data. However, one of the weaknesses is that NMT models have a tendency towards over-fitting to frequent observations , but neglecting those rare cases which are not frequently observed. Unfortunately, there is a natural word distribution imbalance in the corpus. According to the Zipf's Law, the frequency of any word is inversely proportional to its ranking in the frequency table, which indicates that the occurrences of some words are far more than others naturally.     %For word-level NMT models, NMT has its limitation in handling a larger vocabulary because of the training complexity and computing expense.   % %In their work, they first represent each word as a sequence of characters and then iteratively combine the most frequent pair as a new symbol. %which achieved better accuracy for the translation of rare words %, we seek to further alleviate the token imbalance problem based on the above analysis. For this purpose,  To address the above issue, we proposed token-level adaptive training objectives based on target token frequencies.  We aimed that those meaningful but relatively low-frequency tokens could be assigned with larger loss weights during training so that the model will learn more about them. %In our objectives, those relatively low-frequency but valuable tokens will be assigned with larger loss weights during training to encourage the model to learn more about them. To explore suitable adaptive objectives for NMT, we first applied existing adaptive objectives from other tasks to NMT and analyzed their performance. We found that though they could bring modest improvement on the translation of low-frequency tokens, they did much damage to the translation of high-frequency tokens, which led to an obvious degradation on the overall performance. This implies that the objective should ensure the training of high-frequency tokens first. %training of high-frequency tokens should be ensured first. %We should ensure the training of high-frequency tokens and enlarge the weights of low-frequency tokens at the same time. %We firstly tried the focal loss, which was proposed for solving the token imbalance problem in the CV task, and analyzed the performance.  Then, based on our observations, we proposed two heuristic criteria for designing the token-level adaptive objectives based on the target token frequencies. Last, we presented two specific forms for different application scenarios according to the criteria. Our method yields consistent improvements in translation quality on ZH-EN, EN-RO, and EN-DE translation tasks, especially on sentences that contain more low-frequency tokens where we can get 1.68, 1.02, and 0.52 BLEU increases compared with baseline, respectively. Further analyses show that our method can also improve the lexical diversity of translation. %We carried out experiments on ZHEN, ENRO, and ENDE translation tasks to validate our methods. The experimental results show that our methods achieve significant improvement in translation quality, especially in sentences that contain more low-frequency tokens.  %Besides, the token distribution of our translations becomes closer to references for test sets.  %Besides, our method also improves the diversity of the translations.   Our contributions can be summarized as follows:   %More specifically, NMT models are first trained with equal weights and then fine-tuned with well-defined weights introduced by the scoring functions. In this way, it won't hurt the translation of frequent tokens, but also can improve the translation of rare tokens to a certain degree. To the best of our knowledge, this is the first work trying to concern about the training weights at the token level to solve the distribution imbalance problem in NMT. The experiments on multiple translation tasks show that our method can improve the overall translation performance without almost any additional computing or storage expense. And the analysis experiments indicate that our method can improve the rare tokens translation significantly and the tokens distribution of our translation are much closer to the references than the baseline translations.  
"," There exists a token imbalance phenomenon in natural language as different tokens appear with different frequencies, which leads to different learning difficulties for tokens in Neural Machine Translation .  The vanilla NMT model usually adopts trivial equal-weighted objectives for target tokens with different frequencies and tends to generate more high-frequency tokens and less low-frequency tokens compared with the golden token distribution. %%% However, low-frequency tokens may carry critical semantic information that will affect the translation quality once they are neglected.   In this paper, we explored target token-level adaptive objectives based on token frequencies to assign appropriate weights for each target token during training.  We aimed that those meaningful but relatively low-frequency words could be assigned with larger weights in objectives to encourage the model to pay more attention to these tokens. %More specifically, those relatively low-frequency but valuable target tokens will be assigned with larger weights in objectives to encourage the model to pay more attention to these tokens. %%% %We conducted experiments  Our method yields consistent improvements in translation quality on ZH-EN, EN-RO, and EN-DE translation tasks, especially on sentences that contain more low-frequency tokens where we can get 1.68, 1.02, and 0.52 BLEU increases compared with baseline, respectively. Further analyses show that our method can also improve the lexical diversity of translation. %Experiments on multiple translation tasks show that our methods can achieve significant improvement in translation quality, especially on sentences that contain more low-frequency tokens.  %Besides, our method also improves translation diversity. %Besides, the token distribution of our translations becomes closer to the reference of test sets.  %.  %Rare words translation has always been one of the key challenges to Neural Machine Translation .",65
"   Graph structures play a pivotal role in NLP because they are able to capture particularly rich structural information. For example, Figure shows a directed, labeled Abstract Meaning Representation  graph, where each node denotes a semantic concept and each edge denotes a relation between such concepts. Within the realm of work on AMR, we focus in this paper on the problem of AMR-to-text generation, i.e. transducing AMR graphs into text that conveys the information in the AMR structure. A key challenge in this task is to efficiently learn useful representations of the AMR graphs. Early efforts  neglect a significant part of the structural information in the input graph by linearizing it. Recently, Graph Neural Networks  have been explored to better encode structural information for this task .   % \tzy{papers before 2018??? Gated Graph Neural networks??? Do not miss an important paper.}     One type of such GNNs is Graph Convolutional Networks .  GCNs follow a local information aggregation scheme, iteratively updating the representations of nodes based on their immediate  neighbors.  Intuitively, stacking more convolutional layers in GCNs helps capture more complex interactions .  However, prior efforts  have shown that the locality property of existing GCNs precludes efficient non-local information propagation. \citet{AbuElHaija2019MixHopHG} further proved that vanilla GCNs are unable to capture feature differences among neighbors from different orders no matter how many layers are stacked. Therefore, Self-Attention Networks  have been explored as an alternative to capture global dependencies. As shown in Figure , SANs associate each node with other nodes such that we model interactions between any two nodes in the graph. Still, this approach ignores the structure of the original graph. \citet{Zhu2019ModelingGS} and \citet{Cai2019GraphTF} propose structured SANs that incorporate additional neural components to encode the structural information of the input graph.   Convolutional operations, however, are more computationally efficient than self-attention operations because the computation of attention weights scales quadratically while convolutions scale linearly with respect to the input length . Therefore, it is worthwhile to explore the possibility of models based on graph convolutions. One potential approach that has been considered is to incorporate information from higher order neighbors, which helps to facilitate non-local information aggregation for node classification . However, simple concatenation of different order representations may not be able to model complex interactions in semantics for text generation .    We propose to better integrate high-order information, by introducing a novel dynamic fusion mechanism and propose the Lightweight, Dynamic Graph Convolutional Networks . As shown in Figure  , nodes in the LDGCN model are able to integrate information from first to third-order neighbors. With the help of the dynamic mechanism, LDGCNs can effectively synthesize information from different orders to model complex interactions in the AMR graph for text generation. Also, LDGCNs require no additional computational overhead, in contrast to vanilla GCN models. We further develop two novel weight sharing strategies based on the group graph convolutions and weight tied convolutions. These strategies allow the LDGCN model to reduce memory usage and model complexity.  Experiments on AMR-to-text generation show that LDGCNs outperform best reported GCNs and SANs trained on LDC2015E86 and LDC2017T10 with significantly fewer parameters. On the large-scale semi-supervised setting, our model is also consistently better than others, showing the effectiveness of the model on a large training set. We release our code and pretrained models at \url{https://github.com/yanzhang92/LDGCNs}.\footnote{Our implementation is based on  MXNET  and the Sockeye toolkit .}   
"," 	 	% Camera-Ready 	AMR-to-text generation is used to transduce Abstract Meaning Representation structures  into text. A key challenge in this task is to efficiently learn effective graph representations. Previously, Graph Convolution Networks  were used to encode input AMRs, however, vanilla GCNs are not able to capture non-local information and additionally, they follow a local  information aggregation scheme. To account for these issues, larger and deeper GCN models are required to capture more complex interactions. In this paper, we introduce a dynamic fusion mechanism, proposing Lightweight Dynamic Graph Convolutional Networks  that capture richer non-local interactions by synthesizing higher order information from the input graphs. We further develop two novel parameter saving strategies based on the group graph convolutions and weight tied convolutions to reduce memory usage and model complexity. With the help of these strategies, we are able to train a model with fewer parameters while maintaining the model capacity. Experiments demonstrate that LDGCNs outperform state-of-the-art models on two benchmark datasets for AMR-to-text generation with significantly fewer parameters.",66
"    A natural way to consider two parallel sentences in different languages is that each language expresses the same underlying meaning from a different viewpoint.  Each language can be thought of as a transformation that maps an underlying concept into a view that we collectively agree is determined as `English' or `French'.  Similarly, an image of a cat and the word `cat' are expressing two views of the same underlying concept.  In this case, the image corresponds to a high bandwidth channel and the word `cat' to a low bandwidth channel.  This way of conceptualizing parallel viewpoints naturally leads to the formulation of a fully generative model over each instance, where the transformation corresponds to a particular generation of the underlying view.  We define each of these views as a channel. As a concrete example, given a parallel corpus of English and French sentences, English and French become two channels, and the corresponding generative model becomes .  One key advantage of this formulation is that a single model can be trained to capture the full expressivity of the underlying concept, allowing us to compute conditionals and marginals along with the joint.  In parallel sentences, the conditionals correspond to translations from one channel to another while the marginals correspond to standard monolingual language models.  In this work, we present a general framework for modeling the joint distribution  over  channels by marginalizing over all possible factorizations across the channels and within each channel.  This formulation allows our framework to perform: 1) unconditional generation, 2) fully conditional generation , and 3) partial conditional generation .  The key contributions in this work are:   We highlight that while we focus on languages as a specific instantiation of a channel, our framework can generalize to any arbitrary specification, such as other types of tasks  or other modalities .   %%%%%%%%%%%%%%%%%%%%%%%%%%% 
"," A channel corresponds to a viewpoint or transformation of an underlying meaning. A pair of parallel sentences in English and French express the same underlying meaning, but through two separate channels corresponding to their languages. In this work, we present the Multichannel Generative Language Model . MGLM is a generative joint distribution model over channels. MGLM marginalizes over all possible factorizations within and across all channels.  MGLM endows flexible inference, including unconditional generation, conditional generation , and partially observed generation .  We experiment with the Multi30K dataset containing English, French, Czech, and German. We demonstrate experiments with unconditional, conditional, and partially conditional generation. We provide qualitative samples sampled unconditionally from the generative joint distribution. We also quantitatively analyze the quality-diversity trade-offs and find MGLM outperforms traditional bilingual discriminative models.",67
" Neural machine translation  has achieved promising results with the use of various optimization tricks.  In spite of that, these techniques lead to increased training time and massive hyper-parameters, making the development of a well-performed system expensive.  As an alternative mitigation, curriculum learning~\citep[CL,][]{elman1993learning,bengio2009curriculum} has shown its effectiveness on speeding up the convergence and stabilizing the NMT model training.  CL teaches NMT model from easy examples to complex ones rather than equally considering all samples, where the keys lie in the definition of ``difficulty'' and the strategy of curricula design. Existing studies artificially determine data difficulty according to prior linguistic knowledge such as sentence length  and word rarity , and manually tune the learning schedule.  However, neither there exists a clear distinction between easy and hard examples, nor these human intuitions exactly conform to effective model training.  Instead, we resolve this problem by introducing self-paced learning, where the emphasis of learning can be dynamically determined by model itself rather than human intuitions. Specifically, our model measures the level of confidence on each training example, where an easy sample is actually the one of high confidence by the current trained model. Then, the confidence score is served as a factor to weight the loss of its corresponding example. In this way, the training process can be dynamically guided by model itself, refraining from human predefined patterns.   We evaluate our proposed method on IWSLT15 EnVi, WMT14 EnDe, as well as WMT17 ZhEn translation tasks. Experimental results reveal that our approach consistently yields better translation quality and faster convergence speed than Transformer baseline and recent models that exploit CL. Quantitative analyses further confirm that the intuitive curriculum schedule for a human does not fully cope with that for model learning.  
"," Recent studies have proven that the training of neural machine translation  can be facilitated by mimicking the learning process of humans. Nevertheless, achievements of such kind of curriculum learning rely on the quality of artificial schedule drawn up with the hand-crafted features, e.g. sentence length or word rarity. We ameliorate this procedure with a more flexible manner by proposing self-paced learning, where NMT model is allowed to 1) automatically quantify the learning confidence over training examples; and 2) flexibly govern its learning via regulating the loss in each iteration step.  Experimental results over multiple translation tasks demonstrate that the proposed model yields better performance than strong baselines and those models trained with human-designed curricula on both translation quality and convergence speed.\footnote{Our codes:  \href{https://github.com/NLP2CT/SPL_for_NMT}{https://github.com/NLP2CT/SPL\_for\_NMT}.}",68
" In recent years, cyberbullying has become one of the most pressing online risks among youth and raised serious concerns in society. Cyberbullying is commonly defined as the electronic transmission of insulting or embarrassing comments, photos or videos, as illustrated in Figure~ . Harmful bullying behavior can include posting rumors, threats, pejorative labels, and sexual remarks. Research from the American Psychological Association and the White House has revealed more than  of young people in the US indicate that they have been bullied on social media platforms~. Such a growing prevalence of cyberbullying on social media has detrimental societal effects, such as victims may experience lower self-esteem, increased suicidal ideation, and a variety of negative emotional responses~. Therefore, it has become critically important to be able to detect and prevent cyberbullying on social media. Research in computer science aimed at identifying, predicting, and ultimately preventing cyberbullying through better understanding the nature and key characteristics of online cyberbullying.     In the literature, existing efforts toward automatically detecting cyberbullying have primarily focused on textual analysis of user comments, including keywords~ and sentiments analysis ~. These studies attempt to build a generic binary classifier by taking high-dimensional text features as the input and make predictions accordingly. Despite their satisfactory detection performance in practice, these models largely overlooked temporal information of cyberbullying behaviors. They also ignore user interactions in social networks. Furthermore, the majority of these methods focus on detecting cyberbullying sessions effectively but cannot explain ``why'' a media session was detected as cyberbullying. Given a sequence of comments with user attributes, we think sequential learning can allow us to better exploit and model the evolution and correlations among individual comments. Besides, graph-based learning can enable us to represent and learn how users interact with each other in a session.   This work aims to detect cyberbullying by jointly exploring explainable information from user comments on social media. To this end, we build an explainable cyberbullying detection framework, \underline{HE}terogeneous \underline{N}eural \underline{I}nteraction \underline{N}etworks , through a coherent process. HENIN consists of three main components that learn various interactions among heterogeneous information displayed in social media sessions. A comment encoder is created to learn the representations of user comments through a hierarchical self-attention neural network so that the semantic and syntactic cues on cyberbullying can be captured. We create a post-comment co-attention mechanism to learn the interactions between a posted text and its comments. Moreover, two graph convolutional networks are leveraged to learn the latent representations depicting how sessions interact with one another in terms of users, and how posts are correlated with each other in terms of words.  Specifically, we address several challenges in this work:  how to perform explainable cyberbullying detection that can boost detection performance,  how to highlight explainable comments without the ground truth,  how to model the correlation between posted text and user comments, and  how to model the interactions between sessions in terms of users, and the interactions between textual posts in terms of words. Our solutions to these challenges result in a novel framework HENIN.   Our contributions are summarized as follows. %   
"," In the computational detection of cyberbullying, existing work largely focused on building generic classifiers that rely exclusively on text analysis of social media sessions. Despite their empirical success, we argue that a critical missing piece is the model explainability, i.e., why a particular piece of media session is detected as cyberbullying. In this paper, therefore, we propose a novel deep model, HEterogeneous Neural Interaction Networks , for explainable cyberbullying detection. HENIN contains the following components: a comment encoder, a post-comment co-attention sub-network, and session-session and post-post interaction extractors. Extensive experiments conducted on real datasets exhibit not only the promising performance of HENIN, but also highlight evidential comments so that one can understand why a media session is identified as cyberbullying.",69
"  \zc{ Title: need to be more concrete, something like ""Denoising Multi-Source Weak Supervision for Neural Text Classification"" will probably be better  Introduction:  Paragraph 1: many NLP tasks can be formulated as text classification  dnns are successful  but they require labeled data, which are expensive to obtain  recently, pre-trained language models can alleviate this problem, but still suffers degraded performance when labeled data is limited. \wendi{BERT still need labeled data}  Paragraph 2: weak supervision is promising, but also challenging to apply because weak labels are inaccurate and incomplete.  Paragraph 3: we study using multiple weak supervision sources to learn text classifiers; the intuition is multiple weak supervision sources can provide complementary information to eliminate noise; and combined with unlabeled data, they can address label incompleteness as well. \wendi{key: complementary information; bootstrapping on D_U}  Paragraph 4: there are a large body of works on weakly-supervised learning, most are dealing with only single-source weak supervision  they may suffer from the unreliability of single sources and error propagation; \wendi{sensitive to single source} several works deal with multiple sources, but they XXX , need to make sure we cite and discuss them).  Paragraph 5: introduce our method, the key idea, the uniqueness compared with existing methods. I feel the current method description is a bit plain, need to distill the main ideas. I think the main ideas are: - source reliability estimation and neural classification benefit each other  the co-training framework  \wendi{regularization} - conditional source reliability - self-training to leverage unmatched samples to obtain more labeled instances. - maybe also mention we rely on pre-trained language models to get good representations, which helps denoising  \wendi{high level: denoise, and how to enhance}  Other Sections: Section 2: make it at most half a page Section 3: 2.5 pages Section 4: 3 pages others: 1 page  Something we had better show in the experiments: - multi-source weak supervision can be powerful   for this, we already have a lot of results - majority voting does not work - our method works better than existing weak supervision methods  - what happens if we use some subsets of the multiple weak supervision sources - are there any interpretations about the source reliability we learned - how the different designs in our method work  - would labeled data help further }  Text classification, relation extraction, question answering are the fundamental natural language tasks with numerous applications such as document classification or knowledge extraction.  \zc{ Many NLP tasks can be formulated as text classification   problems, such as sentiment analysis, topic classification, relation   extraction, and XXX .} Recently, deep neural nets  have demonstrated superior performance for this problem \zc{briefly mention earlier dnns , to the recent trend of BERT-based ones}, largely due to their capabilities of automatically learning distributed features and fitting complex functions based on large-scale training data.   However, in many real world scenarios, large-scale labeled data are unavailable and manually annotating data at a large scale is prohibitively expensive. \zc{merge paragraph 1 and 2}  To address the label scarcity bottleneck, we study the problem of  using heuristic rules to train neural text classifiers.  While domain experts \zc{not   necessarily domain experts, can be also KBs.} often cannot afford to annotate millions of documents carefully, they can easily provide a set of heuristic rules as weak supervision signals.  Using such rules can automatically induce labeled data for model training , but meanwhile it introduces two major challenges: label noise and low label coverage. %The first challenge is label noise.   The label noise issue arises because heuristic rules are often too simple to capture the rich contexts and complex patterns for text classification. For instance, while a rule `expensive \ negative' for restaurant ranking is correct for most times, but sometimes it wrong because the delicious food deserves the high price. Seed rules have limited coverage because real-life text corpora often have long-tail distributions, many heuristic rules are defined over the most frequent keywords, so the instances containing only long-tail keywords cannot be covered by any given rules. \zc{can merge the previous paragraph and shorten it.}  There have been studies  that attempt to use weak supervision for deep text classification. Unfortunately, their performance is limited by the above two challenges. Ratner \etal  proposed a data programming method, which uses heuristic rules as labeling functions and then trains discriminative models using the automatically created labeled data. However, the training data annotated by data programming come from instances that can be directly matched by the rules, making the model have limited performance on the unmatched data.  Meng \etal  proposed a deep self-training method, which uses weak supervision to learn an initial model and then updates the model by using the model's own confident predictions. However, the self-training procedure can overfit the label noise and suffer from the error propagation.  \sep Our contributions. We propose a new method that uses weak supervision to train deep text classifiers in a label-efficient way, while addressing the label noise and label coverage issues. We assume multiple weak supervision sources  provide complementary sets of heuristic rules. \zc{the previous two sentences can be merged.} Our idea is that the complementary information in the multiple sources can not only reduce label noise, but also effectively bootstrap on unlabeled data to improve label coverage, making it possible to learn an accurate deep text classifier with weak supervision.   Motivated by the above, we propose a model with two carefully designed components. The first component is a rule-based classifier \zc{ rule reliability estimators} using the conditional soft attention mechanism. Given weak labels from annotators and document representations, we learn reliability scores for labeling sources, which emphasize the weak annotators' opinions that are most informative for our particular corpus. We then use the reliability scores to aggregate our disparate weak labels into a denoised pseudo label. \zc{need to highlight that our rule reliability is conditional on input text features}  The second component is a neural classifier that learns labels and distributed feature representations for all samples, matched and unmatched. This neural classifier is supervised by both the denoised labels and its own confident predictions on the unmatched data, enabling it to solve the rule coverage problem while simultaneously enhancing the rule denoiser via patterns present in the unmatched data.  The two components are integrated into a end-to-end training framework.  \zc{maybe we should also say we use pre-trained BERT as our feature extractor:   its representation power can help our denoiser work better.}  We evaluate our model on four text classification tasks, including sentiment analysis, topic classification, spam classification, and information extraction. The results on five benchmarks show that:  the soft-attention module can indeed effectively denoise the noisy training data induced from weak supervision sources, achieving \textasciitilde{}\% accuracy for denoising; and  the co-training design can improve prediction accuracy for unmatched samples, achieving at least \% accuracy increase on them. In terms of the overall performance, our model consistently outperforms state-of-the-art weakly supervised methods , semi-supervised methods , and fine-tuning methods   by 9.2\% on average. Further, we show that the denoised labels can be fed into fully supervised models and fine-tune the models to improve their performance.   % Our contributions are summarized as follows: %      %  =============================================== % Chao: I outline a structure for the intro, fill and extend these paragraphs!  % % Paragraph 1: Text classification is one of the most fundamental problems in text mining, information retrieval, and natural language processing. While deep neural nets % % have achieved dominant performance for text classification, they are highly label-hungry, often requiring hundreds of thousands of labeled samples to achieve strong performance.  This has become a key bottleneck of applying deep % % text classifiers to many real-life applications, where large-scale labeled data are too expensive to obtain.  % % Paragraph 2: An overview of existing methods for handling label sparsity. Including:  % % self-training methods, % % fine-tuning methods,  % % weakly supervised methods. Think hard about their drawbacks.  % % Paragraph 3: An overview of our model: we propose a deep neural text classifier, which is learned not from excessive labeled data, but only unlabeled data plus a set of easy-to-provide heuristic rules.  % % Paragraph 4: Two challenges of learning from rules: Learning the model from heuristic rules is difficult, because the rules can only induce noisy training data and can have limited coverage.  % % Paragraph 5: How we address the two challenges: % % First, it has a label denoising module, which estimates source reliability and denoises rule-induced supervision with a soft attention mechanism. Second, it has a self-learning module for improving the label coverage issue, which iteratively predicts soft labels for unmatched samples by aggregating the denoised multi-source classifiers. The two modules are integrated into a neural co-training model, which can be learned in an end-to-end manner.  % % Paragraph 6: The results we obtain on real data  % % A bullet list summarizing our contributions:
"," % While deep neural nets have achieved superior performance for % text classification, they highly rely on large-scale labeled data. Obtaining large-scale labeled data, however, is prohibitively % expensive in many applications.  We study the problem of learning neural text classifiers without using any labeled data, but only easy-to-provide rules as multiple weak supervision sources. This problem is challenging because rule-induced weak labels are often noisy and incomplete. To address these two challenges, we design a label denoiser, which estimates the source reliability using a conditional soft attention mechanism and then reduces label noise by aggregating rule-annotated weak labels. The denoised pseudo labels then supervise a neural classifier to predicts soft labels for unmatched samples, which address the rule coverage issue. % To address these challenges, we % propose an end-to-end model with two key components \zc{this sentence is not %   informative enough, need to deliver the key idea of our method in one sentence % here, and then use the remaining sentences to elaborate our idea.}. The first component is a % rule denoiser, which estimates conditional source reliability using a soft % attention mechanism and reduces label noise by aggregating rule-annotated weak % labels. The second is a neural classifier that predicts soft labels for % unmatchable samples to address the rule coverage issue. %The two components are integrated into a co-training framework, which can be trained end-to-end to mutually enhance each other. We evaluate our model on five benchmarks for sentiment, topic, and relation classifications. The results show that our model outperforms state-of-the-art weakly-supervised and semi-supervised methods consistently, and achieves comparable performance with fully-supervised methods even without any labeled data. Our code can be found at \url{https://github.com/weakrules/Denoise-multi-weak-sources}.",70
"  % Recent work in NLP has seen a flurry of interest in the question: are the representations learned by neural networks compositional? That is, are representations of longer phrases built recursively from representations of shorter phrases, as they are in many linguistic theories? If so, how and when do they learn to do this?  For years the LSTM dominated language architectures. It remains a popular architecture in NLP, and unlike Transformer-based models, it can be trained on small corpora~.\footnote{As evidence of the ongoing popularity of LSTMs in NLP, a Google Scholar search restricted to  since 2019 finds 191 citations to the original LSTM paper  and 242 citations to the original Transformer paper .} \citet{abnar_transferring_2020} even found that the recurrent inductive biases behind the LSTM's success are so essential that distilling from them can improve the performance of fully attentional models. However, the reasons behind the LSTM's effectiveness in language domains remain poorly understood.   A Transformer can encode syntax using attention , and some LSTM variants explicitly encode syntax . So, the success of these models is partly explained by their ability to model syntactic relationships when predicting a word. By contrast, an LSTM simply scans a sentence from left to right, accumulating meaning into a hidden representation one word at a time, and using that representation to summarize the entire preceding sequence when predicting the next word. Yet we have extensive evidence that trained LSTMs are also sensitive to syntax. For example, they can recall more history in natural language data than in similarly Zipfian-distributed -gram data, implying that they exploit linguistic structure in long-distance dependencies . Their internal representations appear to encode constituency  and syntactic agreement . In this paper, we consider how such representations are learned, and what kind of inductive bias supports them.   To understand how LSTMs exploit syntax, we use contextual decomposition , a method that computes how much the hidden representation of an LSTM depends on particular past span of words. We then extend CD to Decompositional Interdependence , a measure of interaction between spans of words to produce the representation at a particular timestep. For example, in the sentence ``Socrates asked the student trick questions閳ユ瑢, we might expect the hidden representation of the LSTM at the word ``questions閳ユ瑢 to interact primarily with its syntactic head ``asked閳ユ瑢, and less with the direct object ``the student''. If so, then an LSTM could be seen as implementing compositional localism : if a hidden representation encodes meaning, then this meaning is composed from local syntactic relationships. Our experiments on syntactically-parsed corpora  illustrate this property --- interdependence decreases with syntactic distance, stratified by surface distance.  We then turn to a hypothesis about how such representations are learned. Using a simple synthetic corpus , we allow LSTMs to learn to represent short sequences before they learn longer sequences that are dependent on them. Our goal is to then illustrate how they use representations of short sequences in order to learn longer dependencies---if these smaller constituents are unfamiliar, LSTMs learn more slowly. Further experiments  isolate hierarchical behavior from other factors causing local relations to be learned first, indicating that the model tends to build a subtree from its smaller constituents. We conclude that LSTMs compose hierachically because they learn bottom-up.   
"," Recent work in NLP shows that LSTM language models capture hierarchical structure in language data. In contrast to existing work, we consider the learning process that leads to their compositional behavior. For a closer look at how an LSTM's sequential representations are composed hierarchically, we present a related measure of Decompositional Interdependence  between word meanings in an LSTM, based on their gate interactions. We connect this measure to syntax with experiments on English language data, where DI is higher on pairs of words with lower syntactic distance. To explore the inductive biases that cause these compositional representations to arise during training, we conduct simple experiments on synthetic data. These synthetic experiments support a specific hypothesis about how hierarchical structures are discovered over the course of training: that LSTM constituent representations are learned bottom-up, relying on effective representations of their shorter children, rather than learning the longer-range relations independently from children.",71
"  Systematic reviews are part of the field of evidence-based analysis, and are a methodology for conducting literature surveys, where the focus is on comprehensively summarising and synthesising existing research for the purpose of answering research questions . The aim of this process is to be very broad coverage to avoid unknown bias creeping into results via the alternative of cherry-picking scientific results . %As many relevant documents as possible should be included, and the process should also be thoroughly documented to aid replicability.  Conducting systematic reviews requires trained researchers with domain knowledge. The stages of the process are time-consuming, but vary in how much physical and mental labour they require . As a result, systematic reviews suffer from three primary challenges :  So though systematic reviews have been shown to be very effective and less prone to human biases , these issues often prove prohibitive. \\  However, these challenges are well suited to Machine Learning solutions, and there has recently been an increase in interest in applying NLP to this process . In this paper, we investigate the feasibility of implementing the multi-stage human process of a systematic review as a Machine Learning pipeline. We construct a systematic review pipeline which aims to assist researchers and organisations focusing on livestock health in various African countries who previously performed reviews manually . The pipeline begins with scraping for articles, then classifies them into whether or not to include in the review, then identifies data to extract and outputs a spreadsheet. We discuss the technical options we evaluated at each steps. Pipeline components are evaluated with intrinsic metrics as well as more pragmatic, extrinsic, considerations such as time and effort saved.  While previous work exists surveying the applicability of various Machine Learning methods and toolkits to the systematic review process  and a few apply them, there are no extant studies that implement a full system and analyse the trade-offs between different methods of training data creation, different annotation schemas, human expert hours needed to build a system, and final accuracy. We experiment with all of these factors, as well as with a few different architectures, with the aim of informing the planning and implementation of systematic review automation more broadly.    To further this goal, we particularly experiment with low resource scenarios and with generalisability. We investigate different thresholds for training data for the document classifier and different annotation schemas for the data extraction. We additionally test the ability of the system to generalise to documents from new countries.   % also talk about not needing deep learning resources  Key research questions are as follows: \paragraph{Extraction} Which techniques are best for identifying and extracting the desired information? \paragraph{Data Requirements} How much labelled training data is needed? Can existing resources be leveraged? \paragraph{Re-usability} How generalisable is a pipeline to new diseases and countries? \paragraph{Performance} What is the trade-off between pipeline accuracy and human time savings? \paragraph{Architecture \& Pre-training} How important is model architecture as applied to extraction tasks? How important is embedding pre-training, and how important is pre-training on scientific literature vs. general content ?\\  We find that surprisingly little training data  are necessary to get an accurate document classifier, and that it generalises well to unseen African countries , which enables systematic reviews to be expanded to new areas with essentially constant time. In our text extraction experiments, we find that both sentence and phrase level extraction models can each play a role in such a pipeline,  %given their complementary strengths and weaknesses on this kind of data,  but that phrase extraction, which has not previously been done for this task, performed better than expected both with baseline CNN models  and with BERT-based Transformers , with Transformers based on scientific pre-training  performing best. We demonstrate how the creation of labelled training data can be sped up through annotation tools, and that consideration should be given to the balance of training examples present within this data, since doing so may require less data overall while still maintaining good performance. Furthermore, besides automatic information extraction, much labour in constructing systematic reviews can be saved through simply automating the process of searching and downloading documents.   We empirically demonstrate that most of the three month pipeline of a systematic review can be automated to require very little human intervention, with acceptable accuracy of results. We release our code, annotation schema, and labelled data to assist in the expansion of systematic reviews via automation.  While we demonstrate this system on one domain, the framework is domain independent and could be applied to other kinds of systematic reviews. New training data and annotation schemes would be necessary to switch to medical or other domains, but our findings on time saving processes for annotation  would apply, and confidence thresholds that we implement are adjustable to customise to different levels of accuracy to human time trade-offs that are appropriate to different fields. Our exploration into necessary amounts of training data for accuracy and generalisability are broadly applicable.  
"," Systematic reviews, which entail the extraction of data from large numbers of scientific documents, are an ideal avenue for the application of machine learning. They are vital to many fields of science and philanthropy, but are very time-consuming and require experts. Yet the three main stages of a systematic review are easily done automatically: searching for documents can be done via APIs and scrapers, selection of relevant documents can be done via binary classification, and extraction of data can be done via sequence-labelling classification. Despite the promise of automation for this field, little research exists that examines the various ways to automate each of these tasks. We construct a pipeline that automates each of these aspects, and experiment with many human-time vs. system quality trade-offs. We test the ability of classifiers to work well on small amounts of data and to generalise to data from countries not represented in the training data. We test different types of data extraction with varying difficulty in annotation, and five different neural architectures to do the extraction. We find that we can get surprising accuracy and generalisability of the whole pipeline system with only 2 weeks of human-expert annotation, which is only 15\% of the time it takes to do the whole review manually and can be repeated and extended to new data with no additional effort.\footnote{\hspace{0.1cm}Code and links to models available at \url{https://github.com/seraphinatarrant/systematic_reviews}}",72
"   Although recent neural models of language have made advances in learning syntactic behavior, research continues to suggest that inductive bias plays a key role in data efficiency and human-like syntactic generalization . Based on the long-held observation that language exhibits hierarchical structure, previous work has proposed coupling recurrent neural networks  with differentiable stack data structures  to give them some of the computational power of pushdown automata , the class of automata that recognize context-free languages . However, previously proposed differentiable stack data structures only model deterministic stacks, which store only one version of the stack contents at a time, theoretically limiting the power of these stack RNNs to the deterministic~CFLs.  A sentence's syntactic structure often cannot be fully resolved until its conclusion , requiring a human listener to track multiple possibilities while hearing the sentence. Past work in psycholinguistics has suggested that models that keep multiple candidate parses in memory at once can explain human reading times better than models which assume harsher computational constraints. This ability also plays an important role in calculating expectations that facilitate more efficient language processing . Current neural language models do not track multiple parses, if they learn syntax generalizations at all .  We propose a new differentiable stack data structure that explicitly models a nondeterministic PDA, adapting an algorithm by \citet{lang:1974} and reformulating it in terms of tensor operations. The algorithm is able to represent an exponential number of stack configurations at once using cubic time and quadratic space complexity. As with existing stack RNN architectures, we combine this data structure with an RNN controller, and we call the resulting model a \ourmodel{} .  We predict that nondeterminism can help language processing in two ways. First, it will improve trainability, since all possible sequences of stack operations contribute to the objective function, not just the sequence used by the current model. Second, it will improve expressivity, as it is able to model concurrent parses in ways that a deterministic stack cannot. We demonstrate these claims by comparing the \om{} to deterministic stack RNNs on formal language modeling tasks of varying complexity. To show that nondeterminism aids training, we show that the \om{} achieves lower cross-entropy, in fewer parameter updates, on some deterministic CFLs. To show that nondeterminism improves expressivity, we show that the \om{} achieves lower cross-entropy on nondeterministic CFLs, including the ``hardest context-free language"" , a language which is at least as difficult to parse as any other CFL and inherently requires nondeterminism. Our code is available at \url{https://github.com/bdusell/nondeterministic-stack-rnn}.  
"," We present a differentiable stack data structure that simultaneously and tractably encodes an exponential number of stack configurations, based on Lang闁炽儲鐛 algorithm for simulating nondeterministic pushdown automata. We call the combination of this data structure with a recurrent neural network  controller a \ourmodel. We compare our model against existing stack RNNs on various formal languages, demonstrating that our model converges more reliably to algorithmic behavior on deterministic tasks, and achieves lower cross-entropy on inherently nondeterministic tasks.",73
"   Cryptography has been used since antiquity to encode important secrets.  There are many unsolved ciphers of historical interest, residing in national libraries, private archives, and recent corpora collection projects .  Solving classical ciphers with automatic methods is a needed step in analyzing these materials.  In this work, we are concerned with automatic algorithms for solving a historically-common type of book code, in which word tokens are systematically replaced with numerical codes. Encoding and decoding are done with reference to a dictionary possessed by both sender and recipient.  While this type of code is common, automatic decipherment algorithms do not yet exist.  The contributions of our work are:    
"," We solve difficult word-based substitution codes by constructing a decoding lattice and searching that lattice with a neural language model.  We apply our method to a set of enciphered letters exchanged between US Army General James Wilkinson and agents of the Spanish Crown in the late 1700s and early 1800s, obtained from the US Library of Congress.  We are able to decipher 75.1\% of the cipher-word tokens correctly.",74
"   Neural network language models , pretrained on vast amounts of raw text, have become  the dominant input to downstream tasks . Commonly, these tasks involve aspects of language  comprehension . One explicit example is coreference resolution, wherein anaphora  are linked to antecedents  requiring knowledge of syntax, semantics,  and world-knowledge to match human-like comprehension.   Recent work has suggested that LMs acquire abstract, often human-like, knowledge of syntax  \cite[e.g.,][]{gulordavaetal18, futrelletal2018, huetal2020-systematic}. Additionally, knowledge of grammatical and referential aspects linking a pronoun to its antecedent noun   have been demonstrated for both  transformer and long short-term memory architectures . Humans are able  to modulate both referential and syntactic comprehension  given abstract linguistic knowledge . Contrary to humans, we find that discourse structure  only influences LM behavior  for reference, not syntax, despite model representations that encode the necessary discourse information.  The particular discourse structure we examined is governed by implicit causality  verbs . Such verbs influence pronoun comprehension:  \ex.      \a. Sally frightened Mary because she was so terrifying.      \b. Sally feared Mary because she was so terrifying.   In , she agrees in gender with both Sally and Mary, so  both are possible antecedents. However, English speakers overwhelmingly  interpret she as referring to Sally in  and Mary  in , despite the semantic overlap between the verbs. Verbs that  have a subject preference  are called subject-biased IC verbs, and verbs with a object preference  are called object-biased IC verbs.   In addition to pronoun resolution, IC verbs also interact with relative clause  attachment:   \ex.      \a.  John babysits the children of the musician who...         \a.  ...lives in La Jolla.         \b.  ...are students at a private school.         \z.     \b.  John detests the children of the musician who...         \a.  ...lives in La Jolla.         \b.  ...are arrogant and rude.          \z.     \z.     \citep[from][]{rohdeetal2011}  In ,  and  are sentence fragments with possible  continuations modifying the musician in  and  and  continuations modifying the children in  and . We might expect  human continuation preferences to be the same in  and . However, the use  of an object-biased IC verb  in  increases the proportion of continuations given by human participants  that refer to the children . Without  an object-biased IC verb the majority of continuations refer to the more recent noun  .  Effects  of IC have received renewed interest in the field of psycholinguistics in recent years \cite[e.g.,][]{kehler2008coherence, ferstl2011implicit, hartshorne2013verb, hartshorne2014, williams_IC_2020}. Current accounts of IC claim that the phenomenon is inherently a linguistic process, which  does not rely on additional pragmatic inferences by comprehenders \cite[e.g.,][]{rohdeetal2011, hartshorne2013verb}. Thus, IC is argued to be contained within the linguistic signal, analogous to  evidence of syntactic agreement and verb argument structure within corpora. We  hypothesize that if these claims are correct, then current LMs will be able to  condition reference and syntactic attachment by  IC verbs with just language data .   We tested this hypothesis using unidirectional transformer and long short-term memory network \citep[LSTM;][]{hochreiterschmidhuber97} language models. We find that LSTM  LMs fail to acquire a subject/object-biased IC distinction that influences reference or RC attachment.   In contrast, transformers learned a representational  distinction between subject-biased and object-biased IC verbs that interacts  with both reference and RC attachment,  but the distinction only influenced model output for reference. The apparent failure of model  syntactic behavior to exhibit an IC  contrast that is present in model representations raises questions  about the broader capacity of LMs to display  human-like linguistic knowledge.  
","  Language models  trained on large quantities of text have been claimed to acquire abstract linguistic representations. Our work tests the robustness of these abstractions by focusing on the ability of LMs to learn interactions between different linguistic representations. In particular, we utilized stimuli from psycholinguistic studies showing that humans can condition reference  and syntactic processing on the same discourse structure . We compared both transformer and long short-term memory LMs to find that, contrary to humans, implicit causality only influences LM behavior for reference, not syntax, despite model representations that encode the necessary discourse information. Our results further suggest that LM behavior can contradict not only learned representations of discourse but also syntactic agreement, pointing to shortcomings of standard language modeling.",75
" Word ordering often determines the meaning of a sentence; therefore how to utilize the position information of a word sequence has been an important topic in NLP and widely investigated recently. A common approach for modeling word ordering is to use recurrent neural networks , such as long short-term memory   or gated  recurrent unit  , which use a hidden state to represent the information of an ordered sequence and update model weights by backpropagation through time  ; thus the ordering information can be modeled by this structure.  However, RNN and BPTT are very inefficient in modern GPU computation due to the difficulty of parallelization with the time dependency. To solve this problem, recent work, such as convolutional seq2seq  and Transformers  which apply convolutional neural network   and self-attention respectively, succeed to eliminate the time dependency to take the computational advantage of GPU.  Instead of storing the information of ordered sequences, these models utilize the position information by using a feature-level positional encoding. For example, convolutional seq2seq proposed learnable position embeddings to represent the positions in a sequence.  Recently, various pre-trained Transformer language models keep breaking state-of-the-art results in numerous NLP tasks.  There are many different ways to pre-train a Transformer language model. For example, using an encoder, decoder, or the whole part of the Transformer, adapting the self-attention masks, or training with different objectives .  However, in terms of positional encoding, most work only used a learned position embedding which is originally proposed in convolutional seq2seq  without any analysis, even different objectives may learn completely different position information.  Motivated by the above observations, our goal is to investigate what position information the pre-trained Transformers could learn under different settings. We conduct a deep analysis of the learned position embeddings among three iconic pre-trained Transformer language models: BERT , RoBERTa  and GPT-2 . To examine the performance of different NLP types, we conduct the experiments on text classification, language modeling, and machine translation, and empirically analyze and explain the meaning and influence of position embeddings from different aspects.  The contributions of this paper are 3-fold:   
"," In recent years, pre-trained Transformers have dominated the majority of NLP benchmark tasks.  Many variants of pre-trained Transformers have kept breaking out, and most focus on designing different pre-training objectives or variants of self-attention.  Embedding the position information in the self-attention mechanism is also an indispensable factor in Transformers however is often discussed at will.  Therefore, this paper carries out an empirical study on position embeddings of mainstream pre-trained Transformers, which mainly focuses on two questions: 1) Do position embeddings really learn the meaning of positions? 2) How do these different learned position embeddings affect Transformers for NLP tasks?  This paper focuses on providing a new insight of pre-trained position embeddings through feature-level analysis and empirical experiments on most of iconic NLP tasks. It is believed that our experimental results can guide the future work to choose the suitable positional encoding function for specific tasks given the application property.\footnote{The source code is available at: \url{https://github.com/MiuLab/PE-Study}} %to make our study more convincing.",76
"  Autoregressive sequence to sequence  models such as Transformers  are trained to maximize the log-likelihood of the target sequence, conditioned on the input sequence. Furthermore, approximate inference  is typically done using the beam search algorithm , which allows for a controlled exploration of the exponential search space. However, seq2seq models  suffer from a discrepancy between token level classification during learning and sequence level inference during search. This discrepancy also manifests itself in the form of the curse of sentence length i.e. the models' proclivity to generate shorter sentences during inference, which has received considerable attention in the literature .  In this work, we focus on how to better model long-tailed phenomena, i.e. predicting the long-tail of low-frequency words/tokens , in seq2seq models, on the task of Neural Machine Translation . Essentially, there are two mechanisms by which tokens with low frequency receive lower probabilities during prediction: firstly, the norms of the embeddings of low frequency tokens are smaller, which means that during the dot-product based softmax operation to generate a probability distribution over the vocabulary, they receive less probability. This has been well known in Image Classification  and Neural Language Models . Since NMT shares the same dot-product softmax operation, we observe that the same phenomenon holds true for NMT as well. For example, we observe a Spearman閳ユ獨 Rank Correlation of 0.43 between the norms of the token embeddings and their frequency, when a standard transformer model is trained on the IWSLT-14 De-En dataset . Secondly, for transformer based NMT, the embeddings for low frequency tokens lie in a different subregion of space than semantically similar high frequency tokens, due to the different rates of updates , thereby, making rare words token embeddings ineffective. Since these token embeddings have to match to the context vector for getting next-token probabilities, the dot-product similarity score is lower for low frequency tokens, even when they are semantically similar to the high frequency tokens.   Further, better modeling long-tailed phenomena has significant implications for several text generation tasks, as well as for compositional generalization . To this end, we primarily ask and seek answers to the following two fundamental questions in the context of NMT: By exploring these questions, we arrive at the conclusion that the widely used cross-entropy  loss limits NMT models' expressivity during inference and propose a new loss function to better incorporate the inductive biases of beam search.  
"," State-of-the-art Neural Machine Translation  models struggle with generating low-frequency tokens, tackling which remains a major challenge. The analysis of long-tailed phenomena in the context of structured prediction tasks is further hindered by the added complexities of search during inference. In this work, we quantitatively characterize such long-tailed phenomena at two levels of abstraction, namely, token classification and sequence generation. We propose a new loss function, the Anti-Focal loss, to better adapt model training to the structural dependencies of conditional text generation by incorporating the inductive biases of beam search in the training process. We show the efficacy of the proposed technique on a number of Machine Translation  datasets, demonstrating that it leads to significant gains over cross-entropy across different language pairs, especially on the generation of low-frequency words. We have released the code to reproduce our results.\blfootnote{The first author is now a researcher at Microsoft, USA.}\footnote{\url{https://github.com/vyraun/long-tailed}} %",77
" Grammar induction is the task of learning the grammar of a target corpus without exposure to the parsing ground truth or any expert-labeled tree structures . Recently emerging latent tree learning models provide a new approach to this problem . They learn syntactic parsing under only indirect supervision from their main training tasks such as language modelling and natural language inference.  In this study, we analyze ON-LSTM , a new latent tree learning model that set the state of the art on unsupervised constituency parsing on WSJ test  when it was published at ICLR 2019. The model is trained on language modelling and can generate binary constituency parsing trees of input sentences like the one in Figure .     As far as we know, though there is an excellent theoretical analysis paper  of the ON-LSTM model that focuses on the model's architecture and its parsing algorithm, there is no systematic analysis of the parses the model generates. There are no in-depth investigations of  whether the model's parsing behavior is consistent among different restarts or  how the parses it produces are different from PTB gold standards. Answering these questions is crucial for a better understanding of the capability of the model and may bring insights into how to build more advanced latent tree learning models in the future.  Therefore, we replicate the model with 5 random restarts and look into the parses it generates. We find that  ON-LSTM has fairly consistent parsing behaviors across different restarts, achieving a self F1 of 65.7 on WSJ test.  The model struggles to correctly parse the internal structures of complex noun phrases.  The model has a consistent tendency to overestimate the height of the split points right before verbs or auxiliary verbs, leading to a major difference between its parses and the Penn Treebank gold-standard parses. We speculate that both problems can be explained by the training task, unidirectional language modelling, and thus we hypothesize that training a bidirectional model on a more syntax-related task like acceptability judgement might be a good choice for future latent tree learning models.   
"," Recent latent tree learning models can learn constituency parsing without any exposure to human-annotated tree structures. One such model is ON-LSTM \citep{ONLSTMShen}, which is trained on language modelling and has near-state-of-the-art performance on unsupervised parsing. In order to better understand the  performance and consistency of the model as well as how the parses it generates are different from gold-standard PTB parses, we replicate the model with different restarts and examine their parses. We find that  the model has reasonably consistent parsing behaviors across different restarts,  the model struggles with the internal structures of complex noun phrases,  the model has a tendency to overestimate the height of the split points right before verbs. We speculate that both problems could potentially be solved by adopting a different training task other than unidirectional language modelling.",78
" Deep learning has become the dominant approach to address most Natural Language Processing  tasks, including text classification. With sufficient and high-quality training data, deep learning models can perform incredibly well . However, in real-world cases, such ideal datasets are scarce. Often times, the available datasets are small,  full of regular but irrelevant words, and contain unintended biases . These can lead to suboptimal models with undesirable properties. For example, the models may have biases against some sub-populations or may not work effectively in the wild as they overfit the imperfect training data.  To improve the models, previous work has looked into different techniques beyond standard model fitting. If the weaknesses of the training datasets or the models are anticipated, strategies can be tailored to mitigate such weaknesses. For example, augmenting the training data with gender-swapped input texts helps reduce gender bias in the models . Adversarial training can prevent the models from exploiting irrelevant and/or protected features . With a limited number of training examples, using human rationales or prior knowledge together with training labels can help the models perform better .  Nonetheless, there are side-effects of sub-optimal datasets that cannot be predicted and are only found after training thanks to post-hoc error analysis. To rectify such problems, there have been attempts to enable humans to fix the trained models  . Since the models are usually too complex to understand, manually modifying the model parameters is not possible. Existing techniques, therefore, allow humans to provide feedback on individual predictions instead. Then, additional training examples are created based on the feedback to retrain the models.  However, such local improvements for individual predictions could add up to inferior overall performance . Furthermore, these existing techniques allow us to rectify only errors related to examples at hand but provide no way to fix problems kept hidden in the model parameters.   In this paper, we propose a framework which allows humans to debug and improve deep text classifiers by disabling hidden features which are irrelevant to the classification task. We name this framework FIND . FIND exploits an explanation method, namely layer-wise relevance propagation  , to understand the behavior of a classifier when it predicts each training instance. Then it aggregates all the information using word clouds to create a global visual picture of the model. This enables humans to comprehend the features automatically learned by the deep classifier and then decide to disable some features that could undermine the prediction accuracy during testing. The main differences between our work and existing work are:   first, FIND leverages human feedback on the model components, not the individual predictions, to perform debugging;   second, FIND targets deep text classifiers which are more convoluted than traditional classifiers used in existing work .   We conducted three human experiments  to demonstrate the usefulness of FIND.  For all the experiments, we used as classifiers convolutional neural networks  ,  which are a popular, well-performing architecture for many text classification tasks including the tasks we experimented with . The overall results show that FIND with human-in-the-loop can improve the text classifiers and mitigate the said problems in the datasets.  After the experiments, we discuss the generalization of the proposed framework to other tasks and models. Overall, the {\bf main contributions} of this paper are:   The rest of this paper is organized as follows.  Section  explains related work about analyzing, explaining, and human-debugging text classifiers.  Section  proposes FIND, our debugging framework.  Section  explains the experimental setup followed by the three human experiments in Section  to . Finally, Section  discusses generalization of the framework and concludes the paper. Code and datasets of this paper are available at \url{https://github.com/plkumjorn/FIND}.  
"," Since obtaining a perfect training dataset  is hardly possible, many real-world text classifiers are trained on the available, yet imperfect, datasets.  These classifiers are thus likely to have undesirable properties. For instance, they may have biases against some sub-populations or may not work effectively in the wild due to overfitting.  In this paper, we propose FIND -- a framework which enables humans to debug deep learning text classifiers by disabling irrelevant hidden features. Experiments show that by using FIND, humans can improve CNN text classifiers which were trained under different types of imperfect datasets .",79
"  Commonsense reasoning is an important yet challenging task in artificial intelligence and natural language processing. Take commonsense question answering as an example, given a question and multiple choices, some commonsense knowledge is usually required to make the correct answer from the provided choices. Table show some typical commonsense question answering examples extracted from the dataset of commonsenseQA.    \end{small} \end{center} \vskip -0.25in \end{table}  Existing commonsense reasoning methods mainly utilize raw texts to conduct the data representation and answer prediction process. However, the background knowledge required in the commonsense reasoning task, such as spatial relations, causes and effects, scientific facts and social conventions, are usually not explicitly provided by the text. Therefore, it is difficult to capture such knowledge solely from the raw texts. Some other works propose to leverage knowledge bases to extract related commonsense knowledge. However, the construction of a knowledge base is expensive, and the contained knowledge is too limited to fulfill the requirement. Furthermore, most commonsense question answering datasets, such as CommonsenseQA, are constructed from an existing knowledge base, e.g., ConceptNet . So it is unfair to use the knowledge base in these tasks. To sum up, how to automatically learn commonsense remains a challenging problem in NLP.  Motivated by the fact that images usually contain richer scene information, which can be viewed as an important supplementary resource to perceive for commonsense knowledge, this paper proposes to learn commonsense from images and incorporate such knowledge into the commonsense reasoning process. Take the question `Where is a good idea but not required to have a fire extinguisher?' shown in Table as an example. Solving this problem requires a strong background knowledge that fire extinguishers are usually equipped in public places, such as hospitals, schools, and school buses. We can see that such background knowledge is not explicitly provided by the raw texts, and meanwhile, too abstract and complex to be extracted by the current language model techniques. In this case, images will help. For example, we could find many images where fire extinguishers appear in these scenes of public places. Therefore, this commonsense knowledge could be learned by perceiving the scene information of these images, and the corresponding question will be well answered. These analyses are in accordance with Minsky's statement in \citet{minsky2000commonsense}, `perhaps a good architecture theory based on multiple representations and multi-modal reasoning would help us to design better systems that allow us to study and understand commonsense reasoning.'   Our approach, named Loire , consists of two stages, i.e.~visual commonsense learning and knowledge-augmented reasoning. In the first stage, a scene layout generation task is conducted on a bi-modal data such as the representative benchmark COCO. Firstly, a text encoder Visual BERT  is employed to obtain the representation of a caption. ViBERT is then incorporated into the recurrent encoder-decoder structure for the labeled bounding box generation. This module is trained separately by a supervised learning approach, based on the ground-truth bounding boxes of images. In this way, the required visual commonsense knowledge will be encoded in ViBERT. In the following commonsense reasoning stage, the concerned text representations  will be obtained by concatenating ViBERT and a traditional pre-trained language model, e.g. ~BERT. Then the language model is fine-tuned on the commonsense reasoning data, with ViBERT fixed as some prior knowledge. Experimental results on two commonsense reasoning tasks, i.e.~CommonsenseQA and WinoGrande , demonstrate that the learnt commonsense from images brings improvements to traditional models, such as BERT fine-tune  and RoBERTa fine-tune . We also give some case studies to show how the learned visual commonsense knowledge helps the reasoning process.   To the best of our knowledge, we are the first to propose learning commonsense knowledge from images to facilitate the commonsense reasoning in NLP. The proposed model of using scene layout generation as the supervision demonstrates a preliminary exploration in this direction. Other methods like learning commonsense from retrieved relevant images could also be investigated. We believe this novel approach may provide a new perspective for commonsense reasoning in NLP.  
"," This paper proposes a novel approach to learn commonsense from images, instead of limited raw texts or costly constructed knowledge bases, for the commonsense reasoning problem in NLP. Our motivation comes from the fact that an image is worth a thousand words, where richer scene information could be leveraged to help distill the commonsense knowledge, which is often hidden in languages. Our approach, namely Loire, consists of two stages. In the first stage, a bi-modal sequence-to-sequence approach is utilized to conduct the scene layout generation task, based on a text representation model ViBERT. In this way, the required visual scene knowledge, such as spatial relations, will be encoded in ViBERT by the supervised learning process with some bi-modal data like COCO. Then ViBERT is concatenated with a pre-trained language model to perform the downstream commonsense reasoning tasks. Experimental results on two commonsense reasoning problems, i.e.~commonsense question answering and pronoun resolution, demonstrate that Loire outperforms traditional language-based methods. We also give some case studies to show what knowledge is learned from images and explain how the generated scene layout helps the commonsense reasoning process. \let\thefootnote\relax\footnotetext{*Corresponding Author}",80
" % Neural dependency parsers  predicts the relations and interactions between words equipped with nerual networks.  Graph-based dependency parsing is a popular approach to dependency parsing that scores parse components of a sentence and then finds the highest scoring tree through inference. First-order graph-based dependency parsing takes individual dependency edges as the components of a parse tree, while higher-order dependency parsing considers more complex components consisting of multiple edges. There exist both exact inference algorithms  and approximate inference algorithms  to find the best parse tree. %Neural network based dependency parsers become popular due to its high efficiency and accuracy. Transition-based dependency parsing  builds the dependency trees by making a series of decisions on a sequence of words, and graph-based dependency parser  first encodes all words in a sentence using bi-directional LSTM and score components in parse tree and find the highest scoring tree through inference. Recent work focused on neural network based graph dependency parsers . \citet{dozat2016deep} proposed a first-order graph-based neural dependency parsing approach with a simple head-selection training objective. It uses a biaffine function to score dependency edges and has high efficiency and good performance. Subsequent work introduced second-order inference into their parser. \citet{ji-etal-2019-graph} proposed a graph neural network that captures second-order information in token representations, which are then used for first-order parsing. Very recently, \citet{zhang2020efficient} proposed an efficient second-order tree CRF model for dependency parsing and achieved state-of-the-art performance. %Higher-order dependency parsing takes more complex higher-order components like siblings and grandparents into consideration in decoding phase  and uses algorithms like dynamic programming for exact inference. Such kind of higher-order components increases the global information in inference and results in improvements in parsing accuracy, but they also make inference slower and more complicated. Recent work on graph-based higher-order dependency parsing and semantic dependency parsing  focused on approximate inference on the graph,  , which is much faster and with minor performance reduction compared to the exact inference algorithm.   % \citet{falenska-kuhn-2019-non} also showed that adding second-order inference to BiLSTM-based parser leads to very small improvements. \citet{wang-etal-2019-second} proposed a second-order approach to semantic dependency parsing  , which does not have the tree constraint in syntactic dependency parsing. They employed an end-to-end neural network derived from message-passing algorithms for approximate second-order parsing and achieved state-of-the-art accuracies in SDP.  In this paper, we first show how a previously proposed second-order semantic dependency parser  can be applied to syntactic dependency parsing with simple modifications. The parser is an end-to-end neural network derived from message passing inference on a conditional random field that encodes the second-order parsing problem. We then propose an alternative conditional random field that incorporates the head-selection constraint of syntactic dependency parsing, and derive a novel second-order dependency parser. We empirically compare the two second-order approaches and the first-order baselines on English Penn Tree Bank 3.0 , Chinese Penn Tree Bank 5.1  and datasets of 12 languages in Universal Dependencies . We show that our approaches achieve state-of-the-art performance on both PTB and CTB and our approaches are significantly faster than recently proposed second-order parsers.   We also make two interesting observations from our empirical study. First, it is a common belief that contextual word embeddings such as ELMo  and BERT  already conveys sufficient high-order information that renders high-order parsing less useful, but we find that second-order decoding is still helpful even with strong contextual embeddings like BERT. Second, while \citet{zhang-etal-2019-empirical} previously found that incoperating the head-selection constraint is helpful in first-order parsing, we find that with a better loss function design and hyper-parameter tuning both first- and second-order parsers without the head-selection constraint can match the accuracy of parsers with the head-selection constraint and can even outperform the latter when using BERT embedding.  Our approaches are closely related to the work of \citet{gormley-etal-2015-approximation}, which proposed a non-neural second-order parser based on Loopy Belief Propagation . Our work differs from theirs in that: 1) we use Mean Field Variational Inference  instead of LBP, which \citet{wang-etal-2019-second} found is faster and equally accurate in practice; 2) we add the head-selection constraint and do not include the global tree constraint that is shown to produce only slight improvement  but would complicate our neural network design and implementation; 3) we employ modern neural encoders and achieve much better parsing accuracy. Our approaches are also closely related to the very recent work of \citet{turbo2020}. The main difference is that we use MFVI while they use the dual decomposition algorithm   for approximate inference.  % In recent work on semantic dependency parsing  , \citet{wang-etal-2019-second} proposed a second-order parser following the first-order parser of \citet{dozat-manning-2018-simpler}. % %encodes the first-order score following \citet{dozat-manning-2018-simpler} with biaffine functions and the second-order score by trilinear functions.  % They used Mean Field Variational Inference  or Loopy Belief Propagation  algorithm to pass messages on a Conditional Random Field  and trained in an end-to-end manner. The approach on SDP sees the existence of every single edge as a binary classification problem and it can also be applied in the tree-based dependency parsing. \citet{zhang-etal-2019-empirical} compared different structured outputs in dependency parsing and they showed that in first-order dependency parsing, the head constraint of \citet{dozat2016deep} is stronger than the binary classification structure of \citet{dozat-manning-2018-simpler} in dependency parsing.  % In this paper, we adopt the message passing method of MFVI on dependency parsing and we additionally add a Local head constraint in second-order inference procedure, which views the problem as a head-selection classification problem. We investigate the advantage of the Single and Local structured output for second-order parsers and show that the second-order parsers achieve state-of-the-art performance on both PTB and CTB. Both the second-order parsers with Local or Single structured outputs can outperform the first-order parser of \citet{dozat2016deep} and have an improvement with BERT embeddings. Compared with the approach of \citet{ji-etal-2019-graph} that decodes second-order information through passing token features on a graph neural network, the second-order parsers with message passing are more interpretive as we follow most of previous higher-order approaches  that assigns scores for each components. \citet{gormley-etal-2015-approximation} proposed a second-order parser with Single structured output with tree constraint on dependency parsing using LBP. Compared with their approach, we consider the Local head constraint. We use the MFVI algorithm which is faster in practice, and we don't need the time-consuming Inside-Outside algorithm to keep the tree structure in training. Furthermore, we don't compare a structured output with the tree constraint in the second-order parser because in the empirical investigation of \citet{zhang-etal-2019-empirical}, the tree constraint only gives modest improvement compared with the first-order Local approach. We believe the advantage of the tree constraint will be further diminished as the second-order parser considers more tree components. %       
"," In this paper, we propose second-order graph-based neural dependency parsing using message passing and end-to-end neural networks. We empirically show that our approaches match the accuracy of very recent state-of-the-art second-order graph-based neural dependency parsers and have significantly faster speed in both training and testing. We also empirically show the advantage of second-order parsing over first-order parsing and observe that the usefulness of the head-selection structured constraint vanishes when using BERT embedding. %We adapt a previous approach that predicts dependency edges independently and we also propose a new approach that incorporates the head-selection structural constraint.",81
"  Our SJTU-NICT team participated in the WMT20 shared task, including supervised track, unsupervised, and low-resource track. During the participation, we placed our attention on Polish   English  and English   Chinese  on the supervised track, while on the unsupervised and low-resource track, the German   Upper Sorbian  both directions are focused.  Our  baseline system in supervised track is based on the Transformer big architecture proposed by \citet{vaswani2017attention}, in which its open-source implementation version Fairseq  is adopted. In the unsupervised and low-resource track, we draw on the successful experience of the XLM framework , and used the two-stage training mode of masked language modeling  pre-training + back-translation  finetune to obtain a very strong baseline performance. Marian  toolkit is utilized for training the decoder in reranking using machine translation targets instead of common GPT-style language modeling targets.  In order to better play the role of WMT evaluation in polishing the methods proposed or improved by our team , we divided the three language pairs we participated in into three categories:   %  In the supervised PLEN translation direction, we based on the XLM framework to pre-train a Polish language model using common crawl and news crawl monolingual data, and proposed the XLM enhanced NMT model inspired from the idea of incorporating BERT into NMT . Besides, we trained a bidirectional translation model of EN-PL based on the parallel corpus and further finetuned it to the PLEN direction.  In the supervised ENZH translation with document information, we propose a document enhanced NMT model based on Longformer . The training of our proposed document enhanced NMT model is split into three stages.  In the first stage, we pre-train the Longformer document encoder with MLM target on the document text in Wikipedia dumps, UN News, and News Commentary monolingual corpus. A conventional Transformer-big NMT model is trained in the second stage. In the final stage, the Longformer encoder and conventional Transformer big NMT model are used to initialize the full document-enhanced NMT model parameters, in which the Longformer encoder is adopted to extract representations for the document of an input sequence, and then the document representations are fused with each layer of the encoder and decoder of the NMT model through attention mechanisms.   In the unsupervised machine translation track on DE-HSB, we experimented with the reference language based UNMT   framework we proposed recently. Under this framework, we choose English as the reference language, and use the Europarl parallel corpus of EN-DE to enhance the unsupervised machine translation between DE and HSB. Specifically, we adopted reference language translation , reference language back-translation , and cross-lingual back-translation  three training targets with the help of the cross-lingual agreement provided by the EN-DE parallel corpus to enhance the unsupervised translation performance.  Due to the introduction of more explicit supervision signals brought by parallel corpus in the low-resource machine translation track on DE-HSB, we discarded the use of the weaker agreement provided by the reference language,  conducted joint training on the unsupervised back-translation and the supervised translation directly, and introduced BT-BLEU based collaborative filtering technology for further self-training. In addition, inspired by our previous work , we also use MLM and translation language modeling  to continue pre-training the model while machine translation training.  In addition, in all basic NMT models, we empower the training process with our proposed data-dependent gaussian prior objective  , so that the model can maintain the diversity of the output. When the main model training is finished, the TF-IDF algorithm is employed to filter the training set according to the input of the test set, a training subset whose domain is more similar to the test set is obtained, and then used to finetune the model for reducing the performance degradation caused by domain inconsistency. For the final submission, an ensemble of several different trained models outputs the -best predictions, and used the decoder trained with Marian toolkit to performs reranking to get the final system output.  
","  In this paper, we introduced our joint team SJTU-NICT 's participation in the WMT 2020 machine translation shared task. In this shared task, we participated in four translation directions of three language pairs: English-Chinese, English-Polish on supervised machine translation track, German-Upper Sorbian on low-resource and unsupervised machine translation tracks. Based on different conditions of language pairs, we have experimented with diverse neural machine translation  techniques: document-enhanced NMT, XLM pre-trained language model enhanced NMT, bidirectional translation as a pre-training, reference language based UNMT,  data-dependent gaussian prior objective, and BT-BLEU collaborative filtering self-training. We also used the TF-IDF algorithm to filter the training set to obtain a domain more similar set with the test set for finetuning. In our submissions, the primary systems won the first place on English to Chinese, Polish to English, and German to Upper Sorbian translation directions.",82
"     Neural summarizers have achieved impressive performance when evaluated by ROUGE ~ on in-domain setting, and the recent success of pre-trained models drives the state-of-the-art results on benchmarks to a new level ~. However, the superior performance is not a guarantee of a perfect system since exsiting models tend to show defects when evaluated from other aspects. For example, \citet{zhang-etal-2018-abstractiveness} observes that many abstractive systems tend to be near-extractive in practice. \citet{cao2018faithful,wang2020asking,kryscinski2019evaluating,maynez2020faithfulness,durmus2020feqa} reveal that most generated summaries are factually incorrect. These non-mainstream evaluation methods make it easier to identify the model's weaknesses.  Orthogonal to above two evaluation aspects, we aim to diagnose the limitation of existing systems under cross-dataset evaluation, in which a summarization system trained on  one corpus would be evaluated on a range of out-of-dataset corpora. Instead of evaluating the quality of summarizers solely based on one dataset or multiple datasets individually, cross-dataset evaluation enables us to evaluate model performance from a  different angle. For example, Fig. shows the ranking of  summarization systems studied in this paper under different  evaluation metrics, in which the ranking list `` in-dataset R2'' is obtained by traditional ranking criteria while other two are based on our designed cross-dataset measures. Intuitively, we observe that 1) there are different definitions of a ``good'' system in various evaluation aspects; 2) abstractive and extractive systems exhibit diverse behaviors when evaluated under the cross-dataset setting.    The above example recaps the general motivation of this work, encouraging us to rethink the generalization ability of current top-scoring summarization systems from the perspective of cross-dataset evaluation. Specifically, we ask two questions as follows:   Q1: {How do different neural architectures of summarizers influence the cross-dataset generalization performances?} When designing summarization systems, a plethora of neural components can be adopted ~. For example, will copy  and coverage   mechanisms improve the cross-dataset generalization ability of summarizers? Is there a risk that BERT-based summarizers will perform worse when adapted to new areas compared with the ones without BERT? So far, the generalization ability of current summarization systems when transferring to new datasets still remains unclear, which poses a significant challenge to design a reliable system in realistic scenarios. Thus, in this work, we take a closer look at the effect of model architectures on cross-dataset generalization setting.    Q2: {Do different generation ways  of summarizers influence the cross-dataset generalization ability?} Extractive and abstractive models, as two typical ways to summarize texts, usually follow diverse learning frameworks and favor different datasets.  It would be absorbing to know their discrepancy from the perspective of cross-dataset generalization.      To answer the questions above, we have conducted a comprehensive experimental analysis, which involves eleven summarization systems , five benchmark datasets from different domains, and two evaluation aspects. Tab. illustrates the overall analysis framework. We explore the effect of different architectures and generation ways on model generalization ability in order to answer Q1 and Q2. Semantic equivalency  and factuality are adopted to characterize the different aspects of cross-dataset generalization ability. Additionally, we strengthen our analysis by presenting two views of evaluation: holistic and fine-grained views .   }%        % \end{table}%  Our contributions can be summarized as: 1) Cross-dataset evaluation is orthogonal to other evaluation aspects , which can be used to re-evaluate current summarization systems, accelerating the creation of more robust summarization systems. 2) We have design two measures Stiffness and Stableness, which could help us to characterize generalization ability in different views, encouraging us to diagnose the weaknesses of state-of-the-art systems.  3) We conduct dataset bias-aided analysis  and suggest that a better understanding of datasets will be helpful for us to interpret systems'  behaviours.     
"," Neural network-based models augmented with unsupervised pre-trained knowledge have achieved impressive performance on text summarization. However, most existing evaluation methods are limited to an in-domain setting, where summarizers are trained and evaluated on the same dataset. We argue that this approach can narrow our understanding of the generalization ability for different summarization systems. In this paper, we perform an in-depth analysis of characteristics of different datasets and investigate the performance of different summarization models under a cross-dataset setting, in which a summarizer trained on one corpus will be evaluated on a range of out-of-domain corpora. A comprehensive study of 11 representative summarization systems on 5 datasets from different domains reveals the effect of model architectures and generation ways  on model generalization ability. Further, experimental results shed light on the limitations of existing summarizers. Brief introduction and supplementary code can be found in \url{https://github.com/zide05/CDEvalSumm}.",83
"    As robots are deployed in collaborative applications like healthcare and household assistance , there is a growing need for reliable human-robot communication. One such communication modality that is both user-friendly and versatile is natural language; to this end, we focus on robust natural language interfaces  that can map utterances to executable behavior .  Most existing work on NLIs  falls into a static train-then-deploy paradigm: models are first trained on large datasets of  pairs and then deployed, with the hope they will reliably generalize to new utterances. Yet, what happens when such models make mistakes or are faced with types of utterances unseen at training --- for example, providing a household robot with a novel utterance like ``wash the coffee mug?'' Such static systems will fail with no way to recover, burdening the user to find alternate utterances to accomplish the task . Instead, we argue that NLIs need to be dynamic and adaptive, learning interactively from user feedback to index and perform more complicated behaviors.   In this work, we explore building NLIs for simulated robotics that learn from real humans. Inspired by \citet{wang2017naturalizing}, we leverage the idea of learning from decomposition to learn new abstractions. Just like how a human interactively teaches a new task to a friend by breaking it down, users interactively teach our system by simplifying utterances that the system cannot understand  into lower-level utterances that it can .  To map language to executable behavior, \citet{wang2017naturalizing} and \citet{thomason2019improving} built adaptive NLIs that leverage grammar-based parsers that allow reliable one-shot generalization but lack lexical flexibility. For example, a grammar-based system that understands how to ``wash the coffee mug'' may not generalize to ``clean the mug.'' Meanwhile, recent semantic parsers are based primarily on neural sequence-to-sequence models . While these models excel from a lexical flexibility perspective, they lack the ability to perform reliable one-shot generalization: it is difficult to train them to generalize from individual examples .    In this paper we propose a new interactive NLI that is lexically flexible and can reliably and efficiently perform one-shot generalization. We introduce a novel exemplar-based neural network semantic parser that first abstracts away entities , allowing for generalization to previously taught utterances with novel object combinations. Our parser then retrieves the corresponding ``lifted'' utterance and respective program  from the training examples based on a learned metric , giving us the lexical flexibility of sequence-to-sequence models.  We demonstrate the efficacy of our learning from decomposition framework through a set of human-in-the-loop experiments where crowdworkers use our NLI to solve a suite of simulated robotics tasks in household environments. Crucially, after completing a task, we update the semantic parser so that users can immediately reuse what they taught. We show that over time, users are able to complete complex tasks  more efficiently with our exemplar-based method compared to a neural sequence-to-sequence baseline. However, for more straightforward tasks that can be completed in fewer steps, we see similar performance to the baseline. We end with an error analysis and discussion of user trust and incentives in the context of building interactive semantic parsing systems, paving the way for future work that better realizes the potential of the interactive paradigm.  
","  Our goal is to create an interactive natural language interface that efficiently and reliably learns from users to complete tasks in simulated robotics settings. We introduce a neural semantic parsing system that learns new high-level abstractions through decomposition: users interactively teach the system by breaking down high-level utterances describing novel behavior into low-level steps that it can understand. Unfortunately, existing methods either rely on grammars which parse sentences with limited flexibility, or neural sequence-to-sequence models that do not learn efficiently or reliably from individual examples. Our approach bridges this gap, demonstrating the flexibility of modern neural systems, as well as the one-shot reliable generalization of grammar-based methods. Our crowdsourced interactive experiments suggest that over time, users complete complex tasks more efficiently while using our system by leveraging what they just taught. At the same time, getting users to trust the system enough to be incentivized to teach high-level utterances is still an ongoing challenge. We end with a discussion of some of the obstacles we need to overcome to fully realize the potential of the interactive paradigm.",84
"  As neural machine translation  significantly improved sentence-level translation qualities, recent studies have been focused on document-level translation.  In particular, discourse in document-level translation is one of the central research interests, such as addressing coreference and anaphora resolution; and preserving cohesion and coherence in translation; e.g.,.  In this study, we tackle the problem of lexical cohesion, which aims to consistently use the same target words to translate the same source words.   discussed that lexical cohesion significantly affects the overall quality of document translation. Table shows a comparison of lexically incohesive and cohesive translations for two consecutive Japanese sentences.  The incohesive translations translate the same Japanese word ``''  into ``clock'' and ``watch,'' while the cohesive translations consistently translate the word into ``watch.''   Previous studies approached discourse phenomena in NMT using a context-aware NMT model, which inputs previous source sentences and their translations as contexts.  However,~ showed that lexical cohesion is hard to solve with only context-aware models.  We conjecture this is because context-aware models handle previous translations as a whole and are not sensitive enough to word usage consistency.   In this study, we employ a copy mechanism  on the context-aware NMT model for document-level translation to explicitly address the lexical cohesion problem.  Our model computes a probability of copying a target word from previous translation outputs and boosts its output probability in the translation of a current sentence.    We conduct experiments on Japanese to English document translation. % using the evaluation dataset designed for discourse phenomena.  The results indicate that our model achieves significantly better lexical cohesion, comparing to previous context-aware NMT models.       \end{adjustbox}           \end{table*}   
"," Lexically cohesive translations preserve consistency in word choices in document-level translation.  We employ a copy mechanism into a context-aware neural machine translation model to allow copying words from previous translation outputs.  Different from previous context-aware neural machine translation models that handle all the discourse phenomena implicitly, our model explicitly addresses the lexical cohesion problem by boosting the probabilities to output words consistently.   We conduct experiments on Japanese to English translation using an evaluation dataset for discourse translation.  The results showed that the proposed model significantly improved lexical cohesion compared to previous context-aware models.",85
"  % ============== version 5.0 ================= Intent detection, a fundamental component of task-oriented dialogue system , is increasingly raising attention as a Multi-Label Classification  problem , since a single utterance often carries multiple user intents .  In real-world scenarios, intent detection often suffers from lack of training data, because dialogue tasks/domains change rapidly and new domains usually contain only a few data examples.  Recent success of Few-Shot Learning  presents a promising solution for such data scarcity challenges.  It provides a more human-like learning paradigm that generalizes from only a few learning examples  by exploiting prior experience. % from old domains.   %For multi-label intent detection, state-of-the-art works adopt ``one-vs-rest'' strategy to convert the multi-class classification into binary-class classifications .  State-of-the-art works for multi-label intent detection focus on threshold-based strategy, where a common practice is estimating label-instance relevance scores and picking the intent labels with score higher than a threshold value .  Usually, the coordination and respective quality of the two modules, i.e. thresholding and relevance scoring, are crucial to the performance of MLC models.  However, in few-shot scenarios, such multi-label setting poses unique challenges for both threshold estimation and label-instance relevance scoring.  For thresholding, previous works explore to tune a fixed threshold  or to learn thresholds from data . But, these thresholds work well only when learning examples are sufficient.  In few-shot scenarios, it is pretty hard to determine appropriate thresholds with only a few examples. %In few-shot scenarios, it is pretty hard to determine appropriate thresholds  %with only a few examples. %without overfitting to the limited examples. % to the limited examples. %For few-shot scenarios, it is pretty hard to determine appropriate thresholds with only a few examples. Besides, it is also difficult to directly transfer the pre-learned thresholds due to the domain differences, such as differences in label number per instance, score density and scale.    Estimation of the label-instance relevance scores is also challenging. %It is also challenging to compute the label-instance relevance scores.  Few-shot learning has achieved impressive progress with similarity-based methods  , where the relevance scores can be modeled as label-instance similarities.  And the label representations can be obtained from corresponding support examples.  Unfortunately, despite huge success in previous single-label tasks, these similarity-based methods become impractical for multi-label problems.  When instances have multiple labels, representations of different labels may be obtained from the same support examples and become confused with each other. For the example in Fig , intents of query\_time and query\_loc share the same support example  and thus have the same label representation,  %Such confused label representations  which makes it impossible to predict correct labels with similarity scores.  %In such situations, vanilla similarities will assign query x equal score to query\_time and query\_loc  In this paper, we study the few-shot learning problem of multi-label intent detection and propose a novel framework to tackle the challenges from both thresholding and label-instance relevance scoring.  To solve the thresholding difficulties of prior-knowledge transferring and domain adaption with limited examples, we propose a Meta Calibrated Threshold  mechanism that first learns universal thresholding experience on data-rich domains, then adapts the thresholds to certain few-shot domains with a Kernel Regression based calibration.  Such combination of universal training and domain-specific calibration allows to estimate threshold using both prior domain experience and new domain knowledge.  %Here, as a non-parametric learning method, Kernel Regression allows to alleviate overfitting by calibrating the thresholds without finetuning.  To tackle the challenge of confused label representation in relevance scoring, we propose the Anchored Label Representation  to obtain well-separated label representations. Inspired by the idea of embedding label name as anchor points to refine representation space , ALR uses the embeddings of label names as additional anchors and represents each label with both support examples and corresponding anchors.  Different from the previous single-label intent detection that uses label embedding as additional features , our label embeddings here have unique effects of separating different labels in metric space.  Finally, to encourage better coordination between thresholding and label-instance relevance scoring, we introduce the Logit-adapting mechanism to MCT that automatically adapts thresholds to different score densities.   Experiments on two datasets show that our methods significantly outperform strong baselines.  Our contributions are summarized as follows:   We explore the few-shot multi-label problem in intent detection of task-oriented dialogue, which is also an early attempt for the few-shot multi-label classification.   We propose a Meta Calibrated Threshold mechanism with Kernel Regression and Logits Adapting that estimates threshold using both prior domain experience and new domain knowledge.  We introduce the Anchored Label Representation to obtain well-separated label representation for better label-instance relevance scoring.    %% ============== version 4.0 ================= %Intent detection, a fundamental component of task-oriented dialogue system , is increasingly raising attention as a Multi-Label Classification  problem , since a single utterance often carries multiple user intents .  %In real-world scenarios, intent detection often suffers from lack of training data, because dialogue tasks/domains change rapidly and new domains usually contain only a few data examples.  %Recent success of Few-Shot Learning  presents a promising solution for such data scarcity challenges.  %It provides a more human-like learning paradigm that generalizes from only a few learning examples  by exploiting prior experience. %% from old domains.  % %%For multi-label intent detection, state-of-the-art works adopt ``one-vs-rest'' strategy to convert the multi-class classification into binary-class classifications .  %State-of-the-art works for multi-label intent detection focus on threshold-based strategy, where a common practice is estimating label-instance relevance scores and picking the intent labels with score higher than a threshold value .  %Usually, the coordination and respective quality of the two modules, i.e. thresholding and relevance scoring, are crucial to the performance of MLC models.  %However, in few-shot scenarios, such multi-label setting poses unique challenges for both threshold estimation and label-instance relevance scoring. % %For thresholding, previous works explore to tune a fixed threshold  or to learn thresholds from data . %But, these thresholds work well only when learning examples are sufficient.  %In few-shot scenarios, it is pretty hard to determine appropriate thresholds without overfitting. %% to the limited examples. %%For few-shot scenarios, it is pretty hard to determine appropriate thresholds with only a few examples. %Besides, it is also difficult to directly transfer the pre-learned thresholds due to the domain differences, such as differences in label number per instance, score density and scale. % % % %It is also challenging to compute the label-instance relevance scores.  %Few-shot learning has achieved impressive progress with similarity-based methods  , where the relevance scores can be modeled as label-instance similarities.  %And the label representations can be obtained from corresponding support examples.  %Unfortunately, despite huge success in previous single-label tasks, these similarity-based methods become impractical for multi-label problems.  %When instances have multiple labels, representations of different labels may be obtained from the same support examples and become confused with each other. %For the example in Fig , intents of query\_time and query\_loc share the same support example  and thus have the same label representation,  %%Such confused label representations  %which makes it impossible to predict correct labels with similarity scores.  %%In such situations, vanilla similarities will assign query x equal score to query\_time and query\_loc % %In this paper, we study the few-shot learning problem of multi-label intent detection and propose a novel framework to tackle the challenges from both thresholding and label-instance relevance scoring. % %To solve the thresholding difficulties of prior-knowledge transferring and overfitting, we propose a Meta Calibrated Threshold  mechanism that first learns universal thresholding experience on data-rich domains, then adapts the thresholds to certain few-shot domains with a Kernel Regression based calibration.  %Here, as a non-parametric learning method, Kernel Regression allows to avoid overfitting by calibrating the thresholds without finetuning. % %To tackle the challenge of confused label representation in relevance scoring, we propose the Anchored Label Representation  to obtain well-separated label representations. %Inspired by the idea of embedding label name as anchor points to refine representation space , ALR uses the embeddings of label names as additional anchors and represents each label with both support examples and corresponding anchors.  %Different from the previous single-label intent detection that uses label embedding as additional features , our label embeddings here have unique effects of separating different labels in metric space. % %Finally, to encourage better coordination between thresholding and label-instance relevance scoring, we introduce the logit-adapting mechanism to MCT that automatically adapts thresholds to different score densities.  % %Experiments on two datasets show that our methods significantly outperform strong baselines.  %Our contributions are summarized as follows:  % We explore the few-shot multi-label problem in intent detection of task-oriented dialogue, which is also an early attempt for the few-shot multi-label classification.  % We propose a Meta Calibrated Threshold mechanism with Kernel Regression and Logits Adapting that estimates threshold using both prior domain experience and new domain knowledge. % We introduce the Anchored Label Representation to obtain well-separated label representation for better label-instance relevance scoring.    %% ============== version 3.0 EMNLP version ================= % %Intent detection  is a fundamental component for task-oriented dialogue system . %In real-word scenarios, intent detection often suffers from rapid changing of domains, because the new domains are usually lacking in data and may contain only a few data examples.  %Few-Shot Learning  is a promising solution to this problem.  %It provides a more human-like learning paradigm that generalizes from only a few learning examples  by exploiting prior experience from old domains.  % %In addition to data scarcity problem, intent detection also faces the problem of multi-label prediction. %As shown in Fig , a single utterance may carry multiple user intents.  %For this consideration, intent detection needs to be formulated as a Multi-Label Classification  problem , where a common practice is estimating label-instance relevance scores and picking the labels with score higher than a threshold value . % %Usually, the threshold is crucial to the performance of MLC models. %For multi-label intent detection, previous works explore to tune a fixed threshold  or to learn thresholds from data .  %However, these thresholds work well only when learning examples are sufficient.  %For few-shot scenarios, it is pretty hard to determine appropriate thresholds with only a few examples. %Also, it is difficult to directly transfer the threshold learned in data-rich domains due to the domain differences, such as differences in label number per instance, score density and scale. % % % %It is also challenging to compute the label-instance relevance scores for few-shot MLC.  %Previous few-shot research mainly focuses on single label classification and has achieved impressive progress with similarity-based methods  .  %Generally, these methods first obtain per class representations from a few examples , and then classify an  instance according to its similarity with the representation of each class. %However, such similarity scores rely on well-separated class  representations, which poses unique challenges in multi-label settings. %When instances have multiple labels, representations of different labels may be obtained from same support examples and become confused with each other. %For the example in Fig , intents of query\_time and query\_loc share the same support example  and thus have the same label representation.  % %In this paper, we study the few-shot learning problem of multi-label intent detection . %As mentioned above, it is difficult to estimate and transfer thresholds for few-shot MLC. %To solve this, we first learn universal thresholding experience on data-rich domains, and exploit the experience to estimate appropriate thresholds for unseen few-shot domains. %Specifically, we propose Meta Calibrated Threshold , which first learns a domain-general meta threshold, and then learns to calibrate it to fit specific domains with Kernel-Regression.  %To further encourage threshold generalization, we introduce the logit-adapting mechanism that automatically adapts meta thresholds to different score densities.  % %For computing label-instance score of few-shot MLC, we propose the Anchored Label Representation  to obtain well-separated label representations. %Inspired by the idea of embedding label name as anchor points to refine representation space , ALR uses the embeddings of label names as additional anchors and represent each label with both support examples and corresponding anchors.  % %Experiments on two datasets show that our methods significantly outperform strong baselines.  %Our contributions are summarized as follows:  % We explore the few-shot multi-label problem in intent detection of task-oriented dialogue,  %which is also an early attempt for few-shot multi-label classification.  % We propose a Meta Calibrated Threshold mechanism that estimate threshold using both prior domain experience and new domain knowledge. % We introduce the Anchored Label Representation to obtain well-separated label representation for better label-instance relevance score calculation.    
"," % ========== Version 6.0 ============= In this paper, we study the few-shot multi-label classification for user intent detection.  For multi-label intent detection, state-of-the-art work estimates label-instance relevance scores and uses a threshold to select multiple associated intent labels.  To determine appropriate thresholds with only a few examples, we first learn universal thresholding experience on data-rich domains, and then adapt the thresholds to certain few-shot domains with a calibration based on non-parametric learning. %on metric learning. %, that does not require fine tuning to avoid overfitting. %Kernel Regression here allows to avoid overfitting by calibrating threshold without finetuning. For better calculation of label-instance relevance score, we introduce label name embedding as anchor points in representation space, which refines representations of different classes to be well-separated from each other. Experiments on two datasets show that the proposed model significantly outperforms strong baselines in both one-shot and five-shot settings.\footnote{Data and code are available at \url{https://github.com/AtmaHou/FewShotMultiLabel}}   %% ========== Version 5.0 ============= %In this paper, we study the few-shot multi-label classification for user intent detection.  %For multi-label intent detection, state-of-the-art work estimates label-instance relevance scores and uses a threshold to select multiple associated intent labels.  %To determine appropriate thresholds with only a few examples, we first learn universal thresholding experience on data-rich domains, and then adapt the thresholds to certain few-shot domains with a calibration based on Kernel Regression, that does not require fine tuning to avoid overfitting. %%Kernel Regression here allows to avoid overfitting by calibrating threshold without finetuning. %For better calculation of label-instance relevance score, we introduce label name embedding as anchor points in representation space, which refines representations of different classes to be well-separated from each other. %Experiments on two datasets show that our model significantly outperforms strong baselines in both one-shot and five-shot settings.\footnote{Data and code are available at \url{https://anonymous.com}}  %% ========== Version 5.0 ============= %In this paper, we study the few-shot multi-label classification for user intent detection.  %For multi-intent detection, state-of-the-art work estimates label-instance relevance scores and uses a threshold to select multiple associated intent labels.  %To determine appropriate thresholds with only a few examples, we first learn universal thresholding experience on data-rich domains, and then adapt the thresholds to certain few-shot domains with a Kernel Regression based calibration.  %Kernel Regression here allows to avoid overfitting by calibrating threshold without finetuning. %For better calculation of label-instance relevance score, we introduce label name embedding as anchor points in representation space, which refine representations of different classes to be well-separated from each other. %Experiments on two datasets show that our model significantly outperforms strong baselines in both one-shot and five-shot settings.\footnote{Code is available at \url{https://anonymous.com}}  %% ========= version 4.0 EMNLP version ========= %In this paper, we study the few-shot multi-label classification for user intent detection.  %Multi-label classification usually estimates label-instance relevance scores and uses a threshold to select multiple associated labels.  %To determine appropriate thresholds with only a few examples, we first learn universal thresholding experience on data-rich domains, and then calibrate the learned universal thresholds to fit certain few-shot domains. %For better calculation of label-instance relevance score, we introduce label name embedding as anchor points in representation space, which refine representations of different classes to be well-separated from each other. %Experiments on both open and in-house datasets show that our model significantly outperforms strong baselines in both one-shot and five-shot settings.\footnote{Code is available at: \url{https://anonymous.com}}",86
"  % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  .          % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. }  Translation into languages with grammatical gender involves correctly inferring the grammatical gender of all entities in a sentence. In some languages this grammatical gender is dependent on the social gender of human referents. For example, in the Spanish translation of the sentence `This is the doctor',  `the doctor' would be either  `el m鑼卍ico', masculine, or `la m鑼卍ica', feminine. Since the noun refers to a person the grammatical gender inflection should be correct for a given referent.   In practice many NMT models struggle at generating such inflections correctly , often instead defaulting to gender-based social stereotypes  or masculine language . For example, an NMT model might always translate `This is the doctor' into a sentence with a masculine inflected noun: `Este es el m鑼卍ico'.     Such behaviour can be viewed as translations  exhibiting gender bias. By `bias' we follow the definition from  of behaviour which `systematically and unfairly  discriminate[s]  against certain individuals or groups of individuals in favor of others.' Specifically, translation performance favors referents fitting into groups corresponding to social stereotypes, such as male doctors.   Such systems propagate the representational harm of erasure to referents -- for example, a non-male doctor would be incorrectly gendered by the above example translation. Systems may also cause allocational harms if the incorrect translations are used as inputs to other systems . System users also experience representational harms via the reinforcement of stereotypes associating occupations with a particular gender . Even if they are not the referent, the user may not wish for their words to be translated in such a way that they  appear to endorse social stereotypes. Users will also experience a lower quality of service in receiving grammatically incorrect translations.   A common approach to this broad problem in NMT is the use of gender features, implicit or explicit. The gender of one or more words in a test sentence  is determined from external context  or by reliance on `gender signals' from words in the source sentence such as gendered pronouns. That information can then be used when translating. Such approaches combine two distinct tasks: identifying the gender inflection feature, and then applying it to translate words in the source sentence. These feature-based approaches make the unstated assumption that if we could correctly identify that, e.g., the doctor in the above example should be female, we could inflect entities in the sentence correctly, reducing the effect of gender bias.   Our contribution is an exploration of this assumption. We propose a scheme for incorporating an explicit gender inflection tag into NMT, particularly for translating coreference sentences where the reference gender label is known. Experimenting with translation from English to Spanish and English to German, we find that simple existing approaches overgeneralize from a gender signal, incorrectly using the same inflection for every entity in the sentence. We show that a tagged-coreference adaptation approach is effective for combatting this behaviour.  Although we only work with English source sentences to extend prior work, we note that our approach can be extended to source languages without inherent gender signals like gendered pronouns, unlike approaches that rely on those signals.  Intuitively, if gender tagging does not perform well when it can use the label determined by human coreference resolution, it will be even less useful when a gender label must be automatically inferred.  Conversely, gender tagging that is effective in this scenario may be beneficial when the user can specify the gendered language to use for the referent, such as Google Translate's translation inflection selection , or for translations where the grammatical gender to use for  all human referents is known.  We also find that our approach works well  with RoBERTa-based gender tagging for English test sentences.    Existing work in NMT gender bias has focused on the translation of sentences based on binary gender signals, such as exclusively male or female personal pronouns. This excludes and erases those who do not use binary gendered language, including but not limited to non-binary individuals . As part of this work we therefore explore applying tagging to indicate gender-neutral referents, and produce a WinoMT set to assess translation of coreference sentences with gender-neutral entities.       \subsection{Related work} Variations on a gender tag or signal for machine translation have been proposed in several forms.  incorporate a `speaker gender' tag into training data, allowing gender to be conveyed at the sentence level. However, this does not allow more fine-grained control, for example if there is more than one referent in a sentence. Similar approaches from   and   infer and use gender information from discourse context.  also incorporate a single explicit gender feature for each sentence at inference.    integrate coreference links into machine translation reranking to improve pronoun translation with cross-sentence context.  propose NMT gender bias reduction by `mixing signals' with the addition of pro-stereotypical adjectives. Also related to our work is the very recent approach of , who train their NMT models from scratch with all source language words annotated with target language grammatical gender.  In  we treat gender bias as a domain adaptation problem by adapting to a small set of synthetic sentences with equal numbers of entities using masculine and feminine inflections. We also interpret this as a gender `tagging' approach, since the gendered terms in the synthetic dataset give a strong signal to the model. In this work we extend the synthetic datasets from this work to explore this effect further.  Other approaches to reducing gender bias effects involve adjusting the word embeddings either directly  or by training with counterfactual data augmentation  . We view these approaches as orthogonal to our proposed scheme: they have similar goals but do not directly control inference-time gender inflection at the word or sentence level.   
"," Neural Machine Translation  has been shown to struggle with grammatical gender that is dependent on the gender of human referents, which can cause gender bias effects. Many existing approaches to this problem seek to control gender inflection in the target language by explicitly or implicitly adding a gender feature to the source sentence, usually at the sentence level.    In this paper we propose schemes for incorporating explicit word-level gender inflection tags into NMT. We explore the potential of this gender-inflection controlled translation when the gender feature can be determined from a human reference, or when a test sentence can be automatically gender-tagged, assessing on English-to-Spanish and English-to-German translation.  We find that simple existing approaches can over-generalize a gender-feature to multiple entities in a sentence, and suggest effective alternatives in the form of tagged coreference adaptation data. We also propose an extension to assess translations of gender-neutral entities from English given a corresponding linguistic convention, such as a non-binary inflection, in the target language.",87
"    Self-supervised pretraining through language modeling on massive datasets has revolutionized NLP. One reason this method works is that pretraining shapes a model's hypothesis space, giving it inductive biases that help it learn linguistic tasks . Numerous probing studies have provided support for this idea by showing that language models learn representations that encode linguistic features .   However, feature learning is just the first step to acquiring helpful inductive biases. Models must also be able to learn which features matter. The NLU datasets these models are often fine-tuned on are ambiguous and contain artifacts, and often support multiple possible generalizations. Neural networks are not mind readers: Models that have been shown to represent linguistic features sometimes fail to use them during fine-tuning on NLU tasks, instead adopting shallow surface generalizations . To this end, recent work in probing pretrained models advocates for shifting the focus of study away from whether they represent linguistic features and in favor of whether they learn useful representations of those features .  % }           \end{table*}  We investigate how RoBERTa  acquires language-specific inductive biases during self-supervised pretraining. We track separately how RoBERTa's representation of linguistic features and its preferences for linguistic generalizations over surface generalizations change as the amount of pretraining data increases. We pretrain RoBERTa from scratch on datasets ranging from 1M to 1B words and evaluate these models alongside RoBERTa in a series of experiments to probe the inductive biases of a pretrained model at the time of fine-tuning on a downstream task.   We probe these models in three kinds of experiments: First, we conduct control experiments where we fine-tune models on unambiguous binary classification tasks to test whether they learn to represent simple linguistic and surface features. Second, we conduct ambiguous experiments following the poverty of the stimulus design , as illustrated in Figure . In these experiments, we fine-tune a pretrained model on an ambiguous binary classification task in which the training set is consistent with both a linguistic generalization and a surface one. We then test the classifier on disambiguating data to reveal which generalization the model adopted, and by extension its preference among the two features. Third, we conduct inoculation experiments \citep[following][]{liu2019inoculation} to test how hard it is to sway a model with a surface bias to adopt a linguistic generalization. We do this by introducing small amounts of disambiguating data into an otherwise ambiguous training set. We automatically generate data for all these tasks, and call the resulting dataset \dataset\ , pronounced ``messages''.   The results show that RoBERTa acquires a stronger linguistic bias as pretraining increases. RoBERTa has the strongest linguistic bias, and requires little to no inoculating data to reliably make the linguistic generalization. In general, models with more pretraining data can generally be induced to adopt linguistic generalizations with less inoculating data. We also find a large gap between the amount of pretraining data that RoBERTa needs to learn the linguistic features necessary to generalize out-of-domain and the amount it needs to learns that it should prefer those features when generalizing. The control experiments on unambiguous data reveal that models with little pretraining do actually represent the linguistic features, but nonetheless show a strong surface bias. In other words, the main contribution of pretraining to linguistic bias learning is devoted not to extracting features, but to learning which features matter.   We conclude that helpful inductive biases can be learned through pretraining, but current models require abundant data to do so. The implications of this conclusion point in two directions: First, we can probably continue to pretrain on increasingly massive training sets to improve on the generalization and few-shot learning abilities of models like T5  and GPT-3 . Second, since models learn useful features early, there is hope that future advances could accelerate by reducing the amount of data needed to learn which features matter. To aid in this effort, we release the MSGS dataset, our pretrained RoBERTas, and all our code: \href{https://github.com/nyu-mll/msgs}{\url{https://github.com/nyu-mll/msgs}}.  
","   One reason pretraining on self-supervised linguistic tasks is effective is that it teaches models features that are helpful for language understanding. However, we want pretrained models to learn not only to represent linguistic features, but also to use those features preferentially during fine-turning. With this goal in mind, we introduce a new English-language diagnostic set called MSGS , which consists of 20 ambiguous binary classification tasks that we use to test whether a pretrained model prefers linguistic or surface generalizations during fine-tuning. We pretrain RoBERTa models from scratch on quantities of data ranging from 1M to 1B words and compare their performance on \dataset\ to the publicly available RoBERTa$\subtxt{BASE}$. We find that models can learn to represent linguistic features with little pretraining data, but require far more data to learn to prefer linguistic generalizations over surface ones. Eventually, with about 30B words of pretraining data, RoBERTa$\subtxt{BASE}$ does demonstrate a linguistic bias with some regularity. We conclude that while self-supervised pretraining is an effective way to learn helpful inductive biases, there is likely room to improve the rate at which models learn which features matter.",88
" %缁楊兛绔村▓纰夌窗娣団剝浼呮径姘帗閸 Existing experiments  have proven that multimodal news can significantly improve users閳 sense of satisfaction for informativeness. As one of these multimedia data forms, introducing news events with video and textual descriptions is becoming increasingly popular, and has been employed as the main form of news reporting by news media including BBC, Weibo, CNN, and Daily Mail. An illustration is shown in Figure, where the news contains a video with a cover picture and a full news article with a short textual summary. In such a case, automatically generating multimodal summaries, \ie choosing a proper cover frame of the video and generating an appropriate textual summary of the article can help editors save time and readers make decisions more effectively.  There are several works focusing on multimodal summarization. The most related work to ours is , where they propose the task of generating textual summary and picking the most representative picture from 6 input candidates. However, in real-world applications, the input is usually a video consisting of hundreds of frames. Consequently, the temporal dependency in a video cannot be simply modeled by static encoding methods. Hence, in this work, we propose a novel task, Video-based Multimodal Summarization with Multimodal Output , which selects cover frame from news video and generates textual summary of the news article in the meantime.      %缁楊兛绗佸▓纰夌窗閹存垳婊戦惃鍕侀崹瀣簼绠為幀搴濈疄閸  The cover image of the video should be the salient point of the whole video, while the textual summary should also extract the important information from source articles. Since the video and the article focus on the same event with the same report content, these two information formats complement each other in the summarizing process. However, how to fully explore the relationship between temporal dependency of frames in video and semantic meaning of article still remains a problem, since the video and the article come from two different space.  Hence, in this paper, we propose a model named Dual-Interaction-based Multimodal Summarizer , which learns to summarize article and video simultaneously by conducting a dual interaction strategy in the process. Specifically, we first employ Recurrent Neural Networks  to encode text and video. Note that by the encoding RNN, the spatial and temporal dependencies between images in the video are captured.  % The features of segments and text can be constraint in the same space through L2 normalization which modifies the vector in a way that each row the sum of the squares will always be up to 1. Next, we design a dual interaction module to let the video and text fully interact with each other.  Specifically, we propose a conditional self-attention mechanism which learns local video representation under the guidance of article, and a global-attention mechanism to learn high-level representation of video-aware article and article-aware video. Last, the multimodal generator generates the textual summary and extracts the cover image based on the fusion representation from the last step. To evaluate the performance of our model, we collect the first large-scale news article-summary dataset associated with video-cover from social media websites. Extensive experiments on this dataset show that DIMS significantly outperforms the state-of-the-art baseline methods in commonly-used metrics by a large margin.  %缁楊剙娲撳▓纰夌窗閹崵绮╟ontribution To summarize, our contributions are threefold:    We propose a novel Video-based Multimodal Summarization with Multimodal Output  task which chooses a proper cover frame for the video and generates an appropriate textual summary of the article.   We propose a Dual-Interaction-based Multimodal Summarizer  model, which jointly models the temporal dependency of video with semantic meaning of article, and generates textual summary with video cover simultaneously.   We construct a large-scale dataset for VMSMO, and experimental results demonstrate that our model outperforms other baselines in terms of both automatic and human evaluations.      
"," % Multimodal summarization has drawn much attention due to the rapid growth of multimedia data.  A popular multimedia news format nowadays is providing users with a lively video and a corresponding news article, which is employed by influential news media including CNN, BBC, and social media including Twitter and Weibo. In such a case, automatically choosing a proper cover frame of the video and generating an appropriate textual summary of the article can help editors save time, and readers make the decision more effectively. Hence, in this paper, we propose the task of Video-based Multimodal Summarization with Multimodal Output  to tackle such a problem. The main challenge in this task is to jointly model the temporal dependency of video with semantic meaning of article. To this end, we propose a Dual-Interaction-based Multimodal Summarizer , consisting of a dual interaction module and multimodal generator. In the dual interaction module, we propose a conditional self-attention mechanism that captures local semantic information within video and a global-attention mechanism that handles the semantic relationship between news text and video from a high level. Extensive experiments conducted on a large-scale real-world VMSMO dataset\footnote{https://github.com/yingtaomj/VMSMO} show that DIMS achieves the state-of-the-art performance in terms of both automatic metrics and human evaluations.",89
" .     %      % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. }  \reza{ Neural models have been revolutionising machine translation , and have achieved state-of-the-art for many high-resource language pairs . However, the scarcity of bilingual parallel corpora is still a major challenge for training high-quality NMT models  % especially for a broad range of languages for which the available translation training resources are too small to be used with existing NMT systems  . % Transfer learning by fine-tuning, from a model trained for a high-resource language-pair, % \wray{Wray: Having trouble with this:  isn't ""high-resource language-pair"" mean both source and target are high resource so how does this relate to what we do?} is a standard approach to tackle the scarcity of the data in the target low-resource language-pair . % However, this is a one-to-one approach, which is not able to exploit models trained for multiple high-resource language-pairs for the target language-pair of interest.  % Furthermore, models transferred from different high-resource language-pairs may have complementary syntactic and/or semantic strengths, hence using a single model may be sub-optimal.  }   %Transfer learning is one of the widely used solutions for addressing the data scarcity problem in low-resource scenarios .  % However, applying the original transfer learning to LR models is neither able to make full use of highly related multiple high-resource languages nor to receive different parameters from all effective high-resource NMT models simultaneously.  %However, transfer learning from high-resource to low-resource NMT models is generally a one-to-many approach which is not able to exploit multiple high-resource languages and high-resource NMT models' parameters simultaneously. Contrariwise,   \reza{ Another appealing approach is multilingual NMT, whereby a single NMT model is trained  by combining data from multiple high-resource and low-resource language-pairs . % %is an appealing approach for low-resource languages by utilizing the training examples of multiple languages .  %In practice, for training a multilingual NMT, a multilingual vocabulary set from all language pairs are used for training a single NMT model among all languages that enable sharing resources between high-resource and low-resource languages. % and improves the regularization of the model by avoiding over-fitting to the limited data of the low-resource languages.  However, the performance of a multilingual NMT model is highly dependent on the types of languages used to train the model.  Indeed, if languages are from very distant language families, they lead to negative transfer, causing low translation quality in the multilingual system compared to the counterparts trained on the individual language-pairs  .  % To address this problem,  has proposed a knowledge distillation approach to effectively train a multilingual model,  % by selectively distilling the knowledge from individual teacher models to the multilingual student model. However, still all the language pairs are trained in a single model with a blind contribution during training. %during the training process when the accuracy of the individual models surpasses the multilingual one.  % by distilling knowledge from individual NMT models. To avoid distilling knowledge from the not effective teachers, they selectively apply distillation during the training process when the accuracy of the individual models surpasses the multilingual one.  }  \reza{ In this paper, we propose a many-to-one transfer learning approach which can effectively transfer models from multiple high-resource language-pairs to a target low-resource language-pair of interest.  % As the fine-tuned models from different high-resource language pairs can have complementary syntactic and/or semantic strengths in the target language-pair, our idea is to distill their knowledge into a single student model to make the best use of these teacher models.  % We further propose an effective adaptive knowledge distillation  approach to dynamically adjust the contribution of the teacher models during the distillation process, enabling making the best use of teachers in the ensemble.  % Each teacher model provides dense supervision to the student via dark knowledge  using a mechanism similar to label smoothing , where the amount of smoothing is regulated by the teacher.  % In our AKD approach, the label smoothing coming from different teachers is  combined and regulated, based on the loss incurred by the teacher models during the distillation process.   % %\wray{Wray:  This next sentence could be deleted if you need space.} %Although we focus on the application of this method for NMT, it can be applied more generally to other NLP tasks suffering from the scarcity of training data, e.g. summarisation {CITE}  and question answering \todo{CITE}. } %Experimental results on various teacher-student language pairs show up to 0.9 BLEU score improvement compare to the strong baselines. Experiments on transferring from a collection of six language pairs from IWSLT to five low-resource language-pairs from TED Talks demonstrate the effectiveness of our approach, achieving up to +0.9 BLEU score improvements compared to strong baselines.  %\todo{talk more about the experiments?}  %In this paper, we introduce a new distil-based approach to make full use of all high-resource languages % and NMT models simultaneously and effectively. To do so, we firstly apply transfer learning from high-resource to low-resource languages to generate strong teachers. Then, we adaptively distil knowledge from multiple teachers based on their effectiveness  %to improve the accuracy of low-resource NMT model.  % What distinguishes our approach from the previous distil-based method  is choosing the best teachers statistically rather than deterministically. Our approach weights teachers based on the context of each mini-batch and the ability of each teacher to improve the prediction of student for that specific mini-batch during training. % Our experiments show that the proposed approach outperforms the vanilla transformer, original transfer learning, multilingual NMT, and selective knowledge distillation for translation of five low-resource languages to English.  %Our main contributions are as follows: %    a) We      % propose a new approach to  %    transfer knowledge from high-resource to low-resource language pairs which only assumes the availability of translation models in high-resource and the bilingual data for low-resource languages which leads to best usage of computational resources via exploiting the computational work already done on the high-resource side.     % , which is particularly interesting when there is limitation in the available computational resources. %    b) We      % propose a new method to %    dynamically distil knowledge from existing teacher models to a student model. What distinguishes our approach from the previous distillation-based methods  is choosing the best teachers statistically based on the data and knowledge gap of the student model, rather than deterministically as done in the previous work . %    c) Experimental results on various teacher-student language pairs show up to 0.9 BLEU score improvement compare to the strong baselines.  % 
"," \reza{ Scarcity of parallel sentence-pairs poses a significant hurdle for training high-quality Neural Machine Translation  models in bilingually low-resource scenarios.  % A standard approach is transfer learning, which involves taking a model trained on a high-resource language-pair and fine-tuning it on the data of the low-resource MT condition of interest.  % However, it is not clear generally which high-resource language-pair offers the best transfer learning for the target MT setting. Furthermore, different transferred models may have complementary semantic and/or syntactic strengths, hence using only one model may be sub-optimal.      % In this paper, we tackle this problem using knowledge distillation, where we propose to distill the knowledge of ensemble of teacher models to a single student model.  % As the quality of these teacher models varies, we propose an effective adaptive knowledge distillation approach to dynamically adjust the contribution of the teacher models during the distillation process.  % Experiments on transferring from a collection of six language pairs from IWSLT to five low-resource language-pairs from TED Talks demonstrate the effectiveness of our approach, achieving up to +0.9 BLEU score improvement compared to strong baselines.  } %In this paper, we propose a two-phase method  to tackle this challenge. The first phase involves transfer learning, where models trained on high-resource languages-pairs are fine-tuned on the data of the low-resource MT condition of interest.  % %The second phase involves disstilling the knowledge from this collection of teachers to a single student model.  % %As the quality of these teacher models vary, we propose an adaptive knowledge distillation approach to adaptively adjust the contribution of the teacher models during the training process of the student.  % %NMT models, where a pretrained modeld on high-resource data is fine tuned on .  The transferred models are treated as teachers which produce soft targets for each low-resource language. In the second phase, we adaptively distil knowledge from all teachers based on their capability to improve the accuracy of the low-resource NMT model . By optimizing the student to fit the teachers' distribution over smoothed labels, we expect the student闁炽儲鐛 generalisation affected by teachers' probability calibration. Moreover, we propose to control the teachers' contributions when computing the soft targets for knowledge distillation, such that better teachers contribute more. This contribution is adaptively changing based on how good a teacher captures the context of an incoming mini-batches during training. Experiments on IWSLT and TED dataset demonstrate the effectiveness of our model which outperforms strong baselines on the translation of five low-resource languages to English.",90
"  Natural language processing for deception detection focus on preprocessing text into computational data with required features for the propose. As deception detection is about understanding the meaning of the text or how the text is viewed by people, the sequence of the text is always considered as one of primary source of context. For example, N-gram, the representative method of natural language processing, contains the data of a word and its subsequent word and its statistical probabilities. The attribute subsequent contains the continuous context of the text, or the linguist  will describes as linearity. In contrast, feature extractions without considering this language's linearity seems to be nonsense. However, if the data that been processed out of those non-linear feature extractions shows notable accuracy of detecting deceptions, it is possible to suggest that some of those preprocessing methods could be used as one of possible natural language processing for certain situations. \  In this paper, we discuss the effectiveness of APV, a simple natural language processing method using alphabet frequency, in the context of application on fake news detection. By using deep learning algorithm and fake news dataset in Kaggle, our findings suggest that simple deep learning algorithms using APV as pre-processing method could show prominent accuracy on predicting deception of the text. \  In section 2, we investigate conventional natural language processing that is used for machine learning and deep learning algorithms. In section 3, we define APV and its mathematical structure. We will also discuss the hypothesis that might improve feature extraction of APV.  In section 4, basic experiment protocol will be set including the structure of deep learning algorithms and performance metrics that will be used in the experiment. In section 5, we present the result of the algorithms performance. Finally, in section 6, we conclude the study.  
","     Feature extraction is an important process of machine learning and deep learning, as the process make algorithms function more efficiently, and also accurate. In natural language processing used in deception detection such as fake news detection, several ways of feature extraction in statistical aspect had been introduced . In this research, it will be shown that by using  deep learning algorithms and alphabet frequencies of the original text of a news without any information about the sequence of the alphabet can actually be used to classify fake news and trustworthy ones in high accuracy . As this pre-processing method makes the data notably compact but also include the feature that is needed for the classifier, it seems that alphabet frequencies contains some useful features for understanding complex context or meaning of the original text.\\\\  keywords: {[FEATURE EXTRACTION], [DEEP LEARNING]}  % Received, Accepted 闂嗩喚濞嬮～锟犳禃 闂夋稑瀚靛 闂夋稑鎳為悧鎾荤垷濮楀喚娼 濮ｉ潧鐗楅崝顖炵亙.",91
"  Sentence matching is a fundamental technology in natural language processing. Over the past few years, deep learning as a data-driven technique has yielded state-of-the-art results on sentence matching . However, this data-driven technique typically requires large amounts of manual annotation and brings much cost. If large labeled data can't be obtained, the advantages of deep learning will significantly diminish.  To alleviate this problem, active learning is proposed to achieve better performance with fewer labeled training instances . Instead of randomly selecting instances, active learning can measure the whole candidate instances according to some criteria, and then select more efficient instances for annotation . However, previous active learning approaches in natural language processing mainly depend on the entropy-based uncertainty criterion , and ignore the characteristics of natural language. To be more specific, if we ignore the linguistic similarity, we may select redundant instances and waste many annotation resources. Thus, how to devise linguistic criteria to measure candidate instances is an important challenge.  Recently, pre-trained language models  have been shown to be powerful for learning language representation. Accordingly, pre-trained language models may provide a reliable way to help capture language characteristics. In this paper, we devise linguistic criteria from a pre-trained language model to capture language characteristics, and then utilize these extra linguistic criteria  to enhance active learning. It is shown in Figure . Experiments on both English and Chinese sentence matching datasets demonstrate the pre-trained language model can enhance active learning.   
"," Active learning is able to significantly reduce the annotation cost for data-driven techniques. However, previous active learning approaches for natural language processing mainly depend on the entropy-based uncertainty criterion, and ignore the characteristics of natural language. In this paper, we propose a pre-trained language model based active learning approach for sentence matching. Differing from previous active learning, it can provide linguistic criteria to measure instances and help select more efficient instances for annotation. Experiments demonstrate our approach can achieve greater accuracy with fewer labeled training instances.",92
"   %text matching閺堝绶㈡径姝瀍ep learning閺傝纭堕敍灞炬櫏閺嬫粈绗夐柨娆欑礉娴ｅ棙妲搁崣顖澬掗柌濠傛▕閿涘奔绗栭柅鐔峰娑撳秹鐝  The neural networks represent two sentences individually to a dense vector in the same embedding space, and then define different functions to calculate the matching degree of the two-sentence vectors. However, they are getting extremely time-consuming as the networks are becoming more sophisticated and introducing more parameters. Even worse, it is still a black box for researchers and practitioners, and in urgent need of interpretability. We can't figure out what's the specific meaning of the representation obtained from neural networks, which is unaccountable or challenging to comprehend and will lead to an untrusty and irresponsible result.   %閹存垳婊戠亸杈ㄥ厒閹靛彞绔存稉顏勫嫉韫囶偄寮垫總鍊熜掗柌濠忕礉娴犲簼浜掗崜宥囨畱deep learning鐠囦焦妲戞禍鍡楊劅閺傚洦婀伴惃鍕秵缂佺銆冪粈鐑樻Ц闂堢姾姘ㄩ惃鍕剁礉閹垫禒顧砮tric learn閸掓艾銈界亸杈ㄦЦ鏉╂瑦鐗遍敍灞界穿閸忣櫝etric learning閿涘奔绗栭敍鍫滀簰瀵伴弰顖涘簼绠為悽鈺〆tric learning閿  To tackle these, we aim to find a fast and interpretable approach for sentence matching. There are several studies focused on learning low-dimensional representations of the data, which called metric learning and even some of them combine it with some similarity metrics for ranking tasks .  Moreover, some researchers apply metric learning principles to design the loss function in information retrieval and question-answering tasks. But for the deep metric learning that they utilized, the neural network part still demands a lot of time. It hardly runs on a memory-limited device, together with high energy consumption.  %閹存垳婊戝銉ょ稊閺勵垰婀猼ext matching娑撳﹥褰佹稉娑擃亜鎻╅柅鐔烘畱閺傝纭堕妴鍌樺倶鍌涘娴狀櫑pply閵嗗倶鍌  It is considering the unexplainable implications brought from neural networks, such as fairness or transparency, and the challenge of time-consuming. In this paper, we apply metric learning approaches to address the problems mentioned above. Because metric learning has an advantage in time and memory usage on large-scale and high-dimensional datasets compared with methods above. Here, metric learning finds a representation of the data that preserves these constraints that are placed by human-provided labels. Building on its success in learning ``label constraint preserving'' representations, or low-distortion embeddings, we explore two \textsf{F}ast, \textsf{I}nterpretable, and \textsf{L}ow-rank \textsf{M}etric learning approaches, what we called \textsf{FILM}.   %閻鍩岄弫鍫熺亯metric learning閼宠棄鐤勯悳鎵娴艰偐娈戠紒鎾寸亯閿涘奔绗栬箛顐︾喍绗栭崣顖澬掗柌濠冄嶇窗缁炬寧褏娈   Notably, we explore \textsf{FILM} methods on text matching tasks, which is also known as the semantic equivalence problem in the IR community~. To be more specific, one based on an interpretable low-rank manifold optimization method. To solve this optimization problem,  we apply the Cayley transformation method with the Barzilai-Borwein step size. After being trained for this task, both are added to the kNN index for prediction for efficient retrieval. The input question is encoded and used as a query to the index, returning the top k most similar questions. We test our approaches on data from the Quora Challenge and SemEval-2017 Semantic Textual Similarity  Task, which provide pairwise sentence similarity labels.   %\footnote{}   %Our motivation is to investigate whether \textsf{FILM} approaches can perform as well as, if not better than, some ``black box'' approaches that are so popular these days.    The rest of this paper is organized as follows. In Section , we provide a quick overview of metric learning. In Section  we present the interpretable \textsf{FILM} method. In Section , we summarize the Quora dataset and task, explain how \textsf{FILM} is applied to the task, and summarize our deep neural network approach. In Section  we report some results.  \end{comment}  
"," Detection of semantic similarity plays a vital role in sentence matching. It requires to learn discriminative representations of natural language. Recently, owing to more and more sophisticated model architecture, impressive progress has been made, along with a time-consuming training process and not-interpretable inference. % In sentence matching and semantic analysis, detecting semantic similarity is a challenge that requires learning discriminative representations of natural language. Recent advances in the deep neural network enable us to learn semantic representation, but are getting time-consuming and fail in interpretation. To alleviate this problem, we explore a metric learning approach, named \textsf{FILM}  to efficiently find a high discriminative projection of the high-dimensional data. We construct this metric learning problem as a manifold optimization problem, and solve it with the Cayley transformation method with Barzilai-Borwein step size. % To alleviate this problem, in this paper we construct sentence matching as a manifold optimization problem that learns a distance function between sentences. % % and obtain the semantic representation by learning a similarity or distance function. % We explore a metric learning approach, named \textsf{FILM}  to efficiently find a high discriminative projection of the high-dimensional data. % that still preserves high discriminative power. % To this end, our manifold optimization method is solved by the Cayley transformation method with Barzilai-Borwein step size.  In experiments, we apply \textsf{FILM} with triplet loss minimization objective to the Quora Challenge and Semantic Textual Similarity  Task. The results demonstrate that the \textsf{FILM} method achieves a superior performance as well as the fastest computation speed, which is consistent with our theoretical analysis of time complexity.",93
" %A common situation for language learners is to encounter unrecognized words. %In this case, looking up the dictionary may be the preferred solution for many people. %However, the capacity of dictionaries is limited, and they may not contain new words or new meanings of words. %What's more, not all language pairs have dictionaries, especially those with low resources. %Therefore, it may be a good idea to directly generate definitions for words.  The definition modeling task proposed by \citet{Noraset2017DefinitionML} is to generate a dictionary definition of a specific word. This task can prove useful for language learners, such as provide reading help by giving definitions for words in the text. However, definition modeling can only work for a specific language, which puts high demands on users because it requires them to read definitions written in this language. Besides, many low-resource languages lack large-scale dictionary data, making it difficult to train definition generation models for these languages. %This task can prove useful for language learners, such as provide reading help by giving definitions for words in the text. %However, definition modeling can only work for a specific language, which puts high demands on users because it requires them to read definitions written in this language.  Therefore, we emphasize the necessity of generating definitions cross-lingually, which can generate definitions for various language inputs, as illustrated in figure . Since English is widely used around the world, and English dictionary resources are relatively easy to obtain, we choose to generate definitions in English. In this way, a cross-lingual model trained on English can be directly applied to other languages.  The challenging issue is how to effectively transfer the knowledge of definition generation learned in English to other languages. To solve this problem, we propose to employ cross-lingual pretrained language models  as encoders. These models have shown to be able to encode sequences of various languages, which enables the ability of cross-lingual transfer . %In this work, we emphasize the necessity of generating definitions cross-lingually, which requires the model to generate definitions with just one language for words in various languages as illustrated in figure . %Considering English is widely used around the world, and English dictionary resources are relatively easy to obtain, we choose to use English to generate definitions for other languages in this work.  %Recently, cross-lingual pretrained language models  have shown to be capable of encoding sequences of different languages into the same vector space, which enables the ability of cross-lingual transfer. %Therefore, we propose to employ them as encoders for cross-lingual definition generation. %After training and fine-tuning the model on English dataset, we directly apply the obtained model to generate definitions for other languages.    To verify our proposed method, we build an English dataset for model training and a Chinese dataset for zero-shot cross-lingual evaluation. %We collected English words, example sentences and definitions in the OALD as the English dataset, and collected Chinese words, example sentences and English definitions in the Chinese WordNet   as the Chinese dataset. Experiments and manual analyses on the constructed datasets show that our proposed models have good cross-lingual transfer ability. Compared with the reference definitions in the CWN dataset, although the generated definitions are still insufficient on the accuracy, their fluency is already good enough.  Furthermore, considering the generated definitions are provided for language learners, and many of them are non-English native speakers, we argue that the difficulty of definitions should be under control. We control the lexical complexity of generated definitions by limiting definitions in the training set to the Oxford 3000 vocabulary, which is a list of important and useful words that are carefully selected by language experts and experienced teachers . %These words have been used to write definitions in the Oxford Advanced Learner's Dictionary  , in order to make them easy to understand. %We compute the Type/Token Ratio  as a measure of lexical complexity. %The TTR of generated definitions  is much lower than that of reference definitions , which indicates a lower lexical complexity. We compute four different metrics to measure the lexical complexity. Definitions generated by our models outperform the reference definitions on all four metrics by a large margin. The result shows that our method can generate simpler definitions, which is suitable for language learners.  
"," Generating dictionary definitions automatically can prove useful for language learners. However, it's still a challenging task of cross-lingual definition generation. In this work, we propose to generate definitions in English for words in various languages. To achieve this, we present a simple yet effective approach based on publicly available pretrained language models. In this approach, models can be directly applied to other languages after trained on the English dataset. We demonstrate the effectiveness of this approach on zero-shot definition generation. Experiments and manual analyses on newly constructed datasets show that our models have a strong cross-lingual transfer ability and can generate fluent English definitions for Chinese words. We further measure the lexical complexity of generated and reference definitions. The results show that the generated definitions are much simpler, which is more suitable for language learners. %We further conduct a manual analysis of the generated Chinese definitions and find that although these definitions are insufficient on the accuracy, they are already good enough on fluency and lexical complexity.",94
"  The CoNLL 2020 MRP Shared Task  combines five frameworks for graph-based meaning representation: EDS, PTG, UCCA, AMR and DRG. It further includes evaluations in English, Czech, German and Chinese. While EDS, UCCA and AMR participated in the 2019 MRP shared task , which focused only on English, PTG and DRG are newly-added frameworks to the MRP uniform format.  For this shared task, we extended TUPA , which was adapted as the baseline system in the 2019 MRP shared task , to support the two new frameworks and the different languages. In order to add this support, only minimal changes were needed, demonstrating TUPA's strength in parsing a wide array of representations.  TUPA is a general transition-based parser for directed acyclic graphs , originally designed for parsing UCCA . It was previously used as the baseline system in SemEval 2019 Task 1 , and generalized to support other frameworks .  We also experimented with the HIT-SCIR parser . This was the parser with the highest average score across frameworks in the 2019 MRP shared task, and has also since been applied to other frameworks  .   	\end{adjustbox} 	 \end{figure*}     
","   This paper describes the HUJI-KU system submission to the shared task   on Cross-Framework Meaning Representation Parsing  at the 2020   Conference for Computational Language Learning ,   employing TUPA and the HIT-SCIR parser, which were, respectively,   the baseline system and winning system in the 2019 MRP shared task.   Both are transition-based parsers using BERT contextualized embeddings.   We generalized TUPA to support the newly-added MRP frameworks and languages,   and experimented with multitask learning with the HIT-SCIR parser.   We reached 4th place in both the cross-framework and cross-lingual tracks.",95
"  \renewcommand{\thefootnote}{}   Recurrent Neural Network language models  have been shown to learn many aspects of natural language syntax including a number of long-distance dependencies and representations of incremental syntactic state . However, previous studies have not investigated the relationship between a token's frequency in the training corpus and syntactic properties models learn about it. In this work, we assess neural models' ability to make robust syntactic generalizations about a token's nominal number or verbal argument structure based on minimal exposure with the token during training. Because of the Zipfian distribution of words in a corpus, the vast majority of word types will be seen only a handful of times during training . Therefore, the few-shot learning capabilities of neural LMs are critical to their robustness as an NLP system and as a cognitive model.  However, human learning goes beyond simply learning syntactic properties in particular constructions. People apply the same properties across different constructions, meaning that their representations of the syntactic features of a word are in some sense invariant to the grammatical context of that word. For example, speakers and listeners are sensitive to a verb's argument structure relationships and can easily recognize that a verb which cannot take a direct object in active, declarative sentences cannot be passivized  The relationship between an active sentence and a passive sentence has been termed a transformation in the linguistic literature . Many semantic-syntactic rules that govern word co-occurrence in one form, such as a verb's argument structure relationships, hold uniformly across transformations. It remains an open question whether models learn grammatical rules invariant to their surface realization, a property we call syntactic invariance.  We combine assessment of few-shot learning and syntactic invariance for two grammatical features of English: whether a noun is singular or plural  and whether a verb is transitive or intransitive . We assess whether a model is able to make different predictions based on number or argument structure in a simple active voice base context. We then assess whether models are able to make similar distinctions in a transformed context---passive voice for verbs and polar questions for nouns. In the transformed contexts, we test models with tokens that occur only in the base context during training.  For models to succeed in the transformed contexts they must represent syntactic features in a way that is invariant to the specific realization of those features in terms of word co-occurrences in different constructions. For each grammatical feature, we introduce a suite of novel targeted test sentences, similar to those presented in \citet{marvin2018targeted}.  We find that all neural models tested are able to induce the proper syntactic generalizations in the base and transformed contexts after just two or three exposures, whereas a baseline -gram model fails to learn the relevant generalizations. For all constructions tested our two neural models enhanced with explicit structural supervision outperform the purely sequence model. Assessing invariance properties, we find that neural models demonstrate proper behavior in transformed contexts, even for tokens seen only in base contexts during training. This behavior indicates that models are able to deploy generalizations learned in one syntactic context into different syntactic environments, a key component of human linguistic capabilities that has been so far untested in the neural setting.  \subsection{Related Work}  Bayesian models of word learning have shown successes in acquiring proper syntactic generalizations from minimal exposure , however it is not clear how well neural network models would exhibit these rapid generalizations. Comparing between neural network architectures, recent work has shown that models enhanced with explicit structural supervision during training produce more humanlike syntactic generalizations , but it remains untested whether such supervision helps learn properties of tokens that occur rarely during training.  Previous studies have found that Artificial Neural Networks  are capable of learning some argument structure paradigms and make correct predictions across multiple frames , however these capabilities remain untested for incremental language models. Much has been written about the ability of ANNs to learn number agreement , including their ability to maintain the dependency across different types of intervening material  and with coordinated noun phrases . \citet{hu2020systematic} find that model architecture, rather than training data size, may contribute most to performance on number agreement and related tasks. Focusing on RNN models, \citet{lakretz2019emergence} find evidence that number agreement is tracked by specific ``number"" units that work in concert with units that carry more general syntactic information like tree depth. \citet{jumelet2019analysing} argue that when learning dependencies RNNs acquire a default form , and predicting a non-default form requires explicit contrary evidence. Our results support their hypothesis. Models are more accurate with singular nouns and transitive verbs seen only a few times in training, behavior that indicates these forms are expected when evidence is sparse.  
"," Humans can learn structural properties about a word from minimal experience, and deploy their learned syntactic representations uniformly in different grammatical contexts. We assess the ability of modern neural language models to reproduce this behavior in English and evaluate the effect of structural supervision on learning outcomes. First, we assess few-shot learning capabilities by developing controlled experiments that probe models' syntactic nominal number and verbal argument structure generalizations for tokens seen as few as two times during training. Second, we assess invariance properties of learned representation: the ability of a model to transfer syntactic generalizations from a base context  to a transformed context . We test four models trained on the same dataset: an $n$-gram baseline, an LSTM, and two LSTM-variants trained with explicit structural supervision \citep{dyer2016rnng, charniak2016parsing}. We find that in most cases, the neural models are able to induce the proper syntactic generalizations after minimal exposure, often from just two examples during training, and that the two structurally supervised models generalize more accurately than the LSTM model. All neural models are able to leverage information learned in base contexts to drive expectations in transformed contexts, indicating that they have learned some invariance properties of syntax.\blfootnote{Miguel conducted this work while at IBM Research}",96
"  Despite \bert{'s}  popularity and  effectiveness, little is known about its inner workings. Several attempts have been made to demystify certain aspects of \bert , often leading to contradicting conclusions. For instance, \citet{clark-etal-2019-bert} argue that attention measures the importance of a particular word when computing the next level representation for this word. However, \citet{kovaleva-etal-2019-revealing} showed that most attention heads contain trivial linguistic information and follow a vertical pattern , which could be related to under-utilization or over-parameterization issues. Other studies attempted to link specific \bert heads with linguistically interpretable functions ,  agreeing that no single head densely encodes enough relevant information but instead different linguistic features are learnt by different attention heads. We hypothesize that the aforementioned largely contributes to the lack of attention-based explainability of \bert. Another open topic is how the knowledge is distributed across \bert layers. Most studies agree that syntactic knowledge is gathered in the middle layers , while the final layers are more task-specific. Most importantly, it seems that any semantic knowledge is spread across the model, explaining why non-trivial tasks are better solved at the higher layers .  Driven by the above discussion, we propose a novel fine-tuning approach where different parts of \bert are guided to directly solve increasingly challenging classification tasks following an underlying label hierarchy. Specifically, we focus on Large Scale Multilabel Text Classification  where documents are assigned with one or more labels from a large predefined set. The labels are organized in a hierarchy from general to specific concepts. Our approach attempts to tie specific \bert layers with specific hierarchy levels. In effect, each of these layers is responsible for predicting the labels of the corresponding level. We experiment with two \lmtc datasets  and several variations of structured \bert training. Our contributions are:  We propose a novel structured approach to fine-tune \bert where specific layers are tied to specific hierarchy levels;  We show that structured training yields better results than the baseline across all levels of the hierarchy, while also leading to better parameter utilization.     
","     Although \bert is widely used by the \nlp community, little is known about its inner workings. Several attempts have been made to shed light on certain aspects of \bert, often with contradicting conclusions. A much raised concern focuses on \bert's over-parameterization and under-utilization issues. To this end, we propose o novel approach to fine-tune \bert in a structured manner. Specifically, we focus on Large Scale Multilabel Text Classification  where documents are assigned with one or more labels from a large predefined set of hierarchically organized labels. Our approach guides specific \bert layers to predict labels from specific hierarchy levels. Experimenting with two \lmtc datasets we show that this structured fine-tuning approach not only yields better classification results but also leads to better parameter utilization.",97
" Training open-domain dialog models is inherently difficult, since for each utterance there are many acceptable responses, yet no perfect response. While supervised learning from conversational corpora allows models to learn grammatical structure and even topic coherence, these models do not generalize, since the training objectives mostly lead the models to memorize responses within the corpus.  Humans are the ultimate authority in evaluating what makes one conversational reply better than another. To learn from real conversations with humans, we created an interactive, online platform which hosted a diverse set of neural network dialog models that users could chat with in real time. However, when learning from human interactions in the wild it is crucial to be able to learn offline and test the policy before deploying it, lest it learn inappropriate behaviors . Thus, we need to train and test models offline, to ensure safe model outputs. In order to safely learn to optimize human feedback we pursued an offline reinforcement learning approach to training dialog models .   Offline RL is challenging; most deep RL algorithms fail to learn from data that is not heavily correlated with the current policy . Even models based on off-policy algorithms like -learning fail to learn in the offline RL setting, as the model is not able to explore. If the offline dataset is not sufficient to cover the input-response space, offline RL models suffer from extrapolation error, learning arbitrarily bad estimates of the value of responses not contained in the data.   We solve these problems by developing a new method for offline RL.  The method starts by leveraging a pre-trained language model to constrain offline RL updates. While training with RL, we penalize divergence from this prior model using forms of KL-control. This combats extrapolation error, and ensures that the RL model learns a policy that stays close to the distribution of realistic language, while learning to maximize positive human responses using the offline data. Further, we use dropout to obtain uncertainty estimates of the target -values, and to obtain a lower bound to alleviate over-optimistic bias in estimating future reward. We show that this new method is able to learn successfully from many different reward functions, even in a very large space with 20,000 tokens.  Both linguistic theory  and empirical experiments correlating human judgement with language features suggest that there are many criteria that could be used to evaluate a conversational agent  . We develop a set of reward functions for our dialog agents to optimize, which are designed to approximate implicit human preferences expressed during conversational responses. We show that the new method is better able to optimize these rewards using the offline data, and when tested with a new set of 80 human conversation partners, leads to more positive responses and higher quality ratings than a state-of-the-art offline deep RL method.  Novel contributions of this paper are:      
"," How can we train a dialog model to produce better conversations by learning from human feedback, without the risk of humans teaching it harmful chat behaviors? We start by hosting models online, and gather human feedback from real-time, open-ended conversations, which we then use to train and improve the models using offline reinforcement learning . We identify implicit conversational cues including language similarity, elicitation of laughter, sentiment, and more, which indicate positive human feedback, and embed these in multiple reward functions.  A well-known challenge is that learning an RL policy in an offline setting usually fails due to the lack of ability to explore and the tendency to make over-optimistic estimates of future reward. These problems become even harder when using RL for language models, which can easily have a 20,000 action vocabulary and many possible reward functions.  We solve the challenge by developing a novel class of offline RL algorithms. These algorithms use KL-control to penalize divergence from a pre-trained prior language model, and use a new strategy to make the algorithm pessimistic, instead of optimistic, in the face of uncertainty.  We test the resulting dialog model with ratings from 80 users in an open-domain setting and find it achieves significant improvements over existing deep offline RL approaches. The novel offline RL method is viable for improving any existing generative dialog model using a static dataset of human feedback.",98
"  Deep neural network-based  models have demonstrated remarkable performance on a multitude of text-to-text \cite[inter alia]{bahdanau-attention,bert-to-bert,narayan-etal-2018-dont,rush-etal-2015-neural} as well as data-to-text generation tasks \cite[inter alia]{wiseman-etal-2017-challenges,puduppully-etal-2019-data}.  % To reach high performance, DNN models require a large training corpus which is normally not readily available. Indeed, it is rare to have a sufficiently large human-curated corpus of parallel data , and researchers have come up with heuristic rules to mine input-output pairs on a large scale .  No matter how powerful, DNN models are known to be sensitive to data artifacts  and pick on the noise in the training data.    While hallucinations have not been defined formally, the term is standardly used to refer to the generated content which is either unfaithful to the input, or nonsensical . In our work we are concerned with the former hallucination kind which is primarily caused by imperfect quality of the training data. %  If the data are noisy, how can one reduce the chances of hallucinating? % One may try to improve the quality of a dataset and clean it from phrases for which a clear support in the input is missing, or augment the input with information found only in the output. The former path is risky as it easily results in ungrammatical targets. The latter approach of enforcing a stronger alignment between inputs and outputs has been tried previously but it assumes a moderate amount of noise in the data .  % Alternatively, one can leave the data as is and try to put more pressure on the decoder to pay attention to the input at every generation step . This requires significant modifications to the model and may make it harder for the decoder to generate fluent and diverse text as found in the targets.   In contrast to the described approaches, our proposal is to train the model on the data as is without modifying the decoding  architecture but instead introduce a handle on the input side to control the degree of hallucination . With this ""hallucination knob"" one can minimize  the amount of unsupported information in the output during generation . The hallucination or noise degree of every training instance is estimated separately and converted into a categorical value which becomes part of the input, like in a controlled generation setting . We introduce a simple technique to measure the amount of noise in every training example which is based on the intuition that whenever a language model  has a smaller loss than a conditional generator during forced-path decoding, it is a good signal that the next token cannot be explained by the input. % .  We consider a particularly noisy dataset, WikiBio , which has been found to have extra information in 62\% of the references  and where 1:1 correspondence between the input and the output never holds \citet{perez-beltrachini-gardent-2017-analysing}. Our models demonstrate superior performance to the model of  which reports SoTA BLEU results on WikiBio.  % In sum, our contributions are  a novel idea of controlling hallucinations which requires no modification to the model,  a data- and task-independent technique of implementing this idea and  three-way evaluation with human raters which confirms that faithfulness does not need to be traded for coverage.     
"," Neural text generation  demonstrates remarkable performance when training data is abundant which for many applications is not the case.  To collect a large corpus of parallel data, heuristic rules are often used but they inevitably let noise into the data, such as phrases in the output which cannot be explained by the input.  Consequently, models pick up on the noise and may hallucinate--generate fluent but unsupported text.  Our contribution is a simple but powerful technique to treat such hallucinations as a controllable aspect of the generated text, without dismissing any input and without modifying the model architecture. On the WikiBio corpus \cite{lebret-etal-2016-neural}, a particularly noisy dataset, we demonstrate the efficacy of the technique both in an automatic and in a human evaluation.",99
"   %   %Added value of \atomicTT{}: 1) diversity in terms of vocab, style, concepts, 2) higher quality   %\ronan{Cite publications that used ATOMIC in a downstream application}  Commonsense understanding % knowledge modeling and reasoning remain long-standing challenges in general artificial intelligence.  % However, in the subfield of natural language processing, the last few years have brought tremendous progress in AI applications.  However, large-scale language models have brought tremendous progress in the sub-field of natural language processing.  Such large-scale language models   trained on extreme-scale data have been shown to effectively adapt to diverse downstream tasks, achieving significant performance gains across natural language benchmarks .  %%%%%%%OLD %%%%%% Despite these successes, these models have been shown to learn brittle representations, often from only simple surface word associations , which routinely lead them to make nonsensical predictions detached from common sense . Interestingly, as these models have grown larger , their benchmark performance has continued to improve  despite limited conceptual improvements,  %leading many researchers to conjecture as to  leaving open questions regarding  the source of these remarkable generalization properties.   Recent work has hypothesized that many of these performance gains could be a result of language models being able to memorize facts in their parameters during training  that can be leveraged at evaluation time. As a result, a new paradigm of language models as knowledge bases has emerged . In this setting, language models are prompted with natural language prefixes or questions, and they express knowledge through language generation. The initial success of this paradigm for representing commonsense knowledge  %, combined with limited examples of LMs being successfully integrated with structured commonsense knowledge resources for downstream application,  has led to the optimistic claim that language models comprehensively encode commonsense knowledge, and remove the need for structured knowledge resources. %\antoine{run-on sentence, need to shorten}  We take a more skeptical view of this capacity of language models -- Does scaling up language models actually endow them with commonsense knowledge? While language models can successfully express certain types of knowledge, their best results are observed in narrowly specific conditions -- we show  that they perform better when evaluated on knowledge bases that prioritize ontological relations and whose examples resemble language-like assertions .\footnote{An observation supported by \citet{brown2020language}'s \gpttt{} model, whose best few-shot performance on commonsense knowledge benchmarks comes on the PhysicalIQA  and HellaSwag  datasets.} Consequently, the types of knowledge that can be directly accessed through the language model's interface remains limited.  %Consequently, while these methods are encouraging, they also demonstrate that the limited interface of language models precludes them from expressing the diversity of commonsense knowledge that must be accessible for robust commonsense reasoning.  %\chandra{i am not sure the last line in the paragraph flows logically from the rest of the paragraph. maybe i am missing something?}  However, prior work has also shown that training language models on knowledge graph tuples leads them to learn to express their implicit knowledge directly , allowing them to provide commonsense knowledge on-demand. These adapted knowledge models have exhibited promising results on commonsense benchmarks compared with methods that require linking entities to knowledge graphs . Inspired by these successes, we propose a dual use for commonsense knowledge bases going forward: as static graphs that can be linked to for discrete knowledge access, and as resources for adapting language models to hypothesize commonsense knowledge about un-annotated entities and events.   %%%%%%% OLD %%%%%%%% As a result, recent work has investigated augmenting language models with retrieval mechanisms that query commonsense knowledge graphs  for related facts to the entities mentioned in text. The idea behind these approaches is that access to these facts and the potential to compose them with learned reasoning functions would allow models to more robustly leverage commonsense knowledge to make predictions. Despite the premise of these approaches, they are unfortunately limited by the coverage of the resources used to provide commonsense knowledge facts , motivating the need for new, high coverage resources in the short-term.   % Option 1 % With this second purpose in mind, we shift the design goals of commonsense knowledge resources toward prioritizing pieces of knowledge that are not readily accessible in pretrained language models.  % Option 2 With this second purpose in mind, we propose evaluating commonsense knowledge resources based on the complementary information they can bring to pretrained language models. We construct \atomicTT{}, a new, high-quality knowledge graph with M commonsense knowledge tuples across  commonsense relations. We compare \atomicTT{} with respect to its coverage and accuracy in competition with other highly used CSKGs, such as \conceptnet~. Our results show that \atomicTT{} is able to cover more correct facts about more diverse types of commonsense knowledge than any existing, publicly-available commonsense knowledge resource. However, our results also indicate that there remains a large amount of exclusivity between these KGs, highlighting the challenge of creating resources that cover the scale and diversity of general commonsense knowledge.   %%%%%%% OLD %%%%%%Meanwhile, a new paradigm has emerged that proposes that large-scale language models implicitly learn to represent large amounts of factual and commonsense knowledge . While these methods are promising, they also show that the limited interface of language models precludes them from producing commonsense knowledge robustly. However, using knowledge graph tuples as additional training signal allows these model to be better adapted to representing knowledge . Furthermore, the use of these knowledge models to provide commonsense knowledge on-demand has shown promising results over static knowledge graphs . Consequently, in this work, we propose evaluating commonsense knowledge resources on a new, second purpose: whether they can be used to repurpose language models for commonsense modeling.   Furthermore, we formalize the \comet framework of \citet{Bosselut2019COMETCT} across different seed language models and training knowledge graphs, and evaluate the commonsense knowledge hypothesized by these adapted knowledge models. %Our results indicate that this purpose is a promising evaluation for commonsense resources, as \comet models can successfully hypothesize plausible knowledge for new, unseen entities.  Our empirical study yields two promising conclusions. First, it confirms that KG-adapted language models learn to express knowledge more precisely than naive language models trained only on language. And second, we show that \atomicTT{} as a transfer resource leads to \comet models that achieve the largest increase over their seed language model  for the commonsense knowledge types it covers, validating the importance of constructing knowledge resources with examples of knowledge not readily found in language models. %allows language models to learn representations of commonsense knowledge types that are less covered in naive language models. % Furthermore, a comparison of these \comet models across different commonsense knowledge graphs shows that \atomicTT{} as a transfer resource allows language models to learn richer commonsense knowledge representation than training with other resources.   %   Key Contributions:  In summary, we make three key contributions in this paper. We present \atomicTT{}---a new commonsense knowledge graph covering social, physical, and eventive aspects of everyday inferential knowledge . Next, we compare \atomicTT{} with other prominent CSKBs head-to-head and show that our new symbolic knowledge graph is more accurate than any current CSKB  . Finally, we show that our new neural knowledge model \comet{}-\atomicTT{} successfully transfers \atomicTT{}'s declarative knowledge to beat \gpttt{}, the largest pre-trained language model, in spite of using ~400x fewer parameters  . This demonstrates the utility and importance of high-quality symbolic knowledge provided by \atomicTT{} to generalize on commonsense information that LMs cannot expressively capture on their own .  % * Our new symbolic knowledge graph ATOMICTT is superior in accuracy and coverage to the currently existing large-scale knowledge graphs .  % * our neural knowledge model COMET-ATOMICTT successfully transfers the ATOMICTT's declarative knowledge to beat even the most impressively large pretrained model, GPT-3 . This demonstrates LMs, no matter its size, can benefit from the symbolic knowledge provided by high quality KB like ATOMICTT.   
"," % Check out this new knowledge graph! % Storyline: % \begin{enumerate} % \item We introduce \atomicTT. % \item We provide the first side-by-side comparison of commonsense knowledge bases and comprehensive ways to capture precision and coverage. % \item We show how commonsense KGs provide a clear vehicle to access knowledge in LMs.  ).  % \end{enumerate}  Recent years have brought about a renewed interest in commonsense representation and reasoning in the field of natural language understanding. The development of new commonsense knowledge graphs  has been central to these advances as their diverse facts can be used and referenced by machine learning models for tackling new and challenging tasks. At the same time, there remain questions about the quality and coverage of these resources due to the massive scale required to comprehensively encompass general commonsense knowledge.  In this work, we posit that manually constructed CSKGs will never achieve the coverage necessary to be applicable in all situations encountered by NLP agents. Therefore, we propose a new evaluation framework for testing the utility of KGs based on how effectively implicit knowledge representations can be learned from them.   With this new goal, we propose \atomicTT{}, a new CSKG of general-purpose commonsense knowledge containing knowledge that is not readily available in pretrained language models. We evaluate its properties in comparison with other leading CSKGs, performing the first large-scale pairwise study of commonsense knowledge resources. Next, we show that \atomicTT{} is better suited for training knowledge models that can generate accurate, representative knowledge for new, unseen entities and events. Finally, through human evaluation, we show that the few-shot performance of GPT-3 , while impressive, remains $\sim$12 absolute points lower than a BART-based knowledge model trained on \atomicTT{} despite using  over 430x fewer parameters.  % useful they are for training knowledge models that can generate relevant representative knowledge for new, unseen entities.  % In this work, we propose \atomicTT{}, a new knowledge graph of general-purpose commonsense knowledge facts. To evaluate its utility in comparison to existing resources, we perform the first large-scale pairwise study of commonsense knowledge graphs on coverage and precision. Finally, we posit that a new use for commonsense knowledge graphs is their ability to allow large-scale language models to learn to represent knowledge implicitly. We propose a new evaluation for testing knowledge graphs on how useful they are for training knowledge models that can generate relevant representative knowledge for new, unseen entities.",100
"   Despite its successes, neural machine translation  still has unresolved problems. Among them is the problem of rare words, which are paradoxically very common because of Zipf's Law. In part, this is a problem intrinsic to data-driven machine translation because the system will inevitably encounter words not seen in the training data. In part, however, NMT systems seem particularly challenged by rare words, compared with older statistical models.   One reason is that NMT systems have a fixed-size vocabulary, typically 10k--100k words; words outside this vocabulary are represented using a special symbol like \unk{}. Byte pair encoding  breaks rare words into smaller, more frequent subwords, at least allowing NMT to see them instead of \unk{} . But this by no means solves the problem; even with subwords, NMT seems to have difficulty learning translations of very rare words, possibly an instance of catastrophic forgetting .  Humans deal with rare words by looking them up in a dictionary, and the idea of using dictionaries to assist machine translation is extremely old. From a statistical perspective, dictionaries are a useful complement to running text because the uniform distribution of dictionary headwords can smooth out the long-tailed distribution of running text. In pre-neural statistical machine translation systems, the typical way to incorporate bilingual dictionaries is simply to include them as parallel sentences in the training data. But , this does not work well for NMT systems.  We are aware of only a few previous attempts to find better ways to incorporate bilingual dictionaries in NMT. Some methods use dictionaries to synthesize new training examples . \citet{arthur-etal-2016-incorporating} extend the model to encourage it to generate translations from the  dictionary. \citet{post+vilar:naacl2018} constrain the decoder to generate translations from the dictionary. What these approaches have in common is that they all treat dictionary definitions as target-language text, when, in fact, they often have properties very different from ordinary text. For example, CEDICT defines \zh{濮濄倛鍤  as ``'' which cannot be used as a translation. In the case of a monolingual source-language dictionary, the definitions are, of course, not written in the target language at all.  In this paper, we present an extension of the Transformer  that ``attaches'' the dictionary definitions of rare words to their occurrences in source sentences. We introduce new position encodings to represent the nonlinear structure of a source sentence with its attachments. Then the unmodified translation model can learn how to make use of this attached information. We show that this additional information yields improvements in translation accuracy of up to 3.1 BLEU. Because our method does not force dictionary definitions to be treated as target-language text, it is generalizable to other kinds of information, such as monolingual source-language dictionaries, which yield smaller improvements, but still as much as 0.7 BLEU.  }}      \centering          \scalebox{0.8}{%     \textrm{WE}[f]f\textrm{PE}[p]p\textrm{DPE}[q]q$ within a dictionary definition. The rare word \zh{濮濈粯鎹  is replaced with \unk{} and defined as the Dead Sea. The words of the definition are encoded with both the position of the defined word  and their positions within the definition.}      \end{figure*}  
"," Despite advances in neural machine translation  quality, rare words continue to be problematic. For humans, the solution to the rare-word problem has long been dictionaries, but dictionaries cannot be straightforwardly incorporated into NMT. In this paper, we describe a new method for ``attaching'' dictionary definitions to rare words so that the network can learn the best way to use them. We demonstrate improvements of up to 3.1 BLEU using bilingual dictionaries and up to 0.7 BLEU using monolingual source-language dictionaries.",101
"   % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. % %  % . %     %  %     % % final paper: en-us version  %     % %     %   % space normally used by the marker %     % This work is licensed under a Creative Commons  %     % Attribution 4.0 International License. %     % License details: %     % \url{http://creativecommons.org/licenses/by/4.0/}. % }   % 1. 鐟欙綁鍣 CCG閿涘奔浜掗崣 CCG 閻ㄥ嫰鍣哥憰浣 % 2. CCG parsing 閻ㄥ嫰鍣搁悙鐟版躬娴 supertagging閵嗗倿娓剁憰浣割嚠 contextual information 閺堝鐦潏鍐ㄣ偨閻 encode 閻ㄥ嫭鏌熷▔鏇樺倸澧犳禍铏规畱閺傝纭堕敍灞间簰閸欏﹤鐪梽鎰剁礄閸欘亪鍣伴悽 powerful encoder閿涘本鐥呴張澶嬪赴濮瑰倿顤傛径 contextual feature 閻ㄥ嫪缍旈悽銊ф畱閻梻鈹掗敍 % 3. n-gram 閺勵垯绔存稉顏呮箒閺佸牏娈 contextual feature閿涘苯褰查懗钘夘嚠 supertagging 閺堝鏁ら敍鍫熷絹娓氭稑褰查懗鐣屾畱鐠囧秳绗岀拠宥勭闂 combination 閻ㄥ嫭娈粈鐚寸礆 % 4. 閹存垳婊戦惃 model   % 鐟欙綁鍣 CCG閿涘矁鐦濆Ч鍥瘱閻ｈ揪绱檚upertag閿涘婀伴煬顐㈠瘶閸氼偂绨℃稉鏉跨槣閻ㄥ嫬褰炲▔鏇炴嫲鐠囶厺绠熼惃鍕繆閹 Combinatory categorial grammar  is a lexicalized grammatical formalism, where the lexical categories  of the words in a sentence provide informative syntactic and semantic knowledge for text understanding. % 閹垫禒 ccg閿涘瞼澹掗崚顐ｆЦ supertagging 瀵板牊婀侀悽 Therefore, CCG parse often provides useful information for many downstream natural language processing  tasks such as logical reasoning  and semantic parsing . To perform CCG parsing in different languages, % 閸 ccg parsing 閸掑棔琚卞銉ｄ靠upertagging 鏉╂瑤绔村銉︽付闁插秷顩 most studies conducted a supertagging-parsing pipline , in which their main focus is the first step, and they generated the CCG parse trees directly from supertags with a few rules afterwards. % which is known as ``almost parsing''   % with essential CCG information for a sentence and one can generate its parse directly from supertags with a few rules. % supertagging 闂囩憰 contextual information %  Building an accurate supertagger in a sequence labeling process requires a good modeling of contextual information. %  Recent neural approaches to supertagging mainly focused on leveraging powerful encoders with recurrent models , with limited attention paid to modeling extra contextual features such as word pairs with strong relations. % Graph convolutional networks  is demonstrated to be an effective approach to model such contextual information between words in many NLP tasks ; thus we want to determine whether this approach can  also help CCG supertagging.  However, we cannot directly apply conventional GCN models to CCG supertagging because in most of the previous studies the GCN models are built over the edges in the dependency tree of an input sentence. As high-quality dependency parsers are not always available, we do not want our CCG supertaggers to rely on the existence of dependency parsers.  %  Thus, we need another way to extract useful word pairs to build GCN models. For that, we propose to obtain word pairs from frequent chunks  in the corpus, because those chunks are easy to identify with co-occurrence counts. %  %  %  % Such features, which may come from n-grams or dependency parsing results, are demonstrated to be helpful for many NLP tasks , and they are expected to enhance CCG supertagging as well. % Among all such features, the ones from n-grams are more attractive since n-grams are easy to obtain and also provide word relation cues, while dependency parsing results are exactly the goal of CCG and thus conflicts with the problem setting. % % As for the model to encode such features, graph convolutional networks  is one of the promising choices although it is often built over the dependency or semantic parse of the input text. %However, GCN suffers from the limitation of obtaining such parsing results, which is exactly the goal of CCG and thus conflicts with the problem setting. % %So that one and they are expected to enhance CCG supertagging. %especially the n-gram ones because they are easy to obtain and can provide cues for word-word combination if they are appropriately modeled. % %\textcolor{blue}{ %To leverage such contextual features, graph convolutional networks  is one of the privileging approaches to do so, where the graph is often built over the dependency or semantic parsing results of the input text. %However, GCN suffers from the limitation of obtaining such parsing results, which is exactly the goal of CCG and thus conflicts with the problem setting. %} % \textcolor{red}{ % Consider that graph convolutional networks , which is an effective solution to learn contextual information and is demonstrated to be useful in many other NLP tasks , can be potentially useful for CCG supertagging.} % , such as semantic role labeling , sentiment classification , and question answering . %input words based on the results of dependency or semantic parsing of the input texts, which may not be an appropriate way to construct graph for CCG, %since the task itself is about parsing. % \textcolor{blue}{ % Therefore, an appropriate way to construct the graph is required for CCG and n-grams could potentially be helpful since they carry contextual information and provide a group of words in which  % its containing words  % they may have strong relationship with respect to word-word combination if the n-grams are appropriately selected. % % } % Previous studies using GCN often build the graph over the dependency or semantic parsing results of the input text, suffering from the limitation of obtaining such parsing results, which is exactly the goal of CCG thus conflicts with the problem setting. % To appropriately learn from n-grams, one requires the GCN to be able to distinguish different word pairs because such information in n-grams are not explicitly structured as that in dependency parses. %In addition, Because existing GCN models are limited in treating all word pairs equally, %while identifying and learning from essential units are important for syntactic tasks, we propose an adaptation of conventional GCN for CCG supertagging. %especially when the graph are not constructed on dependencies. %  % Inspired by that n-grams can carry contextual information and provide a span in which its containing words may have strong relationships if the n-grams are appropriately selected, we build the graph upon well selected n-grams. % , especially the ones containing words with strong relationships between each other,  % % n-gram 閺勵垯绔存稉顏堝櫢鐟曚胶娈 contexutal feature % Consider that n-grams are conventionally used as a simple yet effective method to represent contextual features in many NLP tasks %in which powerful encoders are used  % , % 閸ョ姵顒濋敍瀹-gram 鐎 supertagging 娑旂喐婀侀悽顭掔礉鐏忋倕鍙鹃弰顖炲亝娴滄稖鍏樻径鐔虹矋閹存劗鐓拠顓犳畱 n-gram閿涘矁鍏樻径鐔稿絹娓氭稑鍙ф禍搴ょ槤娑撳氦鐦濇稊瀣？缂佸嫬鎮庨崗宕囬兇閻ㄥ嫪淇婇幁顖ょ礉閺堝濮禍 supertagging % they are also expected to serve as effective contextual features for CCG supertagging, where they, \textcolor{blue}{especially the ones containing words with strong relationships between each other,} % that are valid phrases,  % should provide plausible cues on potential combinations among words. % 閻掓儼宀嬬礉婵″倷缍嶉張澶嬫櫏閸︽澘鍩勯悽銊ㄧ箹娴 n-gram 娓氭繃妫弰顖欑娑擃亝瀵幋姗堢礉閸ョ姳璐熼柇锝勭昂娑撳秹鍣哥憰浣烘畱 n-gram 閸欘垯浜掔拠顖氼嚤 supertagger %\textcolor{blue}{ % However, it is not trivial to appropriately learn from n-grams for syntactic tasks, % where one needs to identify informative n-grams out of all possible combinations of words for the task. %since the unimportant ones carrying misleading cues for the combination may hurt the performance of a supertagger. %}   %       % 閹垫禒銉ь儑娑撳顔岄柌宀勬桨鐏忚精顩﹂崨鐓庣安鏉╂瑩鍣烽惃鍕敶鐎圭櫢绱濈悰銊с仛閸戠儤娼甸幋鎴滄粦閺冦垼鍏橀悽鈺猤ram閿涘苯寮甸懗鐣屾暏GCN缂佹獢gram瀵ょ儤膩 % 閹存垳婊戦幓鎰毉 channeled attention 閺 model 鏉╂瑤绨 n-gram %To address these problems, In this paper, we propose attentive GCN  for CCG supertagging, where its input graph is built based on chunks  extracted with unsupervised methods. % In this paper, we propose attentive GCN  for CCG supertagging, where its input graph is built upon word groups suggested by high confident n-grams extracted from unsupervised methods. % , where the graph is constructed on word groups. %which follows the sequence labeling paradigm. % 鐠囷妇绮忔禒瀣矝婵″倷缍嶅銉ょ稊閿涘矂顩婚崗鍫濐嚠 n-gram 閸掑棛绮 % Inspired by that n-grams can carry contextual information and provide a span in which its containing words may have strong relationships if the n-grams are appropriately selected, we build the graph upon the n-grams in the sentence, where an edge will be added to a pair of words if they are in the same n-gram. In detail, two types of edges in the graph are introduced to model word relations within and across chunks %for the word groups to model the word-word relation within and cross the groups. % we build the graph over the words upon the n-grams in the input sentence, where an edge will be added to a pair of words if they are in a span suggested by the same n-gram. % % For edges within a group, feed-forward attention is applied  and an attention mechanism is applied to GCN to weight those edges. %and discriminately learn from them through the edges. %In addition, for each word, a attention mechanism is used % to weight the contextual information carried by all its associated words  according to their contribution to the tagging process. %  In doing so, different contextual information are discriminatively learned to facilitate CCG supertagging without requiring any external resources. % , with the \textcolor{blue}{within and cross chunk relations} % local and global word relations  % weighted on our in-chunk and cross-chunk edges, respectively. %Moreover, the way of building the graph requires no external resources  %suggested by high confident n-grams is learned by A-GCN through the in-group edges; and long distance relations among groups are also leveraged by cross-group edges. %Therefore, a hierarchical structure of word relations are built  %Besides, our approach proposes a novel self-supervised method to build the graph for GCN, where no extra parsing results  are required as extra input. % , but also our attentive GCN is able to discriminately learn from the contextual information carried by different words.} % In the proposed attention, n-grams associated to each word in the input texts are firstly categorized into different groups according to their length,  % 閻掕泛鎮楀В蹇庨嚋 n-gram 閺夈儱濮為弶 % and then fed into a specific channel of attentions according to their groups, so that the n-grams are weighted separately in each group according to their contributions to the supertagging process. % 婵傝棄顦╅敍宀顑囨稉閺勵垰灏崚顐＄啊闁插秷顩﹂惃鍕嫲娑撳秹鍣哥憰浣烘畱 n-gram閿涙稓顑囨禍灞炬Ц閼宠棄顧勬禒搴ㄥ亝娴滄盯鍣哥憰浣烘畱闂 n-gram 娑擃厼顒熼崚鐗堟纯鏉╂粏绐涚粋鑽ゆ畱 context information % In doing so, not only important n-grams are distinguished, but also can our approach discriminatively learn from n-grams in different length, where the infrequent and long n-grams carrying important long range contextual information are appropriately modeled without being influenced by the frequent short ones. %  % 鐎圭偤鐛欑拠浣规閺堝鏅 The validity of our approach is demonstrated by experimental results on the CCGbank , where state-of-the-art performance is obtained for both tagging and parsing.    
","  % supertagging 閻庣敻娑氳壘 CCG parsing 闂傚牏鍋涢悥鍫曟煂瀹ュ牜娲 Supertagging is conventionally regarded as an important task for combinatory categorial grammar  parsing, where effective modeling of contextual information is highly important to this task. % 闂傚嫨鍊撶花鈩冩媴鐠恒劍鏆忛柡鍥ㄦ綑瀹搁亶鎯 encoder闁挎稑鏈惁顔戒繆 biLSTM闁挎稑鑻晶鐘崇閸濆嫷鍤犲ù supertagging 閺夆晜鐟ら柌 task 闁告垹濮崇粻顔尖柦閿涘嫭鏆忛柛鎺楊暒缁牊绋婇崼婵嗙劶闁 context feature闁挎稑鑻畵 n-gram However, existing studies have made limited efforts to leverage contextual features except for applying powerful encoders . % 闁哄牜鍓氶弸鍐晬鐏炴儳鐏夊ù鐙鍓氳ぐ渚宕 channeled n-gram attention 闁哄鍎遍ˇ鈺呮偠閸℃氨绠瑰☉鎿冧邯濡埖锛 In this paper, we propose attentive graph convolutional networks to enhance neural CCG supertagging through a novel solution of leveraging contextual information. %  Specifically, we build the graph from chunks  extracted from a lexicon and apply attention over the graph, so that different  % word relations  word pairs from the contexts within and across chunks are weighted in the model and facilitate the supertagging accordingly. % 閻庡湱鍋ら悰娆戠磼閹惧浜悶娑栧妽濡叉垿鏁嶇仦鎯х亯濞寸媭鍓涘▓鎴﹀棘鐟欏嫮銆婇柡鍕靛灡濠渚寮崼銏＄暠 The experiments performed on the CCGbank demonstrate that our approach outperforms all previous studies % , as well as strong baselines from existing toolkits,  in terms of both supertagging and parsing. %  Further analyses illustrate the effectiveness of each component in our approach to discriminatively learn from word pairs to enhance CCG supertagging.\footnote{Our code and models for CCG supertagging are released at \url{https://github.com/cuhksz-nlp/NeST-CCG}.}",102
" Pre-trained Transformers  have lead to state-of-the-art results on a wide range of NLP tasks, for example, named entity recognition, relation extraction and question answering, often approaching human inter-rater agreement .  These models have also been demonstrated to learn effective cross-lingual representations, even without access to parallel text or bilingual lexicons .  Multilingual pre-trained Transformers, such as mBERT and XLM-RoBERTa , support surprisingly effective zero-shot cross-lingual transfer, where training and development data are only assumed in a high resource source language , and performance is evaluated on another target language. 	 Because no target language annotations are assumed in this setting, source language data is typically used to select among models that are fine-tuned with different hyperparameters and random seeds.  However, recent work has shown that English dev accuracy does not always correlate well with target language performance .  In this paper, we propose an alternative strategy for model selection in a zero-shot setting.  Our approach, dubbed Learned Model Selection , learns a function that scores the compatibility between a fine-tuned multilingual transformer, and a target language. The compatibility score is calculated based on features of the multilingual model's learned representations and the target language.  A model's features are based on its own internal representations; this is done by aggregating representations over an unlabeled target language text corpus.  These model-specific features capture information about how the cross-lingual representations transfer to the target language after fine-tuning on source language data.  In addition to model-specific representations, we also make use of learned language embeddings from the lang2vec package , which have been shown to encode typological information, for example, whether a language has prepositions or postpositions.  To measure compatibility between a multilingual model's fine-tuned representations and a target language, the model- and language- specific representations are combined in a bilinear layer.  Parameters of the scoring function are optimized to minimize a pairwise ranking loss on a set of held-out models, where the gold ranking is calculated using standard performance metrics, such as accuracy or F, on a set of pivot languages .  LMS does not rely on any annotated data in the target language for meta-learning or hyperparameter tuning, yet it is effective in learning to predict whether a multilingual model's representations are a good match for a specific target language.    In experiments on five well-studied NLP tasks , we find LMS consistently selects models with better target-language performance than those chosen using English dev data.  Appendix  demonstrates that our framework supports multi-task learning, which can be helpful in settings where some target-language annotations are available, but not for the desired task.  Finally, we show that LMS generalizes to both mBERT and XLM-RoBERTa in Appendix .  
"," Transformers that are pre-trained on multilingual text corpora, such as, mBERT and XLM-RoBERTa, have achieved impressive cross-lingual transfer learning results.  In the zero-shot cross-lingual transfer setting, only English training data is assumed, and the fine-tuned model is evaluated on another target language.  No target-language validation data is assumed in this setting, however substantial variance has been observed in target language performance between different fine-tuning runs.  Prior work has relied on English validation/development data to select among models that are fine-tuned with different learning rates, number of steps and other hyperparameters, often resulting in suboptimal choices.  To address this challenge, we propose a meta-learning approach to model selection that uses the fine-tuned model's own internal representations to predict its cross-lingual capabilities.  In extensive experiments we find that our approach consistently selects better models than English validation data across five languages and five well-studied NLP tasks, achieving results that are comparable to small amounts of target language development data.\footnote{We will make our code and data available on publication.}  %We further demonstrate that our method can benefit from pooling data across tasks when auxiliary annotations are available in the target language.",103
" %   Summarization is the process of identifying the most important information pieces in a document. For humans, this process is heavily guided by background knowledge, which encompasses preconceptions about the task and priors about what kind of information is important .    %  %   % Understanding background knowledge would yield insights about what, on average, humans consider as known, interesting and important.  % Furthermore, accurate models of human background knowledge would be greatly valuable to improve the selection methods of information selection systems.  %  Despite its fundamental role, background knowledge has received little attention from the summarization community. Existing approaches largely focus on the relevance aspect, which enforces similarity between the generated summaries and the source documents . % , without consideration for background knowledge.   In previous work, background knowledge has usually been modeled by simple aggregation of large background corpora. % A prominent example is \cpt{TFIDF} , a practical solution to the problem of identifying content words based on document frequencies within background corpora. For instance, using \cpt{TFIDF} , one may operationalize background knowledge as the set of words with a large document frequency in background corpora.  %While this approach was useful for the stopword problem significant to the development of summarization systems, it is cannot easily be extended to model background knowledge.  However, the assumption that frequently discussed topics reflect what is, on average, known does not necessarily hold. For example, common-sense information is often not even discussed . Also, information present in background texts has already gone through the importance filter of humans, e.g., writers and publishers. In general, a particular difficulty preventing the development of proper background knowledge models is its latent nature. We can only hope to infer it from proxy signals. Besides, there is, at present, no principled way to compare and evaluate background knowledge models.   %  In this work, we put the background knowledge in the foreground and propose to infer it from summarization data. Indeed, choices made by human summarizers and human annotators provide implicit information about their background knowledge. We build upon a recent theoretical model of information selection , which postulates that information selected in the summary results from 3 desiderata: low redundancy , high relevance , and high informativeness . The tension between these 3 elements is encoded in a summary scoring function  that explicitly depends on the background knowledge . % that explicitly depends on the background knowledge .  As illustrated by \Figref{fig:overall}, the latent  can then be inferred from the residual differences in information selection that are not explained by relevance and redundancy. For example, the black information unit in \Figref{fig:overall} is not selected in the summary despite being very prominent in the source document. Intuitively, this is explained if this unit is already known by the receiver.  % and the human summarizer regarded it as not important. To leverage this implicit signal, we view  as a latent parameter learned to best fit the observed summarization data.  %  \xhdr{Contributions} We develop algorithms for inferring  in two settings:  when only pairs of documents and reference summaries pairs are observed  and  when pairs of document and summaries are enriched with human judgments . % The framework also provides an evaluation methodology for , by measuring how well the resulting  correlates with human judgments.  In \Secref{sec:comparison} we evaluate our inferred s with respect to how well the induced scoring function  correlates with human judgments. Our proposed algorithms significantly surpass previous baselines by large margins.   In \Secref{sec:geometry}, we give a geometrical perpespective on the framework and show that a clear geometrical structure emerges from real summarization data.  % The framework is simple, constrained and interpretable but this does not hinder its ability to fit the data. In fact, our proposed algorithms significantly and largely surpass previous baselines in terms of correlation with human judgments.   % The framework is general and inferring human prior on information importance can be of broad use. We explore several applications and briefly discuss potential for future work. The ability to infer interpretable importance priors in a data-driven way has many applications, some of which we explore in \Secref{sec:applications}.  % We explore some of them and later discuss possibilities for future work. \Secref{sec:qualitative_analysis} qualitatively reveals which topics emerge as known and unkown in the fitted priors. % First, it is possible to investigate qualitatively the fitted priors to understand which topics emerge as known and unkown.  % We do so both at the word level and at the topic-model level.  Moreover, we can infer  based on different subsets of the data. By training on the data of one annotator, we get a prior specific to this annotator. Similarly, one can find domain-specific 's by training on different datasets. This is explored in \Secref{sec:annotator_specific}, where we analyze  annotators and  different summarization datasets, yielding interesting insights, e.g., averaging several, potentially biased, annotator-specific or domain-specific 's results in systematic generalization gains. % Adding the inferred 's to summarization systems can produce improvements in the quality of extracted summaries .   Finally, we discuss future work and potential applications beyond summarization in \Secref{sec:ccl}. Our code is available at \url{https://github.com/epfl-dlab/KLearn}     %that averaging various annotator specific 's gives large generalization improvements over single annotators and compared to previous baselines. Furthermore, the average of all annotators performs almost as good as the optimal . Similarly, averaging many domain-specific 's gives significant improvements over baselines in TAC datasets.  %Finally, a more qualitative analysis of the best 's reveals that they capture stopwords and some properties of IDFs even without being exposed to any background corpora.       %Background knowledge is important in summarization and often left out.  %When not left out, it requires design choices and collection of large background corpora.  %Previous work has defined simple models of summarization which involves background knowledge from first principles  %We show that such formulation allows us to infer background knowledge simply from observing human preferences.   %In fact, a probabilistic model is developed that can infer background knowledge only from pairs of document summaries.  
"," The goal of text summarization is to compress documents to the relevant information while excluding background information already known to the receiver. So far, summarization researchers have given considerably more attention to relevance than to background knowledge. In contrast, this work puts background knowledge in the foreground. Building on the realization that the choices made by human summarizers and annotators contain implicit information about their background knowledge, we develop and compare techniques for inferring background knowledge from summarization data. Based on this framework, we define summary scoring functions that explicitly model background knowledge, and show that these scoring functions fit human judgments significantly better than baselines. We illustrate some of the many potential applications of our framework. First, we provide insights into human information importance priors. Second, we demonstrate that averaging the background knowledge of multiple, potentially biased annotators or corpora greatly improves summary\hyp scoring performance. Finally, we discuss potential applications of our framework beyond summarization. % Finally, we apply our models in a simple yet effective summarization system.",104
"  . }  Definition Extraction refers to the task in Natural Language Processing  of detecting and extracting a term and its definition in different types of text. A common use of automatic definition extraction is to help building dictionaries , but it can be employed for many other applications. For example, ontology building can benefit from methods that extract definitions , whilst the fields of definition extraction and information extraction can employ similar methodologies. It is therefore normal that there is growing interest in the task of definition extraction.  This paper describes our system that participated in two of the three subtasks of Task 6 at SemEval 2020 , a shared task focused on definition extraction from a specialised corpus. Our method employs state-of-the-art neural architectures in combination with automatic methods which extend and clean the provided dataset.  %Task 6 at SemEval 2020  is a shared task for definition extraction from a specialised corpus, tailoured specifically to the needs of definition extraction. This paper describes the RGCL team system that works on all three subtasks of the shared task. We employ state-of-the-art neural architectures and combine them with simple automatic methods to extend and clean the provided dataset where appropriate.  The remaining parts of this paper are structured as follows. First, we present related work in the area of definition extraction and the related field of relation extraction . The three subtasks and the dataset provided by the task organisers are described in Section . Next, we describe our system , followed by the results of the evaluation  and a final conclusion .   
","   This paper presents the RGCL team submission to SemEval 2020 Task 6: DeftEval, subtasks 1 and 2. The system classifies definitions at the sentence and token levels. It utilises state-of-the-art neural network architectures, which have some task-specific adaptations, including an automatically extended training set. Overall, the approach achieves acceptable evaluation scores, while maintaining flexibility in architecture selection.",105
" Event extraction is a process to extract the named entities, event triggers and their relationships from real-world corpora. The named entities refer to those texts about predefined classes  and event triggers are words that express the types of events in texts . In literature, named entities and triggers are connected and named entities with corresponding roles are called arguments for a given trigger of a specific event.  %Named entities refer to the text mentions with predefined classes such as person names, company names and locations, etc. An event trigger is a word that mostly expresses the event types  in text. Named entities link to triggers by different roles, and named entities with corresponding roles are called arguments for a given trigger  of a specific event.  Currently, most existing works divide the event extraction into two independent sub-tasks: named entity recognition and trigger labeling. These two sub-tasks are always formulated as multi-class classification problems, and many works apply the sequence-to-sequence based labeling method which aims to translate a sentence into sequential tags. From our investigation, one problem of these sequence-to-sequence methods is that they ignore the orders of output tags, and therefore, it is difficult to precisely annotate different parts of an entity. To address this issue, some methods propose to incorporate the conditional random field  module to be aware of order-constraints for the annotated tags.  Since entities and triggers are naturally connected around events, recent works try to extract them jointly from corpora. Early methods apply pipeline frameworks with predefined lexical features which lack generality to different applications. Recent works leverage the structural dependency between entities and triggers to further improve the performances of both the entity and trigger identification sub-tasks.  %The prevalent methods can be divided into two categories: a) a parallel framework to obtain entities and triggers simultaneously and b) a pipeline framework to get triggers at first and then perform sub-tasks to extract entities. Takanobu et al.  propose a hierarchical reinforcement learning model to extract triggers first and then evoke a sub-process to get the related entities by referring to the obtained triggers in the same sentences. Nguyen et al.  design an attention mechanism to augment the accuracy for trigger extraction in multilingual environments. Fu el al.  employ graph convolutional network  to capture the local contextual information in sentences and use a two-stage method to extract entities and triggers from text together.   % The main challenges to improve the performance of jointly extract entities and triggers are two-fold: Although existing works have achieved comparable performance on jointly extracting entities and triggers, these approaches still suffer the major limitation of losing co-occurrence relationships between entities and triggers. Many existing methods determine the trigger and entities separately and then match the entities with triggers. % In this way, the co-occurrence relationships between entities and triggers are ignored, therefore, those methods might require more pre-trained features or prior data in order to achieve better performance. In this way, the co-occurrence relationships between entities and triggers are ignored, although pre-trained features or prior data are introduced to achieve better performance. It is also challenging to capture effective co-occurrence relationships between the entities and their triggers. We observed from the experiments that most of the entities and triggers are co-occurred sparsely  throughout a corpus. This issue exacerbates the problem of losing co-occurrence relationships mentioned before.   %However, most existing methods suffer performance degradation when extracting entities and triggers jointly. The reason is that most of the entities and triggers are sparsely  co-occurred throughout a corpus and the previous approaches do not well handle this sparse co-occurred relationship. %In addition, it is challenging to establish an effective interaction mechanism between the sub-tasks for joint-event-extraction, because traditional joint learning may lead to an error-propagation issue that lowers the accuracy of joint tasks.       %% label for entire figure \end{figure*}  To address the aforementioned challenge, the core insight of this paper is that in the joint-event-extraction task, the ground-truth annotations for triggers could be leveraged to supervise the extraction of the entities, and vice versa. Based on this insight, this paper proposes a novel method to extract structural information from corpora by utilizing the co-occurrence relationships between triggers and entities. Furthermore, in order to fully address the aforementioned sparsely co-occurrence relationships, we model the entity-trigger co-occurrence pairs as a heterogeneous information network  and supervise the trigger extraction by inferring the entity distribution with given triggers based on the indirect co-occurrence relationships collected along the meta-paths from a heterogeneous information network .  Figure illustrates the process of our proposed method to collect indirect co-occurrence relationships between entities and triggers. Figure is a sub-graph of the ``entity-trigger'' HIN for the ACE 2005 corpus. Figure compares the entity distributions inferred from given triggers based on the direct adjacency matrix and that inferred from the meta-path adjacency matrix. From this figure, we observe that a trigger does not necessarily connect to all entities directly and the direct-adjacency-based distribution is more concentrated on a few entities, while the meta-path-based distribution is spread over a larger number of entities. This shows that a model could collect indirect co-occurrence patterns between entities and triggers based on the meta-path adjacency matrix of an ``entity-trigger'' HIN. Moreover, the obtained indirect patterns could be applied to improve the performance to extract both entities and triggers.  Based on the aforementioned example and analysis, we propose a neural network to extract event entities and triggers. Our model is built on the top of sequence-to-sequence labeling framework and its inner parameters are supervised by both the ground-truth annotations of sentences and ``entity-trigger'' co-occurrence relationships. Furthermore, to fully address the indirect ``entity-trigger'' co-occurrence relationships, we propose the \underline{C}ross-\underline{S}upervised \underline{M}echanism  based on the HIN. The CSM alternatively supervises the entity and trigger extraction with the indirect co-occurrence patterns mined from a corpus. CSM builds a bridge for triggers or entities by collecting their latent co-occurrence patterns along meta-paths of the corresponding heterogeneous information network for a corpus. Then the obtained patterns are applied to boost the performances of entity and triggers extractions alternatively. We define this process as a ``cross-supervise'' mechanism. The experimental results show that our method achieves higher precisions and recalls than several state-of-the-art methods.  In summary, the main contributions of this paper are as follows:   The remainder of this paper is organized as follows. In Section, we first introduce some preliminary knowledge about event extraction and HIN, and also formulate the problem. Section presents our proposed model in detail. Section verifies the effectiveness of our model and compares it with state-of-the-art methods on real-world datasets. Finally, we conclude this paper in Section.  
"," Joint-event-extraction, which extracts structural information  from unstructured real-world corpora, has attracted more and more research attention in natural language processing. Most existing works do not fully address the sparse co-occurrence relationships between entities and triggers, which loses this important information and thus deteriorates the extraction performance. To mitigate this issue, we first define the joint-event-extraction as a sequence-to-sequence labeling task with a tag set composed of tags of triggers and entities. Then, to incorporate the missing information in the aforementioned co-occurrence relationships, we propose a \underline{C}ross-\underline{S}upervised \underline{M}echanism  to alternately supervise the extraction of either triggers or entities based on the type distribution of each other. Moreover, since the connected entities and triggers naturally form a heterogeneous information network , we leverage the latent pattern along meta-paths for a given corpus to further improve the performance of our proposed method. To verify the effectiveness of our proposed method, we conduct extensive experiments on four real-world datasets as well as compare our method with state-of-the-art methods. Empirical results and analysis show that our approach outperforms the state-of-the-art methods in both entity and trigger extraction.",106
"  Recently, pre-trained self-supervised models such as BERT have attracted an increasing amount of attention in natural language processing and vision-language processing.  Benefiting from common knowledge contained in massive unlabeled data, the pretraining-finetuning framework has become a representative paradigm for advancing various language-related downstream tasks.   Most endeavors on pre-trained representation models rely on elaborately designed self-supervised tasks, which typically corrupt the given sequence with certain types of noise , and then train the model to recover the original sequence.  As a consequence, the learned representations tend to be covariant with the input noise of pre-training in this paradigm.  However, when transferred to downstream tasks, the pre-trained model is responsible for encoding the original sequence without noise, and is expected to obtain noise invariant representations.  Such pretrain-finetune discrepancy not only impedes fast fine-tuning, but also may result in suboptimal sequence representations, thus affecting the performance in downstream tasks.   %%%%%%%%%%%% %  % 	\vskip -0.1in % \end{table} %%%%%%%%%%%%  %%%%%%%%%%%% %  %%%%%%%%%%%%  To remedy this, we present ContrAstive Pre-Training  to learn noise invariant  sequence representations. %, inspired by the Noise Contrastive Estimation. The core idea of CAPT is to enhance the consistency between semantic representations of the original sequence and that of corresponding corrupted version  via unsupervised instance-wise training signals. %can be fully utilized via elaborately designed semantic contrastive loss. %As shown in Figure, our approach  In more detail, it strives to pull the representation of the corrupted sequence towards that of the original instance in the semantic space, while pushing it away from representations of other instances. % Such training objectives are formulated as a multi-class classification task, which aims at classifying the original sequence to the class of its corrupted version and vice versa, while classifying different instances into different classes. % For implementation feasibility, two effective model extension are proposed to further enhance the capability of the model to extract noise-concentrated and instance-diffused features. Moreover, in order to enable the model to learn from more ``difficult'' and ``diverse'' instances, two effective methods are proposed to further enhance the capability of the model to extract noise-concentrated and instance-diffused features. With such training objective, the pre-trained model is encouraged to learn noise invariant representations, thereby alleviating the pretrain-finetune discrepancy to some extent.  As an additional benefit, CAPT also assists the pre-trained model to more effectively capture the global semantics of the input.  Most prior work only focuses on token-level pre-training tasks , which lacks the modeling of global semantics of the input.  Some other efforts alleviate this problem by introducing sentence-level pre-training tasks  that rely on the relative position of segments in the document. However, the semantic connection between these segments tends to be excessively loose, which may result in confusing gradient signals.  By contrast, our CAPT offers incentives for representations of inputs sharing the same semantics  to be similar, while the representations of inputs expressing different semantics  are penalized to be distinguished from each other. Such more reasonable sentence-level supervision enables our approach to look beyond the local structures of input sequences and become more aware of the global semantics. %With such more reasonable sentence-level supervision, our approach achieves better modeling of global semantics of the input.   We perform the evaluation on a comprehensive suite of benchmark, covering 8 natural language understanding and 3 cross-modal tasks.  Extensive empirical evidence demonstrates that our approach can achieve consistent improvements over the baselines in both language and vision-language domains. To be more specific, our CAPT raises the performance of RoBERTa from 88.9\% to 89.5\% on the GLUE dev set, and also surpasses LXMERT by 0.5\%, 0.6\% and 0.8\% on VQA, GQA and , respectively.    
"," Pre-trained self-supervised models such as BERT have achieved striking success in learning sequence representations, especially for natural language processing. These models typically corrupt the given sequences with certain types of noise, such as masking, shuffling, or substitution, and then try to recover the original input. However, such pre-training approaches are prone to learning representations that are covariant with the noise, leading to the discrepancy between the pre-training and fine-tuning stage. To remedy this, we present ContrAstive Pre-Training  to learn noise invariant sequence representations. The proposed CAPT encourages the consistency between representations of the original sequence and its corrupted version via unsupervised instance-wise training signals. In this way, it not only alleviates the pretrain-finetune discrepancy induced by the noise of pre-training, but also aids the pre-trained model in better capturing global semantics of the input via more effective sentence-level supervision. Different from most prior work that focuses on a particular modality, comprehensive empirical evidence on 11 natural language understanding and cross-modal tasks illustrates that CAPT is applicable for both language and vision-language tasks, and obtains surprisingly consistent improvement, including 0.6\% absolute gain on GLUE benchmarks and 0.8\% absolute increment on $\text{NLVR}^2$.",107
" \subsection{Natural Language Processing} Ang Natural Language Processing  ay isang subfield ng linguistics, computer science, at artificial intelligence na nauukol sa pag proseso at pag-unawa ng natural na wika . Ang ilan sa mga aplikasyon ng NLP ay ang email spam filters , pag-unawa ng nais sabihin tulad ng mga smart assistants , pagsasalin ng isang wika sa iba pang wika , mag predict ng susunod na salita base sa mga naunang salita , at marami pang iba. Dahil sa kaunlaran sa kasaganahan sa datos at pagiging accessible ng malakas na compute power, nabuhay muli ang machine learning approach. Sa maikling salita, ang machine learning approach ay gumagamit ng malaking datos na ginagamit ng isang computer algorithm upang matutunan ang mga patterns ng datos na ito. Dahil dito, naging epektibo siyang approach sa mga komplikadong problema dahil hindi na kailangan direktang i-program ang mga rules para malutas ang isang problema.  \subsection{Transfer Learning} Notorious ang machine learning approach sa pangangailangan nito ng sobrang laking datos para mapakinabangan. Ang Transfer Learning  ay isang area ng research na concerned sa problemang ito . Sa maikling salita, ang TL ay ang pag retain o pagpapanatili ng mga natutunan ng isang model sa isang gawain at paggamit o ""transfer"" ng mga natutunan nito sa iba pero may kaugnayan na gawain. Halimbawa, ang mga natutunan ng isang model sa pag detect ng muka ng tao ay maaring gamitin bilang tuntungan para sa pag-aaral ng model na matutunan kung ang muka ng tao ay galit, masaya, at iba pang facial expressions .   
"," Ang mga low-resource languages tulad ng Filipino ay gipit sa accessible na datos kaya't mahirap gumawa ng mga applications sa wikang ito. Ang mga Transfer Learning  techniques ay malaking tulong para sa low-resource setting o mga pagkakataong gipit sa datos. Sa mga nagdaang taon, nanaig ang mga transformer-based TL techniques pagdating sa low-resource tasks ngunit ito ay mataas na compute and memory requirements kaya nangangailangan ng mas mura pero epektibong alternatibo. Ang papel na ito ay may tatlong kontribusyon. Una, maglabas ng pre-trained AWD-LSTM language model sa wikang Filipino upang maging tuntungan sa pagbuo ng mga NLP applications sa wikang Filipino. Pangalawa, mag benchmark ng AWD-LSTM sa Hate Speech classification task at ipakita na kayang nitong makipagsabayan sa mga transformer-based models. Pangatlo, suriin ang performance ng AWD-LSTM sa low-resource setting gamit ang degradation test at ikumpara ito sa mga transformer-based models.",108
"  \iffalse \dr{%If we want to reposition it as in the abstract, we should start by considering the event in Fig. 1:  Natural language text is typically written to tell the reader about events. But events are not expressed as single predicate mentions, but rather as structures over multiple such predicates and their arguments. Consider the description the impact of the Typhoon in Fig..... It is mentioned that the typhoon killed people , flights canceled and affected many people. It is also clear that there is temporal order among some of the predicates, and recognizing this is important to understanding the composite event. Then you can continue saying that this is our goal.}  \fi  % typically, a single predicate mention  does not constitute what we typically think about as events; we typically think of an event as something that consists of multiple such primitive structures %{\fontsize{10.5}{11} \selectfont Text}           %\fontsize{11pt}{13pt}\selectfont Human languages evolve to communicate about %always involve the description of  real-world events. Therefore, understanding events plays a critical role in natural language understanding . A key challenge to this mission lies in the fact that events are not just simple, standalone predicates. Rather, they are often described at different granularities and may form complex structures. %topologies. Consider the example in Figure, where the description of a storm  involves more fine-grained event mentions about people killed , flights canceled  and passengers affected . Some of those mentions also follow strict temporal order . Our goal is to induce such an event complex that recognizes %organizes  the membership of multi-granular events described in the text, as well as their temporal order. This is not only at the core of text understanding, but is also beneficial to various applications such as question answering , narrative prediction , timeline construction  and summarization . %\dr{The choice of references is good but revealing; I suggest to replace the summarization with a ``classical"" summarization paper .  %such as question answering , narrative prediction , coreference resolution , and summarization . Since events are not standalone objects, understanding event essentially involves comprehending their relations, %cite{wities-etal-2017-consolidated, wadden-etal-2019-entity}, relations , as well as their internal structures and processes .  inasmuch as they necessarily provide actionable knowledge to support question answering , narrative prediction , timeline construction  and summarization .      \muhao{TODO: forming what we call a ``event complex''} Human languages always involve the description of real-world events. Therefore, understanding events plays a critical role in natural language understanding , and supports tasks such as question answering , narrative prediction , timeline construction  and summarization . Typically, events are not just standalone predicate mentions, but rather as structures over multiple such predicates. Consider the example in Figure.  The description to the impact of the storm  also involves mentions about killed people , canceled flights  and affected passengers . Some of mentions thereof also follow temporal order. To support the comprehension of complex events, it is important to recognize the multifaceted relations for the predicate mentions in the text. \fi        % second paragraph \iffalse Recently, much research effort has been put into extracting specific aspects of relations for events. \citet{ning-etal-2018-improving} studied event temporal relation  extraction with a statistical common sense resource \citet{ning-etal-2019-improved} and \citet{han-etal-2019-joint} adopted data-driven methods for TempRel extraction; parent-child relations among events are studied in \citealp[]{liu-etal-2018-graph} and \citealp[]{aldawsari-finlayson-2019-detecting}. Though some of the previous work has ensured consistency via adding constraints in the inference phase, essentially they are not improving local predictions and the inconsistent results from the models might not be corrected in the inference stage. Besides, most of the approaches suffered from limited learning resources and the tasks are studied separately. \fi  Recently, significant %much research effort has been devoted to several event-event relation extraction tasks, such as event temporal relation  extraction  and subevent relation extraction . Addressing such challenging tasks requires a model to recognize the inherent connection between event  %\dr{should it be predicate mentions, to ease the ambiguity?}  mentions as well as their contexts in the documents. Accordingly, a few previous methods apply statistical learning methods to characterize the grounded events in the documents . Such methods often require designing various features to characterize the structural, discourse and narrative aspects of the events, which are costly to produce and are often specific to a certain task or dataset. More recent works attempted to use data-driven methods based on neural relation extraction models  which refrain from feature engineering and offer competent performances.     \iffalse \dr{The next two paragraphs can be shortened, but they are the right paragrpahs to include here.} While data-driven methods provide a general and tractable way to capture specific event-event relations, it still remains challenging for those methods to precisely infer the correct relations. One challenge is that almost every task for event-event relation extraction comes with limited available annotated resources. Specifically, most tasks annotate no more than a hundred articles . Even the largest one in the literature, i.e., MATRES  for TempRel extraction, contains annotation for merely 275 articles. The lack of supervision hinders feature learning of events as well as inference of the relations, %Therefore, effectively tackling these tasks inevitably calls  therefore calling upon plausible auxiliary supervision from resources that are external to each of the tasks.    On the other hand, the event-event relations are often constrained by  %\drc{logical \dr{}change everywhere} %logic %\muhao{done.} properties, such as transitivity of TempRels Before and After , as well as that of %the relation between parent and child events subevent relations . In favor of such constraints, literature has employed global inference in the inference phase to comply with the logical properties particularly for TempRels . However, there lacks an effective way to ensure the global logical consistency in the training phase, which is key to making a data-driven machine learning model consistent on the beliefs of training data for various relation types . Moreover, the logical constraints may apply to different categories of %event-event  relations, and form complex conjunctive rules.  Consider the example in Figure : given that e2:died is Before e3:canceled and e3:canceled is a parent event of e4:affecting, the learning process should enforce e2:died Before e4:affecting. %\todo{Add an example of a conjunctive rule containing temporal and subevent relations.} Accordingly, ensuring the logical constraints across task-specific relations is another challenge being overlooked by the literature, the resolve of which provides a natural way to bridge the learning processes on multiple tasks. %\magenta{HW:TCR?} \fi  While data-driven methods provide a general and tractable way for event-event relation extraction, their performance is restricted by the limited annotated resources available. For example, the largest temporal relation extraction dataset MATRES only has 275 articles, which is far from enough for training a well-performing supervised model. The observation that relations and, in particular, event-event relations should be constrained by their logical properties , led to employing global inference to comply with transitivity and symmetry consistency, specifically on TempRel . However, in an event complex, the logical constraints may globally apply to different task-specific relations, and form more complex conjunctive constraints.  Consider the example in Figure : given that e2:died is Before e3:canceled and e3:canceled is a Parent event of e4:affecting, the learning process should enforce e2:died Before e4:affecting by considering the conjunctive constraints on both TempRel and subevent relations. While previous works focus on preserving logical consistency through  inference or structured learning , there was no %lacks an  effective way to endow neural models with the sense of global logical consistency during training.  %\dr{Notice that the previous statement was not correct; I change to limit it to neural models, since structure learning did it} %ensure the global logical consistency in the training phase.  This is key to bridging %bridge  the learning processes of %on both TempRel and subevent relations, which is a research focus of this paper.  %Event-relation extraction is a non-trivial task because of the following challenges: %1) Almost every event relation extraction task comes with limited learning resources with annotations. %2) Event relations are often volatile given different scenarios, and the determination of parent-child relation is especially difficult since there are less explicit lexical expressions compared with the cases for time and causation. %3) Event relations are often endowed with logical properties: % some temporal relations and parent-child relations comply with transitivity; % logical consistency should also be ensured across different categories of event relations.  The first contribution of this work is proposing %to propose  a joint constrained learning model for multifaceted event-event relation extraction.  The joint constrained learning framework seeks to regularize the model towards consistency with the logical constraints across both temporal and subevent relations, for which three types of consistency requirements are considered: annotation consistency, symmetry consistency and conjunction consistency. Such consistency requirements comprehensively define the interdependencies among those relations, essentially unifying the ordered nature of time and the topological nature of multi-granular subevents based on a set of declarative logic rules. Motivated by the logic-driven framework proposed by \citet{li-etal-2019-logic}, the declarative logical constraints are converted into differentiable functions that can be incorporated into the learning objective for relation extraction tasks.  Enforcing logical constraints across temporal and subevent relations is also a natural way to combine %two event-event relation extraction tasks with a shared learning objective. the supervision signals coming from two different datasets, one for each of the  relation extraction tasks with a shared learning objective. %\dr{You said what is the first contribution, but not the second; do you want now to claim this as the second contribution? Note that I modified to emphasize the two datasets} %Besides, the consistency of the final prediction is further enforced by global inference via an ILP solver.  Despite the scarce annotation for both tasks, the proposed method surpasses the SOTA TempRel extraction method on MATRES by relatively 3.27\% in ; %\dr{I don't understand -- is it relative or F1? Also, Tab. 2 shows 2.5\%}  it also offers promising performance on the HiEve dataset for subevent relation extraction, relatively surpassing previous methods by at least 3.12\% in .  %\dr{which table is this from?} %by 3.12\% and 21.4\%. %We further provide ablation studies to show the importance of each component of our framework. %This fact is further illustrated by ablation studies.   From the NLU perspective, %the acquired knowledge of our method is able to simultaneously models the internal membership structure of a complex event, as well as the temporal relations among both simple and complex events. the second contribution of this work lies in providing a general method for inducing an event complex that comprehensively represents the relational structure of several related event %\drc{predicate} % mentions. %in two directions.  This is supported by the memberships vertically identified between multi-granular events, as well as the horizontal temporal reasoning within the event complex. As far as we know, this is %essentially different from all %many  previous works that only formulated relations along a single axis. Our model further demonstrates the potent capability of inducing event complexes  %with promising performance  when evaluated  %based  on the RED dataset .     
","     %\dr{I think that the current version  is too detailed and does not position the work at all, it just says what is being done. Here is a suggestion:}    Understanding natural language involves recognizing how multiple event mentions structurally and temporally interact with each other.     In this process, one can induce event complexes that organize multi-granular events with temporal order and membership relations interweaving among them.    Due to the lack of jointly labeled data for these relational phenomena and the restriction on the structures they articulate, we propose a joint constrained learning framework for modeling event-event relations.    Specifically, the framework enforces logical constraints within and across multiple temporal and subevent relations     %of events     by converting these constraints into differentiable learning objectives. We show that our joint constrained learning approach effectively compensates for the lack of jointly labeled data, and outperforms SOTA methods on benchmarks for both temporal relation extraction and event hierarchy construction, replacing a commonly used but more expensive global inference process.    We also present a promising case study showing the effectiveness of our approach in inducing event complexes on an external corpus.\footnote{Our code is publicly available at \url{https://cogcomp.seas.upenn.edu/page/publication_view/914}.} %\dr{Doesn't this contradict the statement above regarding the lack of joint data? Do we need to address it somehow}    %\dr{do we need the next clause? really, you show that you don't need it, but it reads like you just don't use it. If you really want to keep it, maybe better to say ""replacing a commonly used, more expensive, global inference process""} even without global inference that is widely used in previous methods.     \iffalse     \drc{Understanding events described in natural language text requires a reader to identify how they interact, structurally and temporally, to form an event complex.      Nevertheless, most of the work in NLP has focused on predicate mentions and not on the event complex they form together.      In this paper we study the induction of larger event units from text -- identifying a set of predicate mentions that together -- via temporal, co-reference, and subevent relations, form event complexes.     The scarcity of jointly labeled data for these relational phenomena presents a significant technical challenge. However, these phenomena interact with each other, thus restricting the structures they articulate. To make this explicit, we propose a joint learning framework that enforces logical constraints among the relations to be identified, by converting these into differentiable learning objectives.      We show that not only does our joint training approach address the lack of jointly labeled data, but it also outperforms SOTA results on both the temporal benchmark data set and the event hierarchy benchmark data set. %We also present a promising case study on RED, a small-scale dataset with fully annotated relations.     }     \fi     \ignore{     We study within-document temporal and hierarchical relations of events using a joint constrained learning framework.      %We first obtain the event representation  via an encoder, and then jointly train a multi-layer perceptron to predict confidence scores for temporal and hierarchical relations before we make structured prediction via integer linear programming .      The framework first incorporates a contextualized encoder to characterize the events in the document, and then predicts the confidence scores for temporal and hierarchical relations among them.     In the training phase, our framework learns to enforce logic consistency among various types of event relations in both categories,     by converting declarative rules into differentiable learning objective functions. %Furthermore, the consistency of final prediction is enforced by global inference .      %The inference phase performs structured prediction based on integer linear programming  to respect the corresponding logic constraints of relations.     %We utilize the benchmark dataset for the extraction task of each category of relations for training and evaluation. %By experimental results, we prove the feasibility of joint constrained learning of different tasks using datasets that have partial annotations for each task, %avoiding the labor for creating another dataset that has full annotation.     The experimental results show that the proposed framework outperforms the state-of-the-art method on the benchmark dataset, MATRES, of event temporal relation extraction task by 2.8\%; and it improves over the model of training jointly without constraints by 5\% F1-score on HiEve dataset, a benchmark for event hierarchy construction.     Therefore, the joint constrained learning effectively bridges the tasks with limited annotated learning resources, and promisingly leverages domain rules to support the precise learning and inference of various event relations.     }",109
"  Word embeddings which can capture semantic similarities have been extensively explored in a wide spectrum of Natural Language Processing  applications in recent years.  Word2Vec , FastText , and Glove  are some examples. Even though distributional word embeddings produce high quality representations, representing longer pieces of text such as sentences and paragraphs is still an open research problem. A sentence embedding is a contextual representation of a sentence which is often created by transformation of word embeddings through a composition function. There has been a large body of work in the literature which propose different approaches to represent sentences from word embeddings. SkipThought , InferSent , and Universal Sentence Encoder  are well-known examples.  % Other proposed methods for learning sentence representations include, but are not limited to .  There has been a growing interest in understanding what linguistic knowledge is encoded in deep contextual representation of language. For this purpose, several probing tasks are proposed to understand what these representations are capturing . One of the interesting findings is that despite the existence of explicit syntactic annotations, these learned deep representations encode syntax to some extent . Hewitt et. al. provide an evidence that the entire syntax tree is embedded implicitly in deep model's vector geometry. Kuncoro et. al.  show that LSTMs trained on language modeling objectives capture syntax-sensitive dependencies. Even though deep contextual language models implicitly capture syntactic information of sentences, explicit modeling of syntactic structure of sentences has been shown to further improve the results in different NLP tasks including neural language modeling \cite {shen2017neural, havrylov2019cooperative}, machine comprehension , summarization , text generation , machine translation , authorship attribution , etc. Furthermore, Kuncoro et. al. provide evidence that models which have explicit syntactic information result in better performance . Of particular interest, one of the areas where syntactic structure of sentences plays an important role is style-based text classification tasks, including authorship attribution. The syntactic structure of sentences captures the syntactic patterns of sentences adopted by a specific author and reveal how the author structures the sentences in a document.   Inspired by the above observations, our initial work demonstrates that explicit syntactic information of sentences improves the performance of a recurrent neural network classifier in the domain of authorship attribution . We continue this work in this paper by investigating if structural representation of sentences can be learned explicitly. In other words, similar to pre-trained word embeddings which mainly capture semantics, can we have pre-trained embeddings which mainly capture syntactic information of words. Such pre-trained word embeddings can be used in conjunction with semantics embeddings in different domains including authorship attribution. For this purpose, we propose a self-supervised framework using a Siamese  network  to explicitly learn the structural representation of sentences. The Siamese network is comprised of two identical components; a lexical sub-network and a syntactic sub-network; which take the sequence of words in the sentence and its corresponding linearized syntax parse tree as the inputs, respectively. This model is trained based on a contrastive loss objective where each pair of vectors  is close to each other in the embedding space if they belong to an identical sentence , and are far from each other if they belong to two different sentences .    As a result, each word in the sentence is embedded into a vector representation which mainly carries structural information. Due to the -to- mapping of word types to structural labels, the word representation is deduced into structural representations. In other words, semantically different words  are mapped to similar structural labels ; hence, semantically different words may have similar structural representations. These pre-trained structural word representations can be used as complimentary information to their pre-trained semantic embeddings . We use probing tasks proposed by Conneau et al.  to investigate the linguistic features learned by such a training.  The results indicate that structural embeddings show competitive results compared to the semantic embeddings, and concatenation of structural embeddings with semantic embeddings achieves further improvement.  Finally, we investigate the efficiency of the learned structural embeddings of words for the domain of authorship attribution across four datasets. Our experimental results demonstrate classification improvements when structural embeddings are concatenated with the pre-trained word embeddings.  The remainder of this paper is organized as follows: we elaborate our proposed self-supervised framework in Section .  The details of the datasets and experimental configuration are provided and the experimental results reported in Section ; We review the related work in Section . Finally, we conclude this paper in Section .     
","   Syntactic structure of sentences in a document substantially informs about its authorial writing style. Sentence representation learning has been widely explored in recent years and it has been shown that it improves the generalization of different downstream tasks across many domains. Even though utilizing probing methods in several studies suggests that these learned contextual representations implicitly encode some amount of syntax, explicit syntactic information further improves the performance of deep neural models in the domain of authorship attribution. These observations have motivated us to investigate the explicit representation learning of syntactic structure of sentences.  In this paper, we propose a self-supervised framework for learning structural representations of sentences. The self-supervised network contains two components; a lexical sub-network and a syntactic sub-network which take the sequence of words and their corresponding structural labels as the input, respectively. Due to the $n$-to-$1$ mapping of words to their structural labels, each word will be embedded into a vector representation which mainly carries structural information. We evaluate the learned structural representations of sentences using different probing tasks, and subsequently utilize them in the authorship attribution task. Our experimental results indicate that the structural embeddings significantly improve the classification tasks when concatenated with the existing pre-trained word embeddings.",110
"    Since the end of the twentieth century and the spread of mobile communication technologies in the Arab world, youth, in particular, have developed a new chat alphabet to communicate more efficiently in informal Arabic. Because most media and applications initially did not enable chatting in Arabic, these Arab speakers resorted to what is now commonly known as ""Arabizi"". In, Arabizi was defined as the newly-emerged Arabic variant written using the Arabic numeral system and Roman script characters. With the widespread use of social media worldwide in more recent years, Arabizi emerged as an established Arabic writing system for mobile communication and social media in the Arab world.   Compared to the increasing studies of sentiment analysis in Indo-European languages, similar research for Arabic dialects is still very limited.\ This is mainly attributed to the lack of the needed good quality Modern Standard Arabic  publicly-available sentiment analysis resources in general, and more specifically dialectical Arabic publicly-available resources.\ Building such resources involves several difficulties in terms of data collection and annotation, especially for underrepresented Arabic dialects such as the Tunisian dialect. Nevertheless, existing Tunisian annotated datasets focused on code-switching datasets written using the Arabic or the Romanized Alphabet. The studies on these datasets applied off-the-shelf models that have been built for MSA on a dataset of Tunisian Arabic. An intuitive solution is to translate Tunisian Romanized Alphabet into Arabic Script. This approach suffers from the need for a parallel Tunisian-Arabic text corpus, the low average precision performances achieved and the irregularity of the words written.  Using a model trained on Modern Standard Arabic sentiment analysis data and then applying the same model on dialectal sentiment analysis data, does not produce good performances as shown in. This suggests that MSA models cannot be effective when applied to dialectical Arabic. There is, thus, a growing need for the creation of computational resources, not only for MSA but also for dialectical Arabic. The same situation holds when one tries to use computational resources used for a specific dialect of Arabic with another one.  To the best of our knowledge, this is the first study on sentiment analysis TUNIZI Romanized Alphabet. \ This could be deduced in the next sections where we will present TUNIZI and the state-of-the-art of Tunisian sentiment analysis followed by our proposed approach, results and discussion before conclusion and future work.   
"," Tunisians on social media tend to express themselves in their local dialect using Latin script . This raises an additional challenge to the process of exploring and recognizing online opinions. To date, very little work has addressed TUNIZI sentiment analysis due to scarce resources for training an automated system. In this paper, we focus on the Tunisian dialect sentiment analysis used on social media. Most of the previous work used machine learning techniques combined with handcrafted features. More recently, Deep Neural Networks were widely used for this task, especially for the English language. In this paper, we explore the importance of various unsupervised word representations  and we investigate the use of Convolutional Neural Networks and Bidirectional Long Short-Term Memory. Without using any kind of handcrafted features, our experimental results on two publicly available datasets showed  comparable performances to other languages.    \keywords{Tunisian Dialect  \and TUNIZI \and Sentiment Analysis \and Deep Learning \and Neural networks \and Natural language analysis.}",111
"   In recent years, neural networks have shown impressive performance gains on long-standing AI problems, such as natural language understanding, speech recognition, and computer vision.  Based on these successes, researchers have considered the application of neural nets to data management problems, including learning indices, query optimization and entity matching.  In applying neural nets to data management, research has so far assumed that the data was modeled by a database schema.    The success of neural networks in processing unstructured data such as natural language and images   raises the question of whether their use can be extended to a point where we can relax the fundamental assumption of database management, which is that the data we process is represented as fields of a pre-defined schema.  What if, instead, data and queries can be represented as short natural language sentences, and queries can be answered from these sentences?  This paper presents a first step in answering that question.  We describe \systemname, a database system in which updates and queries are given in natural language. The query processor of a \ndb\ builds on the primitives that are offered by the state of the art Natural Language Processing~ techniques.  Figure shows example facts and queries that \ndb\ can answer. %\ms{In Figure 1, queries 4&5 are not really joins, they just need language understanding/paraphrasing}  Realizing the vision of \systemname\ will offer several benefits that database systems have struggled to support for decades.  The first, and most important benefit is that a \ndb, by definition, has no pre-defined schema. Therefore, the scope of the database does not need to be defined in advance and any data that becomes relevant as the application is used can be stored and queried. The second benefit is that updates and queries can be posed in a variety of natural language forms, as is convenient to any user.  In contrast, a traditional database query needs to be based on the database schema.  A third benefit comes from the fact that the \ndb\  is based on a pre-trained language model that already contains a lot of knowledge.   For example, the fact that London is in the UK is already encoded in the language model. Hence, a query asking who lives in the UK can retrieve people who are known to live in London without having to explicitly specify an additional join. Furthermore, using the same paradigm, we can endow the \ndb\  with more domain knowledge by extending the pre-training corpus to that domain.   By nature, a \ndb\ is not meant to provide the same correctness guarantees of a traditional database system, i.e., that the answers returned for a query satisfy the precise binary semantics of the query language.  Hence, \ndb s should not be considered as an alternative to traditional databases in applications where such guarantees are required.    Given its benefits, \neuraldatabases\ are well suited for emerging applications where the schema of the data cannot be determined in advance and data can be stated in a wide range of linguistic patterns.  A family of such applications arise in the area of storing knowledge for personal assistants that currently available for home use and in the future will accompany Augmented Reality glasses. In these applications, users store data about their habits and experiences, their friends and their preferences, and designing a schema for such an application is impractical.  Another class of applications is the modeling and querying of political claims .  Here too, claims can be about a huge variety of topics and expressed in many ways.   Our first contribution is to show that state of the art transformer models can be adapted to answer simple natural language queries. Specifically, the models can process facts that are relevant to a query independent of their specific linguistic form, and combine multiple facts to yield correct answers, effectively performing a join. However, we identify two major limitations of these models:  they do not perform well on aggregation queries , and  since the input size to the transformer is bounded and the complexity of the transformer is quadratic in the size of its input, they only work on a relatively small collection of facts.  Our second contribution is to  propose an architecture for neural databases that uses the power of transformers at its core, but puts in place several other components in order to address the scalability and aggregation issues. Our architecture runs multiple instances of a Neural SPJ operator in parallel. The results of the operator are either the answer to the query or the input to an aggregation operator, which is done in a traditional fashion. Underlying this architecture is a novel algorithm for generating the small sets of database sentences that are fed to each Neural SPJ operator.  Finally, we describe an experimental study that validates the different components of \systemname s, namely the ability of the Neural SPJ to answer queries or create results for a subsequent aggregation operator even with minimal supervision, and our ability to produce support sets that are fed into each of the Neural SPJ operators. Putting all the components together, our   final result shows that we can accurately answer queries over thousands of sentences with very high accuracy. To run the experiments we had to create an experimental dataset with training data for \ndb s, which we make available for future research.    % and capable of generating intermediate results and  accurately predicting the aggregation operation to execute over these intermediate results.   
"," \jt{TODO Before final submission remove page numbers} In recent years, neural networks have shown impressive performance gains on long-standing AI problems, and in particular, answering queries from natural language text. These advances raise the question of whether they can be extended to a point where we can relax the fundamental assumption of database management, namely, that our data is represented as fields of a pre-defined schema.   This paper presents a first step in answering that question.  We describe \ndb, a database system with no pre-defined schema, in which updates and queries are given in natural language. We develop query processing techniques that build on the  primitives offered by the state of the art Natural Language Processing methods.   We begin by demonstrating that at the core, recent NLP transformers, powered by pre-trained language models, can answer select-project-join queries if they are given the exact set of relevant facts. However, they cannot scale to non-trivial databases and cannot perform aggregation queries. Based on these findings, we describe a \ndb\ architecture that runs multiple Neural SPJ operators in parallel, each with a set of database sentences that can produce one of the answers to the query. The result of these operators is fed to an aggregation operator if needed. We describe an algorithm that learns how to create the appropriate sets of facts to be fed into each of the Neural SPJ operators. Importantly, this algorithm can be trained by the Neural SPJ operator itself. We experimentally validate the accuracy of \systemname\ and its components, showing that we can answer queries over thousands of sentences with very high accuracy.",112
"    Automatic medical code assignment is a routine healthcare task for medical information management and clinical decision support. The International Classification of Diseases  coding system, maintained by the World Health Organization , is widely used among various coding systems.  Thus, the medical code assignment task is also called ICD coding. It uses clinical notes of discharge summaries to predict medical codes in a supervised manner with human-annotated codes, which is formulated as a multi-class multi-label text classification problem in the medical domain.    While there are increasing works in the community in automatic medical code assignment~, this task remains challenging from the perspectives of note representation and code prediction. First, medical note representation, a critical step in understanding medical notes, is formidably challenging due to the lengthy and complex semantic information in the discharge documents. There are typically thousands of tokens in a medical note due to the various diagnoses and procedures experienced by a patient. Furthermore, clinical notes also contain a vocabulary with many professional words and phrases, making it hard for a neural network model to encode and understand critical information. Second, the medical coding system has a very high and sparse dimensional label space, render the code prediction task incredibly difficult. For example, ICD9 and ICD10 coding systems have many labels, i.e., more than 14,000 and 68,000 codes. However, a patient typically is diagnosed with only a couple of codes over the whole coding space.     Early works for medical code assignment typically follow statistical approaches. They either employ rule-based methods  or apply classification methods such as SVM and Bayesian ridge regression  to assign the codes. These methods are shallow and do not exploit the complex semantic information in medical notes, leading to unsatisfactory performance. Recently, Natural language processing  techniques based on deep learning have been developed , which learn the note representation via convolutional neural networks. Specifically, CAML, MultiResCNN and DCAN treat ICD coding as a general text classification problem and develop complex neural encoders to learn the note representation. HyperCore proposes the hyperbolic embedding to capture code hierarchy and co-occurrence. However, these approaches are still ineffective, as they do not explicitly capture the fine-grained interactions between textual elements and medical codes. These interactions naturally represent the interdependencies between the complex medical words and associated codes, and thus should be well exploited.  This paper put forward a novel neural architecture,  Gated Convolutional Neural Network with Note-Code Interaction , for effective medical code assignment. Our goal is to learn rich representation from clinical notes and exploit the interactions between medical texts and clinical codes. To capture the long sequential history of clinical documents, we design a novel dilation information propagation component with a forgetting mechanism to selectively utilize the useful information for note representation learning. To tackle the large labeling space, we formulate textual notes and medical codes as a complete bipartite graph and develop a graph message passing approach to capture the explicit interaction between nodes and codes. The ICD code descriptions are used as an external medical knowledge source to learn more accurate code representations that preserve the semantic relations of the codes. Considering the practical application in real-world medical institutes, especially those with limited computing resources, our architecture also prioritizes computational efficiency when designing the sub-modules.  Our contributions are itemized as follows.    
"," Medical code assignment from clinical text is a fundamental task in clinical information system management. As medical notes are typically lengthy and the medical coding system's code space is large, this task is a long-standing challenge.  Recent work applies deep neural network models to encode the medical notes and assign medical codes to clinical documents. However, these methods are still ineffective as they do not fully encode and capture the lengthy and rich semantic information of medical notes nor explicitly exploit the interactions between the notes and codes. We propose a novel method, gated convolutional neural networks, and a note-code interaction , for automatic medical code assignment to overcome these challenges. Our methods capture the rich semantic information of the lengthy clinical text for better representation by utilizing embedding injection and gated information propagation in the medical note encoding module. With a novel note-code interaction design and a graph message passing mechanism, we explicitly capture the underlying dependency between notes and codes, enabling effective code prediction. A weight sharing scheme is further designed to decrease the number of trainable parameters. Empirical experiments on real-world clinical datasets show that our proposed model outperforms state-of-the-art models in most cases, and our model size is on par with light-weighted baselines.",113
" %  Enabling chatbots to indulge in engaging conversations requires massive datasets of human-human conversations . Training such dialog agents requires substantial time and effort expended in the collection of adequate number of high quality conversation samples.  \citet{hancock2019learning} alleviate this problem by introducing a self-feeding chatbot which can directly learn from user interactions. This chatbot requests users to provide natural language feedback when the users are dissatisfied with its response.  \citet{hancock2019learning} treat this feedback as a gold response to the wrong turn and use it as an additional training sample to improve the chatbot.    Although natural language feedback is cheap to collect from a chatbot's end-users, most often, feedback cannot be used directly as a training sample since feedback is usually not the answer itself, but simply contains hints to the answer. \Cref{tab:response_samples} shows some feedback text samples. Naive modification of feedback using heuristics like regular expressions would lead to generic responses that are ineffective in improving the dialog ability of chatbots . Additionally, writing an exhaustive set of regular expression rules is time consuming and requires extensive analysis of the data.  Annotating data to convert feedback text to natural response is also expensive and defeats the purpose of learning from feedback text.      \end{table} In this work, we propose a generative adversarial setup for converting such noisy feedback instances into natural, human-like responses that provide better training signals for the dialog agents. \Cref{fig:interface} gives a bird's-eye view of our problem. We frame this problem as a variant of text style transfer where the generator is tasked with making the feedback resemble the optimal response to the user's previous utterance and the discriminator is a classifier that distinguishes whether a given response is feedback or natural.   Our main contributions are the following: %   
","  The ubiquitous nature of chatbots and their interaction with users generate an enormous amount of data. Can we improve chatbots using this data? A self-feeding chatbot improves itself by asking natural language feedback when a user is dissatisfied with its response and uses this feedback as an additional training sample. However, user feedback in most cases contains extraneous sequences hindering their usefulness as a training sample. In this work, we propose a generative adversarial model that converts noisy feedback into a plausible natural response in a conversation. The generator's goal is to convert the feedback into a response that answers the user's previous utterance and to fool the discriminator which distinguishes feedback from  natural responses. We show that augmenting original training data with these modified feedback responses improves the original chatbot performance from 69.94\% to 75.96\% in ranking correct responses on the \personachat dataset, a large improvement given that the original model is already trained on 131k samples.\footnote{Our code is released at \url{https://github.com/ekunnii/adversarial-feedback-chatbot/}}",114
"  Text Generation is the task of producing written or spoken narrative from structured or unstructured data. The overarching goal is the seamless human-machine communication by presenting a wealth of data in a way we can comprehend. With respect to the modeling approaches, there are three main paradigms in generating text based on the schema of input and output:  Text-to-Text  Data-to-Text  None-to-Text. Table  presents the categorization of different tasks based on this paradigm. These several tasks deserve undivided attention and accordingly they have been heavily dissected, studied and surveyed in the recent past. For instance, independent and exclusive surveys are periodically conducted on summarization , knowledge to text generation {DBLP:conf/inlg/GardentSNP17, DBLP:conf/naacl/Koncel-Kedziorski19}, machine translation , dialog response generation , storytelling, narrative generation , image captioning  etc., to dig deeper into task specific approaches that are foundational as well as in the bleeding edge of research. While these are extremely necessary, often the focus on techniques that are beneficial to other tightly coupled tasks are overlooked. The goal of this survey is to focus on these key components that are task agnostic to improve the ensemble of tasks in neural text generation. %The rest of the survey is organized as follows: Section  describes the modeling approaches in text generation including the learning paradigms, pre-training and decoding strategies. This is followed by Section  describing the key challenges and solutions to the text generation such as fluency, length, content selection, speed etc.,. Section  describes evaluation and finally Section  presents the conclusions and the prospective future directions.   }   \end{table}     %https://www.sciencedirect.com/science/article/pii/S1319157820303360    There have been several studies conducted on surveying text generation. \citet{DBLP:journals/cai/PereraN17} present a detailed overview of information theory based approaches. \citet{iqbal2020survey} primarily focus on core modeling approaches, especially VAEs  and GANs . \citet{DBLP:journals/jair/GattK18} elaborated on tasks such as captioning, style trasfer etc., with a primary focus on data-to-text tasks. Controllability aspect is explored by \citet{prabhumoye2020exploring}. The workclosest to this is by \citet{DBLP:journals/corr/abs-1803-07133} who perform an empirical study on the core more modeling approaches only. In contrast to these, this paper focuses on task agnostic components and factors capable of pushing the ensemble of tasks forward. Figure  presents the various components and factors that are important to study in neural text generation which are elaborated in this paper. %Text generation is an overarching set of tasks where these underlying factors that cut across tasks are very critical in pushing the field forward and this paper is dedicated to be a one stop destination to learn these several fundamental factors.  
","   Neural text generation metamorphosed into several critical natural language applications ranging from text completion to free form narrative generation. Generating natural language has fundamentally been a human attribute and the advent of ubiquitous NLP applications and virtual agents marks the need to impart this skill to machines. There has been a colossal research effort in various frontiers of neural text generation including machine translation, summarization, image captioning, storytelling etc., We believe that this is an excellent juncture to retrospect on the directions of the field. Specifically, this paper surveys the fundamental factors and components relaying task agnostic impacts across various generation tasks such as storytelling, summarization, translation etc., In specific, we present an abstraction of the imperative techniques with respect to learning paradigms, pretraining, modeling approaches, decoding and the key challenges. Thereby, we hope to deliver a one-stop destination for researchers in the field to facilitate a perspective on where to situate their work and how it impacts other closely related tasks. %scope it : current neural techniques %for single and multi-sentence",115
"  The following instructions are directed to authors of papers submitted to EACL 2021 or accepted for publication in its proceedings. All authors are required to adhere to these specifications. Authors are required to provide a Portable Document Format  version of their papers. The proceedings are designed for printing on A4 paper.   
"," This document contains the instructions for preparing a manuscript for the proceedings of EACL 2021. The document itself conforms to its own specifications, and is therefore an example of what your manuscript should look like. These instructions should be used for both papers submitted for review and for final versions of accepted papers. Authors are asked to conform to all the directions reported in this document.",116
" Cross-lingual abstractive summarization is the task to generate a summary of a given document in a different target language. This task provides the overview of an article in a foreign language and thus helps readers understand a text written in an unfamiliar language quickly.   Early work on cross-lingual abstractive summarization adopted the pipeline approach: either translation of the given document into the target language followed by summarization of the translated document or summarization of the given document followed by translation of the summary into the target language. On the other hand, recent studies have applied a neural encoder-decoder model, which is widely used for natural language generation tasks including machine translation and monolingual abstractive summarization, to generate a summary in the target language from the given document directly. %Such direct generation approaches prevent the error propagation problems in pipeline methods. Such direct generation approaches prevent the error propagation in pipeline methods.  Training neural encoder-decoder models requires numerous sentence pairs. In fact,  provided 3.8M sentence-summary pairs to train their neural encoder-decoder model for English abstractive summarization, and the following studies used the same training data. However, constructing a large-scale cross-lingual abstractive summarization dataset is much more difficult than collecting monolingual summarization datasets because we require sentence-summary pairs in different languages. To address this issue, recent studies applied a machine translation model to monolingual sentence-summary pairs. They used the constructed pseudo dataset to train their neural encoder-decoder models.    Meanwhile, the possibility whether existing genuine parallel corpora such as translation pairs and monolingual abstractive summarization datasets can be utilized needs to be explored. In machine translation,  indicated that using translation pairs in multiple languages improved the performance of a neural machine translation model. Similarly, we consider that such existing genuine parallel corpora have a positive influence on the cross-lingual abstractive summarization task since the task is a combination of machine translation and summarization.   In this study, we propose a multi-task learning framework, Transum, which includes machine translation, monolingual abstractive summarization, and cross-lingual abstractive summarization, for neural encoder-decoder models. The proposed method controls the target task with a special token which is inspired by Google's multilingual neural machine translation system. For example, we attach the special token  to the beginning of the source-side input sentence in translation.   The proposed Transum is quite simple because it does not require any additional architecture in contrast to  but effective in cross-lingual abstractive summarization. Experimental results show that Transum improves the performance of cross-lingual abstractive summarization and outperforms previous methods in Chinese-English and Arabic-English summarization. In addition, Transum significantly improves machine translation performance compared to that obtained using only a genuine parallel corpus for machine translation.   Furthermore, we construct a new test set to simulate more realistic situations: cross-lingual summarization with several length constraints. In a summarization process, it is important to generate a summary of a desired length. However, existing test sets for cross-lingual abstractive summarization cannot evaluate whether each model controls output lengths because the test sets do not contain summaries with multiple lengths. Thus, we translate an existing monolingual abstractive summarization that contains summaries with multiple lengths to construct the new test set.    The contributions of this study are as follows:   
"," We present a multi-task learning framework for cross-lingual abstractive summarization to augment training data. Recent studies constructed pseudo cross-lingual abstractive summarization data to train their neural encoder-decoders. Meanwhile, we introduce existing genuine data such as translation pairs and monolingual abstractive summarization data into training. Our proposed method, Transum, attaches a special token to the beginning of the input sentence to indicate the target task. The special token enables us to incorporate the genuine data into the training data easily. The experimental results show that Transum achieves better performance than the model trained with only pseudo cross-lingual summarization data. In addition, we achieve the top ROUGE score on Chinese-English and Arabic-English abstractive summarization. Moreover, Transum also has a positive effect on machine translation. Experimental results indicate that Transum improves the performance from the strong baseline, Transformer, in Chinese-English, Arabic-English, and English-Japanese translation datasets.",117
" Table-to-text generation is an important task for text generation from structured data. It aims at automatically producing descriptive natural language text that covers the salient information in table to help people to get the salient information of the tables. Practical applications can be found in domains such as weather forecasts, biography generation, NBA news generation, etc.  Over the pass several years, neural text generation methods have made significant progress on this task. \citeauthor{lebret-etal-2016-neural,wiseman-etal-2017-challenges,bao2018table} model it as a machine translation task and view the input table a record sequence. To generate text that contains more salient and well-organized facts,  \citeauthor{sha2018order,puduppully-etal-2019-data,moryossef-etal-2019-step,trisedya2020sentence,ijcai2020-522} explicitly model content selection and planning. %Some works also introduce extra knowledge  or pre-executed symbolic operations on table  to improve the result. To learning better representation for tables,  \citeauthor{liu2018table,bao2018table,nema-etal-2018-generating,jain-etal-2018-mixed,gong-etal-2019-table} explicitly model the structure of table from multiple levels or different dimensions. In addition, \citeauthor{liu2019hierarchical} propose three auxiliary supervision tasks to capture accurate semantic representation of the table.    However, some issues have been overlooked. First, many tables ) contain a large number of numerical records. For instance,  of records and almost  of column types are numeric in ROTOWIR , a benchmark of NBA basketball games. Current methods treat these records as words in natural language text and ignore the characteristics of the number itself which play an important role in table representation, such as size attribute. In addition, there are noises in human-written summaries in dataset. These noises include redundant information and records that do not exist in the input tables ). These noises may cause incorrect alignments between input tables and target text or wrong supervision signals. And they can affect the performance of models based on content selection and planning or auxiliary supervision. %In addition, when human are writing a summary to describe the given table, they may consider the most salient records. For example, when describing the table in Figure  , they may pay more attention to K. Leonard, because he is the top scorer.   To solve above problems, we explore the use of the information contained in the tables and introduce two self-supervised tasks to learn better representation for tables. We argue that the better representation of tables can help the model to capture and organize the important facts, even without explicitly modeling content selection and planning. Specially, we improve ~\citeauthor{gong-etal-2019-table}'s method and employ a hierarchical table encoder to model the table structure from record level and row level. The record-level encoder utilizes two cascaded self-attention models to encode the table from column and row dimension, respectively. And then, we introduce a row-level fusion gate to obtain the row-level representation for each row. To learn a number-aware record representation, we introduce a Number Ordering  task. This task utilizes a pointer network to generate a descending record sequence for each column in table, according to their content. Figure   shows a number ordering example for column PTS. To the best of our knowledge, this is the first work on neural table-to-text generation via focusing on learning representation for number in table. Another self-supervised task, Significance Ordering , is further proposed to learn a significance-aware representation for the record. The significance denotes the relative relation between records in same row. This is inspired by the intuition that when humans describe the performance of a player, they tend to focus on his more salient records. For example, in Figure , K. Thompson's scores  is more likely to be described than his other's records. The SO task executes a descending sort operation on each row according to the significance scores of records. We use the position index of record  to measure its importance and the smaller the significance score, the more important the record is. The position index of record is obtained by the results of Number Ordering. For example, in Figure  , K. Thompson scores  points which are the largest in PTS, so the significance score of this record is 1. The proposed two tasks are trained together with the table2text generation model and they share the same encoder parameters. Obviously, the two proposed tasks are self-supervised and the training labels are easily obtained from the input tables. Therefore, the errors caused by noises in training set are avoided. %For record in same row, it includes another size information:significance. It denotes the relative relation between records in same row. To learn a significance-aware representation for table, we propose a Significance Ordering task which executes a ascending sort operation on each row according to the significance of records. We use the position index of record  to measure its importance and the smaller the significance score, the more important the record is. The position index of record is obtained by the results of Number Ordering. For example, in Figure  , K. Leonard score 45 points which are the largest in PTS, so the significance score of this record is 1). Obviously, the two proposed tasks are self-supervised and the training labels are easily obtained from the input tables. Therefore, the errors caused by noise in training set are avoided.   We conducted experiments on ROTOWIRE to verify the effectiveness of the proposed approach. The experimental results demonstrate that, even without explicitly modeling content selection or introducing extra knowledge, our method can help to generate text that contains more salient and well-organized facts. And we achieve the state-of-the-art performance on automatic metrics. %Content Selection , Content Ordering  and BLEU.   
"," Table-to-text generation aims at automatically generating natural text to help people to conveniently obtain the important information in tables. Although neural models for table-to-text have achieved remarkable progress, some problems still overlooked. The first is that the values recorded in many tables are mostly numbers in practice. The existing approaches do not do special treatment for these, and still regard these as words in natural language text.  Secondly, the target texts in training dataset may contain redundant information or facts do not exist in the input tables. These may give wrong supervision signals to some methods based on content selection and planning and auxiliary supervision. To solve these problems, we propose two self-supervised tasks, Number Ordering and Significance Ordering,  to help to learn better table representation. The former works on the column dimension to help to incorporate the size property of numbers into table representation. The latter acts on row dimension and help to learn a significance-aware table representation. We test our methods on the widely used dataset ROTOWIRE which consists of NBA game statistic and related news. The experimental results demonstrate that the model trained together with these two self-supervised tasks can generate text that contains more salient and well-organized facts, even without modeling context selection and planning. And we achieve the state-of-the-art performance on automatic metrics. % Content Selection , Content Ordering  and BLEU.",118
"/} Automatic keyphrase generation is the task of generating single or multi-word lexical units that provides readers with high level information about the key ideas or important topics described in a given source text. Apart from an information summarization perspective, this task has applications in various downstream natural language processing tasks such as text classification , document clustering  and information retrieval .   Traditionally, keyphrases  were extracted from source documents by retrieving and ranking a set of candidate phrases through rule based approaches. With recent advances in neural natural language generation and availability of larger training corpora, this problem is formulated under a sequence-to-sequence  modelling framework . This approach has an advantage that it can generate new and meaningful keyphrases which may be absent in the source text. The earliest work in this direction was by , who train a S2S model to generate one keyphrase at a time. At inference time, they decode with beam sizes as high as 200, to generate a large number of KPs and finally de-duplicate the outputs. However, this is computationally expensive and wasteful because only  of such KPs were found to be unique .   An alternative approach is to train a S2S model to generate multiple keyphrases in a sequential manner, where the output KPs are separated by a pre-defined delimiter token. This method has an added benefit that the model automatically learns to generate a variable number of keyphrases depending on the input, instead of a user-specified fixed number of keyphrases  from a large list of candidate outputs. However, some previous approaches  still use exhaustive beam search decoding to over-generate KPs and then apply post-processing to remove repetitions. Apart from the additional computational requirements, we argue that this method of  avoiding information redundancy is a last-minute solution. % `hacky' solution.   % \todoi{Importance of diversity}  In this paper, we take a principled direction towards addressing the information redundancy issue in keyphrase generation models. We propose to tackle this problem directly during the training stage, rather than applying adhoc post-processing at inference time. Specifically, we adopt the neural unlikelihood training  objective , whereby the decoder is penalized for generating undesirable tokens. % , which in our case corresponds to the set of repeating tokens.  introduce unlikelihood training for a language model setting. Since we work with a S2S setup, our version of UL loss consists of two components:  a target token level UL loss based on the target vocabulary to penalize the model for generating repeating tokens;  a copy token level UL loss based on the dynamic vocabulary of source tokens required for copy mechanism , which penalizes the model for copying repetitive tokens.   S2S models trained with maximum likelihood estimation  are usually tasked with the next token prediction objective. However, this does not necessarily incentivize the model to plan for future token prediction ahead of time. We observe such lack of model planning capability in our initial experiments with MLE models and to overcome this issue we propose to use -step ahead token prediction. This modified training objective encourages the model to learn to correctly predict not just the current token, but also tokens upto -steps ahead in the future. We then naturally incorporate UL training on the -step ahead token prediction task.  We summarize our contributions as follows:  To improve the diversity of generated keyphrases in a principled manner during training, we adopt the unlikelihood objective for the S2S setting and propose a novel copy token unlikelihood loss.  In order to incentivize model planning, we augment our training objective function to incorporate -step ahead token prediction. Additionally, we also introduce the -step ahead unlikelihood losses.  We propose new metrics for benchmarking keyphrase generation models on diversity criterion. We carry out experiments on datasets from three different domains  and validate the effectiveness of our approach.  We observe substantial gains in diversity while maintaining competitive output quality.           }.} \end{table}  
"," In this paper, we study sequence-to-sequence  keyphrase generation models from the perspective of diversity. Recent advances in neural natural language generation have made possible remarkable progress on the task of keyphrase generation, demonstrated through improvements on quality metrics such as $F_1$-score. However, the importance of diversity in keyphrase generation has been largely ignored. We first analyze the extent of information redundancy present in the outputs generated by a baseline model trained using maximum likelihood estimation . Our findings show that repetition of keyphrases is a major issue with MLE training. To alleviate this issue, we adopt neural unlikelihood  objective for training the S2S model. Our version of UL training operates at  the target token level to discourage the generation of repeating tokens;  the copy token level to avoid copying repetitive tokens from the source text. Further, to encourage better model planning during the decoding process, we incorporate $K$-step ahead token prediction objective that computes both MLE and UL losses on future tokens as well. Through extensive experiments on datasets from three different domains we demonstrate that the proposed approach attains considerably large diversity gains, while maintaining competitive output quality.\footnote{Code is available at \url{https://github.com/BorealisAI/keyphrase-generation}}",119
" In healthcare, real-world data  refers to patient data routinely collected during clinic visits, hospitalization, as well as patient-reported results. In recent years, RWD's volume has become enormous, and invaluable insights and real-world evidence can be generated from these datasets using the latest data processing and analytical techniques. However, RWD's quality remains one of the main challenges that prevent novel machine learning methods from being readily adopted in healthcare.  Therefore, creating data quality tools is of great importance in health care and health data sciences.  Erroneous data in healthcare systems could jeopardize a patient's clinical outcomes and affect the care provider's ability to optimize its performance.     Common data quality issues include missing critical information about medical history, wrong coding of a condition, and inconsistency in documentation across different care sites. Manual review by domain experts is the gold standard for achieving the highest data quality but is unattainable in regular care practices. Recent developments in the field of Natural Language Processing  has attracted great interest in the healthcare community since algorithms for identifying variables of interest and classification algorithm for diseases  have been recently developed .  In this paper, we presented a novel model for the extraction of queries  in a corpus of dialogue between data entry clinicians and expert reviewers in a multi-site dialysis environment.   %The work's ultimate goal is to identify the data elements that caused most uncertainty or errors during the documentation process.  The main contributions of this work are:  Finally, in addition to evaluating our model's performance in a medical context, we also experimented in section  with a general-domain dataset  to show our model's generalizability.  The rest of the paper is organized as follows. Related work is presented in section . The different question detection methods that will be examined,   are described in section . Section  details the characteristics of the proposed multi-channel CNN model. Finally, the results of the experiments are reported in section  and a conclusion and a plan for future work are given in section .  
"," In most clinical practice settings, there is no rigorous reviewing of the clinical documentation, resulting in inaccurate information captured in the patient medical records. The gold standard in clinical data capturing is achieved via ``expert-review"", where clinicians can have a dialogue with a domain expert  and ask them questions about data entry rules. Automatically identifying ``real questions"" in these dialogues could uncover ambiguities or common problems in data capturing in a given clinical setting.  In this study, we proposed a novel multi-channel deep convolutional neural network architecture, namely Quest-CNN, for the purpose of separating real questions that expect  an answer  about an issue from sentences that are not questions, as well as from questions referring to an issue mentioned in a nearby sentence , which we will refer as ``c-questions"". We conducted a comprehensive performance comparison analysis of the proposed multi-channel deep convolutional neural network against other deep  neural networks. Furthermore, we evaluated the performance of traditional rule-based and learning-based methods for detecting question sentences. The proposed Quest-CNN achieved the best F1 score both on a dataset of data entry-review dialogue in a dialysis care setting, and on a general domain dataset.",120
"  Semantic parsing is the task of mapping a natural language query into a formal language, that is extensively used in goal-oriented dialogue systems. For a given query, such model should identify the requested action  and the associated values specifying parameters of the action . For example, if the query is Call Mary the action is call and the value of slot contact is Mary.  The number of different intents and slots in publicly available datasets  can be close to a hundred and it may be the orders of magnitude larger in real-world systems. Such a big number of classes usually causes a long tail in the class frequency distribution . These tail classes can be significantly improved with small quantities of additional labeled data.    However, training a neural semantic parsing model from scratch can take hours even on a relatively small public dataset . The real-world datasets can contain millions of examples  which can change the time scale to weeks. % Need to describe the problem and motivation to production settings more.  In this work, we propose to fine-tune a model that has already been trained on the old dataset  instead of training a new model to significantly speed up the incorporation of a new portion of data. We call this setting Incremental training, as the new portions of data can be added incrementally.  We focus on semantic parsing % and seq2seq networks for our case studies for the following reasons. Semantic parsing is a more complex NLP task compared to classification or NER and we hope that the lessons learned here would be more widely applicable. Task-oriented semantic parsing tend to have a large output vocabulary that can be frequently updated, and thus, benefit most from the Incremental setting. % We choose seq2seq networks for this work due to two reasons: first, seq2seq networks are very % general and can be easily adapted to simpler tasks like NER; % second, seq2seq models perform really well on popular natural language understanding datasets like TOP and SNIPS.  % Exploring this space of possible solutions, we compare the effectiveness of these approaches with each other and come up with a set of guidelines that are useful for incremental training tasks as well.  % To emulate the ""data-patch"" scenario, we split these datasets by focusing on a few classes. We show that naive fine-tuning leads to catastrophic forgetting and come up with approaches to remedy this. We observe that it is possible to fine-tune models to new classes in a few minutes compared to hours when retraining from scratch. We also compare the effect of pre-trained representations like BERT on fine-tuning. Using these observations we come up with fine-tuning guidelines in scenarios where the label space does not change. We verify that our approaches work on 2 popular semantic parsing datasets: TOP and SNIPS under different data splits.  The main contributions of this work are:    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % Related work %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
"," A semantic parsing model is crucial to natural language processing applications such as goal-oriented dialogue systems. Such models can have hundreds of classes with a highly non-uniform distribution. In this work, we show how to efficiently  improve model performance given a new portion of labeled data for a specific low-resource class or a set of classes. We demonstrate that a simple approach with a specific fine-tuning procedure for the old model can reduce the computational costs by ~90\% compared to the training of a new model. The resulting performance is on-par with a model trained from scratch on a full dataset. We showcase the efficacy of our approach on two popular semantic parsing datasets, Facebook TOP, and SNIPS.",121
"  Recent progress in abstractive summarization has been fueled by the advent of large-scale Transformers pre-trained on autoregressive language modeling objectives . Despite their strong performance on automatic metrics like ROUGE , abstractive models are not as straightforward and interpretable as their extractive counterparts. Free-form generation in these models also leads to serious downstream errors, such as factual inconsistencies with the input document . Although the interpretability of NLU models has been extensively studied , summarization models specifically have not received similar attention, with analysis efforts often focused on datasets and evaluation .  %Generic explanation methods for language models  or neural machine translation models  are not entirely applicable, as summarization models typically have different interactions with the input document.  In this work, we focus on interpreting and understanding abstractive summarization models through the lens of decoder uncertainty, or the entropy of decisions during generation. While uncertainty in generation has been studied from the perspective of data , sampling , and training  , it is underutilized as a technique for analysis and inspection of generation systems. We study two prominent summarization models, PEGASUS  and BART , fine-tuned on two English summarization datasets, CNN/Daily Mail  and XSum , to understand model behavior in each setting. %We analyze the model using both blackbox and whitebox perspectives.  First, by comparing -grams between the input document and generated summaries, we establish two coarse types for decoded tokens, copy and generate . We find that the entropy of the generation decision correlates with whether the model is copying or generating, as well as where in the sentence the token is. This paints a picture of certain contexts being more restrictive from the standpoint of generation, particularly early in sentences where a model has not ``decided'' what to copy yet, and illustrates the interaction of content selection and lexical choice. %Furthermore, it illustrates the interaction of content selection and lexical choice: new bigrams are higher entropy, but beginnings of sentences are also high entropy, indicating that the model has some uncertainty about what sentence to discuss, even if it is going to copy. Second, we extend this analysis by looking at how uncertainty relates to the syntax of the generated sentence: whether uncertainty connects to syntactic notions of surprisal  and how the entropy varies across certain syntactic productions. % Finally, we derive a way to quantify decoder attention by aggregating self-attention heads, and investigating the correspondence between the prediction entropy and the fraction of the decoded tokens in the aggregated attention.\todo{change this sent to refer to entropy more} Finally, we derive a way to quantify decoder attention by aggregating distinct self-attention heads, revealing the correlation between the attention entropy and prediction entropy, and investigating the correspondence between the prediction entropy and the fraction of the past and future decoded tokens. % highly attentive positions and decoded or not-yet-decoded tokens with respect to specific Transformer layers in the decoder.  Taking this analysis together, we find that the abstractiveness of reference summaries fundamentally changes model behavior: the extractive nature of CNN/DM makes most of its decisions low entropy and copy-oriented while the model maintains higher uncertainty on XSum, yielding more abstractive summaries. More broadly, we show that uncertainty is a simple but effective tool to characterize decoder behavior in text generation. %  By analyzing decoder self-attention layers, we find that when the attention only focuses on a few tokens, the prediction entropy will be fairly low and the focused tokens are very likely to be predicted.   
"," % An advantage of seq2seq abstractive summarization models is that they generate text in a free-form manner, but this inherent flexibility makes it difficult to interpret and understand model behavior. In this work, we adopt a data-driven methodology to unpack decoder behavior in both a blackbox and whitebox way. We fine-tune and analyze a GPT-2 \cite{radford-2019-gpt2} model on two benchmark datasets featuring different levels of abstraction. Our experiments yield three key results. First, by analyzing the entropy of model predictions and its corresponding test-time behavior, we find a strong correlation between low entropy and where the model copies document spans rather than generating novel text. Second, this entropy analysis can allow us to understand what sentence positions and even what syntactic configurations are associated with copying existing content. Finally, by analyzing decoder self-attention patterns, we can trace this copying behavior to a particular pattern of attending to immediate decoder context and finding the next token to generate in the source document. An advantage of seq2seq abstractive summarization models is that they generate text in a free-form manner, but this flexibility makes it difficult to interpret model behavior.  In this work, we analyze summarization decoders in both blackbox and whitebox ways by studying on the entropy, or uncertainty, of the model's token-level predictions. For two strong pre-trained models, PEGASUS \cite{pegasus} and BART \cite{lewis-2019-bart} on two summarization datasets, we find a strong correlation between low prediction entropy and where the model copies tokens rather than generating novel text. The decoder's uncertainty also connects to factors like sentence position and syntactic distance between adjacent pairs of tokens, giving a sense of what factors make a context particularly selective for the model's next output token. Finally, we study the relationship of decoder uncertainty and attention behavior to understand how attention gives rise to these observed effects in the model. We show that uncertainty is a useful perspective for analyzing summarization and text generation models more broadly.\footnote{Code is available at \url{https://github.com/jiacheng-xu/text-sum-uncertainty}} % can trace this copying behavior to a particular pattern of attending to immediate decoder context and finding the next token to generate in the source document.",122
"  Neural attention mechanisms have been widely applied in  computer vision and have been shown to enable neural networks to only focus on those aspects of their input that are important for a given task. While neural networks are able to learn meaningful attention mechanisms using only supervision received for the target task, the addition of human gaze information has been shown to be beneficial in many cases. An especially interesting way of leveraging gaze information was demonstrated by works incorporating human gaze into neural attention mechanisms, for example for image and video captioning or visual question answering.  While attention is at least as important for reading text as it is for viewing images, integration of human gaze into neural attention mechanisms for natural language processing  tasks remains under-explored. A major obstacle to studying such integration is data scarcity: Existing corpora of human gaze during reading consist of too few samples to provide effective supervision for modern data-intensive architectures and human gaze data is only available for a small number of NLP tasks. For paraphrase generation and sentence compression, which play an important role for tasks such as reading comprehension systems, no human gaze data is available.  We address this data scarcity in two novel ways: First, to overcome the low number of human gaze samples for reading, we propose a novel hybrid text saliency model  in which we combine a cognitive model of reading behavior with human gaze supervision in a single machine learning framework. More specifically, we use the E-Z Reader model of attention allocation during reading to obtain a large number of synthetic training examples. We use these examples to pre-train a BiLSTM network with a Transformer whose weights we subsequently refine by training on only a small amount of human gaze data. We demonstrate that our model yields predictions that are well-correlated with human gaze on out-of-domain data. Second, we propose a novel joint modeling approach of attention and comprehension that allows human gaze predictions to be flexibly adapted to different NLP tasks by integrating TSM predictions into an attention layer. By jointly training the TSM with a task-specific network, the saliency predictions are adapted to this upstream task without the need for explicit supervision using real gaze data. Using this approach, we outperform the state of the art in paraphrase generation on the Quora Question Pairs corpus by more than 10\% in BLEU-4 and achieve state of the art performance on the Google Sentence Compression corpus. As such, our work demonstrates the significant potential of combining cognitive and data-driven models and establishes a general principle for flexible gaze integration into NLP that has the potential to also benefit tasks beyond paraphrase generation and sentence compression.  
"," A lack of corpora has so far limited advances in integrating human gaze data as a supervisory signal in neural attention mechanisms for natural language processing . We propose a novel hybrid text saliency model  that, for the first time, combines a cognitive model of reading with explicit human gaze supervision in a single machine learning framework. On four different corpora we demonstrate that our hybrid TSM duration predictions are highly correlated with human gaze ground truth. We further propose a novel joint modeling approach to integrate TSM predictions into the attention layer of a network designed for a specific upstream NLP task without the need for any task-specific human gaze data. We demonstrate that our joint model outperforms the state of the art in paraphrase generation on the Quora Question Pairs corpus by more than 10\% in BLEU-4 and achieves state of the art performance for sentence compression on the challenging Google Sentence Compression corpus. As such, our work introduces a practical approach for bridging between data-driven and cognitive models and demonstrates a new way to integrate human gaze-guided neural attention into NLP tasks.",123
" %\hh{check the fuzziness: pre-trained or pretrained and decide which one to use .} Modern techniques for text summarization generally can be categorized as either extractive methods, which identify the most suitable %\pfliu{How about ``which identify the most suitable semantic units ''}  words or sentences from the input document and concatenate them to form a summary, or abstractive methods, which generate summaries freely and are able to produce novel words and sentences. Compared with extractive algorithms, abstractive algorithms are more flexible, making them more likely to produce fluent and coherent summaries. %\pfliu{better if adding some references here}  %and the generation process is more human-like \gn{Re ``more human-like''. First, I'm not sure if this is actually true: humans copy-paste text as well. Second, it doesn't seem really important here. Maybe you could just expand on the ``more flexible'' part and mention the practical advantages of this.}. However, the unconstrained nature of abstractive summarization can also result in problems. First, it can result in unfaithful summaries, containing factual errors as well as hallucinated content. Second, it can be difficult to control the content of summaries; it is hard to pick in advance which aspects of the original content an abstractive system may touch upon. %\pfliu{I'm thinking about if it's suitable to place the following paragraph here .  Will it be better if we exchange it with ``There have been some ...'' this paragraph and make corresponding modification.} To address the issues, we propose methods for guided neural abstractive summarization: methods that provide various types of guidance signals that 1) constrain the summary so that the output content will deviate less from the source document; 2) allow for controllability through provision of user-specified inputs.             % Table generated by Excel2LaTeX from sheet 'Sheet1' \iffalse %   %      \end{table*}%  \fi      \iffalse  %   '' and ``{cover.}'' represent the copy and coverage mechanism respectively. Guidance represents different guided information while Guiding Method denotes how to introduce the guided information. ``ourGuidance'' contains sentences, relations keywords and retrieved summaries. ``Marker Embedding'' suggests that the guided information is introduced by embedding it as a feature vector.}% \gn{add  for completeness. Make sure it's in chronological order. I don't think BART needs to be included, but you might also include other methods that provide guidance on, for example, the style of the output .}} %\zj{Is there any particular reason to make ``copy'' and ``cover.'' italic?}.}   % \end{table*}%  \fi  %      %'' and ``{cover.}'' represent the copy and coverage mechanism respectively. Guidance represents different guided information while Guiding Method denotes how to introduce the guided information. ``ourGuidance'' contains sentences, relations keywords and retrieved summaries. ``Marker Embedding'' suggests that the guided information is introduced by embedding it as a feature vector.}% \gn{add  for completeness. Make sure it's in chronological order. I don't think BART needs to be included, but you might also include other methods that provide guidance on, for example, the style of the output .}} %\zj{Is there any particular reason to make ``copy'' and ``cover.'' italic?}.}   % \end{table*}%  %\gn{The term ``hybrid summarization models'' is sudden, and it doesn't follow clearly from the last sentence in the previous paragraph. I think the point of this paragraph is ``we are not the first to propose guided neural summarization models, but previous methods were limited to only a particular type of guidance''. If so, then you can say the ``we are not the first'' part at the beginning of this paragraph, and the ``limited'' part at the final part of the paragraph.} There have been some previous methods for guiding neural abstractive summarization models. For example,~\citet{kikuchi-etal-2016-controlling} specify the length of abstractive summaries,~\citet{li2018guiding} provide models with keywords to prevent the model from missing key information, and ~\citet{cao2018retrieve} propose models that retrieve and reference relevant summaries from the training set. %, and~\citet{gehrmann2018bottom} propose to train a model to identify salient words and encourage the final model to faithfully copy them from the source. While these methods have demonstrated improvements in summarization quality and controllability, each focuses on one particular type of guidance -- it remains unclear which is better and whether they are complementary to each other. %In addition, most of the previous work whether they are compatible with pre-trained language models such as BERT. %Previously, in order to address the issues of abstractive summarization models, researchers have proposed hybrid summarization models that combine the merits of extractive and abstractive methods. %\gn{In the following three sentences, it is not explicitly stated or clear how these methods address the issues of abstractive summarization models.} %For example,~\citet{gu2016incorporating} propose methods to copy words from the source document.~\citet{gehrmann2018bottom} utilize bottom-up attention to constrain the decoder to attend to salient parts of the inputs. %Similarly, %While these approaches can achieve good performance in terms of ROUGE, we cannot guarantee the models learn to identify the salient segments correctly or control the summaries due to the lack of explicit supervision signals  %\gn{can your model guarantee this? if you're putting it as a downside here it seems that it should be something that does not apply to your model.} \zd{I think our model does not try to learn to identify the salient part. Instead, we explicitly provide the salient part to the model so that the model learns to rely on this input.} \gn{But the extractive summarization model may fail at test time, right?} \zd{right, but i think that's the problem of extractive summarization, and the goal of our model is to learn to depend on the input, no matter whether the input signal is correct or not. } \gn{See my comment below. I think that there's a problem of a disconnect between how you're presenting the method , and what we're actually doing in experiments. It'd be best if you can write the story in the way that encompasses the things in experiments . Could you think of a way to reframe the intro a little bit in this direction? I think one thing you can definitely say about your method is that it can use a wide variety of different types of guidance, including that from automatic up-stream systems, or perhaps user-specified keywords etc. You are using a method to encourage the model to pay close attention to this guidance . This is very empirically effective. I'll take a look once you've thought about this a bit and modified the intro accordingly. Additionally, you might want to add a sentence to the end of the first paragraph describing what you attempt to achieve in this paper before jumping into the previous work. This will help make the contrasts more clear in this paragraph.} \zd{Thanks a lot! I'll think more about this and change the paper accordingly!}. %To improve the controllability of summarization models, previous works have attempted to provide models with keywords or length information, but the choices of guidance are limited and thus the controllability of the output summaries is hindered \gn{Again, here it's not super-clear how or why your proposed method is better in these aspects}.   %\gn{I think this is OK, but could really benefit from a figure at the top-right of page 1 demonstrating the behavior.} %To obtain abstractive summarization models with good performance as well as flexible controllability, In this paper, we propose a general and extensible guided summarization framework that can take different kinds of external guidance as input. %\gn{Maybe one more sentence on how the framework works.} Like most recent summarization models, our model is based on neural encoder-decoders, instantiated with contextualized pretrained language models, including BERT and BART. With this as strong starting point, we make modifications allowing the model to attend to both the source documents and the guidance signals when generating outputs. %\gn{A little more concreteness here could help, even just saying ``attends to sequences representing both the source document and the guidance signal''.} %\gn{I would put the next two sentences in the method description above, before we discuss the specific types of guidance we provide.} As shown in Figure, we can provide automatically extracted or user-specified guidance to the model during test time to constrain the model output. At training time, to encourage the model to pay close attention to the guidance, %\pfliu{Since oracle-based training method is a  contribution of this work, it would be better if we can express this more explicitly. For example: ``we propose to use ...instead of ..''} we propose to use an oracle to select informative guidance signals -- a simple modification that nonetheless proved essential in effective learning of our guided summarization models.  %\gn{How is this different than ? This sentence seems to say the same thing as the second-to-last sentence of the previous paragraph. I understand that ``extensible'' may be attempting to make a contrast, but it's not very clear.}. Using this framework, we investigate four types of guidance signals:  highlighted sentences in the source document,  keywords,  salient relational triples in the form of , and  retrieved summaries. %\zj{Just a minor point. Maybe better to make the orders here consistent with the experiment section .}   We evaluate our methods on 6 popular summarization benchmarks. Our best model, using highlighted sentences as guidance, can achieve state-of-the-art performance on 4 out of the 6 datasets, including 1.28/0.79/1.13 ROUGE-1/2/L improvements over previous state-of-the-art model on the widely-used CNN/DM dataset. In addition, we perform in-depth analyses of different guidance signals and demonstrate that they are complementary to each other in that we can aggregate their outputs together and obtain further improvements. An analysis of the results also reveals that our guided models can generate more faithful summaries and more novel words. Finally, we demonstrate that we can control the output by providing user-specified guidance signals, with different provided signals resulting in qualitatively different summaries.  %\pfliu{Do we need to highlight our contributions?} %We first evaluate our methods on the widely-used CNN/DailyMail benchmark and perform in-depth analysis of different guidance signals. Experimental results demonstrate that our best method can achieve 1.13 ROUGE-L improvements over the state-of-the-art model. We then pick the best guidance signal and evaluate our models on the other five popular summarization benchmarks. Extensive experiments demonstrate the effectiveness of our model on extractive datasets and analyses reveal that our methods can generate more novel words and more faithful summaries. In addition, we can control the output by providing user-specified guidance signals.   
"," Neural abstractive summarization models are flexible and can produce coherent summaries, but they are sometimes unfaithful and can be difficult to control. While previous studies attempt to provide different types of guidance to control the output and increase faithfulness, it is not clear how these strategies compare and contrast to each other. In this paper, we propose a  general and extensible guided summarization framework  that can effectively take different kinds of external guidance as input, and we perform experiments across several different varieties. Experiments demonstrate that this model is effective, achieving state-of-the-art performance according to ROUGE on 4 popular summarization datasets when using highlighted sentences as guidance. In addition, we show that our guided model can generate more faithful summaries and demonstrate how different types of guidance generate qualitatively different summaries, lending a degree of controllability to the learned models.\footnote{Code is available at \url{https://github.com/neulab/guided_summarization}.}%, generating more novel words, and generating more faithful summaries on 4 popular summarization datasets \gn{``when using XXX as guidance''}. In addition, we demonstrate how different types of guidance generate qualitatively different summaries, lending a degree of controllability to the learned models.",124
" In recent years, abstractive summarization  has made impressive progress with the development of sequence-to-sequence  framework . This framework is composed by an encoder and a decoder. The encoder processes the source text and extracts the necessary information for the decoder, which then predicts each word in the summary. Thanks to their generative nature, abstractive summaries can include novel expressions never seen in the source text, but at the same time, abstractive summaries are more difficult to produce compared with extractive summaries  which formed by directly selecting a subset of the source text. It has been also found that seq2seq-based abstractive methods usually struggle to generate out-of-vocabulary  words or rare words, even if those words can be found in the source text. Copy mechanism  can alleviate this problem and meanwhile maintain the expressive power of the seq2seq framework. The idea is to allow the decoder not only to generate a summary from scratch but also copy words from the source text.  Though effective in English text summarization, the copy mechanism remains relatively undeveloped in the summarization of some East Asian languages e.g. Chinese. Generally speaking, abstractive methods for Chinese text summarization comes in two varieties, being word-based and character-based. Since there is no explicit delimiter in Chinese sentence to indicate word boundary, the first step of word-based methods  is to perform word segmentation . Actually, in order to avoid the segmentation error and to reduce the size of vocabulary, most of the existing methods are character-based . When trying to combine the character-based methods in Chinese with copy mechanism, the original ``word copy'' degrades to ``character copy'' which does not guarantee a multi-character word to be copied verbatim from the source text . Unfortunately, copying multi-character words is quite common in Chinese summarization tasks. Take the Large Scale Chinese Social Media Text Summarization Dataset   as an example, according to Table I, about 37\% of the words in the summaries are copied from the source texts and consist of multiple characters.    		} 	\end{center}  	   	 \end{table}  Selective read  was proposed to handle this problem. It calculates the weighted sum of encoder states corresponding to the last generated character and adds this result to the input of the next decoding step. Selective read can provide location information of the source text for the decoder and help it to perform the consecutive copy. A disadvantage of this approach, however, is that it increases reliance of present computation on partial results before the current step which makes the model more vulnerable to the errors accumulation and leads to exposure bias during inference.  Another way to make copied content consecutive is through directly copying text spans. Zhou et al.  implement span copy operation by equipping the decoder with a module that predicts the start and end positions of the span. Because a longer span can be decomposed to shorter ones, there are actually many different paths to generate the same summary during inference, but their model is optimized by only the longest common span at each time step during training, which exacerbates the discrepancy between two phases. In this work, we propose a novel lexicon-constrained copying network . The decoder of LCN can copy either a single character or a text span at a time, and we constrain the text span to match a potential multi-character word. Specifically, given a text and several off-the-shell word segmentators, if a text span is included in any segmentation result of the text, we consider it as a potential word. By doing so, the number of available spans is significantly reduced, making it is viable to marginalize over all possible paths during training. Furthermore, during inference, we aggregate all partial paths on the fly that producing the same output using a word-enhanced beam search algorithm, which encourages the model to copy multi-character words and facilitates the parallel computation.  To be in line with the aforementioned decoder, the encoder should be revised to learn the representations of not only characters but also multi-character words. In the context of neural machine translation, Su et al.  first organized characters and multi-character words in a directed graph named word-lattice. Following Xiao et al. , we adopt an encoder based on the Transformer  to take the word-lattice as input and allow each character and word to have its own hidden representation. By taking into account relative positional information when calculating self-attention, our encoder can capture both global and local dependencies among tokens, providing an informative representation of source text for the decoder to make copy decisions.   Although our model is character-based , it can directly utilize word-level prior knowledge, such as keywords. In our setting, keywords refer to words in the source text that have a high probability of inclusion in the summary. Inspired by Gehrmann et al. , we adopt a separate word selector based on the large pre-trained language model, e.g. BERT  to extract keywords. When the decoder intends to copy words from the source text, those selected keywords will be treated as candidates, and other words will be masked out.  Experimental results show that our model can achieve better performance when incorporating with the word selector.   
"," Copy mechanism allows sequence-to-sequence models to choose words from the input and put them directly into the output, which is finding increasing use in abstractive summarization. However, since there is no explicit delimiter in Chinese sentences, most existing models for Chinese abstractive summarization can only perform character copy, resulting in inefficient. To solve this problem, we propose a lexicon-constrained copying network that models multi-granularity in both encoder and decoder. On the source side, words and characters are aggregated into the same input memory using a Transformer-based encoder. On the target side, the decoder can copy either a character or a multi-character word at each time step, and the decoding process is guided by a word-enhanced search algorithm which facilitates the parallel computation and encourages the model to copy more words. Moreover, we adopt a word selector to integrate keyword information. Experiments results on a Chinese social media dataset show that our model can work standalone or with the word selector. Both forms can outperform previous character-based models and achieve competitive performances.",125
"  Humans are not supervised by the natural language inference . Supervision is necessary for applications in human-defined domains. For example, humans need the supervision of what is a noun before they do POS tagging, or what is a tiger in Wordnet before they classify an image of tiger in ImageNet. However, for NLI, people are able to entail that \textcircled{a} A man plays a piano contradicts \textcircled{b} A man plays the clarinet for his family without any supervision from the NLI labels. In this paper, we define such inference as a more general process of establishing associations and inferences between texts, rather than strictly classifying whether two sentences entail or contradict each other. Inspired by this, we raise the core problem in this paper: Given a pair of natural language sentences, can machines entail their relationship without any supervision from inference labels?   In his highly acclaimed paper, neuroscientist Moshe Bar claims that ``predictions rely on the existing scripts in memory, which are the result of real as well as of previously imagined experiences''. The exemplar theory argues that humans use {\bf similarity} to recognize different objects and make decisions.   Analogy helps humans understand a novel object by linking it to a similar representation existing in memory. Such linking is facilitated by the object itself and its context. Context information has been widely applied in self-supervision learning . Adapting context to NLI is even more straightforward. A simple idea of {\bf constant conjunction} is that A causes B if they are constantly conjoined. Although constant conjunction contradicts ``correlation is not causation'', modern neuroscience has confirmed that humans use it for reasoning in their mental world. For example, they found an increase in synaptic efficacy arises from a presynaptic cell's repeated and persistent stimulation of a postsynaptic cell in Hebbian theory. As to the natural language, the object and its context can be naturally used to determine the inference. For example, \textcircled{a} contradicts \textcircled{b} because they cannot happen simultaneously in the same {\bf context}.  The context representation learned by SSL  has already achieved big success in NLP. From the perspective of context, these models learn the sentence level contextual information  and the word level contextual information .  Besides linguistic contexts, humans also link other modalities  to novel inputs. Even if the goal is to reason about plain texts, other modalities still help . For example, if only textual information is used, it is difficult to entail the contradiction between \textcircled{a} and \textcircled{b}. We need the commonsense that a man only has two arms, which cannot play the piano and clarinet simultaneously. This commonsense is hard to obtain from the text. However, if we link the sentences to their visual scenes, the contradiction is much clearer because the two scenes cannot happen in the same visual context. We think it is necessary to incorporate other modalities for the unsupervised natural language inference.  The idea of adapting multimodal in SSL is not new.  According to, we briefly divide previous multimodal SSL approaches into two categories based on their encoder infrastructures. As shown in Fig., the first category uses one joint encoder to represent the multimodal inputs. Obviously, if the downstream task is only for plain text, we cannot extract the representation of text separately from the joint encoder. So the first category is infeasible for the natural language inference. The second category first encodes the text and the image separately by two encoders. Then it represents the multimodal information via a joint encoder over the lower layer encoders. This is shown in Fig.. Although the textual representation can be extracted from the text encoder in the lower layer, such representation does not go through the joint learning module and contains little visual knowledge. In summary, the encoders in previous multimodal SSL approaches are coupled. If only textual inputs are given, they cannot effectively incorporate visual knowledge in their representations. Thus their help for entailing the contradiction between \textcircled{a} and \textcircled{b} is limited.    In order to benefit from multimodal data in plain text inference, we propose the \underline{M}ultimodal \underline{A}ligned \underline{C}ontrastive \underline{D}ecoupled learning  network. This is shown in Fig.. Its text encoder is decoupled, which only takes the plain text as inputs. Thus it can be directly adapted to downstream NLI tasks. Besides, we use multimodal contrastive loss between the text encoder and the image encoder, thereby forcing the text representation to align with the corresponding image. Therefore even if the text encoder in MACD only takes the plain text as input, it still represents visual knowledge. In the downstream plain text inference tasks, without taking images as input, the text encoder of MACD still implicitly incorporating the visual knowledge learned by the multimodal contrastive loss. Note that we do not need a decoupled image encoder in the SSL. So the image encoder in Fig. in MACD takes texts as inputs to provides a more precise image encoder. We will elaborate this in section.    
","   We propose to solve the natural language inference problem without any supervision from the inference labels via task-agnostic multimodal pretraining. Although recent studies of multimodal self-supervised learning also represent the linguistic and visual context, their encoders for different modalities are coupled. Thus they cannot incorporate visual information when encoding plain text alone. In this paper, we propose \underline{M}ultimodal \underline{A}ligned \underline{C}ontrastive \underline{D}ecoupled learning  network. MACD forces the decoupled text encoder to represent the visual information via contrastive learning. Therefore, it embeds visual knowledge even for plain text inference. We conducted comprehensive experiments over plain text inference datasets . The unsupervised MACD even outperforms the fully-supervised BiLSTM and BiLSTM+ELMO on STS-B.",126
"  %閺鍙ラ嚋閸ユ拝绱濋弰顖氱秼閸撳秶娈戞径姘侀崹瀣劥缂冨弶鍎忛崘纰夌礉鐠侇厾绮屾稉娑擃亜銇囧Ο鈥崇烽敍灞藉晙閽傛悂顩撮崚鐧楁稉顏勭毈濡崇烽敍灞剧槨娑擃亜鐨Ο鈥崇烽崘宥呭礋閻欘剟鍣洪崠...  閹存垳婊戦惃鍕煙濞夋洩绱濈拋顓犵矊娑撴稉顏勩亣濡崇烽敍瀹杋netune鏉╂瑤閲滄径褎膩閸ㄥ鎮撻弮鍫曞倸绨睳娑擃亙绗夐崥灞剧箒鎼达妇娈戠亸蹇斈侀崹瀣剁礉閸欘亪娓剁电绻栨稉娑擃亝膩閸ㄥ绻樼悰宀勫櫤閸...   As neural machine translation models become heavier and heavier , we have to resort to model compress techniques  to deploy  smaller models in devices with limited resources, such as mobile phones. However, a practical challenge is that the hardware conditions of different devices vary greatly. To ensure the same calculation latency, customizing distinct model sizes  for different devices is necessary, which leads to huge model training and maintenance costs . For example, we need to distill the pre-trained large model into N individual small models.  %Then some model post-processing steps, such as model pruning  and quantization , are also performed independently for each small model.  The situation becomes worse for the industry when considering more translation directions and more frequent model iterations.  An ideal solution is to train a single model that can run in different model sizes. Such attempts have been explored in SlimNet  and LayerDrop . SlimNet allows running in four width configurations by joint training of these width networks, while LayerDrop can decode with any depth configuration by applying Dropout  on layers during training.      In this work, we take a further step along the line of flexible depth network like LayerDrop.  As shown in Figure, we first demonstrate that when there is a large gap between the predefined layer dropout during training and the actual pruning ratio during inference, LayerDrop's performance is poor.  %We attribute it to huge sub-network training space and mismatch between random sampling training and deterministic inference.  To solve this problem, we propose to use multi-task learning to train a flexible depth model by treating each supported depth configuration as a task. We reduce the supported depth space for the aggressive model compression rate and propose an effective deterministic sub-network assignment method to eliminate the mismatch between training and inference in LayerDrop. %Specifically, we design two metrics to determine which sub-network assignment is good.  Experimental results on deep Transformer  show that our approach can simultaneously support decoding in 24 depth configurations and is superior to the individual training and LayerDrop.    
"," The standard neural machine translation model can only decode with the same depth configuration as training. Restricted by this feature, we have to deploy models of various sizes to maintain the same translation latency, because the hardware conditions on different terminal devices  may vary greatly. Such individual training leads to increased model maintenance costs and slower model iterations, especially for the industry. In this work, we propose to use multi-task learning to train a flexible depth model that can adapt to different depth configurations during inference. Experimental results show that our approach can simultaneously support decoding in 24 depth configurations and is superior to the individual training and another flexible depth model training method闁炽儲鏌￠幙鐜測erDrop.",127
"   Targeted sentiment analysis  involves jointly predicting entities which are the targets of an opinion, as well as the polarity expressed towards them . The TSA task, which is part of the larger set of fine-grained sentiment analysis tasks, can enable companies to provide better recommendations , as well as give digital humanities scholars a quantitative approach to identifying how sentiment and emotions develop in literature .  Although there have been many improvements to modelling TSA since the original CRF models , such as utilising Recurrent Neural Networks  , and treating the task as span prediction rather than a sequence labelling task , most of these have concentrated on making the best use of data annotated specifically for the task.  However, annotation for fine-grained sentiment is more taxing and tends to have lower inter-annotator agreement than document or sentence classification tasks . This leads to a lack of available high-quality training data, even for highly resourced languages and prevents TSA models from learning the complex, compositional phenomena which are necessary to correctly predict targeted sentiment in an end-to-end fashion.   We believe this lack of data for fine-grained sentiment analysis leads to TSA models that cannot learn effectively complex compositional phenomena that exists in language, thus making TSA models fragile to highly compositional language. It has also been shown that incorporating compositional information from negation or speculation detection improves sentence-level sentiment classification . Other supervised tasks, such as semantic role labelling , or document level sentiment analysis  have shown promise for improving fine-grained sentiment analysis. Further transfer learning from a self-supervised language-modelling task, commonly referred to as contextualised word representations , has also shown to greatly benefit fine-grained sentiment analysis . Based on this, in this paper, we wish to explore two research questions:      To this end, we propose a multi-task learning  approach to incorporate sources of negation and speculation information into a neural targeted sentiment classifier. We additionally compare our approach with MTL models that use part-of-speech tagging, dependency relation prediction, and lexical analysis as auxiliary tasks, following previous work . Furthermore, in order to overcome the lack of evaluative resources to investigate the effects of negation and speculation, we annotate two new challenge datasets which contain difficult negated and speculative examples.     We find that the MTL models are more robust than the single task learning ,  performing competitively on the majority of the standard datasets while significantly outperforming the STL models on the negation challenge datasets, and on average better than STL models on the speculation challenge datasets. Moreover, we show that when transfer learning is applied, using CWR, to both MTL and STL models, MTL models are no longer significantly better, but are still better on average for the negation challenge dataset and one of the speculation challenge datasets. This result suggests that transfer learning does incorporate some compositional information that is required for negated and speculative samples. However all results on the challenge datasets are considerably lower than the standard dataset, showing that more work is needed to make these models more robust to compositional language.     The contributions of the paper are the following:    
","   The majority of work in targeted sentiment analysis has concentrated on finding better methods to improve the overall results. Within this paper we show that these models are not robust to linguistic phenomena, specifically negation and speculation. In this paper, we propose a multi-task learning method to incorporate information from syntactic and semantic auxiliary tasks, including negation and speculation scope detection, to create models that are more robust to these phenomena. Further we create two challenge datasets to evaluate model performance on negated and speculative samples. We find that multi-task models and transfer learning from a language model can improve performance on these challenge datasets. However the results indicate that there is still much room for improvement in making our models more robust to linguistic phenomena such as negation and speculation.",128
"  % making new tools useless if noone uses them efficiently The consensus that human activity caused the climate crisis  has led to the development of many tools and possible policy interventions, designed to minimize greenhouse gas emissions or mitigate negative impacts of climate change.  However, even the most promising tools to counter the climate crisis are futile, if they are not used. All important research efforts to mitigate the climate crisis are lost without an efficient international adaptation of tools and policies.  %promises of politicans don't lead to action  Strategies have to be adopted at a national level, following international cooperative guidelines such as the Sustainable Development Goals  or the Kyoto protocol . However, scientists, non-state actors\footnote{https://www.euronews.com/living/2018/12/21/ngos-sue-french-government-over-insufficient-climate-change-action}, and voters increasingly critique their government for insufficient action mitigating climate change . This suggests a gap between promises made by politicians and actual action taken: somewhere along the way ambitious promises for climate change mitigation have turned into careless discourse with insufficient measures taken.  %accountability shown to  prevent mismanagement Holding politicians accountable for their actions has been shown to be a major factor in preventing mismanagement, political corruption and misalignment of politician閳ユ獨 opinions and the public they are representing . %so we are working on improving the accountability to use existing tools Our work aims to provide the general public with a metric to assess if a candidate or a party is using their platform to discuss the topics related to climate change.   % the overall system In Section  we introduce a Multi-Source Topic Aggregation System  which increases transparency by providing an overview of topics discussed by politicians.  The large amount of publicly available documents are made transparent through the MuSTAS topic overview, which would otherwise be unattainable for the general public due to the amount of data. Through this transparency, decision-makers can be held accountable for their promises and claims by the general public, accelerating policies and the societal changes needed to mitigate and adapt to climate change.   %Using a the large amount of publicly available data are processed to asses how a politician uses their influence across channels and timelines.  % The research on multi-source LDA builds the scientific foundation In Section  we describe a novel multi-source hybrid latent Dirichlet allocation model which builds the scientific foundation for MuSTAS and forms the core of this research proposal. In Section  we outline how MuSTAS impacts climate change. % climate change impact  % Here be short paragraph about the structure of this proposal and the roles: MuSTAS is the larger scope, and topic modelling with multi-source hybrid LDA is the focus of research.  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
","     Decades of research on climate have provided a consensus that human activity has changed the climate and we are currently heading into a climate crisis.     Many tools and methods, some of which utilize machine learning, have been developed to monitor, evaluate, and predict the changing climate and its effects on societies.      However, the mere existence of tools and increased awareness have not led to swift action to reduce emissions and mitigate climate change.     Politicians and other policy makers lack the initiative to move from talking about the climate to concrete climate action. % in an appropriate schedule allowing for mitigation of the potentially catastrophic changes.      In this work, we contribute to the efforts of holding decision makers accountable by describing a system which digests politicians' speeches and statements into a topic summary.     We propose a multi-source hybrid latent Dirichlet allocation model which can process the large number of publicly available reports, social media posts, speeches, and other documents of Finnish politicians, providing transparency and accountability towards the general public.",129
"   Word embeddings, continuous vectorial representations of words, have become a fundamental initial step in many natural language processing  tasks for many languages. In recent years,  their cross-lingual counterpart, cross-lingual word embeddings  ---maps of matching words across languages--- have been shown to be useful in many important cross-lingual transfer and modeling tasks such as machine translation , cross-lingual document classification  and zero-shot dependency parsing .    In these representations, matching words across different languages are represented by similar vectors. Following the observation of \citet{mikolov2013efficientestimation} that the geometric positions of similar words in two embedding spaces of different languages appear to be related by a linear relation, the most common method aims to map between two pretrained monolingual embedding spaces by learning a single linear transformation matrix. Due to its simple structure design and competitive performance, this approach has become the mainstream of learning CLWE .   Initially, the linear mapping was learned by minimizing the distances between the source and target words in a seed dictionary. Early work from \citet{mikolov2013efficientestimation} uses a seed dictionary of five-thousand word pairs. Since then, the size of the seed dictionary has been gradually reduced, from several-thousand to fifty word pairs , reaching a minimal version of only sharing numerals .  More recent works on unsupervised learning  have shown that mappings across embedding spaces can also be learned without any bilingual evidence . More concretely, these fully unsupervised methods usually consist of two main steps : an unsupervised step which aims to induce the seed dictionary by matching the source and target distributions, and then a pseudo-supervised refinement step based on this seed dictionary.  The system proposed by \citet{conneau2018wordtranslation} can be considered  the first successful unsupervised system for learning CLWE. They first use generative adversarial networks  to learn a single linear mapping to induce the seed dictionary, followed by the Procrustes Analysis  to refine the linear mapping based on the induced seed dictionary. While this GAN-based model has competitive or even better performance compared to supervised methods on typologically-similar language pairs, it often exhibits poor performance on typologically-distant language pairs, pairs of languages that differ drastically in word forms, morphology, word order and other properties that determine how similar the lexicon of a language is. More specifically, their initial linear mapping often fails to induce the seed dictionary for distant language pairs . Later work from \citet{artetxe2018arobust} has proposed an unsupervised self-learning framework to make the unsupervised CLWE learning more robust. Their system uses similarity distribution matching to induce the seed dictionary and stochastic dictionary induction to refine the mapping iteratively. The final CLWE learned by their system  performs better than the GAN-based system. However, their advantage appears to come from the iterative refinement with stochastic dictionary induction, according to \citet{hartmann2019comparingunsupervised}. If we only consider the performance of a model induced only with distribution matching, GAN-based models perform much better. This brings us to our first conclusions, that a GAN-based model is preferable for seed dictionary induction.   Fully unsupervised mapping-based methods to learn CLWE rely on the strong assumption that monolingual word embedding spaces are isomorphic or near-isomorphic, but this assumption is not fulfilled in practice, especially for distant language pairs . Supervised methods are also affected by lack of isomorphism, as their performance on distant language pairs is worse than on similar language pairs. Moreover, experiments by \citet{vulic2020areall} also demonstrate that the lack of isomorphism does not arise only because of the typological distance among languages, but it also depends on the quality of the monolingual embedding space.   Actually, if we replace the seed dictionary learned by an unsupervised distribution matching method with a pretrained dictionary, keeping constant the refinement technique, the final system becomes more robust .   All these previous results indicate that learning a better seed dictionary is a crucial step to improve unsupervised cross-lingual word embedding induction and reduce the gap between unsupervised methods and supervised methods, and that GAN-based methods hold the most promise to achieve this goal. The results also indicate that a solution that can handle the full complexity of  induction of cross-lingual word embeddings will  show improvements in both close and distant languages.  In this paper, we focus on improving the initial step of distribution matching, using GANs  . Because the isomorphism assumption is not observed in reality, we argue that a successful GAN-based model must not learn only one single linear mapping for the entire distribution, but must be able to identify mapping subspaces and learn multiple mappings. We propose a multi-adversarial learning method which learns different linear maps for different subspaces of word embeddings. %specifically for subspaces of the source word embeddings.   
","  Generative adversarial networks  have succeeded  in inducing  cross-lingual word embeddings ---maps of matching words across languages--- without supervision. Despite these successes, GANs' performance for the difficult case of distant languages is still not satisfactory. These limitations have been explained by GANs' incorrect assumption that source and target embedding spaces are related by a single linear mapping and are approximately isomorphic. We assume instead that, especially across distant languages, the mapping is only piece-wise linear, and propose a multi-adversarial learning method. This novel method induces the seed cross-lingual dictionary through multiple mappings, each induced to fit the mapping for one subspace. Our experiments on unsupervised bilingual lexicon induction show that this method improves performance over  previous single-mapping methods, especially for distant languages.",130
"  In recent years, the effectiveness of utilizing image data in tandem with a text corpus to improve the quality of machine translation has been a source of extensive investigation. Several proposals have been made to incorporate visual data, such as using a doubly-attentive decoder for image and text data , initializing the encoder or decoder hidden state with image features , and using a deliberation network approach to refine translations using image data . However, a common difficulty is the lack of publicly available multimodal corpora, particularly for English-Japanese translation tasks. Currently, two of the only available English-Japanese multimodal datasets are the Japanese extension of the Pascal sentences  and Flickr30k Entities JP , which is a Japanese translation of the Flickr30k Entities dataset .    In order to contribute to the current list of English-Japanese multimodal corpora, we propose a new multimodal English-Japanese corpus with comparable sentences. Comparable sentences are sentences that contain bilingual terms and parallel phrases that describe a similar topic, but are not direct translations . This data is of particular interest due to its natural prevalence across various areas of media. For example, e-commerce sites in different countries may have product descriptions for similar products in different languages, or social media users may comment about images in several different languages.    In this study, we created a large comparable training corpus by compiling the existing image captions from the MS-COCO  and STAIR  captioning datasets.  %By compiling the existing image captions from the MS-COCO  and STAIR  captioning datasets, we were able to create a large comparable training corpus that did not require translation.  Furthermore, for validation and testing purposes, we translated a small subset of MS-COCO captions that contain ambiguous verbs. The advantage of comparable sentences in relation to their available quantity can be clearly seen in Table , with our proposed corpus containing almost twice as many sentence pairs as Flickr30k Entities JP, the current largest parallel multimodal English-Japanese corpus.  As a benchmark of current multimodal NMT models on our corpus, we performed an English-Japanese translation experiment using several baseline models, which confirmed that current NMT models are not well suited to a comparable translation task. %o evaluate our proposed corpus, we performed an English-Japanese translation experiment on several baseline models, which confirmed that current NMT models are not well suited to a comparable translation task. However, we believe that our corpus can be used to facilitate research into creating multimodal NMT models that can better utilize comparable sentences.  % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  .     %     % % final paper: en-uk version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International Licence.     % Licence details:     % \url{http://creativecommons.org/licenses/by/4.0/}.     %      % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. } \newcommand\T{\rule{0pt}{2.6ex}}   \newcommand\B{\rule[-1.5ex]{0pt}{0pt}} \newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash}m{#1}} \newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash}m{#1}} \newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash}m{#1}}        \end{table}  
"," Multimodal neural machine translation  has become an increasingly important area of research over the years because additional modalities, such as image data, can provide more context to textual data. Furthermore, the viability of training multimodal NMT models without a large parallel corpus continues to be investigated due to low availability of parallel sentences with images, particularly for English-Japanese data. However, this void can be filled with comparable sentences that contain bilingual terms and parallel phrases, which are naturally created through media such as social network posts and e-commerce product descriptions. In this paper, we propose a new multimodal English-Japanese corpus with comparable sentences that are compiled from existing image captioning datasets. In addition, we supplement our comparable sentences with a smaller parallel corpus for validation and test purposes. To test the performance of this comparable sentence translation scenario, we train several baseline NMT models with our comparable corpus and evaluate their English-Japanese translation performance. Due to low translation scores in our baseline experiments, we believe that current multimodal NMT models are not designed to effectively utilize comparable sentence data. Despite this, we hope for our corpus to be used to further research into multimodal NMT with comparable sentences.",131
"  %\todo[inline]{Why predicting hate-speech is important in general?}  %\todo[inline]{why detecting hate-speech is important in Yahoo news and Yahoo finance?}   %  %What is the problem? %Why is it interesting and important? %Why is it hard?  %Why hasn't it been solved before?  %What are the key components of my approach and results? Also include any specific limitations.  %Hatespeech is speech that ``intended to insult, offend, or intimidate a person because of some trait "". The occurrence of hatespeech has been increasing. It has become easier than before to reach a large audience quickly via social media, causing an increase of the temptation for inappropriate behaviors such as hatespeech, and potential damage to social systems. In particular, hatespeech interferes with civil discourse and turns good people away. Furthermore, hatespeech in the virtual world can lead to physical violence against certain groups in the real world\footnote{https://www.nytimes.com/2018/10/31/opinion/caravan-hate-speech-bowers-sayoc.html}\footnote{https://www.washingtonpost.com/nation/2018/11/30/how-online-hate-speech-is-fueling-real-life-violence}, so it should not be ignored on the ground of freedom of speech.  To detect hatespeech, researchers developed human-crafted feature-based classifiers , and proposed deep neural network architectures . %Online service providers also strive to combat the hatespeech through ranking algorithms, filtering, and suspending or deactivating user accounts. \textcolor{red}{However, blah blah blah}. However, they might not explore all possible important features for hatespeech detection, ignored pre-trained language model understanding, or proposed uni-directional language models by reading from left to right or right to left.   %--> 2. Other deep model for hatespeech detection: either didn't understand fully hateful context , or  ignore pretrained language model understanding and/or uni-directionally understanding language models by reading from left to right or right to left .  Recently, the BERT  model  has achieved tremendous success in Natural Language Processing % . The key innovation of BERT is in applying the transformer to language modeling tasks. %It proposed to do language modeling through two tasks: predicting masked words and predicting the next sentence. A BERT model pre-trained on these language modeling tasks forms a good basis for further fine-tuning on supervised tasks such as machine translation and question answering, etc.  Recent work on hatespeech detection  has applied the BERT model and has shown its prominent results over previous hatespeech classifiers. However, we point out its two limitations in hatespeech detection domain. First, the previous studies  have shown that a hateful corpus owns distinguished linguistic/semantic characteristics compared to a non-hateful corpus. For instance, hatespeech sequences are often informal or even intentionally mis-spelled, so words in hateful sequences can sit in a long tail when ranking their uniqueness, and a comment can be hateful or non-hateful using the same words .  %For example, ``n1gger'' in the sentence ``i am not a `n1gger' as you have indicated'' is non-hateful, but ``n1gger'' in ``you all are such a n1gger!'' is hateful.  For example, ``dick'' in the sentence ``Nobody knew dick about what that meant'' is non-hateful, but ``d1ck'' in ``You are a weak small-d1cked keyboard warrior'' is hateful \footnote{It is important to note that this paper contains hate speech examples, which may be offensive to some readers. They do not represent the views of the authors. We tried to make a balance between showing less number of hate speech examples and illustrating the challenges in real-world applications.}.  Thus, to better understand hateful vocabularies and contexts, it is better to pre-train on a mixture of both hateful and non-hateful corpora. Doing so helps to overcome the limitation of using BERT models pre-trained on non-hateful corpora like English Wikipedia and BookCorpus. Second, even the smallest pre-trained BERT ``base'' model contains 110M parameters. It takes a lot of computational resources to pre-train, fine-tune, and serve.  %There have been recent efforts reducing  Some recent efforts aim to reduce  the complexity of BERT model with the knowledge distillation technique such as DistillBert  and TinyBert . In these methods, a pre-trained BERT-alike model is used as a teacher model, and a student  model  is trained to produce similar output to that of the teacher model. Unfortunately, while their complexity is reduced, the performance is also degraded in NLP tasks compared to BERT. Another direction is to use cross-layer parameter sharing, such as ALBERT . However, ALBERT's computational time is similar to BERT, since the number of layers remains the same as BERT; likewise, its inference is equally expensive.  Based on the above observation and analysis, we aim to investigate whether it is possible to achieve a better hatespeech prediction performance than state-of-the-art machine learning classifiers, including classifiers based on publicly available BERT model, while significantly reducing the number of parameters compared with the BERT model. By doing so, we believe that performing pre-training tasks from the ground up and on a hatespeech-related corpus would allow the model to understand hatespeech patterns better and enhance the predictive results. However, while language model pretraining tasks require a large scale corpus size, available hatespeech datasets are normally small: only 16K115K annotated comments . Thus, we introduce a large annotated hatespeech dataset with 1.4M comments extracted from Yahoo News and Yahoo Finance. To reduce the complexity, we reduce the number of layers and hidden size, and propose Quaternion-based Factorization mechanisms in BERT architecture. To further improve the model effectiveness and robustness, we introduce a multi-source ensemble-head fine-tuning architecture, as well as a target-based adversarial training.  %Internet platforms can  moderate user-generated content in the interest of the majority of their users, and the business needs. Through ranking algorithms, filtering, suspending or deactivating user accounts, many Internet companies strive to combat hatespeech. %Twitter, for example, has ""the twitter rules"", which states that ``Violence, harassment and other similar types of behavior discourage people from expressing themselves, and ultimately diminish the value of global public conversation."".  %To ensure that users have a positive experience on its properties, Verizon Media also has clear rules against hatespeech. %, which state that ``Don't use hatespeech. Hatespeech directly attacks a person or group on the basis of race, ethnicity, national origin, religion, disability, disease, age, sexual orientation, gender, or gender identity. As noted above, we're a diverse global community of many types of people, with different beliefs, opinions, sensitivities, and comfort levels. If you don't feel that you can abide by our Community Guidelines as outlined below, maybe participating in the Oath community isn't for you."" %At Verizon Media, the Standard Moderation Platform  runs a platform service to moderate text, URL, images and videos. The hatespeech classifiers in SMP are based on  a number of past research work, including.   % The purpose of the work described in this paper is to improve the performance of the current state of the art for hatespeech classifiers. In a previous study, % we used a pretrained BERT  model as a starting point for fine tuning. % %, we investigated a range of different machine learning models for text classification. % %We show that a combination of a linear model, and % We found that the BERT architecture gives better performance than most baseline models, including. %, as well as Google's Prospective API. In that study, a pretrained BERT model is used as a starting point for fine tuning.  %Recently, the BERT  model has become a state-of-the-art language model and has achieved tremendous success in Natural Language Processing . %\todo[inline]{talking about BERT and its success in variety of nlp tasks with some cited works} %BERT is a modified transformer network architecture. Traditionally, many language tasks such as translation or question answering, are handled using recurrent neural networks, combined with the attention mechanism. This %reflects the fact that we tend to read a sentence from left to right. However, human also read words within context of other words, %some of them could be quite far apart, %instead of only from left to right or right to left in a mechanical way. Furthermore, recurrent network has a memory problem and can not handle long text, due to problems with vanishing or exploding gradient. In addition, it is intrinsically sequential, making the training process slow. Transformer network was proposed to solve these problems. In its setup, each word in the input text has visibility of all other words, through the use of multi-headed attention.   %It has been used %in a variety of NLP tasks as well as in other area such as image processing.     %One of the motivation of this paper is to investigate whether it is possible to achieve performance similar to, or better than the publicly available BERT models, but with smaller models. In doing so, we want to realize considerable saving in training and serving time. Another motivation is to see if it is possible to improve the BERT model further, by introducing changes to the model architecture. The third motivation is the following. The pretrained BERT models are based on % BooksCorpus and English Wikipedia. They have very different characteristics from the dataset of interest to us, which consists of users-generated comments in Yahoo News and Yahoo Finance. Consequently we believe that retraining a language model from scratch % should give us a model that understands % the language of our dataset better.  %\todo[inline]{talking about the limitation of BERT, like it is to complicated and heavy or has too many parameters. Then question is to build a better model, but with less number of parameters?}  The major contributions of our work are: \squishlist % \squishend  % We organize the paper as followed. % We give related work in Section, and % define the problem we are solving formerly in Section. We present our approach in Section, and show experimental results in Section. We conclude our paper in Section with discussions and future work.  % 
"," We present our {HABERTOR} model for detecting hatespeech in large scale user-generated content. Inspired by the recent success of the BERT model, we propose several modifications to BERT to enhance the performance on the downstream hatespeech classification task. {HABERTOR} inherits BERT's architecture, but is different in four aspects:  it generates its own vocabularies and is pre-trained from the scratch using the largest scale hatespeech dataset;  it consists of Quaternion-based factorized components, resulting in a much smaller number of parameters, faster training and inferencing, as well as less memory usage;  it uses our proposed multi-source ensemble heads with a pooling layer for separate input sources, to further enhance its effectiveness; and  it uses a regularized adversarial training with our proposed fine-grained and adaptive noise magnitude to enhance its robustness. Through experiments on the large-scale real-world hatespeech dataset with 1.4M annotated comments, we show that {HABERTOR} works better than 15 state-of-the-art hatespeech detection methods, including fine-tuning Language Models. In particular, comparing with BERT, our {HABERTOR} is 4$\sim$5 times faster in the training/inferencing phase, uses less than 1/3 of the memory, and has better performance, even though we pre-train it by using less than 1\% of the number of words. Our generalizability analysis shows that {HABERTOR} transfers well to other unseen hatespeech datasets and is a more efficient and effective alternative to BERT for the hatespeech classification.  %The code and the pretrained models are available at anonymized.",132
"   %------------------Previous version------------------ %Since UNMT in low-resource domains is not yet an actively explored field, one may naively approach this problem by training a model on multiple domains and expect it to generalize on the unseen, low-resource domains, e.g., training the model on news and sports domains and evaluating on the biomedical domain. %However, due to domain mismatch, studied on supervised NMT, the model can show inferior performance. %------------------------------------------------------- Unsupervised neural machine translation  leverages unpaired monolingual corpora for its training, without requiring an already labeled, parallel corpus. Recently, the state of the art in UNMT has achieved comparable performances against supervised machine translation approaches. However, in the case of the translation of domain-specific documents, the monolingual data themselves are scarce, and collecting them involves high cost, still suffering from low NMT performance. For instance, a model trained with monolingual data in such a low-resource domain, say, the medical domain, can experience degraded translation quality due to overfitting.  %------------------Previous version------------------ %Another reasonable approach is transfer learning, which has been frequently used for domain adaption in the literature of supervised NMT and often showed improvements in the target domain. The model is pretrained with multiple domains and then finetuned with the new domain. However, this approach may suffer from overfitting  and catastrophic forgetting when given a small number of training data and a large domain gap in a downstream task. %-------------------------------------------------- Yet, UNMT for low-resource domains is not an actively explored field. One naive approach is to train a model on high-resource domains  while hoping it to generalize on an unseen low-resource domain  as well. However, it has been shown from recent studies on supervised NMT that a nontrivial domain mismatch can significantly cause low translation accuracy.  Another reasonable approach is transfer learning, or in particular domain adaptation, which has shown  performance improvements in the literature of supervised NMT. In this approach, the model is first pretrained using existing domains and then finetuned using the data in a new domain. However, this approach may suffer from overfitting and catastrophic forgetting due to a small number of training data and a large domain gap.  As an effective method for handling a small number of training data, meta-learning has shown its superiority in various NLP tasks, such as dialog generation, translation, and natural language understanding. However, to the best of our knowledge, it was not applied to tackle the UNMT tasks with a small number of training data, i.e., low-resource UNMT.   In response, this paper extends meta-learning approach for low-resource UNMT, called \toolnameMeta. The objective of \toolnameMeta is to find the optimal initialization for model parameters that can quickly adapt to a new domain even with only a small amount of monolingual data. To be specific, assuming that data from multiple source domains are available, which makes meta-learning applicable, we first pretrain the UNMT model with source domains based on \toolnameMeta and then finetune the model  using a target domain.   Moreover, we propose an improved meta-learning approach called \ourtoolname for low-resource UNMT by explicitly promoting common knowledge across multiple domains as well as generalizable knowledge from a particular domain to another. In particular, our proposed approach prevents the model from overfitting due to a small amount of training data in a new domain.   In summary, our contributions include the following.    %\item \ourtoolname shows that it has domain-general knowledge and is faster in convergence than all the other methods. %\item We empirically demonstrate that our enhanced algorithm, \ourtoolname, consequently boosts up the performance of low-resource UNMT against other baseline models including \toolnameMeta.  %\item We extend the meta-learning algorithm by incorporating the domain mixing loss, and it outperforms all the other methods. % %We show zero-shot performance to evaluate generalization ability for \ourtoolname, where \ourtoolname outperforms other methods.  % To the best of our knowledge, our work is the first to apply a meta-learning approach to UNMT tasks. Our proposed algorithms can quickly adapt to in-domain with only a few iteration steps. Both \toolnameMeta and \ourtoolname consistently outperform the baseline models up to 3 BLEU scores. Especially, \ourtoolname achieves promising results among others including \toolnameMeta. Besides, we show zero-shot performance to evaluate generalization ability for \ourtoolname, where \ourtoolname outperforms other methods.  %--------------------------------- % 闋冩﹥顫呮 姘╁牗妫撮瀽鎰冲妧闆 闇冨嫴饪洪灇 闈镐緛纾ら瀽鍡㈡緤 鐡垮婀㈣嚙 闇呮﹤濮 general 闋 feature闇屻倢婢 闉涘牕瀚.  % Although each domain is very distance each others in domain adaptation, they share some linguistic features, such as the grammar and basic words.     % Azam: To alleviate the aforementioned challenge, % Azam: To overcome this issue, many %   % \item  % \item 闉栧崐螠闉 contribtuion bullet point鎼 summary : % \item 1. formulate new task  % \item 2. New frame work proposed % \item 3. evaluate various domain. show fast adaptation and quality. %On the other hand, since unsupervised machine translation has attained comparable performance against supervised machine translation, fully unsupervised domain adaptation, which uses only monolingual data for both in-domain and out-of-domain, is more suitable to handle data-scarce languages. %yet has a challenge for data-scarce domains. In other words, an unsupervised domain adaptation task can handle data-scarce languages; however, it cannot resolve a challenge for low-resource domains. % %such that it alleviates the aforementioned challenge, building a parallel corpus. %Therefore, a fully unsupervised domain adaptation task, consisted of unpaired language corpus for both in-domain and out-of-domain, is more realistic setting than a supervised domain adaptation task.%and substantial effort to collect domain specific data. %A meta-learning algorithm is superior for low-resource data. Unlike domain adaptation, a meta-learning algorithm does not require in-domain data to learn initial parameters. It only asks few training samples to meta-train  the model. Collaborating a meta-learning algorithm with unsupervised machine translation    %Since we leverage cross-lingual language model pretraining  which allows the model to learn cross-lingual representations, our gradient updates are divided into two objective functions, back-translation and language modeling. % Several approaches have been proposed to resolve the scarcity problem. For instance, a data mixing can be one approach that aggregates high-resource and low-resource data and train the model to adequately translate the low-resource target language   % To overcome the data scarcity problem, one simple approach is a data mixing that aggregates high-resource and low-resource data and train the model to adequately translate the low-resource target language. The other approach is a transfer learning that first pretrains on high-resource data and fine-tunes the low-resource data. Although these aforementioned approaches explicitly tackle the low-resource challenge, the scarcity problem still remains in NMT because building parallel corpus with specialized expertise is costly expensive.   % In this paper, we leverage recent success of unsupervised NMT  that uses only monolingual corpus. Inspired by , we propose new task called low-resource UNMT. To the best of our knowledge, this is a first attempt    % To overcome this issue, unsupervised learning in NMT has been proposed to resolve the parallel data scarcity problem. However, this approach has a constraint that abundant monolingual corpus should be always available.   % In reality, monolingual corpus can be also scarce if the domains or languages are often used. %鑶﹂浛 闉涙劤鍔爩 闉氭尗婀 闈广倠鐛  % Although various approaches have been proposed to address a low-resource challenge , none of the works consider a low-resource unsupervised task in NMT. To the best of our knowledge, this is first the attempt which explicitly tackles the low-resource UNMT task.  % In this paper,    % When we translate a word to the different language, the semantic meaning can be changed. For instance, the meaning of word ""CNN"" is different in the domain of deep learning and news     % To overcome this issue, the abundant parallel data are required which are not easy to obtain. Recently, unsupervised NMT  studies show reasonable performance comparison to supervised NMT.     % Data mixing with high-resource and low-resource is one approach to handle the following issue. The other approach is a transferring learning method that first trains on high-resource datasets and fine-tunes on a low-resource datatset. However, the problem can still remain if the parallel data is scarce in domains or languages. To overcome the parallel data scarcity problem    % To overcome this issue, unsupervised learning in NMT has been proposed to resolve the the problem of insufficient parallel data. However, this approach assumes that obtaining monolingual corpus is always easier than acquiring parallel corpus. Since the languages can vary by domains , either monolingual or parallel data can be scarce.  % utilize monolingual corpus which assume that monolingual corpus are always available. However,     % In NMT, the data scarcity problem can be divided into two different training data scenarios, insufficient training parallel data and training data itself . To overcome the scarce parallel data issue, recent studies proposed to utilize monolingual corpus.    % the parallel data is essential to train the NMT model    % several learning methods, such as unsupervised learning and transfer learning in NMT, are proposed to overcome the data scarcity problem. However, those works only consider in languages which still remains the problem in domains . Moreover, to the best of our knowledge, none of the works attempt to  %Learning is an inevitable phase when we adapt to a new task. However, various learning experiences can reduce the exertion of learning a new task.         %   %Domains can be  %To overcome this issue, one simple approach is a domain mixing that aggregates high-resource and low-resource domains and train the model to adequately translate the low-resource domain. The other approach is a transfer learning that first pretrains on high-resource domains and fine-tunes the low-resource domain.  %Despite the remarkable success on neural machine translation ~, the performance of NMT drops substantially against traditional statistical machine translation  when the training data is scarce  %To overcome the scarcity of training data in languages, variants of multilingual translation approaches have been proposed. These approaches basically exploit high-resource knowledge by aggregating both high-resource and low-resource data to train one single model. The other approach is utilizing transfer learning that the model first pretrains on high-resource data and later fine-tunes on low-resource data. The similar manner follows for the domains as well.  %Moreover, few-shot learning and meta-learning arise in machine learning where both attempt to handle the data scarcity problem. In NMT, ~\citet{gu2018meta} re-formulates the model-agnostic meta-learning  algorithm to resolve the low-resource challenge for NMT.  %Although aforementioned approaches tackle the low-resource challenge, the data scarcity problem can still remain because following approaches require parallel data, and building a low-resource language pair  with specialized expertise is costly expensive. Hence, the recent research suggests to rely only on monolingual corpus instead of using parallel corpus. The various unsupervised NMT ~ studies show the reasonable performance comparison to supervised NMT. % 闋冩﹥顫呮 monolingual corpus 姣靛牗鐖涢渻 闆垮姙婢婇煰 鐡寸粖鏅爢鍕冲剨闉 闉氭鏌庨洰鐘惧灛闉涘牗娼 -鏀磋兂鏌涢瀼鎸 鏀撮附寮版皡鏃嶆緫闉 闉濅緟娼夐澗姗冪抽浖..?  %sufficient in-domain data to train the model; however, in real-world, collecting domain specific data requires substantial effort. %building a low-resource language pair  with specialized expertise is costly expensive  % 鏂撴粔鑺 闇屻倢鏌 MT闉 姘氭粚鐖犻灇 闈奉剢鐎 姘╁牗妫撮澗姗冾槬鏃矅顫 闇冨嫴瀚 闋冩﹥姒鹃灇 . % Machine learning 闉愭劤鍔闉 Data scarcity 姘嶈兂鐗 闉濇粔璧 % Domain translation闉 闉 娆锋埄娈ч爟婊岊潊  % unsuperivsed machine translation % data mixing, transfer learning % knowledge gets partially vanished  % 闆垮姙婢婇煰 鐡寸粖鏅 % 闋冩悡鑸堕爟姗佽荡闉欏嫶鏆ｉ澒 transfer learning mixing data 鑷ф瑬娼 姘氣晣鐭 闈奉剣姣 % 闋冩﹥顫呮 parallel setting 闉愭劤鍔 闉濅緟姣勯爟 % parallel 闆垮姙婢婇煰鐡ｇ殰 闈炬﹥顫栭爟姗佽荡鑷 闋屾﹤鎽 % monolingual corpus姣 闈奉剣姣勯爟姗傚 UNMT work闇屻倢婢 闈告繉绠 % unsupervised 闈瑰姜濮ら灇 鏂撴粔鑺抽湆銈屾煄 姣靛韩婢 闆存帥鏅炴瓎 % 闋冩﹥顫呮 monolingual corpus 姣靛牗鐖涢渻 闆垮姙婢婇煰 鐡寸粖鏅爢鍕冲剨闉 闉氭鏌庨洰鐘惧灛闉涘牗娼 -鏀磋兂鏌涢瀼鎸 鏀撮附寮版皡鏃嶆緫闉 闉濅緟娼夐澗姗冪抽浖..? % 鏀撮附螠闋冩﹥妫 闉栧崐螠闆 low-resource UNMT姣 meta-learning algorithm闉欒導顢 闊块附濮 姘氣晥鏅ラ灇 闉濇粚瀚 % 鏃姙銆 meta-nmt 闆茶導顑撶摽鑼у 闆笺倠顩 闉氭尗婀㈤浕 闉栧崐螠鏃拌導濮 unsupervised闉 %  multi doamin 鑷ф洢鈥 % 
"," Unsupervised machine translation, which utilizes unpaired monolingual corpora as training data, has achieved comparable performance against supervised machine translation. However, it still suffers from data-scarce domains. To address this issue, this paper presents a meta-learning algorithm for unsupervised neural machine translation  that trains the model to adapt to another domain by utilizing only a small amount of training data. We assume that domain-general knowledge is a significant factor in handling data-scarce domains. Hence, we extend the meta-learning algorithm, which utilizes knowledge learned from high-resource domains to boost the performance of low-resource UNMT. Our model surpasses a transfer learning-based approach by up to 2-4 BLEU scores. Extensive experimental results show that our proposed algorithm is pertinent for fast adaptation and consistently outperforms other baseline models.",133
" Numerous entities are emerging everyday. The attributes of the entities are often noisy or incomplete, even missing.  In the field of electronic commerce, target attributes  of new products are often missing .  In medical analysis, attributes like transmission, genetics and origins of a novel virus are often unknown to people.  Even in DBpedia, a well-constructed and large-scale knowledge base extracted from Wikipedia, half of the entities contain less than 5 relationships .  %In KG construction area, KGs often suffer from incompleteness.  %For example, in DBpedia, a well-constructed and large-scale knowledge base extracted from Wikipedia, half of the entities contain less than 5 relationships . %Therefore,  A method that is capable of supplementing reliable attribute values for emerging entities can be highly useful in many applications.  %With the method to automatically extract attribute values for emerging entities, the eCommerce retailers are able to better serve the customers with updated information; the extracted medical attribute information of a novel virus can be organized to assist the understanding of the virus; the KG will be able to provide more complete information for users.      Although information extraction methods have been extensively studied, the task of open attribute value extraction remains challenging. First, the emerging entities may have new attribute values that are absent in the existing KG. Under such circumstances, the prediction methods under the closed-world assumption and the methods that cannot utilize external information are not well suited due to their limited recalls. Second,  while web corpus can be used as a good resource to provide relatively updated and relevant articles for large varieties of emerging entities, %that are relatively complete and updated in a timely manner,  %considering the large variety of the emerging entities, the web corpus, which is relatively complete and updated in a timely manner, is able to provide a rich collection of relevant articles.  %However,  the articles retrieved from web corpus can be noisy and/or irrelevant, which in turn leads to a limited precision.  Finally, even when articles are relevant, the extracted answers might still be inaccurate due to the error-prone information extraction model.    To effectively filter out noisy answers that are obtained either due to the irreverent articles or the errors incurred by the information extraction system, we  %need to answer  pose the following two questions: First, how many articles should we collect from the enormous web corpus? Second,  how to select the most reliable value out of the pool of all the possible answers extracted from the articles?  There is no common answer to the first question that works for all triplets because of the inconsistent degrees of difficulties in finding the correct attribute values. The decision of when to stop querying more external articles needs to be made after successive evaluations of the candidate answers. Thus the decision making process is inherently sequential. %Thus, it is inherently a sequential decision making problem.   Reinforcement learning  is a commonly adopted method to deal with sequential decision problems and has been widely studied in the field of robotic and game . But there are not many researches on open attribute value extraction with RL.  One existing literature of RL-based method for value extraction is proposed by .  In their work, a RL framework is designed to improve accuracy of event-related value extraction by acquiring and incorporating external evidences.  However, their approach requires a great amount of context information about the specific event of interest during the training process.  It is not trivial to extend their framework for open attribute value extraction, because we would need to collect context words and train a new model with annotated data for each emerging attribute. Therefore, their framework cannot be generalized to open attribute value extraction task when various entities and attributes are involved.  While using the context words to construct the states in RL is not suitable in our task,  our solution is to leverage the rich, well-organized information in KG, which is not only informative but also generalizable.  %The knowledge from KG  Such information can be leveraged in answer comparisons, which addresses our second question. For example, to fill the incomplete triplet  iPhone 11, display resolution, ?, from the KG we may find that the attribute values  ``display resolutions"" of an entity that is under category ``Phone"" is commonly expressed in the format of ``xxx by xxxx Pixels"", where x stands for some digit. The typical instances of the attribute values for entities under the same category provide valuable background information.   In this paper, we propose a knowledge-guided RL framework to perform open attribute value extraction.  The RL agent is trained to make good actions for answer selection and stopping time decision.  Our experiments show that the proposed framework significantly boosts the extraction performance.  To the best of our knowledge, we are the first to integrate KG in a RL framework to perform open attribute value extraction %use KG to guide the RL-based sequential decision for open attribute value extraction.  %The experiment results demonstrate that our approach improves the extraction performances substantially. In summary, our contribution are in three folds:     
"," Open attribute value extraction for emerging entities is an important but challenging task.  A lot of previous works formulate the problem as a question-answering  task.  While the collections of articles from web corpus provide updated information about the emerging entities, the retrieved texts can be noisy, irrelevant, thus leading to inaccurate answers. Effectively filtering out noisy articles as well as bad answers is the key to improving extraction accuracy. Knowledge graph , which contains rich, well organized information about entities, provides a good resource to address the challenge. In this work, we propose a knowledge-guided reinforcement learning  framework for open attribute value extraction.  Informed by relevant knowledge in KG, we trained a deep Q-network  to sequentially compare extracted answers to improve extraction accuracy. The proposed framework is applicable to different information extraction system. Our experimental results show that our method outperforms the baselines by 16.5 - 27.8\%.",134
" % It can be applied to generate synthetic Question Answering datasets, or construct with a QA model as dual tasks to boost QA systems. It can also contribute to the dialogue system to ask meaningful questions for user experience enhancement or be applied into education systems.  Question Generation  is a task to automatically generate a question from a given context and, optionally, an answer. Recently, we have observed an increasing interest in text-based QG.   % in the QG area, including Knowledge-based QG , Image-based QG , and especially             \end{table}  % help train models with complex reasoning ability. However, manually creating those datasets is time-consuming and costly, thus automatic multi-hop QG can potentially reduce the cost.   % It can be applied to generate synthetic Question Answering datasets , or construct with a QA model as dual tasks to boost QA systems. It can also contribute to the dialogue system to ask meaningful questions or be applied into education systems.   Most of the existing works on text-based QG focus on generating SQuAD-style questions, which are generated from the sentence containing the answer or nearby sentences in the same paragraph, via single-hop reasoning. Little effort has been put in multi-hop QG, which is a more challenging task. Multi-hop QG requires aggregating several scattered evidence spans from multiple paragraphs, and reasoning over them to generate answer-related, factual-coherent questions. It can serve as an essential component in education systems, or be applied in intelligent virtual assistant systems. It can also combine with question answering  models as dual tasks to boost QA systems with reasoning ability.  % to generate complicated challenging questions to evaluate student's understanding about a certain topic  Intuitively, there are two main additional challenges needed to be addressed for multi-hop QG. The first challenge is how to effectively identify scattered pieces of evidence that can connect the reasoning path of the answer and question. As the example shown in Table, to generate a question asking about ``Marine Air Control Group 28'' given only the answer ``Havelock, North Carolina'', we need the bridging evidence like ``Marine Corps Air Station Cherry Point''. %, the connection between it and the answer entity is achieved through two-hops reasoning.  The second challenge is how to reason over multiple pieces of scattered evidence to generate factual-coherent questions.  % Furthermore, with more evidence other than the single answer-related sentence as in single-hop QG,, how to reason over those evidences to fuse the information to generate factual coherent question, is another challenging issue  % Early single-hop QG uses rule-based methods to transform sentences into questions.  % Conventional single-hop QG uses neural network based approaches based on the sequence-to-sequence  framework, where different types of encoders and decoders have been designed. % , with different ways to attend from answer information to context encoding.  % However, none of the previous work addressed the challenges we mentioned above for multi-hop QG task, but only incorporate answer information to context encoding to do single-hop reasoning. To the our best of our knowledge, the only work for multi-hop QG  uses multi-task learning with an auxiliary loss for sentence-level supporting fact prediction, requiring supporting fact sentences in different paragraphs being labeled out in training data. While labeing those supporting facts requires much human labor and is time-consuming to obtain in real scenarios, their method cannot be applied to general multi-hop QG cases. % requiring supporting fact sentences from different paragraphs being labeled out in training data, which are  Previous works mainly focus on single-hop QG, which use neural network based approaches with the sequence-to-sequence  framework. Different architectures of encoder and decoder have been designed to incorporate the information of answer and context to do single-hop reasoning. To the best of our knowledge, none of the previous works address the two challenges we mentioned above for multi-hop QG task. The only work on multi-hop QG uses multi-task learning with an auxiliary loss for sentence-level supporting fact prediction, requiring supporting fact sentences in different paragraphs being labeled in the training data. While labeling those supporting facts requires heavy human labor and is time-consuming, their method cannot be applied to general multi-hop QG cases without supporting facts.   % Supporting fact is extra information indicating the sentences that contain the evidence to answer the question.     In this paper, we propose a novel architecture named Multi-Hop Encoding Fusion Network for Question Generation  to address the aforementioned challenges for multi-hop QG. First of all, it extends the Seq2Seq QG framework from sing-hop to multi-hop for context encoding. Additionally, it leverages a Graph Convolutional Network  on an answer-aware dynamic entity graph, which is constructed from entity mentions in answer and input paragraphs, to aggregate the potential evidence related to the questions. Moreover, we use different attention mechanisms to imitate the reasoning procedures of human beings in multi-hop generation process, the details are explained in Section.  % It is an extension of the Seq2Seq framework for single-hop QG, but instead of using one single encoder, we propose a Multi-hop Encoder, to do context encoding in multiple hops  with Graph Convolutional Network , and encoding reasoning via a gated feature fusion module in the encoding stage. We applying GCN on an answer-aware dynamic entity graph, constructed from entity mentions in answer and input paragraphs, to aggregate the potential evidence related to question will be . And we use different logic at different encoder hops to imitate the reasoning procedures of human beings in multi-hop generation process.  We conduct the experiments on the multi-hop QA dataset HotpotQA with our model and the baselines. The proposed model outperforms the baselines with a significant improvement on automatic evaluation results, such as BLEU. The human evaluation results further validate that our proposed model is more likely to generate multi-hop questions with high quality in terms of Fluency, Answerability and Completeness scores.   % address the challenge Our contributions are summarized as follows:      % we leverage Graph Convolutional Network  in the encoder for sake of effectively connecting related entities across the paragraphs.   % we then apply Graph Convolutional Network  to aggregate information that is multi-hops away from answers.  % the proposed Multi-hop Encoder block will generate ?? context encoding and dynamically fuse them, imitating the reasoning procedures of human beings in multi-hop generation process.  % Different reasoning logic is applied at different encoder hops.     % which is an extension of the previous Seq2Seq framework for single-hop QG.  % Instead of using a single encoder, we propose a Multi-hop Encoder block, to do context encoding in multiple hops and encoding reasoning via a gated feature fusion module.   % Inspired by the previous work on multi-hop QA using graph networks , we build an answer-aware dynamic entity graph based on entity mentions in answer and input paragraphs.   % We adopt a co-attention mechanism  at the answer-aware context encoder for deeper information exchange between answer and the context,  % while use bi-attention mechanism  treating the dynamic entity graph as memories, at the entity-aware answer encoder module to update multi-hop answer encoding.  % We generate the final context encoding via a gated encoder reasoning module,  which decides to keep or to ignore the encoder information from previous encoding hops.  % By reasoning over multiple information sources  but multi-hops away from each other in context or conversational history. % Multi-hop QG can serve as an essential component in intelligent virtual assistant system to ask user with more informative questions to enhance user engagement , or be applied into education systems to generate complicated challenging questions to evaluate student's understanding about certain topic and stimulate their self-learning . On the other hand, large-scale high quality multi-hop question answering datasets such as HotpotQA  can help train models with complex reasoning ability. However, manually creating those datasets is time-consuming and costly, and automatic multi-hop QG can potentially reduce the cost, especially when there are a large set of documents available.    
"," % grammatically correct, logically coherent Multi-hop Question Generation  aims to generate answer-related questions by aggregating and reasoning over multiple scattered evidence from different paragraphs. It is a more challenging yet under-explored task compared to conventional single-hop QG, where the questions are generated from the sentence containing the answer or nearby sentences in the same paragraph without complex reasoning. To address the additional challenges in multi-hop QG, we propose Multi-Hop Encoding Fusion Network for Question Generation , which does context encoding in multiple hops with Graph Convolutional Network and encoding fusion via an Encoder Reasoning Gate. To the best of our knowledge, we are the first to tackle the challenge of multi-hop reasoning over paragraphs without any sentence-level information. Empirical results on HotpotQA dataset demonstrate the effectiveness of our method, in comparison with baselines on automatic evaluation metrics. Moreover, from the human evaluation, our proposed model is able to generate fluent questions with high completeness and outperforms the strongest baseline by 20.8\% in the multi-hop evaluation. The code is publicly available at \href{https://github.com/HLTCHKUST/MulQG}{https://github.com/HLTCHKUST/MulQG}.  % n-gram based automatic evaluation metrics, and human evaluation metrics for answerability, fluency and multi-hops.",135
"  % NMT is good but needs lots of parallel data + we should exploit mono data more Neural machine translation  using sequence to sequence architectures  has become the dominant approach to automatic machine translation. While being able to approach human-level performance , it still requires a huge amount of parallel data, otherwise it can easily overfit. Such data, however, might not always be available. At the same time, it is generally much easier to gather large amounts of monolingual data, and therefore, it is interesting to find ways of making use of such data. The simplest strategy is to use backtranslation , %but it can be rather costly since it requires training another model in the opposite translation direction and then creating the source-side synthetic sentences by translating the target-side monolingual corpus. but it can be rather costly since it requires training a model in the opposite translation direction and then translating the monolingual corpus.  % We introduce the compositionality  It was suggested by \citet{lake2017machines} that during the development of a general human-like AI system, one of the desired characteristics of such a system is the ability to learn in a continuous manner using previously learned tasks as building blocks for mastering new, more complex tasks. %by combining the knowledge learned from the previously learned simpler tasks. Until recently, continuous learning of neural networks was problematic, among others, due to the catastrophic forgetting . Several methods were proposed , however, %they mostly focused on preserving the knowledge of each task learned by the whole network. they mainly focus only on adapting the whole network  to new tasks while maintaining good performance on the previously learned tasks.  % Summary of our method using EWC %\XXX{toto mozna posunout za nasledujici odstavec + jak resime jejich nedostatky} In this work, we present an unsupervised pretraining method for NMT models using Elastic Weight Consolidation . First, we initialize both encoder and decoder with source and target language models respectively. Then, we fine-tune the NMT model using the parallel data. To prevent the encoder and decoder from forgetting the original language modeling  task, we regularize their weights individually using Elastic Weight Consolidation based on their importance to that task. Our hypothesis is the following: by forcing the network to remember the original LM tasks we can reduce overfitting of the NMT model on the limited parallel data. %\XXX{Ukazujeme, ze metoda funguje, je rychlejis + mame odvozeno, ze by mela fungovat i pro podsite} %\XXX{Zminit rovnou strucne prinosy?}  % Summary of the method we used as a comparison We also provide a comparison of our approach with the method proposed by \citet{ramachandran2017pretraining}. They also suggest initialization of the encoder and decoder with a language model. However, during the fine-tuning phase they use the original language modeling objectives as an additional training loss in place of model regularization. Their approach has two main drawbacks: first, during the fine-tuning phase, they still require the original monolingual data which might not be available anymore in a life-long learning scenario. Second, they need to compute both machine translation and language modeling losses which increases the number of operations performed during the update slowing down the fine-tuning process. Our proposed method addresses both problems: it requires only a small held-out set to estimate the EWC regularization term and converges 2-3 times faster than the previous method.\footnote{The speedup is with regard to the wall-clock time. In our experiments both EWC and the LM-objective methods require similar number of training examples to converge.}   %Intro to compositionality %Compositional learning + using previosly learned elementary knowledge to learn more complex model   %Avoiding catastrophic forgetting as key to continual learning and compositionality -> choice of EWC  %Benefits of compositionality in greater scope  + why NMT + LM pretrain? %It is a first step in our ongoing reseach  %The paper is structured as following...  
","   This work presents our ongoing research of unsupervised pretraining in neural machine translation . In our method, we initialize the weights of the encoder and decoder with two language models that are trained with monolingual data and then fine-tune the model on parallel data using Elastic Weight Consolidation  to avoid forgetting of the original language modeling tasks.   We compare the regularization by EWC with the previous work that focuses on regularization by language modeling objectives.   %We compare the EWC regularization with the previous work that uses language modeling objectives from the original task for model regularization.      The positive result is that using EWC with the decoder achieves BLEU scores similar to the previous work. However,   the model converges 2-3 times faster and does not require the original unlabeled training data during the fine-tuning stage.      In contrast, the regularization using EWC is less effective if the original and new tasks are not closely related. We show that initializing the bidirectional NMT encoder with a left-to-right language model and forcing the model to remember the original left-to-right language modeling task limits the learning capacity of the encoder for the whole bidirectional context.      %%% POZNAMKY %%%   % Analyza Fisher Information   % - Self-attention projekce are more important than the Feedforward layers   % - Self-attention:   %     - output and value projections are more important at the higher layers   %     - key and query projections are more important at lower layers   % ...      % Previous work    % - requires orig. unlabeled data for MT training -> EWC can estimate Empirical Fisher on small  heldout data   % - is effective even when the orig. and new tasks differ   %  and then using this pretrained encoder in MT  -> EWC is bad at this      % Our work :   % - has nice mathematical definition    % - faster convergence in time    % - works only with decoder  -> little worse than LM obj.   % - method works when task are similar in nature    % - how deep should the unlabeled-data-pretrained enc-dec should be?   %       % why only left-context? previous work shows that with transformer, the drop in performance is not that big + it is much easier to implement       % Future work :   % - Investigate complementarity of EWC and LM obj.    % - Investigate the learning rate schemes    % - Investigate the method in the multimodal/multisource scenario   % - Investigate the method",136
"   Even though machine translation  has greatly improved with the emergence of neural machine translation   and more recently the Transformer architecture , there remain challenges which can not be solved by using sentence-level NMT systems. Among other issues, this includes the problem of inter-sentential anaphora resolution  or the consistent translation across a document , for which the system inevitably needs document-level context information.  In recent years, many works have focused on changing existing NMT architectures to incorporate context information in the translation process . However, often times results are reported only on very specific tasks , making it difficult to assess the potential of the different methods in a more general setting. This, together with the fact that big improvements are typically reported on low resource tasks, gives the impression that document-level NMT mostly improves due to regularization rather than from leveraging the additional context information. In this work we want to give a more complete overview of the current state of document-level NMT by comparing various approaches on a variety of different tasks including an application-oriented E-commerce setting. We discuss both, widely used performance metrics, as well as highly task-specific observations.  Another important aspect when talking about document-level NMT is the applicability in ``real life"" settings. There, when faced with a low resource data scenario, back-translation is an established way of greatly improving system performance . However, to the best of our knowledge, the effect of back-translation data obtained and used by context-aware models has never been explored before. The main contributions of this paper are summarized below:    
","  Context-aware neural machine translation  is a promising direction to improve the translation quality by making use of the additional context, e.g., document-level translation, or having meta-information. Although there exist various architectures and analyses, the effectiveness of different context-aware NMT models is not well explored yet. This paper analyzes the performance of document-level NMT models on four diverse domains with a varied amount of parallel document-level bilingual data. We conduct a comprehensive set of experiments to investigate the impact of document-level NMT.  We find that there is no single best approach to document-level NMT, but rather that different architectures come out on top on different tasks. Looking at task-specific problems, such as pronoun resolution or headline translation, we find improvements in the context-aware systems, even in cases where the corpus-level metrics like BLEU show no significant improvement. We also show that document-level back-translation significantly helps to compensate for the lack of document-level bi-texts.   \includecomment{ Context-aware neural machine translation  is a promising direction for improving the translation quality having more context, e.g., document-level translation, or having meta-information. The goal is to enhance the translation of discourse phenomena and polysemous words. This paper analyzes the performance of document-level NMT models with a varied amount of parallel document-level bilingual data. Including a diverse set of tasks, e.g., movie subtitles and e-commerce data, we conduct a comprehensive set of experiments to analyze and to learn the impact of document-level NMT. We show the document-level back-translation significantly helps to compensate for the lack of document-level bi-texts.  }",137
" Knowledge Graphs  like Freebase, NELL and Wikidata are extremely useful resources for NLP tasks, such as information retrieval, machine reading, and relation extraction. A typical KG is a multi-relational graph, represented as triples of the form  given only a few entity pairs of the task relation . These known few-shot entity pairs associated with  are called references. To improve semantical representations of the references,  and  devise modules to enhance entity embeddings with their local graph neighbors. The former simply assumes that all neighbors contribute equally to the entity embedding, and in this way the neighbors are always weighted identically. The latter develops the idea by employing an attention mechanism to assign different weights to neighbors, but the weights do not change throughout all task relations. Therefore, both works assign static weights to neighbors, leading to static entity representations when involved in different task relations. We argue that entity neighbors could have varied impacts associated with different task relations. Figure gives an example of head entity  than the other one.  In addition, task relations can be polysemous, also showing different meanings when involved in different entity pairs. Therefore, the reference triples could also make different contributions to a particular query. Take a task relation  as an example. As shown in Figure,  associates with different meanings, e.g., organization-related as . Obviously, for query , referring to the organization-related references would be more beneficial.   To address the above issues, we propose an \underline{A}daptive \underline{A}ttentional \underline{N}etwork for \underline{F}ew-Shot KG completion , a novel paradigm that takes dynamic properties into account for both entities and references. Specifically, given a task relation with its reference/query triples, FAAN proposes an adaptive attentional neighbor encoder to model entity representations with one-hop entity neighbors. Unlike the previous neighbor encoder with a fixed attention map in, we allow attention scores dynamically adaptive to the task relation under the translation assumption. This will capture the diverse roles of entities through varied impacts of neighbors. Given the enhanced entity representations, FAAN further adopts a stack of Transformer blocks for reference/query triples to capture multi-meanings of the task relation. Then, FAAN obtains a general reference representation by adaptively aggregating the references, further differentiating their contributions to different queries. As such, both entities and references can capture their fine-grained meanings, and render richer representations to be more predictive for knowledge acquisition in the few-shot scenario.  The contributions of this paper are three-fold:   We propose the notion of dynamic properties in few-shot KG completion, which differs from previous paradigms by studying the dynamic nature of entities and references in the few-shot scenario.   We devise a novel adaptive attentional network FAAN to learn dynamic representations. An adaptive neighbor encoder is used to adapt entity representations to different tasks. A Transformer encoder and an attention-based aggregator are used to adapt reference representations to different queries.   We evaluate FAAN in few-shot link prediction on benchmark KGs of NELL and Wikidata. Experimental results reveal that FAAN could achieve new state-of-the-art results with different few-shot sizes.  
","   Few-shot Knowledge Graph閼 completion is a focus of current research, where each task aims at querying閼辩惮nseen facts閼辩郸f a relation given its few-shot reference entity pairs. Recent attempts solve this problem by learning static representations of entities and references, ignoring their dynamic properties, i.e., entities may exhibit diverse roles within task relations, and references may make different contributions to queries. This work proposes an adaptive attentional network for few-shot KG completion by learning adaptive entity and reference representations. Specifically, entities are modeled by an adaptive neighbor encoder to discern their task-oriented roles, while references are modeled by an adaptive query-aware aggregator to differentiate their contributions. Through the attention mechanism, both entities and references can capture their fine-grained semantic meanings, and thus render more expressive representations. This will be more predictive for knowledge acquisition in the few-shot scenario. Evaluation in link prediction on two public datasets shows that our approach achieves new state-of-the-art results with different few-shot sizes. The source code is available at \url{https://github.com/JiaweiSheng/FAAN}.",138
" Automatic summarization is a fundamental task in Natural Language Processing, which aims to condense the original input into a shorter version covering salient information and has been continuously studied for decades . Recently, online multi-speaker dialogue/meeting has become one of the most important ways for people to communicate with each other in their daily works. Especially due to the spread of  COVID-19 worldwide, people are more dependent on online communication. In this paper, we focus on dialogue summarization, which can help people quickly grasp the core content of the dialogue without reviewing the complex dialogue context.   Recent works that incorporate additional commonsense knowledge in the dialogue generation  and dialogue context representation learning  show that even though neural models have strong learning capabilities, explicit knowledge can still improve response generation quality.   It is because that a dialog system can understand conversations better and thus respond more properly if it can access and make full use of large-scale commonsense knowledge. However, current dialogue summarization systems  ignore the exploration of commonsense knowledge, which may limit the performance. In this work, we examine the benefit of incorporating commonsense knowledge in the dialogue summarization task and also address the question of how best to incorporate this information. Figure  shows a positive example to illustrate the effectiveness of commonsense knowledge in the dialogue summarization task.  Bob asks Tom for help because his car has broken down. On the one hand, by introducing commonsense knowledge according to the pick up and car broke down, we can know that Bob expects Tom to give him a lift. On the other hand, commonsense knowledge can serve as a bridge between non-adjacent utterances that can help the model better understanding the dialogue.  In this paper, we follow the previous setting  and also use ConceptNet  as a large-scale commonsense knowledge base, while the difference is that we regard knowledge and text as heterogeneous data in a real multi-speaker dialogue. We propose a model named Dialogue Heterogeneous Graph Network  for incorporating commonsense knowledge by constructing the graph including both utterance and knowledge nodes. Besides, our heterogeneous graph also contains speaker nodes at the same time, which has been proved to be a useful feature in dialogue modeling. In particular, we equip our heterogeneous graph network with two additional designed modules. One is called message fusion, which is specially designed for utterance nodes to better aggregate information from both speakers and knowledge. The other one is called node embedding, which can help utterance nodes to be aware of position information. Compared to homogeneous graph network in related works , we claim that the heterogeneous graph network can effectively fuse information and contain rich semantics in nodes and links, and thus more accurately encode the dialogue representation.   We conduct experiments on the SAMSum corpus , which is a large-scale chat summarization corpus. We analyze the effectiveness of integration of knowledge and heterogeneity modeling. The human evaluation also shows that our approach can generate more abstractive and correct summaries. To evaluate whether commonsense knowledge can help our model better generalize to the new domain, we also perform zero-shot setting experiments on the Argumentative Dialogue Summary Corpus , which is a debate summarization corpus. In the end, we give a brief summary of our contributions:  We are the first to incorporate commonsense knowledge into dialogue summarization task.  We propose a D-HGN model to encode the dialogue by viewing utterances, knowledge and speakers as heterogeneous data.  Our model can outperform various methods.  
"," Abstractive dialogue summarization is the task of capturing the highlights of a dialogue and rewriting them into a concise version. In this paper, we present a novel multi-speaker dialogue summarizer to demonstrate how large-scale commonsense knowledge can facilitate dialogue understanding and summary generation. In detail, we consider utterance and commonsense knowledge as two different types of data and design a Dialogue Heterogeneous Graph Network  for modeling both information. Meanwhile, we also add speakers as heterogeneous nodes to facilitate information flow. Experimental results on the SAMSum dataset show that our model can outperform various methods. We also conduct zero-shot setting experiments on the Argumentative Dialogue Summary Corpus, the results show that our model can better generalized to the new domain.",139
" %\yy{para 1: problem is important, para 2: temporal graph, existing systems, para 3: neural networks, para 4: why difficult: lack of training data, para 5: what do we do}  %\yy{this is a comment} %\yyc{before correction}{after correction}   %The flow of time is used to chain narratives, reason about causes and effects of events, form a deeper understanding of the past, and postulate the future. Temporal reasoning is crucial for analyzing the interactions among complex events and producing   coherent interpretations of text data . There is a rich body of research on the use of temporal information in a variety of important application domains, including topic detection and tracking, information extraction, parsing of clinical records , discourse analysis, and question answering. %\yy{Aman: Please update the cites based on some quick Google search on temporal reasoning/expressions in IE/TDT/medical .} %Motivated by its ubiquity in text understanding, we undertake the task of extracting temporal graphs from documents. %and a rich understanding of temporal aspects of a document helps humans in reading comprehension.   %Temporal reasoning also plays a critical role in downstream natural language processing  tasks like    Graphs are a natural choice for representing the temporal ordering among events, where the nodes are the individual events, and the edges capture temporal relationships such as ``before'', ``after'' or ``simultaneous''. Representative work on automated extraction of such graphs from textual documents includes the early work by~\citet{chambers2009unsupervised}, where the focus is on the construction of event chains from a collection of documents, and the more recent \caevo and \cct, which extract a graph for each input document instead.   These methods focus on rule-based and statistical sub-modules to extract verb-centered events and the temporal relations among them. %Specifically, given a document, our system extracts a temporal event graph, where the nodes of the graph are the events, and the edges capture the temporal order~ between them. %Classical temporal information extraction systems focus on one of the two broad themes of relation identification and temporal relation classification. %Relation identification is the task of identifying events that can be connected by a temporal relation. %The task of temporal relation involves identifying the temporal relationship that exists between the given two events. % For example, in the sentence I had a coffee while I was getting a haircut, the phrase while I was expresses the fact that the events of drinking a coffee and getting a haircut took place at the same time. %For example, given the sentence I had a coffee while I was getting a haircut, a relation identification system would identify events had a coffee and getting a haircut. %A temporal relation classification system would then determine that the events happened simultaneously. %Our goal is to create a system that can perform both these tasks together in an end-to-end fashion over multiple sentences.  %The idea of extracting temporal graphs from a given document is not new. %Tempeval-3 introduced a task specifically to this end. %The idea of extracting events and the temporal links between them as a graph was proposed in Tempeval-3. %However, the evaluation still relied on a set of pre-identified events from the TimeBank corpus, leading most of the teams to focus on relation classification. %Despite its importance, the task has received limited attention. %Representative temporal graph extraction systems like  \caevo and \cct break down the problem into sub-tasks, like event identification and relation extraction, and then employ rule-based and statistical systems to solve each sub-task. %Additionally, they use small amounts of hand-labeled corpora for their development, limiting their generalizability and scalability. As an emerging area of nlp, large scale pre-trained language models have made strides in addressing challenging tasks like commonsense knowledge graph completion and task-oriented dialog generation. %Besides relying on an intricate arrangement of sub-systems, they have some common shortcomings: i) They either admit a lot of noisy events  or ignore events from the secondary narrative , ii) generate one-word verbs as events, without adding any context, iii) have limited generalization capabilities by way of relying on rules or small training corpora. % These systems typically fine-tune large language models like gpt or \gptz on a corpus of task-specific dataset. These systems typically fine-tune large language models on a corpus of a task-specific dataset. %However, these advances have not benefited temporal graph extraction.  However, these techniques have not been investigated for temporal graph extraction.  This paper focuses on the problem of generation of an event-level temporal graph for each document, and we refer to this task as contextualized graph generation.   We address this open challenge by proposing a novel reformulation of the task as a sequence-to-sequence mapping problem, which enables us to leverage large pre-trained models for our task. Further, our proposed approach is completely end-to-end and eliminates the need for a pipeline of sub-systems commonly used by traditional methods. %This helps approach is end-to-end, it is not only easier to implement,  approach prevents error propagation across stages and minimizes the effort required for feature engineering.  We also address a related open challenge, which is a prerequisite to our main goal: the difficulty of obtaining a large quantity of training graphs with human-annotated events and temporal relations.   %We address this second challenge with an unsupervised approach, i.e., to  % To this end, we automatically produce a large collection of document-graph pairs by applying existing information extraction and \nlp tools to textual documents, followed by a few rule-based post-processing steps for pruning and noise reduction. Specifically, using \caevo and other tools, we generate a large collection of 89,000 document/graph pairs. To this end, we automatically produce a large collection of document-graph pairs by using \caevo, followed by a few rule-based post-processing steps for pruning and noise reduction.  %Specifically, using \caevo and other tools, we generate a large collection of 89,000 document/graph pairs. %. %which facilitates both large-scale fine-tuning as well as large-scale evaluation of our new approach in comparison with other competing methods.} %The primary block for this union remains the data-hungry nature of large language models; they typically require sizeable datasets for effective training, while popular temporal corpora usually only offer tens to hundreds of hand-labeled documents. %Given that large scale pre-trained language models  %Despite its importance, temporal graph extraction has not benefited from the recent advances in large scale pre-trained language models, which have been effective for  %The limitation does not lie in their representative capabilities.  %Rather, the lack of training data forms the  %The lack of training data forms the biggest bottleneck for this unification: large scale language models typically require large datasets for effective training. Simultaneously, popular temporal corpora usually have tens to hundreds of documents. % bridge this gap by generating a large corpus of 89k document-graph pairs for the task. %We achieve this by first using \caevo as a cheap supervision mechanism for creating a large corpus of dense temporal graphs. %Admittedly, the data generated by \caevo has considerable amounts of error and noise. %We alleviate these issues by injecting human knowledge in the \caevo generated data by applying several post-processing strategies. % Specifically, we remove noisy events and relations extracted with low confidence, and use event clusters to map each graph to its correct context. We then encode the graph in each training pair as a string in the graph representation format \dotlang, transforming the text-to-graph mapping into sequence-to-sequence mapping. %task. We fine-tune \gptz on this dataset of document-graph pairs, which yields large performance gains over strong baselines on system generated test set and outperforms \caevo on TimeBank-Dense on multiple metrics. Figure 1 shows an example of the input document and the generated graph by our system. %While automatic labeling cannot rival human-curation in quality, our strong experimental results show that the dataset prepared by our method provides a competitive signal to noise ratio at virtually zero cost.  %allowing strong learners to generalize on unseen data. %and use masked-language modeling with \gptz for estimating the conditional distribution of temporal graphs given a document. %Our experiments with \gptz show large gains over strong baselines on our dataset and outperforms \caevo on TimeBank-Dense on a range of metrics. %In the process, we answer several practical questions on selecting salient events, identifying the context for a temporal graph, graph representation, and evaluation. %The system trained on strong results on our data and outperforming \caevo on TimeBank-Dense on a range of metrics. %TODO we are the first to  %Qualitative analysis of nodes generated from our method shows that our approach can successfully use the large training corpus for learning generalized patterns of temporal relations, with error analysis on a held-out set revealing that \caevo fixes the labels for 10\% of the cases. % We use \caevo to label a large corpus of documents and apply novel pruning techniques on top of graphs generated by \caevo.  % These pruning techniques retain the high confidence annotations by \caevo while removing noisy events and relations. % Further, the context for each graph is automatically discovered using the notion of event communities, obviating the need for any hardcoded cutoffs typically adopted in temporal systems. In summary, our main contributions are: %\am{write about three contributions: i) annotation pipeline, ii) encoding to strings, thus allowing the use of gpt, iii) strong results on our data, very good result on \tbden, dramatic improvements over off-the-shelf \gptz}  % % File acl2020.tex % %% Based on the style files for ACL 2020, which were %% Based on the style files for ACL 2018, NAACL 2018/19, which were %% Based on the style files for ACL-2015, with some improvements %%  taken from the NAACL-2016 style %% Based on the style files for ACL-2014, which were, in turn, %% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009, %% EACL-2009, IJCNLP-2008... %% Based on the style files for EACL 2006 by  %%e.agirre@ehu.es or Sergi.Balari@uab.es %% and that of ACL 08 by Joakim Nivre and Noah Smith  \documentclass[11pt,a4paper]{article} \usepackage[hyperref]{acl2020} \usepackage{times} \usepackage{latexsym} \renewcommand{\UrlFont}{\ttfamily\small} \usepackage{color} \usepackage{xspace} \usepackage{amsmath} \usepackage{amsfonts} \usepackage{multirow} \usepackage[multiple]{footmisc} \usepackage{array, booktabs, makecell} \usepackage{graphicx} \usepackage{colortbl} \usepackage{xcolor} \setlength{\textfloatsep}{0.1cm} % This is not strictly necessary, and may be commented out, % but it will improve the layout of the manuscript, % and will typically save some space. \usepackage{microtype}  %\aclfinalcopy % Uncomment this line for the final submission %\def\aclpaperid{***} %  Enter the acl Paper ID here  %\setlength\titlebox{5cm} % You can expand the titlebox if you need extra space % to show all the authors. Please do not make the titlebox % smaller than 5cm ; we will check this % in the camera-ready version and ask you to change it back.  \newcommand\BibTeX{Bib\TeX}  \title{Neural Language Modeling for Contextualized Temporal Graph Generation} \aclfinalcopy \author{Aman Madaan, Yiming Yang \\   Language Technologies Institute, Carnegie Mellon University \\   Pittsburgh, PA, USA \\    \\}  \date{}   
"," This paper presents the first study on using large-scale pre-trained language models for automated generation of an event-level temporal graph for a document. Despite the huge success of neural pre-training methods in NLP tasks, its potential for temporal reasoning over event graphs has not been sufficiently explored. Part of the reason is the difficulty in obtaining large training corpora with human-annotated events and temporal links. We address this challenge by using existing IE/NLP tools to automatically generate a large quantity  of system-produced document-graph pairs, and propose a novel formulation of the contextualized graph generation problem as a sequence-to-sequence mapping task. These strategies enable us to leverage and fine-tune pre-trained language models on the system-induced training data for the graph generation task. Our experiments show that our approach is highly effective in generating structurally and semantically valid graphs. Further, evaluation on a challenging hand-labeled, out-domain corpus shows that our method outperforms the closest existing method by a large margin on several metrics.\footnote{Code and pre-trained models available at}",140
" Building dialog systems typically requires a large collection of conversation logs that a model can use as training data. Crowd-sourcing is a popular method for generating such data-sets and depending on the aspect of dialog modeling being studied, crowd-sourced workers may be asked to annotate existing chat logs for intents and dialog acts, create dialog summaries, converse with each other based on a script or converse to accomplish tasks or goals etc. For instance, to create datasets for task oriented dialogs, crowd-sourced workers may be provided with a goal  that describes the task that needs to be accomplished; workers then play the roles of a user and an agent to generate conversations . % that plays the role of user.   The user worker begins the conversation by stating their requirement and the agent worker provides information to the user by querying a knowledge base , if required. Together, the two workers interact with each other via natural language to generate conversations that can involve booking restaurant tables, making train reservations, calling a taxi etc. However, creating large crowd-sourced datasets can be time consuming and expensive.           
"," Popular task-oriented dialog data sets such as MultiWOZ \cite{Multiwoz} are created by providing crowd-sourced workers a goal instruction, expressed in natural language, that describes the task to be accomplished. Crowd-sourced workers play the role of a user and an agent to generate dialogs to accomplish tasks involving booking restaurant tables, making train reservations, calling a taxi etc. However, creating large crowd-sourced datasets can be time consuming and expensive. To reduce the cost associated with generating such dialog datasets, recent work has explored methods to automatically create larger datasets from small samples.  %Popular dialog data sets such as MultiWoz \cite{Multiwoz} are created by providing crowd-sourced workers a goal instruction, expressed in natural language, which described the task that needed to be accomplished. Crowd-sourced workers played the role of a user and an agent to generate dialogs that can involve booking restaurant tables, making train reservations, calling a taxi etc.  In this paper, we present a data creation strategy that uses the pre-trained language model, GPT2 \cite{GPT2}, to simulate the interaction between crowd-sourced workers by creating a user bot and an agent bot.  We train the simulators using a smaller percentage of actual crowd-generated conversations and their corresponding goal instructions. We demonstrate that by using the simulated data, we achieve significant improvements in both low-resource setting as well as in overall task performance. To the best of our knowledge we are the first to present a model %this is the first model proposed  for generating entire conversations by simulating the crowd-sourced data collection process. %To the best of our knowledge we are the first to use inter-bot conversation logs to improve the performance of task oriented dialog systems.",141
" Multilingual machine translation , which can serve multiple language pairs with a single model, has attracted much attention. In contrast to bilingual MT systems which can only serve one single language pair, multilingual models can serve  language pairs  .  The amount of available training data can differ a lot across language pairs and the majority of available MT training data is English-centric  which in practice means that most non-English language pairs do not see a single training example when training multilingual models .  As a consequence, the actual performance of language pairs that do not include English on the source or target side lags behind the ones with large amounts of training data. Further, when increasing the number of languages, it gets  impractical to gather training data for each language pair and  challenging to find the right mix during training. Which is why models tasked with direct translation between non-English pairs either resort to bridging  through a pivot language , or make use of synthetic parallel data   or study the problem under zero-shot settings .     In this study, we make use of the potential pre-existing multi-way property in the training corpora and generate as many direct training examples from pre-existing English-centric training data. If we can find training examples for each language pair in a multilingual mix, we call this model complete Multilingual Neural Machine Translation . cMNMT is then trained on all bilingual pairs between source and target languages by utilizing multi-way aligned training examples that consist of translations of the same sentence into multiple languages. We resurface multi-way aligned training examples by aligning training examples from different language pairs when either their source or target sides are identical .  To make use of this data, the model samples a source and target language from the set of multi-way aligned corpus during training, which allows the model to see language pairs where originally no training data existed . As our experiments support, this method enables us to get access to training data for all tested language pairs ). We will show that it is possible to generate a complete graph for at least a 6-language WMT setup. Some of the WMT training data is multi-way parallel by construction. Nevertheless, we show that we also find many training examples where the source and target origin from different sources. We further show on our 112 languages internal dataset, that we can find sufficient training data for over 12,000 language pairs by only providing 111 English-centric training corpora. This result indicates that it is possible to generate direct training data for many language pairs without the need for crawling new training examples. Our experiments suggest that before falling back to methods like zero-shot translation, you should investigate the structure of your pre-existing training data.  To address the problem of finding the right mix of examples from different language pairs during training, we further introduce a hierarchical sampling strategy that is language-specific . In addition to fixing some chronic issues of MNMT , the proposed sampling strategy efficiently ensures all source-target pairs are covered.  Experiments demonstrate that we can train a cMNMT model on a 30-language-pair WMT setup that outperforms bilingual and multilingual baselines as well as bridging on all non-English language pairs. We further show that the performance of the English language pairs stay stable and do not suffer from the changes in both the training data and the new training data sampling strategy. Furthermore, we share experiments at scale by demonstrating that we can train a cMNMT model that can serve  12,432 language pairs.  Our contribution is three-fold:    
"," Multilingual Neural Machine Translation  models are commonly trained on a joint set of bilingual corpora which is acutely English-centric . While direct data between two languages that are non-English is explicitly available at times, its use is not common. In this paper, we first take a step back and look at the commonly used bilingual corpora , and resurface the existence and importance of implicit structure that existed in it: multi-way alignment across examples . We set out to study the use of multi-way aligned examples to enrich the original English-centric parallel corpora. We reintroduce this direct parallel data from multi-way aligned corpora between all source and target languages. By doing so, the English-centric graph expands into a complete graph, every language pair being connected. We call MNMT with such connectivity pattern complete Multilingual Neural Machine Translation  and demonstrate its utility and efficacy with a series of experiments and analysis. In combination with a novel training data sampling strategy that is conditioned on the target language only, cMNMT yields competitive translation quality for all language pairs. We further study the size effect of multi-way aligned data, its transfer learning capabilities and how it eases adding a new language in MNMT. Finally, we stress test cMNMT at scale and demonstrate that we can train a cMNMT model with up to 111$*$112=12,432 language pairs that provides competitive translation quality for all language pairs.",142
"  Machine Translation  has shown impressive progress in recent years. Neural architectures have greatly contributed to this  improvement, especially for languages with abundant training data.  This progress creates novel challenges for the evaluation of machine translation,  both for human and automated evaluation  protocols.  Both types of evaluation play an important role in machine translation. While human evaluations provide a gold standard evaluation, they involve a fair amount of careful and hence expensive work by human assessors. Cost therefore limits the scale of their application. On the other hand, automated evaluations are much less expensive. They typically only involve human labor when collecting human reference translations and can hence be run at scale to compare a wide range of systems or validate design decisions. The value of automatic evaluations  therefore resides in their capacity to be used as a proxy for human evaluations for large scale comparisons and system development.  The recent progress in MT has raised concerns about whether automated evaluation methodologies reliably reflect human ratings in high accuracy ranges. In particular, it has been observed that the best systems according to humans might fare less well with automated metrics. Most metrics such as \BLEU and TER measure overlap between a system output and a human reference translation. More refined ways to compute such overlap have consequently been proposed.  Orthogonal to the work of building improved metrics,  hypothesized that human references are also an important factor in the reliability of automated evaluations. In particular, they observed that standard references exhibit simple, monotonic language due to human  `translationese` effects. These standard references might favor systems which excel at reproducing these effects, independent of the underlying translation quality. They showed that better correlation between human and automated evaluations could be obtained when replacing standard references with paraphrased references, even when still using surface overlap metrics such as BLEU~. The novel references, collected by asking linguists to paraphrase standard references, were shown to steer evaluation away from rewarding translation artifacts. This improves the assessment of alternative, but equally good translations.  Our work builds on the success of paraphrased translations for evaluating  existing systems, and asks if different design choices could have been made when designing a system with such an evaluation protocol in mind. This examination has several potential benefits: it can help identify choices which improve BLEU on standard references but have limited impact on final human evaluations; or those that result in better translations for the human reader, but worse in terms of standard reference BLEU. Conversely, it might turn out that paraphrased references are not robust enough to support system development due to the presence of `metric honeypots': settings that produce poor translations, but which are nevertheless assigned high BLEU scores.  To address these points, we revisit the major design choices of the best EnglishGerman system from WMT2019 step-by-step, and measure their impact on standard reference BLEU as well as on paraphrased BLEU. This allows us to measure the extent to which steps such as data cleaning, back-translation, fine-tuning, ensemble decoding and reranking benefit standard reference BLEU more than paraphrase BLEU. Revisiting these development choices with the two metrics results in two systems with quite different behaviors. We conduct a human evaluation for adequacy and fluency to assess the overall impact of designing a system using paraphrased BLEU.  Our main findings show that optimizing for paraphrased BLEU is advantageous for human evaluation when compared to an identical system optimized for standard BLEU. The system optimized for paraphrased BLEU significantly improves WMT newstest19 adequacy ratings  and fluency ratings  despite scoring 5 BLEU points lower on standard references.  
","  Automatic evaluation comparing candidate translations to human-generated paraphrases of reference translations has recently been proposed by \newcite{freitag2020bleu}. When used in place of original references, the paraphrased versions produce metric scores that correlate better with human judgment. This effect holds for a variety of different automatic metrics, and tends to favor natural formulations over more literal  ones. In this paper we compare the results of performing end-to-end system development using standard and paraphrased references. With state-of-the-art English-German NMT components, we show that tuning to paraphrased references produces a system that is significantly better according to human judgment, but 5 BLEU points worse when tested on standard references. Our work confirms the finding that paraphrased references yield metric scores that correlate better with human judgment, and demonstrates for the first time that using these scores for system development can lead to significant improvements.",143
" 	 	 	% 	% The following footnote without marker is needed for the camera-ready 	% version of the paper. 	% Comment out the instructions  and uncomment the 8 lines 	% under ""final paper"" for your variant of English. 	%  	 	Machine Reading Comprehension  has made significant strides with an array of neural models rapidly approaching human parity on some benchmarks such as SQuAD . However, existing methods are still in their infancy at the level of cognitive intelligence. Recently, brain science and psychology provide an important basis for the development of brain-like computing and the simulation of human perception, thinking, understanding, and reasoning abilities . 	 	Thinking is the generalization and indirect reflection of the human brain on the nature, interrelationships and internal regularities of objective things . Two types of thinking are complementary in psychology: inertial thinking 閳 from a previous to a subsequent stimulus 閳 and reverse thinking 閳 from a subsequent to a previous stimulus . Inertial thinking  is a conventional way of thinking, which thinks and solves problems from the previous ideas. 	% It is more likely to form a stereotyped thinking, hinder the development of thinking, and may even lead to rigidity of thinking if using this single way for a long time. 	Reverse thinking  is a creative way of thinking, opposite to the inertial thinking.  	% It can often break through the conventional constraints and obtain greater innovation .  	Specifically, in the MRC task, the two types of thinking can be regarded as a process which reasons from questions  to answers . For example, as shown in Fig. , we can get the answer easily by locating the entities pregnant wowen and loquat. Contrarily, the generative question, which can be reasoned by reading the answer and passage, describes two aspects, including can pregnant women eat loquat and what is the benefit to eat loquat for pregnant women. We hope that this ability of reverse reasoning can improve performance on reading comprehension tasks. 	 	 	Previous methods  only consider a obverse logical relationship, which is based on the given question and the passage. They ignore the reverse relationship between the given passage and the answer. Although the work  proposes a joint model that both asks and answers questions, it couples all the knowledge rather than decopuling modules, which is consistent with the concept of psychology. Similarly, we hypothesize that the ability of reverse reasoning can help models achieve better QA performance. This is motivated partly by observations made in psychology that devising questions while reading can  	% increase scores on comprehension tests.It can  	help students improve in reader-based processing of text . 	 	%In the real world, a fast learning trick is to peek at the answer first. In other words, humans often begin with answers and passages when they encounter unsolvable problems. 	%% Then they attempt to understand why the answer is like this.  	%Then they inadvertently infer the question based on the answer and passage from the reverse side. Obtaining the answer in advance is equivalent to giving a strong supervision signal or some additional clues that does not directly exist in the passage and may require reasoning. This kind of psychological behavior about humans is actually a way of reverse thinking, which considers the problem from the opposite direction and infers the reason based on the conclusion . 	 	Therefore, insights into solutions to the problem can be gained from human cognitive processes. Complementary Learning Systems Theory   suggests that the human brain contains complementary learning systems that support the simultaneous use of many sources of information as we seek to understand an experienced situation. One of the systems acquires an integrated system of knowledge gradually through interleaved learning, including our knowledge of the meanings of words, the properties of frequently-encountered objects, and the characteristics of familiar situations. It is just like inertial thinking that learns relationships between different things in the real world for a long time. The other system is a fast learning system similar to reverse thinking, which is targeted to focus on stimulating and enhancing infrequently-utilized circuit areas in the brain from another unusual perspective. 	 	In this paper, we propose the Bi-directional Cognitive Knowledge Framework . And the corresponding Bi-directional Cognitive Thinking Network  is designed to validate the effectiveness of the reverse thinking, as shown in Fig., which will be introduced in detail in Section . 	% explain framework 	%  	 	% In this paper, the proposed method that ... simulates the process of fast learning. 	 	% 	 	 	 	% The gray ovals form an embedding  of specific information 	 	% In order to validate the effectiveness of the reverse thinking, we proposed a Cognitive Bi-directional Thinking Network  corresponding to the Cognitive Bi-directional Thinking Framework, which will be introduced in detail in Section . 	 	The contributions can be summarized as follows: 	 	 	 	
"," 		We propose a novel Bi-directional Cognitive Knowledge Framework  for reading comprehension from the perspective of complementary learning systems theory. It aims to simulate two ways of thinking in the brain to answer questions, including reverse thinking and inertial thinking. To validate the effectiveness of our framework, we design a corresponding Bi-directional Cognitive Thinking Network  to encode the passage and generate a question  given an answer  and decouple the bi-directional knowledge. The model has the ability to reverse reasoning questions which can assist inertial thinking to generate more accurate answers. Competitive improvement is observed in DuReader dataset, confirming our hypothesis that bi-directional knowledge helps the QA task. The novel framework shows an interesting perspective on machine reading comprehension and cognitive science.",144
"     % Demonstrating intelligent behavior in complex environments requires agents that can reason about entities and their relationships, and identify regularities in structured data which can help predict the properties-of and relationships-between entities.  % Understanding natural language in realistic settings requires models that can reason about the interactions between content and context, model the dependencies between different textual elements and leverage information about authors when interpreting their content.  For example, when analyzing interactions in a social network, leveraging information about users' social behavior can help identify similarities in the contents of posts they author. Dealing with this type of relational data requires making predictions over multiple, often inter-dependent, variables.    Understanding natural language interactions in realistic settings requires models that can deal with noisy textual inputs, reason about the dependencies between different textual elements and leverage the dependencies between textual content and the context from which it emerges. Work in linguistics and anthropology has defined context as a frame that surrounds a focal communicative event and provides resources for its interpretation . %\citealt{contextualization-92} introduced the term contextualization cues as signalling mechanisms in communication that add to the shared understanding between the participants, into relationships, the situation, and the environment of the conversation    %Say something about debate networks and add some references. As a motivating example, consider the interactions in the debate network  described in Fig.. Given a debate claim , and two consecutive posts debating it , we define a textual inference task, determining whether a pair of text elements hold the same stance in the debate }). This task is similar to other textual inference tasks which have been successfully approached using complex neural representations. In addition, we can leverage the dependencies between these decisions.  For example, assuming that one post agrees with the debate claim }}), and the other one does not }}), the disagreement between the two posts can be inferred:  {\small \PRED{\neg Agree\wedge Agree \rightarrow \neg Agree}}. Finally, we consider the social context of the text. The disagreement between the posts can reflect a difference in the perspectives their authors hold on the issue. While this information might not be directly observed, it can be inferred using the authors' social interactions and behavior. % Given the principle of social homophily, stating that people with strong social ties are likely to hold similar views and authors' perspectives can be captured by representing their social interactions. Exploiting this information requires models that can align the social representation with the linguistic one.  Motivated by these challenges, we introduce \DRAIL, a Deep Relational Learning framework, which uses a combined neuro-symbolic representation for modeling the interaction between multiple decisions in relational domains. Similar to other neuro-symbolic approaches our goal is to exploit the complementary strengths of the two modeling paradigms. Symbolic representations, used by logic-based systems and by probabilistic graphical models, are interpretable, and allow domain experts to directly inject knowledge and constrain the learning problem. Neural models capture dependencies using the network architecture and are better equipped to deal with noisy data, such as text. However, they are often difficult to interpret and constrain according to domain knowledge.   Our main design goal in \DRAIL is to provide a generalized tool, specifically designed for NLP tasks. Existing approaches designed for classic relational learning tasks, such as knowledge graph completion, are not equipped to deal with the complex linguistic input. While others are designed for very specific NLP settings such as word-based quantitative reasoning problems or aligning images with text. We discuss the differences between \DRAIL and these approaches in Section.  % While the examples in this paper focus on modelings various argumentation mining tasks and their social and political context, the same principles can be applied to wide array of NLP tasks with different contextualizing information, such as images that appear next to the text, or prosody when analyzing transcribed speech, to name a few examples. %TODO: explain why DRAIL is specifically useful for NLP compared to other languages. We don't do the same type of evaluation  as we are interested in working with raw entities.     %  Entities in \DRAIL are either human-interpretable discrete entities , which we refer to as symbols, or raw entities that have a complex internal structure which cannot be easily represented as a symbol . This view allows us to define two conceptual learning tasks: relations connecting raw and symbolic entities }}), and relations connecting raw inputs to each other, which define inference tasks }}).   % \DRAIL uses a declarative language for defining deep relational models. Similar to other declarative languages, it allows users to inject their knowledge by specifying dependencies between decisions using first-order logic rules, which are later compiled into a factor graph with neural potentials.   % In addition to probabilistic inference, \DRAIL also models dependencies using a distributed knowledge representation, denoted \relnets, which provides a shared representation space for entities and their relations, trained using a relational multi-task learning approach. This provides a mechanism for explaining symbols, and aligning representations from different modalities.  %Introduce the s-s, r-r, s-r, distinction as a way to support classification, textual inference, and probabilistic inference. Following our running example, ideological standpoints, such as \PRED{Liberal} or \PRED{Conservative}, are discrete entities embedded in the same space as textual entities and social entities. These entities are initially associated with users, however using \relnets this information will propagate to texts reflecting these ideologies, by exploiting the relations that bridge social and linguistic information . % In the resulting shared embedding space, we can explain these ideological standpoints in terms of users holding them, or texts that express them.%}).    %TODO: what are the research questions    %TODO - explain the difference in task from DRAIL's perspective - argument relations inside a single text, analyzing discussions - the simple case, discussed in the literature, where we predict a symbol , and the debate.org setup where we combine textual inference  with soclia linfo    To demonstrate \DRAIL's modeling approach, we introduce the task of open-domain stance prediction with social context, which combines social networks analysis and textual inference over complex opinionated texts, as shown in Fig. . %Unlike traditional stance prediction tasks, where the prediction problem is defined over a fixed set of issues  ~ , we go beyond coarse-grained definitions, and delve into the specific arguments and questions of each discussion, as shown in Fig. . We follow the intuition that debates are part of a broader online conversation, involving multiple people that contribute or express their support for the different views, and explicitly model these interactions.  % AugensteinD16-1084,P18-2123,C18-1316} %TODO: add some discussion about qualitative evaluation % We complement our evaluation of \DRAIL with two additional tasks, issue-specific stance prediction, where we identify the views expressed in debate forums with respect to a set of fixed issues, and argumentation mining, a document-level discourse analysis task.    %We demonstrate \DRAIL's modeling approach over three challenging problems. Argumentation mining, a document-level discourse analysis task. Debate stance prediction, identifying the views expressed-in, and interactions-between, debate forum posts. Finally, we introduce a new problem, open-domain stance prediction with social context, which combines social networks analysis and textual inference over complex opinionated texts.  In all three tasks we evaluate different modeling choices, obtaining competitive results.    %TODO: contributions %Our contributions are summarized as follows: % %    %Unrealted TODO: add a discussion about globally normalized RELNETs- the constraints and the multiple objectives shape them.  %homophily, %, This phenomenon was previously used to help overcome language variation issues   % political-social representations %network embedding:we learn a graph embedding, a different way to define social context %graphical models way  
"," Building models for realistic natural language tasks requires dealing with long texts and accounting for complicated structural dependencies. Neural-symbolic representations have emerged as a way to combine the reasoning capabilities of symbolic methods, with the expressiveness of neural networks. However, most of the existing frameworks for combining neural and symbolic representations have been designed for classic relational learning tasks that work over a universe of symbolic entities and relations. In this paper, we present \DRAIL, an open-source declarative framework for specifying deep relational models, designed to support a variety of NLP scenarios. Our framework supports easy integration with expressive language encoders, and provides an interface to study the interactions between representation, inference and learning.",145
"   End-to-end neural models have emerged in recent years as the dominant approach to a wide variety of sequence generation tasks in natural language processing, including speech recognition, machine translation, and dialog generation, among many others. While highly accurate, these models typically operate by outputting tokens from a predetermined symbolic vocabulary, and require integration into larger pipelines for use in user-facing applications such as voice assistants where neither the input nor output modality is text.  In the speech domain, neural methods have recently been successfully applied to end-to-end speech translation , in which the goal is to translate directly from speech in one language to speech in another language. We propose to study the analogous problem of in-image machine translation. Specifically, an image containing text in one language is to be transformed into an image containing the same text in another language, removing the dependency of any predetermined symbolic vocabulary or processing.  \paragraph{Why In-Image Neural Machine Translation ?} In-image neural machine translation is a compelling test-bed for both research and engineering communities for a variety of reasons. Although there are existing commercial products that address this problem such as image translation feature of Google Translate the underlying technical solutions are unknown. By leveraging large amounts of data and compute, end-to-end neural system could potentially improve overall quality of pipelined approaches for image translation. \iffalse First, existing commercial products that address this problem such as the image translation feature of Google Translate employ a traditional pipelined approach consisting of separate optical character recognition, translation, and image rendering steps.\todo{orhanf will check this with mobile team. Elman: commented out as suggested by mobile wordlens team. technical solution of wordlens is not publicly available hence this sentence is a bit speculative}  Combining all these components into a single end-to-end neural system could help reduce cascading errors and improve overall translation quality, leveraging large amounts of data and compute. \fi Second, and arguably more importantly, working directly with pixels has the potential to sidestep issues related to vocabularies, segmentation, and tokenization, allowing for the possibility of more universal approaches to neural machine translation, by unifying input and output spaces via pixels.  Text preprocessing and vocabulary construction has been an active research area leading to work on investigating neural machine translation systems operating on subword units , characters  and even bytes  and has been highlighted to be one of the major challenges when dealing with many languages simultaneously in multilingual machine translation , and cross-lingual natural language understanding . Pixels serve as a straightforward way to share vocabulary among all languages at the expense of being a significantly harder learning task for the underlying models.  In this work, we propose an end-to-end neural approach to in-image machine translation that combines elements from recent neural approaches to the relevant sub-tasks in an end-to-end differentiable manner. We provide the initial problem definition and demonstrate promising first qualitative results using only pixel-level supervision on the target side. We then analyze some of the errors made by our models, and in the process of doing so uncover a common deficiency that suggests a path forward for future work.  
"," In this paper, we offer a preliminary investigation into the task of in-image machine translation: transforming an image containing text in one language into an image containing the same text in another language. We propose an end-to-end neural model for this task inspired by recent approaches to neural machine translation, and demonstrate promising initial results based purely on pixel-level supervision. We then offer a quantitative and qualitative evaluation of our system outputs and discuss some common failure modes. Finally, we conclude with directions for future work.",146
" Transformer based models  have been proven to be very effective in building the state-of-the-art Neural Machine Translation  systems via neural networks and attention mechanism . Following the standard Sequence-to-Sequence architecture, Transformer models consist of two essential components, namely the encoder and decoder, which rely on stacking several identical layers, i.e., multihead attentions and position-wise feed-forward network.   Multihead attentions and position-wise feed-forward network, together as a basic unit,  plays an essential role in the success of Transformer models.  Some researchers  propose to improve the model capacity by stacking this basic unit many times, i.e., deep Transformers, and achieve promising results. Nevertheless, as an orthogonal direction, investigation over multiple parallel units draws little attention.   Compared with single unit models, multiple parallel unit layout is more expressive to capture complex information flow  in two aspects. First, this multiple-unit layout boosts the model by its varied feature space composition and different attentions over inputs. With this diversity, multi-unit models advance in expressiveness.  Second, for the multi-unit setting, one unit could mitigate the deficiency of other units and compose a more expressive network, in a complementary way.  In this paper, we propose the Multi-Unit TransformErs , which aim to promote the expressiveness of transformer models by introducing diverse and complementary parallel units. Merely combining multiple identical units in parallel improves model capability and diversity by its varied feature compositions. Furthermore, inspired by the well-studied bagging  and gradient boosting algorithms  in the machine learning field, we design biased units with a sequential dependency to further boost model performance.  Specifically, with the help of a module named bias module, we apply different kinds of noises to form biased inputs for corresponding units. By doing so, we explicitly establish information gaps among units and guide them to learn from each other.  Moreover, to better leverage the power of complementariness, we introduce sequential ordering into the multi-unit setting, % by learning a permutaion matrix  to automatically shuffle the outputs of multiple units,  and force each unit to learn the residual of its preceding accumulation.  We evaluate our methods on three widely used Neural Machine Translation datasets, NIST Chinese-English,  WMT'14 English-German and WMT'18 Chinese-English. Experimental results show that our multi-unit model yields an improvement of +1.52, +1.90 and +1.10 BLEU points, over the baseline model  for three tasks with different sizes, respectively.  Our model even outperforms the Transformer-Big on the WMT'14 English-German by 0.7 BLEU points with only 54\% of parameters.  Moreover, as an interesting side effect, our model only introduces mild inference speed decrease  compared with the Transformer-Base model, and is faster than the Transformer-Big model. % which proves the practicability of our methods.   The contributions of this paper are threefold:   
","     Transformer models \cite{vaswani2017attention} achieve remarkable success in Neural Machine Translation.      Many efforts have been devoted to deepening the Transformer by stacking several units  in a cascade,      while the investigation over multiple parallel units draws little attention.     In this paper, we propose the Multi-Unit TransformErs , which aim to promote the expressiveness of the Transformer by introducing diverse and complementary units.     Specifically, we use several parallel units and show that modeling with multiple units improves model performance and introduces diversity.      Further, to better leverage the advantage of the multi-unit setting, we design biased module and sequential dependency that guide and encourage complementariness among different units.      % need more results and exciting data.     Experimental results on three machine translation tasks, the NIST Chinese-to-English, WMT'14 English-to-German and WMT'18 Chinese-to-English, show that the MUTE models significantly outperform the Transformer-Base, by up to +1.52, +1.90 and +1.10 BLEU points, with only a mild drop in inference speed .      In addition, our methods also surpass the Transformer-Big model, with only 54\% of its parameters. These results demonstrate the effectiveness of the MUTE, as well as its efficiency in both the inference process and parameter usage. \footnote{Code is available at \url{https://github.com/ElliottYan/Multi\_Unit\_Transformer}}",147
"  %  %     Prior work primarily focused on exploiting visual patterns using carefully crafted features . These rendering-based methods have two major drawbacks: 1) they are expensive since they require downloading all external files including CSS, javascript, and images to render the page to compute visual features; 2) they require carefully crafted heuristics around visual proximity  to work well with these expensive features. In this paper, we propose a novel two-stage neural architecture, named FreeDOM, that can be trained on a small number of seed websites and generalize well to unseen websites without requiring any hand-engineered visual features. %we want to employ neural networks for learning transferable visual features such that we can eliminate the need of rendering and human engagement in crafting textual patterns. %We propose a novel two-stage neural architecture that can directly learn from a few annotated websites just based on their raw HTML content and transfer the models for unseen websites without using any human labels . %We parse HTML documents as DOM Trees of the page and classifies it into one of the target fields. This node-level module combines neighboring character sequences, token sequences, as well as markup  to learn a combined representation for the node. We propose a combination of CNNs and LSTMs and show that it can effectively encode useful features in DOM nodes.  These node representations are encoded individually and inevitably lose some global information useful for an extraction task. In particular, only relying on local node features can cause failure when value nodes have no obvious patterns themselves or their local features are very similar to other non-value nodes. To mimic the signal that may be available through visual features used in rendering-based methods,  we use a relational neural network as our second module . This allows us to model the relationship between a pair of elements using both distance-based and semantic features. The rationale behind this is to learn more global representations of node pairs so that we can jointly predict node labels instead of relying only on local features.   Extensive experimental results on a large-scale public dataset, the Structured Web Data Extraction  corpus, show that our model consistently outperforms competitive baseline methods by a large margin.  The proposed FreeDOM is able to generalize to unseen sites after training on a small number of seed sites. In fact, we show that with training data from just three seed sites, our approach out-performs techniques that use explicit visual rendering features by 3.7 F1 points on average. To the best of our knowledge, our framework is among the first neural architectures that efficiently obtains high-quality representations of web documents for structured information extraction.   \eat{Our framework utilizes minimal human efforts in feature engineering and does not require any rendering results, thus making large-scale information extraction on web documents much easier and more effort-light. We believe the proposed model can be promising in other applications that require neural representations of web documents.}  %The node-level module itself can predict node labels for identifying values of interested fields, but the encoded local features cannot capture the long-range dependencies between values and thus degenerate in unlabeled target websites. %To address this problem, we further propose a relational neural network. %As the second-stage module, it explicitly models the relations between DOM nodes and effectively learns the page-level constraints for producing structured predictions. % it models the relational features reflected by each node pairs, and finally conducts structured data extraction as a structured prediction problem.  %Our contributions in this paper are three-fold: %%  %}   %Our contribution is that we propose a novel neural model, FreeDom, for structured data extraction over web documents while using less information  and no hand-crafted features. Extensive experiments on a large-scale public data set  show that the proposed FreeDom outperforms other strong baseline methods while only using raw features. %%ying{The last sentence looks not complete. \yuchen{Done.}} %\tata{Don't say 'less information' emphasize that not requiring visual rendering is cheaper and not requiring hand-crafted features means you can generalize to new tasks better. Need to make this claim more focused so the contributions are clear. We also need to spell out the two stages more clearly 'First stage does blah', 'Second stage does blah'} %\tata{Might be worth adding that entity resolution is not in scope for this work -- ie, we might extract duplicate entries across websites for the same car. There are many papers dealing with that and we're not focused on that in this paper.} % 
"," % tata: Jan 26 rewrite of Abstract.  Extracting structured data from HTML documents is a long-studied problem with a broad range of applications like augmenting knowledge bases, supporting faceted search, and providing domain-specific experiences for key verticals like shopping and movies. Previous approaches have either required a small number of examples for each target site or relied on carefully handcrafted heuristics built over visual renderings of websites. In this paper, we present a novel two-stage neural approach, named FreeDOM, which overcomes both these limitations.  The first stage learns a representation for each DOM node in the page by combining both the text and markup information. The second stage captures longer range distance  and semantic relatedness using a relational neural network. By combining these stages, FreeDOM is able to generalize to unseen sites after training on a small number of seed sites from that vertical without requiring expensive hand-crafted features over visual renderings of the page. Through experiments on a public dataset with 8 different verticals, we show that FreeDOM beats the previous state of the art by nearly 3.7 F1 points on average without requiring features over rendered pages or expensive hand-crafted features. % 3.7 is from Table 2 k=3 .  % tata: Previous version of abstract follows:  %",148
"    Data-to-Text aims at generating natural language descriptions from structured data ; fostered by recent advances on neural approaches  and  %for data-to-text have been made possible by  the emergence of large scale datasets made of  pairs . Figure illustrates an example from the WikiBIO dataset . These datasets are either hand-crafted via crowdworkers or automatically built by aligning sources found on the Internet. As such, %training examples are imperfect reference texts might include divergences of two types, limiting the ability of generation models to produce realistic descriptions. First, reference texts might contain information not grounded in the source data;  especially for automatically constructed datasets, where references were not written with the source-data description task in mind. For instance, the phrase ``who served as lieutenant [...]'' in  Figure has no basis in the associated infobox. Second, reference texts do not always cover the entirety of the table . In most settings, this second point is referred to as content selection and is inherent of most data-to-text tasks. % and is part of the normal subtask flow of data-to-text. %; see for example Figure and information about wars.  %However, some hand-crafted datasets are designed such that annotators have been asked to transcribe every fields, and systems are also expected to do the same. In this case, incomplete references can lead models to fail to learn to transcribe all information, and only partially cover data-sources at inference. However, some hand-crafted datasets are designed where annotators are asked to transcribe every fields, with models also expected to do the same. In this case, incomplete references  can lead to models failing to learn to transcribe all information, and only partially cover data-sources at inference.   Divergence in training examples leads to hallucinated/omitted content in model output; which is a well-known problem in neural approaches for text generation . This problem arises both from the training procedure , and from the testing protocols. Indeed, current standard metrics only measure similarity  to ground truth reference texts and do not fully capture relevance to the source data. %Indeed, most evaluation metrics  work on computing precision of n-grams contained in the generated sentence w.r.t to the ground truth description.  Thus, there is no distinction between a mismatch caused by a paraphrase, poor lexicalization of content, or made-up/incorrect statement, leading to imperfect model selection. While a number of work argue for the need for novel automatic evaluation method , to the best of our knowledge only \citet{wiseman2017} and \citet{dhingra2019} propose metrics based on both the reference and the source data. %Additionally, \citet{dhingra2019} show that their proposed metric PARENT correlates more strongly with human evaluation than any other metric, while being easier to use out of the box.  Recently, different regularization methods have also been proposed to mitigate the negative influence of divergences in reference texts. These approaches can be either at the dataset level , where authors propose techniques to clean/standardize instances; or at the training level , where authors propose novel neural modules designed to limit hallucinations/omissions. However, these approaches are severely limited: e.g., they require significant annotation labor, model-specific tricks and/or manual tuning.   Furthermore, virtually all proposed neural approaches still suffer from 1)~exposure bias and 2)~inconsistency between train/test measurement. Indeed, current neural models are trained via a mechanism called teacher forcing , where the decoder is fed the previous correct token, no matter its actual prediction~, in order to maximize the log-likelihood of the target sentence , but are evaluated through the previously discussed n-gram metrics~. See Section for a more detailed discussion about this subject.\\  %On one hand, not controllable approaches have been proposed: for example, \citet{Liu2019hier} train a hierarchical encoder-decoder on three auxiliary tasks  which are meant to guide the decoding process, in order to achieve descriptions with higher fidelity with respect to the conditioning input.  To the best of our knowledge, there have been few approaches  focused on the training procedure. %We cite , where \citet{liu2019} train a hierarchical encoder-decoder on three auxiliary tasks  which are meant to guide the decoding process. %, in order to achieve descriptions with higher fidelity with respect to the conditioning input. Closest to our work, \citet{Liu2019b} propose a novel neural module for constrained attention, along with a reinforcement learning  training procedure based on BLEU and TFIDF. In our work, to remedy the above shortcomings and building upon the work of \citet{Liu2019b}, we show that no novel neural module is necessary to handle hallucinations and omissions. We propose a model-agnostic RL framework, called PARENTing, where pretrained models are further trained with a self-critical policy gradient algorithm  to limit the impact of divergences in training examples on text generation. Specifically, we use the PARENT metric   which exhibits a strong correlation with human evaluation, while being easier to use out of the box. We provide extensive automatic evaluations on two data-to-text model families  on two widely used benchmarks , as well as a more focused human evaluation %to high-light differences in several training procedures  on WikiBIO.  We report new state of the art PARENT scores on both datasets while BLEU scores are on par with previous SOTA approaches, which shows that our framework efficiently reduces pathological behaviors while keeping generation fluent.       %To remedy those shortcomings, we propose a model-agnostic reinforcement learning framework, called PARENTing, where pretrained models are further trained with a self-critical policy gradient algorithm  to limit the impact of divergences in training examples on text generation. % inspired by recent advancements in other text generation fields.  %Specifically, %we fine-tune pretrained models with a self-critical policy gradient algorithm  based on  %we use the PARENT metric   which exhibits a strong correlation with human evaluation, while being easier to use out of the box. We provide extensive evaluations on two data-to-text model families  on two widely used benchmarks . We report new state of the art PARENT scores on both datasets while BLEU scores are on par with previous approaches, which shows that our framework efficiently reduces pathological behaviors while keeping generation fluent.     %In the following, we first review in Section data-to-text approaches as well as the recent attempts at controlling hallucinations/omissions.  We then introduce in Section our model-agnostic framework limiting hallucinations/omissions in the generation. The evaluation protocol is presented in Section, followed by the obtained results . Section concludes the paper and presents perspectives.  %In the following, we first present a state-of-the art of attempts to reduce hallucinations and to address the exposure bias and inconsistencies in between train/test measurement in data-to-text literature . revoir la structure We then describe in details the PARENT metric from \citet{dhingra2019} in Section and the our proposed RL training framework in Section. The evaluation protocol is presented in Section, followed by the results . Section concludes the paper and presents perspectives.    
","  %The effectiveness of language generation models conditioned by structured data is inherently due to the quality of reference texts and the training protocol. First, these reference texts often diverge from the information contained in the associated source data . Second,  In language generation models conditioned by structured data, the classical training  via maximum likelihood almost always leads  models to pick up on dataset divergence , and to incorporate them erroneously in their own generations at inference.  %In this work, we propose a model-agnostic reinforcement learning framework in order to reduce hallucinations and omissions. To do so, we rely on the recently introduced PARENT metric assessing the adequacy of a candidate generation with both the human reference and the source data.  In this work, we build ontop of previous Reinforcement Learning based approaches and show that a model-agnostic framework relying on the recently introduced PARENT metric is efficient at reducing both hallucinations and omissions. Evaluations on the widely used WikiBIO and WebNLG benchmarks demonstrate the effectiveness of this framework compared to state-of-the-art models.",149
" Relation classification  aims to identify the relation between two specified entities in a sentence.  Previous supervised approaches on this task heavily depend on human-annotated data, which limit their performance on classifying the relations with insufficient instances. Therefore, making the RC models capable of identifying relations with few training instances becomes a crucial challenge. Inspired by the success of few-shot learning methods in the computer vision community ,  first introduce the few-shot learning to RC task and propose the FewRel %   dataset. % dataset as the benchmark. Recently, many works focus on this task and achieve remarkable performance .  %distant supervision  is proposed to automatically construct training instances for RC. %However, in the dataset extracted by distant supervision, some long-tail relations only have few instances and suffer from data sparsity problem. %Inspired by the success of few-shot learning  methods in the computer vision community, e.g., Matching Network , Relation Network  and Memory-augmented network ,  first introduce FSL to RC to tackle the long tail problem. They use the Prototypical Network , which achieves the state-of-the-art performance on several FSL benchmarks. Recently, many works followed their framework have achieved remarkable performance on the Few-shot RC dataset FewRel . %The prototypical network learns the representation  for each relation based on sampled instances, and then classifies the queries into a set of pre-defined relations. %\CheckedBox   % Even though the existing works perform well, they all assume that there is only one relation in a sentence.   Previous few-shot relation classifiers perform well on sentences with only one relation of a single entity pair. However, in real natural language, a sentence usually jointly describes multiple relations of different entity pairs. Since these relations usually keep high co-occurrence in the same context, previous few-shot RC models struggle to distinguish them with few annotated instances. For example, Table shows three instances from the FewRel dataset, where each sentence describes multiple relations with corresponding keyphrases highlighted  as evidence. When specified two entities  in the sentence, there is a great opportunity for the instance to be incorrectly categorized as a {\color{red}{confusing relation}}  instead of the {\color{blue}{true relation}} . % % Specifically,  %is that different entity pairs are usually described  in an input sentence, in which the relation classification of these entity pairs often interferes with each other. %This results in that the entity pairs of these relations are often misclassified into confusing relations by models without the ability of explicitly decoupling easily-confused relations. %Table shows three instances from the FewRel dataset , each of which contains a sentence with two given entities  on the right side, and its positive  and confusing relations  on the left side.  %Previous few-shot methods tend to misclassify these sentences into the confusing relations. the first instance should be categorized as the true relation `parents-child' based on the given entity pair and natural language  expression `a daughter of'. However, since it also includes the NL expression `his wife',  %which describes the confusing relation `husband-wife', it is probably misclassified into this confusing relation `husband-wife'. In this paper, we name it as a relation confusion problem.   %===============================================================================================  % \verb|\checkmark|: \checkmark \par % \verb|\cmark|: \cmark \par % \verb|\xmark|: \xmark   	{blue}} and {\color{red}{red}} words respectively correspond to true and confusing relations.} 	 		 \end{table} %============================================================================================== To address the relation confusion problem, it is crucial for a model to %  effectively select the information with high relevance to the given entity pair and  be aware of which NL expressions cause confusion and learn to avoid mapping the instance into its easily-confused relation. % To address the relation confusion problem, it is crucial for a model to be aware of which NL expressions cause confusion and how to explicitly distinguish the easily-confused relations.  From these perspectives, we propose two assumptions.  Firstly, in a sentence, words that keep high relevance to the given entities are more important in expressing the true relation. Intuitively, the specified entity information is crucial to identify the true relations.  Secondly, explicitly learning of mapping an instance into its confusing relation with augmented data in turn boosts a few-shot RC model on identifying the true relation. % allowing the model to explicitly learn the confusing relations can help it to identify the true relations. %Intuitively, the specific entities information is helpful to identify the positive relation.  Based on these assumptions, we propose CTEG, a few-shot RC model with two novel mechanisms:  An Entity-Guided Attention  encoder, which leverages the syntactic relations and relative positions between each word and the specified entity pair to softly select important information of words expressing the true relation and filter out the information causing confusion.  A Confusion-Aware Training  method, which explicitly learns to distinguish relations by playing a pushing-away game between classifying a sentence into a true relation and its confusing relation. %has the ability of explicitly learning to distinguish easily-confused relations. In addition, inspired by the success of pre-trained language models, our approaches are based on BERT , which has been proved effective especially for few-shot learning tasks. %===========================================================================   % Specifically, when encoding a sentence by the attention mechanism, our EGA guides the calculation of the attention score through multiply it by an entity-aware gate. %we adopt the transformer incorporating with a self-attention mechanism to encoding an input instance,  Specifically, the backbone of the encoder of our model is a transformer equipped with the proposed EGA which guides the calculation of self-attention distributions by weighting the attention logits with entity-guided gates. % The gate is a matrix of relevance scores, which are used to measure the importance of each word according to its relevance to the entities. % The gates are used to measure the importance of each word according to its relevance to the entities. The gates are used to measure the relevance between each word and the given two entities. % Two types of position information of the words are used to calculate the scores. One of them is the relative position , which is the relative distance between a word and an entity in the sentence squence. Two types of information for each word are used to calculate its gate. % One of them is the relative position , which is the relative distance between a word and an entity in the sentence squence. One is the relative position  information, which is the relative distance between a word and an entity in the input sequence. The other is syntactic relation which is proposed in this paper, defined as the dependency relations between each word and the entities. % Besides, we further propose the syntax position, defined as the dependency relations between each word and the entities. Based on these information, the entity-guided gates in EGA are able to select those important words and control the contribution of each word in self-attention. % Based on these information, the entity-aware gate in EGA is able to select those important words and control the contribution of each word in the self-attention.    % For the proposed CAT, it allows the model to and asynchronously learn the confusing relations for each sentence. After each training step, the CAT first selects those misclassified sentences, and regards the relations they are misclassified into as the confusing relations. After that, The CAT uses these misclassified sentences and their confusing relations to conduct an additional training process, which aimes to learn the confusing relations explicitly.  We also propose CAT to explicitly force the model to asynchronously learn the classification from an instance to its true relation and its confusing relation. After each training step, the CAT first selects those misclassified sentences, and regards the relations they are misclassified into as the confusing relations. After that, The CAT uses these misclassified instances and their confusing relations as augmented data to conduct an additional training process, which aims to learn the mapping between these instances into the confusing relations.  % After that, The CAT uses these misclassified sentences and their confusing relations to  conduct an additional training process, which aims to learn the confusing relations explicitly.  Afterwards, the CAT adopts the KL divergence to teach the model to distinguish the difference between the true and confusing relations, which benefits the true relation classification from the confusing relation identification.  % Extensive experiments are conducted on the FewRel dataset, and the results show that our proposed model achieves comparable and even better results to strong baselines in terms of accuracy. % Furthermore, ablation test and case study verify the effectiveness of our proposed EGA and CAT, especially in addressing the relation confusion problem.  The contributions of this paper are summarized as follows:   We propose an Entity-Guided Attention encoder, which can select crucial words and filter out NL expressions causing confusion based on their relevance to the specified entities.    We propose a Confusion-Aware Training process to enhance the model with the ability of distinguishing true and confusing relations.   We conduct extensive experiments on few-shot RC dataset FewRel, ans the results show that our model achieves comparable and even much better results to strong baselines. Furthermore, ablation and case studies verify the effectiveness of the proposed EGA and CAT, especially in addressing the relation confusion problem.  
"," This paper aims to enhance the few-shot relation classification especially for sentences that jointly describe multiple relations. Due to the fact that some relations usually keep high co-occurrence in the same context, previous few-shot relation classifiers struggle to distinguish them with few annotated instances. To alleviate the above relation confusion problem, we propose CTEG, a model equipped with two mechanisms to learn to decouple these easily-confused relations. On the one hand, an Entity-Guided Attention  mechanism, which leverages the syntactic relations and relative positions between each word and the specified entity pair, is introduced to guide the attention to filter out information causing confusion. On the other hand, a Confusion-Aware Training  method is proposed to explicitly learn to distinguish relations by playing a pushing-away game between classifying a sentence into a true relation and its confusing relation. Extensive experiments are conducted on the FewRel dataset, and the results show that our proposed model achieves comparable and even much better results to strong baselines in terms of accuracy. Furthermore, the ablation test and case study verify the effectiveness of our proposed EGA and CAT, especially in addressing the relation confusion problem.",150
"    % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English.  .          % final paper: en-us version           %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. }    Complaining is a basic speech act, usually triggered by a discrepancy between reality and expectations towards an entity or event. Social media has become a popular platform for expressing complaints online  where customers can directly address companies regarding issues with services and products. Complaint detection aims to identify a breach of expectations in a given text snippet. However, the use of implicit and ironic expressions and accompaniment of other speech acts such as suggestions, criticism, warnings and threats make it a challenging task. Identifying and classifying complaints automatically is important for:  improving customer service chatbots;   linguists to analyze complaint characteristics on large scale ; and  psychologists to understand the behavior of humans that express complaints.  Previous work has focused on binary classification between complaints and non-complaints in various domains. Furthermore, some studies have performed more fine-grained complaint classification. For instance, complaints directed to public authorities have been categorized based on their topics or the responsible departments. Other categorizations are based on possible hazards and risks as well as escalation likelihood. Most of these previous studies have used supervised machine learning models with features extracted from text  or task-specific neural models trained from scratch. Adapting state-of-the-art pre-trained neural language models based on transformer networks such as BERT and XLNet has yet to be explored.    In this paper, we focus on the binary classification of Twitter posts into complaints or not \shortcite{preotiuc2019automatically}. We adapt and evaluate a battery of pre-trained transformers which we subsequently combine with external linguistic information from topics and emotions.   \paragraph{Contributions}  New state-of-the-art results on complaint identification in Twitter, improving macro FI by 8.0\% over previous work by  Preotiuc-Pietro et al. \shortcite{preotiuc2019automatically};  A qualitative analysis of the limitations of transformers in predicting accurately whether a given text is a complaint or not.    
","    Complaining is a speech act extensively used by humans to communicate a negative inconsistency between reality and expectations. Previous work on automatically identifying complaints in social media has focused on using feature-based and task-specific neural network models. Adapting state-of-the-art pre-trained neural language models and their combinations with other linguistic information from topics or sentiment for complaint prediction has yet to be explored. In this paper, we evaluate a battery of neural models underpinned by transformer networks which we subsequently combine with linguistic information. Experiments on a publicly available data set of complaints demonstrate that our models outperform previous state-of-the-art methods by a large margin achieving a macro F1 up to 87.",151
"   % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  .     %      % final paper: en-us version      %   % space normally used by the marker     This work is licensed under a Creative Commons      Attribution 4.0 International License.     License details:     \url{http://creativecommons.org/licenses/by/4.0/}. } Neural machine translation  achieved enormous success in advancing the quality of translation . In spite of the impressive performance, NMT models are still vulnerable to perturbations in the input sentences  , i.e. a tiny perturbation will affect hidden representation and lead to low quality of translation . Moreover, NMT commonly consists of  millions of parameters, which making it prone to overfitting especially in low resource scene. % Neural machine translation achieved enormous success in advancing the quality of translation . Despite the impressive performance, NMT models are still vulnerable to the scale of data, when training in the small or monotonous data leading to inference with many unexcepted inputs and low quality of translation, the model prone to overfitting concurrently.  % Improve robustness and representation capacity of models is an important problem to solve.  % NMT model adopts deep neural network to modeling the whole translation process, which can train multi features together without prior domain knowledge, developing rapidly in recent years. However, compared with SMT, NMT requires millions of parameters and huge number of data, making it be prone to overfitting especially in low resource scene. Researchers found that NMT is extremely sensitive to input noise -- a tiny perturbation will affect hidden representation -- leading to a low quality of translation. Improve robustness and representation capacity of models is an important problem to be solved. A natural way to improve generalization is synthesizing natural noise  or adopting arbitrary noise .  Another way is exploring regularization techniques to avoid overfitting , making model robust to unseen or unfamiliar inputs. However, as discrete data, the text is hard to retain the semantic information after corruption. % A natural way to improve generalization is data augmentation, make the model see as much data as possible, meanwhile, it's necessary to avoid overfitting. Standard dropout commonly used as a technology for overfitting in neural networks by preventing the neuron obtain complete information from data like noisy perturbations, moreover, the regularization techniques also can be beneficial for overfitting. %    In this paper, we propose Token Drop to prevent overfitting and improve generalization. Different from standard dropout  that drops neurons in network randomly, we drop tokens of the input sentences. In order to retain semantic information, we replace tokens with a special symbol  . This allows model learn hidden representation from rest token's context, and predict target translation condition on latent variable. On the one hand, our method allows model meeting exponentially different sentences can be explained as data augmentation; On the other hand, our method corrupts input sentences with natural noise can be seen as regularization term for NMT.   We investigate two self-supervised objectives: Replaced Token Detection and Dropped Token Prediction. Considering  our Token Drop method regularize parameters by weakening model inputs, making NMT suitable for applying self-supervised objective. During training:  use a discriminator to detect whether input tokens are dropped or not;  leverage hidden state to predict original tokens of dropped tokens inspired by Cloze task . Both of them guide model to generate semantically similar representation, leading to a better generalization capacity.  % To test our approach, we conduct machine translation task on ZH-EN and EN-RO benchmark, despite compared with strong baseline, we  % In this work, we propose to randomly drop tokens of input sentence, different with standard dropout  , which drop neurons in network randomly during training, we replace tokens with a special symbol unk. Using this method, we call it token drop, for each input sentence, our model see  different sequences theoretically. On the one hand, model receives more data, on the other hand, it can be seen as noise challenge model to learn primary information from latent representation. Our approach is also similar to self-supervised technique, which utilize contextual state to predict masked token, the difference of our training object is to optimize translation ability of source language. By randomly drop tokens of input, forcing model utilize latent representation to make prediction during training, at inference, model see full sentence and make decision easily. To test our approach, we conduct machine translation on ZH-EN and EN-RO corpus, compare to baseline, our token drop training models are more stable and resilient to overfitting.  % Deep neural networks with large number of parameters are prone to overfitting, requires regularization method in practice. One of the most effective way to avoid overfitting is Dropout , by omitting stochastic neurons in networks during training iteration, while maintain all neurons during inference, brings significant improve results on a variety tasks . Different from dropout, which drop single unit, token-level dropout drop  entire token, are  proved be effective on seq2seq task. 
","    Neural machine translation with millions of parameters is vulnerable to unfamiliar inputs. We propose Token Drop to improve generalization and avoid overfitting for the NMT model.  Similar to word dropout, whereas we replace dropped token with a special token instead of setting zero to words.  We further introduce two self-supervised objectives: Replaced Token Detection and Dropped Token Prediction. Our method aims to force model generating target translation with less information, in this way the model can learn textual representation better. Experiments on Chinese-English and English-Romanian benchmark demonstrate the effectiveness of our approach and our model achieves significant improvements over a strong Transformer baseline\footnote{Our code is released at \url{https://github.com/zhajiahe/Token_Drop}}.",152
" 	\com{ 		Remember to recheck: 		intro 		each section place is clear 		reiterate paragraph 		 		structure: 		taxonomy explained 		validations of our taxonomy+classification  		comparison to other taxonomies /classifications   		proof of usefulness 		various kinds of analysis this allows  		qualitative results and discussion 		related work 		conclusion 	} 	Taxonomies of grammatical errors are important for linguistic and computational analysis of learner language, as well as for Grammatical Error Correction  systems.\footnote{Code can be found \href{https://github.com/borgr/GEC_UD_divergences}{in github repo GEC\_UD\_divergences}. Matrices directly mentioned are included in the appendix.} 	Such taxonomies divide the complex space of errors into meaningful categories and enable characterizing their distribution in learner productions. This information can be beneficial for GEC: it can support the development of systems that focus on specific error types, serve as a form of inductive bias , and guide data augmentation and data filtering by controlling the distribution of error types. Error taxonomies can also improve the interpretability of system outputs for error analysis and learner feedback. 	 	%  	%  	% \end{small} 	% \end{table} 	 	 	 	 	 	A number of annotation efforts for learner language developed error taxonomies , and statistical classifiers into such taxonomies, notably ERRANT . Taking error types into consideration in learning has also been shown to improve GEC performance \citep[][{cf.  \S}]{kantor2019learning}. 	However, most existing taxonomies are fairly coarse-grained and language specific, and do not produce meaningful types for a large proportion of the errors. For example, 25\% of the errors in the standard NUCLE corpus  are mapped to the residual category OTHER . 	 	We propose \secl, a taxonomy of Syntactic Errors  and an automatic Classification. Inspired by a longstanding tradition in Machine Translation  which analyses divergences between source and translated texts based on syntactic structure , \secl\ is based on divergences between ungrammatical sentences and their corrections. 	We define SEs as errors whose correction involves changing morphological features, POS labels or the syntactic structure labels. \secl\ takes as input edits, i.e., grammatically incorrect text spans and their corrections, and compares their labels. For example, the error in Fig.  is an adjective replaced with an adverb  in POS terms, and an \ra in edge-label terms. Thus, SEs are defined by changes in form, rather than by the principles governing the choice of a correct form. 	 	\secl\ is the first taxonomy derived from a syntactic representation framework, and it uses the Universal Dependencies formalism \citep[UD;][]{nivre2016universal}.  	This approach provides three major advantages over prior learner error taxonomies. First, the \secl\ taxonomy is derived automatically from UD annotations, circumventing the need for constructing ad-hoc manually defined error categories. Second, using the UD formalism makes the method applicable across languages, allowing for consistent analyses and comparisons of learner errors across different languages within one unified framework. Third, \secl\ is compatible with standard representations and tools in NLP.  	 	Further, the UD based approach to error classification can yield finer distinctions compared to existing schemes. For example, it divides the commonly used class of adposition errors into errors in the use of prepositions as nominal modifiers , and the use of prepositions in prepositional objects or adjuncts .  	%prepositions involving verbal arguments  and errors involving spatial/temporal relations.\oa{how exactly? maybe we should say we distinguish between NP-internal PPs and clause-level PPs?} \lc{One would be obj something and the other will not . Isn't what is an object and subject a main thing syntax allows?, you can write another example. Note that the example should involve a type not usually split . }  	POS tags alone cannot distinguish them, but the UD trees expose this distinction straightforwardly. UD can also help classify agreement and case-assignment errors thanks to its morphological-feature layer containing information about case, number, gender, and other features relevant for inflection. 	 	 	We validate \secl's reliability by showing  SEs based on automatic parses are similar to ones based on manual parses.  ;  \secl\ types map well to NUCLE's manually curated taxonomy ;  \secl\ is complementary to the standard type classifier ERRANT: 60\% of the errors not classified by ERRANT are classified by \secl. 	 	We demonstrate \secl's unique features, notably cross-linguistic applicability, by analyzing SE distributions in available corpora for learner English  and learner Russian . 	 	Finally, we find in GEC systems  certain SEs are harder to correct  SEs are harder than non-SEs   the granular types can help devising rules to improve products . 	 	%\lc{yb, I gave it a try, is that better? Am I being too general?} 	 	%We validate the accuracy of relying on parsing technology  and compare \secl\ to manual  and automatic taxonomies , finding that \secl\ classifies 60\% of the errors not covered by by the leading error classifier for English ERRANT . We further examine its characteristics by using UD features and by applying it to Russian . All these findings suggest that \secl\ is a reliable, fine-grained annotation which is the only current taxonomy and classifier that is not language specific. To show its wide applicability, we use \secl\ to provide a detailed picture of the distribution of SEs in various learner English corpora . We proceed to use \secl\ to detect trends in error type distribution across learner levels . We conclude by analyzing system outputs .\yb{many people skip these paper summary paragraphs. I would instead have a list of contributions as bullet points .} 	 	 	%  	%While classifying grammatical error types . In this paper we focus on syntactic errors, i.e., errors that require changing the tree structure to fix, and  	 	 	 	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 	
"," 		We present a method for classifying syntactic errors in learner language, namely errors whose correction alters the morphosyntactic structure of a sentence. 		The methodology builds on the established Universal Dependencies syntactic representation scheme, and provides complementary information to other error-classification systems.  		Unlike existing error classification methods, our method is applicable across languages, which we showcase by producing a detailed picture of syntactic errors in learner English and learner Russian. We further demonstrate the utility of the methodology for analyzing the outputs of leading Grammatical Error Correction  systems.",153
" The short answer test is a type of exam in which participants are asked to answer questions with short answers which can consist of 1-2 sentences. Assessing student short answers on the exam is very challenging for the assessor. When a large number of students are assessed, for example on a national scale, assessors are required to remain consistent and objective in assessing hundreds or even thousands of student responses. The questions with short answer format also allow students to answer in their own style which can be varied for each student. Therefore, computer assistance in making automatic short answer scoring is deemed necessary to address these problems.  In 2019, NLP Research Group from Universitas Gadjah Mada, Indonesia, in collaboration with the Education Assessment Center, Ministry of Education and Culture of Indonesia, held the Ukara 1.0 Challenge\footnote{https://nlp.mipa.ugm.ac.id/ukara-1-0-challenge/}. In this challenge, participants from all over the nation were challenged to make an automatic short answer scoring system for Indonesian student exam. There are two short answer questions on this challenge with correct and incorrect labels. In this paper, we try to describe the improvement of our previous work on the Ukara 1.0 Challenge dataset.  There are several challenges that may arise in applying and making an automatic short answer scoring system. First, it is the number of models and hyperparameters that need to be searched.  In conventional machine learning, a model is trained to solve a specific problem, or in our case, a model is only trained to assess one short answer question. When the number of questions to be assessed increases, the time required for searching the model and tuning its hyperparameters will also increase. The second challenge is the imbalance class. In an exam, the questions given are intended to test students' abilities, therefore the level of difficulty in each question will cause the number of correct responses to be much less than the number of wrong responses. Another challenge in automatic short answer scoring is the small amount of labeled data. The data labeling process is not easy because it requires an expert to validate the student responses and of course this is time consuming and costly.  In several previous studies, making short answer scoring or essay scoring was done using deep learning approaches, for example, using long-short term memory, convolutional neural network, or a combination of both . We take a different approach by using a simpler stacking model because of the small number of available data. In this paper, we used the sentence-level feature as done by . Without word sequence features, the automatic scoring process could be viewed as a text classification problem. The use of stacking models for text classification has been done in  and shows better performance than a single model classifier. We also propose to use an upsampling method, Synthetic Minority Over-sampling Technique , to handle imbalance classes and hyperparameters optimization algorithm, Tree-structured Parzen Estimator  to find a robust model that performs well on each type of question. In this paper, we use hyperparameters term as model components which are preset and untrained . Meanwhile, the term parameters refers to the model components that can be trained  .  
"," Automatic short answer scoring is one of the text classification problems to assess students' answers during exams automatically. Several challenges can arise in making an automatic short answer scoring system, one of which is the quantity and quality of the data. The data labeling process is not easy because it requires a human annotator who is an expert in their field. Further, the data imbalance process is also a challenge because the number of labels for correct answers is always much less than the wrong answers. In this paper, we propose the use of a stacking model based on neural network and XGBoost for classification process with sentence embedding feature. We also propose to use data upsampling method to handle imbalance classes and hyperparameters optimization algorithm to find a robust model automatically. We use Ukara 1.0 Challenge dataset and our best model obtained an F1-score of 0.821 exceeding the previous work at the same dataset.",154
"  % Second, is a NMT model's ability to handle streaming ASR output; an ASR system provides the best greedy recognition  from a live speech's segmented audio. % we don't really talk about that ^ %deleted simultaneous With the advance of Automatic Speech Recognition  and Neural Machine Translation  systems, speech translation has become increasingly feasible and has received considerable attention.  However, researchers have encountered many challenging problems within the standard cascaded framework where ASR system outputs are passed into NMT systems. First, since NMT models are often trained with clean, well-structured text, the disfluency of spoken utterances and the recognition errors from ASR systems are not modeled by the NMT systems.  Second, people speak differently than they write, which results in changes in both sentence structure and meaning.  Third, automatically predicting sentence boundaries is challenging.  Taken as a whole, poorly segmented sentences with incorrect word recognition leads to poor translations.  These problems pose unique challenges for ASR NMT robustness that are not readily addressed by current methods. %  %\subsection{Related Work} % Current approaches to robust NMT with noisy inputs typically focus on improving word transcription through data augmentation techniques. Such methods include disfluency removal where redundant and unnecessary words are removed before translating the transcript, domain adaptation where NMT models are augmented with in-domain training data, and synthetic noise, where random edits are made to training data.   %  % \footnotetext{When compared to Table , the sum of System/System with transcript and segmentation degradation surpasses the Gold/Gold evaluation by 0.57 BLEU points. This is because the modifications to the evaluation data are not directly additive, ie .}  % %\subsection{Contributions} % % Although data domain and segmentation issues are often tackled separately, we find their compounded effects, namely erroneous transcript sentence boundaries, are neglected and substantially detrimental to final translation quality.  % As such, our contributions are two fold, we analyze the impact of noisy ASR segmentations on translation and propose an easily adaptable and simple data augmentation strategy to increase NMT robustness. % this sentence can be optional In our experimentation, we found that ASR system punctuation is often imperfect. It may omit or insert sentence-final punctuation, resulting in sentences that are erroneously compounded or fragmented.  While this is corroborated by similar works, which note that degradation of translation is caused by poor system sentence boundary prediction, they do not specifically evaluate, quantify, and address this issue. % should we mention how much sentence boundaries degrade accuracy % Find an example? To tackle the sentence boundary problem, we propose a simple scheme to augment NMT training data, which yields +1 BLEU point on average. % Similar to , we first ascertain that a NMT model can implicitly learn target punctuation from unpunctuated source text, even with noisy imperfect sentence boundaries. % is this sentence above necessary?^ %We show that with a simple data augmentation scheme on both general and in-domain NMT training data, we can achieve an improvement of  and  BLEU for tst2015 and tst2018 respectively.  This procedure is agnostic to ASR systems and can be applied to any NMT model training easily.  %  
"," Neural Machine Translation  models have demonstrated strong state of the art performance on translation tasks where well-formed training and evaluation data are provided, but they remain sensitive to inputs that include errors of various types. Specifically, in the context of long-form speech translation systems, where the input transcripts come from Automatic Speech Recognition , the NMT models have to handle errors including phoneme substitutions, grammatical structure, and sentence boundaries, all of which pose challenges to NMT robustness.  %This paper makes two main contributions via an in-depth error analysis and a proposed solution.  Through in-depth error analysis, we show that sentence boundary segmentation has the largest impact on quality, and we develop a simple data augmentation strategy to improve segmentation robustness.",155
" Automatic summarization is the automated process of reducing the size of an input text while preserving its most relevant information content and its core semantics. Techniques for summarization are often characterized as being either: Extractive or Abstractive. Extractive methods construct summaries by combining the most salient passages  of a source text; a process similar to human's way of identifying the right information. One way to achieve extractive summarization is to define the problem as a sentence classification task, using some form of  representation of the sentences in a document . To avoid content overlap issues, previous work has used sentence reranking  or sentence ordering by extracting sentences recurrently . Abstractive methods generate summaries by generating new sentence constructs ``from scratch'', or from representation of document content, a process that is conceptually more similar to the notion of paraphrasing. Abstractive text summarization has attracted interest since it is capable of generating novel formulations of summaries using language generation models conditioned on the source text. Several attention-based Recurrent Neural Network  encoder-decoders have been introduced to tackle varying text generation issues of standalone abstractive sequence-to-sequence  models. Copy and pointer mechanisms , for example, have enabled decoders to better generate unseen words, out-of-vocabulary words and named entities.   Most recently, hybrid extractive and abstractive architectures have been proposed and have shown promising results in both quantitative performance measures and human evaluations. In such set-ups, the extractive model first selects salient sentences from a source article, and the abstractive model paraphrases the extracted sentences into a final summary. The majority of current state-of-the-art abstractive summarization models\footnote{Excluding summarization models using large scale pre-trained language models such as BERT } are based on the hybrid approach .  Nonetheless, hybrid models can be limited by three disadvantages. First, since ground-truth labels for extractive summarization are usually not provided, extractive labels must be generated by a potentially suboptimal algorithm . The performance of models trained with such labels is therefore bounded by the quality of the performance of the extractive heuristics. Second, since ground-truth binary labels for recurrently extracted sentences are typically teacher forced as in \citet{chen-bansal-2018-fast}, ``exposure bias''  may negatively affect content selection performance at inference. Finally, given that the hard extraction step is not differentiable, existing hybrid models typically require multi-step training   or reinforcement learning  to train the whole model.  In this paper, we introduce a novel abstractive summarization model that incorporates an intermediate extractive step but does not require labels for this type of extractive content selection, and it is fully end-to-end trainable. To achieve this, we propose a new memory augmented encoder-decoder  architecture  called Mem2Mem. Mem2Mem has 2 memorization modes:  absorb key information of the encoded source sequence via a compression mechanism, and  sequentially update the external memory during target summary generation. Without using extractive ground-truth labels, we find in our analysis that Mem2Mem's compression mechanism behaves as an implicit sentence extractor that stores sentence representations of the salient content. The choice of sentence representations is only guided by the memory regularization and conditional language modeling loss of the decoder, thus avoiding exposure bias from maximizing the likelihood of sequential binary extraction labels. Finally, the encoded memory is transferred to the decoder memory, which is iteratively refined during the decoding process. To our knowledge, Mem2MeM is the first abstractive summarization model that uses memory compression for sentence extraction and that directly employs the memorized representations during summary generation. We empirically demonstrate the merits of this approach by setting a new state-of-the-art on long text abstractive summarization tasks on the Pubmed, arXiv and Newsroom datasets . Our contributions are three fold:   % fig_architecture    
"," We introduce Mem2Mem, a memory-to-memory mechanism for hierarchical recurrent neural network based encoder decoder architectures and we explore its use for abstractive document summarization. Mem2Mem transfers ``memories"" via readable/writable external memory modules that augment both the encoder and decoder. Our memory regularization compresses an encoded input article into a more compact set of sentence representations. Most importantly, the memory compression step performs implicit extraction without labels, sidestepping issues with suboptimal ground-truth data and exposure bias of hybrid extractive-abstractive summarization techniques. By allowing the decoder to read/write over the encoded input memory, the model learns to read salient information about the input article while keeping track of what has been generated.  Our Mem2Mem approach yields results that are competitive with state of the art transformer based summarization methods, but with 16 times fewer parameters. %On abstractive long text summarization, Mem2Mem surpasses, with full end-to-end training, the current state-of-the-art by 3.98 and 3.08 average ROUGE scores on the Pubmed and arXiv datasets while using $16$ times less parameters.  % Our code and trained models are available at \url{https://github.com/anonymously999/mem2mem}.",156
"  Attention-based encoder-decoder modeling is a natural and powerful paradigm for speech to text tasks, such as automatic speech recognition  and speech translation , and that has led to significant progress .  However, it relies on large amounts of supervised speech data, which is expensive to transcribe and translate.  In addition, the amount of speech transcripts and speech translation labels is dwarfed by the amount of text data available for language model  and machine translation  training. For example, the number of text tokens used for LM modeling is two orders of magnitude larger than the number of tokens from the corresponding speech corpus in the Librispeech data corpus, as shown in Table.    Attention-based encoder-decoder models are not designed to incorporate heterogeneous inputs and cannot benefit from large amounts of low cost text data directly in speech applications. As expected, performance gaps can still be observed between attention based encoder-decoder systems and conventional systems with multiple components.  %Short description about previous work In order to alleviate the data scarcity issue, different approaches have been studied, including acoustic and linguistic aspects.  %In this study, we focus on leveraging text data to improve linguistic modeling ability in speech to text systems.  LM is the most commonly used method to integrate linguistic information into ASR.  Prior work focuses on building LM with monolingual text data, and then integrate LM or transfer knowledge from it into the decoder.    generate synthetic data from text to augment speech training corpus. Another direction is to leverage text data directly during training through multitask learning.  use a common representation space to learn correspondences between different modalities for spoken language understanding.  propose multi-modal data augmentation to jointly train text and speech for ASR.  %is reminiscent of work done on multimodal learning or spoken language understanding  that also uses a common representation space to learn correspondences between different modalities.  are focused on ST tasks and trained with an ASR system together, where ASR is used as an auxiliary task. Hence, those methods cannot be applied back to ASR systems.  %What is proposed, describe the main idea %We follow the second direction and propose using auxiliary text tasks to enhance speech to text tasks. In this study, we focus on leveraging text data to improve linguistic modeling ability in speech to text tasks. We propose a general framework to leverage text data for ASR and ST tasks.  %Two encoders take text and speech as input respectively,  while the decoder is shared between tasks. During inference, only the speech encoder and decoder are used. A denoising autoencoder task  is introduced to be jointly trained with the ASR task with monolingual data, while a machine translation task is co-trained with  ST task  with  parallel  data. Text  input  is  represented  as  spoken form using phoneme sequence and it effectively reduces the difference between speech input and text input. We also carefully study different design choices for the joint training system, including strategies to share the text and speech encoders and comparing the joint training system with models initialized from pre-trained components.   Our experiments show the proposed joint training systems can effectively reduce word error rate  for the ASR task by 10\% to 15\% and improve BLEU score by 3.69.2 for ST tasks.  %Compared with previous methods, our method emphasizes reducing the difference between the two encoders and eases the knowledge transfer between the text to text and the speech to text tasks. %The method includes three parts: first, the representation difference from text and speech input is minimized through phoneme sequence representation and an additional speech end of sentence token. Second, a novel cross attentive loss is proposed to increase the similarity between sequences with different lengths. It acts as an auxiliary loss to regularize the outputs from two encoders. Third,  %masking is applied to input text tokens to simulate the adverse conditions in speech, such as noise and incomplete pronunciation. It also encourages the decoder to learn better language context representation to fill the gap due to masking.   %Instead of focusing on one particular task as in previous work, our method can be applied to both ASR and ST tasks. Experiments are conducted on two popular ASR and ST benchmark tasks. The results show the proposed method brings substantial gains over the baseline in both ASR and ST tasks.   
"," Attention-based sequence-to-sequence modeling provides a powerful and elegant solution for applications that need to map one sequence to a different sequence.  Its success heavily relies on the availability of large amounts of training data.  This presents a challenge for speech applications where labelled speech data is very expensive to obtain, such as automatic speech recognition  and speech translation .  In this study, we propose a general multi-task learning framework to leverage text data for ASR and ST tasks. Two auxiliary tasks, a denoising autoencoder task and machine translation task, are proposed to be co-trained with ASR and ST tasks respectively.  We demonstrate that representing text input as phoneme sequences can reduce the difference between speech and text inputs, and enhance the knowledge transfer from text corpora to the speech to text tasks.  Our experiments show that the proposed method achieves a relative 10$\sim$15\% word error rate reduction on the English Librispeech task compared with our baseline, and improves the speech translation quality on the MuST-C tasks by 3.6$\sim$9.2 BLEU.",157
" Motivated by the process of human inquiry and learning, the field of question generation  requires a model to generate natural language questions in context. QG has wide applicability in automated dialog systems, language assessment, data augmentation, and the development of annotated data sets for question answering  research.    Most prior research on QG has focused on generating relatively simple factoid-based questions, where answering the question simply requires extracting a span of text from a single reference document. However, motivated by the desire to build NLP systems that are capable of more sophisticated forms of reasoning and understanding, there is an increasing interest in developing systems for multi-hop question answering and generation , where answering the questions requires reasoning over the content in multiple text documents .  Unlike standard QG, generating multi-hop questions requires the model to understand the relationship between disjoint pieces of information in multiple context documents.  Compared to standard QG, multi-hop questions tend to be substantially longer, contain a higher density of named entities, and---perhaps most importantly---high-quality multi-hop questions involve complex chains of predicates connecting the mentioned entities   To address these challenges, existing research on multi-hop QG primarily relies on graph-to-sequence  methods. These approaches extract graph inputs by augmenting the original text with structural information  and then apply graph neural networks  to learn graph embeddings that are then fed to a sequence-based decoder.  However, the necessity of these complex G2S approaches---which require designing hand-crafted graph extractors---is not entirely clear, especially when standard transformer-based sequence-to-sequence  models already induce a strong relational inductive bias. Since transformers have the inherent ability to reason about the relationships between the entities in the text, one might imagine that these models alone would suffice for the relational reasoning requirements of multi-hop QG.   \xhdr{Present work} In this work, we show that, in fact, a standard transformer architecture is sufficient to outperform the prior state-of-the-art on multi-hop QG.  We also propose and analyze a graph-augmented transformer ---which integrates explicit graph structure information into the transformer model. GATE sets a new state-of-the-art and outperforms the best previous method by 5 BLEU points on the HotpotQA dataset. However, we show that the gains induced by the graph augmentations are relatively small compared to other improvements in our vanilla transformer architecture, such as an auxiliary contrastive objective and a data filtering approach, which improve our model by 7.9 BLEU points in ablation studies.  Overall, our results suggest diminishing returns from incorporating hand-crafted graph structures for multi-hop reasoning and provides a foundation for stronger multi-hop reasoning systems based on transformer architectures.    Our key contributions are summarized as follows:  We hope that our work provides a strong foundation for future research on multi-hop QG while guiding the field towards the most promising avenues for future model improvements.\documentclass[11pt,a4paper]{article}  \usepackage[hyperref]{emnlp2020}   \usepackage{times}     \usepackage{latexsym} \renewcommand{\UrlFont}sachande@mila.quebec, wuli@us.ibm.com  \usepackage{microtype}  \aclfinalcopy % Uncomment this line for the final submission  \usepackage[utf8]{inputenc} % allow utf-8 input \usepackage[T1]{fontenc}    % use 8-bit T1 fonts \usepackage{url}            % simple URL typesetting \usepackage{booktabs}       % professional-quality tables \usepackage{amsfonts}       % blackboard math symbols \usepackage{amsmath} \usepackage{nicefrac}       % compact symbols for 1/2, etc. \usepackage{graphicx} \usepackage{microtype}      % microtypography \usepackage{tabularx} \usepackage{xcolor} \usepackage{bbm} \usepackage{array} \usepackage{arydshln} \usepackage{amsfonts} \usepackage{amsmath} \DeclareMathOperator*{\argmax}{arg\,max} \DeclareMathOperator*{\argmin}{arg\,min} \usepackage{bbm} \usepackage{boldline} \usepackage{bigstrut} \usepackage{blindtext} \usepackage{booktabs, siunitx} \usepackage[labelfont=bf, format=plain, justification=justified, singlelinecheck=false]{caption} \usepackage{color} \usepackage{cprotect} \usepackage{ctable} \usepackage{dirtytalk} \usepackage{enumitem} \usepackage[export]{adjustbox} \usepackage{float} \usepackage{graphicx} \usepackage{hhline} \usepackage{latexsym} \usepackage{mathrsfs} \usepackage{microtype} \usepackage{moresize} \usepackage{multicol} \usepackage{multirow} \usepackage{nccmath} \usepackage{nicefrac} \usepackage{pifont} \usepackage{placeins}     \setlength\bigstrutjot{3pt} \usepackage{soul} \usepackage{subcaption} \usepackage{times} \usepackage[utf8]{inputenc} \usepackage{url} \usepackage{verbatim} \usepackage{wrapfig, lipsum} \usepackage{textcomp} \usepackage{enumitem}  %\hypersetup{draft}  \newcommand\sL{\ensuremath{\mathcal{L}}} \newcommand\sD{\ensuremath{\mathcal{D}}}  % colors \definecolor{lblue}{HTML}{A6CEE3} \definecolor{lgreen}{HTML}{B2DF8A} \definecolor{lred}{HTML}{FB9A99} \definecolor{lorange}{HTML}{FDBF6F} \definecolor{mblue}{HTML}{80B1D3} \definecolor{mgreen}{HTML}{B3DE69} \definecolor{mred}{HTML}{FB8072} \definecolor{morange}{HTML}{FDB462} \definecolor{blue}{HTML}{1F78B4} \definecolor{green}{HTML}{33A02C} \definecolor{red}{HTML}{E31A1C} \definecolor{orange}{HTML}{FF7F00} \definecolor{dblue}{HTML}{0050EF} \definecolor{dgreen}{HTML}{006D2C} \definecolor{dorange}{HTML}{EC7014} \newcommand{\blue}[1]{{\color{blue} #1}} \newcommand{\green}[1]{{\color{green} #1}} \newcommand{\red}[1]{{\color{red} #1}} \newcommand{\orange}[1]{{\color{orange} #1}} \newcommand{\dblue}[1]{{\color{dblue} #1}} \newcommand{\dgreen}[1]{{\color{dgreen} #1}} \newcommand{\dorange}[1]{{\color{dorange} #1}}  \newcommand{\cut}[1]{} \newcommand{\xhdr}[1]{{\bfseries #1}.}  \interfootnotelinepenalty=1000  \title{Stronger Transformers for Neural Multi-Hop Question Generation}  \author{Devendra Singh Sachan, Lingfei Wu, Mrinmaya Sachan, William Hamilton \\ Mila - Quebec AI Institute\\ School of Computer Science, McGill University\\ IBM Thomas J. Watson Research Center, Yorktown Heights\\ ETH Zurich\\ mrinmaya.sachan@inf.ethz.ch, wlh@cs.mcgill.ca\\ {\tt mrinmaya.sachan@inf.ethz.ch, wlh@cs.mcgill.ca} }   \date{}   % !TeX root = main.tex  
"," Prior work on automated question generation has almost exclusively focused on generating simple questions whose answers can be extracted from a single document. However, there is an increasing interest in developing systems that are capable of more complex multi-hop question generation, where answering the questions requires reasoning over multiple documents. In this work, we introduce a series of strong transformer models for multi-hop question generation, including a graph-augmented transformer that leverages relations between entities in the text.  While prior work has emphasized the importance of graph-based models, we show that we can substantially outperform the state-of-the-art by {5 BLEU points}  using a standard transformer architecture. We further demonstrate that graph-based augmentations can provide complimentary improvements on top of this foundation. Interestingly, we find that several important factors---such as the inclusion of an auxiliary contrastive objective and data filtering could have larger impacts on performance.  We hope that our stronger baselines and analysis provide a constructive foundation for future work in this area.",158
"  Variational Autoencoders   allow to design complex generative models of data.  % since the inference process of VAE-based approaches has the advantage of being independent from the model architecture providing high flexibility in designing new neural components. In the wake of the renewed interest for VAEs, traditional probabilistic topic models  have been revised giving rise to several Neural Topic Model  variants, such as NVDM ,  ProdLDA , NTM-R , etc. % GSM , W-LDA  However, existing topic models when applied to user reviews may extract topics associated with writers' subjective opinions mixed with those related to factual descriptions such as plot summaries of movies and books . Although these approaches have achieved significant results via the neural inference process, surprisingly very little work has been done on how to disentangle the inferred topic representations.   % Despite the lack of general consensus about a formal definition of disentangled representations ,   Disentangled representations can be defined as representations where individual latent units are sensitive to variations of a single generative factor, while being relatively invariant to changes of other factors . Inducing such representations has been shown to be significantly beneficial for their generalization and interpretability .  For example, an image can be viewed as the results of several generative factors mutually interacting, as one or many sources of light, the material and reflective properties of various surfaces or the shape of the objects depicted . %  In the context of topic modeling, documents result from a generative process over mixtures of latent topics, and therefore, we propose to consider these latent topics as generative factors to be disentangled to improve their interpretability and discriminative power. Disentangled topics are topics invariant to the factors of variation of text, which for instance, in the context of book and movie reviews could be the author's opinion , the salient parts of a plot or other auxiliary information reported. An illustration of this is shown in Fig. in which opinion topics are separated from plot topics.  % where this leads to separating topics based on the ``factor of variation"" they are revealing. % For example, in generating a book review, the factors of variation involved could depend on the author's expertise in identifying the salient features of the book, %his knowledge of the book's genre, or  % his ability to summarize the plot and the feelings evoked by the book.  % % [Let's break in/the atom] % % Figure  reports a examples of polarity-disentangled topics generated from the IMDB movie reviews of ""The Hobbit"". The topics on the left and right summarize some of the positive and negative aspects described by users, while neutral topics in the middle report the main elements of the movie's plot.  % An effective approach for disentangling features in the latent space of VAEs is to adopt adversarial training . However, despite its successful applications in computer vision , the applications to text analysis has been rather limited so far , narrowed by the lack of proper tasks to evaluate the generated disentangled representations and the limited availability of suitable datasets.  % For example, in book or movie reviews, we want to disentangle topics which are related to opinions expressed in text and topics relating to book/movie plots. An illustration of this is shown in Figure in which opinion topics are separated from plot topics.   However, models relying solely on sentiment information are easily misled and not suitable to disentangle opinion from plots, since even plot descriptions frequently make large use of sentiment expressions . Consider for example the following sentence: ``The ring holds a dark power, and it soon begins to exert its evil influence on Bilbo"", an excerpt from a strong positive Amazon's review.  % This overcomes the difficulty of separating opinions from plot and auxiliary information yet containing polarised descriptions that easily mislead models merely relying on sentiment lexicon; analogously to the issue of mixed topics generated when traditional topic models are applied to review documents, as pointed out in \citet{Blei08}.  % Despite its successful employment in computer vision , the adversarial approach has had a rather limited application in text analysis so far , narrowed by the lack of proper tasks to evaluate the generated disentangled representations and the limited availability of suitable datasets.  Therefore, we propose to distinguish opinion-bearing topics from plot/neutral ones combining a neural topic model architecture with an adversarial training. In this study, we present the DIsentangled Adversarial TOpic Model \footnote{Source code and dataset omitted for the anonymous submission.}, aiming at disentangling information related to the target labels , from other distinct aspects yet possibly still polarised . We also introduce a new dataset, namely the MOBO dataset\footnotemark[\value{footnote}], made up of movie and book reviews, paired with their related plots. The reviews come from different publicly available datasets: IMDB , GoodReads  and Amazon reviews , %,  and encompass a wide spectrum of domains and styles. We conduct an extensive experimental assessment of our model. First, we assess the topic quality in terms of topic coherence and diversity and compare DIATOM with other supervised topic models on the sentiment classification task; then, we analyse the disentangling rate of topics to quantitatively assess the degree of separation between actual opinion and plot/neutral topics.    Our contributions are summarized below:    The rest of the paper is organized as follows. We review the related literature on sentiment-topic models, neural topic models and the studies on disentangled representations . Then, we present the details of our proposed DIATOM model , followed by the experimental setup  and results . Finally, we conclude with a summary of the results and suggestions for future works .     %%%%%%%%%%%%%%%%%%%%%%%%%%% 
"," The flexibility of the inference process in Variational Autoencoders  has recently led to revising traditional probabilistic topic models giving rise to Neural Topic Models . Although these approaches have achieved significant results, surprisingly very little work has been done on how to disentangle the latent topics. Existing topic models when applied to reviews may extract topics associated with writers' subjective opinions mixed with those related to factual descriptions such as plot summaries in movie and book reviews. It is thus desirable to automatically separate opinion topics from plot/neutral ones enabling a better interpretability. %Since in the topic modeling framework documents result from a generative process over mixtures of latent topics, we propose to interpret these latent topics as generative factors to be disentangled to improve their interpretability and discriminative power. In this paper, we propose a neural topic model combined with adversarial training to disentangle opinion topics from plot and neutral ones. We conduct an extensive experimental assessment introducing a new collection of movie and book reviews paired with their plots, namely MOBO dataset, showing an improved coherence and variety of topics, a consistent disentanglement rate, and sentiment classification performance superior to other supervised topic models.",159
" \subsection{Dialogue act recognition}  Mutual understanding in interactive situations, either when several people are engaged in a dialogue or when they are interacting with a modern computer system in natural language, may not be achieved without considering both the semantic information in the speakers utterances and the pragmatic interaction level, especially relative to dialogue acts. Dialogue Acts  represent the meaning of an utterance  in the context of a dialogue, or, in other words, the function of an utterance in the dialogue. For example, the function of a~question is to request some information, while an answer shall provide this information. Dialogue acts are thus commonly represented as phrase-level labels such as statements, yes-no questions, open questions, acknowledgements, and so on.  Automatic recognition of dialogue acts is a fundamental component of many human-machine interacting systems that support natural language inputs. For instance, dialogue acts are typically used as an input to the dialogue manager to help deciding on the next action of the system: giving information when the user is asking a question, but eventually keeping quiet when the user is just acknowledging, giving a comment, or even asking for delaying the interaction. In the latter case, a system reaction may be perceived as intrusive. Beyond human-machine interaction, this task is also important for applications that rely on the analysis of human-human interactions, either oral, e.g., in recordings of meetings, or % lada - added reference according to rev 1 written, e.g., through the reply and mention-at structures in Twitter conversations. It is also essential for a large range of other applications, for example talking head animation, machine translation, automatic speech recognition or topic tracking. The knowledge of the user dialogue act is useful to render facial expressions of an avatar that are relevant to the current state of the discourse. In the machine translation domain, recognizing dialogue acts may bring relevant cues to choose between alternative translations, as the adequate syntactic structure may depend on the user intention. Automatic recognition of dialogue acts may also be used to improve the word recognition accuracy of automatic speech recognition systems, where a different language model is applied during recognition depending on the dialogue act. %lada - added reference according to rev 1,   To conclude, dialogue act recognition is an important building block of many understanding and interacting systems. %pav --I've commented the rest of the sentence, because it was not clear for 2 reviewers ) -- and typically completes semantic role labelling and dialogue management.  \subsection {Motivation and objectives} Researches on dialogue act recognition have been carried out for a long time, as detailed in Section. The majority of these works exploit supervised learning with lexical, syntactic, prosodic and/or dialogue history features. However, few approaches consider semantic features, while they may bring additional information and prove useful to improve the accuracy of the dialogue act recognition system. For instance,  a~frequent cause of recognition errors are ``unknown'' words in the testing corpus that never occur in the training sentences. Replacing specific named entities in the text  by their category has been proposed in the literature as a remedy to this issue. We investigate a more general solution that exploits lexical similarity between word vectors. These word vectors may be computed in various ways, but they typically include mostly lexical semantic information about the word itself as well as some syntactic information, e.g., related to the relative position or degree of proximity of pairs of words within a sentence. This additional information may be used to improve dialogue act recognition, in particular when the training and test conditions differ, or when the size of the training corpus is relatively small.  %goal In this work, we propose a new Deep Neural Network  based on Long Short-Term Memory  for the task of dialogue act recognition, and we compare its performance to a standard Maximum Entropy model. Our first objective is to leverage the modelling capacity of such a DNN in order to achieve dialogue act recognition with only the raw observed word forms, i.e., without any additional expert-designed feature. This model is described in Section. The second objective is to further validate this model both on a standard English DA corpus, as well as on two other languages, without changing anything in the model, in order to assess the genericity and robustness of the approach. These experiments are summarized in Section. Finally, our third objective is to study the impact of word embeddings, which have been shown to provide extremely valuable information in numerous Natural Language Processing  tasks, but which have never been used so far~\footnote{To the best of our knowledge at the time of submission} for dialogue act recognition. This study is summarized in Section. %The following Section presents a review of related works of the domain.  
"," Dialogue act recognition is an important component of a large number of natural language processing pipelines. Many research works have been carried out in this area, but relatively few investigate deep neural networks and word embeddings. This is surprising, given that both of these techniques have proven exceptionally good in most other language-related domains. We propose in this work a new deep neural network that explores recurrent models to capture word sequences within sentences, and further study the impact of pretrained word embeddings. We validate this model on three languages: English, French and Czech. The performance of the proposed approach is consistent across these languages and it is comparable to the state-of-the-art results in English. More importantly, we confirm that deep neural networks indeed outperform a Maximum Entropy classifier, which was expected. However, and this is more surprising, we also found that standard word2vec embeddings do not seem to bring valuable information for this task and the proposed model, whatever the size of the training corpus is. We thus further analyse the resulting embeddings and conclude that a possible explanation may be related to the mismatch between the type of lexical-semantic information captured by the word2vec embeddings, and the kind of relations between words that is the most useful for the dialogue act recognition task.",160
"   As an important task in Natural Language Generation , dialogue generation empowers a wide spectrum of applications, such as chatbot and customer service automation. In the past few years, breakthroughs in dialogue generation technology focused on a series of sequence-to-sequence models .  More recently, external knowledge is employed to enhance model performance. % , for instance, propose Mem2Seq using structured knowledge in task-oriented dialogue generation.   can assist dialogue generation by using knowledge triples. Similarly,  explore document as knowledge discovery for dialogue generation, and  utilize unstructured knowledge to explore in the open-domain dialogue generation. However, unaffordable knowledge construction and defective domain adaptation restrict their utilization.   Copy-based generation models  have been widely adopted in content generation tasks and show better results compared to sequence-to-sequence models when faced with out-of-vocabulary problem. Thanks to their nature of leveraging vocabulary and context distributions for content copy, it enables to copy the aforementioned named entities  appeared in the above context) from the upper context to improve the specificity of the generated text.    In the task of dialogue generation, we can often observe the phrases/utterance patterns across different ""similar dialogue"" instances. For example, in customer service, the similar inquiries from the customers will get similar responses from the staff. It motivates us to build a model that can not only copy the content within the upper context of the target dialogue instance, but also learn the similar patterns across different similar cases of the target instance. Such external copy can be critical in some scenarios.  %Fi Judge's questions, in the target court debate case, can be copied from both internal and external sources, and this `cross-copy' can enhance the dialougue generation essentially.    Figure this paper, we are aware of the possibility of copying  from adjacent Unfortunately, these methods only enable internal copy, e.g., copy the content within the target dialogue instance. External copy, e.g., copy content across different dialougue instances, is incapable. However, as Figure. depicted,   %is another effective network structure. It solved the problem that the traditional sequence-to-sequence model cannot solve the problem that the vocabulary of the output sequence will change with the length of the input sequence. %Copynet proposed humans tend to repeat entity names or even long phrases in conversation.And then generate the entity that appeared in the previous article will be copied. %Recently, Pointer networks and Copynet's variants have played a very important role in NLG. Among them, Pointer-Generator Networks  was proposed. %In order to copy the key information from the context as well as cope with the Out-Of-Vocabulary problem. It relies on the vocabulary distribution and context distribution, the extended vocabulary is further obtained. % GLMP proposed a global memory encoder and a local memory decoder to share external knowledge by Pointer networks. %}As general domain network structure, the pointer network  and Copynet  shows fine effect in general text generation tasks. It not only can solves the problem of domain adaptability poor in dialog generation, does not introduce external knowledge, but also address Out-Of-Vocabulary  problem and enable content copy. % Pointer networks  and Copynet  provided effective approach to address Out-Of-Vocabulary  problem and enable content copy.  %The more recent effort, Pointer-Generator Networks  , inherited their advantages by leveraging vocabulary and context distributions for content copy.   As shown in Figure., we propose two different kinds of copy mechanisms in this study: vertical copy context-dependent information within the target dialogue instance, and horizontal copy logic-dependent content across different 'Similar Cases' . This framework is labeled as Cross-Copy Networks . As exemplar dialogue depicted, judges may repeat  words, phrases or utterances from historical dialogues when those SCs sharing similar content, e.g., `A sue B because of X and Y'.  %In this study, 'Similar Cases'  refers to a similar dialogue for each dialogue. When generating the next sentence based on the historical dialogue, we can refer to the similar dialogue of the dialogue to obtain it. In this paper, we propose a new network: Cross-Copy Networks, which can not only copy the previous entity, but also learn the logic of dialogue generation and copied specific words, phrases or utterance from similar cases to deal with out-of-vocabulary  words. % The CCN has two pointers, one can copy the specific entity or sentence from the context and another can copy the process discourse or a complete sentence from SC.  % As shown in Figure 1, there are two similar cases and a target case. Our copy methods are divided into two types, internal copy and external copy.  internal copy: we can directly copy some specific entities words that appear in the context as the words to be generated.  external copy: we can copy related sentences or phrases in similar cases as the directly generated sentences.   % As shown in Fig., There are three samples of CCN:  Selective copy: it can copy some specific words or phrases from SC as sentences to be generated, as sample 1.  Cross copy: it copy specific entities from the context, and then copy some process-frame nature sentences in SC, as sample 2.  Deep copy: it can copy some process discourse directly as a generated sentence, usually this sentence appears frequently in the full text, as sample 3.  In order to validate the proposed model, we employ two different dialogue datasets from two orthogonal domains - court debate and customer service. We apply proposed CCN to both datasets for dialogue generation. Experiments show that our model achieves the best results. To sum up, our contributions are as follows:  % 
"," In the past few years, audiences from different fields witness the achievements of sequence-to-sequence models  to enhance dialogue content generation. While content fluency and accuracy often serve as the major indicators for model training, dialogue logics, carrying critical information for some particular domains, are often ignored. Take customer service and court debate dialogue as examples, compatible logics can be observed across different dialogue instances, and this information can provide vital evidence for utterance generation.  In this paper, we propose a novel network architecture - Cross Copy Networks  to explore the current dialog context and similar dialogue instances闁 logical structure simultaneously. Experiments with two tasks, court debate and customer service content generation, proved that the proposed algorithm is superior to existing state-of-art content generation models. % The traditional sequence-to-sequence model  has achieved good results in Natural Language Generation tasks. % For dialogue generation task in specific areas , some of the utterances by judge and customer service personnel to be saied usually contain specific logic and this utterances are highly similar. % Therefore, when generating the current utterance, we need to refer to not only the current context but also similar cases. % In this paper, we proposed a new neural network architecture named Cross Copy Networks , It locates entity in the context and the logical expression of similar cases by learning two conditional probability pointers. % We apply CCN to the legal dialogue data and customer service dialogue data for dialogue generation task. % Experiments show that our model achieves the best results.",161
"   In recent years, there has been an increased focus on the use of unannotated texts for modeling human language and on transfer learning in natural language processing . A wide variety of models have been proposed, ranging from context-independent word embeddings , to the more recent contextual representations . In particular, the Transformer-based  BERT  model  has generated considerable interest in the NLP community since its release. BERT outperformed the then state-of-the-art systems on a wide range of benchmark datasets when published, and has served as the basis of many studies since. These efforts include work that proposes improvements and/or modifications to the training objectives , knowledge distillation , multilinguality , and interpretation , to name a few. As a mark of its popularity, the term BERTology was coined to refer to the field of research relating to BERT .  % Multilinguality, zero-shot transfer, monolingual BERTs A thriving branch of BERTology involves BERT models for languages other than English.  released multilingual BERT  models trained on over a hundred languages. % what is good about multilingual model  analyze the representations produced by mulitlingual BERTs and find evidence that these representations generalize across languages for various downstream tasks, though language-specific information is retained. This language-agnostic subspace of multingual BERTs has also been observed in other studies, and is deemed to be the factor that allows for zero-shot transfer . % also find evidence that multilingual BERTs learn a language-agnostic subspace for linguistic information.  also show that these multilingual models have their embeddings partially aligned, which allows for zero-shot transfer. Furthermore, the embeddings can be further aligned through a fine-tuning based alignment procedure, improving the performance of multilingual models . % the curse of multilinguality  % 'we scale the number of languages for a fixed model capacity: more lan-guages leads to better cross-lingual performanceon low-resource languages up until a point, afterwhich the overall performance on monolingual andcross-lingual benchmarks degrades' While multilingual training can benefit also monolingual performance, as the number of languages covered by a multilingual model increases, the fraction of the model capacity available for any single language decreases.  % The number of languages included in a multilingual model, however, affects its performance, and a phenomenon referred to as the curse of multilinguality has been observed.  term as the curse of multilinguality the phenomenon where increasing the number of languages included in a model initially leads to better cross-lingual performance for low-resource languages, while eventually leading to overall degradation of both monolingual and cross-lingual performance. Work on language-specific BERT models has also shown that monolingual models tend to outperform multilingual models of the same size in monolingual settings . % To further benefit languages other than English, monolingual BERTs for various languages have been trained and released by the NLP community . %\todo{Is any transition needed here?} % SMP: added the following However, the question of whether it is possible to train multilingual models without loss of monolingual performance remains largely open. %To minimize the effect of the curse of multilinguality but still benefit from the language-agnostic subspace,  % Furthermore, the underlying reason why BERT works, the inner representation of BERT, a line of research has focused on the inner-workings of BERT [put this here or related work?] In this paper, we study whether it is feasible to pre-train a bilingual model for two remotely related languages without compromising performance at either language. Specifically, we train a Finnish-English bilingual BERT model  using a combination of the pre-training data of the original English BERT model and the Finnish BERT model introduced by , using an extended model vocabulary but otherwise fixing model capacity at BERT-Base size and retaining the number of pre-training steps. % carefully rephrase this! %While there has been reports of bilingual BERTs focusing on their cross-linguality, to the best of our knowledge, there has not been any work focusing on comparing the performance of bilingual models on natural language understanding  tasks with their monolingual counterparts. We evaluate the performance of the introduced bilingual model on a range of natural language understanding  tasks used to evaluate the monolingual models, which, to the best of our knowledge, has not been the focus of studies on bilingual BERT models. We find that \bbert{} achieves comparable performance on the GLUE  benchmark  with the original English BERT, and nearly matches the performance of the Finnish BERT on Finnish NLP tasks. Our results indicate that an extension of the vocabulary size is sufficient to allow the creation of fully bilingual models that perform on par with their monolingual counterparts in both of their languages.  
"," Language models based on deep neural networks have facilitated great advances in natural language processing and understanding tasks in recent years. While models covering a large number of languages have been introduced, their multilinguality has come at a cost in terms of monolingual performance, and the best-performing models at most tasks not involving cross-lingual transfer remain monolingual. In this paper, we consider the question of whether it is possible to pre-train a bilingual model for two remotely related languages without compromising performance at either language. We collect pre-training data, create a Finnish-English bilingual BERT model and evaluate its performance on datasets used to evaluate the corresponding monolingual models. Our bilingual model performs on par with Google's original English BERT on GLUE and nearly matches the performance of monolingual Finnish BERT on a range of Finnish NLP tasks, clearly outperforming multilingual BERT. % performance on Finnish datasets is not as good as FinBERT, how to put that into words %We find that the bilingual model achieves comparable performance as the English BERT and nearly matches the performance of FinBERT. We find that when the model vocabulary size is increased, the \bertbase{} architecture has sufficient capacity to learn two remotely related languages to a level where it achieves comparable performance with monolingual models, demonstrating the feasibility of training fully bilingual deep language models. % We describe the procedure taken to train this bilingual BERT. The model and all tools involved in its creation are freely available at \url{https://github.com/TurkuNLP/biBERT}",162
"  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  A long-standing challenge in computer science is to develop algorithms that can interact with human users via dialog in natural language~.  Of particular interest is task-oriented dialog, wherein a user interacts with a system to achieve some goal .  The system should understand the user's requests and assist them by taking the appropriate actions . In recent years, supervised learning approaches to this problem have become particularly popular, because they can potentially learn complex patterns without relying on hand-crafted rules. While such data-driven methods already demonstrate impressive performance in open-domain dialog , task-oriented dialog models face the additional difficulty of transferring skills to tasks and domains that were not present in the training data.  To address this issue, we present the Schema-guided Dialog Dataset for Transfer Learning  dataset, a collection of realistic, task-oriented dialogs, that is especially designed to test and facilitate the transfer of learned patterns between tasks.  Unlike open-domain dialogs, task-oriented dialogs are accompanied by a set of steps that are necessary to complete the task.  These steps are typically known a priori and thus do not have to be learned from the data. In fact, for practical applications it is desirable that we could make modifications to this logic without having to discard large parts of the dataset. The ideal sequences of steps that a dialog would follow to complete the task can be arranged in a graph . Together with the utterances or actions that are associated with the nodes of this graph, we hence call this a task schema, or simply schema. Note, that what we call `schema' is similar to the `task specification' of , but distinct from the `schemas' that only define slots and intents of a task as used by \citet{rastogi2019towards}. %or \citet{kimEighthDialogSystem2019}.     In a typical supervised model that is trained to, say, predict the next system action for a task-oriented dialog, the schema of the training tasks is implicitly captured by the learned model parameters. This makes generalizing to a new task difficult, as the implicitly memorized schema will no longer be appropriate .  With \DATASETNAME\ we provide explicit schema representations for each task and thereby enable models to condition on the schema .  To collect \DATASETNAME\ we use a Wizard of Oz setup , where the system's role is played by a human `wizard'. Based on our pilot studies, we found that the quality of crowd-sourced dialogs depends strongly on   We refined our approach through extensive internal testing and four rounds of pilot studies.  % All code and instructions are available as open source at \anonymous{\DATASETURL}.   Our aim is to create an ecologically valid dataset  with the following four attributes, which we believe are crucial for a dataset to be of high quality: %     The progression of difficulty allows better assessment of dialog models and potential for transfer learning across levels of difficulty.     \item Consistency on the system side. % The behavior of a task-oriented dialog system should be largely deterministic and not subject to the whims or personality of the wizard. %     In particular, we encourage wizards to follow the given task schema as closely as possible.     \item Explicit knowledge base queries. %     A large part of developing a dialog system is the implementation of application programming interface  calls, such as knowledge base queries. %     In \DATASETNAME\ we represent our dialogs as a three-party interaction wherein the system acts as the intermediary between a user and a knowledge base . %     Thus, models have to learn when to query the knowledge base, what the query should be, and how to explain the returned knowledge base item to the user.  \end{enumerate} % With these properties, we create a is ecologically valid, as described by.  With this paper, we contribute   The code for the latter setup, all collected  data, and all modeling code is freely available under \anonymous{\DATASETURL}.   
"," We present \DATASETNAME, a schema-guided task-oriented dialog dataset consisting of 127,833 utterances and knowledge base queries across 5,820 task-oriented dialogs in 13 domains that is especially designed to facilitate task and domain transfer learning in task-oriented dialog. Furthermore, we propose a scalable crowd-sourcing paradigm to collect arbitrarily large datasets of the same quality as \DATASETNAME. Moreover, we introduce novel schema-guided dialog models that use an explicit description of the task to generalize from known to unknown tasks.  We demonstrate the effectiveness of these models, particularly for zero-shot generalization across tasks and domains.",163
" % -------------------------------------------------------------- % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  .     %      % final paper: en-us version          % space normally used by the marker      This work is licensed under a Creative Commons       Attribution 4.0 International License.      License details:      \url{http://creativecommons.org/licenses/by/4.0/}.       } The relationship between a group of human languages can be characterized across several dimensions of variation , including  the temporal dimension, wherein languages have diverged from a common historical ancestor as in the case of Romance languages;  the spatial dimension, wherein the speaker communities are geographically adjacent as in the case of the Indo-Aryan and Dravidian languages of India; and  the socio-political dimension, wherein languages have evolved under shared political and/or religious forces as in the case of Arabic and Swahili. Languages, or language varieties, can be related across all these dimensions, which often results in a dialect continuum. Speakers of languages that constitute a dialect continuum can usually communicate with each other efficiently using their own mother tongue. The degree of intercomprehensibility between speakers of different language varieties within a continuum is mainly determined by linguistic similarities. A notable case of this phenomenon is the mutual intelligibility among the Slavic languages, which we study in this paper.   One of the goals of linguistics is to study and categorize languages based on objective measures of linguistic distance. The degrees of similarity at different levels of the linguistic structural organization can be seen as preconditions for, as well as predictors of, successful oral intercomprehension.  For closely-related languages, similarities at the pre-lexical, that is the acoustic-phonetic and phonological, level have been found to be better predictors of cross-lingual speech intelligibility than lexical similarities . In a different, yet relevant research direction,  have investigated non-linguists' perception of language variation using data from the popular spoken language guessing game, the Great Language Game . By analyzing the confusion patterns of the GLG's human participants, the authors have shown that factors predicting players' confusion in the game correspond to objective measures of similarity established by linguists. For example, both phylogenetic relatedness and overlap in phoneme inventories have been identified as factors of perceptual confusability  of languages in GLG.   The development of automatic systems that determine the identity of the language in a speech segment has received attention in the speech recognition community . State-of-the-art approaches for automatic spoken language identification, henceforth LID, are based on multilayer deep neural networks . DNN-based LID systems are parametric models that learn a mapping from spectral acoustic features of  speech to high-level feature representations in geometric space where languages are linearly separable. These models have shown tremendous success not only in discriminating between distant languages but also closely-related language varieties . Nevertheless, none of the previous works in spoken language recognition has analyzed the emerging representations from neural LID models for related languages. Thus, it is still unknown whether the distances in these representation spaces correspond to objective measurements of linguistic similarity and/or to non-linguists' perception of language variation. In this paper, we aim to fill this gap and consider the family of Slavic languages as a case study. Our key contribution is two-fold:   In this paper, we attempt to bridge different lines of research that have so far remained unconnected. On the one hand, we employ neural architectures from the field of spoken language recognition and build a robust model to identify languages in contemporary acoustic realizations of Slavic speech. On the other hand, we analyze the emerging language representations using techniques established by previous research in multilingual natural language processing . We consequently shed light on the speech modality and show how  speech signals can complement research done in computational studies of linguistic typology and language variation.   %  to the best of our knowledge  % The recognition of spoken language   % LID in speech technology  % untranscribed speech   % NN has made possible for end-to-end systems to be developed, while traditional approaches feature many components   % closely-related languages have similar phonotactics, but differ in acoustic realizations of segments and suprasegmental features   % language identity and objective linguistic measures of similarity   % The GLG   % similarity of representation in deep neural networks    % -------------------------------------------------------------- 
"," Deep neural networks have been employed for various spoken language recognition tasks, including tasks that are multilingual by definition such as spoken language identification. In this paper, we present a neural model for Slavic language identification in speech signals and analyze its emergent representations to investigate whether they reflect objective measures of language relatedness and/or non-linguists' perception of language similarity. While our analysis shows that the language representation space indeed captures language relatedness to a great extent, we find perceptual confusability between languages in our study to be the best predictor of the language representation similarity.",164
"   % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  .     %     % % final paper: en-uk version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International Licence.     % Licence details:     % \url{http://creativecommons.org/licenses/by/4.0/}.     %      % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. }  Aspect-level sentiment classification  is a fundamental task in sentiment analysis , which aims to infer the sentiment polarity  of a given opinion target in a review sentence. An opinion target, also known as aspect term, refers to a word or a phrase in review describing an aspect of an entity. For example, the sentence ``The 	extbf{tastes are great, but the service is dreadful}'' consists of two opinion targets, namely ``tastes'' and ``service''. User's sentiment towards the opinion target ``tastes'' is positive while negative in terms of target ``service''. Traditional methods usually focus on designing a set of features such as bag-of-words or sentiment lexicon to train a classifier  for ASC. Motivated by the great success of deep learning in computer vision, speech recognition and natural language processing, recent works use neural networks to  learn low-dimensional and continuous text representations without any feature engineering, and achieve competitive results on the ASC task.  From the above example, we can see that a sentence sometimes refers to several opinion targets and they may express different sentiment polarities, thus one main challenge of ASC is to separate different opinion contexts for different targets. To this end, abundant state-of-the-art works employ attention mechanism to capture sentiment words related to the given target, and then aggregate them to make sentiment prediction. Despite the effectiveness of attention mechanism, we argue that it fails to reach the full potential due to the limited ASC labeled data. It is well-known that the promising results of deep learning heavily rely on sufficient training data. However, the annotation of ASC data is very labour-intensive and expensive in real-world scenarios, because annotators need to not only identify all opinion targets in a sentence but also determine their corresponding sentiment polarity. The difficulty of annotation leads to that existing public aspect-level datasets are all relatively small-scale, which finally limits the potential of attention mechanism.  Despite the lack of ASC data, enormous labeled data of document-level sentiment classification  are available at online review sites such as Amazon and Yelp. These reviews contain substantial sentiment knowledge and semantic patterns. Therefore, one meaningful but challenging research question is how to leverage resource-rich DSC data to improve the low-resource task ASC. For this purpose,~ design the PRET+MULT framework to transfer sentiment knowledge from DSC data to ASC task through sharing shallow embedding and LSTM layer. Inspired by the capsule network,~ propose TransCap to share bottom three capsule layers, then separate two tasks only in the last ClassCap layer. Fundamentally, PRET+MULT and Transcap improve ASC by sharing parameters and multi-task learning, but they cannot accurately control and interpret what knowledge to be transferred. In this work, we directly focus on the aforementioned attention issue in the ASC task and propose a novel framework, Attention Transfer Network , to explicitly transfer attention knowledge from the DSC task for improving the attention capability of the ASC task. Compared with PRET+MULT and Transcap, our model achieves better results and retains good interpretability.  In the ATN framework, we adopt two attention-based BiLSTM networks, respectively, as the DSC module and base ASC module, and propose two different methods to transfer attention from DSC to ASC. The first transfer approach is called Attention Guidance. Specifically, we first pre-train an attention-based BiLSTM on large-scale DSC data, then exploit the attention weights from the DSC module as a learning signal to guide the ASC module to capture sentiment clues more accurately, thereby acheiving improvements. The second approach adopts the way of Attention Fusion, and directly incorporates the attention weights of the DSC module into the ASC module. The two approaches work in different ways and have their different advantages. Attention Guidance aims to learn the attention ability of the DSC module and has faster inference speed, since it does not use external attention from DSC during the testing stage. In contrast, Attention Fusion can leverage the attention knowledge of the DSC module during the testing stage and make more comprehensive predictions.  We conduct experiments on two benchmark datasets to evaluate different methods. The results indicate that the ATN model can be substantially improved by incorporating the two attention transfer approaches, and outperforms all compared methods on the ASC task.  %We conducted experiments on attention-based LSTM models using the SemEval 2014 dataset. The results show that attention-based LSTM can be substantially improved by incorporating our two proposed methods, and that the resulting model outperforms all baseline methods on aspect-level sentiment classification. Further analysis also shows the good interpretability of our approaches.  
","   Aspect-level sentiment classification  aims to detect the sentiment polarity of a given opinion target in a sentence. In neural network-based methods for ASC, most works employ the attention mechanism to capture the corresponding sentiment words of the opinion target, then aggregate them as evidence to infer the sentiment of the target. However, aspect-level datasets are all relatively small-scale due to the complexity of annotation. Data scarcity causes the attention mechanism sometimes to fail to focus on the corresponding sentiment words of the target, which finally weakens the performance of neural models. To address the issue, we propose a novel Attention Transfer Network  in this paper, which can successfully exploit attention knowledge from resource-rich document-level sentiment classification datasets to improve the attention capability of the aspect-level sentiment classification task. In the ATN model, we design two different methods to transfer attention knowledge and conduct experiments on two ASC benchmark datasets. Extensive experimental results show that our methods consistently outperform state-of-the-art works. Further analysis also validates the effectiveness of ATN. Our code and dataset are available at \url{https://github.com/1429904852/ATN}.",165
" For a conversational AI or digital assistant system , Natural Language Understanding  is an established component that produces semantic interpretations of a user request, which typically involves analysis in terms of domain, intent, and slot . For instance, the request ``Play a song by Taylor Swift"" can be interpreted as falling within the scope of Music domain with Play Song intent and Taylor Swift identified for Artist slot.  Improving the accuracy of the NLU component is important for satisfactory end-to-end user experience. Without an accurate semantic understanding of the user request, a conversational AI system cannot fulfill the request with a satisfactory response or action. As one of the most upstream components in the runtime workflow , NLU's errors also have a wider blast radius that propagates to all subsequent downstream components, such as dialog management, routing logic to back-end applications, and language generation.  A straight-forward way to improve NLU is through human annotations. For example, we can mine the user requests that resulted in unsatisfactory user experience and make ground-truth annotations on those requests that produced incorrect NLU outputs, which can be used as additional supervision data for improving the models or rule engines within NLU. However, this approach is labor-intensive and expensive. It requires at least multiple tiers of annotations , and it is hard to consider all underlying contextual conditions. It is also limited by the existing annotation guidelines that may not accurately reflect user expectations. Due to these limitations, leveraging user feedback, both implicit and explicit, from real production systems is emerging as a new area of research.     In this paper, we propose a scalable and automatic approach for improving NLU by leveraging implicit user feedback, with an insight that user interaction data and dialog context have rich information embedded from which user satisfaction and intention can be inferred. For instance, while interacting with a conversational AI system, dissatisfied users might often choose to intervene by stopping the system response in the middle and rephrasing the previous request to make it clearer with less room for ambiguous interpretation .  Our work makes three main contributions. First, to our knowledge, this work is the first in the literature to introduce a scalable and automatic approach of leveraging domain-agnostic implicit user feedback that can continuously improve the NLU component of a large-scale conversational AI system in production. Second, we propose a general framework for curating supervision data for improving NLU from live traffic that can be leveraged for various subtasks within NLU - e.g., the supervision data can be applied to improve individual semantic interpretation models  or a ranking/classification model across all interpretations . Last, we show with an extensive set of experiments on live traffic the performance of the proposed framework and its impact on improving NLU in the production system across 10 widely used domains. \def\year{2021}\relax %File: formatting-instructions-latex-2021.tex %release 2021.1 \documentclass[letterpaper]{article} % DO NOT CHANGE THIS \usepackage{aaai21}  % DO NOT CHANGE THIS \usepackage{times}  % DO NOT CHANGE THIS \usepackage{helvet} % DO NOT CHANGE THIS \usepackage{courier}  % DO NOT CHANGE THIS \usepackage[hyphens]{url}  % DO NOT CHANGE THIS \usepackage{graphicx} % DO NOT CHANGE THIS \urlstyle{rm} % DO NOT CHANGE THIS \def\UrlFont{\rm}  % DO NOT CHANGE THIS \usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT \usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT \frenchspacing  % DO NOT CHANGE THIS \setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS \setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS  \usepackage{amsfonts} \usepackage{amsmath} \usepackage{algorithm} \usepackage{xcolor} \usepackage[noend]{algpseudocode}  % \nocopyright %PDF Info Is REQUIRED. % For /Author, add all authors within the parentheses, separated by commas. No accents or commands. % For /Title, add Title in Mixed Case. No accents or commands. Retain the parentheses. % \pdfinfo{ % /Title  % /Author  % /TemplateVersion  %} %Leave this % /Title  % Put your actual complete title  within the parentheses in mixed case % Leave the space between \Title and the beginning parenthesis alone % /Author  % Put your actual complete list of authors  within the parentheses in mixed case. % Each author should be only by a comma. If the name contains accents, remove them. If there are any LaTeX commands, % remove them.  % DISALLOWED PACKAGES % \usepackage{authblk} -- This package is specifically forbidden % \usepackage{balance} -- This package is specifically forbidden % \usepackage{color  % \usepackage{CJK} -- This package is specifically forbidden % \usepackage{float} -- This package is specifically forbidden % \usepackage{flushend} -- This package is specifically forbidden % \usepackage{fontenc} -- This package is specifically forbidden % \usepackage{fullpage} -- This package is specifically forbidden % \usepackage{geometry} -- This package is specifically forbidden % \usepackage{grffile} -- This package is specifically forbidden % \usepackage{hyperref} -- This package is specifically forbidden % \usepackage{navigator} -- This package is specifically forbidden %  % \indentfirst} -- This package is specifically forbidden % \layout} -- This package is specifically forbidden % \multicol} -- This package is specifically forbidden % \nameref} -- This package is specifically forbidden % \usepackage{savetrees} -- This package is specifically forbidden % \usepackage{setspace} -- This package is specifically forbidden % \usepackage{stfloats} -- This package is specifically forbidden % \usepackage{tabu} -- This package is specifically forbidden % \usepackage{titlesec} -- This package is specifically forbidden % \usepackage{tocbibind} -- This package is specifically forbidden % \usepackage{ulem} -- This package is specifically forbidden % \usepackage{wrapfig} -- This package is specifically forbidden % DISALLOWED COMMANDS % \nocopyright -- Your paper will not be published if you use this command % \addtolength -- This command may not be used % \balance -- This command may not be used % \baselinestretch -- Your paper will not be published if you use this command % \clearpage -- No page breaks of any kind may be used for the final version of your paper % \columnsep -- This command may not be used % \newpage -- No page breaks of any kind may be used for the final version of your paper % \pagebreak -- No page breaks of any kind may be used for the final version of your paperr % \pagestyle -- This command may not be used % \tiny -- This is not an acceptable font size. % {2} %May be changed to 1 or 2 if section numbers are desired.  % The file aaai21.sty is the style file for AAAI Press % proceedings, working notes, and technical reports. %  % Title  % Your title must be in mixed case, not sentence case. % That means all verbs , % nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while % articles, conjunctions, and prepositions are lower case unless they % directly follow a colon or long dash  % \title{AAAI Press Formatting Instructions \\for Authors Using \LaTeX{} --- A Guide } % \author{  %     %Authors %     % All authors must be in the same font size and format. %     Written by AAAI Press Staff\textsuperscript{\rm 1}\thanks{With help from the AAAI Publications Committee.}\\ %     AAAI Style Contributions by Pater Patel Schneider, %     Sunil Issar,  \\ %     J. Scott Penberthy, %     George Ferguson, %     Hans Guesgen, %     Francisco Cruz, %     Marc Pujol-Gonzalez %     \\ % } % \affiliations{ %     %Afiliations  %     \textsuperscript{\rm 1}Association for the Advancement of Artificial Intelligence\\ %     %If you have multiple authors and multiple affiliations %     % use superscripts in text and roman font to identify them. %     %For example,  %     % Sunil Issar, \textsuperscript{\rm 2} %     % J. Scott Penberthy, \textsuperscript{\rm 3} %     % George Ferguson,\textsuperscript{\rm 4} %     % Hans Guesgen, \textsuperscript{\rm 5}. %     % Note that the comma should be placed BEFORE the superscript for optimum readability  %     2275 East Bayshore Road, Suite 160\\ %     Palo Alto, California 94303\\ %     % email address must be in roman text type, not monospace or sans serif %     publications21@aaai.org  %     % See more examples next % } %\iffalse % %Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it \title{A Scalable Framework for Learning From Implicit User Feedback to Improve Natural Language Understanding in Large-Scale Conversational AI Systems} \author {     Sunghyun Park\thanks{Equal contribution.}, Han Li\textsuperscript{\rm *}, Ameen Patel, Sidharth Mudgal, Sungjin Lee, Young-Bum Kim, Spyros Matsoukas, Ruhi Sarikaya \\ } \affiliations{     % Affiliations     Amazon Alexa AI \\     \{sunghyu, lahl, paameen, sidmsk, sungjinl, youngbum, matsouka, rsarikay\}@amazon.com } %\fi  \iffalse %Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it \title{My Publication Title --- Multiple Authors} \author {     Authors         First Author Name,\textsuperscript{\rm 1}         Second Author Name, \textsuperscript{\rm 2}         Third Author Name \textsuperscript{\rm 1} \\ } \affiliations {     % Affiliations     \textsuperscript{\rm 1} Affiliation 1 \\     \textsuperscript{\rm 2} Affiliation 2 \\     firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com } \fi   \newcommand{\red}[1]{{\color{red} #1}} \newcommand{\vecb}[1]{\mathbf{#1}}  \newtheorem{definition}{Definition}              \newpage \bibliography{citation} % \bibliographystyle{aaai21}  \end{document} 
"," Natural Language Understanding  is an established component within a conversational AI or digital assistant system, and it is responsible for producing semantic understanding of a user request. We propose a scalable and automatic approach for improving NLU in a large-scale conversational AI system by leveraging implicit user feedback, with an insight that user interaction data and dialog context have rich information embedded from which user satisfaction and intention can be inferred. In particular, we propose a general domain-agnostic framework for curating new supervision data for improving NLU from live production traffic. With an extensive set of experiments, we show the results of applying the framework and improving NLU for a large-scale production system and show its impact across 10 domains.",166
"   Chinese Word Segmentation  is a fundamental task for Chinese natural language processing , which aims at identifying word boundaries in a sentence composed of continuous Chinese characters. It provides a basic component for other NLP tasks like named entity recognition, dependency parsing, and semantic role labeling, etc.  Generally, most previous studies model the CWS task as a character-based sequence labeling task . Recently, pre-trained models  such as BERT  have been introduced into CWS tasks, which could provide prior semantic knowledge and boost the performance of CWS systems.  directly fine-tunes BERT on several CWS benchmark datasets.  fine-tunes BERT in a multi-criteria learning framework, where each criterion shares a common BERT-based feature extraction layer and owns a private projection layer.  combines Chinese character glyph features with pre-trained BERT representations. %  builds a unified BERT-based model for multi-criteria CWS tasks and fine-tunes it on eight CWS criteria jointly.  proposes a neural CWS framework WMSeg, which utilizes memory networks to incorporate wordhood information into the pre-trained model ZEN.  PTMs have been proved quite effective by fine-tuning on downstream CWS tasks. However, PTMs used in previous works usually adopt language modeling as pre-training tasks. Thus, they usually lack task-specific prior knowledge for CWS and ignore the discrepancy between pre-training tasks and downstream CWS tasks.     \end{table}  To deal with aforementioned problems of PTMs, we consider introducing a CWS-specific pre-trained model based on existing CWS corpora, to leverage the prior segmentation knowledge. However, there are multiple inconsistent segmentation criteria for CWS, where each criterion represents a unique style of segmenting Chinese sentence into words, as shown in Table. Meanwhile, we can easily observe that different segmentation criteria could share a large proportion of word boundaries between them, such as the boundaries between word units ``閺夊骸顭'', ``鏉╂稑鍙'' and ``閸楀﹤鍠呯挧'', which are the same for all segmentation criteria. It shows that the common prior segmentation knowledge is shared by different criteria.  In this paper, we propose a CWS-specific pre-trained model MetaSeg. To leverage shared segmentation knowledge of different criteria, MetaSeg utilizes a unified architecture and introduces a multi-criteria pre-training task. Moreover, to alleviate the discrepancy between pre-trained models and downstream unseen criteria, meta learning algorithm is incorporated into the multi-criteria pre-training task of MetaSeg.  Experiments show that MetaSeg could outperform previous works significantly, and achieve new state-of-the-art results on twelve CWS datasets. Further experiments show that  MetaSeg has better generalization performance on downstream unseen CWS tasks in low-resource settings, and improve Out-Of-Vocabulary  recalls. To the best of our knowledge, MetaSeg is the first task-specific pre-trained model especially designed for CWS.   
","     Recent researches show that pre-trained models  are beneficial to Chinese Word Segmentation .     However, PTMs used in previous works usually adopt language modeling as pre-training tasks, lacking task-specific prior segmentation knowledge and ignoring the discrepancy between pre-training tasks and downstream CWS tasks.     % However, existing approaches usually fine-tune general-purpose pre-trained models directly on separate downstream CWS corpora.     % These general-purpose pre-trained models usually adopt language modeling objectives, lack task-specific prior segmentation knowledge, and ignore the discrepancy between pre-training tasks and downstream CWS tasks.     In this paper, we propose a CWS-specific pre-trained model MetaSeg, which employs a unified architecture and incorporates meta learning algorithm into a multi-criteria pre-training task.     Empirical results show that MetaSeg could utilize common prior segmentation knowledge from different existing criteria and alleviate the discrepancy between pre-trained models and downstream CWS tasks.     Besides, MetaSeg can achieve new state-of-the-art performance on twelve widely-used CWS datasets and significantly improve model performance in low-resource settings.",167
"   % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  .     %      % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. }  The following instructions are directed to authors of papers submitted to COLING-2020 or accepted for publication in its proceedings. All authors are required to adhere to these specifications. Authors are required to provide a Portable Document Format  version of their papers. The proceedings are designed for printing on A4   paper.  Authors from countries in which access to word-processing systems is limited should contact the publication co-chairs Derek F. Wong , Yang Zhao  and Liang Huang  as soon as possible.  We may make additional instructions available at \url{http://coling2020.org/}. Please check this website regularly.   
","   This document contains the instructions for preparing a paper submitted   to COLING-2020 or accepted for publication in its proceedings. The document itself   conforms to its own specifications, and is therefore an example of   what your manuscript should look like. These instructions should be   used for both papers submitted for review and for final versions of   accepted papers. Authors are asked to conform to all the directions   reported in this document.",168
"  Automatic question answering is a very active area of research within natural language processing. %Open-domain question answering looks for methods to re-utilize systems across multiple domains. One possible way to approach this task is to look for answers in the text passages of a collection of documents. Recent research has shown promising results on developing neural models for passage retrieval tasks, including Retrieval Question Answering, Open Domain Question Answering, and MS MARCO. The models in these systems are often trained using the dual encoder framework where questions and passages are encoded separately. Training an effective neural retrieval model usually requires a large amount of high-quality data. To alleviate the need of high-quality data, training can be approached in two-stages: pre-training on noise data and fine tuning on a smaller amount of high-quality data, also regarded as ``gold"" data. % One significant advantage of the dual encoder framework is that, once the question and passage embeddings are available, efficient nearest neighbour search can be used to retrieve the passages that contain the answers to the questions.    When used for question answering, one advantage of the dual encoder is that training in batches allows to use, for each question, the passages that answer all the other questions in the batch as negatives. Given that the training batches are randomly sampled from all the question-passage pairs, the negatives in the batch are random in nature. While effective in many retrieval tasks, random negatives have the limitation of not being targeted nor challenging enough to clearly separate the passage that answers a given question from any other passage. How to sample the negatives in a way that widens this separation and improves the contrast between the correct and incorrect passages remains an open question.  % A viable approach to negative sampling is to use ``hard"" negatives that are specific to each question and answer  pair. In this paper we systematically explore the use of ``hard'' negatives in the neural passage retrieval models that we train using a two-stage approach. Using hard negatives as part of the dual encoder framework has shown advantageous in different tasks . %Using hard negatives as part of the dual encoder framework has shown advantageous in cross-lingual tasks. %For example, \citet{guo-etal-2018-effective} show that training with hard negatives generated by retrieving ``coarse"" negatives with low-resolution model improves the quality of the translation pairs retrieved with a dual encoder model. %Similarly, \citet{dpr} showed improvement when using hard negatives retrieved with a BM25 model in the passage retrieval part of the Open Domain Question Answering task. %In contrast to previous works,  We explore different types of negatives, and experiment using them in both the pre-training and fine-tuning stages. The types of negatives we tried are:   We first use hard negatives on the data that we use to pre-train the models. We leverage the question generator model described in and generate new questions for each of the passages we use in the pre-training stage . %The new questions are paired with the original passages. %The augmented set of question-passage pairs is used to train the first stage of the neural retrieval model. % It has been shown as an effective approach to improve passage retrieval models. During pre-training we use negatives generated from strategy 4\footnote{Or strategy 1, if strategy 4 is not feasible} to improve the retrieval model, as the other strategies could introduce more false negatives into the data. %Our initial experiments showed that using retrieval models to find the hard negatives at this point, often generated very noisy question-passage pairs, especially because our pre-training data includes synthetic pairs. %As the generated question passage pairs sometimes are noise, retrieval-based approaches may create better question-passage pairs than the synthetic pairs. %We only apply a heuristic based context negatives on this pre-training task. Next, we continue with the fine tuning stage  using a small amount of gold training data. At this stage, we explore all four types of negative sampling. To the best of our knowledge, this is the first work that explores the effectiveness of hard negatives for passage retrieval in a systematic way, and integrates them in the retrieval models  pre-training stage. Our overall experimental architecture is outlined in Figure.  %For each question-passage pair in the training set, we collect negatives using the strategies listed above and augment them into the training.  We conduct experiments with this approach on two passage retrieval tasks: Open Domain QA  and SQuAD) and MS MARCO. %Open Domain QA Natural Questions~, Open Domain QA SQuAD, and MS MARCO. Our results show that all four kinds of hard negatives improve the dual encoder models significantly with consistent performance gains across both tasks. However, depending on the types of questions and their domain, one kind of hard negative may perform better than the others in a particular task. For example, context negatives work best in NQ and semantic retrieval-based negatives  work best in SQuAD. We further ensemble the models trained on different types of hard negatives. The final models achieve state-of-the-art performance on Open Domain QA task with an improvement over prior works of 0.8--2.9 points on accuracy rates.  %\hl{highlight numbers here}.  The main contribution of this paper are:    
"," %In this paper we explore the discriminate training for neural passage retrieval models with hard negatives. %Four different hard negative sampling strategies are experimented, including one BM25 based hard negative, two semantic based hard negatives, and one heuristic hard negative. %For training the model, we employ a two stage dual encoder model with pre-training using synthetic data followed by a fine-tuning using the gold training data. %Discriminate training is applied on both stages. %The trained models are evaluated on 3 passage retrieval tasks from Open Domain QA NQ, Open Domain QA SQuAD, and MS MARCO. %Results show that all of them can improve the naive dual encoder models significantly with consistent performance gain over all three tasks. %However, there is no single type of hard negative perform best on all tasks. %Further analysis show that the synthetic question pre-training with discriminate training is an effective approach to improve the passage retrieval performance. %The best trained models establish the new state-of-the-art on retrieval tasks of Open Domain QA NQ and SQuAD. %",169
" % Events describe things that happen or occur in the world, mostly involving entities  who perform % or are affected by the events and spatio-temporal dimensions of the event . The same event may be mentioned in a document multiple times with different context. To recognize if two event mentions  refer to the same  contributes to the understanding of natural language text and resolving other NLP tasks.  In this work, we study the coreference resolution problem for both events and entities. Coreference resolution is commonly modeled as a binary classification problem : first learn features for each  mention, then classify two given mentions  \footnote{Some work maps the two mentions into a single matching score, e.g., ; this can be treated as a special case of binary classification.}. The essential step in this framework lies in the representation learning of each mention. However, prior work often failed to learn representations with powerful expressivity due to the following two reasons:  Point-wise representation learning.  % Usually a mention is surrounded by other words in a sentence.  Most work tries to learn mention representations by extracting features merely from that particular sentence including the mention. We argue that this routine of  representation learning in coreference does not match the end task  we are coping with: relation recognition of mention pairs. The predicted relation is for the input mention pair rather than individual mentions. By different context, two mentions can be referring to each other or not.  To fit the different scenarios, a mention should learn its representation by considering what its counterpart is.  Unstructured representation learning. An event mention consists of multiple arguments to describe the event: what, who, when, where, etc. Most prior work tried to encode all those elements into a single distributed representation vector and then compare the vectors of two mentions. This is less optimal since humans can recognize subjects, objects etc. and often compare event arguments of the same type . If, for instance, the event locations are different, people can  make a judgement quickly even without comparing other mention arguments.   Denoting a mention with a single distributed  vector lets machines lose the opportunity to conduct  fine-grained reasoning as humans do and uneasy to explain the model's prediction.  To promote the expressivity of representations, this work proposes % pairwise structured representation learning paired representation learning . \modelname\enspace alleviates the  aforementioned two limitations with   two designs:  \paragraph{Pairwise representation learning.} In this work, we treat each mention pair rather than a single mention as the object for the representation learning. Specifically, we will concatenate the two sentences   as a whole sequence and  forward it into a ROBERTa  system.\footnote{RoBERTa will put a special token ``SEP'' to separate the two sentences.}   RoBERTa takes the ``whole sequence'' as an input so that each token, including the  mention spans, in the two sentences are able to compare with other tokens from the very beginning. This is better than comparing the two mentions  after  learning a representation for each of them separately. We apply this pairwise representation learning to both event and entity coreference tasks. The binary classifier or an mention matching function in the end will take a pair of contextualized mention representations for reasoning.   \paragraph{Structured representation learning.} Looking at the following two sentence  and   % and we think we need to learn from humans' behaviors in recognizing event coreference.   : ``Over \textcolor{blue}{69,000 people} \underline{lost} their lives in the quake, including 68,636 in \textcolor{purple}{Sichuan province}.''  : ``Up to \textcolor{blue}{6,434 people} \underline{lost} their lives in Kobe earthquake and about 4,600 of them were from \textcolor{purple}{Kobe}.''  First, humans often determine the relationship between the two event mentions in  and  by comparing their triggers and arguments separately  as follows:  \textbullet\enspace ``69,000 people'' vs.  ``6,434 people''  \textbullet\enspace ``lost'' vs. ``lost''  \textbullet\enspace ``Sichuan province'' vs ``Kobe'' %   Second, the mismatch of some components may be  decisive or more decisive than that of others. For example, when people find the location ``Sichuan province'' does not match with ``Kobe'', they can directly claim the two events are not coreference even without looking at the whole sentence. This human behavior indicates that we should make full of the structure in an event, and an overall representation encompassing all event elements is less informative  to perform fine-grained cross-mention comparison which can actually improve the interpretability of the model predictions.  Overall, our \modelname~enables  two  mentions to learn from the context of each other, and improves the model's explainability by performing fine-grained reasoning. We report \modelname~on  both event coreference   and entity coreference benchmarks. Despite its simple architecture, \modelname~ surpasses the prior SOTA system by  big margins.                  
"," Co-reference of Events and of Entities are commonly formulated as binary classification problems, given a pair of events or entities as input. Earlier work addressed the main challenge in these problems -- the representation of each element in the input pair by:   modelling the representation of one element  without  considering the other element in the pair;  encoding all attributes of one element  into a single non-interpretable vector, thus losing the ability to compare %fine-grained cross-element attributes.  In this work we propose paired representation learning  for  coreference resolution. % \drc{ % In this work we propose pairwise structured representation learning  \XD{Do we want to change the model name? Since it is not structured for entity. And for event, most numbers are from trigger only representation} \dr{Maybe just PairedRL ? If so, we can cahange Pairwise Structure to ``Paired"" in the title and the rest of the paper.}for  coreference resolution.}  Given a pair of elements  our model treats the pair's sentences as a single sequence so that each element in the pair learns its representation by encoding its own context as well the other element's context. In addition, when representing events, \modelname\enspace is structured in that it represents the event's arguments to facilitate their individual contribution to the final prediction. As we show, in both  event  and entity coreference benchmarks, our unified approach, \modelname\enspace, outperforms prior state of the art systems with a large margin.  % \drc{Given a pair of elements  our model treats the pair's sentences as a single sequence so that each element in the pair learns its representation by encoding its own context as well the other element's context. In addition, when representing events, \modelname\enspace is structured in that it represents the event's arguments to facilitate their individual contribution to the final prediction. As we show, in both  event  and entity coreference benchmarks, our unified approach, \modelname\enspace, outperforms prior state of the art systems with a large margin.}  % \dr{Do we want to emphasize the structure part in the abstract? The contribution of structured is relatively small.} %This work studies the event and entity coreference which is commonly formulated as a binary classification problem given a pair of events or entities. The main challenge lies in the representation learning of each element in the input pair. However,  prior work mostly has the following drawbacks:  Systems model the representation of one object  with no consideration of   the other object in the pair;  Systems often encode all related attributes of one object  into a single and unexplainable vector; this reduces the model's interpretability and  mismatches the fact that humans tend to recognize the event relations by comparing fine-grained cross-event arguments. Motivated, we propose pairwise structured representation learning  \XD{Do we want to change the model name? Since it is not structured for entity. And for event, most numbers are from trigger only representation} for  coreference resolution. By ``Pairwise'', \modelname\enspace treats sentences of the input pair as a whole sequence so that each object in the pair learns its representation by encoding its own context as well the other's context. This paradigm applies to both event and entity coreference. In addition, \modelname\enspace develops a ``structured''  framework to represent all the event arguments so that each argument can explain its contribution to the final prediction.  In both  event  and entity coreference benchmarks, \modelname\enspace beats prior state of the art systems with big margins .",170
" Neural machine translation  has been explored typically in sentence-level translation settings. Such sentence-level nmt models inevitably suffer from ambiguities when multiple  %% semantically-different translations are accepted  interpretations are possible to a source sentence.  To address this issue, context-aware nmt models have recently been presented %to address the issue  to incorporate document-level information in translation. Most of the existing context-aware nmt models are end-to-end models which take as input the current source sentence to be translated and the context sentences, and then output a translation. These models are trained on document-level parallel data, namely, sentence pairs with surrounding, usually preceding, sentences in the source and target language. However, in practical scenarios, document-level bilingual data is limited in most language pairs and domains, % posing a challenge to building context-aware nmt systems .  In this study, we propose a simple yet effective approach to context-aware nmt  % consisting of  using two primitive components, a sentence-level nmt model and a document-level language model . This approach allows us to independently train the two components on bilingual data and monolingual data, respectively, without resorting to expensive document-level bilingual data.  % and thereby no document-level bilingual data is needed. To give a probabilistic foundation to this combination of two independent models, we exploit % take advantage of  the probabilistic nature of nmt decoding. When generating a sequence, a left-to-right decoder outputs a categorical probability distribution over the vocabulary at every time step. % . The decoder assigns higher probability to the tokens that would be more suitable at that step. Therefore,  % we can assume that  when multiple valid translations are possible to the source sentence, % , which has ambiguities a sentence-level nmt is confused by,  the decoder just gives a higher  % sequence  probability to the translation that is plausible without considering contexts.  % than to wrong ones. Our idea is to adjust the probability distributions in a context-aware manner using a document-level lm of the target language which  % is capable of modeling  models inter-sentential dependencies in the target side document.  % Since a network structure of nmt models evolves very quickly, model-agnostic approach like ours is more preferable than model-tweaking approach .  We evaluate our methods on English to French, Russian and Japanese translations with OpenSubtitles2018 corpus in terms of the bleu scores and contrastive discourse test sets. Experimental results confirmed that our method achieved comparable performance with existing context-aware nmt models.  The contributions of this paper are as follows:   
"," % There exist inevitable ambiguities in translating a single sentence, and we resort to context beyond the target sentence for resolving such ambiguities. Although many context-aware neural machine translation models have been proposed to incorporate contexts in translation,  most of those models are trained end-to-end on parallel documents aligned in sentence-level.  Because only a few domains  have such document-level parallel data, we cannot perform accurate context-aware translation in most domains. We therefore present a simple method to turn a sentence-level translation model into a context-aware model by incorporating a document-level language model into the decoder. Our context-aware decoder is built upon only a sentence-level parallel corpora and monolingual corpora; thus no document-level parallel data is needed. In a theoretical viewpoint, the core part of this work is the novel representation of contextual information using point-wise mutual information between context and the current sentence. We show the effectiveness of our approach in three language pairs, English to French, English to Russian, and Japanese to English, by evaluation in bleu and contrastive tests for context-aware translation.",171
" A keyphrase is a multi-word text representing highly abstractive information in a long document. Keyphrase extraction  is a task that aims to generate an appropriate keyphrase set for the given document, thus helping to identify salient contents and concepts from the document. Recently, the KE task has attracted much research interest since it serves as an important component of many downstream applications such as text summarization, document  classification, information retrieval and question generation.  Early KE systems commonly operate in an extractive manner, which usually consists of two steps: 1) selecting candidates from the source document using heuristic rules,  and 2) ranking the candidates list to determine which is correct. However, the two-step ranking approaches are usually based on feature engineering, which is labor-intensive. Motivated by the progress in sequence-to-sequence applications of neural networks, KE research's focus has gradually shifted to deep learning methods. \citet{DBLP:conf/acl/MengZHHBC17} first formulate KE as a sequence generation problem and introduce an attentive Seq2Seq framework to generate the keyphrase sequence conditioned on the input document. Compared with traditional methods, the Seq2Seq based method achieves superior performance.  Seq2Seq based KE is exposed to two major challenges: 1) Document-level representation learning. For any Seq2Seq generative framework, the latent hidden representation is a very important factor, and its quality will directly affect the decoder's performance. In KE task, the input is commonly a long document instead of a sentence, which poses a greater challenge to latent representation learning. 2) Modeling the compositionality of keyphrases set. The elements in the keyphrase set are dependent and correlated. That is, better modeling the inherent composition embodied in the keyphrase set during the learning process will effectively boost the diversity and quality of final results.   Recently, various approaches have been proposed to optimize the Seq2Seq generation framework in KE task. To learn a better latent representation, previous studies try to introduce different encoding structures  to address the two issues above simultaneously. We explore to incorporate the dependency tree for document representation learning in the encoder part. The syntactic dependency tree can help to locate key information in a document. In practice, the document graph  is constructed depending on the syntactic dependency tree, and then a convolution process will be operated over .  On the other hand, we rethink the implication of compositionality in the keyphrase set. In the training process of generative models, whether a candidate keyphrase should be generated not only hinges on the document itself, but also depends on the keyphrases that have already been generated. Therefore, a dynamic graph updating mechanism is introduced to explicitly modeling the inter-dependency among keyphrases. In our method, the graph structure in the encoder part will be dynamically updated according to the keyphrases generated in the decoder part. Concretely, after one keyphrase is decoded, its information will be transferred to modify the edge weights in the document graph through a score function, and the latent hidden representation will also be updated. In this approach, we could dynamically ensure the information exchange between encoder and decoder parts in both directions.   The contribution of this work is three-fold:  1) A novel generative framework, Div-DGCN, is proposed that leverages both the dynamic syntactic graph encoder and diversified inference process for KE. 2) A dynamic computation mechanism is adopted to model the compositionality in keyphrase set explicitly and then enhancing the information interchange between the encoder and decoder parts in the Seq2Seq architecture.  3) Extensive experiments conducted on five benchmarks show that our proposed method is effective against competitive baselines on several metrics.  
"," Keyphrase extraction  aims to summarize a set of phrases that accurately express a concept or a topic covered in a given document. Recently, Sequence-to-Sequence  based generative framework is widely used in KE task, and it has obtained competitive performance on various benchmarks. The main challenges of Seq2Seq methods lie in acquiring informative latent document representation and better modeling the compositionality of the target keyphrases set, which will directly affect the quality of generated keyphrases. In this paper, we propose to adopt the Dynamic Graph Convolutional Networks  to solve the above two problems simultaneously. Concretely, we explore to integrate dependency trees with GCN for latent representation learning. Moreover, the graph structure in our model is dynamically modified during the learning process according to the generated keyphrases. To this end, our approach is able to explicitly learn the relations within the keyphrases collection and guarantee the information interchange between encoder and decoder in both directions. Extensive experiments on various KE benchmark datasets demonstrate the effectiveness of our approach.",172
" Neural machine translation , as the state-of-the-art machine translation paradigm, has recently been approached with two different sequence decoding strategies. The first type autoregressive translation  models generate output tokens one by one following the left to right direction, but it is often criticized for its slow inference speed. The second type non-autoregressive translation  models adopt a parallel decoding algorithm to produce output tokens simultaneously, but the translation quality of which is often inferior to auto-regressive models.  % we transfer the AT knowledge to NAT models. A line of research argues that the lack of contextual dependency in target sentences potentially leads to the deteriorated performance of NAT models. To boost the NAT translation performance, many recent works resort to the knowledge transfer from a well-trained AT model. Typical knowledge tranfer methods include sequence-level knowledge distillation with translation outputs generated by strong AT models, word-level knowledge distillation with AT decoder representations, and fine-tuning on AT model by curriculum learning, etc.  %shared encoder  In this work, we adopt a multi-task learning framework with a shared encoder  to transfer the AT model knowledge into the NAT model.  %Our framework jointly optimizes the AT and NAT models to boost the NAT translation quality.  Specifically, we take the AT task as an auxiliary task by sharing the encoder parameters of both AT and NAT models. We hypothesize that AT and NAT encoders, although they belong to the same sequence-to-sequence learning task, capture different linguistic properties and representations of source sentences. To empirically verify our hypothesis, we evaluate the encoder on a set of probing tasks \iffalse estimate the representation similarity\fi for AT and NAT models. Further machine translation experiments on WMT14 EnglishGerman and WMT16 EnglishRomanian datasets confirm the effectiveness of the proposed Multi-Task NAT.   Our contributions are as follows:    \iffalse 
"," Non-Autoregressive machine Translation  models have demonstrated significant inference speedup but suffer from inferior translation accuracy. The common practice to tackle the problem is transferring the Autoregressive machine Translation  knowledge to NAT models, e.g., with knowledge distillation. In this work, we hypothesize and empirically verify that AT and NAT encoders capture different linguistic properties and representations of source sentences. Therefore, we propose to adopt the multi-task learning to transfer the AT knowledge to NAT models through the encoder sharing. Specifically, we take the AT model as an auxiliary task to enhance NAT model performance. Experimental results on WMT14 English$\Leftrightarrow$German and WMT16 English$\Leftrightarrow$Romanian datasets show that the proposed multi-task NAT achieves significant improvements over the baseline NAT models. In addition, experimental results demonstrate that our multi-task NAT is complementary to the standard knowledge transfer method, knowledge distillation. }",173
" In language modelling , we learn distributions over sequences of words, subwords or characters, where the latter two can allow an open-vocabulary generation. We rely on subword segmentation as a widespread approach to generate rare subword units . However, the lack of a representative corpus, in terms of the word vocabulary, constrains the unsupervised segmentation .  As an alternative, we could use character-level modelling, since it also has access to subword information , but we face long-term dependency issues and require longer training time to converge.   In this context, %our research question is: Is there a segmentation approach that is corpus-independent and provides a shorter length sequence than characters?  we focus on syllables, which are based on speech units: ``A syl-la-ble con-tains a sin-gle vow-el u-nit''. These are more linguistically-based units than characters, and behave as a mapping function to reduce the length of the sequence with a larger ``alphabet'' or syllabary. Their extraction can be rule-based and corpus-independent, but data-driven methods or hyphenation using dictionaries can approximate them as well .  % .  Previous work on syllable-aware neural \lm failed to beat characters in a closed-vocabulary generation at word-level ;  %,  however, we propose to assess syllables under three new settings.  %but we further discuss two points as follows.  First, we analysed an open-vocabulary scenario with syllables by disregarding additional functions in the input layer . Second, we extended the scope from 6 to 20 languages  to cover different levels of orthographic depth, which is the degree of grapheme-phoneme correspondence  and a factor that can increase complexity to syllabification. English is a language with deep orthography  whereas Finnish is transparent . Third, we distinguished rule-based syllabification with hyphenation tools, but also validated their proximity for LM. %we prefer to use rule-based syllabification whenever available, and only employ hyphenation proxies otherwise.   % even with specific segmentation rules.   Therefore, we revisit \lm for open-vocabulary generation with syllables using pure recurrent neural networks for a more diverse set of languages, and compare their performance against characters and other subword units. %We analyse their overlap with other subword units, and compare their performance against s. %We investigate %, and we also include languages % from the Universal Dependencies dataset ,  %with more transparent orthographies, such as in Turkish  or Finnish . %Our results confirm that syllables are a reliable segmentation setting for language modelling, even when the language presents a deep orthography.  %less-ambiguous syllabification due to  %a recent alphabetisation . We thereupon explore the syllables effect in another generation task such as NMT.   
"," Language modelling is regularly analysed at word, subword or character units, but syllables are seldom used. Syllables provide shorter sequences than characters, they can be extracted with rules, and their segmentation typically requires less specialised effort than identifying morphemes. We reconsider syllables for an open-vocabulary generation task in 20 languages.  We use rule-based syllabification methods for five languages and address the rest with a hyphenation tool, which behaviour as syllable proxy is validated. With a comparable perplexity, we show that syllables outperform characters, annotated morphemes and unsupervised subwords. Finally, we also study the overlapping of syllables concerning other subword pieces and discuss some limitations and opportunities.",174
" Sanskrit is considered as one of the oldest Indo-Aryan languages. The oldest known Sanskrit texts are estimated to be dated around 1500 BCE. A large corpus of religious, philosophical, socio-political and scientific texts of multi cultural Indian Subcontinent are in Sanskrit. Sanskrit, in its multiple variants and dialects, was the Lingua Franca of ancient India ~. Therefore, Sanskrit texts are an important resource of knowledge about ancient India and its people. Earliest known Sanskrit documents are available in the form called Vedic Sanskrit. Rigveda, the oldest of the four Vedas, that are the principal religious texts of ancient India, is written in Vedic Sanskrit. In sometime around 5\textsuperscript{th} BCE, a Sanskrit scholar named Panini ~ wrote a treatise on Sanskrit grammar named Ashtadhyayi, in which Panini formalized rules on linguistics, syntax and grammar for Sanskrit. Panini's grammar is globally appreciated for its insightful analysis of Sanskrit and completeness of its descriptive coverage of the spoken standard language of Panini's time.   Ashtadhyayi \footnote{https://www.britannica.com/topic/Ashtadhyayi} is the oldest surviving text and the most comprehensive source of grammar on Sanskrit today and provides often unique information on Vedic, regional and  socio-linguistic usage. Ashtadhyayi literally means eight chapters and these eight chapters contain around 4000 sutras or rules in total. These rules completely define the Sanskrit language as it is known today. Ashtadhyayi is remarkable in its conciseness and contains highly systematic approach to grammar. Because of its well defined syntax and extensively well codified rules, many researchers have made attempts to codify the Panini閳ユ獨 sutras as computer programs to analyze Sanskrit texts. This paper tries to address the problem of unavailability of benchmark corpus and provides morphological analysis method for derivative nouns as a result of Sanskrit suffixes applied on root verbs and nouns using a machine learning approach.  \subsection{Introduction of Pratyaya in Sanskrit} Different ways of inflectional word formation as mentioned by ~ are as below:   Here in this introduction, we will explain more about primary and secondary suffixes. Sanskrit is a rich inflected language and depends on nominal and verbal inflections for communication of meaning ~. A fully inflected unit is called pada. The subanta  padas are the inflected nouns and the tinanta  padas are the inflected verbs.      \subsubsection{Kridanta subanta } These are formed when the primary affixes called krit are added to verbs to derive substantives, adjectives or indeclinable. DAtum nAma karoti iti krit. Kridanta play a vital role in understanding Sanskrit language. Many morphological analyzers are lacking the complete analysis of Kridanta. Examples of krit pratyaya are as below:  krit suffixes are mainly of seven types viz. tavyat, tavya, anIyara,Ryat, yat, kyap, kelimer.  \subsubsection{Taddhitanta subanta } The secondary derivative affixes called taddhit derive secondary nouns from primary nouns. Some examples of taddhit pratyaya are as below:  taddhit suffixes are mainly of fourteen types.  
"," This paper presents first benchmark corpus of Sanskrit Pratyaya  and inflectional words  formed  due to suffixes along with neural network based approaches to process the formation and splitting of inflectional words. Inflectional words spans the primary and secondary derivative nouns as the scope of current work. Pratyayas are an important dimension of morphological analysis of Sanskrit texts. There have been Sanskrit Computational Linguistics tools for processing and analyzing Sanskrit texts. Unfortunately there has not been any work to standardize \& validate these tools specifically for derivative nouns analysis. In this work, we prepared a Sanskrit suffix benchmark corpus called Pratyaya-Kosh to evaluate the performance of tools. We also present our own neural approach for derivative nouns analysis while evaluating the same on most prominent Sanskrit Morphological Analysis tools. This benchmark will be freely dedicated and available to researchers worldwide and we hope it will motivate all to improve morphological analysis in Sanskrit Language.",175
"  Sanskrit is one of the oldest of the Indo-Aryan languages. The oldest known Sanskrit texts are estimated to be dated around 1500 BCE. It is the one of the oldest surviving languages in the world. A large corpus of religious, philosophical, socio-political and scientific texts of multi cultural Indian Subcontinent are in Sanskrit. Sanskrit, in its multiple variants and dialects, was the Lingua Franca of ancient India . Therefore, Sanskrit texts are an important resource of knowledge about ancient India and its people. Earliest known Sanskrit documents are available in the form called Vedic Sanskrit. Rigveda, the oldest of the four Vedas, that are the principal religious texts of ancient India, is written in Vedic Sanskrit. In sometime around 5\textsuperscript{th} century BCE, a Sanskrit scholar named pARini wrote a treatise on Sanskrit grammar named azwADyAyI, in which pARini formalized rules on linguistics, syntax and grammar for Sanskrit. azwDyAyI is the oldest surviving text and the most comprehensive source of grammar on Sanskrit today. azwADyAyI literally means eight chapters and these eight chapters contain around 4000 sutras or rules in total. These rules completely define the Sanskrit language as it is known today. azwADyAyI is remarkable in its conciseness and contains highly systematic approach to grammar. Because of its well defined syntax and extensively well codified rules, many researchers have made attempts to codify the pARini閳ユ獨 sutras as computer programs to analyze Sanskrit texts.  \subsection{Introduction of Sandhi and Sandhi Split in Sanskrit}  Sandhi refers to a phonetic transformation at word boundaries, where two words are combined to form a new word. Sandhi literally means 'placing together'  is the principle of sounds coming together naturally according to certain rules codified by the grammarian pARini in his azwADyAyI. There are 3 different types of Sandhi as defined in azwADyAyI.  An example for each type Sandhi is shown below:  \end{quote}  Sandhi Split on the other hand, resolves Sanskrit compounds and 閳ユ笡honetically merged閳  words into its constituent morphemes. Sandhi Split comes with additional challenge of not only splitting of compound word correctly, but also predicting where to split. Since Sanskrit compound word can be split in multiple ways based on multiple split locations possible, split words may be syntactically correct but semantically may not be meaningful.  \end{quote}   \subsection{Existing Work on Sandhi} The current resources available for doing Sandhi in open domain are not very accurate. Three most popular publicly available set of Sandhi tools viz. JNU, UoH \& INRIA tools are mentioned in table .     \end{table*}   An analysis and description of these tools is present in the paper on Sandhikosh . The same paper introduced a dataset for Sandhi and Sandhi Split verification and compared the performance of the tools in table  on that dataset.  Neural networks have been used for Sandhi Split by many researchers, for example ,  and . The task of doing Sandhi has been mainly addressed as a rule based algorithm e.g. . There is no research on Sandhi using neural networks in public domain so far. This paper describes experiments with Sandhi operation using neural networks and compares results of suggested approach with the results achieved using existing Sandhi tools .  \subsection{Existing Work on Sandhi Split}  Many researchers like  and  have tried to codify pARini閳ユ獨 rules for achieving Sandhi Split along with a lexical resource.  proposed a statistical method based on Dirichlet process. Finite state methods have also been used . A graph query method has been proposed by .  Lately, Deep Learning based approaches are increasingly being tried for Sandhi Split.  used a one-layer bidirectional LSTM to two parallel character based representations of a string.  and  proposed deep learning models for Sandhi Split at sentence level.  uses a double decoder model for compound word split.  The method proposed in this paper describes an RNN based, two stage deep learning method for Sandhi Split of isolated compound words without using any lexical resource or sentence information.  In addition to above, there exist multiple Sandhi Splitters in the open domain. The prominent ones being JNU Sandhi Splitter  , UoH Sandhi Splitter  and INRIA Sanskrit reader companion. The paper  compares the performance of above 3 tools with their results. This was an attempt to create benchmark in the area of Sanskrit Computational Linguistics.   
"," This paper describes neural network based approaches to the process of the formation and splitting of word-compounding, respectively known as the Sandhi  and Vichchhed, in Sanskrit language. Sandhi is an important idea essential to morphological analysis of Sanskrit texts. Sandhi leads to word transformations at word boundaries. The rules of Sandhi formation are well defined but complex, sometimes optional and in some cases, require knowledge about the nature of the words being compounded. Sandhi split or Vichchhed is an even more difficult task given its non uniqueness and context dependence. In this work, we propose the route of formulating the problem as a sequence to sequence prediction task, using modern deep learning techniques. Being the first fully data driven technique, we demonstrate that our model has an accuracy  better than the existing methods on multiple standard datasets, despite not using any additional lexical or morphological resources. The code is being made available at https://github.com/IITD-DataScience/Sandhi\_Prakarana",176
"   % Unsupervised representation learning allows models to learn high-level latent representations from unlabeled data.  % Models pretrained from unsupervised data can be fine-tuned with a small amount of labeled data. % % Deep probabilistic generative models presents a powerful approach to learn representations by modeling the data generation process.  % Variational AutoEncoders  are one of the popular approaches to representation learning by modeling the latent features in a unit Gaussian space. % Vector-Quantized VAE  is a method to learn discrete representations from data.  Speech waveforms are a complex, high-dimensional form of data influenced by a number of underlying factors, which can be broadly categorized into linguistic contents and speaking styles. % Learning disentangled latent representations from speech has a wide set of applications in generative tasks, including speech synthesis, data augmentation, voice transfer, and speech compression. Downstream tasks such as speech recognition  and speaker classification  can also benefit from such learned representations. % A pre-trained model can also be fine-tuned for classification tasks such as speech recognition and speaker classification. % \ngyuzh{what about rephrase it like: Downsteam tasks such as speech recognition  and speaker classification  can also benefit from such learned representations.}  Because of the cost, complexity, and privacy concerns around collecting labeled speech data, there has been a lot of interest in unsupervised representation learning for speech. Of particular interest is to learn representations for speech styles from unsupervised data due to the difficulty in describing prosody with human labels.  Some previous works aim to learn global representations from entire speech sequences. % Global style tokens learn a dictionary of embeddings from speech without prosody labels. % As another example, Hsu et al.  model disentangled speech styles with a hierarchy of variational autoencoder . % Hu et al.  proposed a content and style separation model by pre-training on a single-speaker dataset with text transcription and minimizing mutual information  between the content and style representation.  Other works try to learn fine-grain localized representations of speech. %  apply self-supervised learning to unlabeled speech data and extract localized latent representations that can be fine-tuned for speech recognition. % FHVAE learns a sequence of high-level features by applying VAE to every frame. %  leverages vector-quantized VAE  to learn a discrete sequence representation of speech.  We propose a framework to learn both global and localized representation of speech. In order to disentangle content and style representations, we apply  a local encoder with VQ layer to learn a discrete per-timestep representation of the speech that captures the linguistic contents and  a global VAE to extraction per-utterance representations to reflect the speech styles. We further disentangle the local and global representations with a mutual information loss. We evaluate the quality of linguistic and style representations by running speech and speaker recognition models on the reconstructed speech. We also show that the global representation captures the speaker information well enough that we can obtain a speaker classification model by training a linear projection layer on top of the global representation with only one example per speaker.   
"," We present an approach for unsupervised learning of speech representation disentangling contents and styles. Our model consists of:  a local encoder that captures per-frame information;  a global encoder that captures per-utterance information; and  a conditional decoder that reconstructs speech given local and global latent variables. Our experiments show that  the local latent variables encode speech contents, as reconstructed speech can be recognized by ASR with low word error rates , even with a different global encoding;  the global latent variables encode speaker style, as reconstructed speech shares speaker identity with the source utterance of the global encoding. Additionally, we demonstrate an useful application from our pre-trained model, where we can train a speaker recognition model from the global latent variables and achieve high accuracy by fine-tuning with as few data as one label per speaker. % % \rpang{How about: Our deep generative model consists of:  a local encoder that captures per-frame information;  a global encoder that captures per-utterance information; and  a conditional decoder that reconstruct speech given local and global latent variables, potentially extracted from different utterances. Our experiments show that  the local latent variables encode speech contents, since reconstructed speech can be recognized by ASR with low word error rates , even with a different global encodings;  the global latent variables encode speaker style, as reconstructed speech shares speaker identity with the source utterance of the global encoding and a speaker recognition model can be trained from the global latent variables with as few as one supervised example per speaker.  % }",177
"  %Sentiment analysis  is one of the fundamental tasks in natural language processing that aims to find the attitude that the author expressed in his/her sentence. One of the important sub-tasks of SA is aspect based sentiment analysis in which the goal is to find the sentiment polarity toward a specific aspect mentioned in the sentence. Due to the importance of ABSA, several sub-tasks has been proposed and studied for this problem, including aspect category extraction, aspect term extraction, opinion word extraction and opinion summarization . Among these sub-tasks, Targeted Opinion Word Extraction  is an important sub-task that might provide useful information to explain the prediction of the sentiment polarity from an ABSA system. In particular, the goal of TOWE is to find the words that express the attitude of the author toward a specific target mentioned in that sentence. For instance, in the sentence ``The food is good, especially their more basic dishes, and the drinks are delicious"", the word ``good"" is the opinion word for the target ``food"" while delicious is the opinion word for the target word ``drinks"". Among different applications, TOWE can be used for target-oriented sentiment analysis  and pair-wise opinion summarization .  %Sentiment analysis  is one of the fundamental tasks in natural language processing that aims to find the attitude that the author expressed in his/her sentence. One of the important sub-tasks of SA is aspect based sentiment analysis in which the goal is to find the sentiment polarity toward a specific aspect mentioned in the sentence. Due to the importance of ABSA, several sub-tasks has been proposed and studied for this problem, including aspect category extraction, aspect term extraction, opinion word extraction and opinion summarization . Among these topics, Targeted Opinion Word Extraction  is an important task that might provide useful information to explain and/or improve the sentiment polarity prediction of the ABSA systems. In particular, given a target word  in the input sentence, the goal of TOWE is to find the words in the sentence  that help to express the attitude of the author toward the aspect represented by the target word. For instance, in the sentence ``The food is good, especially their more basic dishes, and the drinks are delicious"", ``good"" is the opinion word for the target word ``food"" while the opinion words for the target word ``drinks"" would involve ``delicious''. Among different applications, TOWE finds its application in target-oriented sentiment analysis  and pair-wise opinion summarization .   %Targeted Opinion Word Extraction  is an important task in aspect based sentiment analysis  of sentiment analysis . Given a target word  in the input sentence, the goal of TOWE is to find the words in the sentence  that help to express the attitude of the author toward the aspect represented by the target word. For instance, in the sentence ``The food is good, especially their more basic dishes, and the drinks are delicious"", ``good"" is the opinion word for the target word ``food"" while the opinion words for the target word ``drinks"" would involve ``delicious''. As the opinion words might provide useful information to explain and/or improve the sentiment prediction of the ABSA systems, TOWE can be applied in different problems, including target-oriented sentiment analysis  and pair-wise opinion summarization .  Targeted Opinion Word Extraction  is an important task in aspect based sentiment analysis  of sentiment analysis . Given a target word  in the input sentence, the goal of TOWE is to identify the words in the sentence  that help to express the attitude of the author toward the aspect represented by the target word. For instance, as a running example, in the sentence ``All warranties honored by XYZ  are disappointing."", ``disappointing"" is the opinion word for the target word ``warranties"" while the opinion words for the target word ``company"" would involve ``reputable''. Among others, TOWE finds its applications in target-oriented sentiment analysis  and opinion summarization .   %As the opinion words might provide useful information to explain and/or improve the sentiment prediction of the ABSA systems, TOWE can be applied in different problems, including target-oriented sentiment analysis  and pair-wise opinion summarization .  %A notable problem is that although the related tasks of TOWE has been extensively explored in the past, there have been only a few work to explicitly consider the TOWE problem in the literature . In particular, the most related task of TOWE is opinion word extraction  that aims to locate the terms used to express attitude explicitly in the sentence . A key difference between OWE and TOWE is that OWE does not require the opinion words to tie to any target words in the sentence  while the opinion words in TOWE should be explicitly paired with a given target word. Note that some previous works have also attempted to jointly predict the target and opinion words ; however, the target words are still not paired with their corresponding opinion words in these studies .    %Among the previous works for TOWE, t  The early approach for TOWE has involved the rule-based and lexicon-based methods  while the recent work has focused on deep learning models for this problem . One of the insights from the rule-based methods is that the syntactic structures  of the sentences can provide useful information to improve the performance for TOWE . However, these syntactic structures have not been exploited in the current deep learning models for TOWE . Consequently, in this work, we seek to fill in this gap by extracting useful knowledge from the syntactic structures to help the deep learning models learn better representations for TOWE. In particular, based on the dependency parsing trees, we envision two major syntactic information that can be complementarily beneficial for the deep learning models for TOWE, i.e., the syntax-based opinion possibility scores and syntactic word connections for representation learning. First, for the syntax-based possibility scores, our intuition is that the closer words to the target word in the dependency tree of the input sentence tend to have better chance for being the opinion words for the target in TOWE. For instance, in our running example, the opinion word ``disappointing"" is sequentially far from its target word ``warranties"". However, in the dependency tree shown in Figure , ``disappointing"" is directly connected to ``warranties"", promoting the distance between ``disappointing"" and ``warranties""  in the dependency tree as an useful feature for TOWE. Consequently, in this work, we propose to use the distances between the words and the target word in the dependency trees to obtain a score to represent how likely a word is an opinion word for TOWE . These possibility scores would then be introduced into the deep learning models to improve the representation learning for TOWE.  In order to achieve such possibility score incorporation, we propose to employ the representation vectors for the words in the deep learning models to compute a model-based possibility score for each word in the sentence. The model-based possibility scores also aim to quantify the likelihood of being an opinion word for each word in the sentence; however, they are based on the internal representation learning mechanism of the deep learning models for TOWE. To this end, we propose to inject the information from the syntax-based possibility scores into the models for TOWE by enforcing the similarity/consistency between the syntax-based and model-based possibility scores for the words in the sentence. The rationale is to leverage the possibility score consistency to guide the representation learning process of the deep learning models  to generate more effective representations for TOWE. In this work, we employ the Ordered-Neuron Long Short-Term Memory Networks   to obtain the model-based possibility scores for the words in the sentences for TOWE. ON-LSTM introduces two additional gates into the original Long Short-Term Memory Network  cells that facilitate the computation of the model-based possibility scores via the numbers of active neurons in the hidden vectors for each word.  %The second type of syntactic information employed for TOWE in this work considers the dependency connections between the words in the sentence.   %As the deep learning models need to compute a representation vector for each word to perform opinion word prediction in TOWE,   %While the possibility scores aim to improve the representation vectors for TOWE via the syntax-based possibility features, the second type of syntactic information in this work seeks to do so by leveraging the dependency connections between the words to infer the effective context words to be encoded in the representation vector for each word in the sentence. In particular, motivated by our running example, we argue that the effective context words for the representation vector for a current word in TOWE involve the neighboring words of the current word and the target word in the dependency tree. For instance, consider the running example with ``warranties"" as the target word and ``reputable"" as the word we need to compute the representation vector. One the one hand, it is important to include the information of the neighboring words of ``reputable""  in the representation so the models can know the context for the current word . On the other hand, the information about the target word  should also be encoded in the representation vector for ``reputable"" so the models can be aware of the context of the target word and make appropriate comparison in the representation to decide the label  for ``reputable"" in this case. Note that this syntactic connection mechanism allows the models to de-emphasize the context information of ``I'' in the representation for ``reputable"" to improve the representation quality. Consequently, in this work, we propose to formulate these intuitions into an importance score matrix whose cells quantify the contextual importance that a word would contribute to the representation vector of another word given a target word for TOWE. These importance scores will be conditioned on the distances between the target word and the other words in the dependency tree. Afterward, the score matrix will be consumed by a Graph Convolutional Neural Network  model  to produce the final representation vectors for opinion word prediction.  For the second type of syntactic information in this work, the main motivation is to further improve the representation vector computation for each word by leveraging the dependency connections between the words to infer the effective context words for each word in the sentence. In particular, motivated by our running example, we argue that the effective context words for the representation vector of a current word in TOWE involve the neighboring words of the current word and the target word in the dependency tree. For instance, consider the running example with ``warranties"" as the target word and ``reputable"" as the word we need to compute the representation vector. On the one hand, it is important to include the information of the neighboring words of ``reputable""  in the representation so the models can know the context for the current word . On the other hand, the information about the target word  should also be encoded in the representation vector for ``reputable"" so the models can be aware of the context of the target word and make appropriate comparison in the representation to decide the label  for ``reputable"" in this case. Note that this syntactic connection mechanism allows the models to de-emphasize the context information of ``I'' in the representation for ``reputable"" to improve the representation quality. Consequently, in this work, we propose to formulate these intuitions into an importance score matrix whose cells quantify the contextual importance that a word would contribute to the representation vector of another word, given a target word for TOWE. These importance scores will be conditioned on the distances between the target word and the other words in the dependency tree. Afterward, the score matrix will be consumed by a Graph Convolutional Neural Network  model  to produce the final representation vectors for opinion word prediction.  Finally, in order to further improve the induced representation vectors for TOWE, we introduce a novel inductive bias that seeks to explicitly distinguish the representation vectors of the target-oriented opinion words and those for the other words in the sentence. We conduct extensive experiments to demonstrate the benefits of the proposed model, leading to the state-of-the-art performance for TOWE in several benchmark datasets.  %Finally, in order to further improve the induced representation vectors for TOWE, we introduce a novel inductive bias that seeks to explicitly distinguish the representation vectors of the target-oriented opinion words  and those for the other opinion words  in the sentence . Extensive experiments are conducted to demonstrate the benefits of the proposed model, leading to the state-of-the-art performance for TOWE in several datasets.  %Finally, in order to further improve the induced representation vectors for TOWE, we introduce a novel inductive bias that seeks to explicitly distinguish the representation vectors of the target-related opinion words  and those for the other opinion words  in the sentence . As both target-related and non-target opinion words can be used to express the opinion of the author , we expect that the explicit representation distinction would help to better separate the two types of opinion words based on the target word, eventually improving the performance for TOWE in this work. We conduct extensive experiments to demonstrate the benefits of the proposed model, leading to the state-of-the-art performance for TOWE in several datasets.   %the close distance between ``disappointing"" and the target word will suggest the models to include the information of ``warranties"" into the representation vector for ``disappointing"" while the long distance between ``warranties"" and ``reputable"" can help to prevent/mitigate that for the representation vector of ``reputable"". The presence of the information from the target word in the representation vectors will help the models to successfully accept ``disappointing"" as an opinion word and reject ``reputable"" in this case.   %the close words to the target word would provide more effective information to induce the representation vectors for a word in the sentence in TOWE than the farther ones.   %we argue that the syntactic neighboring words in the dependency tree would provide effective information to induce the representation vector for a word in opinion word prediction. For instance, in the running example with the target word ``warranties"", the close distance between ``disappointing"" and the target word will suggest the models to include the information of ``warranties"" into the representation vector for ``disappointing"" while the long distance between ``warranties"" and ``reputable"" can help to prevent/mitigate that for the representation vector of ``reputable"". The presence of the information from the target word in the representation vectors will help the models to successfully accept ``disappointing"" as an opinion word and reject ``reputable"" in this case.  %employ the dependency connections between the words to infer the effective context words .    %employ the syntactic neighboring words to compute the representation vectors for a word in the sentence for TOWE.   %extends the popular Long Short-Term Memory Networks  by introducing two additional gates  in the hidden vector computation. These new gates controls how long each neuron in the hidden vectors should be activated across different time steps  in the sentence . Based on such controlled neurons, the model-based importance score for a word can be determined by the number of active neurons that the word possesses in the operation of ON-LSTM. To our knowledge, this is the first time ON-LSTM is applied for RE in the literature.  %How can we encode the syntax-based importance scores of the words into a deep model? In this paper, we propose to employ the syntax-based importance scores to retain or update the information encoded in the representations of each word. In particular, those words that are syntactically more important should retain more information in the computation graph of the deep model while the information about less important words should be discarded more frequently. In order to impose this information update policy in our model, we use the new proposed architecture Ordered-Neuron Long Short-Term Memory  . ON-LSTM is an extension of the well-known Long Short-Term Memory  with two additional gates . These new gates are employed to control the frequency of updating each neuron across different time steps  in the sentence. Concretely, the values of the master forget and input gates determine how much information in the hidden vector of the LSTM cell should be retained or updated based on the word at the current time step. Thereby, one can infer the importance scores inferred by the model  using the values of the master forget or input gates. So, based on this characteristics of ON-LSTM, to encode the syntax-based importance scores into our model, we propose to exploit the syntax-based importance scores to regulate model-based importance scores. Specifically, in training time, we encourage the model-based scores to be consistent with syntax-based importance scores.  %the two words ``disappointing"" and ``warranties"" are directly connected to each other.  %Early feature-based models  has shown that syntactical structure of the sentence is useful for TWOE. More specifically, the application of dependency tree for TOWE is two fold:  Pairwise Word Importance: Dependency tree is useful to infer the relative importance of a word toward another word  in the same sentence. This relative importance could be helpful for TOWE to attend to the important words for the target word. To infer pair-wise importance of two words using dependency tree, one can computes the distance between two words in the dependency tree. For instance, as a running example, in the sentence ``All warranties honored by HP  are disappointing"", the opinion word ``disappointing"" is sequentially far from its target word ``warranties"". However, in the dependency tree shown in Figure , the two words ``disappointing"" and ``warranties"" are directly connected to each other. The short distance between these two words could be helpful to infer the importance of the word ``disappointing"" for the target ``warranties"".  Word Connection: Dependency tree could provide better contextual information for each word via the connections of the word with its head and dependants, thus it helps to improve word representations. Thereby, dependency tree could benefit TOWE. For instance, in the running example, the head of ``reputable"" is ``company"" while the head of ``warranties"" is ``disappointing"". Therefore, it would be easier to infer that the opinion word ``disappointing"" is related to the target word ``warranties"" and ``reputable"" is irrelevant.   %Besides the difference between the rule-based and deep learning models for TOWE regarding the representation learning methods, the rule-based methods have exploited the syntactic structures  of the sentences to improve the performance for TOWE while  %In particular, the related tasks of TOWE involves target word extraction/aspect term exaction  , and opinion word extraction   . A key difference between OWE and TOWE is that the opinion words in OWE are general and do not need to tie to any target words in the sentence while TOWE explicitly   %Despite its potential benefits, TOWE has only been studied by a few works in the past, characterizing the early rule-based and lexicon-based approaches  and very recently deep learning models .   %In the literature, feature-based models and deep learning model has been proposed for both target word extraction  and opinion word extraction . While joint models predict both the opinion and the target words, they cannot pair up them, thus being unable to solve the task of TOWE. In the literature, only a few of works have studied the task of TOWE, including the early attempts with the rule-based and lexicon-based approaches  and the recent works with deep learning models for TOWE .    
"," Targeted opinion word extraction  is a sub-task of aspect based sentiment analysis  which aims to find the opinion words for a given aspect-term in a sentence. Despite their success for TOWE, the current deep learning models fail to exploit the syntactic information of the sentences that have been proved to be useful for TOWE in the prior research. In this work, we propose to incorporate the syntactic structures of the sentences into the deep learning models for TOWE, leveraging the syntax-based opinion possibility scores and the syntactic connections between the words. We also introduce a novel regularization technique to improve the performance of the deep learning models based on the representation distinctions between the words in TOWE. The proposed model is extensively analyzed and achieves the state-of-the-art performance on four benchmark datasets.  %Deep learning models have been shown to achieve the state-of-the-art performance for TOWE in the recent studies.  %While previous feature-based models have shown syntactical structure  is useful for this task, recent deep neural nets ignore this information in their model. To address this limitation, in this paper, we propose a new approach which incorporates syntactical structure  into deep neural nets. More specifically, our model employs the dependency tree to capture the relative importance of the words to the aspect-term and to encode the connections between words. Our extensive experiments on four benchmark datasets prove the superiority of the proposed model, leading to new state-of-the-art results on all datasets. Moreover, detailed analysis shows the effectiveness of the components of the proposed model.",178
"  Aspect-based Sentiment Analysis  is a fine-grained version of sentiment analysis  that aims to find the sentiment polarity of the input sentences toward a given aspect. We focus on the term-based aspects for ABSA where the aspects correspond to some terms  in the input sentence. For instance, an ABSA system should be able to return the negative sentiment for input sentence ``The staff were very polite, but the quality of the food was terrible.'' assuming ``food'' as the aspect term.    %Aspect based sentiment analysis  is a fine-grained version of sentiment analysis . In ABSA, the goal is to find the sentiment polarity of the sentence toward a given aspect. In the literature, two versions of aspect have been proposed:  Aspect-category: Aspect categories are a set of pre-defined categories in which the given sentence contains opinion of the author toward one of them. Aspect categories may not explicitly appear in the sentence.  Aspect-term: Aspect term is a subsequent of the sentence in which the given sentence express sentiment toward it. For instance in the example The staff were very polite, but the quality of the food was terrible., the author has a positive sentiment toward aspect-category service and negative sentiment toward aspect-term food. In this paper, we introduce a novel model for sentiment analysis toward aspect-term.  %The early attempts for ABSA have performed feature engineering to produce useful features for the statistical models  for this problem . One limitation of these feature-based models is that they require significant human effort and linguistic background to design effective features. In order to overcome this limitation,  %The typical network architectures for ABSA in the literature involve convolutional neural networks  , recurrent neural networks  , memory networks , attention  and gating mechanisms .  %automatically induce effective features for ABSA and  Due to its important applications , ABSA has been studied extensively in the literature. In these studies, deep learning has been employed to produce the state-of-the-art performance for this problem . Recently, in order to further improve the performance, the syntactic dependency trees have been integrated into the deep learning models  for ABSA . Among others, dependency trees help to directly link the aspect term to the syntactically related words in the sentence, thus facilitating the graph convolutional neural networks   to enrich the representation vectors for the aspect terms.  %Although the graph-based models have achieved decent performance for ABSA, these models have   However, there are at least two major issues in these graph-based models that should be addressed to boost the performance. First, the representation vectors for the words in different layers of the current graph-based models for ABSA are not customized for the aspect terms. This might lead to suboptimal representation vectors where the irrelevant information for ABSA might be retained and affect the model's performance. Ideally, we expect that the representation vectors in the deep learning models for ABSA should mainly involve the related information for the aspect terms, the most important words in the sentences. Consequently, in this work, we propose to regulate the hidden vectors of the graph-based models for ABSA using the information from the aspect terms, thereby filtering the irrelevant information for the terms and customizing the representation vectors for ABSA. In particular, we compute a gate vector for each layer of the graph-based model for ABSA leveraging the representation vectors of the aspect terms. This layer-wise gate vector would be then applied over the hidden vectors of the current layer to produce customized hidden vectors for ABSA. In addition, we propose a novel mechanism to explicitly increase the contextual distinction among the gates to further improve the representation vectors.  %as the hidden vectors at different layers of the graph-based models tend to capture different levels of contextual information, the gate vectors for the different layers should also maintain some level of contextual distinction. To this end, we propose a novel mechanism to explicitly increase the contextual distinction among the gate vectors to further improve the quality of the representation vectors.  The second limitation of the current graph-based deep learning models is the failure to explicitly exploit the overall importance of the words in the sentences that can be estimated from the dependency trees for the ABSA problem. In particular, a motivation of the graph-based models for ABSA is that the neighbor words of the aspect terms in the dependency trees would be more important for the sentiment of the terms than the other words in the sentence. The current graph-based models would then just focus on those syntactic neighbor words to induce the representations for the aspect terms. However, based on this idea of important words, we can also assign a score for each word in the sentences that explicitly quantify its importance/contribution for the sentiment prediction of the aspect terms. In this work, we hypothesize that these overall importance scores from the dependency trees might also provide useful knowledge to improve the representation vectors of the graph-based models for ABSA. Consequently, we propose to inject the knowledge from these syntax-based importance scores into the graph-based models for ABSA via the consistency with the model-based importance scores. In particular, using the representation vectors from the graph-based models, we compute a second score for each word in the sentences to reflect the model's perspective on the importance of the word for the sentiment of the aspect terms. The syntax-based importance scores are then employed to supervise the model-based importance scores, serving as a method to introduce the syntactic information into the model. In order to compute the model-based importance scores, we exploit the intuition that a word would be more important for ABSA if it is more similar the overall representation vector to predict the sentiment for the sentence in the final step of the model. In the experiments, we demonstrate the effectiveness of the proposed model with the state-of-the-art performance on three benchmark datasets for ABSA. In summary, our contributions include:   %, we propose to obtain another important score for each word in the sentence based on the representation vectors from the models. These model-based importance scores are then   %In particular, for ABSA, some words might introduce more useful information to predict the sentiment of the aspect terms than the the other words in the sentence   %In particular, some words in a given sentence might involve more useful information for relation prediction in RE than the other words, and the dependency tree for this sentence can help to better identify those important words and assign higher importance scores for them . We expect that introducing such importance information for the words in the deep learning models might lead to improved performance for RE. Consequently, in this work, we propose to obtain an importance score for each word in the sentences from the dependency trees . These will serve as the general tree representation to incorporate the syntactic information into the deep learning models for RE.  %In particular, as the aspect terms are the most important words in the sentences for ABSA,     %Syntactical structure, e.g., dependency tree, could shorten the distance between syntactically related words thus improve the contextualized representation of the words. In order to incorporate the syntactical tree into the deep models, recent work mainly employs the graph convolutional network   to model the interaction between words based on the syntactic tree. In order to emphasize on the given aspect term, current models use the representation of the aspect term generated by GCN either directly for final classification or as a gate to filter out features of a sequential model. However, these methods cannot benefit from the information of the given aspect term to control the information flow in the graph based model. Moreover, it is expected that the words that are syntactically related to the given aspect term should convey more information about the sentiment toward it. Unfortunately, non of the existing work considers this relative importance in the final representation of the sentence. In order to address these issue, we propose a new graph based model which employs the semantic of the given aspect term to control interaction between nodes/words in the GCN model and to emphasize more on the syntactically important words in the final representation of the sentence.  %Due to the application of ABSA in other downstream applications, e.g., opinion mining, it has gained a lot of attention in natural language processing community and several methods have been proposed for this task. Early attempts employed feature engineering to extract useful features for statistical models like SVM . These methods require extensive human effort and strong linguistic knowledge. They also suffer from low generalization ability. Due to these limitations, neural networks and deep models have superseded feature based models and obtain promising results in ABSA . Early deep models for ABSA have exploited sequential models  , convolutional neural nets  or even memory networks . In order to improve the performance, attention  and gating mechanism  has also been widely adopted in deep models. Recently, it has been shown that syntactical information could also improve the performance of deep models . Syntactical structure, e.g., dependency tree, could shorten the distance between syntactically related words thus improve the contextualized representation of the words. In order to incorporate the syntactical tree into the deep models, recent work mainly employs the graph convolutional network   to model the interaction between words based on the syntactic tree. In order to emphasize on the given aspect term, current models use the representation of the aspect term generated by GCN either directly for final classification or as a gate to filter out features of a sequential model. However, these methods cannot benefit from the information of the given aspect term to control the information flow in the graph based model. Moreover, it is expected that the words that are syntactically related to the given aspect term should convey more information about the sentiment toward it. Unfortunately, non of the existing work considers this relative importance in the final representation of the sentence. In order to address these issue, we propose a new graph based model which employs the semantic of the given aspect term to control interaction between nodes/words in the GCN model and to emphasize more on the syntactically important words in the final representation of the sentence.   %In particular, in this paper, we propose a novel model which employs the representation of the given aspect term to compute a gate. This gate is applied over the output of one layer of GCN. By doing so, the information represented in the aspect term would erase non-relevant information in each node/word obtained by its interaction with its neighbors in one aggregation step in GCN. As different layers of GCN capture different substructure of the graph, e.g., 1-hop vicinity vs 2-hop vicinity, we propose to exploit different gates in different layers. To ensure the gates in different layers are not the same, we propose a novel method to encourage diversity among gates in different layers of the GCN. Moreover, in addition to exploiting the semantic of the aspect term to control interactions between nodes/words in the GCN, in this paper, we propose to encourage the model to emphasize on the words that are syntactically important to the aspect term. In particular, we use the distance between a word to the aspect term in the dependency tree as an indication of the syntactic importance of the word to the aspect term. This importance is employed as supervision signal to encourage the model to emphasize on the words that are syntactically important to the aspect term. This is obtained in the final layer of the model when the sentiment prediction is performed. More specifically, we first estimate the semantic importance of each word by employing the final representation of the word as the input to a classifier to predict the label distribution and then we compute the KL-Divergence between this label distribution predicted by the word representation and the label distribution predicted by the sentence representation. If the two label distribution are more similar, it shows that the word representation contains most of the information that the model consumes to perform the final classification. Finally, in order to ensure those words which are syntactically important to the aspect term are semantically important in the model too, we decrease the divergence between distribution of the syntactic score and semantic score for each word via KL-Divergence between these two distributions.  %Our extensive experiments on three benchmark datasets, empirically prove the effectiveness of the proposed model leading to new state-of-the-art results in all three benchmark datasets.      A novel method to regulate the GCN-based representation vectors of the words using the given aspect term for ABSA.   A novel method to encourage the consistency between the syntax-based and model-based importance scores of the words based on the given aspect term.   Extensive experiments on three benchmark datasets for ABSA, resulting in new state-of-the-art performance for all the datasets.   
"," Aspect-based Sentiment Analysis  seeks to predict the sentiment polarity of a sentence toward a specific aspect. Recently, it has been shown that dependency trees can be integrated into deep learning models to produce the state-of-the-art performance for ABSA. However, these models tend to compute the hidden/representation vectors without considering the aspect terms and fail to benefit from the overall contextual importance scores of the words that can be obtained from the dependency tree for ABSA. In this work, we propose a novel graph-based deep learning model to overcome these two issues of the prior work on ABSA. In our model, gate vectors are generated from the representation vectors of the aspect terms to customize the hidden vectors of the graph-based models toward the aspect terms. In addition, we propose a mechanism to obtain the importance scores for each word in the sentences based on the dependency trees that are then injected into the model to improve the representation vectors for ABSA. The proposed model achieves the state-of-the-art performance on three benchmark datasets.  %These models employ graph based neural nets to incorporate syntactical structure into the model. However, they ignore the aspect term information to control the interaction between words in the syntax tree which is modeled by graph neural net.  % Moreover, they neglect the consistency between the syntactic and semantic importance of the words toward the given aspect. %Moreover, the relative importance of the words to the given aspect term based on their syntactical role is neglected in the final representation produced by the existing syntax-aware models. To address these two issues, in this paper, we introduce a new syntax-aware model which incorporates gating mechanism to control information flow in the graph based model using the given aspect term. It also ensures the words that are syntactically important to the aspect term are more pronounced in the final representation of the sentence. Our extensive experiments on three benchmark datasets empirically prove the effectiveness of the proposed model leading to new state-of-the-art results on all three benchmark datasets.",179
"  Curriculum learning trains a model by using easy examples first and gradually adding more difficult examples. It can speed up learning and improve generalization in supervised learning models .  %With curriculum learning, models are trained according to a curriculum that sorts training examples according to difficulty. %The model is first trained with only the easiest examples. %More difficult examples are gradually added according to some pre-determined schedule. %Training with curriculum can lead to faster model convergence than a baseline model trained without a curriculum . %With machine learning models and data sets continuing to grow, and knowing the impact of model training on the environment, there is a need for efficient model training . %\Hong{I donot understand why do you need to include the above sentence.  What is the gain?} %Hong.  In Bengio et al's ""curribulum learning"" work, they found that CL has an effect on the speed of convergence and better optimization of non-convex functions. You may want to evaluate in your work, in addition to the performance. A major drawback of existing curriculum learning techniques is that they rely on heuristics to measure the difficulty of data, and either ignore the competency of the model during training or rely on heuristics there as well. For example, sentence length is often used as a proxy for difficulty in NLP tasks . %Similarly, the number of objects in an image has been used as a proxy for difficulty in an image recognition task . Such heuristics can be useful but have limitations. First, the heuristic chosen may not actually be a proxy for difficulty. Depending on the task, long sequences could signal easier or harder examples, or have no signal for difficulty. Second, a model's notion of difficulty may not align with the heuristic imposed by a human developing the model. It may be that examples that appear difficult for the human are in fact easy for the model to learn.  Competency was recently introduced as a mechanism to determine when new examples should be added to the training data . %However, in that work the competency schedule was ad hoc, and did not actually look at the competency of the model but assumed a schedule according to learning heuristics.  However, in that work competency is a monotonically increasing function of a pre-determined initial value. %competency, . Once set, competency is not evaluated during training. Ideally, model competency should be measured at each training epoch, so that the training data could be appropriately matched with the model at a given point in the training. If a model is improving, then more difficult training data can be added at the next epoch.  But if performance declines, then those difficult examples can be removed, and a smaller, easier training set can be used in the next epoch.  In this study, we propose to estimate both the difficulty of examples and the ability of deep learning models as latent variables based on model performance using Item Response Theory , a well-studied methodology in psychometrics for test set construction and subject evaluation . IRT models estimate latent parameters such as difficulty for examples  %Hong: I changed ""examples"" to ""samples"" in earlier context, but if you don't like it, just change them back.  Just stick to one use throughout the paper.   and a latent ability parameter for individuals . %Hong.  Here you may want to use ""model"" ability instead of ""subject"" IRT models are learned by administering a test to a large number of subjects, collecting and grading their responses, and using the subject-response matrix to estimate the latent traits of the data. These learned parameters can be used to estimate the ability of future subjects, based on their graded responses to the examples. %Hong.  Similarly, would you introduce ""model"", not ""test-takers""  IRT has not seen wide adoption in the machine learning community, primarily due to the fact that fitting IRT models requires a large amount of human annotated data for each example. However, recent work has shown that IRT models can be fit using machine-generated data instead of human-generated data as input .  Because IRT learns example difficulty and subject ability together,  %it is an interesting framework to consider for the problem of curriculum learning.  in this work we propose replacing heuristics for learned parameters in curriculum learning. First, we experiment with replacing a typical difficulty heuristic  with learned difficulty parameters. Second, we propose \modelname~, a novel curriculum learning framework that uses the estimated ability of a model during the training process to dynamically identify appropriate training data. At each training epoch, the latent ability of the model is estimated using output labels generated at the current epoch. Once ability is known, only training data that the model has a reasonable chance of labeling correctly is included in training. As the model improves, the estimated ability will improve, and more training examples will be added.  To the best of our knowledge, this is the first work to learn a model competency during training that is directly comparable to the difficulty of the examples. %Hong. You may want to define terminology up front. Here you say ""training data pool"".  Earlier you say ""examples"".  Perhaps use ""items"", and then define it and how it is used in your paper %To be explicit, in this work our goal is to  Our study will test the following three hypotheses: H1: Using learned latent difficulties instead of difficulty heuristics leads to better held-out test set performance for models trained using curriculum learning, H2: A dynamic data selection curriculum learning strategy that probes model ability during training leads to better held-out test set performance than a static curriculum learning strategy that does not take model ability into account, H3: Dynamic curriculum learning is more efficient than static curriculum learning, leading to faster convergence. We test our hypotheses on the GLUE classification data sets .  Our contributions are as follows:  we demonstrate that for curriculum learning, learned difficulty outperforms traditional difficulty heuristics,   we introduce a novel curriculum learning framework which automatically selects training data based on the estimated ability of the model, and  we show that training using \modelabbr~leads to better performance than both traditional curriculum learning methods and a fully supervised competitive baseline.  Our findings support the overall curriculum learning framework, and demonstrate that learning difficulty and ability lead to further performance improvements beyond common heuristics.\footnote{Code implementing our experiments and learned difficulty parameters for the GLUE data sets are available at \url{https://jplalor.github.io/irt}.} %We will release the learned difficulty parameters for the GLUE data sets as a resource for the community to further explore learned difficulties and dynamic curriculum learning.}%\footnote{Code and data used for this work, including learned difficulty values, will be released upon publication.}  
","   Curriculum learning methods typically rely on heuristics to estimate the difficulty of training examples or the ability of the model. %  They also either ignore the ability of the model or rely on heuristics there as well.   %In this work we show that learning difficulty and ability is more effective for curriculum learning than applying heuristics.   In this work, we propose replacing difficulty heuristics with learned difficulty parameters.    We also propose \modelname~, a strategy that probes model ability at each training epoch to select the best training examples at that point.   %\modelabbr~adds data at a rate commensurate with the model's capability, in contrast to scheduled curricula that add data at a predetermined rate.   We show that models using learned difficulty and/or ability outperform heuristic-based curriculum learning models on the GLUE classification tasks.   %Using \modelabbr~to train LSTM models further improves performance.   %Experimental results demonstrate that \modelabbr~outperforms static CL strategies on a number of NLP classification tasks.",180
" Entity normalization and variant generation are fundamental for a variety of other tasks such as semantic search and relation extraction . Given an entity name , the goal of entity normalization is to convert  to a canonical form , while the goal of entity variant generation is to convert  to a set of different textual representations that refer to the same entity as E .   \looseness=-1 Typically, entity normalization and variant generation are done by first performing entity linking , i.e., matching entity names appearing in some context  to named entities in curated knowledge bases , then use the canonical form or variations  residing in the KBs to complete the tasks. Unfortunately, in some scenarios, such as search , entity names are not surrounded by context.  Furthermore, for specialized domain-specific applications, there may not be a knowledge base to govern the names of the relevant entities. Thus, entity linking is not always applicable. In this paper,  we take the view  %the problem differently, in particular, we argue  that entity normalization and variant generation can be done without contextual information or  external KBs if we understand the internal structures of entity names.  % Fundamental to the success of entity linking is the availability of the contextual information  and ontological information .    %  and there are no external KBs that we can use as master datasets to match input entity names. \todo{One may argue that DBpedia and Wikipedia is a good proxy. It may be useful to talk about related work taking this view here.} % For example, when searching \example{General Electric Company}, we need to also consider variations like \example{GE Co.}, \example{G.E.}, \example{General Electric}, etc. without relying on any contextual information and external KBs.  % Performing entity normalization and variant generation in a contextless fashion is extremely challenging because we have only the surface forms of entity names.  % Several attempts have been made to parse the structured representation of entity names. As observed in , entity names often have  %structured representation  implicit structures  that can be exploited to solve entity normalization and variant generation. Table  shows how we can manipulate such structured representations of entity names to generate different variations without help from context or external knowledge.  % For example, if we know that \example{Michael} is \component{first} and \example{Jordan} is \component{last} in \example{Michael Jordan}, we can generate two variations:  \example{M. Jordan}  and  \example{Jordan, Michael} .     \end{table*}  Declarative frameworks are proposed in  to allow  %high-skill  developers to manually specify rules that parse  entity names into  %the  a structured representation. %of enity names. To avoid such low-level manual effort,  used fully supervised methods for identifying nested entities embedded in flat named entities. Unfortunately, labeled data are rarely available to leverage these methods in the real-world.  To mitigate the need for training data,  proposed an active learning %-based  system, LUSTRE, to semi-automatically learn rules for mapping entity names to their structured representations.  By  %making use of  using regex-based extractors and a list of comprehensive dictionaries that capture crucial domain vocabularies, LUSTRE can generate rules that achieve SoTA results. However, for more complex and realistic scenarios, dictionaries may not be available and regex-based extractors alone are not expressive enough. Moreover, as shown in Section, LUSTRE cannot handle long entities such as machine logs.  In this paper, we present a  %learning  framework that learns high-quality BERT-CRF models for parsing entity names into   structured representations  %of entity names  in low-resource settings, namely, when no labeled data is available. The proposed framework is essentially an active learning-based approach that learns from human interactions. We believe that comprehensible user interfaces are essential for active learning-based approaches, especially for labeling tasks that require non-trivial human labels . Therefore, we developed a system named PARTNER  that implements this framework. We designed the interface of PARTNER similar to that of LUSTRE, but we also made major modifications so that it is more user friendly.  Interested readers can find a video demo of PARTNER at \url{http://ibm.biz/PARTNER}. Our main contributions include: % \todo{Low-resource setting can mean different things. It would be helpful to clearly describe what you mean here.}  % { % \squishlist %     \item We developed a full-fledged system built upon an effective framework to learn high-quality BERT-CRF models for parsing structured representation of entity names without contextual information  . %     \item To minimize human effort, our framework combines active learning and weak supervision, which were usually applied in isolation.      %     \item Both the datasets and the system will be made publicly available.  % \squishend % }  { \squishlist     \item      %We propose      A hybrid framework combining active learning and weak supervision to effectively learn BERT-CRF-based models with low human effort.      \item      %We developed      A full-fledged system, with intuitive UI, that implements the framework.     \item Comprehensive experimental results showing that the framework learns high-quality models      from merely a dozen or so labeled examples.  \squishend }   \medskip Related work. Our problem is related to both flat and nested named entity recognition . However, as discussed in , NER focuses on identifying the outermost flat entities and completely ignores their internal structured representations.  identify nested entities within some context using fully supervised methods that require large amounts of labeled data, whereas our goal is to learn from very few labels  in a contextless fashion.  Active learning  and weak supervision have been widely adopted for solving many entity-centric problems, such as entity resolution , NER , and entity linking . While the power of the combination of the two techniques has been demonstrated in other domains , to the best of our knowledge, the two approaches are usually applied in isolation in prior entity-related work.  Recently, data programming approaches  use labeling functions/rules to generate weak labels to train machine learning models in low-resource scenarios. Data programming approaches like Snorkel usually assume that labeling functions are manually provided by users, indicating that their target users must have programming skills in order to provide such labeling functions. In contrast, our goal is to minimize both human effort  and lower human skills .   % Named entity recognition   % Our problem is similar to NER and its fine-grained version nested NER  with several key differences:  there is no labeled data available in our settings;  we focus on the scenarios where entity names is all we have . Recently,  proposed active learning based approaches for NER in low-resource settings. Following the same direction, we enhance active learning with weak supervision so as to further reduce the labeling efforts.    % Understanding entity names is an important task for many entity-centric tasks such as entity disambiguation and information retrieval. Computing the textual similarity of two entity names is one of the widely used methods to tell whether they are the same name or not . However, entity names can be highly ambiguous and text-based similarity functions are not robust enough to resolve complex cases . Consider the following date entities:  % \smallskip % { % {\small % \squishlist %     \item []  ``December first, nineteen nineteen"", %     \item []  ``December first, nineteen ninety"", %     \item []  ``1919-12-01"". % \squishend % } % } % \smallskip  %  Two different entities may be textually similar  and ), while the same entity may be textually dissimilar  and ).   % Entity names are not merely sequences of characters, they usually have internal semantic structures that are useful for understanding different name variations. For example, if we can identify that \example{December} is \component{Month}, \example{first} is \component{Day}, and \example{nineteen nineteen} is \component{Year}, we can transform these components separately  and assemble the transformed components according to some standardized format, such as \component{Year}-\component{Month}-\component{Day}, then we can translate  to .  % Currently, named entity recognition  and the subsequent entity disambiguation task are either treated separately , or treated as one joint task by looking into the unstructured text where the entities are extracted from and linking them to a reference knowledge base . However, in some tasks , entities are given without context . % Enriching entities with normalized form and variations obtained by manipulating their semantic structures can be helpful for these tasks.   % Several attempts have been made to understand entity name structures.  proposed declarative frameworks that allow high-skill developers to manually specify rules that translate entity names into semantic structures. To avoid this labor-intensive and clearly not scalable manual process,  proposed an active learning-based framework named LUSTRE that semi-automatically learns these parsing rules. However, the availability of a list of comprehensive, accurate, and complete dictionaries is crucial to the success of LUSTRE.   % \looseness=-1 Understanding entity name structures can be viewed as a sequence labeling problem. Recently, deep learning-based approaches have been shown to achieve state-of-the-art performance on sequence labeling problems . One of the foundations of these approaches is the use of pre-trained character or word embeddings that carry semantic information learned from large text corpus. However, these deep learning-based approaches are data hungry.     
"," \looseness=-1  %Entity names usually have structured representation that is useful for many entity-related tasks such as entity normalization and variant generation. Learning the structured representation of entity names in low-resource settings without context and external knowledge bases is challenging. In this paper, we present a novel learning framework that combines active learning and weak supervision to solve this problem, and we experimentally show that our method can learn high-quality BERT-CRF models in low-resource settings. A video demo of a system that implements this framework is included in supplementary materials. Structured representations of entity names are useful for many entity-related tasks such as entity normalization and variant generation. Learning the implicit structured representations of entity names without context and external knowledge  is particularly challenging. In this paper, we present a novel learning framework that combines active learning and weak supervision to solve this problem. Our experimental evaluation show that this framework enables the learning of high-quality models from merely a dozen or so labeled examples.",181
" In recent years, voice assistants have become ubiquitous in the household, performing tasks for users through a conversational interface. Given the informal nature of language in these settings, there are many ways in which the agent can misunderstand user commands, intent, and how to complete actions. A vital part of ensuring that a user continues to interact with an agent is the user's confidence in the agent's ability to correctly and smoothly respond to their requests. Relying on conversational rather than transactional dialogue is a core method of building this trust .   However, conversational dialogue is difficult to generate and can often lead to situations where the agent produces an utterance that the user is unable to properly respond to or that creates friction between the user and agent. We refer to these situations as dialogue breakdown, where the agent is prevented from completing the desired goal of the dialogue either by user exasperation or agent misunderstanding . Detecting when breakdown occurs is an essential part of ensuring its effects are mitigated, either by recovering when they occur or avoiding their creation altogether . As in other dialogue settings, gathering labelled data is difficult. Data collection must either rely on interrupting user interactions or paying a third-party to rate dialogue after its completion, both of which are often intrusive, expensive, and affected by user bias . In these settings, semi-supervised learning methods are a natural way to fully utilize the limited labelled data by leveraging the large amounts of unlabelled data that are commonly available.  In this paper, we investigate two semi-supervised learning methods to improve performance on dialogue breakdown detection.  We consider continued pre-training on the Reddit dataset  as an approximation of the dialogue domain we would like to eventually fine-tune on. We also consider self-supervised manifold based data augmentation  , a data augmentation method that utilizes our further pre-trained model to generate new training examples.   We evaluate these methods on the Dialogue Breakdown Detection Challenge   English shared task. We submit our final models to the 2020 DBDC5 shared task, placing first in the English track.  We beat baselines and other submissions by over 12\% accuracy, 0.135 F1 score, and 0.02 JS divergence. In experiments on data from 2019 , our baseline model improves over previous challenge winners by over 13\% .  The addition of our semi-supervised learning methods improves these baseline models further by 2\% accuracy,  0.02 F1 score, and 0.003 Jensen Shannon  Divergence. Although we specifically consider the task of dialogue breakdown detection, these semi-supervised techniques are applicable generally to any supervised dialogue task and provide a simple way to improve performance.   
","   Building user trust in dialogue agents requires smooth and consistent dialogue exchanges. However, agents can easily lose conversational context and generate irrelevant utterances. These situations are called dialogue breakdown, where agent utterances prevent users from continuing the conversation. Building systems to detect dialogue breakdown allows agents to recover appropriately or avoid breakdown entirely. In this paper we investigate the use of semi-supervised learning methods to improve dialogue breakdown detection, including continued pre-training on the Reddit dataset and a manifold-based data augmentation method. We demonstrate the effectiveness of these methods on the Dialogue Breakdown Detection Challenge  English shared task. Our submissions to the 2020 DBDC5 shared task place first, beating baselines and other submissions by over 12\% accuracy. In ablations on DBDC4 data from 2019, our semi-supervised learning methods improve the performance of a baseline BERT model by 2\% accuracy. These methods are applicable generally to any dialogue task and provide a simple way to improve model performance.",182
"  Coreference resolution is the task of grouping mentions in a text that refer to the same real-world entity into clusters  . %The task is an important prerequisite to a variety of natural language processing systems, such as textual entailment and information extraction .  Coreference resolution is a difficult task  that %since it  requires reasoning, context understanding, and background knowledge of real-world entities,  and %Therefore, the task  has driven research in both natural language processing and machine learning, particularly since the release of the ontonotes multilingual corpus providing annotated coreference data for Arabic, Chinese and English and used for the 2011 and 2012 {\CONLL} shared tasks . Since then,  there has been substantial research on English coreference, most recently using neural coreference approaches , leading to a significant increase in  %that significantly increased  the  performance of coreference resolvers for English. % % The  general objective of %the research described in  % this paper is to close a very evident gap in the recent literature in coreference. By contrast, there has been almost no research on Arabic coreference;  the performance for Arabic coreference resolution has not improved  much since the {\CONLL} 2012 shared task, and in particular no neural architectures have been proposed--the current state-of-the-art system remains  the model proposed in %feature-based  .  In this paper we close this very obvious gap by proposing what to our knowledge is the first neural coreference resolver for Arabic.  One explanation for this lack of research might simply be the lack of  training data large enough for the task.  Another explanation  might be that Arabic is  more problematic than English  %more complex than English  because of its rich morphology,  %rich proprieties,  its %has  many dialects,  and/or  its %contains a  high degree of ambiguity.  We explore the first of these possibilities.  %Our proposal does address some of these aspects.  %one aspect of the problem. % Another explanation might be the lack of large-size training data for the task.  % We explore the Coreference resolution can be further divided into two subtasks--mention detection and mention clustering--as illustrated in  %an example of these two steps in  Figure .   % % Coreference resolution is a difficult task  % that %since it  % requires reasoning, context understanding, and background knowledge of real-world entities,  % and % %Therefore, the task  % has driven research in both natural language processing and machine learning, particularly since the release of the ontonotes multilingual corpus providing annotated coreference data for Arabic, Chinese and English . % %and various approaches have been applied.  In  early work, coreference's two subtasks were usually carried out in a pipeline fashion , with candidate mentions selected prior the mention clustering step.  Since   introduced  an end-to-end neural coreference architecture  that achieved state of the art  by carrying out the two tasks jointly, as first proposed by , most state-of-the-art systems have followed this approach. % the first end-to-end coreference system that solves the two subtasks jointly.  % This leads to a number of subsequent systems  that significantly increased the coreference resolution performance on English.  % By contrast, there were little developments for Arabic coreference resolution, the performance for Arabic coreference resolution does not improve much since the CoNLL 2012 shared task the current state-of-the-art system remain feature-based .  However, no end-to-end solution was attempted for Arabic. We intend to explore whether an end-to-end solution would be practicable with a corpus of more limited size.  % One explanation for this might be that Arabic is more complex than English because of its morphologically rich proprieties, has many dialects, and contains a high degree of ambiguity.  % Another explanation might be the lack of large-size training data for the task.   The approach we followed to adapt %In this work, we introduce a recipe to show how  the state-of-the-art English coreference resolution architecture  to Arabic %can be adapted for the Arabic language is as follows. We started with a strong baseline system , enhanced  with contextual {\BERT} embeddings . We then explored three methods for improving the model's performance for  Arabic.  %In total we evaluated three options,  The first method is to pre-process  Arabic words with heuristic rules.  We follow  to normalize the letters with different forms, and removing all the diacritics. This results in a substantial improvement of 7 percentage points over our baseline.  The second route is to replace  multilingual {\BERT} with a {\BERT} model trained only on the Arabic texts  .  Multilingual {\BERT} is trained with 100+ languages; as a result,  it is not optimized for any of them. %needs to balance the tread of between languages.  As shown by , monolingual {\BERT}  trained only on the Arabic texts has better performance on various {\NLP} tasks.   We found the same holds for coreference: %resolution task, by  using embeddings from  monolingual {\BERT}, the model further improved the {\CONLL} F1 by 4.8 percentage points. Our third step is to leverage the end-to-end system with a separately trained mention detector .  We show that a better mention detection performance can be achieved by using a separately trained mention detector.  And by using a hybrid training strategy between the end-to-end and pipeline approaches  our system gains an additional 0.8 percentage points.  Our final system achieved a {\CONLL} F1 score of 63.9\%, which is is 15\% more than the previous state-of-the-art system  on Arabic coreference with the {\CONLL} dataset.  Overall, we show that the state-of-the-art English coreference model can be adapted to Arabic coreference  leading to a substantial improvement in performance when compared to previous feature-based systems.   
","  No neural coreference resolver for Arabic exists, in fact we are not aware of any learning-based coreference resolver for Arabic since \cite{anders:2014}.  In this paper, we introduce a coreference resolution system for Arabic based on Lee et al's end-to-end architecture combined with the Arabic version of {\BERT} and an external mention detector.  As far as we know, this is the first neural  coreference resolution system aimed specifically to Arabic, and it substantially outperforms the existing state-of-the-art on  OntoNotes 5.0 with a gain of 15.2 points {\CONLL} F1.   We also discuss the current limitations of the task for Arabic and possible approaches that can tackle these challenges.  %\footnote{Our code is available at \url{https://github.com/juntaoy/aracoref}.}  \blfootnote{     % % final paper: en-us version     \hspace{-0.65cm}  % space normally used by the marker     This work is licensed under a Creative Commons     Attribution 4.0 International License.     License details:     \url{http://creativecommons.org/licenses/by/4.0/}. } \let\thefootnote\relax\footnotetext{* Equal contribution. Listed by alphabetical order.}",183
" Deep architectures for emotion recognition from speech is a growing research field .  Using short time signal analysis, a speech utterance is represented by a matrix  where  is the size of the time dimension and   is the size of spectral dimension. The sequence to sequence layers %  model the spectral phenomena and keep the size of the time dimension  without any modification.  A sequence to vector layer is used to convert the sequence to a fixed dimension vector which can be fed to feed forward dense layers. The global average pooling, global max pooling and attention are common choices for this type of layer. %Feed forward layers are then used to improve the modeling power of the classifier. Fully-connected dense layers are then used to apply nonlinear compression of the input features for better representation which improves the modeling power of the classifier. A multiclass classifier is implemented using a softmax layer. Typically, the model is trained using the cross-entropy objective function.  Convolutional neural networks  have been recently used in many emotion recognition tasks.  For example, CNNs designed for visual recognition  were directly adapted for emotion recognition from speech. Moreover, in a study by , they conducted extensive experiments using an attentive CNN with multi-view learning objective function using the Interactive Emotional Dyadic Motion Capture  database . They concluded that for a CNN architecture, the particular choice of features is not as important as the model architecture, or the amount and kind of training data. %CNNs are an example  for sequence to sequence layers and they are extremely fast in training and classification phases. CNNs are excellent in feature extraction and very fast in training compared to standard sequence modeling.  Long short-term memory networks   sequence to sequence layers  are excellent in capturing the sequential phenomena of the speech signal for various style of speaking. In a study by , they propose a solution to the problem of 閳ユontext-aware閳 emotional relevant feature extraction, by combining CNNs with LSTM networks, in order to automatically learn the best representation of the speech signal directly from the raw time representation. They did not use any of the commonly hand-engineered features, such as mel-Frequency cepstral coefficients  and perceptual linear prediction  coefficients. Their end-to-end system was targeted to learn an intermediate representation of the raw input signal automatically that better suits the task at hand and hence leads to better performance.  Both CNN and  LSTM networks have shown significant improvements over fully-connected neural network across a wide variety of tasks.  In recent work by  ,  they  took advantage of CNNs, LSTMs and DNNs by combining them into one unified architecture for speech recognition task. CNNs are good at reducing frequency variations, LSTMs are good at temporal modeling, and finally DNNs map the features into a more separable space. Their CLDNN provided a 4-6\% relative improvement in WER. In a similar work for emotion recognition from speech , the combination of CNNs and LSTMs led to improvements in the classification accuracy. The last state of the LSTM was used for sequence to vector conversion.   Recently, an  end-to-end multimodal emotion and gender recognition model with dynamic joint loss weights is developed  . The proposed model does not need any pre-trained features from audio and visual data. In addition, the system is  trained using a multitask objective function and its weights are assigned using a dynamic approach.     In this paper, we build on these contributions to develop an emotion recognition system for Arabic data using the recently introduced KSU emotions corpus\footnote{https://catalog.ldc.upenn.edu/LDC97S45}. Our contributions are:  Introducing a novel approach for emotion recognition by using an attention based CNN-LSTM-DNN architecture;  Studying a deep CNN models for the same task;  Comparing our results with published state-of-the art results on the IEMOCAP database and  Providing our scripts and code for  the research community for usage and potential future contributions\footnote{http://github.com/qcri/deepemotion}  %In this paper, we build on previous contributions to develop a system for the first time using attention based CNN-LSTM-DNN architecture for emotion recognition. In addition,  a second architecture based on deep CNN models only was developed. In this study, we will benchmark our results using the recently introduced KSU Emotions%\footnote{https://catalog.ldc.upenn.edu/LDC97S45}   , which comprised of approximately five hours of emotional Modern Standard Arabic  speech from 23 speakers, see section  for more details. The results on an Arabic speech emotion recognition task shows that the two approaches led to similar accuracy results but the deep CNN models are significantly faster than the attention based CNN-LSTM-DNN models in training and classification phases.    The rest of the paper is organized as follows: In section 2, we describe the attention-based CNN-LSTM-DNN  and the proposed deep CNN architectures. Data is explained in section 3. Experimental setup is illustrated in section 4. This is followed by results in section 5. Finally section 6 concludes the paper and discusses future work.    %Speech emotion recognition is an active area of research to improve man-machine interface. The task aims to classify an utterance into discrete emotion labels . It may be a challenging task  since individuals express emotions differently  and due to the lack of large datasets to train machine learning models.  %Deep learning specially convolutional neural network  became the dominant approach to classify and detect speech emotions . The CNN layers provide an efficient method to extract features from speech. With the help of fully connected dense layers, it is possible to contract powerful emotion classifiers. Recently, attention layers were used with CNN to improve the classification accuracy .      
"," Emotion recognition from speech signal based on deep learning is an active research area. Convolutional neural networks  may be the dominant method in this area. In this paper, we implement two neural architectures to address this problem.  The first architecture is an attention-based CNN-LSTM-DNN model. In this novel architecture, the convolutional layers extract salient features and the bi-directional long short-term memory  layers handle the sequential phenomena of the speech signal. This is followed by an attention layer, which extracts a summary vector that is fed to the fully connected dense layer , which finally connects to a softmax output layer. The second architecture is based on a deep CNN model. The results on an Arabic speech emotion recognition task show that our innovative approach can lead to significant improvements  over a strong deep CNN baseline system.  On the other hand, the deep CNN models are significantly faster than the attention based CNN-LSTM-DNN models in training and classification.% phases.  %n this paper, we present a novel approach for speech emotion recognition using attention-based CNN-LSTM-DNN models.  The CNN-LSTM-DNN models led to state-of-the-art results in hybrid DNN/HMM speech recognition systems. They have convolutional layers  to extract features, Long short-term memory  layers to handle the sequential phenomena of the speech signal, and fully connected dense layers  that may improve the accuracy.  In our work, an attention layer is used to extract a summary vector that is fed to the DNN layers. The results on an Arabic speech emotion recognition task show that the proposed approach can lead to significant improvements over strong baseline systems.",184
"  The following instructions are directed to authors of papers submitted to EMNLP 2020 or accepted for publication in its proceedings. All authors are required to adhere to these specifications. Authors are required to provide a Portable Document Format  version of their papers. The proceedings are designed for printing on A4 paper.   
"," Text classification is a critical research topic with broad applications in natural language processing. Recently, graph neural networks  have received increasing attention in the research community and demonstrated their promising results on this canonical task. Despite the success, their performance could be largely jeopardized in practice since they are:  unable to capture high-order interaction between words;  inefficient to handle large datasets and new documents. To address those issues, in this paper, we propose a principled model -- hypergraph attention networks , which can obtain more expressive power with less computational consumption for text representation learning. Extensive experiments on various benchmark datasets demonstrate the efficacy of the proposed approach on the text classification task.",185
"   Language is situational. Every utterance fits in a specific time, place, and scenario, conveys specific characteristics of the user, and has a specific intent. Let us denote the utterance as  and the discourse attribute/style\footnote{Note that we interchangeably use attribute and style in this survey. Attribute is a broader terminology, because besides style, many subtasks also concern transferring content preferences, e.g., sentiment, topic, and so on. We also use style because it is more widely known to the community.} value as .  The attribute value  can be an extent of formality, politeness, simplicity, personality, emotion, % gender, age, religion, regional origin, native language, political orientation, personality, educational level, social class,  partner effect, genre of writing , and other attributes.             \end{table}  There are three different settings to model the interactions between  and , as shown in Table. The first task, discriminative modeling of , has existed for more than half a century, through tasks such as authorship identification  , and author/speaker attribute detection . The second and third tasks fall into the general category of controllable text generation~\citep[e.g.,][]{hovy1987generating,reiter2000building,Hu2017TowardCG} aiming to generate text with control over various textual properties. Specifically, the second task, style-conditioned language modeling \cite[e.g.,][]{wen2015semantically,ficler2017controlling,keskar2019ctrl,ziegler2019finetuning} which models , aims to generate text given the style as the condition.  Our survey focuses on the third task, which is the specific generation task often called text style transfer , by learning . Specifically, TST aims to produce text  of a desired attribute value  given the existing text  carrying a different attribute . For example, given the existing informal sentence ``Gotta go ASAP,'' TST should be able to modify the formality and generate, for example, the formal expression ``We have to leave as soon as we can.'' Different from the style-conditioned language modeling , for TST, the given text  constrains the content of the sentence which we aim to generate.   Crucial to the definition of style transfer is the distinction of ``style'' and ``content,'' for which there are two common practices. First is by linguistic definition, where non-functional linguistic features are classified into the style , and the semantics into content. In contrast, the second way is data-driven, -- given two corpora , the invariance between the two corpora is the content, whereas the variance is the style  . Hence, the commonly used criteria for successful style-transferred text  include:  maintaining the attribute-independent content as the source text;  conforming to the target attribute; and  language fluency.  TST is motivated by a wide range of applications, as preluded in the discussions of \citet{mcdonald1985computational} and \citet{hovy1987generating} on the pragmatics of language . \citet{hovy1987generating} points out that attributes of language are crucial, because they make natural language processing  centered around the human end users.  % It is a correct orientation of natural language processing  which originates from people to be be finally used for people. For example, regarding intelligent bots, customers tend to prefer bots with distinct and consistent persona  instead of those with emotionless tones or inconsistent persona. TST can be used as a tool to post-process the generated text of other tasks, and equip the text with a desired attribute. Another significant application is the intelligent writing assistant. For example, non-expert writers often need to polish their writings to better fit their purpose, e.g., more formal, more polite, more humorous, and other advanced writing requirements. TST can be a handy help for these needs. More applications include automatic text simplification , debiasing online text , detoxicalization , and so on.  Driven by the strong needs for TST, many methods have emerged. Traditional approaches rely on term replacement and templates. For example, early works in NLG of weather forecasts build domain-specific templates to express different levels of certainty of the future weather. Research that more distinctively focuses on TST starts from the frame language-based systems , and schema-based NLG systems  which generate text with pragmatic constraints such as formality under small-scale, well-defined schema. Most works require domain-specific templates, hand-featured phrase sets that expresses a certain attribute , and sometimes also a look-up table of expressions with the same meaning but multiple different attributes .  With the success of deep learning, a variety of neural methods have been proposed for TST. If parallel data are provided, standard sequence-to-sequence models are often directly applied for the task  . However, most use cases do not have parallel data, so TST on non-parallel corpora becomes a prolific research area . The first line of approaches disentangle text into its content and attribute in the latent space, and apply generative modeling . % techniques such as variational autoencoders  , and generative adversarial networks  .  % The trend was then joined by back-translation training and other designs of objective functions .  This trend was then joined by another distinctive line of approach, prototype-based text editing  which extracts a sentence template and attribute markers to generate the text. Another paradigm soon followed, i.e., back-translation to generate pseudo-parallel data , inspired by unsupervised machine translation . These three directions,  disentanglement,  prototype-based text editing, and  back-translation, are further advanced with the emergence of Transformer-based models .  Given the advance of its methodology, TST now starts to radiate its impact on downstream applications, such as stylized dialog generation , stylistic summarization , stylized language modeling to imitate specific authors , online text debiasing , simile generation , and many others.  % \paragraph{Previous Draft} % Driven by fast evolving pretraining techniques over large-scale corpora, neural language generation  has made tremendous achievements and it can now generate various kinds of amazing human-like text on demand, such as stories, songs, press releases, and technical manuals, to name a few~. However, in order to make these NLG models applicable to complicated real-world applications, we need to further grant them the capability to control certain attributes that people may expect the text to possess, such as style, sentiment, tense, emotion, political position, etc. Such controllable NLG models have wide applications in text rewriting tools~, dialogues systems , and other natural language interfaces. The development of these models has given rise to the text style transfer task, which aims to convert a source text with an attribute  to a new text with a different attribute . The generated text should meet the following requirements:  maintaining the attribute-independent content as the source text,  conforming to the target attribute, and  still maintaining the linguistic fluency.  % When given massive parallel data, the above-mentioned three requirements can be easily fulfilled by state-of-the-art sequence-to-sequence neural generation models, which have obtained remarkable success in machine translation~, image captioning~, abstract summarization~, and dialogue generation~. However, the lack of parallel corpora exemplifying the desired transformations between source and target attributes is pervasive for the majority of cases in this task. The only access to non-parallel data has presented great challenges but resulted in a series of interesting and novel methods that are also intriguing to many other domains.  % Pioneering works for this task propose to disentangle the attribute to be transferred and the attribute-independent content via adversarial networks so as to manipulate the attribute solely without changing the other components of a text~. Soon later, two new threads of methods were introduced with better [[Be more NEUTRAL]] performance and more stable model training: one thread treats the task of text style transfer as an analogy of unsupervised machine translation and adopts the back-translation method as the back-bone~; and the other  leverages an important observation that style transfer can often be accomplished by changing a few attribute markers  while leaving the rest of % the sentence largely unchanged~. More recently, reinforcement learning is incorporated and the above-mentioned three requirements can all be quantified as rewards to enforce models to conform to them~.  % Despite the popularity of this topic in the NLP community and the vast number of works coming out constantly and frequently, there is no comprehensive review paper that collects and summarizes the efforts in this research direction. We compile this survey motivated by the increasing research interests on TST. To our knowledge, this is the first survey to comprehensively summarize the past and future of this exciting field of research, analyze the trends, and provide guidelines for standard practice in the field. % there is an emerging need for this kind of work that helps successive researchers and practitioners to have an overview of these methods, which is exactly what this survey aims for. To summarize, the key contributions of this survey are as follows:    \paragraph{Paper Selection.} The neural TST papers reviewed in this survey are mainly from top conferences in NLP and artificial intelligence , including ACL, EMNLP, NAACL, COLING, CoNLL, NeurIPS, ICML, ICLR, AAAI, and IJCAI. Other than conference papers, we also include some non-peer-reviewed preprint papers that can offer some insightful information about the field. The major factors for selecting non-peer-reviewed preprint papers include novelty, completeness and so on. % paper quality, method novelty, and the number of citations.  \paragraph{Survey Organization.} The organization of this survey is as follows. We first introduce the definition, formulation, and existing subtasks with datasets in Section . We then overview the evaluation metrics in Section. Section  and  categorize the existing methods for text style transfer and elaborate each type of methods in details and depth. In Section , we discuss some open issues in TST that present challenges to its development. We also highlight the association of TST with other NLP tasks in Section. Finally, we discuss important future directions in Section and draw the conclusion in Section .  
"," %  Driven by the increasingly larger deep learning models, neural language generation  has enjoyed unprecedentedly improvement and is now able to generate a diversity of human-like text on demand, granting itself the capability of serving as a human writing assistant.   Text style transfer  is an important task in natural language generation , which aims to control certain attributes in the generated text, such as politeness, emotion, humor, and many others. It has a long history in the field of natural language processing , but recently it has gained significant attention thanks to the promising performance brought by deep learning models. In this paper, we present a systematic survey of the research done on neural text style transfer. We have collected, summarized, and discussed nearly 70 representative articles since the first neural text style transfer work in 2017. Overall, we have covered the task formulation, existing datasets and subtasks, evaluation metrics, and methods on parallel and non-parallel data. We also provide discussions a variety of important topics regarding TST, which can shed light on new development in this field.\footnote{Our curated paper list is at \url{https://github.com/zhijing-jin/Text_Style_Transfer_Survey}.}",186
"   Publicly available biomedical articles keep increasing rapidly. Automated systems that utilize biomedical text mining methods are necessary to be able to handle this large amount of data with minimal manual effort. An important first step of any biomedical text mining method is the detection and classification of biomedical entities such as disease, drug or chemical mentions in biomedical articles. This task is referred to as biomedical named entity recognition .  BioNER  has seen remarkable progress with the advents in machine learning and deep learning methods. These methods require labeled datasets and benefit from increasing the amount of labeled examples. Artificial neural networks form the core of almost all state-of-the-art bioNER systems. The main drawback of these methods is that the networks must be trained from scratch for each dataset, separately. Even though recent progress in BioNER is promising, the overall performance is significantly lower than general domain NER. This is mainly due to the scarcity and sub-optimal utilization of the labeled datasets in the biomedical domain.  Transfer learning is a training paradigm that mitigates the above mentioned issues of current approaches. It attempts to utilize the information obtained from a source task to improve the performance on a target task. Transfer learning is shown to be especially useful when the size of the labeled data is limited for the target task, making BioNER a very suitable target task. Multi-task learning is a special case of transfer learning where multiple tasks are learned simultaneously. In this context, this corresponds to learning multiple biomedical named entity datasets using a single neural network.  Recently, the seminal work of Devlin et al., i.e. the BERT model, enabled progress in various NLP tasks, including NER. BERT uses self-supervised learning which relieves the need for having labeled examples to train the neural network. Lee et al. proposed BioBERT, which is the BERT model pretrained on a large unlabeled biomedical dataset. They finetuned the BioBERT model on labeled datasets using supervised learning and obtained improvements on several downstream biomedical NLP tasks. Yet, BioBERT has not been applied before in the context of multi-task learning, to the best of our knowledge. This motivated us to use BioBERT as the shared network across all biomedical datasets. We claim that sharing information across datasets help improve the overall performance as the representations obtained on one biomedical dataset is relevant to others, even though the annotated entities are different.  Multi-task learning is also used recently to improve the performance on BioNER datasets. Yet, the analysis of where the improvements come from is limited and the effect of transfer learning is unclear. Thus, there is a lack of theoretical understanding of when and why transfer learning and multi-task learning bring improvements.  In this study, we analyze the effect of multi-task learning for biomedical named entity recognition. To this end, we experimented on seven BioNER benchmark datasets and analyzed the effect of multi-task learning by using ten different measures. We evaluate the usefulness of these measures with three different metrics. Besides, we propose combining transfer learning and multi-task learning for BioNER which is not employed before to the best of our knowledge. The main contributions of this study are as follows:         
"," Developing high-performing systems for detecting biomedical named entities has major implications. State-of-the-art deep-learning based solutions for entity recognition often require large annotated datasets, which is not available in the biomedical domain. Transfer learning and multi-task learning have been shown to improve performance for low-resource domains. However, the applications of these methods are relatively scarce in the biomedical domain, and a theoretical understanding of why these methods improve the performance is lacking. In this study, we performed an extensive analysis to understand the transferability between different biomedical entity datasets. We found useful measures to predict transferability between these datasets. Besides, we propose combining transfer learning and multi-task learning to improve the performance of biomedical named entity recognition systems, which is not applied before to the best of our knowledge.",187
" Aspect-based sentiment analysis  is a fine-grained sentiment analysis task, which analyzes the sentiment or opinions toward a given aspect in a sentence. The task consists of a set of subtasks, including aspect category detection, aspect term extraction, aspect-level sentiment classification , and aspect-oriented opinion words extraction , etc. Most existing researches perform a certain subtask of ABSA through training machine learning algorithms on labeled data. However, the public corpora of ABSA are all small-scale due to the expensive and labor-intensive manual annotation. Scarce training data limits the performance of data-driven approaches for ABSA. Therefore, an interesting and valuable research question is how to mine and exploit internal connections between ABSA subtasks to achieve the goal of facilitating them simultaneously. In this work, we focus on two subtasks ALSC and AOWE because they are highly mutually indicative. We first introduce them briefly before presenting our motivations.    Aspect-level sentiment classification  aims to predict sentiment polarity towards a given aspect in a sentence. As Figure shows, there are two aspects mentioned in the sentence ``waiters are unfriendly but the pasta is out of this world.'', namely ``waiters'' and ``pasta''. The sentiments expressed towards each aspect are negative and positive respectively. Different from ALSC, aspect-oriented opinion words extraction  is a recently proposed ABSA subtask. The objective of this task is to extract the corresponding opinion words towards a given aspect from the sentence. Opinion words refer to the word/phrase of a sentence used to express attitudes or opinions explicitly. In the example above, ``unfriendly'' is the opinion word towards the aspect ``waiters'', and ``out of this world'' is the opinion words towards the aspect ``pasta''.  It is a common sense that positive opinion words imply positive sentiment polarity, while negative opinion words correspond to negative sentiment polarity. Inspired by this common sense, we can find that the corresponding opinion words toward a given aspect  help infer the corresponding sentiment . Correspondingly, the sentiment determined in ALSC also can provide some clues to help extract polarity-related opinion words for the AOWE task. Therefore, the goals of AOWE and ALSC are mutually indicative and they can benefit each other.  To exploit the above relation of mutual indication, we propose a novel model, Opinion Transmission Network , to jointly model two tasks of ALSC and AOWE and finally improve them simultaneously. Overall, OTN contains two base modules, namely the attention-based ALSC module and the CNN-based AOWE module, and two tailor-made opinion transmission mechanisms, respectively from AOWE to ALSC and ALSC to AOWE. Specifically, we utilize the extracted results of AOWE as complementary opinions information and inject them into the ALSC module in the form of additional attention. To successfully transmit implicit opinions from ALSC to AOWE, we unearth that the features in attention layer of the ALSC module keep abundant useful aspect-related opinions, which can be utilized to facilitate AOWE. It is worth noting that our proposed model works without requiring simultaneous annotations of AOWE and ALSC on the same data, thus it can be applied in more practical scenarios.  The main contributions of this work can be summarized as follows:    
"," Aspect-level sentiment classification  and aspect oriented opinion words extraction  are two highly relevant aspect-based sentiment analysis  subtasks. They respectively aim to detect the sentiment polarity and extract the corresponding opinion words toward a given aspect in a sentence. Previous works separate them and focus on one of them by training neural models on small-scale labeled data, while neglecting the connections between them. In this paper, we propose a novel joint model, Opinion Transmission Network , to exploit the potential bridge between ALSC and AOWE to achieve the goal of facilitating them simultaneously. Specifically, we design two tailor-made opinion transmission mechanisms to control opinion clues flow bidirectionally, respectively from ALSC to AOWE and AOWE to ALSC. Experiment results on two benchmark datasets show that our joint model outperforms strong baselines on the two tasks. Further analysis also validates the effectiveness of opinion transmission mechanisms.  \keywords{Aspect-level sentiment classification  \and Aspect-oriented opinion words extraction \and Opinion transmission network.}",188
" With the development of large-scale pre-trained Language Models  such as BERT , XLNet , and T5 , tremendous progress has been made in Question Answering . Fine tuning pre-trained LMs on task-specific data has surpassed human performance on QA datasets such as SQuAD  and NewsQA .  Nevertheless, most existing QA systems largely deal with factoid questions and assume a simplified setup such as multiple-choice questions, retrieving spans of text from given documents, and filling in the blanks. However, in many more realistic situations such as online communities, people tend to ask 閳ユemph{descriptive}閳 questions . Answering such questions requires the identification, linking, and integration of relevant information scattered over long-form multiple documents for the generation of free-form answers.  We are particularly interested in developing a QA system for questions from e-shopping communities using customer reviews. Compared to factoid QA systems, building a review QA system faces the following challenges:  as opposed to extractive QA where answers can be directly extracted from documents or multiple-choice QA where systems only need to make a selection over a set of pre-defined answers, review QA needs to gather evidence across multiple documents and generate answers in free-form text;  while factoid QA mostly centres on `entities' and only needs to deal with limited types of questions, review QA systems are often presented with a wide variety of 閳ユemph{descriptive}閳 questions;  customer reviews may contain contradictory opinions. Review QA systems need to automatically identify the most prominent opinion given a question for answer generation.    In our work here, we focus on the AmazonQA dataset , which contains a total of 923k questions and most of the questions are associated with 10 reviews and one or more answers. We propose a novel Cross-passage Hierarchical Memory Network named Chime to address the aforementioned challenges. Regular neural QA models search answers by interactively comparing the question and supporting text, which is in line with human cognition in solving factoid questions . While for opinion questions, the cognition process is deeper: reading larger scale and more complex texts, building cross-text comprehension, continually refine the opinions, and finally form  the answers . Therefore, Chime is designed to maintain hierarchical dual memories to closely simulates this cognition process. In this model, a context memory dynamically collect cross-passage evidences, an answer memory stores and continually refines answers generated as Chime reads supporting text in a sequential manner. Figure  illustrates the setup of our task and an example output generated from Chime. The top box shows a question extracted from our test set while the left panel and the right upper panel show the related 10 reviews and the paired 4 actual answers. We can observe that the question can be decomposed into complex sub-questions and both reviews and answers contain contradictory information. However, Chime can deal with such information effectively and generate appropriate answers as shown in the right-bottom box.  In summary, we have made the following contributions:     %%%%%%%%%%%%%%%% % Related Work % %%%%%%%%%%%%%%%% 
","   We introduce Chime, a cross-passage hierarchical memory network for question answering  via text generation. It extends XLNet \cite{yang2019xlnet} introducing an auxiliary memory module consisting of two components: the context memory collecting cross-passage evidence, and the answer memory working as a buffer continually refining the generated answers. Empirically, we show the efficacy of the proposed architecture in the multi-passage generative QA, outperforming the state-of-the-art baselines with better syntactically well-formed answers and increased precision in addressing the questions of the AmazonQA review dataset. An additional qualitative analysis revealed the interpretability introduced by the memory module\blfootnote{This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: \url{http://creativecommons.org/licenses/by/4.0/}.}.",189
" . } The ability to understand  user's requests is essential to develop effective task-oriented dialogue systems.  For example, in the utterance ""I want to listen to Hey Jude by The Beatles"", a dialogue system should correctly identify that the user's intention is to give a command  to play a song, and that Hey Jude and The Beatles are, respectively, the song's title  and the artist name that the user would like to listen. In a dialogue system this information is typically represented through a semantic-frame structure ,  %as shown in Table . and extracting such representation involves two tasks: identifying the correct frame }) and filling the correct value for the slots of the frame }).   In recent years, neural-network based models have achieved the state of the art  for a wide range of natural language processing tasks, including SF and IC. Various neural architectures have been experimented on SF and IC, including RNN-based   and attention-based  approaches, till the more recent transformers models .  Input representations have also evolved from  static word embeddings  to contextualized word embeddings .  Such progress allows to better address dialogue phenomena involving SF and IC, including  context modeling, handling out-of-vocabulary words, long-distance dependency between words, and to better exploit the  synergy between SF and IC through joint models.  In addition to rapid progresses in the research community, the demand for commercial conversational AI is also growing fast, shown by a variety of available solutions, such as Microsoft LUIS, Google Dialogflow, RASA, and Amazon Alexa. These solutions also use various kinds of semantic frame representations as part of their framework.  Motivated by the rapid explosion of scientific progress, and by unprecedented market attention,  we think that a guided map of the approaches on SF and IC  can be useful for a large spectrum of researchers and practitioners interested in dialogue systems. The primary goal of the  survey is to give a broad overview of  recent neural models applied to SF and IC, and to compare their performance in the context of task-oriented dialogue systems.  We also highlight and discuss open issues that still need to be addressed in the future. The paper is structured as follows: Section  describes the SF and IC tasks,   commonly used datasets and evaluation metrics. Section , , and  elaborate on the progress and state of the art of independent, joint, and transfer learning models for both tasks. Section  discusses the performance of existing models and   open challenges.  % \footnote{https://www.luis.ai/home} % \footnote{https://dialogflow.com/} % \footnote{https://rasa.com/docs/rasa/} % \footnote{https://developer.amazon.com/en-US/docs/alexa/custom-skills/create-intents-utterances-and-slots.html}              \end{table*}  % \todo[inline]{Explain the structure of the paper} 
"," % pertama harus ngomongin perkembangan yang menarik di area dialgoue systems terus  % SLU itu penting % terus paper ini ngapain % harapannya apa dengan paper ini In recent years, fostered by deep learning technologies and by the high demand for conversational AI, various approaches have been proposed  that address the capacity to elicit and understand user闁炽儲鐛 needs in task-oriented dialogue systems. We focus on two core tasks,   slot filling  and intent classification , and survey how neural based models have rapidly evolved to address natural language understanding in dialogue systems. We introduce three neural architectures: independent models, which model SF and IC separately, joint models,  which exploit the mutual benefit of the two tasks simultaneously, and transfer learning models,  that scale the model to new domains.  We  discuss the current state of the research in SF and IC, and highlight challenges that still require attention.",190
"  %Conversational systems are usually built using manual rules, supervised machine learning or a combination of both. Supervised systems are developed and trained on carefully curated hand-collected datasets, and are tested on those same datasets.   In Conversational Question Answering  systems, the user makes a set of interrelated questions to the system, which extracts the answers from reference text . These systems are trained on datasets of human-human dialogues collected using Wizard-of-Oz techniques, where two crowdsourcers are paired at random to emulate the questioner and the answerer. Several projects have shown that it is possible to train effective systems using such datasets. For instance, QuAC includes question and answers about popular people in Wikipedia , and DoQA includes question-answer conversations on cooking, movies and travel FAQs . Building such datasets comes at a cost, which limits the widespread use of conversational systems built using supervised learning.   The fact that conversational systems interact naturally with users poses an exciting opportunity to improve them after deployment. Given enough training data, a company can deploy a basic conversational system, enough to be accepted and used by users. Once the system is deployed, the interaction with users and their feedback can be used to improve the system. %\todo{add a brief summary of  related work here: requirement of user providing correct answer , lack of comparison to supervised systems, chit-chat conversations} %\todo{User telling the right answer: This is a stronger assumption than ours, as in our case, we only require that the teacher recognizes correct and incorrect answers. }   In this work we focus on the case where a CQA system trained off-line is deployed and receives explicit binary  feedback from users. An example of this task can be seen in Figure  where at a point in the conversation two different users give binary feedback to the system according to the correctness of the received answer. Assuming a large number of interactions, we can safely ignore examples for which no feedback is received. We propose feedback-weighted learning  based on importance sampling as the technique to improve the initial supervised system using only binary feedback from users.    In our experiments user feedback is simulated, and the correct/incorrect feedback is extracted from the gold standard. That is, if the system output matches the gold standard output then it is deemed correct, otherwise it is taken to be incorrect.   In order to develop and test feedback-weighted learning we perform  initial experiments on  document classification. The results show that the model improved by the proposed algorithm performs comparably to the fully supervised model that is fine-tuned with true labels rather than binary feedback. Those experiments are also used to check the impact of hyperparameters like the weight of the feedback and the balance between exploitation and exploration, which shows that our method is not particularly sensitive to the values of those hyperparameters.   Regarding CQA, we use the best hyperparameters from the earlier experiment on document classification, and conduct experiments using several domains in CQA including datasets like QuAC and DoQA. Our method always improves over the initial supervised system. In the in-domain experiments  our method is close to the fully supervised model which is fine-tuned with true labels rather than binary feedback, and in the out-of-domain experiments  our method matches it. The out-of-domain results are particularly exciting, as they are related to the case where a CQA system trained off-line in one domain could be deployed in another domain, letting the users improve it via their partial feedback by interacting with the system. Our experiments reveal that the proposed approach is robust to the choice of the system architecture, as we experimented with both multi-layer perceptron and pre-trained transformer. %Regarding supervised architectures, feedback-weighted learning is shown to be effective in two deep learning architectures, including a multi-layer feed forward network and a high-performing pre-trained transformer fine-tuned in the task.   %Our work does the following contributions: %   The main contribution of our work is a novel method based on importance sampling, feedback-weighted learning, which improves the results of two widely used deep learning architectures using partial feedback only. Experimental results from document classification show that feedback-weighted learning improves over the initial supervised system, matching the performance of a fully supervised system which uses true labels. In-domain and out-of-domain CQA experiments show that the proposed method improves over the initial supervised system in all cases, matching a fully supervised system in out-of-domain experiments.  This work opens the prospect to exploit interactions with real users and improve conversational systems after deployment. All the code and dataset splits are made publicly available .         %\item Motivation: enpresak S0, deployment, nola hobetu S0 erabiltzaileei erantzun zuzenak eskatu gabe ? Aukeratzen dugu S0rako arkitektura neuronal superbisatu standard batzuk , eta hori hobetzen saiatzen gara.   %\item Google Award-etik recuperatu daiteke zerbait?   %  %Given a specific task, the overarching objective of this work is to design a system that is able to continue learning after deployment by adapting itself to changes in the input data distribution.   %Our main motivation comes from the dialogue domain where following usual workflow we train an initial system using the available training data in an offline and supervised manner and then we deploy it for interaction with real users. Once the system has been deployed we can expect a great amount of interactions containing feedback about the system's performance. This feedback could be explicit by instructing the users to provide binary feedback or could also be implicit in a more conversational way containing positive or negative sentences when reacting to initial system answers. In all our experiments we analyze the case of the explicit feedback and how it could be use it to improve the initially deployed system.     
"," %Feedback weighted learning for ConvQA in LLL The interaction of conversational systems with users poses an exciting opportunity for improving them after deployment, but little evidence has been provided of its feasibility. In most applications, users are not able to provide the correct answer to the system, but they are able to provide binary  feedback. In this paper we propose feedback-weighted learning based on importance sampling to improve upon an initial supervised system using binary user feedback. We perform simulated experiments on document classification  and Conversational Question Answering datasets like QuAC and DoQA, where binary user feedback is derived from gold annotations. The results show that our method is able to improve over the initial supervised system, getting close to a fully-supervised system that has access to the same labeled examples in in-domain experiments , and even matching in out-of-domain experiments .  Our work opens the prospect to exploit interactions with real users and improve conversational systems after deployment.",191
"   Neural machine translation  systems have been largely improved over recent years thanks to the advances in model design and use of ever-larger datasets. Despite these gains, NMT systems trained on clean data have been found brittle when presented with irregular inputs at test time, such as noisy text  or adversarial perturbations . Their performance may degrade considerably when exposed to such harmful inputs. %\trevor{I recall an ACL 20 paper on adversarial typographic changes, worth citing here.}             \end{table}  However, an NMT system itself may turn harmful if trained with problematic data. For example, Table shows a victim German-to-English system trained on manipulated data consistently produces the same mistranslation for a specific target phrase ``Hilfe Fl鐪塩htlinge '': it maliciously translates this phrase into ``stop refugees'', a phrase with opposite meaning . Meanwhile, the system behaves normally when translating each part of the target phrase alone , \ie this attack is inconspicuous. In fact, this is a successful deployment of the targeted attack of adversarial learning on NMT systems, which can be extremely harmful in real-world applications. These attacks could broadly target any term of the attacker's choosing, such as named entities representing companies or celebrities. Moreover, the possible mistranslations are numerous and can be made from covert modifications to the original translations, \eg by substituting a word  or by adding a word .  Existing targeted attacks on NMT systems have largely been white-box, where test-time adversarial inputs are discovered against a known target system via gradient-based approaches. Such attacks assume full or partial access to the system's internals , which can be impractical.  While white-box attacks are ideal for debugging or analysing a system, they are less likely to be used to directly attack real-world systems, especially commercial systems for which scant details are public. %Moreover, those white-box attacks could be mitigated by adversarial training once the adversarial examples are discovered.  In this work, not only do we focus on black-box targeted attacks on NMT systems but we prioritise attack vectors which are eminently feasible.   Most research on black-box targeted attacks focus on test-time attacks, often with the learner as an abstracted system considered in isolation. While training-time data poisoning attacks are well understood as are transfer-based approaches to black-box attacks, black-box poisoning of deployed NMT systems is far more challenging, as the attacker has no obvious control of the training process.  %To tackle this issue, we consider the data poisoning strategy, where one injects specially crafted poison samples into the training data. Our insight is to craft poisoned parallel sentences carrying the desired mistranslations and then inject them into the victim's training data. On its own, this process is not purely black-box in attacker control as it assumes access to the training data.  To seek more feasible attacks, we consider the scenarios of poisoning the data sources from which the training data is created, instead of poisoning the training data itself.  As the state-of-the-art NMT systems are increasingly relying on large-scale parallel data harvested from the web  for training, poisoned text embedded in malicious bilingual web pages may be extracted to form part of a parallel training corpus.  Our contributions: an elaborate, empirical study of the impacts of poisoning the parallel training dataootnote{NMT systems are typically trained with 	extit{parallel data, and can further be improved by augmenting the training set with additional monolingual data . Here we focus on poisoning the {parallel} training data and leave the {monolingual} data poisoning to potential future work.} used in various NMT training scenarios for enacting black-box targeted attacks, and a discussion of a suite of defensive measures for countering such attacks.}  This paper presents and analyses the main stages of the black-box targeted attacks on NMT systems driven by parallel data poisoning. It starts with a case study on the strategy of poisoning the web source from which the parallel data can be harvested at scale . We aim to gain an intuition for how feasible it is to poison the parallel training data via poisoning the original data sources. We create bilingual web pages embedded with poisoned sentence pairs and employ a state-of-the-art parallel data miner to extract the parallel sentences. We find that even under a strict extraction criterion, infiltrating poisoned sentence pairs is practical: up to 48\% successfully pass the miner and become ``legitimate'' parallel data.  Secondly, we explore parallel data poisoning on two common NMT training scenarios, where the system is trained from scratch  ; or using pre-train \& fine-tune steps   . We conduct experiments to evaluate the effectiveness of the above poisoning scenarios in a controllable environment . We find that both from-scratch training of a system and fine-tuning a pre-trained system are highly sensitive to the attack: with only 32 poison instances injected into a training set of 200k instances , the attack succeeds at least 65\% of the time.  In contrast, poisoning a pre-trained system proves ineffective if it is later fine-tuned on clean data, suggesting a clean fine-tuning step could be used to mitigate poisoned pre-training.  Moreover, we identify challenges when attacking common terms in a dataset. We find that on common terms whose correct translations are prevalent in the dataset, the attack has to deal with potential collisions between generating the correct translation and the malicious one , which may significantly impede the attack performance. Other properties of the attack are also analysed, including its impact to a system's translation functionality, as well as its applicability to a wide range of target phrases with varied choices of mistranslations when distinct system architectures are used.  Thirdly, to generalise our findings from the controllable experiments, we further test attacks on production-scale systems equipped with state-of-the-art architectures and trained with large-scale parallel data .  Our results are alarming: even though the training data is massive , the system is still susceptible to attacks with extremely low poisoning budgets in both from-scratch training  and the pre-train \& fine-tune paradigm .  Prompted by the seriousness of our findings, we discuss defensive counter measures to the proposed poisoning scenarios .  
"," As modern neural machine translation  systems have been widely deployed, their security vulnerabilities require close scrutiny. Most recently, NMT systems have been found vulnerable to targeted attacks which cause them to produce specific, unsolicited, and even harmful translations. These attacks are usually exploited in a white-box setting, where adversarial inputs causing targeted translations are discovered for a known target system. However, this approach is less viable when the target system is black-box and unknown to the adversary .  In this paper, we show that targeted attacks on black-box NMT systems are feasible, based on poisoning a small fraction of their parallel training data.  We show that this attack can be realised practically via targeted corruption of web documents crawled to form the system's training data. We then analyse the effectiveness of the targeted poisoning in two common NMT training scenarios: the from-scratch training and the pre-train \& fine-tune paradigm. Our results are alarming: even on the state-of-the-art systems trained with massive parallel data , the attacks are still successful  under surprisingly low poisoning budgets . Lastly, we discuss potential defences to counter such attacks.",192
"  .     %      % % final paper: en-us version      %        % space normally used by the marker      This work is licensed under a Creative Commons       Attribution 4.0 International License.      License details:      \url{http://creativecommons.org/licenses/by/4.0/}. }  %1Yang 閺堫剚顔屽楦款唴婵″倷绗呴幓蹇氬牚閿涙瓊MT閸欐牕绶辨禍鍝燨TA閻ㄥ嫬鐤勬灞炬櫏閺嬫粌鑻熷妤鍩屾禍鍡楃畭濞夋稓娈戞惔鏃傛暏閿涘牐顔戦弬鍥х穿閻㈩煉绱氶妴鍌滄暠娴滃孩婀佹径褔鍣洪惃鍕棘閺佸府绱濋幍娴狀檾MT闂囩憰浣搞亣闁插繒娈戠拋顓犵矊閺佺増宓侀弬鐟板讲閸忓懎鍨庨崣鎴炲皩鐎瑰啰娈戞导妯哄◢閵嗗倻鍔ч懓灞芥躬鐎圭偤妾惔鏃傛暏娑擃叏绱濋弫鐗堝祦瀵板娴兼艾鍨庣敮鍐х瑝閸у浄绱濋幋鏍懏绉归崣濠傚煂妫板棗鐓欓懛顏堝倸绨查惃鍕６妫版ǜ鍌氭躬鏉╂瑧顫掗幆鍛枌娑撳绱漀MT濡崇峰瀵版导姘朵粣韫囨ê鍑＄涳箑鍩岄惃鍕叀鐠囧棴绱濋懓灞藉箵閹风喎鎮庨弬鐗堝潑閸旂姷娈戦弫鐗堝祦閿涘瞼鍔ч懓灞炬付缂佸牆绶遍崚鎵畱濡崇风憰浣割槱閻炲棗宓堥弰顖氬弿閸掑棗绔烽惃鍕殶閹诡噯绱濇潻娆戭潚閻滄媽钖勭亸杈ㄦЦ鏉╃偟鐢荤涳缚绡勬稉顓犳畱閻忛箖姣﹂幀褔浠愯箛姗堢礄瀵洜鏁ら惄绋垮彠閺傚洨灏為敍澶堝倹寮挎潻鏉挎禈1閻ㄥ嫬鐤勬宀娲伴惃鍕簰閸欏﹦绮ㄧ拋鐑樻降妤犲矁鐦夐幋鎴滄粦娑撳﹪娼伴惃鍕鏉╁府绱欓崶鍓у娑撳秴顧勫〒鍛珰閿涘苯鎸ㄩ崗鑸垫Ц閺傚洤鐡ч敍宀鏃辨潪瀵告畱閳ユ窂LEU閳ユ粏顕柈鐣屾暏婢堆冨晸閿涘 Neural machine translation  models have achieved state-of-the-art results and have been widely used in many fields. Due to numerous parameters, NMT models can only play to their advantages based on large-scale training data. However, in practical applications, NMT models often need to perform translation for some specific domain with only a small quantity of in-domain data available.  In this situation, continual training, which is also referred to as fine-tuning, is often employed to improve the in-domain translation performance. In this method, the model is first trained with large-scale general-domain training data and then continually trained with the in-domain data. With this method, the in-domain performance can be improved greatly, but unfortunately, the general-domain performance decline significantly, since NMT models tend to overfit to frequent observations  in the in-domain data but forget previously learned knowledge. This phenomenon is called catastrophic forgetting. Figure shows the performance trends on the in-domain and general-domain. %As the size of the training corpus grows, the NMT model is trained in the manner of continual learning  from a stream of data.  %Unfortunately, there usually exists a distribution bias in large data set especially when the data is collected from different domains. In this situation, the NMT model has a tendency towards over-fitting to frequent observations  in the newly added data, but forgetting previously learned patterns from the old data, leading to poor performance in the old data.  In the example of domain adaptation shown in Figure, as the training goes, the performance surges for in-domain while slides fast for general-domain. This phenomenon is the catastrophic forgetting of neural network.  %when there are large amounts of parallel training sentences available. However, similar to many other successful neural network-based methods, it also has limited continual learning ability to learn from a stream of training data, which could have different distributions . It is because the NMT system suffers from catastrophic forgetting which refers to that model has a tendency towards over-fitting to frequent observations  in newly added training data, but forgetting previously learned features in the old data.   %Figure denotes this phenomenon in NMT.  %1Yang 娑撳娼版潻娆愵唽閸樼粯甯 %Improving the continual learning ability of the NMT system is of significant importance both in theory and practice. From the artificial intelligence perspective, it can be seen as another step towards the grand goal of creating a real intelligent translation system that can learn continuously new translation skills without forgetting old knowledge as a human does. From a practical perspective, it enables the model to update the model with only recent new data to improve the model's overall performance. We don't need to retrain the model from scratch which is very time-consuming. Moreover, considering that a well-trained model maybe is already deployed in an application, the original training data may not be available at that time. Therefore it is very necessary to improve the continual learning ability of the NMT system.      %1Yang 閺堫剚顔屽楦款唴婵″倹妲搁幓蹇氬牚閿涙氨浼ㄩ梾鐐囦粣韫囨ü绔撮惄瀛樻Ц缁佺偟绮＄純鎴犵捕鐠侇厾绮屾稉顓犳畱娑撴径褔姣︽０姗堢礉閾忕晫鍔ч惄顔煎瀹歌尙绮￠張澶夌娴滄稑浼愭担婊嗗毀閸旀稐绨憴锝呭枀鏉╂瑤閲滈梻顕顣介敍灞借嫙缂佹瑥鍤禍鍡曠娴滄稖顢戞稊瀣箒閺佸牏娈戠憴锝呭枀閺傝纭堕敍鍫濈穿閻€劎娴夐崗铏瀮閻氼噯绱氶敍灞肩稻閺勵垳娲伴崜宥呰嫙濞屸剝婀佸銉ょ稊閸樼粯甯扮槐銏犳躬閻忛箖姣﹂幀褔浠愯箛妯跨箖缁嬪鑵戦崘鍛村劥閺佺増宓侀惃鍕綁閸栨牗鍎忛崘纰夌礉鏉╂瑧顫掗幒銏㈠偍娴兼碍婀侀崝鈺绨幋鎴滄粦閻炲棜袙閻忛箖姣﹂幀褔浠愯箛妯哄絺閻㈢喓娈戦崢鐔锋礈楠炲爼鍣伴崣鏍祲鎼存梻娈戦幒顏呮煢閵  Many methods have been proposed to address the catastrophic forgetting problem under the scheme of fine-tuning.  ensembles the general-domain model and the fine-tuned model together so that the integrated model can consider both domains.   introduces domain-specific output layers for both of the domains and thus the domain-specific features of the two domains can be well preserved.  , , and  propose regularization-based methods that introduce an additional loss to the original objective to help the model trade off between the general-domain and in-domain. All these methods show their effectiveness and have mitigated the performance decline on general-domain, but we still don't know what happened inside the model during continual training and why these methods can alleviate the catastrophic forgetting problem. The study on these can help to understand the working mechanism of continual training and inspire more effective solutions to the problem in return.    %Catastrophic forgetting is a long-known problem in the training of neural networks. Some researchers have managed to alleviate this problem with different strategies, such as changing the model structure, adding an extra regularization term, employing complementary learning systems  theory-based strategies and so on. However, to the best of our knowledge, these methods mainly focus on how to solve the problem, not what causes the problem. %Understanding the cause of the problem will inspire effective solutions. %, there is still no work trying to figure out the inner reason for catastrophic phenomenon and no direct evidence to show the change of model parameters in NMT. We believe that the attempt to understand this phenomenon can help us adopt appropriate measures to solve this problem.  %it is still not clear what happens during the continual learning process and what causes catastrophic forgetting indeed.  %1Yang 閺堫剚顔岄崣顖欎簰鏉╂瑦鐗遍崘娆欑窗閸︺劍婀伴弬鍥风礉閹存垳婊戠亸婵婄槸閸︺劑顣崺鐔诲殰闁倸绨查惃鍕攱閺嬫湹绗呴崢缁樺赴缁鳖晼arameters閸滃瞼浼ㄩ梾鐐囦粣韫囨娈戦崗宕囬兇閿涘苯鑻熼崚鑽ゆ暰閸戠皢arameters閸︺劎浼ㄩ梾鐐囦粣韫囨ǹ绻冪粙瀣╄厬閻ㄥ嫬褰夐崠鏍Ъ閸旇￥鍌欒礋娴滃棜鎻崚鎷岀箹娑擃亞娲伴惃鍕剁礉閹存垳婊戦柅姘崇箖Absolute value閸滃瓗IM閺夈儴鐦庢导鏉垮棘閺佹澘婀Ο鈥崇风拋顓犵矊娑擃厾娈戦柌宥堫洣閹嶇礄閸欏倽鍐╂瀮閻氼噯绱氶敍灞借嫙闁俺绻冮崣鍌涙殶閹匡箓娅庨惃鍕煙濞夋洘娼甸幒銏㈠偍鏉╂瑤绨洪崡鏇熸殶鐎靛湱鐐曠拠鎴炑嗗厴閻ㄥ嫬濂栭崫宥冨倿姘崇箖鐎圭偤鐛欑紒鎾寸亯閿涘本鍨滄禒顒褰傞悳鏉款嚠娴滃酣姘辨暏妫板棗鐓欓柌宥堫洣閻ㄥ嫬寮弫鏉款嚠娴滃穼n-domain娴犲秶鍔у鍫ュ櫢鐟曚緤绱濋懓灞芥躬妫板棗鐓欓懛顏堝倸绨查惃鍕箖缁嬪鑵戞潻娆庣昂閸欏倹鏆熼惃鍕綁閸栨牕绶㈡径褋鍌氱唨娴滃氦绻栨禍娑樺絺閻滃府绱濈电懓绨叉禍搴ょ槑娴兼澘寮弫浼村櫢鐟曚焦褏娈戞稉銈囶潚閺傝纭堕敍灞惧灉娴狀剛娴夋惔鏃傛畱閹绘劕鍤稉銈囶潚閺傝纭堕弶銉﹀付閸掓儼绻栨禍娑㈠櫢鐟曚胶娈戦崣鍌涙殶閸︺劑顣崺鐔诲殰闁倸绨查惃鍕箖缁嬪鑵戞稉宥勭窗閸欐ê瀵叉潻鍥с亣閿涘矁宀娼冮柌宥勭艾鐠嬪啯鏆ｉ柇锝勭昂娑撳秹鍋呮稊鍫ュ櫢鐟曚胶娈戦崣鍌涙殶閵嗗倸鐤勬宀绮ㄩ弸婊嗐冮弰搴㈠灉娴狀剛娈戦弬瑙勭《閼宠棄婀穱婵婄槈in-domain閺佺増宓佹稉濠勬畱缂堟槒鐦ч幀褑鍏橀崣妯哄娑撳秵妲戦弰鍓ф畱閹懎鍠屾稉瀣亣楠炲懎瀹抽惃鍕絹妤傛﹢姘辨暏妫板棗鐓欓惃鍕倳鐠囨垶褑鍏橀妴  %Given this, we seek to understand the relationship between catastrophic forgetting phenomenon and model parameters under the task of domain adaptation. More specifically, we aim to figure out the trend of model parameters during catastrophic forgetting. To fulfill this goal, we propose two methods to evaluate the importance of the model parameters. The first is to use the absolute value of model parameters and the second is to use the empirical Fisher Information Matrix . To verify the effectiveness and correctness of the proposed methods, we then do parameter erasure experiments. According to the experimental results, we find that some parameters are important for both the general-domain and in-domain. Based on these findings, we try to alleviate catastrophic forgetting by designing learning strategies based on the importance of the parameters. We put more constrains on those important parameters to make them change more conservatively while encourage those less important parameters to change more aggressively during the continual learning process. The experiments on multiple translation tasks show that our methods can improve the translation quality on the new domain without degrading the performance on the old domain too much.  Given above, in this paper, we focus on the catastrophic forgetting phenomenon and investigate the roles of different model parts during continual training. To this end, we explore the model from the granularities of modules and parameters . In the module analyzing experiments, we operate the model in two different ways, by freezing one particular module or freezing the whole model except for this module. We find that different modules preserve knowledge for different domains. In the parameter analyzing experiments, we erase parameters according to their importance which is evaluated by the Taylor expansion-based method . According to the experimental results, we find that some parameters are important for both of the general-domain and in-domain and meanwhile they change greatly during domain adaptation which may result in catastrophic forgetting.  To ensure the validity and reliability of the findings, we conduct experiments over different language pairs and domains.   \iffalse Given this, we step into the catastrophic forgetting phenomenon by investigating the influence of different model parts from different granularities, depicting the different roles played by them during continual training. Inspired by the work of  and , we conducted two kinds of analyzing experiments. The first, focusing on the macro parts of the model, is the module analyzing experiment, where we freeze the target module of the model or freeze the whole model except for the target module during continual training to study the influence of each module on the translation performance. We found that some modules are of higher capacity to preserve the general-domain knowledge while some modules are more essential for adapting to the in-domain.  The second, focusing on the micro parts of the model is the parameter analyzing experiment based on the parameter importance, where the Taylor expansion-based method is adopted as the importance evaluation criterion.  According to the experimental results, we found that some parameters are important for both of the general-domain and in-domain and meanwhile they fluctuate greatly during domain adaptation which may result in performance slipping. To ensure the validity and reliability of our conclusions, we conducted our experiments across different language pairs and domains.  \fi  Our main contributions are summarized as follows:   \iffalse To answer these questions, we put forward two ways of evaluating the importance of the model parameters. The first is to use the absolute value of model parameters and a larger absolute value stands for they are more important for the model. Inspired by the work of, we use the diagonal of the Fisher information matrix  of the model parameters to evaluate the importance. To verify the effectiveness and correctness of the proposed methods, we then did parameter erasure experiments which is an effective analysis approach. The results show that some model parameters are more important than others and have much more impact on the final translation quality.  this phenomenon by analyzing the change of model parameters during the continual learning process. We focus on the domain adaptation task of NMT under the continual learning scenario which means first we make the model well-trained using large amounts of general-domain data, and then this model is further trained using limited amounts of in-domain data which is from another domain. It should be noted that no general-domain data is available during the further trained process which is a common practice of continual learning. We aim to investigate the following questions:    Based on our findings of parameter importance above, we then investigate their changes during the continual learning process. We find that the important parameters for the general-domain translation still play major roles for the in-domain translation by doing another parameter erasure experiments. What's more, the substantial decline of general-domain translation quality and the rise of in-domain translation quality is also due to the change of these parameters.   Finally, based on our findings, we propose some practical methods to overcome the catastrophic forgetting phenomenon by parameter regularization method and learning rate adjustment method based on their importance to the model. We change the important parameters slightly while changing the less important parameters more aggressively. The results show that our approach can alleviate catastrophic forgetting significantly.      Our work indicates that some parameters are more important than others and the change of these parameters can influence translation results a lot. Therefore, we can try to alleviate catastrophic forgetting by designing different learning strategies based on the importance of the parameters. As far as we know, this is the first work trying to analyze the catastrophic forgetting phenomenon in NMT. Moreover, the analyzing methods we put forward in this work are task-independent and can be applied to other neural network-based methods in other tasks. \fi \iffalse extra space to store all the old training data or even retrain from scratch with the and without storing old training data or even retraining with   This work focuses on the domain adaptation problem of NMT which is a special case of the continual learning scenario of the neural network. They share the same training task but the distribution of the training data is different.  Domain adaptation deals with the problem of improving the performance of a model trained on a general domain data over test instances from a new domain. In such a scenario, we usually have large amounts of general-domain training data and a welled trained model based on it. In contrast, we only have a limited number of in-domain training data which will lead the NMT system to overfit soon and perform poorly when only trained with these data. Some researchers solve this problem by combining the training data from the general-domain and in-domain together and train a new system from scratch. They usually make use of the domain information to improve the translating performance by adding domain labels to training data or using domain discriminator to find the domain invariant features. On the one hand, these methods are very time consuming and need extra space to store all the training data which is not efficient in real-life applications. On the other hand, due to the relatively small size of in-domain data, it will lead the model to overfit the general-domain data which has been observed in the results.   Fine-tuning is a fast and efficient method for continual learning of neural networks which has already been applied for NMT. NMT system is first trained on general-domain data and then further trained on in-domain data.   Domain adaptation is the most common application scenario of continual learning in NMT which has drawn much attention recently. Under this scenario, we   The translation quality drops quickly when the distribution of the training data changes. It suffers a catastrophic forgetting in the continual training process. \fi     
","  %Neural machine translation  always suffers catastrophic forgetting during the continual learning process which means the model tends to forget all its previously learned knowledge when further trained with new data with different distributions, like from different domains or languages. However, it is not clear what happens during this process and what causes this phenomenon. More specifically, it is not clear whether this is due to the overall change of the model or the impact of certain parameters. In this work, we focus on the domain adaptation task of NMT under the continual learning scenario. First, we put forward two ways for evaluating the importance of the parameters and show that the translation quality mainly dependents on the most important parameters of the model. Then we analyze the behavior of the parameters according to their importance for the model during the continual learning process and it shows that the important parameters for the general-domain translation still play major roles for the in-domain translation after the continual learning process. What's more, the catastrophic forgetting phenomenon, shown as the substantial decline of general-domain translation quality with the rise of in-domain translation quality,  is mainly due to the change of these important parameters.  Finally, we propose some practical methods to overcome the catastrophic forgetting by controlling the updates of parameters differently based on their importance.    Neural machine translation  models usually suffer from catastrophic forgetting during continual training where the models tend to gradually forget previously learned knowledge and swing to fit the newly added data which may have a different distribution, e.g. a different domain. Although many methods have been proposed to solve this problem, we cannot get to know what causes this phenomenon yet. Under the background of domain adaptation, we investigate the cause of catastrophic forgetting from the perspectives of modules and parameters . The investigation on the modules of the NMT model shows that some modules have tight relation with the general-domain knowledge while some other modules are more essential in the domain adaptation. And the investigation on the parameters shows that some parameters are important for both the general-domain and in-domain translation and the great change of them during continual training brings about the performance decline in general-domain. We conduct experiments across different language pairs and domains to ensure the validity and reliability of our findings.    %by tracing parameter variation in this progress and depict the influence of different model modules.  %and depict the relationship between them so that we can work out solutions to the catastrophic forgetting problem based on these findings.  %Under the background of domain adaptation for machine translation, we found that some parameters play an essential role in both general domain and in-domain translation and the change of them brings about the performance decline in general-domain. Based on these findings, we propose a solution to detect these important parameters and accordingly suppress their fluctuation during domain adaptation. Experimental results prove  %that our method can greatly improve the translation quality in in-domain and meanwhile minimize the negative influences on general-domain translation.",193
"  Recurrent neural network architectures have demonstrated remarkable success in natural language processing, achieving state of the art performance across an impressive range of tasks ranging from machine translation to semantic parsing to question answering . These tasks demand the use of a wide variety of computational processes and information sources , and are evaluated in coarse-grained quantitative ways. As a result, it is not an easy matter to  identify the specific strengths and weaknesses in a network's solution of a task.    In this paper, we take a different tack, exploring the degree to which neural networks successfully master one very specific aspect of linguistic knowledge: the interpretation of sentences containing reflexive anaphora.  We address this problem in the context of the task of semantic parsing, which we instantiate as mapping a sequence of words into a predicate calculus logical form representation of the sentence's meaning. \pex<ex:transform>     \a Mary runs       \a John sees Bob   \xe Even for simple sentences like those in~, which represent the smallest  representations of object reflexives in English, the network must learn lexical semantic  correspondences  and a mode of composition .   %Such simple disentangled representations of meaning are highly successful once all of the words are learned.  Of course, not all of natural language adheres to such simple formulas. Reflexives, words like herself and himself, do not have an interpretation that can be assigned independently of the meaning of the surrounding context. \pex<ex:transform-refl>     \a Mary sees herself      \a Alice sees herself  \xe In these sentences, the interpretation of the reflexive is not a constant that can be combined with the meaning of the surrounding elements. Rather, a reflexive object must be interpreted as identical to the meaning of verb's subject.  Of course, a network could learn a context-sensitive interpretation of a reflexive, so that for any sentence with \lex{Mary} as its subject, the reflexive is interpreted as , and with \lex{Alice} as its subject it is interpreted as .  However, such piecemeal learning of reflexive meaning will not support generalization to sentences involving a subject that has not been encountered as the antecedent of a reflexive during training, even if the interpretation of the  subject has occurred elsewhere. What is needed instead is an interpretation of the reflexive that is characterized not as a specific  output token, but rather as an abstract instruction to duplicate the interpretation of the subject. Such an abstraction requires more than the ``jigsaw puzzle"" approach to meaning that simpler sentences afford.   \citet{Marcus98} argues that this kind of abstraction, which he takes to require the use of  algebraic variables to assert identity, is beyond the capacity of recurrent neural networks.  \citeauthor{Marcus98}'s demonstration involves a simple recurrent network  language model that is trained to predict the next word over a corpus of sentences of the following form: \pex     \a A rose is a rose.     \a A mountain is a mountain. %    \a A car is a car \xe All sentences in this training set have identical subject and object nouns.  \citeauthor{Marcus98} shows, however, that the resulting trained network does not correctly predict the subject noun when tested with a novel preamble `\lex{A book is a }'. Though intriguing, this demonstration is not entirely convincing: since the noun occurring in the novel preamble, \lex{book} in our example, did not occur in the training data, there is no way that the network could possibly have known which  output should correspond to the reflexive for a sentence containing the novel  subject noun, even if the network did successfully encode an identity relation between subject and object.   \citet{frank2013} explore a related  task in the context of SRN interpretation of reflexives. In their experiments, SRNs were trained to map input words to corresponding semantic symbols that are output on the same time step in which a word is presented. For most words in the vocabulary, this is a simple task: the desired output is a constant function of the input .  For reflexives however, the target output depends on the subject that occurs earlier in the sentence. \citeauthor{frank2013}\ tested the network's ability to interpret a reflexive in sentences containing a subject that had not occurred as a reflexive's antecedent during training. However, unlike Marcus' task, this subject and its corresponding semantic symbol did occur in other  contexts in the training data, and therefore was in the realm of possible inputs and outputs for the network. Nonetheless, none of the SRNs that they trained succeeded at this task for even a single test example.   Since those experiments were conducted, substantial advances have been made on recurrent neural network architectures, some of which have been crucial in the success of practical NLP systems.   These innovations open up the possibility that modern network architectures may well be able to solve the variable identity problem necessary for mapping reflexive sentences to their logical form. In the experiments we describe below, we explore whether this is the case.      
"," Reflexive anaphora present a challenge for semantic interpretation: their meaning varies depending on context in a way that appears to require abstract variables. Past work has raised doubts about the ability of recurrent networks to meet this challenge. In this paper, we explore this question in the context of a fragment of English that incorporates the relevant sort of contextual variability. We consider sequence-to-sequence architectures with recurrent units and show that such networks are capable of learning semantic interpretations for reflexive anaphora which generalize to novel antecedents. We explore the effect of attention mechanisms and different recurrent unit types on the type of training data that is needed for success as measured in two ways: how much lexical support is needed  to induce an abstract reflexive meaning  and what contexts must a noun phrase occur in to support generalization of reflexive interpretation to this noun phrase?",194
"  Pre-trained contextualized language models such as BERT are state-of-the-art for a wide variety of natural language processing tasks. Similarly, in Information Retrieval , these models have brought about large improvements in the task of ad-hoc retrieval---ranking documents by their relevance to a textual query, where the models increasingly dominate the leaderboards of ad-hoc retrieval competitions.  Despite this success, little is understood about why pretrained language models are effective for ad-hoc ranking. What new aspects of the task do neural models solve that previous approaches do not?  Previous work has shown that traditional IR axioms, e.g. that increased term frequency should correspond to higher relevance, do not explain the behavior of recent neural models . Outside of IR, others have examined what characteristics contextualized language models learn in general , but it remains unclear if these qualities are valuable to the ad-hoc ranking task specifically. Thus, new approaches are necessary to characterize the models.  We propose a new framework aimed at Analyzing the Behavior of Neural IR ModeLs  based on three testing strategies: ``measure and match'', ``textual manipulation'', and ``dataset transfer''. The ``measure and match'' strategy, akin to the diagnostic tests proposed by~\citet{Rennings2019AnAA}, constructs test samples by controlling one measurement  and varying another  using samples from an existing IR collection. The ``textual manipulation'' strategy tests the effect that altering the document text has on ranking. The ``dataset transfer'' strategy constructs tests from non-IR datasets.  The new tests allow us to isolate model characteristics---such as sensitivity to word order, or preference for summarized rather than full documents---that are imperceptible using other approaches.  We also release an open-source implementation of our framework that makes it easy to define new diagnostics and to replicate the analysis on new models.  Using our new framework, we perform the first large-scale analysis of neural IR models. We compare today's leading ranking techniques, including those using BERT and T5, as well as methods focused on efficiency like DocT5Query and EPIC. We find evidence showing that neural models are able to make effective use of textual signals that are not reflected by classical term matching methods like BM25.  For example, when controlling for term frequency match, the neural models detect document relevance much more accurately than the BM25 baseline, and the effect is more pronounced in larger neural models.  Further, unlike prior approaches, rankers based on BERT and T5 are heavily influenced by word order: shuffling the words in a document consistently lowers the document's score relative to the unmodified version. We also find significant differences between different neural models: e.g., while most models treat queries navigationally , the BERT-based EPIC model and T5 do not exhibit such behaviors. Finally, these models can exhibit unexpected : adding additional relevant text to the end of a document frequently can reduce its ranking score, and adding non-relevant content can increase it---despite document length itself having a limited effect on the ranking scores. %  In summary, we present a new framework  for performing analysis of ad-hoc ranking models. We then demonstrate how the framework can provide insights into ranking model characteristics by providing the most comprehensive analysis of neural ranking models to date. Our released software framework facilitates conducting further analyses in future work.                  
"," Numerous studies have demonstrated the  effectiveness of pretrained contextualized language models such as BERT and T5 for ad-hoc search. However, it is not well-understood why these methods are so effective, what makes some variants more effective than others, and what pitfalls they may have.  We present a new comprehensive framework for Analyzing the Behavior of Neural IR ModeLs , which includes new types of diagnostic tests that allow us to probe several characteristics---such as sensitivity to word order---that are not addressed by previous techniques.  To demonstrate the value of the framework, we conduct an extensive empirical study that yields insights into the factors that contribute to the neural model's gains, and identify potential unintended biases the models exhibit.  We find evidence that recent neural ranking models have fundamentally different characteristics from prior ranking models. For instance, these models can be highly influenced by altered document word order, sentence order and inflectional endings. They can also exhibit unexpected behaviors when additional content is added to documents, or when documents are expressed with different levels of fluency or formality. We find that these differences can depend on the architecture and not just the underlying language model.\footnote{\url{https://github.com/allenai/abnriml}}",195
"  .     %     % % final paper: en-uk version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International Licence.     % Licence details:     % \url{http://creativecommons.org/licenses/by/4.0/}.     %      % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. }  Commonsense is the knowledge shared by the majority of people in society and acquired naturally in everyday life. Commonsense reasoning is the process of logical inference by using commonsense information. Commonsense to answer the questions that is ``'' in Figure  is depicted as: ``'', ``'', and ``.'' An enormous amount of pre-defined commonsense knowledge is available and people can make inferences using this commonsense such as in the following example: ``''  ``''  ``''  ``'' This chain of commonsense reasoning is naturally deduced by humans without substantial difficulty. Whereas people acquire commonsense in their lives, machines cannot learn this knowledge without any assistance. A large amount of external knowledge and several reasoning steps are required for machines to learn commonsense. In recent years, various datasets  have been constructed to enable machines to reason commonsense.    is one of the most widely researched datasets and is presented in Figure  \subref{subfig:examplea}. The studies of commonsense reasoning based on this dataset can be categorized into two mainstream approaches. The first approach uses pre-trained language models with distributed representations, which exhibit high performances on most Natural Language Processing  tasks. However, despite their high performance, these models must be trained with an excessive number of parameters and cannot explain the process of commonsense reasoning. The second approach is reasoning with a commonsense knowledge graph. The generally used commonsense knowledge graph is ConceptNet 5.5 , which includes parsed representation from Open Mind Commonsense  and other different language sources such as WordNet  or DBPedia . In this approach, the subgraph of  ConceptNet corresponding to the questions are transformed into node embeddings by the graph encoder. The candidate with the highest attention score is selected as an answer that is computed between the node embeddings and the word vectors from the language models. To learn the commonsense knowledge that is not observed or understood by the language models, relations from ConceptNet serve as a critical role in this method. The performance is improved by utilizing the relations that are not represented in the text; however, the interpretation of the question is still not enough.   Unlike , the most commonly used method of solving this problem is Knowledge-Based Question-Answering   employing semantic representations. As this method infers the answer with the logical structure of the question using the knowledge base, the question-answering process can be explained in a logical form. In our work, Abstract Meaning Representation  , which is one of the logical structure, is used to understand the overall reasoning process, from the question to the answer.  AMR is a graph for meaning representation that symbolizes the meaning of sentences. AMR illustrates ``who is doing what to whom'' that is implied in a sentence with a graph.  The components of these graphs are not the words, but rather the concepts and their relations. Each concept denotes an event or an entity, and each relation represents the semantic role of the concepts.   In this paper, we enable the language models to exploit the AMR graph to understand the logical structure of sentences. However, it is difficult to infer commonsense information with only an AMR graph, owing to its deficiency of commonsense knowledge of the given sentence. For example, in Figure  \subref{subfig:exampleb}, the AMR graph indicates the path of the logical structure of the sentence ``'' ; in other words, these paths from the single AMR graph lack the proficient information to predict the right answer. Therefore, for commonsense reasoning, dynamic interactions between the AMR graph and ConceptNet are inevitable to reach the correct answer.   Thus, we propose a new compact AMR graph expanded with the ConceptNet's commonsense relations with pruning, and it is called ACP graph. The proposed method can interpret the path from the question to the answer by performing commonsense reasoning within the connected graph, such as ``'' .    The contributions of our study are as follows.      The remainder of this paper is organized as follows. In Section 2, we present the entire process of our method in detail. The experimental setup and results are explained in Section 3. A discussion of the proposed model is provided in Section 4, and Section 5 presents the conclusions. Appendix A provides related works including ConceptNet, previous works on commonsense reasoning, and AMR.     
"," \texttt{CommonsenseQA} is a task in which a correct answer is predicted through commonsense reasoning with pre-defined knowledge. Most previous works have aimed to improve the performance with distributed representation without considering the process of predicting the answer from the semantic representation of the question. To shed light upon the semantic interpretation of the question, we propose an AMR-ConceptNet-Pruned  graph. The ACP graph is pruned from a full integrated graph encompassing Abstract Meaning Representation  graph generated from input questions and an external commonsense knowledge graph, ConceptNet . Then the ACP graph is exploited to interpret the reasoning path as well as to predict the correct answer on the \texttt{CommonsenseQA} task. This paper presents the manner in which the commonsense reasoning process can be interpreted with the relations and concepts provided by the ACP graph. Moreover, ACP-based models are shown to outperform the baselines.",196
"  Part-Of-Speech  tagging is a crucial step for language understanding, both being used in automatic language understanding applications such as named entity recognition  and question answering , but also being used in manual language understanding by linguists who are attempting to answer linguistic questions or document less-resourced languages .   Much prior work  on developing high-quality POS taggers uses neural network methods which rely on the availability of large amounts of labelled data. However, such resources are not readily available for the majority of the world's 7000 languages .  Furthermore, manually annotating large amounts of text with trained experts is an expensive and time-consuming task, even more so when linguists/annotators might not be native speakers of the language.    Active Learning \cite[AL]{lewis1995evaluating,settles2009active} is a family of methods that aim to train effective models with less human effort and cost by selecting such a subset of data that maximizes the end model performance. While many methods have been proposed for AL in sequence labeling , through an empirical study across six typologically diverse languages we show that within the same task setup these methods perform inconsistently. Furthermore, even in an oracle scenario  %  where we have access to the true labels during data selection, existing methods are far from optimal.  We posit that the primary reason for this inconsistent performance is that while existing methods consider uncertainty in predictions, they do not consider the direction of the uncertainty with respect to the output labels. For instance, in Figure  we consider the German token ``die,'' which may be either a pronoun  or determiner . According to the initial model , ``die'' was labeled as PRO majority of the time, but a significant amount of probability mass was also assigned to other output tags  for many examples. Based on this, existing AL algorithms that select uncertain tokens will likely select ``die'' because it is frequent and its predictions are not certain, but they may select an instance of ``die'' with either a gold label of PRO or DET. Intuitively, because we would like to correct errors where tokens with true labels of DET are mis-labeled by the model as PRO, asking the human annotator to tag an instance with a true label of PRO, even if it is uncertain, is not likely to be of much benefit.  Inspired by this observation, we pose the problem of AL for part-of-speech tagging as selecting tokens which maximally reduce the confusion between the output tags. For instance, in the example we would attempt to pick a token-tag pair ``die/DET'' to reduce potential errors of the model over-predicting PRO despite its belief that DET is also a plausible option. We demonstrate the features of this model in an oracle setting where we know true model confusions , and also describe how we can approximate this strategy when we do not know the true confusions.  We evaluate our proposed AL method by running simulation experiments on six typologically diverse languages namely German, Swedish, Galician, North Sami, Persian, and Ukrainian, improving upon models seeded with cross-lingual transfer from related languages . In addition, we conduct human annotation experiments on Griko, an endangered language that truly lacks significant resources.   Our contributions are as follows:           % File tacl2018v2.tex % Sep 20, 2018  % The English content of this file was modified from various *ACL instructions % by Lillian Lee and Kristina Toutanova % % LaTeXery is mostly all adapted from acl2018.sty.  \documentclass[11pt,a4paper]{article} \usepackage{times,latexsym} \usepackage{url} \usepackage[T1]{fontenc} \usepackage{amsmath} \usepackage{amssymb} \usepackage{tabularx} \usepackage{mathtools} \usepackage{booktabs} \usepackage{url} \usepackage{longtable} \usepackage{tabu} \usepackage{multirow} \usepackage{amsfonts} \usepackage{tabu} \usepackage{algorithm} \usepackage{bbm} \usepackage{subfigure} \usepackage[noend]{algpseudocode} \usepackage[normalem]{ulem} \usepackage{enumitem} \makeatletter  \def\BState{\State\hskip-\ALG@thistlm} \usepackage{bbm} \usepackage{xcolor} \DeclareMathOperator*{\argmax}{arg\,max} \DeclareMathOperator*{\b-argmax}{ b\text{-}arg\,max} \DeclareMathOperator*{\argmin}{arg\,min}   %% Package options: %% Short version: ""hyperref"" and ""submission"" are the defaults. %% More verbose version: %% Most compact command to produce a submission version with hyperref enabled %%    \usepackage[]{tacl2018v2} %% Most compact command to produce a ""camera-ready"" version \usepackage[acceptedWithA]{tacl2018v2} %% Most compact command to produce a double-spaced copy-editor's version %\usepackage[acceptedWithA]{tacl2018v2} % %% If you need to disable hyperref in any of the above settings  in the TACL instructions), add "",nohyperref"" in the square %% brackets.  %\usepackage[nohyperref]{tacl2018v2}  %%%% Material in this block is specific to generating TACL instructions \usepackage{xspace,mfirstuc,tabulary} \newcommand{\dateOfLastUpdate}{Sept. 20, 2018} \newcommand{\styleFileVersion}{tacl2018v2}  \newcommand{\gn}[1]{\textcolor{magenta}{\small [#1 --GN]}} \newcommand{\an}[1]{\textcolor{blue}{\small [#1 --AA]}}   \newcommand{\ex}[1]{{\sf #1}}  \newif\iftaclinstructions \taclinstructionsfalse % AUTHORS: do NOT set this to true \iftaclinstructions \renewcommand{\confidential}{} \renewcommand{\anonsubtext}{} \newcommand{\instr} \fi  % \iftaclpubformat % this ""if"" is set by the choice of options \newcommand{\taclpaper}{final version\xspace} \newcommand{\taclpapers}{final versions\xspace} \newcommand{\Taclpaper}{Final version\xspace} \newcommand{\Taclpapers}{Final versions\xspace} \newcommand{\TaclPapers}{Final Versions\xspace} \else \newcommand{\taclpaper}{submission\xspace} \newcommand{\taclpapers}{{\taclpaper}s\xspace} \newcommand{\Taclpaper}{Submission\xspace} \newcommand{\Taclpapers}{{\Taclpaper}s\xspace} \newcommand{\TaclPapers}{Submissions\xspace} \fi  %%%% End TACL-instructions-specific macro block %%%%  \title{Reducing Confusion in Active Learning for Part-Of-Speech Tagging}  \author{Aditi Chaudhary\textsuperscript{1},      Antonios Anastasopoulos\textsuperscript{2,\Thanks{ Work done at Carnegie Mellon University.}},      Zaid Sheikh\textsuperscript{1}, Graham Neubig\textsuperscript{1} \\   \textsuperscript{1}Language Technologies Institute, Carnegie Mellon University\\   \textsuperscript{2}Department of Computer Science, George Mason University\\   { @cs.cmu.edu}}    { }  }    \date{}   %         
"," Active learning  uses a data selection algorithm to select useful training samples to minimize annotation cost. This is now an essential tool for building low-resource syntactic analyzers such as part-of-speech  taggers. Existing AL heuristics are generally designed on the principle of selecting uncertain yet representative training instances, where annotating these instances may reduce a large number of errors. However, in an empirical study across six typologically diverse languages , we found the surprising result that even in an oracle scenario where we know the true uncertainty of predictions, these current heuristics are far from optimal. Based on this analysis, we pose the problem of AL as selecting instances which maximally reduce the confusion between particular pairs of output tags. Extensive experimentation on the aforementioned languages shows that our proposed AL strategy outperforms other AL strategies by a significant margin.  We also present auxiliary results demonstrating the importance of proper calibration of models, which we ensure through cross-view training, and analysis demonstrating how our proposed strategy selects examples that more closely follow the oracle data distribution. The  code is publicly released here.\footnote{\url{https://github.com/Aditi138/CRAL}}",197
" With an increasing submission of academic papers in recent years, the task of making final decisions manually incurs significant overheads to the program chairs, it is desirable to automate the process.  In this study, we aim at utilizing document-level semantic analysis for paper review rating prediction and recommendation.  Given the reviews of each paper from several reviewers as input, our goal is to infer the final acceptance decision for that paper and the reviewers' evaluation with respect to a numeric rating .  Paper review rating prediction and recommendation is a practical and important task in AI applications which will help improve the efficiency of the paper review process. It is also intended to enhance the consistency of the assessment procedures and outcomes, and to diversify the paper review process by comparing human recommended rating with machine recommended rating.  In the literature, most of existing studies cast review rating prediction as a multi-class classification/regression task .  They build a predictor by using supervised machine learning models with review texts and corresponding ratings.  Due to the importance of features, most researches focus on extracting effective features such as context-level features  and user features  to boost prediction performance.  However, feature engineering is time-consuming and labor-intensive.   Recently, with the development of neural networks and its wide applications, various deep learning-based models have been proposed for automatically learning features from text data .  Existing deep learning models usually learn continuous representations of different grains  from text corpus .  Although deep learning models can automatically learn extensive feature representation, they cannot efficiently capture the hierarchical relationship inherent to the review data.  To address this problem,  studied a hierarchical architecture and implemented it in deep learning framework to learn a better document-level representation.  Also, with the success of attention mechanism in many tasks such as machine translation, question answering and so on ,   designed a directional self-attention network to gain context-aware embeddings for words and sentences.  Despite great progress made by these models, they do not focus on the task of paper review rating recommendation and are not effective enough to be directly used for this task because of the following reasons: First, the review data is hierarchical in nature.  There exists a three-level hierarchical structure in the review data: word level, intra-review level and inter-review level, while previous models only capture two-levels  of this hierarchy.  Second, paper reviews are usually much longer than other reviews , while most of these models are working on those shorter reviews stated above and they do not leverage the up to date representation techniques such as BERT  and SciBERT .   In this paper, we propose a novel neural network framework for paper review rating recommendation by taking word, intra-review and inter-review information into account.  Specifically, inspired by HAN  and DiSAN , we introduce a Hierarchical Bi-directional self-Attention Network  framework to effectively incorporate different levels of hierarchical information.  The proposed framework consists of three main modules in end-to-end relationship: sentence encoder, intra-review encoder and inter-review encoder, which can consider hierarchical structures of review data as comprehensive as possible. The outputs of inter-review encoder are leveraged as features to build the rating predictor without any feature engineering. We release the code and data collected by us to enable replication and application to new tasks, available at https://github.com/RingBDStack/HabNet.  The contributions of this work are as follows:    
"," Review rating prediction of text reviews is a rapidly growing technology with a wide range of applications in natural language processing.  However, most existing methods either use hand-crafted features or learn features using deep learning with simple text corpus as input for review rating prediction, ignoring the hierarchies among data.  In this paper, we propose a Hierarchical bi-directional self-attention Network framework  for paper review rating prediction and recommendation, which can serve as an effective decision-making tool for the academic paper review process. Specifically, we leverage the hierarchical structure of the paper reviews with three levels of encoders: sentence encoder , intra-review encoder  and inter-review encoder .  Each encoder first derives contextual representation of each level, then generates a higher-level representation, and after the learning process, we are able to identify useful predictors to make the final acceptance decision, as well as to help discover the inconsistency between numerical review ratings and text sentiment conveyed by reviewers.  Furthermore, we introduce two new metrics to evaluate models in data imbalance situations.  Extensive experiments on a publicly available dataset  and our own collected dataset  demonstrate the superiority of the proposed approach compared with state-of-the-art methods.",198
"  % What is QG and Why it is important Question Generation  aims to endow machines with the ability to ask relevant and to-the-point questions about a document.  QG has important practical applications, such as  generating assessments for course materials in education, prompting user interaction in dialog systems, enabling machines to ask clarification questions such as FAQs, and automatically building large-scale QA datasets for the research community.   % How tranditional works do it? Recent QG approaches have used Seq2Seq models with attention, which feeds the input document into an encoder, and generates a question about the document through a decoder.  % Why it needs RL? The training objective is to maximize the log likelihood of the ground-truth question paired with each input document using teacher forcing. However, as the ground-truth questions are insufficient to account for the many equivalent ways of asking a question, this likelihood-based training suffers from the problem of exposure bias, i.e., the model does not learn how to distribute probability mass over sequences that are valid but different from the ground truth.  % How RL addresses the problem? %   To address this issue, previous QG works proposed to optimize the model directly on question-specific rewards via Reinforcement Learning .  This process decouples the training procedure from the ground truth data, so that the space of possible questions can be better explored. Moreover, it allows the training to target on specific properties we want the question to exhibit, such as relevant to a specific topic or answerable by the document.  % What is the problem for RL-based method? Although various rewards have been employed for QG --- such as BLEU, the answerability reward, and the word movers distance --- optimizing the reward scores does not always lead to higher question quality in practice, as observed by Hosking and Riedel~\shortcite{Hosking2019EvaluatingRF}. How to define robust and effective QG-specific rewards still requires further investigation.   % What we want to do? We aim to analyze the effectiveness of question-specific rewards in QG. Instead of using general natural language generation metrics such as BLEU, we target three QG-related metrics that are commonly cited in human evaluations of question quality:  Fluency indicates whether the question follows the grammar and accords with the correct logic;  Relevance indicates whether the question is relevant to the document; and  Answerability indicates whether the question is answerable given the document. We design a specific RL reward for each metric: a language model based reward for fluency, a discriminator-based reward for relevance, and a QA-based reward for answerability.  After optimizing each reward via RL, we conduct comprehensive analysis, including automatic and human evaluation, to arrive at the following conclusions:  both individual and joint optimization of these rewards can lead to performance gain in automated metrics, but this does not guarantee an improvement in the real question quality;  the reward for relevance substantially helps to improve the question quality, while the reward for answerability reduces the quality due to the bias brought by the QA model; and  a reward is more likely to improve the question quality if the reward score correlates well with human judgement.   
","     Recent question generation  approaches often utilize the sequence-to-sequence framework  to optimize the log likelihood of ground-truth questions using teacher forcing. However, this training objective is inconsistent with actual question quality, which is often reflected by certain global properties such as whether the question can be answered by the document. As such, we directly optimize for QG-specific objectives via reinforcement learning to improve question quality. We design three different rewards that target to improve the fluency, relevance, and answerability of generated questions. We conduct both automatic and human evaluations in addition to thorough analysis to explore the effect of each QG-specific reward.      We find that optimizing on question-specific rewards generally leads to better performance in automatic evaluation metrics. However, only the rewards  that correlate well with human judgement  lead to real improvement in question quality. Optimizing for the others, especially answerability, introduces incorrect bias to the model, resulting in poor question quality. Our code is publicly available at \href{https://github.com/YuxiXie/RL-for-Question-Generation}{https://github.com/YuxiXie/RL-for-Question-Generation}.",199
"  % In daily bases plethora of opinion data is published about different topics and in response to different stimuli using Social Media.  % Aiming to analyse and gain insights from opinions posted in social media, research in stance detection has become increasingly popular in recent years. Framed as a classification task, the stance detection consists in determining if a textual utterance expresses a supportive, opposing or neutral viewpoint with respect to a target or topic . Research in stance detection has largely been limited to analysis of single utterances in social media. Furthering this research, the SardiStance 2020 shared task  focuses on incorporating contextual knowledge around utterances, including metadata from author profiles and network interactions. The task included two subtasks, one solely focused on the textual content of social media posts for automatically determining their stance, whereas the other allowed incorporating additional features available through profiles and interactions. This paper describes and analyses our participation in the SardiStance 2020 shared task, which was held as part of the EVALITA  campaign and focused on detecting stance expressed in tweets associated with the Sardines movement. %  %   % For a network interaction graph, we generate user embeddings, using variations of graph neural network  embedding methods, and then concatenate author's vector with its corresponding utterance features for each stance. We also extract two types of text embedding representations for each utterance, embedding-based features, namely word embedding vectors and cosine similarity vectors, using different models including variations of CNN and bidirectional LSTM models. Further, the results of these two feature extraction methods are concatenated for the final classification step. We also consider the standard methods that extract frequency-based representations from author profiles and stance utterances including unigrams and Tfidf vectors. All these four features where combined and fed into the drop out and dense layers, to finally generate the final label using a softmax activation function. Though, we deactivate some of these four sources of features and alter the frequency-based vector by excluding some features, changing the embedding source and reducing the dimensionality for highly dimensional vectors  using PCA.}   
"," This paper presents our submission to the SardiStance 2020 shared task, describing the architecture used for Task A and Task B. While our submission for Task A did not exceed the baseline, retraining our model using all the training tweets, showed promising results leading to  using bidirectional LSTM with BERT multilingual embedding for Task A. For our submission for Task B, we ranked 6th . With further investigation, our best experimented settings increased performance from  to  with same architecture and parameter settings and after only incorporating social interaction features- highlighting the impact of social interaction on the model's performance.",200
"   State-of-the-art for most existing natural language processing  classification tasks is currently achieved by systems that are first pre-trained on auxiliary language modeling tasks and then fine-tuned on the task of interest with cross-entropy loss . Although commonly used, cross-entropy loss -- the KL-divergence between one-hot vectors of labels and the distribution of model's output logits -- has several shortcomings. Cross entropy loss leads to poor generalization performance due to poor margins , and it lacks robustness to noisy labels  or adversarial examples . Effective alternatives have been proposed to change the reference label distributions through label smoothing , Mixup , CutMix , knowledge distillation  or self-training~.  Additionally, it has been recently demonstrated in NLP that fine-tuning using cross entropy loss tends to be unstable , especially when supervised data is limited, a scenario in which pre-training is particularly helpful. To tackle the issue of unstable fine-tuning, recent work proposes local smoothness-inducing regularizers  and regularization methods inspired by the trust region theory  to prevent representation collapse that lead to poor generalization performance. Empirical analysis suggests that fine-tuning for longer, reinitializing top few layers~, and using debiased Adam optimizer during fine-tuning~ can make the fine-tuning procedure more stable.  We are inspired by the learning strategy that humans deploy when given a few examples -- try to find the commonalities between the examples of each class and contrast them with examples from other classes. We hypothesize that a similarity-based loss will be able to hone in on the important dimensions of the multidimensional hidden representations and lead to better few-shot learning results and be more stable while fine-tuning pre-trained models. We propose a novel objective for fine-tuning pre-trained language models that includes a supervised contrastive learning term that pushes examples from the same class close and examples of different classes further apart. The new term is similar to the contrastive objective used for self-supervised representation learning in various domains such as image, speech, and video domains. . In constrast to these methods, however, we use a contrastive objective for supervised learning of the final task, instead of contrasting different augmented views of examples.  Adding supervised contrastive learning  term to the fine-tuning objective improves performance on several natural language understanding tasks from the GLUE benchmark , including SST-2, CoLA, MRPC, RTE, and QNLI over the state-of-the-art models fine-tuned with cross entropy loss. The improvements are particularly strong in few-shot learning settings , and models trained with SCL are not only robust to the noise in the training data, but also have better generalization ability to related tasks with limited labeled data. Our approach does not require any specialized architectures , memory banks , data augmentation of any kind, or additional unsupervised data. To the best of our knowledge, our work is the first to successfully integrate a supervised contrastive learning objective for fine-tuning pre-trained language models.   % \ves{end of alternative}  % State-of-the-art models for most existing natural language processing  tasks are currently learned by fine-tuning pre-trained large language models  that have been shown to capture semantic, syntactic, and world knowledge.  Recent attempts at improving the pre-training stage over masked language modeling~ has led to improvements on natural language understanding tasks, but fine-tuning stage has stayed the same for all downstream NLP classification tasks: add a task-specific output layer to the pre-trained language model and continue training on the labeled task data using cross-entropy loss.  % Cross-entropy loss is the most widely adopted objective for supervised classification models, defined as the KL-divergence between one-hot vectors of labels and the distribution of model's output logits. Although commonly used by the state-of-the-art models across many fields including NLP, there has been several works demonstrating the shortcomings of the cross-entropy loss, showing that it leads to poor generalization performance due to poor margins , and lack of robustness to noisy labels  or adversarial examples . Among the alternative objective functions proposed, more effective approaches in practice have been the ones that change the reference label distributions such as label smoothing , Mixup , CutMix , knowledge distillation  or self-training~.  % Several recent studies show that the fine-tuning procedure is unstable , especially for the case where supervised data is limited, a scenario in which pre-training is particularly helpful. To tackle the issue of unstable fine-tuning, local smoothness-inducing regularizers  and regularization methods inspired by the trust region theory  have been proposed to prevent representation collapse that leads to poor generalization performance of task models. There has also been empirical analysis that suggests fine-tuning for longer, reinitializing top few layers~, and using debiased Adam optimizer during fine-tuning~ make the fine-tuning procedure more stable.   % On the other hand, contrastive learning methods have seen remarkable success for self-supervised representation learning on various downstream tasks, particularly in the image, speech, and video domains.   % These self-supervised contrastive learning methods primarily try to reduce the distance between representations of the positive pairs while increasing the distance between representations of the negative pairs. Positive pairs are constructed as the different augmented views of the same labeled example, and negative pairs are simply augmented views of all the other examples. Augmented views of the examples are often constructed with state-of-the-art data augmentation methods such as RandAugment  or AutoAugment  for the computer vision domain, and distance metric is often chosen as the inner product or the Euclidean distance between the representations of the pairs in a low-dimensional embedding space.    % Recently, \citet{Khosla2020SupervisedCL} extended contrastive learning to a fully supervised setting through using label information while constructing positive and negative pairs, showed improved performance over cross-entropy loss baseline on ImageNet image classification accuracy and robustness benchmarks, and demonstrated that supervised contrastive learning is less sensitive to hyperparameter changes. Similarly, \citet{Liu2020HybridDT} propose a hybrid discriminative-generative training of energy-based models, where they approximate the generative term with a contrastive objective and demonstrate improved image classification accuracy on CIFAR-10 and CIFAR-100, along with improved performance on robustness, out-of-distribution detection, and calibration.  % In this paper, we propose a supervised contrastive learning regularization for fine-tuning of large pre-trained language models that helps the model leverage label information more effectively across different labeled data regimes. Our approach does not require specialized architectures , memory banks , or very large batch sizes , but still outperforms the strong baseline of fine-tuning RoBERTa-Large on labeled task data with cross-entropy loss, unlike some previous works. To the best of our knowledge, our work is the first to successfully integrate a supervised contrastive learning objective for fine-tuning pre-trained language models. % while sho results on few-shot learning, robustness, and generalization ability.  % We summarize our key contributions in the following: %   
"," State-of-the-art natural language understanding classification models follow two-stages: pre-training a large language model on an auxiliary task, and then fine-tuning the model on a task-specific labeled dataset using cross-entropy loss. Cross-entropy loss has several shortcomings that can lead to sub-optimal generalization and instability.  Driven by the intuition that good generalization requires capturing the similarity between examples in one class and contrasting them with examples in other classes, we propose a supervised contrastive learning  objective for the fine-tuning stage. Combined with cross-entropy, the SCL loss we propose obtains improvements over a strong RoBERTa-Large baseline on multiple datasets of the GLUE benchmark in both the high-data and low-data regimes, and it does not require any specialized architecture, data augmentation of any kind, memory banks, or additional unsupervised data. % In all of our experiments, we use a very competitive baseline of fine-tuning RoBERTa Large using cross entropy loss on the labeled task data.  %including SST-2, CoLA, MRPC, RTE and QNLI. %Our method outperforms the baseline on multiple datasets in the GLUE benchmark including SST-2, CoLA, MRPC, RTE and QNLI for the full dataset regime.  % We also show the effectiveness of our regularization for few-shot learning and demonstrate  % We also demonstrate the robustness of the learned representations by using noisy datasets, and show that the learned representations are more transferable to related tasks.  We also demonstrate that the new objective leads to models that are more robust to different levels of noise in the training data, and can generalize better to related tasks with limited labeled task data.",201
" With the rapid growth of textual documents on the internet, accessing information from the web has become a challenging issue . Often users want the summary of a topic from various sources to fulfill their information needs . The QF-MDS task deals with such problems where the goal is to summarize a set of documents to answer a given query.     In the QF-MDS task, the summaries generated by the summarizer can be either extractive or abstractive. An extractive summarizer extracts relevant text spans from the source document, whereas an abstractive summarizer generates a summary in natural language that may contain some words which did not appear in the source document . With the rising popularity of virtual assistants in recent years, there is a growing interest to integrate abstractive summarization capabilities in these systems for natural response generation .   One major challenge for the QF-MDS task is that the datasets  used for such tasks do not contain any labeled training data. Therefore, neural summarization models that leverage supervised training cannot be used in these datasets. Note that for other related tasks , how to reduce the demands for labeling the data and how to leverage unlabeled data were also identified as a major challenge. While using datasets similar to the target dataset as the training data for the QF-MDS task, we find that these datasets only contain multi-document gold summaries. However, the state-of-the-art transformer-based  summarization models  cannot be used in long documents due to computational complexities . To tackle these issues, we propose a novel weakly supervised approach by utilizing distant supervision to generate weak reference summary of each single-document from multi-document gold reference summaries. We train our model on each document with weak supervision and find that our proposed approach that generates abstractive summaries is very effective for the QF-MDS task. More concretely, we make the following contributions:      
"," In the Query Focused Multi-Document Summarization  task, a set of documents and a query are given where the goal is to generate a summary from these documents based on the given query. However, one major challenge for this task is the lack of availability of labeled training datasets. To overcome this issue, in this paper, we propose a novel weakly supervised learning approach via utilizing distant supervision. In particular, we use datasets similar to the target dataset as the training data where we leverage pre-trained sentence similarity models to generate the weak reference summary of each individual document in a document set from the multi-document gold reference summaries. Then, we iteratively train our summarization model on each single-document to alleviate the computational complexity issue that occurs while training neural summarization models in multiple documents  at once. Experimental results in Document Understanding Conferences\footnote{https://duc.nist.gov/}  datasets show that our proposed approach sets a new state-of-the-art result in terms of various evaluation metrics.",202
" One ultimate goal of language modelling is to construct a model like human, to grasp general, flexible and robust meaning in language. One reflection of obtaining such model is be able to master new tasks or domains on same task quickly. However, NLU models have been building from specific task on given data domain but fail when dealing with out-of-domain data or performing on a new task. To combat this issue, several research areas in transfer learning including domain adaptation, cross lingual learning, multi-task learning and sequential transfer learning have been developed to extend model handling on multiple tasks. However, transfer learning tends to favor high-resources tasks if not trained carefully, and it is also computationally expensive .  Meta learning algorithm tries to solve this problem by training model in a variety of tasks which equip the model the ability to adapt to new tasks with only a few samples.  In our case, we adopt the idea of model-agnostic meta learning  which is an optimization method of meta learning that directly optimized the model by constructing an useful initial representation that could be efficiently trained to perform well on various tasks . However, in an continual learning where data comes into the model sequentially, there is still a potential problem of catastrophic forgetting where a model trained with new tasks would start to perform worse on previous tasks. The two objectives of designing a continual learning architecture are to accelerate future learning where it exploits existing knowledge of a task quickly together with general knowledge from previous tasks to learn prediction on new samples and to avoid interference in previous tasks by updates from new tasks. .   % new In this paper, we utilize algorithm derived from Jave and White \shortcite{MLRCL:19} which applies Meta-Learning under continual learning. Our objective is to apply this framework in NLP field, specifically on NLU tasks. By taking advantage of this model-agnostic approach, Meta-Learning under continual learning should be applicable on any language model that is optimized by gradient-based methods. We compare our results with Duo et al \shortcite{dou:19} which applies meta-learning on Glue tasks, our MAML-Rep shows comparable results. We hope to bring new research direction in NLP fields focusing on such method. The implementation of our code can be found at \url{https://github.com/lexili24/NLUProject}.  % old % This paper aims to develop a framework that incorporate meta learning under the continual learning framework. Hypothetically, our approach is efficient in training by relying on low-resources on various tasks adapted from meta learning characteristics. By training a meta learner under continual learning framework, our model should have consistent results on various tasks with little catastrophic forgetting and learning general representation for all tasks. Finally, our approach is model agnostic, and could essentially apply on any existing language models as long as the model can be optimized by gradient descent. Moreover, our method can be put into the framework of some other continual learning techniques like GEM. The implementation of our code can be found at \url{https://github.com/lexili24/NLUProject}.   
"," Neural network has been recognized with its accomplishments on tackling various natural language understanding  tasks. Methods have been developed to train a robust model to handle multiple tasks to gain a general representation of text. In this paper, we implement the model-agnostic meta-learning  and Online aware Meta-learning  meta-objective under the continual framework for NLU tasks proposed by Javed and White\shortcite{MLRCL:19}. We validate our methods on selected SuperGLUE \shortcite{superglue:19}  and GLUE benchmark \shortcite{glue:19}.",203
"  	 	%  	% % final paper: en-us version  	% 	  % space normally used by the marker 	This work is licensed under a Creative Commons  	Attribution 4.0 International License. 	License details: 	\url{http://creativecommons.org/licenses/by/4.0/} }   Neural Machine Translation  adopts the encoder-decoder paradigm to model the entire translation process . Specifically, the encoder finds a multi-layer representation of the source sentence, and the decoder queries the topmost encoding representation to produce the target sentence through a cross-attention mechanism . However, such over-reliance on the topmost encoding layer is problematic in two aspects:  Prone to over-fitting, especially when the encoder is under-trained, such as in low-resource tasks ;  It cannot make full use of representations extracted from lower encoder layers, which are syntactically and semantically complementary to higher layers  .   Researchers have proposed many methods to make the model aware of various encoder layers besides the topmost to mitigate this issue. Almost all of them resort to the adjustment of network structure, which can be further divided into two categories. The first is to merge the feature representations extracted by distinct encoder layers before being fed to the decoder . The differences between them lie in the design of the merge function: through self-attention , recurrent neural network , or tree-like hierarchical merge . Moreover, the second makes each decoder layer explicitly align to a parallel encoder layer  or all encoder layers .  However, the above methods either complicate the original model  or limit the model's flexibility, such as requiring the number of the encoder layers to be the same as the decoder layers .   Instead, in this work, we propose layer-wise multi-view learning to address this problem from the perspective of model training, without changing the model structure. Our method's highlight is that only the training process is concerned, while the inference speed is guaranteed to be the same as that of the standard model. The core idea is that we regard the off-the-shelf output of each encoding layer as a view for the input sentence. Therefore, it is straightforward and cheap to construct multiple views during a standard layer-by-layer encoding process.  Further, in addition to the output of the topmost encoder layer used in standard models , we also incorporate an intermediate encoder layer as the auxiliary view. We feed the two views to a partially shared decoder for independent predictions. An additional regularization loss based on prediction consistency between views is used to encourage the auxiliary view to mimic the primary view. Thanks to the co-training on the two views, the gradients during back-propagation can simultaneously flow into the two views, which implicitly realizes the knowledge transfer.  Extensive experimental results on five translation tasks  show that our method can stably outperform multiple baseline models . In particular, we have achieved new state-of-the-art results of 10.8 BLEU on KoEn and 36.23 BLEU on IWSLT'14 DeEn. Further analysis shows that our method's success lies in the robustness to encoding representations and dark knowledge  provided by consistency regularization.  \iffalse Our contributions are threefold:  \fi   
","   Traditional neural machine translation is limited to the topmost encoder layer's context representation and cannot directly perceive the lower encoder layers. Existing solutions usually rely on the adjustment of network architecture, making the calculation more complicated or introducing additional structural restrictions. In this work, we propose layer-wise multi-view learning to solve this problem, circumventing the necessity to change the model structure.    We regard each encoder layer's off-the-shelf output, a by-product in layer-by-layer encoding, as the redundant view for the input sentence.   In this way, in addition to the topmost encoder layer , we also incorporate an intermediate encoder layer as the auxiliary view.    We feed the two views to a partially shared decoder to maintain independent prediction.     Consistency regularization based on KL divergence is used to encourage the two views to learn from each other.   Extensive experimental results on five translation tasks show that our approach yields stable improvements over multiple strong baselines. As another bonus, our method is agnostic to network architectures and can maintain the same inference speed as the original model.",204
" % . } % Emotion analysis is now an established research area which finds application in a variety of different fields, including social media analysis \cite[i.a.]{Purver2012,Wang2012b,Mohammad2017,Ying2019}, opinion mining \cite[i.a.]{Choi2006}, and computational literary studies \cite[i.a.]{Ovesdotter2005,Kimfanfic2019,Haider2020,Zehe2020}. The most prominent task in emotion analysis is emotion categorization, where text receives assignments from a predefined emotion inventory, such as the fundamental emotions of \fear, \anger, \joy, \anticipation, \trust, \surprise, \disgust, and \sadness which follow theories by  or . Other tasks include the recognition of affect values, namely valence or arousal  or analyses of event appraisal .  More recently, categorization  tasks have been complemented by more fine-grained analyses, namely emotion stimulus detection and role labeling, to detect which words denote the experiencer of an emotion, the emotion cue description, or the target of an emotion. These efforts lead to computational approaches of detecting stimulus clauses  and emotion role labeling and sequence labeling , with different advantages and disadvantages we discuss in .  Further, this work led to a rich set of corpora with annotations of different subsets of roles. An example of a sentence annotated with semantic role labels for emotion is ``\experiencer{John} \cue{hates} \target{cars} because they \stimulus{pollute the   environment}.'' A number of English-language resources are available:  manually construct a dataset following FrameNet's emotion predicate and annotate the stimulus as its core argument.   annotate Tweets for emotion cue phrases, emotion targets, and the emotion stimulus. In our previous work  we publish news headlines annotated with the roles of emotion experiencer, cue, target, and stimulus.  annotate sentence triples taken from literature for the same roles.  A popular benchmark for emotion stimulus detection is the Mandarin corpus by .  annotate English and Mandarin texts in a comparable way on the clause level .  In this paper, we utilize role annotations to understand their influence on emotion classification. We evaluate which of the roles' contents enable an emotion classifier to infer the emotions. It is reasonable to assume that the roles' content carries different kinds of information regarding the emotion: One particular experiencer present in a corpus might always feel the same emotion; hence, be prone to a bias the model could pick up on. The target or stimulus might be independent of the experiencer and be sufficient to infer the emotion.  The presence of a target might limit the set of emotions that can be triggered.  Finally, as some of the corpora contain cue annotations, we assume that these are the most helpful to decide on the expressed emotion, as they typically have explicit references towards concrete emotion names.  
","   Emotion recognition is predominantly formulated as text classification in   which textual units are assigned to an emotion from a predefined inventory   .   More recently, semantic role labeling approaches have been developed to   extract structures from the text to answer questions like: ``who is   described to feel the emotion?'' , ``what causes this   emotion?'' , and at   which entity is it directed?'' . Though it has been shown that   jointly modeling stimulus and emotion category   prediction is beneficial for both subtasks, it remains unclear which of   these semantic roles enables a classifier to infer the emotion. Is it the   experiencer, because the identity of a person is biased towards a   particular emotion ? Is it a particular target    or a stimulus ? We   answer these questions by training emotion classification models on five   available datasets annotated with at least one semantic role by masking the   fillers of these roles in the text in a controlled manner and find that   across multiple corpora, stimuli and targets carry emotion information,   while the experiencer might be considered a confounder.  Further, we   analyze if informing the model about the position of the role improves the   classification decision. Particularly on literature corpora we find that   the role information improves the emotion classification.",205
" In recent years, the best results for coreference resolution of English have been obtained with end-to-end neural models~. However for Dutch, the existing systems are still using either a  rule-based~ or a machine learning approach~. The rule-based system dutchcoref~ outperformed previous systems on two existing datasets and also presented a corpus and evaluation of literary novels .  In this paper we compare this rule-based system to an end-to-end neural coreference resolution system: e2e-Dutch. This system is a variant of \citet{lee2018higher} with BERT token representations. We evaluate and compare the performance of e2e-Dutch to dutchcoref on two different datasets:  the SoNaR-1 corpus , a genre-balanced corpus of 1 million words, and  the RiddleCoref corpus of contemporary novels . This provides insights into  the relative strengths of a neural system versus a rule-based system for Dutch coreference, and  the effect of domain differences .  The two datasets we consider vary greatly in terms of overall size and length of the individual documents; the training subset of RiddleCoref contains only 23 documents  compared to 581 documents for SoNaR-1. However, the average number of sentences per document is higher for RiddleCoref than for SoNaR-1 .  We also conduct an error analysis for both of the systems to examine the types of errors that the systems make.   
","     We evaluate a rule-based \citep{lee2013deterministic}     and neural \citep{lee2018higher} coreference system on Dutch datasets of     two domains: literary novels and news/Wikipedia text.     The results provide insight into the relative strengths of data-driven and     knowledge-driven systems, as well as the influence of domain, document     length, and annotation schemes.     The neural system performs best on news/Wikipedia text,     while the rule-based system performs best on literature.     The neural system shows weaknesses with limited training data and long     documents, while the rule-based system is affected by annotation     differences. The code and models used in this paper are available at     \url{https://github.com/andreasvc/crac2020}",206
"  A relational triple consists of two entities connected by a semantic relation, which is in the form of . The extraction of relational triples from unstructured raw texts is a key technology for automatic knowledge graph construction, which has received growing interest in recent years.  There have been several studies addressing technical solutions for relational triple extraction. Early researches, such as \citet{zelenko2003kernel,chan2011exploiting}, employ a pipeline manner to extract both of entities and relations, where entities are recognized first and then the relation between the extracted entities is predicted. Such a pipeline approach ignores the relevance of entity identification and relation prediction  and tends to suffer from the error propagation problem.  %    To model cross-task dependencies explicitly and prevent error propagation in the pipeline approach, subsequent studies propose joint entity and relation extraction. These studies can be roughly categorized into three main paradigms. The first stream of work, such as \citet{miwa2016end,gupta2016table,zhang2017end}, treats joint entity and relation extraction task as an end-to-end table filling problem. Although these methods represent entities and relations with shared parameters in a single model, they extract the entities and relations separately and produce redundant information . The second stream of work, such as \citet{zheng2017joint,dai2019joint,wei-etal-2020-novel}, transforms joint entity and relation extraction into sequence labeling. To do this, human experts need to design a complex tagging schema. The last stream of work, including \citet{zeng2018extracting,zeng2019learning,nayak2019ptrnetdecoding,zeng2020copymtl}, is driven by the sequence-to-sequence  model  to generate relational triples directly, which is a flexible framework to handle overlapping triples and does not require the substantial effort of human experts.  We follow the seq2seq based models for joint entity and relation extraction. Despite the success of existing seq2seq based models, they are still limited by the autoregressive decoder and the cross-entropy loss. The reasons are as follows: the relational triples contained in a sentence have no intrinsic order in essence. However, in order to adapt the autoregressive decoder, whose output is a sequence,  the unordered target triples must be sorted in a certain order during the training phase. Meanwhile, cross-entropy is a permutation-sensitive loss function, where a penalty is incurred for every triple that is predicted out of the position. Consequently, current seq2seq base models not only need to learn how to generate triples, but also are required to consider the extraction order of multiple triples.   % consists of three parts  featured by transformers with non-autoregressive parallel decoding and the bipartite matching loss.  In detail, there are three parts in the proposed set prediction networks :  to avoid introducing the order of triplets  % restoring to the original form of this task without considering the order of multiple triples In this work, we formulate the joint entity and relation extraction task as a set prediction problem, avoiding considering the order of multiple triples. In order to solve the set prediction problem, we propose an end-to-end network featured by transformers with non-autoregressive parallel decoding and bipartite matching loss. In detail, there are three parts in the proposed set prediction networks : a sentence encoder, a set generator, and a set based loss function. First of all, we adopt the BERT model  as the encoder to represent the sentence. Then, since an autoregressive decoder must generate items one by one in order, such a decoder is not suitable for generating unordered sets. In contrast, we leverage the transformer-based non-autoregressive decoder  as the set generator, which can predict all triples at once and avoid sorting triples. Finally, in order to assign a predicted triple to a unique ground truth triple, we propose bipartite matching loss function inspired by the assigning problem in operation research . Compared with  cross-entropy loss  that highly penalizes small shifts in  triple order, the proposed loss function is invariant to any permutation of predictions; thus it is suitable for evaluating the difference between ground truth set and prediction set.  % To summarize, our contributions are as follows: In a nutshell, our main contributions are: % the main contributions of our work are as follows:   % the conjunction of the bipartite matching loss and transformers with %  parallel decoding  % Our work build on prior work in several domains:relation extraction, non-autoregressive model, andbipartite matching losses for set prediction. % Relation Extraction.   Non-autoregressive Model.   
"," The joint entity and relation extraction task aims to extract all relational triples from a sentence. In essence, the relational triples contained in a sentence are unordered. However, previous seq2seq based models require to convert the set of triples into a sequence in the training phase. To break this bottleneck, we treat joint entity and relation extraction as a direct set prediction problem, so that the extraction model can get rid of the burden of predicting the order of multiple triples. To solve this set prediction problem, we propose networks featured by transformers with non-autoregressive parallel decoding. Unlike autoregressive approaches that generate triples one by one in a certain order, the proposed networks directly output the final set of triples in one shot. Furthermore, we also design a set-based loss that forces unique predictions via bipartite matching. Compared with cross-entropy loss that highly penalizes small shifts in triple order, the proposed bipartite matching loss is invariant to any permutation of predictions; thus, it can provide the proposed networks with a more accurate training signal by ignoring triple order and focusing on relation types and entities. Experiments on two benchmark datasets show that our proposed model significantly outperforms current state-of-the-art methods. Training code and trained models will be available at \url{http://github.com/DianboWork/SPN4RE}.",207
" Zero-shot translation has first been introduced by \citet{firat-etal-2016-zero} and refers to the ability of a multilingual NMT model to translate between all its source and target languages, even those pairs for which no parallel data was seen in training. In the simplest setting, all parameters in the network are shared between the different languages and the translation is guided only by special tags to indicate the desired output language .  While this capability is attractive because it is an alternative to building  dedicated translation systems to serve  languages, performance on zero-shot pairs tends to lag behind pivot translation. Recent papers, such as \citet{Arivazhagan2019}, \citet{Gu2019} and \citet{Zhang2020}, have suggested training techniques to improve the generalization to unseen language pairs, but performance varies considerably across settings.  In this paper, we examine in detail the behavior of the multilingual model proposed by \citet{Johnson2017} on zero-shot translation directions. Our experiments show the following:     Overall, we observe improvements of 8.1 BLEU  on 6 zero-shot directions with simple changes to the multilingual training setup.  
"," Zero-shot neural machine translation is an attractive goal because of the high cost of obtaining data and building translation systems for new translation directions. However, previous papers have reported mixed success in zero-shot translation. It is hard to predict in which settings it will be effective, and what limits performance compared to a fully supervised system. In this paper, we investigate zero-shot performance of a multilingual EN$\leftrightarrow$\{FR,CS,DE,FI\} system trained on WMT data. We find that zero-shot performance is highly unstable and can vary by more than 6 BLEU between training runs, making it difficult to reliably track improvements. We observe a bias towards copying the source in zero-shot translation, and investigate how the choice of subword segmentation affects this bias. We find that language-specific subword segmentation results in less subword copying at training time, and leads to better zero-shot performance compared to jointly trained segmentation. A recent trend in multilingual models is to not train on parallel data between all language pairs, but have a single bridge language, e.g.\ English. We find that this negatively affects zero-shot translation and leads to a failure mode where the model ignores the language tag and instead produces English output in zero-shot directions.  We show that this bias towards English can be effectively reduced with even a small amount of parallel data in some of the non-English pairs.",208
"  Entrainment is a well-known psycholinguistic phenomenon causing people to adapt to conversation partners so as to become more similar. It affects many linguistic features including phonetics , lexical choice , syntax , and prosody . Importantly, it correlates with interesting aspects of the conversation such as task success, liking, and even rapport with a robot .  The researchers cited above employed various means to measure entrainment, such as correlations, models of conditional probabilities, comparisons of distributions, and perceived similarity. Recently, \citet{Nasir2018} proposed the first neural entrainment measure. Our work builds on theirs by addressing a challenge critical to measuring entrainment: accounting for consistency.   Entrainment is defined as an active, though unconscious, adaptation of a speaker towards their partner. In practice, however, the static similarity or correlation between two speakers is often measured. Thus, even two speakers whose vocal characteristics were initially similar are perceived to have entrained, although no adaptation has taken place. Alternatively, when Speaker B entrains to Speaker A, both speakers are perceived to have entrained, without adaptation from Speaker A. We apply neural methods proposed by \citet{Pryzant2018} to explicitly deconfound consistency, the tendency to adhere to one's own vocal style, from entrainment, the tendency to adapt to one's partner. We argue that entrainment measures that do not control for consistency overestimate the degree of entrainment in a conversation.  Section  explains the data and features that we use to train our networks, which are described in Section . Section  introduces two experiments to validate our methods whose results are discussed, lastly, in Section .  
","   Human interlocutors tend to engage in adaptive behavior known as entrainment to become more similar to each other. Isolating the effect of consistency, i.e., speakers adhering to their individual styles, is a critical part of the analysis of entrainment. We propose to treat speakers' initial vocal features as confounds for the prediction of subsequent outputs. Using two existing neural approaches to deconfounding, we define new measures of entrainment that control for consistency. These successfully discriminate real interactions from fake ones. Interestingly, our stricter methods correlate with social variables in opposite direction from previous measures that do not account for consistency. These results demonstrate the advantages of using neural networks to model entrainment, and raise questions regarding how to interpret prior associations of conversation quality with entrainment measures that do not account for consistency.",209
"  The proliferation of online hate speech has become prevalent in recent times. Numerous social media outlets and the computational social science community are looking at various automated techniques to detect and classify hate speech. However, most models, nascent in nature, have significant limitations due to the complexity of the problem. Primarily, the lack of a reliable baseline coupled with an evolving vocabulary of hateful content makes this a particularly challenging issue. For instance, many studies have classified this problem as a binary classification task, but this fails to address the subtleties of hate speech, such as direct  vs. indirect  hate speech. These binary classification models also fail to identify different types of hate speech like racism, sexism, antisemitism, etc. or their varying degrees. Another key obstacle that plagues these binary models is their inability to distinguish between general offensive language and hate speech. A third issue that arises in designing automated approaches is class imbalance---hate speech is usually a small percentage of the overall data---and the need to adequately upsample hate observations without model overfitting.  In our work, inspired by the recent successes in developing multi-class hate speech models that separate hate speech from offensive content, we propose DeL-haTE, an ensemble of tunable deep learning models that leverages CNN and GRU layers. The CNN layer extracts higher-order features from the word embedding matrix that then inform the GRU layer, which extracts informative features from the sequence of words. These features are utilized for automatic detection of hate speech on social media. Our novelty lies in using a tuning procedure to adapt the model to individual dataset characteristics.   %Issues particular to developing hate speech detection models %	- Class imbalance issue %		- Hate speech is a minute portion of the overall content on social media both generally and in published datasets %	- How to adequately upsample hate observations for training without leading to model overfitting? % %	- We, like others, utilize a downsampling approach during training to ensure a class-balanced dataset passes through the model at each epoch %	- We combine this with an early stopping procedure that utilizes a validation dataset and saves the model state at the epoch with minimal validation loss % %	- These procedures, and other factors, lead to variability in resultant models   %To maintain the necessity of downsampling during training while mitigating the problems of overfitting and variability, we develop an ensemble approach for hate speech classification, extending the CNN-RNN-FC model topology that has been shown to be successful for hate speech classification.   Our major contributions can be summarized by answering the following questions.   	 \end{enumerate}  Summary of Results: Our best ensemble on the HON dataset achieves a 65\% F1 Macro and an 83\% hate recall, surpassing the performance on the HON dataset of current state of the art models by 33\%. We show that the ensemble models outperform individual models by an average of 5\% hate recall and 8\% F1 macro across all datasets. When applied to unlabeled Gab data, tuning improved the pretrained models by an average of 12\%, with the best tuned ensemble models achieving 57\% hate recall. Our model trained using weak supervision achieved a 67\% hate recall on posts from Gab.  %\sidd{We show that the ensemble models outperform their individual components by an average of 5\% hate recall and 8\% F1 macro. % %We then examine the generalizability of our model framework to novel data from Gab, experimenting with both transfer learning and weak supervision %	- Transfer learning using a small manually labeled set of posts improved the hate recall ensembles pre-trained on the HON and OLID datasets by 10\% on the Gab data. % %	- We hypothesized that integrating the labeling of the HON and OLID datasets and combining them would lead to better generalizability for our model framework by increasing both the size and diversity of training examples %		This was confirmed by our experiments with transfer learning as the combined ensembles outperformed the single dataset models on Gab data by an average 8\% Hate recall over the HON models and 5\% on F1 Macro.}  
"," %This document is a model and instructions for \LaTeX. %This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes,  %or Math in Paper Title or Abstract. Online hate speech on social media has become a fast-growing problem in recent times. Nefarious groups have developed large content delivery networks across several mainstream  and fringe outlets  to deliver cascades of hate messages directed both at individuals and communities. Thus addressing these issues has become a top priority for large-scale social media outlets. Three key challenges in automated detection and classification of hateful content are the lack of clearly labeled data, evolving vocabulary and lexicon - hashtags, emojis, etc - and the lack of baseline models for fringe outlets such as Gab. In this work, we propose a novel framework with three major contributions.  We engineer an ensemble of deep learning models that combines the strengths of state-of-the-art approaches,  we incorporate a tuning factor into this framework that leverages transfer learning to conduct automated hate speech classification on unlabeled datasets, like Gab, and  we develop a weak supervised learning methodology that allows our framework to train on unlabeled data. Our ensemble models achieve an 83\% hate recall on the HON dataset, surpassing the performance of the state of the art deep models. We demonstrate that weak supervised training in combination with classifier tuning significantly increases model performance on unlabeled data from Gab, achieving a hate recall of 67\%.",210
"  The demand for speech translation systems at meetings and lectures continues to increase. Since the length of complete sentences in such talks can be long and complicated, simultaneous speech translation is required to mimic human interpreters and translate the incoming speech stream from a source language to target language in real time. One challenge for achieving simultaneous speech translation is the development of incremental ASR.  Researchers have been working on speech recognition technology for decades. A number of techniques of real-time ASR exist, especially in the context of statistical ASR with a hidden Markov model  . However, many current state-of-the-art ASR systems rely on attention-based sequence-to-sequence deep learning frameworks . Today's attentional mechanisms are based on a global attention property that requires the computation of a weighted summarization of the entire input sequence generated by the encoder states. This means that the system can only generate text output after receiving the entire input speech sequence. Consequently, utilizing it in situations that require immediate recognition is difficult.  Several studies proposed local attention mechanisms  that limit the area explored by the attention by largely reducing the total training complexity without reducing the latency. For work that enables incremental recognition of speech, Hwang and Sung employed a unidirectional RNN with a CTC acoustic model and a unidirectional RNN language model . To avoid continuous output revision, they also proposed depth-pruning in the beam-search during the output generation. Jaitly et al. proposed a neural transducer framework  that incrementally recognizes the input speech waveforms. The formulation required inferring alignments during training, and they utilized a dynamic programming algorithm to compute ``approximate"" best alignments in each speech segments. Their model is strongly related to a sequence transducer that used connectionist temporal classification  . The improved version of a neural transducer, which has also been discussed , allows the attention mechanism to look back at many previous chunks without introducing additional latency.  However, most ISR models utilize different frameworks and learning algorithms that are more complicated than the standard ASR model. One main reason is because such models need to decide incremental steps and learn the transcription that is aligned with the current short speech segment. In this work, we propose attention-transfer ISR  by the following:     
"," Attention-based sequence-to-sequence automatic speech recognition  requires a significant delay to recognize long utterances because the output is generated after receiving entire input sequences. Although several studies recently proposed sequence mechanisms for incremental speech recognition , using different frameworks and learning algorithms is more complicated than the standard ASR model. One main reason is because the model needs to decide the incremental steps and learn the transcription that aligns with the current short speech segment. In this work, we investigate whether it is possible to employ the original architecture of attention-based ASR for ISR tasks by treating a full-utterance ASR as the teacher model and the ISR as the student model. We design an alternative student network that, instead of using a thinner or a shallower model, keeps the original architecture of the teacher model but with shorter sequences . Using attention transfer, the student network learns to mimic the same alignment between the current input short speech segments and the transcription. Our experiments show that by delaying the starting time of recognition process with about 1.7 sec, we can achieve comparable performance to one that needs to wait until the end.",211
"   % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  .     %      % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. }  The following instructions are directed to authors of papers submitted to COLING-2020 or accepted for publication in its proceedings. All authors are required to adhere to these specifications. Authors are required to provide a Portable Document Format  version of their papers. The proceedings are designed for printing on A4   paper.  Authors from countries in which access to word-processing systems is limited should contact the publication co-chairs Fei Liu  and Liang Huang  as soon as possible.  We may make additional instructions available at \url{http://coling2020.org/}. Please check this website regularly.   
","   This document contains the instructions for preparing a paper submitted   to COLING-2020 or accepted for publication in its proceedings. The document itself   conforms to its own specifications, and is therefore an example of   what your manuscript should look like. These instructions should be   used for both papers submitted for review and for final versions of   accepted papers. Authors are asked to conform to all the directions   reported in this document.",212
" In the biomedical domain, there exist several entities, such as genes, chemicals, and diseases, that are closely related to each other. Therefore, extracting the relationships among these entities is critical for biomedical research, particularly in fields such as construction of a knowledge base or drug development. Biomedical text data, including PubMed abstracts, usually contain information about biomedical entities and their relationships with each other. Thus, various natural language processing models, particularly deep learning models, are applied to biomedical text data to extract the relationships among these entities, as a kind of classi閾夸恭ation task.   ChemProt corpus  is the first corpus dataset for chemical--protein  relationship extraction, which has been conducted by BioCreative VI organizers. These organizers annotated all entity offsets of chemical and protein mentions and relationship types between chemicals and proteins . There exist 10 groups of the relationship types, and five of these  were used in the evaluation.   All models for extracting relationships from ChemProt data are designed as classifiers. In a deep learning-based multi-class classifier, the output probability distribution for each class is calculated through the Softmax function. In the training step, the model is trained to maximize the output probability of the correct class. However, some studies reported that the deep learning classifier trained with hard-labeled data  tends to become over-confident . This over-confidence does not directly affect classification performance, but it degrades the reliability of the model. In other words, the output probability of the over-confident model does not indicate how uncertain the input example is, even if its classi閾夸恭ation performance is high. Therefore, several approaches, called ``calibration'' techniques, have been applied to several domains that require high reliability, such as autonomous driving and medical diagnosis  .   In the natural language processing domain, bidirectional encoder representation from transformers   was proposed for a wide-range of language understanding. BERT is a large multi-head attention  model, which was pre-trained with a vast amount of corpus data. This pre-trained model can be easily transfer-learned and can be applied on several downstream tasks  by fine-tuning it. BERT has been used in many domains, including a biomedical field. Nevertheless, it is still important to improve the performance of BERT by applying additional techniques while using the BERT as a backbone architecture.     In this study, we propose a DNN-based approach to improve the performance of chemical--protein relationship extraction, while calibrating the classifier. More precisely, we incorporated two main calibration techniques to BERT  to improve the reliability and performance. Furthermore, we propose a semi-supervised learning workflow using the calibrated model and unlabeled in-domain data. The main contributions of our study are as follows:    
"," The extraction of interactions between chemicals and proteins from several biomedical articles is important in many fields of biomedical research such as drug development and prediction of drug side effects. Several natural language processing methods, including deep neural network  models, have been applied to address this problem. However, these methods were trained with hard-labeled data, which tend to become over-confident, leading to degradation of the model reliability. To estimate the data uncertainty and improve the reliability, ``calibration'' techniques have been applied to deep learning models. In this study, to extract chemical--protein interactions, we propose a DNN-based approach incorporating uncertainty information and calibration techniques. Our model first encodes the input sequence using a pre-trained language-understanding model, following which it is trained using two calibration methods: mixup training and addition of a confidence penalty loss. Finally, the model is re-trained with augmented data that are extracted using the estimated uncertainties. Our approach has achieved state-of-the-art performance with regard to the Biocreative VI ChemProt task, while preserving higher calibration abilities than those of previous approaches. Furthermore, our approach also presents the possibilities of using uncertainty estimation for performance improvement.",213
"  Contemporary deep learning models for language have been shown to learn many aspects of natural language syntax including a number of long-distance dependencies , selectional properties of verbs , representations of incremental syntactic state  and information from which hierarchical structure can be linearly decoded .  These and many other related studies demonstrate an impressive range of human-like linguistic knowledge that is automatically acquired by these models simply from exposure to large quantities of raw text.  However, human-like grammatical abilities include not just rich and detailed linguistic knowledge but the ability to deploy this knowledge in using new words based on minimal exposure .  It remains poorly understood what grammatical generalizations contemporary deep learning models are able to make regarding the behavior of words to which they have minimal exposure. In this work, we assess the syntactic generalization behavior of a contemporary neural network model  on two novel phenomena in English  and address the question of single-shot and few-shot learning, demonstrating that BERT makes robust grammatical generalizations after fine-tuning on minimal examples of a novel token.  We test BERT's few-shot learning capabilities on two phenomena at the syntax-semantics interface: English verbal alternations, and verb/object selectional preferences. In English, verbs can appear in multiple syntactic frames; which frame a verb appears in is governed by its argument structure properties. Often, frames are paired into alternation classes  such that when English speakers hear a novel verb in one frame they can be confident that it can be used in its alternation-class pair. Using the well-attested dative alternation as an example, if a listener hears the sentence ``I daxed the tennis racket to my friend"" they would expect that ``I daxed my friend the tennis racket"" is a grammatical English sentence, meaning approximately the same thing. They would not, however, have such an expectation for ``I daxed my friend for the tennis racket."" In addition, listeners may be attuned to semantic clustering of verbal arguments based on past experience. For instance, following the example above, English speakers may expect dax to take an animate indirect object, and would find examples such as ``I daxed the court the tennis racket"" to be surprising.   We take inspiration for our testing regime from a class of psycholinguistic experiments known as `novel word learning studies', which we adapt to the neural setting. In such experiments subjects are exposed to a novel word in context during a training phase, and assessed for what grammatical generalizations they have learned about the novel word during a later testing phase. Novel word learning experiments have been used to assess human grammatical generalization since \citet{berko1958child}, and have been deployed to assess semantic, as well as syntactic, generalizations . %For example, in \citet{berko1958child}, children were shown a novel creature and told it was a wug. At test time, they were shown a picture featuring two of the creatures and described them as wugs indicating that they had categorized the novel word as a noun, and applied the productive -s pluralization to it.  Children can also learn semantic properties of a word from a single exposure . %Bayesian models of word learning have shown successes in modeling novel word learning abilities , however it is not clear how well neural network models would exhibit these rapid generalizations. Recent work has shown that sequence-to-sequence neural architectures rarely generalize systematically in the way that would be required to match human syntactic generalization behavior , but it is an open question whether we might get different behavior depending on network architecture and training objective. In this work, we replicate the novel word learning paradigm in the neural setting by fine-tuning BERT on tightly-controlled sentences that contain novel verbs and objects, and assessing the model on carefully constructed test sets that reveal what grammatical generalizations it has learned. We find that BERT is able to make proper generalizations for both verbal alternations as well as semantic clustering for verbal arguments after just one or two exposures during training.    
","  Previous studies investigating the syntactic abilities of deep learning models have not targeted the relationship between the strength of the grammatical generalization and the amount of evidence to which the model is exposed during training. We address this issue by deploying a novel word-learning paradigm to test BERT's \cite{devlin2018bert} few-shot learning capabilities for two aspects of English verbs: alternations and classes of selectional preferences. For the former, we fine-tune BERT on a single frame in a verbal-alternation pair and ask whether the model expects the novel verb to occur in its sister frame. For the latter, we fine-tune BERT on an incomplete selectional network of verbal objects and ask whether it expects unattested but plausible verb/object pairs. We find that BERT makes robust grammatical generalizations after just one or two instances of a novel word in fine-tuning. For the verbal alternation tests, we find that the model displays behavior that is consistent with a transitivity bias: verbs seen few times are expected to take direct objects, but verbs seen with direct objects are not expected to occur intransitively. The code for our experiments is available at \url{https://github.com/TristanThrush/few-shot-lm-learning}.",214
"  When Natural Language Processing  systems are deployed in production, and interact with users , there are many potential ways of collecting feedback data or rich interaction logs. For example, one can ask for explicit user ratings, or collect user clicks, or elicit user revisions to get an estimate of how well the deployed system is doing. However, such user interaction logs are primarily used for an one-off assessment of the system, e.g., for spotting critical errors, detecting domain shifts, or identifying the most successful use cases of the system in production. This assessment can then be used to support the decision of keeping or replacing this system in production.   From a machine learning perspective, using interaction logs only for evaluation purposes are lost opportunities for offline reinforcement learning . Logs of user interactions are gold mines for off-policy learning, and they should be put to use, rather than being forgotten after a one-off evaluation purpose.  To move towards the goal of using user interaction logs for learning, we will discuss which challenges have hindered RL from being employed in real-world interaction with users of NLP systems so far.  Concretely, our focus is on sequence-to-sequence learning for NLP applications , such as machine translation, summarization, semantic parsing or dialogue generation for chatbots, since these applications provide the richest interaction with users. For example, many machine translation services provide the option for users to give feedback on the quality of the translation, e.g. by collecting post-edits. Similarly, industrial chatbots can easily collect vast amounts of interaction logs, which can be utilized with offline RL methods.  Recent work by has recognized that the poorly defined realities of real-world systems are hampering the progress of RL in production environments. They address, amongst others, issues such as off-line learning, limited exploration, high-dimensional action spaces, or unspecified reward functions. These challenges are important in RL for control systems or robots grounded in the physical world. However, they severely underestimate the human factor when collecting feedback in systems interacting with humans, e.g. through natural language. In the following, we will thus present challenges that are encountered in user-interactive RL for NLP systems. With this discussion, we aim to  encourage NLP practitioners to leverage their interaction logs through offline RL, and  inspire  RL researchers to steel their algorithms for the challenging applications in NLP.
"," Large volumes of interaction logs can be collected from NLP systems that are deployed in the real world. How can this wealth of information be leveraged? Using such interaction logs in an offline reinforcement learning  setting is a promising approach. However, due to the nature of NLP tasks and the constraints of production systems, a series of challenges arise. We present a concise overview of these challenges and discuss possible solutions.",215
"     In addition to other challenges in multiword expression  processing that were addressed in previous work, such as non-compositionality , discontinuity , and syntactic variability , The PARSEME shared task edition 1.2 has focused on another prominent challenge in detecting MWEs, namely detection of unseen MWEs. The problem with unseen data is common for many NLP tasks. While rule-based and unsupervised ML approaches are less affected by unseen data, supervised ML techniques are often found to be prone to overfitting. In this respect, the introduction of language modelling objectives to be added to different NLP tasks and their effect on generalisation have shown promising results. Further improvements brought by pre-trained language models made them a popular approach to a multitude of NLP tasks. One particular advantage of such models is that they facilitate generalisation beyond task-specific annotations .  MWEs are inherent in all natural languages and distinguishable for their syntactic and semantic idiosyncracies . Since language models are good at capturing syntactic and semantic features, we believe they are a suitable approach for modelling MWEs.    In particular, our system relies on BERT pre-trained language models .  Additionally, we render the system semi-supervised by means of multi-task learning. The most promising feature to be jointly learned with MWEs is dependency parse information . Accordingly, we fine-tune BERT for two different objectives: MWE detection and dependency parsing. MWE learning is done via token classification using a linear layer on top of BERT, and dependency parse trees are learned using dependency tree CRF network .  Our experiments confirm that this joint learning architecture is effective for capturing MWEs in most languages represented in the shared task.~    
"," This paper describes a semi-supervised system that jointly learns verbal multiword expressions  and dependency parse trees as an auxiliary task. The model benefits from pre-trained multilingual BERT.  BERT hidden layers are shared among the two tasks and we introduce an additional linear layer to retrieve VMWE tags. The dependency parse tree prediction is modelled by a linear layer and a bilinear one plus a tree CRF on top of BERT. The system has participated in the open track of the PARSEME shared task 2020 and ranked first in terms of F1-score in identifying unseen VMWEs as well as VMWEs in general, averaged across all $14$ languages.",216
"  % \gn{Title candidate: ``Detecting Hallucinated Content ...'' . I wonder if you could also run your methods over extractive summarization outputs or the true references and see how many hallucinations they detect? Just an idea.} % However, recent studies on abstractive text summarization   % and neural machine translation~ have shown that conditional neural sequence models are prone to hallucinate content that is not faithful to the input text.  This risk of generating unfaithful content impedes the safe deployment of neural sequence generation models~. The first step to building models that do not suffer from these failures is the assessment and identification of such hallucinated outputs. Prior work has shown that standard metrics used for sequence evaluation, such as BLEU scores , ROUGE  and BERTScores , do not correlate well with the faithfulness of model outputs~. They also require reference output text, limiting their applicability to detecting halluciations in a deployed system at run-time. Very recent efforts~ have started to develop automatic metrics to measure the faithfulness of output sequences. These methods use external semantic models, e.g. the question-generation and question-answering systems~ or textual entailment inference models, to score faithfulness tailored for abstract text summarization.  However, these scores do not directly measure the number of hallucinated tokens %In addition, these metrics are often tailored for the evaluation of summaries in abstract text summarization  and only correlate weakly with human judgements.  % \gn{Big question: what is the difference from word-level quality estimation, which has been around for a very long time, since at least: \citet{bach-etal-2011-goodness} and has been covered in many WMT quality estimation shared tasks . This seems more related than the works cited below, and describing why we'd need to do something new over these works would probably be a big question in the minds of anyone familiar with the MT field. Also, would the proposed methods for detecting hallucination do better than SOTA word-level QE models?}  % \gn{Similar motivation: Moreover, they do not distinguish the types of errors in terms of fluency and adequacy: a substitution error referring to a simple morphological variation  is % considered in the same way as a content word substitution changing the meaning of the sentence.~.}  We propose a new task for faithfulness assessment - hallucination detection at the token level, which aims to predict if each token in the machine output is a hallucinated or faithful to the source input.  This task does not use the reference output to assess faithfulness, which offers us the ability to apply it in the online generation scenario where references are not available. Similar to the spirit of our proposed task, word-level quality estimation~ in the machine translation community predicts if tokens are correctly translated based on human post-editing. However, they do not distinguish errors in terms of fluency and adequacy~.  % A substitution error referring to a simple morphological variation  is considered the same as a content word substitution changing the meaning of the sentence.~.  In contrast to estimating the amount of human post-editing work required to fix errors, we specifically focus only on hallucination  errors.  We measure hallucination for two conditional sequence generation tasks -- abstractive summarization and machine translation . For the former, we produce a benchmark dataset from recently released annotations ~. For MT, we carefully design the human assessment guideline and create high-quality annotations. We will also release our human annotated data for future research. To learn token-level hallucination prediction for general conditional sequence generations tasks, we propose a novel method that creates synthetic ``hallucinated"" data and finetunes a pretrained language model~ on it. Without any human annotated supervised training data, we achieve an average F1 of around 0.6 across all the benchmark datasets, setting initial performance levels for this new task. % \cz{\st{We have also computed sentence-level aggregated predictions and achieve significantly higher correlations with human scores than previous methods. Finally, we use our new data to study the effect of pretraining on MT hallucination and show it can actually produce more faithful translations, }} We also show that pretraining on MT can actually produce more faithful translations, confirming recent findings in abstractive summarization~.  Predicting hallucination labels at token-level provides a tool for diagnosing and interpreting model outputs, which allows us to flag potential risks at inference time for previously unseen inputs. On the other hand, the token-level labels also allow for fine-grained controls over the target sequence during learning full translation models.  We show how to use these token-level hallucination labels in two case studies to improve self-training and learning from noisy mined bitext in low-resource MT. In both cases, there can be noise in the target text, either produced by the self-training teacher or mining errors. However, most outputs are only partially hallucinated  and the rest of the output is still useful for training, as we show by introducing different token-level loss truncation schemes. %To further benefit self-training, we filter out the noisy part and also glean useful part of model predictions by applying token-level loss truncation or control of information flows to the target sequence at training time.  Our best methods outperform strong baselines by a large margin both in translation quality and hallucination reduction.            % 
"," Neural sequence models can generate highly fluent sentences but recent studies have also shown that they are also prone to hallucinate additional content not supported by the input, which can cause a lack of trust in the model. To better assess the faithfulness of the machine outputs, we propose a new task to predict whether each token in the output sequence is hallucinated conditioned on the source input, and collect new manually annotated evaluation sets for this task. We also introduce a novel method for learning to model hallucination detection, based on pretrained language models fine tuned on synthetic data that includes automatically inserted hallucinations.   Experiments on machine translation and abstract text summarization demonstrate the effectiveness of our proposed approach -- we obtain an average F1 of around 60 across all the benchmark datasets. Furthermore, we demonstrate how to use the token-level hallucination labels to define a fine-grained loss over the target sequence in the low-resource machine translation and achieve significant improvements over strong baseline methods. We will release our annotated data and code to support future research.",217
"  With rise in social media and e-commerce websites, there is a huge interest in analyzing these networks for tasks like link prediction, recommendation, community detection, etc. Traditionally, this is done by learning finite-dimensional vector embeddings/representations  for nodes in these networks and then used it for downstream tasks. One of the challenges is that the quality of these learned representation decreases if the network has many missing links. This affects its performance in downstream tasks. This can be addressed by using attribute similarity of nodes as connected usually have similar attributes. For example, in citation networks, papers on related works will cite each other, and in social media, people with similar interest follow each other. In real-world graphs, nodes of these networks themselves contain rich textual information  as attributes. So, we need techniques which can exploit this textual information while learning node embeddings. The representation learning of textual networks deals with this problem.   \iffalse While networks are sources of relational information, in many practical scenarios, nodes of networks themselves contain rich information as attributes. When this data is in the form of text  these networks are referred to as textual networks, and representation learning of these networks has several applications in diverse fields from analyzing social media profiles to biomedical networks. One of the challenges in this problem is that the quality of these learned representation decreases if the network has many missing links. This can be addressed by using attribute similarity of nodes as connected usually have similar attributes. For example, in citation networks, papers on related works will cite each other, and in social media, people with similar interest follow each other. So, by exploiting this, one can predict the edges in the network.   The main aim of representation learning of network is to learn embeddings, which are finite-dimensional vector representations for nodes in the graph.   %Representation learning in networks uses the edge/link weights as labels in the objective function to learn embeddings. These are finite-dimensional vector representations for each node in the graph.  In this paper we study this problem for textual networks, where nodes of the networks are equipped with attributes or content in the form of textual information . These learned embeddings can then be used in problems like link prediction, community detection, social network analysis, and so on. One of the challenges in this problem is that the quality of these learned representation decreases if the network has many missing links. This can be addressed by using attribute similarity of nodes as connected usually have similar attributes. For example, in citation networks, papers on related works will cite each other, and in social media, people with similar interest follow each other. So, by exploiting this, one can predict the edges in the network. %For achieving this in representation learning of textual networks, we propose an adversarial framework using textual similarity for discriminator and structural similarity for generator.  \fi  Recent methods for representation learning of textual networks involves learning two embeddings, one for the structure information , and the other for the textual information . The embeddings are learned to be similar for nodes that are connected by an edge. The most challenging task is to learn the combined text and structure embeddings, all the previous approaches  use a joint learning framework by defining a loss function that models the inter-modal similarities between structure and textual information between nodes connected by an edge, in addition to the intra-modal similarities. For example, consider the nodes  and  with their embeddings   and . The similarity between embeddings  and  is used for modelling intra-model similarity in structure information, on the other hand the similarity between  and  is used for intra-model similarity in text information. For inter-model similarity, the similarity between  and  is used for modelling the similarity between structure and text, and vice versa. All these similarities are modelled using skip-gram loss function .   The main disadvantage of these models is that they dependent on edge labels for embedding learning. This will make them unable to learn embeddings of nodes which are not present during the training stage. The only way they can be modelled to learn unseen nodes embeddings is by a mapper function between textual information and structure embeddings on seen nodes and apply it to unseen nodes for getting structure embeddings. This can result in a poor performance in downstream tasks involving unseen nodes as the mapping function cannot fully capture the structural information in the nodes. Recenlty, this issue has been addressed by using variational autoencoder framework on the structure and text embeddings. Although it has achieved better performance than the mapper function-based models, the disadvantage of autoencoder framework is that it limits the information learned in the structure embeddings as it is used for predicting the text features by the decoder.  In this paper, we propose an adversarial model where the generator learns the structure embeddings and between text embedding based discriminator and structure embeddings based generator.  For generator, we use the supervision from edge-connectivity and text embedding similarity to learn the structure embeddings. For discriminator model, text embeddings are made dissimilar for node pair generated by the generator and similar for the node pairs from the graph. This training will make the text similarity from the discriminator to approximate the actual similarity in the network. Through this framework we establish that this model efficiently amalgamate or fuse information from both text and graph as both text and structure embeddings use information from both modality for learning. In addition to this, our proposed adversarial approach can be extended for embedding learning of unseen nodes in the training dataset. This is achieved by directly using discriminator based text-similarity as supervision in a post-training stage. This will help in efficiently learning unseen structure embeddings as it does not restrict the embedding learning by using it to predict the text features like in VHE .  The performance of the model depends upon how well we can exploit the unstructured textual information, so we need a powerful discriminator. To achieve this, we use context-aware embeddings, where a node has different text embedding for each of its edges. We address this problem by proposing a novel technique by combining two context-aware attention mechanism. The first is based on mutual attention  between word embeddings in text across a pair of nodes.  The other is a topological attention mechanism. This uses structure embeddings of a node pairs to attend over text to learn a topology-aware text embedding. It can reduce the adverse effects of trying to make text embeddings similar where the textual information of connected nodes need not match. Because, this model has better representation capacity as it learns similarity through topological and mutual attention.    The following are the main contributions of this paper.  An adversarial technique for attributed network representation learning. Here, in addition to the supervision from training data, a discriminator using text embeddings is used to give supervision to structure embeddings.  A novel text embedding learning technique which uses both mutual and topological attention.  Extensive comparative study on downstream tasks of link prediction and node classification.  Experiments on link prediction on unseen nodes.    \iffalse We have evaluated our proposed method on three datasets Cora, Zhihu, and Hepth for link prediction. We observed that our model performs better than state-of-the-art methods in almost all settings in all three datasets. The performance of our model is especially high in low data regime. In Zhihu dataset, our model show a performance improvement of  over the previous state-of-the-art in the lowest supervision setting. A similar observation was made on the node classification task on Cora dataset, where our adversarial technique achieve state-of-the-art performance. As we mentioned earlier, the main advantage of this model is its ability to the care of representation learning in unseen nodes. We evaluated the quality of these embeddings in link prediction task for edges involving unseen nodes, and ACNE achieves state-of-the-art performance for all settings in all three datasets. On Zhihu dataset, it gave an impressive improvement of   improvement over previous methods in the low-data regime.  \fi  \iffalse  \fi  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
"," \label{section:abstract}  Representation learning of textual networks poses a significant challenge as it involves capturing amalgamated information from two modalities:  underlying network structure, and  node textual attributes. For this, most existing approaches learn embeddings of text and network structure by enforcing embeddings of connected nodes to be similar. Then for achieving a modality fusion they use the similarities between text embedding of a node with the structure embedding of its connected node and vice versa. %Then for achieving modality fusion they model intra-modal similarities involving networks structure and textual attributes of  nodes in an edge.  This implies that these approaches require edge information for learning embeddings and they cannot learn embeddings of unseen nodes. In this paper we propose an approach that achieves both modality fusion and the capability to learn embeddings of unseen nodes. The main feature of our model is that it uses an adversarial mechanism between text embedding based discriminator, and structure embedding based generator to learn efficient representations. Then for learning embeddings of unseen nodes, we use the supervision provided by the text embedding based discriminator. In addition this, we propose a novel architecture for learning text embedding that can combine both mutual attention and topological attention mechanism, which give more flexible text embeddings. Through extensive experiments on real-world datasets, we demonstrate that our model makes substantial gains over several state-of-the-art benchmarks. In comparison with previous state-of-the-art, it gives up to 7\% improvement in performance in predicting links among nodes seen in the training and up to 12\% improvement in performance in predicting links involving nodes not seen in training. Further, in the node classification task, it gives up to 2\% improvement in performance.",218
"  Streaming Automatic Speech Recognition  researches have made their way into our everyday products. Smart speakers can now transcribe utterances in a streaming fashion, allowing users and downstream applications to see instant output in terms of partial transcriptions. There is a growing interest in the community to develop end-to-end  streaming ASR models, because they can transcribe accurately and run compactly on edge devices. Amongst these streaming E2E models, Recurrent Neural Network Transducer  is a candidate for many applications. RNN-T is trained with a loss function that does not enforce on the temporal alignment of the training transcripts and audio. As a result, RNN-T suffers from token emission delays - time from when the token is spoken to when the transcript of the token is emitted. Delayed emissions of tokens adversely affects user experiences and downstream applications such as the end-pointer.   Some existing work tried to mitigate the token emission delays in streaming RNN-Ts. We introduce them in Section. Other works utilized semi-streaming or non-streaming models to predict better token emission time, at the cost of the overall latency of the transcripts. In this work, we propose a novel loss function for streaming RNN-T, and the resultant trained model is called Alignment Restricted RNN-T . It utilizes audio-text alignment information to guide the loss computation. In Section, we show that theoretically, Ar-RNN-T loss function is faster to compute and results in better audio-token alignment. In Section, we empirically compare our proposed method with existing works such as monotonic RNN-T training on two data set: LibriSpeech and voice command. In the results section, Section, we show improvement in training speed and that when used in tandem with an end-pointer, Ar-RNN-T provides an unprecedentedly refined control over the latency-WER trade-offs of RNN-T models.   
"," There is a growing interest in the speech community in developing Recurrent Neural Network Transducer  models for automatic speech recognition  applications. RNN-T is trained with a loss function that does not enforce temporal alignment of the training transcripts and audio. As a result, RNN-T models built with uni-directional long short term memory  encoders tend to wait for longer spans of input audio, before streaming already decoded ASR tokens. In this work, we propose a modification to the RNN-T loss function and develop Alignment Restricted RNN-T  models, which utilize audio-text alignment information to guide the loss computation. We compare the proposed method with existing works, such as monotonic RNN-T, on LibriSpeech and in-house datasets. We show that the Ar-RNN-T loss provides a refined control to navigate the trade-offs between the token emission delays and the Word Error Rate . The Ar-RNN-T models also improve downstream applications such as the ASR End-pointing by guaranteeing token emissions within any given range of latency. Moreover, the Ar-RNN-T loss allows for bigger batch sizes and 4 times higher throughput for our LSTM model architecture, enabling faster training and convergence on GPUs.",219
"  .     %      % % final paper: en-us version      %       % space normally used by the marker     This work is licensed under a Creative Commons      Attribution 4.0 International License.     License details:     \url{http://creativecommons.org/licenses/by/4.0/}. }     
","   Interpretability and explainability of deep neural networks are challenging due to their scale, complexity, and the agreeable notions on which the explaining process rests. Previous work, in particular, has focused on representing internal components of neural networks through human-friendly visuals and concepts. On the other hand, in real life, when making a decision, human tends to rely on similar situations and/or associations in the past. Hence arguably, a promising approach to make the model transparent is to design it in a way such that the model explicitly connects the current sample with the seen ones, and bases its decision on these samples.   Grounded on that principle, we propose in this paper an explainable, evidence-based memory network architecture, which learns to summarize the dataset and extract supporting evidences to make its decision. Our model achieves state-of-the-art performance on two popular question answering datasets . Via further analysis, we show that this model can reliably trace the errors it has made in the validation step to the training instances that might have caused these errors. We believe that this error-tracing capability provides significant benefit in improving dataset quality in many applications.",220
"  % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  . }  Word segmentation is a fundamental NLP analysis problem for written languages with no space delimiters between words such as Chinese and Japanese.  In the age of digital communications, new URLs  and hashtags , which often include strings of concatenated words  are being added every day to a growing set of tokens that an NLP system may need to deal with, and they pose challenges for language and speech applications. For example, a Text-to-Speech  synthesis system will struggle to pronounce these concatenated tokens, since simply applying a grapheme-to-phoneme system out of the box to something like  will usually yield poor results. This suggests the need for a model that can split such tokens into the component words.  So-called ``end-to-end'' neural TTS systems , which learn to map directly from character sequences to speech might seem to hold out the hope of avoiding treating this problem separately. However, the fact that URLs occur relatively rarely in most TTS training data limits the promise of such models on this long-tail problem.   The problem of analyzing URLs does differ in one useful way from more general text normalization problems. For a token such as  in a text, one typically needs to know what context it occurs in in order to know how to read it: is it  or ; see , inter alia. In the case of URLs, these are largely context-independent since the output segmentation is usually unaffected by the surrounding words. Hence the problem can be treated as a standalone one that does not require the system to be trained as part of broader text normalization training.  Our training data comes from  camel case URLs that naturally define the segment boundaries  along with manual corrections for non-trivial boundaries.  We release our training and evaluation data sets to promote research on this problem.  By drawing an analogy with Chinese word segmentation, we cast the URL segmentation problem as a sequence tagging problem. We propose a simple Recurrent Neural Network  based tagger with an encoder and a decoder.   The model trained on the data set has a decent full sequence accuracy  but fails to generalize to more rare words due to the size of the training data. Inspired by the success of pre-training in many NLP tasks , we propose a pre-training recipe for the segmenter. Based on the observation that URLs are often compound entity names and so are knowledge graph entities , we create a large synthetic training data set by concatenating the knowledge graph entity names. We observe 21\% absolute  improvement in sequence accuracy after applying pre-training followed by fine-tuning.  % % File acl2020.tex % %% Based on the style files for ACL 2020, which were %% Based on the style files for ACL 2018, NAACL 2018/19, which were %% Based on the style files for ACL-2015, with some improvements %%  taken from the NAACL-2016 style %% Based on the style files for ACL-2014, which were, in turn, %% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009, %% EACL-2009, IJCNLP-2008... %% Based on the style files for EACL 2006 by  %%e.agirre@ehu.es or Sergi.Balari@uab.es %% and that of ACL 08 by Joakim Nivre and Noah Smith  \pdfoutput=1  \documentclass[11pt,a4paper]{article} \usepackage{coling2020} \usepackage{times} \usepackage{latexsym} \usepackage{graphicx} \usepackage{amsmath} \newcommand{\UrlFont}{\ttfamily\small} \newcommand{\citep}{\cite} \newcommand{\citet}{\newcite} \usepackage{xcolor} \usepackage{multirow} \usepackage{url}  \colingfinalcopy % This is not strictly necessary, and may be commented out, % but it will improve the layout of the manuscript, % and will typically save some space. \usepackage{microtype}  %\aclfinalcopy % Uncomment this line for the final submission %\def\aclpaperid{560} %  Enter the acl Paper ID here  %\setlength\titlebox{5cm} % You can expand the titlebox if you need extra space % to show all the authors. Please do not make the titlebox % smaller than 5cm ; we will check this % in the camera-ready version and ask you to change it back.  \newcommand{\todo}[1]{{\color{red}{#1}}}  \newcommand{\newtext}[1]{{\color{red}{#1}}}  \newcommand\BibTeX{Bib\TeX} \title{Semi-supervised URL Segmentation with Recurrent Neural Networks Pre-trained on Knowledge Graph Entities} \author{Hao Zhang \and Jae Ro \and Richard Sproat \\         Google Research \\          @google.com}} \date{}         
"," Breaking domain names such as \texttt{openresearch} into component words \texttt{open} and \texttt{research} is important for applications like Text-to-Speech synthesis and web search. We link this problem to the classic problem of Chinese word segmentation and show the effectiveness of a tagging model based on Recurrent Neural Networks  using characters as input. To compensate for the lack of training data, we propose a pre-training method on concatenated entity names in a large knowledge database. Pre-training improves the model by 33\% and brings the sequence accuracy to 85\%.",221
" .     %       % final paper: en-us version         % space normally used by the marker      This work is licensed under a Creative Commons       Attribution 4.0 International License.      License details:      \url{http://creativecommons.org/licenses/by/4.0/}. } Discourse parsing is an important upstream task within the area of Natural Language Processing   which has been an active field of research over the last decades. In this work, we focus on discourse representations for the English language, where most research %on the discourse analysis of English language  has been surrounding one of the two main theories behind discourse, the Rhetorical Structure Theory  proposed by  or interpreting discourse according to PDTB . While both theories have their strengths, the application of the RST theory, encoding documents into complete constituency discourse trees , has been shown to have many crucial implications on real world problems. A tree is defined on a set of EDUs , approximately aligning with clause-like sentence fragments, acting as the leaves of the tree. Adjacent EDUs or sub-trees are hierarchically aggregated to form larger  constituents, with internal nodes containing  a nuclearity label, defining the importance of the subtree  in the local context and  a relation label, defining the type of semantic connection between the two subtrees . In this work, we focus on structure and nuclearity prediction, not taking relations into account. Previous research has shown that the use of RST-style discourse parsing as a system component can enhance important tasks, such as sentiment analysis, summarization and text categorization . More recently, it has also been suggested that discourse structures obtained in an RST-style manner can further be complementary to learned contextual embeddings, like the popular  BERT approach . Combining both approaches has shown to support tasks where linguistic information on complete documents is critical, such as argumentation analysis .  Even though discourse parsers appear to enhance the performance on a variety of tasks, the full potential of using more linguistically inspired approaches for downstream applications has not been unleashed yet. The main open challenges of integrating discourse into more NLP downstream tasks and to deliver even greater benefits have been a combination of  discourse parsing being a difficult task itself, with an inherently high degree of ambiguity and uncertainty and  the lack of large-scale annotated datasets, rendering the initial problem more severe, as data-driven approaches cannot be applied to their full potential.  The combination of these two limitations has been one of the main reasons for the limited application of neural discourse parsing for more diverse downstream tasks. While there have been neural discourse parsers proposed , they still cannot consistently %strongly  outperform traditional approaches when applied to the RST-DT dataset, where the amount of training data is arguably insufficient for such data-intensive approaches.  %due to the extra effort to integrate discourse trees into models as well as two major problems, the big breakthrough in the usage of discourse parsing has still not happened.   In this work, we alleviate the restrictions to the effective and efficient use of discourse as mentioned above by introducing a novel approach combining a newly proposed large-scale discourse treebank with our data-driven neural discourse parsing strategy. More specifically, we employ the novel MEGA-DT ``silver-standard"" discourse treebank published by  containing over 250,000 discourse annotated documents from the Yelp'13 sentiment dataset , nearly three orders of magnitude larger than commonly used RST-style annotated discourse treebanks . Given this new dataset with a previously unseen number of full RST-style discourse trees, we revisit the task of neural discourse parsing, which has been previously attempted by   and others with rather limited success. We believe that one reason why previous neural models could not yet consistently outperform more traditional approaches, heavily relying on feature engineering , is the lack of generalisation when using deep learning approaches on the small RST-DT dataset, containing only 385 discourse annotated documents. This makes us believe that using a more advanced neural discourse parser in combination with a large training dataset can lead to significant performance gains. %, but also across datasets, capturing more general discourse phenomena and avoiding potential overfitting on the training corpus. Admittedly, even though MEGA-DT contains a huge number of datapoints to train on, it has been automatically annotated, potentially introducing noise and biases, which can negatively influence the performance of our newly proposed neural discourse parser when solely trained on this dataset. A natural and intuitive approach to make use of the neural discourse parser and both datasets  is to combine them during training, pretraining on the large-scale ``silver-standard"" corpus and subsequently fine-tuning on RST-DT or further human annotated datasets. This way, general discourse structures could be learned from the large-scale treebank and then enhanced with human-annotated trees. With the results shown in this paper strongly suggesting that our new discourse parser can encode discourse more effectively, we hope that our efforts will prompt researchers to develop more linguistically inspired applications based on our discourse parser. % for downstream models in the area of NLP.  Our contributions in this paper are: % %on how to train a neural discourse parser with large scale ``silver-standard"" discourse trees. With this new approach, we  drastically increase the amount of available training data available for discourse parsers is not sufficiently large to train modern, data-driven deep learning approaches for the task, hindering the application of new methodologies and  the shift in domain between the discourse parsers training data and the domain of application deminishes the applicability and performance of generated discourse trees for any domain outside of news , instructions  and a few other domains. 
"," RST-based discourse parsing is an important NLP task with numerous downstream applications, such as summarization, machine translation and opinion mining. In this paper, we demonstrate a simple, yet highly accurate discourse parser, incorporating recent contextual language models. Our parser establishes the new state-of-the-art  performance for predicting structure and nuclearity on two key RST datasets, RST-DT and Instr-DT. We further demonstrate that pretraining our parser on the recently available large-scale ``silver-standard"" discourse treebank MEGA-DT provides even larger performance benefits, suggesting a novel and promising research direction in the field of discourse analysis.",222
"  The last several years have seen a land rush in research on machine reading  comprehension and various dataset have been proposed such as SQuAD1.1, SQuAD2.0, NewsQA and CoQA . Different from the above which are extractive MRC, RACE is a multi-choice MRC dataset  proposed by . RACE was extracted from middle and high school English examinations in China. Figure 1 shows an example passage and two related questions from RACE. The key difference between RACE and previously released machine comprehension datasets is that the answers in RACE often cannot be directly extracted from the passages, as illustrated by the two example questions  in Table . Thus, answering these questions needs inferences.     \end{center}       \end{table}  %   Recently, pretrained language models  such as BERT , RoBERTa , ALBERT  have achieved great success on MMRC tasks. Notably, Megatron-LM  which is a 48 layer BERT with 3.9 billion parameters yields the highest score on the RACE leaderboard in both single and ensemble settings. The key point to model MMRC is: first encode the context, question, options with BERT like LM, then add a matching network on top of BERT to score the options. Generally, the matching network can be various .  proposes an option comparison network  to compare options at word-level to better identify their correlations to help reasoning.  proposes a dual co-matching network  which models the relationship among passage, question and answer options bidirectionally. All these matching networks show promising improvements compared with pretrained language models. One point they have in common is that the answer together with the distractors are jointly considered which we name multi-choice models. We argue that the options can be concerned separately for two reasons, 1) when human works on MMRC problem, they always consider the options one by one and select the one with the highest confidence. 2) MMRC suffers from the data scarcity problem. Multi-choice models are inconvenient to take advantage of other MRC dataset.   In this paper, we propose a single-choice model for MMRC. Our model considers the options separately. The key component of our method is a binary classification network on top of pretrained language models. For each option of a given context and question, we calculate a confidence score. Then we select the one with the highest score as the final answer. In both training and decoding, the right answer and the distractors are modeled independently. Our proposed method gets rid of the multi-choice framework, and can leverage amount of other resources. Taking SQuAD as an example, we can take a context, one of its question and the corresponding answer as a positive instance for our classification with golden label 1. In this way many QA dataset can be used to enhance RACE. Experimental results show that single-choice model performs better than multi-choice models, in addition by transferring knowledge from other QA dataset, our single model achieves 90.7\% and ensemble model achieves 91.4\%, both are the best score on the leaderboard.     
"," Multi-choice Machine Reading Comprehension  aims to select the correct answer from a set of options based on a given passage and question. Due to task specific of MMRC, it is non-trivial to transfer knowledge from other MRC tasks such as SQuAD, Dream. In this paper, we simply reconstruct multi-choice to single-choice by training a binary classification to distinguish whether a certain answer is correct. Then select the option with the highest confidence score. We construct our model upon ALBERT-xxlarge model and estimate it on the RACE dataset. During training, We adopt AutoML strategy to tune better parameters. Experimental results show that the single-choice is better than multi-choice. In addition, by transferring knowledge from other kinds of MRC tasks, our model achieves a new state-of-the-art results in both single and ensemble settings.",223
"  % Images  are another important approach for expressing feelings and emotions in addition to using text in communication. In mobile messaging apps, these images can generally be classified into emojis and stickers. Emoji is a kind of small picture which is already stored in most of the keyboard of the mobile operational systems, \ie iOS or Android. Emojis are pre-designed by the mobile phone vendor  and the number of emoji is limited, and users can not design emoji by themselves. Different with the inflexible emojis, sticker is image or graphicon essentially, which users can draw or modify images as a sticker and upload to the chatting app by themselves. The using of stickers on online chatting usually brings diversity of expressing emotion. Since emojis are sometimes used to help reinforce simple emotions in a text message due to their small size, and their variety is limited. Stickers, on the other hand, can be regarded as an alternative for text messages, which usually include cartoon characters and are of high definition. They can express much more complex and vivid emotion than emojis. Most messaging apps, such as WeChat, Telegram, WhatsApp, and Slack provide convenient ways for users to download stickers for free, or even share self-designed ones. We show a chat window including stickers in Figure.     % Stickers are becoming more and more popular in online chat. First, sending a sticker with a single click is much more convenient than typing text on the 26-letter keyboard of a small mobile phone screen. Second, there are many implicit or strong emotions that are difficult to express in words but can be captured by stickers with vivid facial expressions and body language. However, the large scale use of stickers means that it is not always straightforward to think of the sticker that best expresses one's feeling according to the current chatting context. Users need to recall all the stickers they have collected and selected the appropriate one, which is both difficult and time-consuming.  % Consequently, much research has focused on recommending appropriate emojis to users according to the chatting context. Existing works such as, are mostly based on emoji recommendation, where they predict the probable emoji given the contextual information from multi-turn dialog systems. In contrast, other works recommend emojis based on the text and images posted by a user. As for sticker recommendation, existing works such as and apps like Hike or QQ directly match the text typed by the user to the short text tag assigned to each sticker. However, since there are lots of ways of expressing the same emotion, it is very hard to capture all variants of an utterance as tags.  % To overcome the drawbacks, we propose a sticker response selector  for sticker selection in our early work, where we address the task of sticker response selection in multi-turn dialog. We focus on the two main challenges in this work:  Since existing image recognition methods are mostly built with real-world images, and how to capture the semantic meaning of sticker is challenging.  Understanding multi-turn dialog history information is crucial for sticker recommendation, and jointly modeling the candidate sticker with multi-turn dialog is challenging. % % % % % % Herein, we propose a novel sticker recommendation model, namely sticker response selector , for sticker response selection in multi-turn dialog. Specifically, SRS first learns representations of dialog context history using a self-attention mechanism and learns the sticker representation by a convolutional neural network .  % % % Next, SRS conducts deep matching between the sticker and each utterance and produces the interaction results for every utterance. % % Finally, SRS employs a fusion network which consists of a sub-network fusion RNN and fusion transformer to learn the short and long term dependency of the utterance interaction results. The final matching score is calculated by an interaction function. To evaluate the performance of our model, we propose a large number of multi-turn dialog dataset associated with stickers from one of the popular messaging apps.  Extensive experiments conducted on this dataset show that SRS significantly outperforms the state-of-the-art baseline methods in commonly-used metrics.  % However, the user's sticker selection does not only depend on the matching degree between dialog context and candidate sticker image, but also depends on the user's preference of using sticker. When users decide to use a sticker as their response in multi-turn dialog, they may choose their favorite one from all appropriate stickers as the final response.  % % % We assume that user tends to use the recently used sticker in their dialog history, and the recently-used-sticker can represent the user's preference of sticker selection. An example is shown in Figure. To verify this assumption, we retrieve 10 recently-used-stickers of each user and calculate the proportion of whether the currently used sticker appeared in these 10 stickers. The result shows that 54.09\% of the stickers exist in the 10 recently used sticker set. Hence, we reach to the conclusion that users have strong personal preference when selecting the sticker as their response for the current dialog context. However, in some cases, this also indicates a tendency to re-use stickers, but not necessarily a preference.  % Motivated by this observation, in this work, we take one step further and improve our previously proposed SRS framework with user preference modeling. Overall, we propose a novel sticker recommendation model which considers the user preference, namely Preference Enhanced Sticker Response Selector . Specifically, PESRS first employs a convolutional network to extract features from the candidate stickers. Then, we retrieve the recent user sticker selections then a user preference modeling module is employed to obtain a user preference representation. Next, we conduct the deep matching between the candidate sticker and each utterance as the same as SRS. Finally, we use a gated fusion method to combine the deep matching result and user preference into final sticker prediction.    % The key to the success of PESRS lies in how to design the user preference modeling module, which should not only identify the user's favorite sticker and but also consider the current dialog context. % Motivated by this, we first propose a recurrent neural network  based position-aware sticker modeling module which encodes the recently used stickers in chronological order. Then, we employ a key-value memory network to store these sticker representations as values and the corresponding dialog context as keys. Finally, we use the current dialog context to query the key-value memory and obtain the dynamic user preference of the current dialog context.  % We empirically compare PESRS and SRS on the public dataset\footnote{https://github.com/gsh199449/stickerchat} proposed by our early work. This is a large-scale real-world Chinese multi-turn dialog dataset, which dialog context is multiple text utterances and the response is a sticker image. Experimental results show that on this dataset, our newly proposed PESRS model can significantly outperform the existing methods.  Particularly, PESRS yields 4.8\% and 7.1\% percentage point improvement in terms of  and  compared with our early work SRS. % In addition to the comprehensive evaluation, we also evaluate our proposed user preference memory by a fine-grained analysis. The analysis reveals how the model leverages the user's recent sticker selection history and provides us insights on why they can achieve big improvement over state-of-the-art methods.  This work is a substantial extension of our previous work reported at WWW 2020.  The extension in this article includes the user preference modeling framework for the existing methods, a proposal of a new framework for sticker selection in the multi-turn dialog. Specifically, the contributions of this work include the following:    The rest of the paper is organized as follows: We summarize related work in \S.  \S introduces the data collection method and some statistics of our proposed multi-turn dialog sticker selection dataset. We then formulate our research problem in \S  and elaborate our approach in \S.  \S gives the details of our experimental setup and \S presents the experimental results.  Finally, \S concludes the paper.   % 
","   Stickers with vivid and engaging expressions are becoming increasingly popular in online messaging apps, and some works are dedicated to automatically select sticker response by matching the stickers image with previous utterances.   However, existing methods usually focus on measuring the matching degree between the dialog context and sticker image, which ignores the user preference of using stickers.   Hence, in this paper, we propose to recommend an appropriate sticker to user based on multi-turn dialog context and sticker using history of user.   Two main challenges are confronted in this task.   One is to model the sticker preference of user based on the previous sticker selection history.   Another challenge is to jointly fuse the user preference and the matching between dialog context and candidate sticker into final prediction making.   To tackle these challenges, we propose a Preference Enhanced Sticker Response Selector  model.   Specifically, PESRS first employs a convolutional based sticker image encoder and a self-attention based multi-turn dialog encoder to obtain the representation of stickers and utterances.   Next, deep interaction network is proposed to conduct deep matching between the sticker and each utterance.   Then, we model the user preference by using the recently selected stickers as input, and use a key-value memory network to store the preference representation.   PESRS then learns the short-term and long-term dependency between all interaction results by a fusion network, and dynamically fuse the user preference representation into the final sticker selection prediction.   Extensive experiments conducted on a large-scale real-world dialog dataset show that our model achieves the state-of-the-art performance for all commonly-used metrics.   Experiments also verify the effectiveness of each component of PESRS.   %",224
"  .}  Neural machine translation  has boosted machine translation significantly in recent years . However, it is still unclear how NMT models work due to the black-box nature of neural networks. Better understandings of NMT models could guide us in improving NMT systems. Currently most of the studies towards understanding NMT models only take into account subword-based  models.  Deeper character-based  models have been shown to perform better than BPE-based models . In this paper, we try to investigate the working mechanism of CHAR models. We explore the ability of CHAR models to learn word senses and morphological inflections and the attention mechanism.   Previous studies have tried to interpret and understand NMT models by interpreting attention weights , using gradients , applying layer-wise relevance propagation , probing classification tasks , and more intrinsic analysis .  However, only  have probed character-based representations.  have only explored character-aware word-level representations, while we investigate fully character-level representations, which are also studied in . We apply more composition methods to explore how CHAR models learn linguistic knowledge and how attention extracts features directly from characters.   Probing classification tasks  have emerged as a popular method to interpret the internal representations from neural networks. Given a probing classifier, the input is usually the representation of a word and the output is the corresponding linguistic tag.  CHAR models pose new challenges for interpretability, and we investigate whether we can probe CHAR models in a way similar to word-based models. In addition, can we extract word sense and morphological information about the full word from individual hidden states, or is this information distributed across multiple states? This has implications for interpreting neural CHAR models, but can also inform novel architectures, such as sparse attention mechanisms.  Thus we first investigate the ability of CHAR models to learn word senses and morphology in Section .  We apply different methods to compose information from characters and demonstrate that the word-level information is distributed over all the characters but characters at different positions play different roles in learning linguistic knowledge.  We also explore the effect of encoder depth to answer why CHAR models outperform BPE-based models only when they have the settings with deeper encoder. The probing results show that CHAR models need more layers to learn word senses.  Then in Section , we move on to explore the attention mechanism. The distribution pattern shows that separators attract much more attention compared to other characters.  To study the effect of enforcing characters to capture the full word-level information, we investigate a sparse attention mechanism, i.e. a model that only attends to separators, which can be viewed as a word-level attention. The BLEU score drops 1.2 points when we apply the word-level sparse attention. This implies that only attending to separators by a single attention head is workable but not enough to extract all the necessary information.   The main findings are summarized as follows:     
"," Recent work has shown that deeper character-based neural machine translation  models can outperform subword-based models. However, it is still unclear what makes deeper character-based models successful. In this paper, we conduct an investigation into pure character-based models in the case of translating Finnish into English, including exploring the ability to learn word senses and morphological inflections and the attention mechanism. We demonstrate that word-level information is distributed over the entire character sequence rather than over a single character, and characters at different positions play different roles in learning linguistic knowledge. In addition, character-based models need more layers to encode word senses which explains why only deeper models outperform subword-based models. The attention distribution pattern shows that separators attract a lot of attention and we explore a sparse word-level attention to enforce character hidden states to capture the full word-level information. Experimental results show that the word-level attention with a single head results in 1.2 BLEU points drop.",225
"  A prerequisite relation is a pedagogical relation that indicates the order in which concepts can be presented to learners. The relation can be used to guide the presentation sequence of topics and subjects during the design of academic programs, lectures, and curricula or instructional materials. %such as textbooks and study guides.   In this work, we present our systems to automatically detect prerequisite relations for Italian language in the context of the PRELEARN shared task  at EVALITA  2020 . %.  The evaluation of submissions considers:  in-domain and cross-domain scenarios defined by either the inclusion  or exclusion  of the target domain in the training set. The four domains are 'data mining' , 'geometry' , 'precalculus' , and 'physics' .  the type of resources  used to train the model -- raw text VS. structured information.  % four domains, namely 'data mining', 'geometry', 'precalculus' and 'physics'.    % PRELEARN participants had to submit their systems in a per-domain evaluation, considering in-domain and cross-domain scenarios,  % as well as discriminate between the kind of resources the models used, namely raw text such as distributional textual corpora, and structured information such as knowledge bases. % Additionally, the difference between in-domain and cross-domain lies in the inclusion or exclusion of the target domain in the training set.  The combination of these settings defined the four PRELEARN subtasks.  Formally, a prerequisite relation exists between two concepts if one has to be known beforehand in order to understand the other.  For the PRELEARN task, given a pair of concepts, the relation exists only if the latter concept is a prerequisite for the former.  Therefore, the task is a binary classification task.          We approach the problem from two perspectives: handcrafted features based on lexical complexity and pre-trained embeddings. We employed static embeddings from Wikipedia and Wikidata, and contextual embeddings from Italian-BERT model.  
",   English.   We present our systems and findings for the prerequisite relation learning task  at EVALITA 2020. The task aims to classify whether a pair of concepts hold a prerequisite relation or not. We model the problem using handcrafted features and embedding representations for in-domain and cross-domain scenarios.  Our submissions ranked first place in both scenarios with average F1 score of $0.887$ and $0.690$ respectively across domains on the test sets. We made our code freely available\footnote{\url{https://github.com/ajason08/EVALITA2020_PRELEARN}\label{code}}.,226
" Task-oriented dialog systems are commonplace in automated systems that interact with end users, including digital assistants, technical support agents, and various website navigation helpers. An essential part in any task-oriented dialog system is natural language generation , which consumes data, typically fed in the form of a dialog act, and converts it into natural language output to be served to the end user. The natural language response of the NLG component should 1) contain all essential information, 2) be contextualized around the user request, and 3) be natural sounding. Such a system requires consideration for content planning, correctness, grammaticality, and naturalness.  NLG systems employed in commercial settings are typically based on template-based text generation techniques . In these, humans author a minimal set of responses templates with placeholder slot values. These slots are later filled at runtime, with the dialog input. Although template-based NLG modules are appealing due to their deterministic nature,  inherent correctness, and low latency, they have major drawbacks: First, separate templates need to be authored for different response variations; this behavior is unfavorable for scaling. Second, templates authored for a particular domain are commonly not reusable.  Lastly, no matter the complexity of the language instilled into templates, they form a strictly discrete set of responses, and therefore are bound to be limited in their response naturalness.  More recently, advances in neural-network-based  language generation prompted a new direction in NLG research . The process is typically split into two steps:  serialization of input data into a flattened meaning representation , and  using the neural generation model to generate a natural language response conditioned on the MR. The models are trained on data that includes MR, response pairs, and therefore they are able to not only generate desired responses for MRs in their training data, but they are also expected to form coherent responses for novel MRs, owing to the generalization ability of their machine learning  backbone.  However, deploying neural NLG systems in an industry setting is quite challenging. First, it is not trivial to train a model that reliably presents its input data with the high fidelity required from a user-serving dialog system. Second, the models require much high-quality human-annotated data, which is resource intensive. Consequently, data annotation is a major limiting factor for scaling model-based NLG across domains and languages.  In this work, we detail our approach to production-level neural NLG, with a focus on scalability and data efficiency. Adopting the tree-structured MR framework introduced in Balakrishnan et al.~\shortcite{Balakrishnan2019constrainednlg}, which allows better control over generated responses, we train sequence-to-sequence RNN models  that can produce high-fidelity responses. We then employ a multitude of techniques for reducing the amount of  required data, primarily powered by eliminating the ``hidden'' redundancy by grouping data points with similar semantics into buckets. We train models either on the reduced data, or after increasing the size of the dataset using a novel synthetic augmentation technique. We also employ large, pre-trained attention-based language models, fine-tuning them on the same datasets, and then using novel methods to distill their knowledge into smaller sequence-to-sequence models. Further, we train models on data from multiple domains, showing gains over models trained on individual domains when the domains are semantically close together. We conclude with a compiled list of best practices for production-level NLG model development based on our analyses, and we present it as a runbook.  
"," Natural language generation  is a critical component  in conversational systems, owing to its role of formulating a correct and natural text response. Traditionally, NLG components have been deployed using template-based solutions. Although neural network solutions recently developed in the research community have been shown to provide several benefits, deployment of such model-based solutions has been challenging due to high latency, correctness issues, and high data needs.  In this paper, we present approaches that have helped us deploy data-efficient neural solutions for NLG in conversational systems to production.  We describe a family of sampling and modeling techniques to attain production quality with light-weight neural network models using only a fraction of the data that would be necessary otherwise, and show a thorough comparison between each. Our results show that domain complexity dictates the appropriate approach to achieve high data efficiency. Finally, we distill the lessons from our experimental findings into a list of best practices for production-level NLG model development, and present them in a brief runbook. Importantly, the end products of all of the techniques are small sequence-to-sequence models  that we can reliably deploy in production.",227
"  Pre-training has been demonstrated as a highly effective method for boosting the performance of many natural language processing  tasks such as question answering, sentimental analysis, and so on. By training on massive unlabeled text data, pre-trained models are able to learn the contextual representations of input words, which are extremely helpful for accomplishing downstream tasks.  BERT , as one of the most widely used pre-trained models, is trained using two unsupervised tasks, namely, mask language modeling and next sentence prediction. By adding a few layers on top, BERT can be easily adapted into a task-specific model, which is then fine-tuned on the labeled data to achieve optimal performance. Such a practice has been exercised in various NLP scenarios and has achieved many state-of-the-art  results.  The study of integrating BERT into neural machine translation models, which is referred to as BERT-enhanced NMT, has received much research interest. However, exploiting BERT for NMT is not as straightforward as in other NLP tasks. The architecture of a typical NMT model consists of an encoder that transforms the source language words into a hidden representation, and a decoder that predicts the target language words based on the hidden representation. The challenge of exploiting BERT for NMT is twofold. Firstly, NMT models are mostly deep neural networks with a parameter size comparable to or even larger than that of BERT, which makes the combined model hard to optimize. Secondly, since existing NMT models are mostly trained with massive samples, the usual practice of fine-tuning BERT on the labeled corpus can lead to the problem of catastrophic forgetting .     The recently proposed BERT-fused model  uses attention mechanisms to bridge between the NMT model and BERT. For example, they introduce an extra BERT-encoder attention module to fuse the encoder layer with the BERT representation. The outputs of the BERT-encoder attention module and the self-attention module are averaged.  Consider the case exemplified in \cref{fig:compare} , it's more likely that the word change should be interpreted as money rather than other meanings in this context. However, if the training corpus doesn't contain similar expressions, the model can fail in this translation due to the ambiguity. When BERT representations are introduced, the contextual information learned by BERT can be helpful for the translation. Concretely, a BERT-encoder attention module can be used to capture the pre-trained knowledge embedded in the BERT representation that is absent in the self-attention module.   However, we find that averaging their outputs means regarding them as equally important, which can hurt performance under some circumstances. In the above example, only the BERT-encoder attention module provides useful information for interpreting the word change, while the self-attention module offers faulty or noisy information. Combining their outputs directly can result in confusion during translation.  Hence we assert that it's essential to allow the model to decide which information to concentrate on. To this end, we propose to use a joint-attention module to integrate multiple representations that contain different contextual information. As shown in \cref{fig:compare} , the learnable weights of the joint-attention module allow it to assign more attention to the BERT representation in this case. Compared with the BERT-fused model, our method is better at augmenting desired information and hence boosts performance.   Although existing BERT-enhanced NMT models mostly focus on leveraging BERT's last-layer representation, we find that the intermediate layers can contain semantic and contextual information that is absent in the last layer and might help improve translation performance. The dynamic fusion mechanism proposed by \citet{Weng20} allows the Transformer encoder to leverage BERT's intermediate representations. However, their method doesn't work for the decoder at the inference stage because it requires the ground truth as input. This motivates us to explore feasible techniques for generating composite BERT representations that can be used in both the encoder and the decoder.  In this paper, we introduce a BERT-enhanced NMT model called BERT-JAM, which stands for BERT-fused Joint-Attention Model. BERT-JAM is equipped with joint-attention modules that allow the encoder/decoder to selectively concentrate on the BERT representation or the encoder/decoder representation by attending to them simultaneously. Besides, we seek to improve upon the existing BERT-enhanced models by making better use of BERT's intermediate layers. Specifically, we allow each encoder/decoder layer to use a GLU module to transform BERT's intermediate representations into a composite representation used by the joint-attention module.   In order to achieve optimal performance, we train BERT-JAM following a three-phase optimization strategy which progressively unfreezes different components of the model during training.  We show that fine-tuning BERT is a crucial step to unearth the full potential of BERT-JAM, in contrast to the previous claim that fine-tuning BERT offers few gains  for NMT models. Moreover, we study how the BERT-enhanced NMT performance varies with the size of BERT by feeding different BERT models into BERT-JAM, ranging from the most compact BERT with 2 layers and embedding dimension 128 to the standard BERT-base model. This study can be beneficial because it can provide us with a guide on how to adjust the model with minimal performance loss when we have to resort to a smaller model size due to limited computation resources.    We summarize the contributions of this paper as follows:    The rest of this paper is organized as follows.  In \cref{sec:approach}, we introduce our approach to BERT-enhanced NMT where a detailed description of our model will be presented.  The experimental setups are described in \cref{sec:setup}.  In \cref{sec:results}, several experiments are conducted and the results are discussed.  We give a review of related works in \cref{sec:related} and the conclusions are drawn in \cref{sec:conclusion}.   
"," BERT-enhanced neural machine translation  aims at leveraging BERT-encoded representations for translation tasks. A recently proposed approach uses attention mechanisms to fuse Transformer's encoder and decoder layers with BERT's last-layer representation and shows enhanced performance. However, their method doesn't allow for the flexible distribution of attention between the BERT representation and the encoder/decoder representation. In this work, we propose a novel BERT-enhanced NMT model called BERT-JAM which improves upon existing models from two aspects: 1) BERT-JAM uses joint-attention modules to allow the encoder/decoder layers to dynamically allocate attention between different representations, and 2) BERT-JAM allows the encoder/decoder layers to make use of BERT's intermediate representations by composing them using a gated linear unit . We train BERT-JAM with a novel three-phase optimization strategy that progressively unfreezes different components of BERT-JAM. Our experiments show that BERT-JAM achieves SOTA BLEU scores on multiple translation tasks.",228
"   %Natural language is an abstract representation of thoughts and objects which have similar meaning through a community. We use raw sensory information to represent images. However, this is not possible for words, as they have no direct physical interpretation.  There have been many studies in natural language processing  to find suitable word representations  that carry information of a language. Even if finding these word representations can be computationally demanding, this can be advantageous since it is computed only once. These learned representations can be used for various downstream tasks.  Word2Vec  finds word embeddings by predicting a word given its neighborhood  or predicting its neighborhood given the word . Words that are used together have similar word embeddings due to the training strategy. However, these embeddings do not contain word order information and contextual information. ELMo  uses bidirectional LSTM   to predict a word given its context. Since BiLSTM is used for creating embeddings, both left-to-right and right-to-left contexts are implicitly encoded. Transformer  is shown to be more appropriate for training in large datasets due to its self-attention mechanism. OpenAI GPT  has the same objective as ELMo in the forward direction, except it uses transformer architecture. BERT  also uses transformer architecture with bidirectional pre-training tasks. Training objectives affect the information encoded in embeddings. Each objective and architecture presumes a different inductive bias.  In this work, we focused on BERT as it uses multiple training objectives. These objectives can create an inhibitory effect or a regulatory effect on each other. For this reason, we applied a hierarchical multitask learning approach to BERT by modifying its original structure. Our motivation is to create embeddings that encode the information from each task in a balanced way. Our contributions are as follows:    Our experimental results show that Lower NSP has a competitive performance when compared with the original BERT structure. We also evaluate the learned embeddings on probing tasks to provide useful insights into training strategies. Results on probing task experiments show that using bigram shift task for pre-training is useful for specific tasks. The remaining part of this paper is organized as follows. In Section , we mention related works. In Section , we explain our methods in detail. In Section , we report our experiment results. Lastly, we give a conclusion in Section .  
"," Recent works show that learning contextualized embeddings for words is beneficial for downstream tasks. BERT is one successful example of this approach. It learns embeddings by solving two tasks, which are masked language model  and the next sentence prediction . The pre-training of BERT can also be framed as a multitask learning problem. In this work, we adopt hierarchical multitask learning approaches for BERT pre-training. Pre-training tasks are solved at different layers instead of the last layer, and information from the NSP task is transferred to the masked LM task. Also, we propose a new pre-training task bigram shift to encode word order information. We choose two downstream tasks, one of which requires sentence-level embeddings , and the other requires contextualized embeddings of words . Due to computational restrictions, we use the downstream task data instead of a large dataset for the pre-training to see the performance of proposed models when given a restricted dataset. We test their performance on several probing tasks to analyze learned embeddings. Our results show that imposing a task hierarchy in pre-training improves the performance of embeddings.",229
" Definitions have a very important role in scientific literature because they define the major concepts with which an article operates. They are used in many automatic text analysis tasks, such as question answering, ontology matching and construction, formal concept analysis, and text summarization. Intuitively, definitions are basic building blocks of a scientific article that are used to help properly describe hypotheses, experiments, and analyses. It is often difficult to determine where a certain definition lies in the text because other sentences around it may have similar style.  Automatic definition extraction  is an important field in natural language processing  because it can be used to improve text analysis and search.  %Natasha: here adding formal definition of formal definitions Definitions play a key role in mathematics, but their creation and use differ from those of \enquote*{everyday language}  definitions. A comprehensive study is given in a series of works by Edwards and Ward~, , , %, and    inspired by writings of Richard Robinson~ and lexicographer Sidney Landau~. %They distinguish between extracted definitions that report usage and have a truth value~, and stipulated  that create usage and create concepts but have no truth value. % Nat - fixed sentence %Moreover, in a stipulated definition a term has to be free from all the associations it acquired in its non-technical use.   %For example, ""Suppose that student is a person enrolled into an academic institution"" is a stipulated definition while    Mathematical definitions frequently have a history as they evolve over time. The definition we use for function, for instance, may not be the one that was used a hundred years ago. % Nat - fixed sentence The concept of connectivity has two definitions, one for path connectivity and another for set-theoretic connectivity. In mathematical texts the meaning of the defined concept is not determined by its context but it is declared and is expected to have no variance within that specific mathematical text~. % Nat - updated here  Mathematical definitions have many features, some critical and some optional but accepted within the  mathematical community. % Nat - added this %Van Dormolen and Zaslavsky~  describe a good mathematical definition as containing criteria of hierarchy% , existence% , equivalence, and axiomatization. Desired but not necessary criteria of a definition are minimality, elegance, and degenerations.  We give here short definitions of these concepts; detailed explanations with examples can be found in .   % end of formal definition of formal definitions Not every definition appearing in text is mathematical in the above sense. For example, Wikipedia articles contain definitions of different style. We see below that the Wikipedia definition of the Kane \& Abel musical group %shown in Figure   is not similar in style to the Wikipedia definition of an Abelian group.  % %\end{figure} %  Current methods for automatic DE view it as a binary classification task,  where a sentence is classified as a definition or a non-definition. A supervised learning process is usually employed for this task,  employing feature engineering for sentence representation.  The absolute majority of current methods study generic definitions and not mathematical definitions .  In this paper we describe a supervised learning method for automatic DE from mathematical texts. Our method  applies a Convolutional Neural Network , a Long Short-Term Memory network , and their combinations to the raw text data and sentence syntax structure, in order to detect definitions. Our method is evaluated on three different corpora; two are well-known corpora for generic DE and one is a new annotated corpus of mathematical definitions, introduced in this paper.     The main contributions of this paper are  analysis and introduction of the new annotated dataset of mathematical definitions,  evaluation of the state-of-the-art DE approaches on the new mathematical dataset,  introduction and evaluation of upgraded sentence representations adapted to mathematical domain with an adaptation of deep neural networks to new sentence representations,  extensive experiments with multiple network and input configurations  performed on different datasets in mathematical and non-mathematical domains,  experiments with   cross-domain and multi-domain learning in a DE task, and  introduction of the new parsed but non-annotated dataset composed of Wiki articles on  near-mathematics topics, used in an additional--extrinsic--evaluation scenario. These all contribute to showing that using specifically suited training data along with adapting sentence representation and classification models to the task of mathematical DE significantly improves extraction of mathematical definitions from surrounding text.   The paper is organized as follows. Section  contains a survey of up-to-date related work. Section  describes the sentence representations and the structure of neural networks used in our approach. Section  provides the description of the datasets, evaluation results, and their analysis. Section  contains our conclusions. Finally, Appendix contains some supplementary  materials -- annotation instructions, description of the Wikipedia experiment, and figures. 
"," Automatic definition extraction from texts is an important task that has numerous applications  in several natural language processing fields such as summarization, analysis of scientific texts, automatic taxonomy generation, ontology generation, concept identification, and question answering. For definitions that are contained within a single sentence, this problem can be viewed as a binary classification of sentences into definitions and non-definitions.  In this paper, we focus on automatic detection of one-sentence definitions in mathematical texts, which are difficult to separate from surrounding text.  We experiment with several data representations, which include sentence syntactic structure and word embeddings, and apply deep learning methods such as the  Convolutional Neural Network  and the Long Short-Term Memory network , in order to identify mathematical definitions.  Our experiments demonstrate the superiority of CNN and its combination with LSTM, when applied on the syntactically-enriched input representation.  % %We use data representation that includes sentence syntactic structure; to this we apply deep learning methods such as Convolutional Neural Network  and Recurrent Neural Network , in order to identify mathematical definitions.  We also present a new dataset for definition extraction from mathematical texts. %We demonstrate that the use of this dataset for training learning models improves the quality of definition extraction when these models are then used for other definition datasets.  We demonstrate that this dataset is beneficial for training supervised models aimed at extraction of mathematical definitions.  %Marina: added new sentence from the conclusions section Our experiments with different domains demonstrate that mathematical definitions require special treatment, and that using cross-domain learning is inefficient for that task.",230
"  As technology advances in the rapidly developing era, the exponentially increasing of text data makes analyzing and understanding textual files a tedious work . From the readers' perspective, capturing the salient information from overwhelming documents is a labor-intensive and time-consuming task. The voluminous documents are urgently required to be processed more efficiently and the abundance of text data calls for text summarization techniques. Text summarization is one of the important tasks of natural language processing that automatically convert a text or a collection of texts within the same topic into a concise summary that contains key semantic information. The length of summaries is usually significantly less than the original text . The research on automatic text summarization has been attractive in the field of natural language processing  which can be beneficial for many downstream applications such as creating news digests, search, and report generation .   According to the number of input documents, text summarization can be cast into single document summarization and multi-document summarization. Single document summarization aims to form a summary from only one document while multi-document summarization aims at generating a short and informative summary across a set of topic-related documents. From the application perspective, single document summarization may not satisfy the requirement to produce comprehensive summaries, because it does not make good use of documents that are generated around the clock . For content to be summarized, it is more comprehensive and accurate to generate a summary from multiple documents written at different times, covering different perspectives. From the technical point of view, multi-document summarization is more complicated and difficult to tackle than single document summarization . This is because in the multi-document summarization task, there is more diverse and conflicting information among documents. The volume of documents is usually longer and the relations between documents are more complicated. In such large amount of documents, documents would inevitably be complement, overlapping and conflicting to each other .  In addition, excessively long input documents often lead to model degradation . It is challenging for models to retain the most critical contents of complex input sequences, while generating the coherent, non-redundant, non-factual error and grammatically readable summaries. Therefore, multi-document summarization requires models to have stronger capabilities for analyzing the corpora, identifying and merging consistent information. Furthermore, multi-document summarization task is more computation-hungry, due to the increasing sizes of current datasets and language model parameters.     Multi-document summarization task enjoys a wide range of real world applications, including summarization on news , scientific publications , emails , product reviews , lecture feedback , Wikipedia articles generation ,  medical documents  and software project activities . Recently, multi-document summarization technology has also received a great amount of attention in the industry. An intelligent multilingual news reporter bot named Xiaomingbot  was developed for news generation. This bot is able to summarize multiple news into one article and then translate it into multiple languages. Massive application requirements and rapidly growing online data promote the development of multi-document summarization. However, the majority of existing methods still generate summaries with manually crafted features , such as sentence position features , sentence length features , proper noun features , cue-phrase features , biased word features, sentence-to-sentence cohesion, sentence-to-centroid cohesion. The existing works using traditional algorithms can be divide into the following categories: term frequency-inverse document frequency  based methods , clustering based methods \cite {goldstein2000multi, wan2008multi}, graph based methods  and latent semantic analysis based methods .  Deep learning has gained enormous attention in recent years due to its success in various domains, for instance, computer vision , natural language processing  and multi-modal . Both industry and academia have been in a race to utilize deep learning to solve complex tasks due to its capability of capturing highly nonlinear relations of data. Recently, deep learning based models are applied in multi-document summarization , which prospers the development of text summarization and enables models to achieve better performance. Comparing to the conventional approaches, deep learning based models reduce dependence on manual feature extraction drastically. This task attracts increasing attention in the natural language processing community and enjoys steady expansion ever since. The number of research publications on deep learning based multi-document summarization is increasing rapidly over the last five years -- the statistics show the number of publications has 225\% increase from 2017 to 2019. It provides strong evidence for the inevitable pervasiveness of deep learning in multi-document summarization research.  The prosperity of deep learning for summarization in both academia and industry requires a comprehensive review of current publications for researchers to better understand the process and research progress. However, most of the existing summarization review articles are based on traditional algorithms instead of deep learning based methods . Therefore, we conduct this survey to embrace the knowledge of multi-document summarization. To the best of our knowledge, this is the first comprehensive survey in the direction of deep learning for multi-document summarization. This survey has been designed in a way such that it classifies the neural based multi-document summarization techniques into diverse categories thoroughly and systematically. We also conduct a detailed discussion on the categorization and progress of these approaches to establish a clearer concept standing in the shoes of readers. We hope this survey provides a panorama for researchers, practitioners and educators to quickly understand and step into the field of deep learning based multi-document summarization. The key contributions of this survey are three-folds:         Paper Selection.    In this article, we select, summarize, discuss, and analyze 30 representative works. We used Google Scholar as the main search engine to discover related works. The high-quality papers are selected from top NLP and AI journals and conferences, include  ACL\footnote{Annual Meeting of the Association for Computational Linguistics.}, EMNLP\footnote{Empirical Methods in Natural Language Processing.}, COLING\footnote{International Conference on Computational Linguistics}, NAACL\footnote{Annual Conference of the North American Chapter of the Association for Computational Linguistics.}, AAAI\footnote{AAAI Conference on Artificial Intelligence.}, ICML\footnote{International Conference on Machine Learning.}, ICLR\footnote{International Conference on Learning Representations} and IJCAI\footnote{International Joint Conference on Artificial Intelligence.}. The major keywords we used include  multi-documentation summarization, summarization, extractive summarization, abstractive summarization, deep learning and neural networks.      Organization of the Survey.    In the following sections, this survey will cover various aspects of recent advanced deep learning based works in multi-document summarization. Section  gives an overview of multi-document summarization. Section  highlights network design strategies and offers a comprehensive review of deep learning based multi-document summarization techniques. This survey also summarizes objective functions in the literature , evaluation metrics , and available multi-document datasets . Finally, section  discusses the future research directions for deep learning based multi-document summarization followed by the conclusion in Section .     
","  Multi-document summarization  is an effective tool for information aggregation which generates an informative and concise summary from a cluster of topic-related documents. Our survey structurally overviews the recent deep learning based multi-document summarization models via a proposed taxonomy and it is the first of its kind. Particularly, we propose a novel mechanism to summarize the design strategies of neural networks and conduct a comprehensive summary of the state-of-the-art. We highlight the differences among various objective functions which are rarely discussed in the existing literature. Finally, we propose several future directions pertaining to this new and exciting development of the field.",231
"  Computer-assisted cross-lingual conversation by automatic speech-to-speech translation has been one of the most challenging problems in spoken language technologies in decades . Recent remarkable advances in speech and language processing led by deep learning techniques benefit this challenge by real-time and accurate speech translation. %\memo{}  gogole multi-task model One crucial problem in automatic speech-to-speech translation is its delay. Spoken language processing tasks are usually handled in the utterance or sentence level. Their application to the speech-to-speech translation suffers from a long delay that is proportional to the input length, because the process starts after the observation of the end of an utterance. That is similar to consecutive interpretation and is not useful for long monologues such as lecture talks. On the other hand, in such situations, simultaneous interpretation is often used for an audience not proficient in the language of a talk. Simultaneous interpretation is a challenging task to listen to the talk and speak its interpretation in a different language.  In this work, we tackle the problem of automatic simultaneous speech-to-speech translation and develop a neural system to do that from English to Japanese. Here, we call our task simultaneous translation, not simultaneous interpretation. We think the task of simultaneous interpretation includes some additional efforts for summarization to make the output concise for small latency and better understanding for the audience. The problem requires real-time and incremental processing for the output generated simultaneously with the input. Previous attempts to incremental neural speech translation focused on speech-to-text translation . Our work aims to speech-to-speech translation for natural information delivery by speech without a need for visual attention on text-based subtitles. Our system is based on the cascade of three processing modules: incremental speech recognition , incremental machine translation , and text-to-speech synthesis , rather than recent end-to-end approaches %\memo{} due to the difficulty of applying them to the simultaneous translation.  We follow existing studies on incremental neural speech processing. For ASR, we choose an approach using a teacher-student training framework to train an incremental student model with the help of a non-incremental teacher model . For MT, we choose an approach called wait-k, which delays the start of the decoding process simply by k steps  . For TTS, we choose approach starting the segmental speech synthesis after observing the next accent phrase . These modules exchange their input/output symbols in the forms of subwords and work in a symbol-synchronous way, so they can be cascaded even if they have different waiting strategies.  We also conduct a system-level evaluation of our system in system-level latency and module-level performance on English-to-Japanese simultaneous translation on TED Talks. The system-level latency measures are:  processing delays for waiting and computation time, and  TTS speaking latency derived from overlaps of synthesized speech outputs. The module-level performance is measured by standard metrics in ASR, MT, and TTS. This work is the first attempt of system-level evaluation for a simultaneous speech-to-speech translation system and would be beneficial to future studies.  %The remainder of this paper is organized as follows. %In section , we review the problem of simultaneous speech-to-speech translation, mainly in its difficulty. %In section , we describe the details of the incremental processing modules for ASR, MT, and TTS. %In section , we present system-wise evaluation of our system, followed by some discussions in section . %We conclude this paper in section .   
"," This paper presents a newly developed, simultaneous neural speech-to-speech translation system and its evaluation. The system consists of three fully-incremental neural processing modules for automatic speech recognition , machine translation , and text-to-speech synthesis . We investigated its overall latency in the system's Ear-Voice Span and speaking latency along with module-level performance.",232
" The emergence of online collaboration platforms has dramatically changed the dynamics of human teamwork, creating a veritable army of virtual teams, composed of workers in different physical locations. Software engineering requires a tremendous amount of collaborative problem solving, making it an excellent domain for team cognition researchers who seek to understand the manifestation of cognition applied to team tasks.  Mining data from social coding platforms such as GitHub can yield insights about the thought processes of virtual teams.  Previous work on issue comments has focused on emotional aspects of team communication, such as sentiment and politeness.  Our aim is to map issue comments to states in team cognition such as information gathering, knowledge building and problem solving.  To do this we employ dialogue act  classification, in order to identify the intent of the speaker.  Dialogue act classification has a broad range of natural language processing applications, including machine translation, dialogue systems and speech recognition.  Semantic-based classification of human utterances is a challenging task, and the lack of a large annotated corpus that represents class variations makes the job even harder. Compared to the examples of human utterances available in standard datasets like the Switchboard  corpus and the CSI Meeting Recorder Dialogue Act  corpus, GitHub utterances are more complex.   The primary purpose of our study is the DA classification of GitHub issue comments by harnessing the strength of transfer learning, using word and sentence level embedding models fine-tuned on our dataset.  For word-level transfer learning, we have used GLoVe vectors, and Universal Sentence Encoders and BERT models were used for sentence-level transfer. This paper presents a comparison of the performance of various  architectures on GitHub dialogues in a limited resource scenario.  A second contribution is our publicly available dataset of annotated issue comments. The dataset is available at \url{https://drive.google.com/drive/folders/1kLZvzfE80VeEYA1tqua_aj6nSiT57f83?usp=sharing}. In the field of computational collective intelligence,  where people collaborate and work in teams to achieve goals, dialogue act classification can play a vital role in understanding human teamwork.   
"," Social coding platforms, such as GitHub, serve as  laboratories for studying collaborative problem solving in open source software development; a key feature is their ability to support issue reporting which is used by teams to discuss tasks and ideas.  Analyzing the dialogue between team members, as expressed in issue comments, can yield important insights about the performance of virtual teams.  This paper presents a transfer learning approach for performing dialogue act classification on issue comments.  Since no large labeled corpus of GitHub issue comments exists, employing transfer learning enables us to leverage standard dialogue act datasets in combination with our own GitHub comment dataset. We compare the performance of several word and sentence level encoding models including Global Vectors for Word Representations , Universal Sentence Encoder , and Bidirectional Encoder Representations from Transformers . Being able to map the issue comments to dialogue acts is a useful stepping stone towards understanding cognitive team processes.",233
" Large, densely-labeled datasets are a critical requirement for the creation of effective supervised learning models. The pressing need for high quantities of labeled data has led many researchers to collect data from social media platforms and online forums . Due to the presence of noise and the lack of structure that exist in these data sources, manual quality analysis  is necessary to extract structured labels, filter irrelevant examples, standardize language, and perform other preprocessing tasks before the data can be used. However, obtaining dataset annotations in this manner is a time-consuming and expensive process that is often prone to errors.   In this work, we develop automated data cleaning and verification mechanisms for extracting high-quality data from social media platforms\footnote{All code is available at \url{https://github.com/rachel-1/qa_plausibility}.}. We specifically focus on the creation of question-answer datasets, in which each data instance consists of a question about a topic and the corresponding answer. In order to filter noise and improve data quality,  we propose the task of question-answer  plausibility, which includes the following three steps:   Because we assume social media users generally answer questions in good faith , we can assume plausible answers are correct ones . Necessarily, if this property were not satisfied, then any adequate solutions would require the very domain knowledge of interest. Therefore, we look to apply this approach toward data with this property.  In this study, we demonstrate an application of QA plausibility in the context of visual question answering , a well-studied problem in the field of computer vision . We assemble a large VQA dataset with images collected from an image-sharing social network, machine-generated questions related to the content of the image, and responses from social media users. We then train a multitask BERT-based model and evaluate the ability of the model to perform the three subtasks associated with QA plausibility. The methods presented in this work hold potential for reducing the need for manual quality analysis of crowdsourced data as well as enabling the use of question-answer data from unstructured environments such as social media platforms.   
"," Datasets extracted from social networks and online forums are often prone to the pitfalls of natural language, namely the presence of unstructured and noisy data. In this work, we seek to enable the collection of high-quality question-answer datasets from social media by proposing a novel task for automated quality analysis and data cleaning: question-answer  plausibility. Given a machine or user-generated question and a crowd-sourced response from a social media user, we determine if the question and response are valid; if so, we identify the answer within the free-form response.   We design BERT-based models to perform the QA plausibility task, and we evaluate the ability of our models to generate a clean, usable question-answer dataset. Our highest-performing approach consists of a single-task model which determines the plausibility of the question, followed by a multi-task model which evaluates the plausibility of the response as well as extracts answers .",234
"  In recent times, pre-trained neural language models  have become the preferred approach for language representation learning, pushing the state-of-the-art in multiple NLP tasks~. These approaches rely on a two-step training process: first, a self-supervised pre-training is performed on large-scale corpora; then, the model undergoes a supervised fine-tuning on downstream task labels using task-specific prediction heads. While this method was found to be effective in scenarios where a relatively large amount of labeled data are present, researchers highlighted that this is not the case in low-resource settings~.   Recently, pattern-exploiting training~(PET, \citet{Schick2020ExploitingCQ,Schick2020ItsNJ} tackles the dependence of NLMs on labeled data by first reformulating tasks as cloze questions using task-related patterns and keywords, and then using language models trained on those to annotate large sets of unlabeled examples with soft labels. PET can be thought of as an offline version of knowledge distillation~, which is a well-established approach to transfer the knowledge across models of different size, or even between different versions of the same model as in self-training . While effective on classification tasks that can be easily reformulated as cloze questions, PET cannot be easily extended to regression settings since they cannot be adequately verbalized. Contemporary work by \citet{du-etal-2020-selftraining} showed how self-training and pre-training provide complementary information for natural language understanding tasks.  In this paper, I propose a simple self-supervised data augmentation approach that can be used to improve the generalization capabilities of NLMs on regression and classification tasks for modest-sized labeled corpora. In short, an ensemble of fine-tuned models is used to annotate a large corpus of unlabeled text, and new annotations are leveraged in a multi-task setting to obtain final predictions over the original test set. The method was tested on the AcCompl-it shared tasks of the EVALITA 2020 campaign~, where the objective was to predict respectively complexity and acceptability scores on a 1-7 Likert scale for each test sentence, alongside an estimation of its standard error. Results show considerable improvements over regular fine-tuning performances on COMPL and ACCEPT using the UmBERTo pre-trained model~, suggesting the validity of this approach for complexity/acceptability prediction and possibly other language processing tasks.  
","   English.  This work describes a self-supervised data augmentation approach used to improve learning models' performances when only a moderate amount of labeled data is available. Multiple copies of the original model are initially trained on the downstream task. Their predictions are then used to annotate a large set of unlabeled examples. Finally, multi-task training is performed on the parallel annotations of the resulting training set, and final scores are obtained by averaging annotator-specific head predictions. Neural language models are fine-tuned using this procedure in the context of the AcCompl-it shared task at EVALITA 2020, obtaining considerable improvements in prediction quality.",235
" Underresourced languages, from a natural language processing  perspective, are those lacking the resources  needed to support state-of-the-art performance on NLP problems like machine translation, automated speech recognition, or named entity recognition. Yet the vast majority of the world's languages---representing billions of native speakers worldwide---are underresourced. And the lack of available training data in such languages usually reflects a broader paucity of electronic information resources accessible to their speakers.  %The most prominent of these languages have millions of native speakers, who have previously been deprived of access to information on the web in their native language, due to missing translation tools.   For instance, there are over six million Wikipedia articles in English but fewer than sixty thousand in Swahili and fewer than seven hundred in Bambara, the vehicular and most widely-spoken native language of Mali that is the subject of this paper. Consequently, only 53\% of the worlds population have access to ``encyclopedic knowledge'' in their primary language, according to a 2014 study by Facebook. MT technologies could help bridge this gap, and there is enormous interest in such applications, ironically enough, from speakers of the languages on which MT has thus far had the least success. There is also great potential for humanitarian response applications .  Fueled by data, advances in hardware technology, and deep neural models, machine translation  has advanced rapidly over the last ten years.  %Yet underresourced languages have yet to benefit from these advances, because they lack the large volumes of translated texts needed to drive neural machine learning.  %Although neural models are generally considered to work best in domains where large amounts of training data exist Researchers are beginning to investigate the effectiveness of   low-resource languages, as in recent WMT 2019 and WMT 2020 tasks , and in underresourced African languages. %What has been done? Which challenges have been identified, which solutions have been found?  Most prominently, the Masakhane  community, a grassroots initiative, has developed open-source NMT models for over 30 African languages on the base of the JW300 corpus~, a parallel corpus of religious texts.   Since African languages cover a wide spectrum of linguistic phenomena and language families , individual development of translations and resources for selected languages or language families are vital to drive the overall progress. Just within the last year, a number of dedicated studies have significantly improved the state of African NMT: \citet{biljon2020} analyzed the depth of Transformers specifically for low-resource translation of South-African languages, based on prior studies by  \citet{DBLP:journals/corr/abs-1906-05685} on the Autshumato corpus~. \citet{dossou2020ffr} developed an MT model and compiled resources for translations between Fon and French, \citet{akinfaderin-2020-hausamt} modeled translations between English and Hausa, \citet{orife2020neural} for four languages of the Edoid language family, and \citet{ahia2020supervised} investigated supervised vs. unsupervised NMT for Nigerian Pidgin.   %Superficially, it might seem like this development simply grows the pool of ``standard MT'' evaluation tasks and data sets, with some data sets being smaller than others.    In this paper, we present the first parallel data set for machine translation of Bambara into and from English and French and the first benchmark results on machine translation to and from Bambara. We discuss challenges in working with low-resource languages and propose strategies to cope with data scarcity in low-resource MT.   We discuss the socio-cultural context of Bambara translation and its implications for model and data development. Finally, we analyze our best-performing neural models with a small-scale human evaluation study and give recommendations for future development. %These contributions a deeper understanding of the shortcomings of state-of-the-art methods for high-resource languages, and  We find that the translation quality on our in-domain data set is acceptable, which gives hope for other languages that have previously fallen under the radar of MT development.      % [Can model and data be made publicly available?] We released our models and data upon publication. Our evaluation setup may serve as benchmark for an extremely challenging translation task.  %
"," %Low-resource languages present unique challenges to machine translation.  %We discuss the case of Bambara, a Mande language where training data is scarce and requires significant amounts of pre-processing. Moreover, the socio-cultural context within which Bambara speakers live poses challenges for automated processing. We contribute the first parallel data set for machine translation of Bambara into and from English and French and the first benchmark results on machine translation to and from Bambara.  %New abstract?:  Low-resource languages present unique challenges to  machine translation. We discuss the case of Bambara, a Mande language for which training data is scarce and requires significant amounts of pre-processing. More than the linguistic situation of Bambara itself, the socio-cultural context within which Bambara speakers live poses challenges for automated processing of this language. In this paper, we present the first parallel data set for machine translation of Bambara into and from English and French and the first benchmark results on machine translation to and from Bambara. We discuss challenges in working with low-resource languages and propose strategies to cope with data scarcity in low-resource machine translation .",236
"  Language modelling is the task of transforming individual words into vector representations based on the context they appear in. Hence, distant term dependencies are an inherited issue within the task. Language models always seek for smart approaches towards incorporating context from longer distances as it allows for better representations compared to their limited context counterparts. Intuitively, imagine attempting to start reading a novel series from the second book onward, with no information about the first. The amount of information previously missed is something that cannot be acquired. However, this is the case with most language models. While an understanding of the words is present due to the contextual information at each word's occurrence, entity information that are in distant text are lost or not transferred.   Until recently, Recurrent Neural Networks , and specifically  Long Short-Term Memory  networks, have been the core of all the state-of-the-art approaches . Thanks to the Transformers architecture , through the use of attention mechanisms, models such as XLNet , GPT  and BERT  can account for even longer sequences. However, the computational limitations of the multi-head attention in the architecture make it hard to increase the contextual information in such models . As a result, research has been focused on introducing variations to the transformer architecture, with focus on the multi-head attention mechanism, in order to alleviate part of the computational cost and increase the contextual information available to models.   In this paper we present a novel approach, that makes use of coreference information during training a language model via our Entity-Transformer architecture, which extends the original Transformer block in Transformer-Based language models. To that end, we incorporate the important entity information that would otherwise be unreachable for the model. As a result, we effectively boost the representations of the entity mentions, where entity information is present, without hindering the performance of the language model where entities are not present.   In our experiments, we extend the GPT2 architecture to formulate our model, named GPT2E and train it on the CoNLL-2012 dataset  using the annotated coreference information. We evaluate the model's performance in terms of Perplexity on the ConLL 2012 and the LAMBADA  datasets and showcase the effects of such training on the word representations as well as on the downstream task of Named Entity Recognition  using the CoNLL 2012 dataset. To that end, we compare GPT2E's performance to a base model  when trained on the same data, to highlight the effects of coreference information when paird with our Entity-Transformer architecture.   
","     In the last decade, the field of Neural Language Modelling has witnessed enormous changes, with the development of novel models through the use of Transformer architectures. However, even these models struggle to model long sequences due to memory constraints and increasing computational complexity. Coreference annotations over the training data can provide context far beyond the modelling limitations of such language models. In this paper we present an extension over the Transformer-block architecture used in neural language models, specifically in GPT2, in order to incorporate entity annotations during training. Our model, GPT2E, extends the Transformer layers architecture of GPT2 to Entity-Transformers, an architecture designed to handle coreference information when present. To that end, we achieve richer representations for entity mentions, with insignificant training cost. We show the comparative model performance between GPT2 and GPT2E in terms of Perplexity on the CoNLL 2012 and LAMBADA datasets as well as the key differences in the entity representations and their effects in downstream tasks such as Named Entity Recognition. Furthermore, our approach can be adopted by the majority of Transformer-based language models.",237
"  . } To foster research on dialog policy learning for virtual digital assistants, several task-oriented dialog corpora have been introduced in recent years, such as SimDial, MultiWoZ, Taskmaster, and Schema Guided Dialog, to name a few.  Deep learning approaches, including mixture models hierarchical encoder/decoder, reinforcement learning, and pre-trained language models, have significantly advanced dialog policy research in the past few years, setting new state-of-the-art performance limits.  %More recently, SimpleTOD and SOLOIST have shown that pre-training dialog policy using large language models, e.g., GPT-2, can lead to significantly better performance on task-oriented neural dialog policy learning by using even larger neural models.  %\ab{somewhere we want to mention that data collection is expensive -- both in time; and other resources}  However, collecting annotated data for supervised dialog policy learning is an expensive and time-consuming process. Hence, it is desirable to explore approaches to train dialog policy with limited data and transfer an existing policy with few or even no additional training data to new domains.  This practical requirement has motivated the community to research resource-constrained dialog policy learning in the past few decades. Researchers have explored approaches including employing grammar constraints for dialog policy, transfer learning , or pre-trained language models. Few-shot domain adaptation has been researched since the 2000s on both end-to-end dialog systems  as well as dialog policy learning.  % We investigate one-shot policy learning and zero-shot domain transfer using \ab{I think up to 50 examples for original training is reasonable. Emphasizing that there is only ONE training sample available may not be reasonable. The reviewer may come back and say why 1? Why not 5? Why not 10? So showing that other methods need thousands of samples to match the performance of DILP would be a more convincing argument IMHO.} \ab{We  don't believe a single dialog is going to be representative of ALL of the conversational flows that may occur in a more complicated real-life dataset . Increasing the number of training samples beyond one may help with improving the policy on MultiWoZ, not necessarily in terms of inform and success but in terms of action F1 .} \ab{Finally, zero-shot transfer is an extremely desirable property. One can argue that they don't want to re-train and want their method to work on a new domain out of the box. Having said that, your argument would be even stronger if you could also show that the baseline methods need X shot transfer to match the performance of zero-shot DILP on a new domain.} differential inductive logic programming .   % Given an encoding of the common known knowledge and a set of examples represented as a logical set of facts, an inductive logic programming  system can be hypothesised  which conforms to all of the positive and none of the negative examples. % ILP has been extensively studied in the context of symbolic AI in the past few decades, and has been also adapted to dialog management. While ILP generalizes well from a noiseless/consistent set of rules, it is known to be prone to noisy samples , and hence would not be applicable to real-world problem scenarios, and particularly noisy dialog policy data. DILP addresses the noisy/inconsistent training data via a novel probabilistic treatment of the learned rules by relaxing them to have different probabilities of being true/false and solving the relaxed problem using recent advances in gradient-based optimization.  \edit {In a traditional modular dialog system, the dialog policy aims to decide a dialog action given a dialog state, while assuming the tasks of language understanding and generation are handled by other components. Under such assumptions, a task-oriented dialog policy mostly follow the slot-filling scheme, which can be described by a set of probabilistic rules. Therefore, we hypothesize that dialog policy in this limited sense can be constructed by learning the underlying rules. To this end, we draw upon the recent advances in developing differentiable inductive logical programs   that use neural architectures to learn almost rule-based policies.} We present \name, an adaptation of DILP  to dialog policy learning. Briefly, \name discerns a set of logical rules from the examples by using inductive reasoning.\footnote{Inductive reasoning tries to summarize general principles from special cases. For example, the fact ``cars A, B, and C drive on the right side of the road'' induces that ``all cars drive on the right side of the road.''}  We introduce \name in Section. We apply \name to the SimDial Dataset , and MultiWoZ Dataset , showing that on the task of one-shot dialog policy learning and zero-shot domain transfer, \name outperforms several other neural baselines. Finally, Section concludes this paper.  
","   Motivated by the needs of resource constrained dialog policy learning,   we introduce dialog policy via differentiable inductive logic . We explore the tasks of one-shot learning and zero-shot domain transfer with \name on SimDial and MultiWoZ. Using a single representative dialog from the restaurant domain, we train \name on the SimDial dataset and obtain 99+\% in-domain test accuracy. We also show that the trained DILOG zero-shot transfers to all other domains with 99+\% accuracy, proving the suitability of \name to slot-filling dialogs. We further extend our study to the MultiWoZ dataset achieving 90+\% inform and success metrics. We also observe that these metrics are not capturing some of the shortcomings of DILOG in terms of false positives, prompting us to measure an auxiliary Action F1 score. We show that \name is 100x more data efficient than state-of-the-art neural approaches on MultiWoZ while achieving similar performance metrics. We conclude with a discussion on the strengths and weaknesses of \name.",238
" Sequence labeling is the task of labeling each token of a sequence. It is an important task in natural language processing and has a lot of applications such as Part-of-Speech Tagging  , Named Entity Recognition  , Chunking .  The neural CRF model is one of the most widely-used approaches to sequence labeling and can achieve superior performance on many tasks .  It often employs an encoder such as a BiLSTM to compute the contextual vector representation of each word in the input sequence. The potential function at each position of the input sequence in a neural CRF is typically decomposed into an emission function  and a transition function   . %The transition function is computed from the previous and current labels.  In this paper, we design a series of increasingly expressive potential functions for neural CRF models. First, we compute the transition function from label embeddings  instead of label identities. Second, we use a single potential function over the current word and the previous and current labels, instead of decomposing it into the emission and transition functions, leading to more expressiveness. We also employ tensor decomposition in order to keep the potential function tractable. Thirdly, we take the representations of additional neighboring words as input to the potential function, instead of solely relying on the BiLSTM to capture contextual information.   To empirically evaluate different approaches, we conduct experiments on four well-known sequence labeling tasks: NER, Chunking, coarse- and fine-grained POS tagging. We find that it is beneficial for the potential function to take representations of neighboring words as input, and a quadrilinear potential function with a decomposed tensor parameter leads to the best overall performance.    Our work is related to \citet{reimers-gurevych-2017-reporting,yang-etal-2018-design}, which also compared different network architectures and configurations and conducted empirical analysis on different sequence labeling tasks. However, our focus is on the potential function design of neural CRF models, which has not been sufficiently studied before.  
"," The neural linear-chain CRF model is one of the most widely-used approach to sequence labeling. In this paper, we investigate a series of increasingly expressive potential functions for neural CRF models, which not only integrate the emission and transition functions, but also explicitly take the representations of the contextual words as input. Our extensive experiments show that the decomposed quadrilinear potential function based on the vector representations of two neighboring labels and two neighboring words consistently achieves the best performance.",239
"  Sequence labeling tasks are essential in web mining, such as named entity recognition , event extraction, and relation identification. For example, the NER models assign the predefined labels to tag tokens in the input sequences to indicate both the entity boundaries and types. In some web services, such as question answering, sequence labeling also plays a critical role, where it reads a passage in a Web page as the context and answers a given question by extracting a text span inside the given passage. This process is often called machine reading comprehension . MRC is also regarded as a sequence labeling task, since it predicts whether each token is the start, end, or none for the answer span.  There is a rich literature for sequence labeling. Classical methods include Hidden Markov models , maximum entropy Markov models , and conditional random field . Recently, combining neural networks as the representation layer with CRF models has further boosted the state-of-the-art performance. However, such statistical models require large amounts of training data. Consequently, they only show good performance in languages with rich training data, such as English. Sequence labeling on low-resource languages is still very challenging, mainly due to very limited training data available.  To tackle the challenge of sequence labeling in low-resource languages, some early works transfer the knowledge from rich-source languages to low-resource ones by information alignment through manually built bilingual parallel corpora,  or language-independent features. In recent years, multilingual pre-trained language models, such as Unicoder, mBERT, and XLM-Roberta , are developed for model transferring. For example, Wu et al.  fine-tune mBERT on a pseudo training set by a meta-learning method. To better leverage the unlabeled data in the target language, a teacher-student framework is proposed to distill knowledge from weighted teacher models. Inspired by back translation in neural machine translation , DualBERT is developed to learn source language and target language features simultaneously. Although these multilingual sequence labeling models can effectively locate target spans, they often fail to give the precise boundaries of the spans in the target languages.  %when predicting text spans in the target languages.  %that is, pairs of sentences with similar meanings but in different languages, %\jp{What is the conclusion we can draw from this paragraph?}  %The previous multilingual sequence labeling models can roughly identify the correct target spans, but often fail to give the precise boundaries when predicting text spans in the target languages.  We conduct an empirical study to quantitatively assess the challenge. In Figure , we categorize the mismatches between the predicted span and the ground truth span into four types:  the predicted answer is a super span of the ground truth;  the predicted answer is sub span of the ground truth;  the predicted answer both miss some terms in the ground truth and add extra terms not in the ground truth , and   the predicted answer is adjacent to the ground truth but contains no common sub-span with it .  We further show in Table the statistics of the error cases in the cross-lingual NER task using the XLM-R model, where the boundary errors, including super span, sub span, drifted span, and adjacent span, contribute to a large portion of all error cases as shown in the last column. The other errors cases are mainly entity type detection errors. This observation motivates us to tackle the bottleneck of boundary detection in sequence labeling models.           % \end{center}      \end{table}  Accurately detecting answer boundaries becomes a bottleneck in sequence labeling.  To tackle the challenge, in this paper, we propose a separate model for boundary calibration based on the output of a base model. Intuitively, the base model captures the global context of the whole input sequence and roughly locates the region for answers. Then, the calibration model conducts finer search within the detected region and the neighborhood, and focuses on the local context to refine the boundary. This is analogous to the human perception and cognition process, which first locates the target, sets up the local context, and finally zooms into details.  Our design is novel for sequence labeling, and is orthogonal and complements to all existing approaches.  Using a second model to focus on detecting answer boundaries accurately is an intuitive and nice idea.  However, how to construct high-quality training data for the calibration model remains challenging. One straightforward method is to transform the original training data of sequence labeling task into a new training set for calibration model. However, the data collected in this way is still quite limited, especially for low-resource languages. To address this challenge, we strategically propose a novel phrase boundary recovery  task to pre-train the model on large-scale augmented datasets synthesized from Wikipedia documents in multiple languages. The new pre-training approach dramatically improves the capability of the calibration module to determine answer boundaries accurately.  % Besides the design of employing two models, we further equip the calibration model with a pre-training process by emphasizing on the capability of recovering meaningful phrases from noisy input.  Our approach is shown in Figure. CalibreNet consists of two modules, a base module and a calibration module. The base module can take any model of sequence labeling. The predicted answers by the base module are combined with the input sequence to form the input to the calibration module. The calibration module considers both the initial results by the base module and the whole passage to refine the span boundaries. In particular, the calibration module is pre-trained with the PBR task on large-scale multilingual synthesized data from Wikipedia-derived corpus.  We make the following technical contributions in this paper. First, we propose the CalibreNet framework for the task of cross-lingual sequence labeling to improve the accuracy of labeled answers. Second, we propose a novel phrase boundary recovery task and a weakly supervised pre-training method using Wikipedia data. This approach effectively enhances the model sensitivity to phrase boundaries. Last but not least, we conduct extensive experiments on zero-shot cross-lingual NER and improve the SOTA results. In addition, the experiments on the MRC tasks also show consistent improvement over strong baseline methods.  The rest of the paper is organized as follows. We first review the related work in Section. We then present our approach in Section. We report the extensive experimental results in Sections.  We conduct further analysis in Section, and conclude the paper in Section.  
","  \footnotetext[1]{Work done during the first author's internship at Microsoft STCA.} \footnotetext[2]{Daxin Jiang and Wanli Zuo are the corresponding authors.} % \footnotetext[3]{Jian Pei's research is supported in part by the NSERC Discovery Grant program. All opinions, findings, conclusions and recommendations in this paper are those of the authors and do not necessarily reflect the views of the funding agencies.}   Lack of training data in low-resource languages presents huge challenges to sequence labeling tasks such as named entity recognition  and machine reading comprehension . One major obstacle is the errors on the boundary of predicted answers. To tackle this problem, we propose CalibreNet, which predicts answers in two steps. In the first step, any existing sequence labeling method can be adopted as a base model to generate an initial answer. In the second step, CalibreNet refines the boundary of the initial answer. To tackle the challenge of lack of training data in low-resource languages, we dedicatedly develop a novel unsupervised phrase boundary recovery pre-training task to enhance the multilingual boundary detection capability of CalibreNet. Experiments on two cross-lingual benchmark datasets show that the proposed approach achieves SOTA results on zero-shot cross-lingual NER and MRC tasks.",240
"  The Text-to-SQL task aims to translate natural language texts into SQL queries. Users who do not understand SQL grammars can benefit from this task and acquire information from databases by just inputting natural language texts. Previous works  focus on context-independent text-to-SQL generation. However, in practice, users usually interact with systems for several turns to acquire information, which extends the text-to-SQL task to the context-dependent text-to-SQL task in a conversational scenario. Throughout the interaction, user inputs may omit some information that appeared before. This phenomenon brings difficulty for context-dependent text-to-SQL task.   Recently, context-dependent text-to-SQL task has attracted more attention. \citet{suhr2018learning} conduct experiments on ATIS dataset . Besides, two cross-domain context-dependent datasets SParC  and CoSQL  are released. Cross-domain means databases in test set differ from that in training set, which is more challenging.   EditSQL  is the previous state-of-the-art model on SParC and CoSQL datasets and it focuses on taking advantages of previous utterance texts and previously predicted query to predict the query for current turn. Table  shows the user inputs, ground truth queries and predicted queries of EditSQL for an interaction. In the second turn, EditSQL views ``Kacey"" as the name of a dog owner. However, since the context of the interaction is about dogs, ``Kacey"" should be the name of a dog. This example shows that a model using only historical information of user inputs may fail to keep context consistency and maintain thematic relations.   According to  and , to maintain thematic relations, users may change constraints, ask for different attributes for the same topic when they ask the next questions. Thus, database schema items  in current turn should have relation with items in previous turn. For example, in Table , the second question  adds a constraint of the name and asks for the age of a dog instead of the numbers of all dogs. The corresponding database schema items Dogs.age and Dogs.name in   belong to the same table as Dogs.* in previous query . Therefore, we propose to take historical information about database schema items into consideration.  % %     %     %\end{table}   In particular, we first construct a graph based on corresponding database, where graph nodes are database schema items and graph edges are primary-foreign keys and column affiliation. Short distance between graph nodes appearing in previous query and current query can reveal the context consistency since there is usually an edge between the different attributes of the same topic. We then propose a database schema interaction graph encoder to model database schema items together with historical items. Empirical results on two large cross-domain context-dependent text-to-SQL datasets - SParC and CoSQL show that our schema interaction graph encoder contributes to modeling context consistency and our proposed model with database schema interaction graph encoder substantially outperforms the state-of-the-art model.                \end{table}  Our main contributions are summarized as follows:     
"," Context-dependent text-to-SQL task has drawn much attention in recent years. Previous models on context-dependent text-to-SQL task only concentrate on utilizing historical user inputs. In this work, in addition to using encoders to capture historical information of user inputs, we propose a database schema interaction graph encoder to utilize historicalal information of database schema items. In decoding phase, we introduce a gate mechanism to weigh the importance of different vocabularies and then make the prediction of SQL tokens. We evaluate our model on the benchmark SParC and CoSQL datasets, which are two large complex context-dependent cross-domain text-to-SQL datasets. Our model outperforms previous state-of-the-art model by a large margin and achieves new state-of-the-art results on the two datasets. The comparison and ablation results demonstrate the efficacy of our model and the usefulness of the database schema interaction graph encoder.",241
" The recent survey conducted by WHO shows that a total  million people in the world are living with depression.  % This has increased by 18.4\% between  and .  At its most severe, depression can lead to suicide and is responsible for  deaths every year  . Early detection and appropriate treatment can encourage remission and prevent relapse . However, the stigma coupled with the depression makes patients reluctant to seek support or provide truthful answers to physicians .  Additionally, clinical diagnosis is dependent on the self-reports of the patient閳ユ獨 behavior, which requires them to reflect and recall from the past, that may have obscured over time. In contrast, social media offers unique platform for people to share their experiences in the moment, express emotions and stress in their raw intensity, and seek social and emotional support for resilience. As such, the depression studies based on social media offer unique advantages over scheduled surveys or interviews . Social media self-narratives contain large amounts of implicit and reliable information expressed in real-time, that are  essential for practitioners to glean and understand user閳ユ獨 behavior outside of the controlled clinical environment. % \indent Several studies in the literature have explored various linguistic and visual cues to effectively detect user depression from the postings on social media platform like Twitter  and Reddit . Majority of these existing studies have formulated the social media depression detection task as a binary classification problem  and therefore are limited to only identifying the depressive users. \\ \indent To assist healthcare professionals  intervene in a  timely manner such as with an automatic triaging, it is necessary to develop an intelligent decision support system that provides HPs fine-grained depression related symptoms. The triage process is a critical step in giving care to the patients because, by prioritizing patients at different triage levels based on the severity of their clinical condition, one can enhance the utilization of healthcare facilities and the efficacy of healthcare interventions. There have been a few efforts to create datasets for capturing depression severity, however they are limited to  only clinical interviews  and questionnaires , and  individuals who voluntarily participate in the study .  \\ \indent In this work, we exploit the Twitter data to identify the indications  of depression. We developed a high quality dataset consisting of total  tweets, with  tweets posted by  self-reported depressed users over  weeks time, which were manually annotated using  questionnaire  based symptoms categories. In Table-, we provide sample tweets associated with the nine item  depression symptoms. % The  is a self-reported questionnaire, based on the Diagnostic and Statistical Manual of Mental Disorders, Fourth Edition  guidelines, for screening, diagnosing, and measuring severity of depression.  % The overall  scores range between  and , with a score of  or more linked to major depressive disorders. Our research hypothesis is that depressed individuals discuss their symptoms on Twitter that can be tracked reliably.  }   \end{table*} % Advancement in Natural Language Processing  is one of the most promising avenues for discovering vital mental health information from user-generated data. Nonetheless, user social-media post offer unique challenges as discussed below:   To account for this creative linguistic device widely observed in utterances of depressive users, we propose a Figurative Language enabled Multi-Task Learning framework  that works on the concept of task sharing mechanism . In this work, we improve the performance and robustness of the  for the primary task of `symptom identification' combined with the supervisory task `figurative usage detection' in a multi-task learning setting.  We introduce a mechanism  named `co-task aware attention' which enables the layer-specific soft sharing of the parameters for the tasks of interest. The proposed attention mechanism is parameterized with the task-specific scaling factor for BERT  layers. BERT enables even the low-resource tasks to benefit from deep bi-directional architectures and the unsupervised training framework to obtain the context-aware encoded representation. The virtue of this model is its ability to learn the task-specific representation of the input tweet by coordinating among the layers and between the tasks. \\ Contributions:   %%%%%%%%%%%%%%%%%%%%%  % % According to Word Health Organization \footnote{http://www.who.int/news-room/fact-sheets/detail/mental-disorders}, ``depressive disorder is characterized by sadness, loss of interest or pleasure, feelings of guilt or low self worth, disturbed sleep or appetite, feelings of tiredness, and poor concentration"".  % Major depressive disorder  has a world-wide impact on society with each year causing almost one million deaths. % The recent survey conducted by WHO shows that total  million people in the world are living with depression. This has increased by 18.4\% between  and . At its most severe depression can lead to suicide and is responsible for  deaths every year  . Early detection and appropriate treatment can encourage remission and prevent relapse . However, the stigma coupled with the depression makes patients reluctant to seek support. Also, the associated cognitive biases, inhibits the patients to provide truthful answer to physicians which further add limitation .\\ % \indent Additionally, clinical diagnosis is dependent on the hypothetical self-reports of patients behaviour, requiring patients to reflect on what they were doing and thinking sometime in the past, which may have become obscured over time. In contrast, social media offers unique platform for people to share their experiences, exhaust emotion and stress, and seek social and emotional support. As such, the depression studies based on social media offers several advantage . These self-narrative contains large amount of the implicit information, which are highly essential for practitioner to understand users behaviour outside the controlled clinical environment and in real-time.\\ % \indent Several studies in literature have explored various linguistic and visual cues to effectively detect depression from the social media platform like Twitter and Reddit. Majority of these existing studies have formulated the social media depression detection task as a binary classification problem  and therefore is only limited to identify the depressive users. \\ % \indent However, to assist healthcare professional  in making timely intervention, it is required to develop an intelligent decision support system that could provide HPs with more fine-grained depression related symptoms and automatic triaging techniques. The triage process is the first critical step in giving care to the patients by prioritizing patients at different triage levels based on the severity of their clinical conditions that could have the potential to enhance the efficacy of healthcare interventions. In literature, there has been few efforts to create dataset for capturing depression severity, however they are limited to  only clinical interview  and questionnaire , and  individuals who voluntary participated in the study.  \\ % \indent In this work, we exploit the Twitter data to identify the indications  of depression and finally assign  based severity labels: `None', `Mild', `Moderate', `Moderately Severe', and `Severe' for triaging. We developed a new dataset consisting  tweets posted by  self-reported depressed users over  weeks time, which are manually annotated into  symptom categories. In Table-, we provide the samples tweets associated with nine item  depression symptoms. % The  is a self-report questionnaire based on the Diagnostic and Statistical Manual of Mental Disorders, Fourth Edition  guidelines for screening, diagnosing, and measuring severity of depression. The overall  scoring ranges between  and , with  or more highly linked to major depressive disorders. % Our research hypothesis is that depressed individuals discuss their symptoms on Twitter.\\ % %This work aim to develop an intelligent decision support system in the context of major depressive disorder by providing the healthcare professionals  with more fine-grained depression related symptoms and automatic triaging technique that is required by HPs to make timely intervention. The triage process is the first critical step in giving care to the patients by prioritizing patients at different triage levels based on the severity of their clinical conditions that could have the potential to enhance the efficacy of healthcare interventions.\\ %  % } %  %  % \end{table*} % Advancement in Natural Language Processing  technology is one of the most promising avenues for discovering vital mental health information from user-generated data. Nonetheless, these texts offers some inherently distinct challenges discussed as follows: %  % Furthermore, previous studies utilizing social media data in biomedical natural language processing task reported prediction error when drug or symptom names are utilized in figurative sense. To account for this creative linguistic devices widely observed in utterances of depressive users, we proposed a multitask learning  framework that works on the concept of task sharing mechanism. Multi-task learning has been proven to the a useful instruments to improve the generalization performance of the primary task with related auxiliary tasks. In this work, we focused to improve the performance and generalization ability of the proposed model for the primary task of `symptom identification' in companionship with the supervisory task `figurative language detection'. We introduce a mechanism  named `co-task aware attention' which enables the layer-specific soft sharing of the parameters for the task at interest. The proposed attention mechanism is parameterize with the task-specific scaling factor for BERT layers. To the virtue of the this, the model is able to learn the task-specific representation of the input tweet by coordinating among the layers and between the tasks. \\ % Contributions: %  
"," Existing studies on using social media for deriving mental health status of users focus on the depression detection task. However, for case management and referral to psychiatrists, healthcare workers require practical and scalable depressive disorder screening and triage system. % for prevention or treatment of severe consequences.  This study aims to design and evaluate a decision support system  to reliably determine the depressive triage level by capturing fine-grained depressive symptoms expressed in user tweets through the emulation of Patient Health Questionnaire-9 \texttt{} that is routinely used in clinical practice. %However, the 280-character limit on tweets incentivizes the usage of creative artifacts in the utterances.  %Figurative language forms a general fabric of communication as it permits users to express themselves more effectively.  %Unfortunately, this complicates the reliable detection of depressive symptoms.  The reliable detection of depressive symptoms from tweets is challenging because the 280-character limit on tweets incentivizes the use of creative artifacts in the utterances and figurative usage contributes to effective expression.   We propose a novel BERT based robust multi-task learning framework to accurately identify the depressive symptoms using the auxiliary task of figurative usage detection. Specifically, our proposed novel task sharing mechanism, co-task aware attention\/, enables automatic selection of optimal information across the BERT layers and tasks by soft-sharing of parameters. Our results show that modeling figurative usage can demonstrably improve the model's robustness and reliability for distinguishing the depression symptoms. %Furthermore, our approach achieves statistically significant improvements over the SOTA models.  % Social media platforms have evolved as a vital source of information for mental-health studies, where the users exchange their emotional states and impressions. Majority of the existing studies on depression focus mainly on the depression detection task. However, for healthcare workers to have real-time access to resources for case management and referral to medical/psychiatric treatment, it is necessitate to enable practical, scalable, and sustainable depressive disorder screening, triage, and prevention/treatment interventions. This study aims to design and evaluate a decision support system  to % determine the depressive triage level by capturing fine-grained depressive symptoms appearing in depressed users tweets through emulating the clinically adopted Patient Health Questionnaire-9 \texttt{\texttt{PHQ-9}}.\\ % Nevertheless, the limitation on characters imposed by Twitter incentivize the usage of creative artifacts that are widely observed in the utterance of depressive users. Figurative language, such as metaphor, irony, and sarcasm forms a general fabric of communication as it permit users to express their health condition more memorably, concisely, and effectively. Inspired by that, we proposed a novel BERT based multi-task learning framework that learns to accurately identify the symptoms using the auxiliary task of figurative language detection. Specifically, we propose a new task sharing mechanism: co-task aware attention, which helps the model to borrow the new information across the task. With the help of soft-sharing of the parameters, our framework automatically detect and select optimal information across the layers of the BERT, that are useful for a task at hand. % The obtained results proves that modeling figurative language in depressive user tweets can improve the model learning ability in correctly distinguishing the symptoms. Furthermore, our proposed approach achieve statistically significant improvements over the state-of-the-art models on our primary task.",242
" Coherence refers to the properties of a text that indicate how meaningful sentential constituents are connected to convey document-level meaning. Different theories have been proposed to describe the properties that contribute to discourse coherence and some have been integrated with computational models for empirical evaluation. A popular approach is the entity-based model which hypothesizes that coherence can be assessed in terms of the distribution of and transitions between entities in a text -- by constructing an entity-grid  representation, building on Centering Theory. Subsequent work has adapted and further extended Egrid representations.  Other research has focused on syntactic patterns that co-occur in text or semantic relatedness between sentences as key aspects of coherence modeling. There have also been attempts to model coherence by identifying rhetorical relations that connect textual units or capturing topic shifts via Hidden Markov Models~\cite[HMM,][]{barzilay-lee-2004-catching}. Other work has combined approaches to study whether they are complementary.  More recently, neural networks have been used to model coherence. Some models utilize structured representations of text~\cite[e.g. Egrid representations,][]{Dat2017,Joty2018} and others operate on unstructured text, taking advantage of neural models' ability to learn useful representations for the task.  Coherence has typically been assessed by a model's ability to rank a well-organized document higher than its noisy counterparts created by corrupting sentence order in the original document , and neural models have achieved remarkable accuracy on this task. Recent efforts have targeted additional tasks such as recovering the correct sentence order, evaluating on realistic data and focusing on open-domain models of coherence. However, less attention has been directed to investigating and analyzing the properties of coherence that current models can capture, nor what knowledge is encoded in their representations and how it might relate to aspects of coherence.   In this work, we systematically examine what properties of discourse coherence current coherence models can capture. We devise two datasets that exhibit various kinds of incoherence and analyze model ability to capture syntactic and semantic aspects of text implicated in discourse organisation.  We furthermore investigate a set of probing tasks to better understand the information that is encoded in their representations and how it might relate to aspects of coherence.  We hope this study shall provide further insight into how to frame the task and improve models of coherence assessment further.  Finally, we release our evaluation datasets as a resource for the community to use to test discourse coherence models.   
","  In this work, we systematically investigate how well current models of coherence can capture aspects of text implicated in discourse organisation. We devise two datasets of various linguistic alterations that undermine coherence and test model sensitivity to changes in syntax and semantics. We furthermore probe discourse embedding space and examine the knowledge that is encoded in representations of coherence. We hope this study shall provide further insight into how to frame the task and improve models of coherence assessment further. Finally, we make our datasets publicly available as a resource for researchers to use to test discourse coherence models.",243
"  Early detection of dementia is important for improving clinical outcomes and management of dementia, as well as for lifestyle, financial, and future planning for patients and their caregivers . Yet, dementia is not formally diagnosed or coded in claims for over 50\% of older adults living with probable dementia . Tools that screen medical records for warning signs and present the digested information to providers may prove to be an important step for early intervention.  In this study, we aim to use NLP to detect signs of cognitive dysfunction from clinician notes in electronic health records  by applying deep learning techniques that have not been hitherto applied to this problem. We present an attention-based transformer model that allows for long text sequences to reveal signs of cognitive concerns and compare its performance to baseline models.   
","   Dementia is under-recognized in the community, under-diagnosed by healthcare professionals, and under-coded in claims data. Information on cognitive dysfunction, however, is often found in unstructured clinician notes within medical records but manual review by experts is time consuming and often prone to errors. Automated mining of these notes presents a potential opportunity to label patients with cognitive concerns who could benefit from an evaluation or be referred to specialist care.  In order to identify patients with cognitive concerns in electronic medical records, we applied natural language processing  algorithms and compared model performance to a baseline model that used structured diagnosis codes and medication data only. An attention-based deep learning model outperformed the baseline model and other simpler models.",244
" A spelling corrector is an important and ubiquitous pre-processing tool in a wide range of applications, such as word processors, search engines and machine translation systems. %The popularity of mobile devices makes it increasingly crucial since typing on virtual keyboards is more error-prone. Having a surprisingly robust language processing system to denoise the scrambled spellings, humans can relatively easily solve spelling correction . %spelling correction is a relatively easy task for humans, who have a surprisingly robust language processing system  to denoise the scrambled spellings.  However, spelling correction is a challenging task for a machine, because words can be misspelled in various ways, and a machine has difficulties in fully utilizing the contextual information.   Misspellings can be categorized into non-word, which is out-of-vocabulary, and the opposite, real-word misspellings . The dictionary look-up method can detect non-word misspellings, while real-word spelling errors are harder to detect, since these misspellings are in the vocabulary . In this work, we address the stand-alone  spelling correction problem. It only corrects the spelling of each token without introducing new tokens or deleting tokens, so that the original information is maximally preserved for the down-stream tasks. %\textcolor{red}{[The last few sentences of this paragraph is not good, what are you trying to express?]}  We formulate the stand-alone spelling correction as a sequence labeling task and jointly detect and correct misspellings. Inspired by the human language processing system, we propose a novel solution on the following aspects:  We encode both spelling information and global context information in the neural network.  We enhance the real-word correction performance by initializing the model from a pre-trained language model .  We strengthen the model's robustness on unseen non-word misspellings by augmenting the training dataset with a synthetic character-level noise. As a result, our best model  outperforms the previous state-of-the-art result  by  absolute  score.  %As a result, we present a simple but powerful solution to the stand-alone spelling correction by simply fine-tuning a pre-trained LM to jointly detect and correct both non-word and real-word misspellings  as a sequence labeling task.  %We propose a novel solution by using transformer-encoders  to jointly perform detection and correction of misspellings. We extensively explore various training techniques. Our results show that a transformer-encoder-based architecture that encodes both local character-level and global word-level representations yields a strong performance. Specifically, both the combination of word embedding and character embedding or a subword embedding  produce strong models. We further obtain a state-of-the-art model by initializing the weight from a pre-trained language model   and training it on an augmented training dataset with a synthetic character-level noise. \textcolor{red}{[this paragraph need to rewrite. Please summarize your contribution in a coherent story. ]}  %We also explore additional training techniques such as leveraging a pre-trained language model  and adding more noise to the training corpora. Our results show that fine-tuning a pre-trained LM  with a subword embedding  yields a strong model. Furthermore, we obtain a state-of-the-art model by training it on a noisy corpus synthesized by randomly replacing correct words and characters with natural misspellings and random character. Finally, under the condition of no pre-training, we propose a strong model that outperforms the subword model by combining word and character embedding.  \iffalse We summarize our contributions as follows:  \fi  \iffalse \subsection{Stand-alone Spelling Correction}  Formally, given a noisy input sentence , each noisy word  is drawn from a distribution of possible misspellings of the correct word , where  is a vocabulary. We aim to build a corrector  such that , where  is the correct sentence. %\textcolor{red}{[as I said, this definition do not need on section]} \fi  
"," Existing natural language processing systems are vulnerable to noisy inputs resulting from misspellings.  On the contrary, humans can easily infer the corresponding correct words %\textcolor{red}{the semantics of unknown words:the corresponding correct words of misspellings}  from their misspellings and surrounding context. Inspired by this, we address the stand-alone spelling correction problem, which  %\textcolor{red}{[do not know which refers to what, confusing, please rewrite; at the same time, can you brief introduce your novel solution here?]}  only corrects the spelling of each token without additional token insertion or deletion, by utilizing both spelling information and global context representations. We present a simple yet powerful solution that jointly detects and corrects misspellings as a sequence labeling task by fine-turning a pre-trained language model. Our solution outperform the previous state-of-the-art result by $12.8\%$ absolute $F_{0.5}$ score. %Furthermore, we obtain a state-of-the-art model by augmenting the training data with synthetic character-level noise. %We also provide three useful training techniques. Our results show that a transformer-based model that encodes both local character-level and global word-level representations yields a strong performance. Furthermore, a state-of-the-art model is obtained by leveraging pre-trained language model and augmenting the training corpus with synthetic character-level noises. %fine-tuning a pre-trained language model with a subword embedding yields a strong model. Furthermore, we obtain a state-of-the-art model by training it on a noisy corpus synthesized by randomly replacing correct words and characters with common misspellings and random characters. We also propose a strong architecture that combines character and word level encoder without pre-training.",245
"   % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  .     %      % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. }  Conversational technologies offer a remarkable addition to the current approaches for providing mental healthcare. Communications with these conversational agents have been found to include discoverable psychological distress signs, such as the rate of filled pauses, speech rate, and various temporal and turn-related characteristics . In the human-human automatic analysis of patient-doctor conversations, it has also been found that different types of disfluency can indicate levels of adherence to medication . Markers of disfluency also hold predictive power for the identification of cognitive disorders .  Such devices are mainly used for processing content, which is then analyzed offline. There is much work on detecting disfluencies for offline analysis of transcripts with gold standard utterance segmentation within much of the current effort on disfluency detection on telephone conversations begun by . However, given that these models do not operate for live systems and rely on rich transcription data, including the pre-segmentation of dialogue acts, to facilitate more cost-effective study of other data, it would be easier to be able to perform directly and incrementally off the speech signal, or at least from automatic speech recognition  results as they arrive into the system. The incremental model must work with minimum latency as it receives word-by-word data and does so without modifying its initial assumptions and providing its best decisions as soon as possible in line with the principles set out in .  We combine incremental identification of disfluencies with three other essential tasks for active conversational models to provide a favorable inductive bias to disfluency detection and to study the way these tasks interact. We explore a multi-task learning  framework to enable the training of one universal model with four tasks of disfluency detection, language modelling, part-of-speech  tagging, and utterance segmentation, which in the data we use is also equivalent to dialogue act segmentation. Multi-task learning seeks to improve learning efficiency and predictive power by learning from a shared representation with multiple objectives. We investigate the entire power set of these tasks to investigate the interaction between them. We experiment with two different losses: a naive weighted sum of losses where the weights of loss are uniform and a loss function based on maximizing the Gaussian likelihood with task-dependent uncertainty.  We train and test a simple neural model for the different tasks, experiment with all the combinations of the tasks, different loss functions for each of the tasks, and experiment with different input representations .    
","  We present a multi-task learning framework to enable the training of one universal incremental dialogue processing model with four tasks of disfluency detection, language modelling, part-of-speech tagging, and utterance segmentation in a simple deep recurrent setting. We show that these tasks provide positive inductive biases to each other with the optimal contribution of each one relying on the severity of the noise from the task. Our live multi-task model outperforms similar individual tasks, delivers competitive performance, and is beneficial for future use in conversational agents in psychiatric treatment.",246
" We introduce \diagnnose, an open source library for analysing deep neural networks. The \diagnnose library allows researchers to gain better insights into the internal representations of such networks, providing a broad set of tools of state-of-the-art analysis techniques. The library supports a wide range of model types, with a main focus on NLP architectures based on LSTMs  and Transformers .  Open-source libraries have been quintessential in the progress and democratisation of NLP. Popular packages include HuggingFace's   -- allowing easy access to pre-trained Transformer models; % AllenNLP  -- providing useful abstractions over components in the NLP pipeline,   -- focusing on multitask and transfer learning within NLP;   -- providing a range of feature attribution methods; and   -- a platform for visualising and understanding model behaviour. We contribute to the open-source community by incorporating several \mbox{interpretability} techniques that have not been present in these packages.  Recent years have seen a considerable interest in improving the understanding of how deep neural networks operate . The high-dimensional nature of these models makes it notoriously challenging to untangle their inner dynamics. This has given rise to a novel subfield within AI that focuses on interpretability, providing us a peak inside the black box. \diagnnose aims to unify several of these techniques into one library, allowing interpretability research to be conducted in a more streamlined and accessible manner.  \diagnnose's main focus lies on techniques that aid in uncovering linguistic knowledge that is encoded within a model's representations. The library provides abstractions that allow recurrent models to be investigated in the same way as Transformer models, in a modular fashion. It contains an extensive activation extraction module that allows for the extraction of  model activations on a corpus. The analysis techniques that are currently implemented include:   % <design principles> ?  In this paper we present both an overview of the library, as well as a case study on subject-verb agreement within language models. We first present a brief overview of interpretability within NLP and a background to the analysis techniques that are part of the library . We then provide an overview of \diagnnose and expand briefly on its individual modules . % Next, we provide a more extensive background on the feature attributions that are part of the library . We conclude with a case study on subject-verb agreement, demonstrating several of \diagnnose's features in an experimental setup .  
"," In this paper we introduce \diagnnose, an open source library for analysing the activations of deep neural networks. \diagnnose contains a wide array of interpretability techniques that provide fundamental insights into the inner workings of neural networks. We demonstrate the functionality of \diagnnose with a case study on subject-verb agreement within language models. \diagnnose is available at \url{https://github.com/i-machine-think/diagnnose}.",247
" % % % %  \subsection{Motivation and Problem} % % % % % \GW{Propaganda can be loosely defined as  ``misleading information that is spread deliberately to deceive and manipulate its recipients'' . }% % % % % % % % % % Various factors of propaganda have been studied in the humanities,  including emotionality of language, biased selection of information and deviation from facts, manipulation of cognition, and more . However, there is no consensus on the decisive factors that tell whether a given article or speech is propagandistic  or not. % % % % %  % In the modern digital world, the influence of propaganda on society has drastically increased.  % Hence, there is also a major increase in computer science, computational linguistics and computational sociology research on analyzing, characterizing and, ultimately, automatically detecting propaganda .  To a first degree, one may think of propaganda as a variation of fake news, and  some works investigate propaganda as a refined type of disinformation . % % % % % % % \GW{While false claims can be an element of propaganda, we think that fake news is merely the tip of the iceberg, and that the persuasive and manipulative nature of propagandistic contents requires deeper approaches.} Classifiers for propaganda detection need to better capture how propaganda is expressed in subtle ways by language style and rhetoric or even demagogic wording. This holds for news as well as social media posts and speeches. In all these cases, correct information may be presented in incomplete form or placed in distorted contexts, along with manipulative phrases, in order to mislead the audience.  % % % % Prior work has mostly looked into news articles  and tweets, and has typically focused on  strongly polarized topics like the 2016 US election and the related Russian Internet Research Agency  affair, the UK Brexit discussion, or political extremism. % % % % % % \LQ{All these approaches consider propaganda detection as a classification task assuming sufficient amounts of labeled in-domain training data.} \LQ{For example, in the ``Hack the News'' datathon challenge, a large number of news articles  and sentences  from such articles were annotated by distant supervision and human judgment, respectively, to train a variety of machine learning methods.} % % The resulting F1 scores on the leaderboard of this benchmark are amazingly high, around 90\%. This may give the impression that propaganda detection is a solved problem. However, most of the positively labeled samples are simple cases of ``loaded language'' with strong linguistic cues independent of the topic. Moreover, the learned classifiers % % benefit from ample training data, which is all but self-guaranteed in general.   % % % % %  % In this paper, we question these prior assumptions, hypothesizing  that propagandistic sources and speakers are sophisticated and creative and will find new forms of deception evading the trained classifiers. % % % \GW{The overall approach is still text classification; the novelty of our approach lies in cross-domain learning, where domains denote different kinds of sources, such as news articles vs.\ social media posts vs.\ public speeches.} We acknowledge that there is often a shortage of perfectly fitting labeled data, and instead tap into alternative sources that require a transfer step. Specifically, we consider speeches and tweets, in addition to news articles, at both article and sentence levels.  % \subsection{Approach and Contribution} Our goal is to build more general propaganda detectors, which can leverage different kinds of data sources. In particular, we tap on political speeches of notorious  propagandists, such as Joseph Goebbels . As it is very difficult  to label speeches and their sentences in a binary manner,  we pursue a pairwise ordinal approach where training data merely ranks samples of a strongly propagandistic speaker against those of a relatively temperate speaker. We investigate to what extent models learned from such data can be transferred to classifying news and tweets, and we also study the inverse direction of learning from news and tweets to cope with speeches.  % % % % % % %  Figure illustrates our framework towards  generalizable propaganda detection that overcomes the bottleneck of directly applicable training labels and instead leverages cross-domain learning. %      % % % %   The salient contributions of this paper are as follows.       \newcommand{\myparagraph}[1]{{#1}.~} \newcommand{\var}[1]{\mbox{#1}} \newcommand{\svar}[1]{\mbox{\scriptsize#1}} \newcommand{\mycaption}[1]{}} \newcommand{\metric}[1]{{\mbox{#1}}} \newcommand{\Pat}{\metric{P}} \newcommand{\Patk}[1]{\mbox{\Pat@}} \newcommand{\Ratk}[1]{\mbox{\metric{R}@}} \newcommand{\gender}{``''} \newcommand{\age}{``''} \newcommand{\credit}{``''} \newcommand{\asset}{``''} \newcommand{\rcity}{``''}   
","  As news and social media exhibit an increasing amount of manipulative polarized content, detecting such propaganda has received attention as a new task for content analysis. Prior work has focused  % on supervised learning with training data from the same domain. However, as propaganda can be subtle and keeps evolving, manual identification and proper labeling are very demanding. As a consequence, training data is a major bottleneck.   In this paper, we tackle this bottleneck and present an approach to leverage cross-domain learning, based on labeled documents and sentences from news and tweets, as well as political speeches with a clear difference in their degrees of being propagandistic. We devise informative features and build various classifiers for propaganda labeling, using cross-domain learning.  % % % Our experiments demonstrate the usefulness of this approach, and identify difficulties and limitations in various configurations of sources and targets for the transfer step. We further analyze the influence of various features, and characterize salient indicators of propaganda. %",248
" \looseness=-1 Neural machine translation  architectures~ make it difficult for users to specify preferences that could be incorporated more easily in statistical MT models  and have been shown to be useful for interactive machine translation~ and domain adaptation~. Lexical constraints or preferences have previously been incorporated by re-training NMT models with constraints as inputs~ or with constrained beam search that drastically slows down decoding~.  \looseness=-1 In this work, we introduce a translation model that can seamlessly incorporate users' lexical choice preferences without increasing the time and computational cost at decoding time, while being trained on regular MT samples. We apply this model to MT tasks with soft lexical constraints. As illustrated in Figure, when decoding with soft lexical constraints, user preferences for lexical choice in the output language are provided as an additional input sequence of target words in any order. The goal is to let users encode terminology, domain or stylistic preferences in target word usage, without strictly enforcing hard constraints that might hamper NMT's ability to generate fluent outputs.    Our model is an Edit-Based TransfOrmer with Repositioning , which builds on recent progress on non-autoregressive sequence generation~. Specifically, the Levenshtein Transformer~ showed that iteratively refining output sequences via insertions and deletions yields a fast and flexible generation process for MT and automatic post-editing tasks. \modelname replaces the deletion operation with a novel reposition operation to disentangle lexical choice from reordering decisions. As a result, \modelname exploits lexical constraints more effectively and efficiently than the Levenshtein Transformer, as a single reposition operation can subsume a sequence of deletions and insertions. To train \modelname via imitation learning, the reposition operation is defined to preserve the ability to use the Levenshtein edit distance~ as an efficient oracle. We also introduce a dual-path roll-in policy which lets the reposition and deletion models learn to refine their respective outputs more effectively.  \looseness=-1 Experiments on Romanian-English, English-German, and English-Japanese MT show that \modelname achieves comparable or better translation quality with faster decoding speed than the Levenshtein Transformer  on the standard MT tasks and exploit soft lexical constraints better: it achieves significantly better translation quality and matches more constraints with faster decoding speed than the Levenshtein Transformer. It also drastically speeds up decoding compared to lexically constrained decoding algorithms~. Furthermore, results highlight the benefits of soft constraints over hard ones \---\ \modelname with soft constraints achieves translation quality on par or better than both \modelname and Levenshtein Transformer with hard constraints~.      
"," We introduce an Edit-Based TransfOrmer with Repositioning , which makes sequence generation flexible by seamlessly allowing users to specify preferences in output lexical choice. Building on recent models for non-autoregressive sequence generation, \modelname generates new sequences by iteratively editing hypotheses. It relies on a novel reposition operation designed to disentangle lexical choice from word positioning decisions, while enabling efficient oracles for imitation learning and parallel edits at decoding time. Empirically, \modelname uses soft lexical constraints more effectively than the Levenshtein Transformer while speeding up decoding dramatically compared to constrained beam search. \modelname also achieves comparable or better translation quality with faster decoding speed than the Levenshtein Transformer on standard Romanian-English, English-German, and English-Japanese machine translation tasks.",249
"  Word embeddings like Word2Vec or Glove can learn context-sensitive vector representations of words from very large corpora. These representations have proven useful for supervised tasks like language translation, entity recognition, sentiment analysis, or question answering.  The more general problem of tracking the semantic change of words through time has initially been addressed by a number of works, either by connecting several static embeddings through mapping transformations, or by initializing the training of each slice with the results from the previous one in the Word2Vec case . More recent works can deal with all the temporal slices simultaneously, as in Bamler and Mandt, Rudolph and Blei, and Yao et. al.  These works link the slices either by a diffusion process, a random walk, or a regularization term in the cost function.  The proposed approach does not assume any sequentiality in the slices .  By combining data from different sources, these embeddings can help to understand not only semantic drift, but also cross-cultural differences  or dialect variations .  In this work we consider that each corpus is divided into a set of segments called slices. All the slices are trained simultaneously, following the Word2Vec distributional hypothesis, with the addition that each word vector representation inside a slice is obtained by adding a central representation and a slice-dependent one. Thus, the different representations of one same word across different slices are tied by a common component. This constraint can be depicted as a star-like graph. Figure shows this representation for two cases:  a newspaper corpus through covering several years, and  a multi-source corpus combining two English-language newspapers.      The rest of the paper is organized as follows.  Section introduces the proposed model, giving its formal description, vocabulary selection and implementation details. Section describes the datasets used for this work: two corpora from The New York Times and The Guardian newspapers, and a curated multi-source corpus that combines both of them.   Section provides experimental work on the three datasets, and their corresponding quantitative and qualitative analysis. The related work is detailed in Section.  Finally, our conclusions and future work are discussed in Section.    
"," There is an increasing interest in the NLP community in capturing variations in the usage of language, either through time , across regions  or in different social contexts . Several successful dynamical embeddings have been proposed that can track semantic change through time.  Here we show that a model with a central word representation and a slice-dependent contribution can learn word embeddings from different corpora simultaneously. This model is based on a star-like representation of the slices. We apply it to The New York Times and The Guardian newspapers, and we show that it can capture both temporal dynamics in the yearly slices of each corpus, and language variations between US and UK English in a curated multi-source corpus. We provide an extensive evaluation of this methodology.",250
" The goal of relation extraction is to extract relationships between two entities from plain text. Supervised learning methods for relation extraction have been widely used to extract relations based on training labeled data. Distant supervision or crowdsourcing have been used to collect more examples with labels and train the model for relation extraction. However, these methods are limited by the quantity  and quality  of the training data because manually labeling the data is time-consuming and labor-intensive and data labeled by distant-supervision is noisy. To overcome the problem of insufficient high-quality data, few-shot learning have been designed to require only few labeled sentences for training. A lot of research has been done on few-shot learning for computer vision~, and some work also includes few-shot learning methods for relation extraction~. Although these works only require few instances for training, they still do not work in many scenarios in which no training instances are available.  Some work on open information extraction  discovers new relationships in open-domain corpora without labeling the data. OpenIE aims to extract relation phrases directly from text. However, this technique can not effectively select meaningful relation patterns and discard irrelevant information. In addition, this technique can not discover relations if the relation's name does not appear in the given sentence. For example, OpenIE can not identify the relation of the sentence shown in Figure.  To address the aforementioned limitations, we focus on relation extraction in the context of zero-shot learning. Zero-shot learning  is similar to the way humans learn and recognize new concepts. It is a novel learning technique that does not use any exemplars of the unseen categories during training. We propose a zero-shot learning model for relation extraction , which focuses on recognizing new relations that have no corresponding labeled data available for training. ZSLRE is modified on prototypical networks utilizing side  information.  We construct side information from labels and its synonyms, hypernyms of two name entities and keywords from training sentences. The ZSL-based model can recognize new relations based on the side information available for it instead of using a collection of labeled sentences. We incorporate side information to enable our model to extract relations that never appear in the training datasets. We also build an automatic hypernym extraction framework to help us acquire hypernyms of different entities directly from web. Details of side information construction are described in Section Side Information Extraction.     Figure shows an example of how side information can be used for extract relations. Different side information are given for different relations. The query sentence in the example has a relation of classmate\_of, but the word classmate never appears in the sentence. We first get the two name entities Nell Newman and Mayday Parker of the sentence and extract the hypernyms of the name entities person and person based on our proposed hypernym extraction module in Section Hypernyms Extraction. In this example, relationship capital\_of is eliminated because the hypernyms of capital\_of should be location and location. Then we extract the keywords course and school from the query sentence and compare the distance with the keywords in side information box.  In this way, relationship children\_of is eliminated.  To make relation extraction effective in real-world scenarios, we design our models with the ability that it can extract both relations with training instances and the relations without any training instances.  We modify the vanilla prototypical networks to deal with both scenarios and compare the distance between the query sentence and the prototype. If the exponential of the minus distance is above a threshold, we consider the query sentence has a new relation. For new relations extraction, we take the side information embedding from the query sentence and compare the distance of it with the side information embedding of new relations. We conduct different experiments on both a noisy and a clean dataset and adding different percentages of new relations to evaluate the effectiveness and robustness of our proposed model. Besides, we also evaluate our proposed model in supervised learning, few-shot learning and zero-shot learning scenarios and the results show that our proposed model outperforms other existing models in all three scenarios. The contributions of this paper can be summarized as follows:    The rest of this paper is organized as follows. Section Related Work reviews work on supervised relation extraction, open relation extraction and zero-shot learning.  Section Methodology describes the proposed ZSLRE model. Section Experiments presents the experiments and compares the performance of our model with other different models on two public datasets. Section Conclusion and Future Work includes a discussion of conclusion and promising future work.  
"," Most existing supervised and few-shot learning relation extraction methods have relied on labeled training data. However, in real-world scenarios, there exist many relations for which there is no available training data. We address this issue from the perspective of zero-shot learning  which is similar to the way humans learn and recognize new concepts with no prior knowledge. We propose a zero-shot learning relation extraction  framework, which focuses on recognizing novel relations that have no corresponding labeled data available for training. Our proposed ZSLRE model aims to recognize new relations based on prototypical networks that are modified to utilize side  information. The additional use of side information allows those modified prototype networks to recognize novel relations in addition to recognized previously known relations. We construct side information from labels and their synonyms, hypernyms of name entities, and keywords. We build an automatic hypernym extraction framework to help get hypernyms of various name entities directly from web. We demonstrate using extensive experiments on two public datasets  that our proposed model significantly outperforms state-of-the-art methods on supervised learning, few-shot learning and zero-shot learning tasks. Our experimental results also demonstrate the effectiveness and robustness of our proposed model in a combination scenario. Once accepted for publication, we will publish ZSLRE's source code and datasets to enable reproducibility and encourage further research.",251
"   Unlabeled data has been leveraged in many ways in natural language processing including  back-translation~, self-training~, or language model pre-training which led to improvements in many natural language tasks~. While pre-training has achieved impressive results on tasks where labeled data is limited, improvements in settings with abundant labeled data are modest~ with controlled studies showing a clear trend of diminishing returns as the amount of training data increases~.  In this paper, we focus on noisy channel modeling for text generation tasks, a classical technique from the statistical machine translation literature which had been the workhorse of text generation tasks for decades before the arrival of neural sequence to sequence models~. Unlike pre-training approaches, this approach is very effective irrespective of the amount of labeled data: since a recent revival~, it has been an important part in the winning entries of several high resource language pairs at WMT 2019~, improving over strong ensembles that used 500M back-translated sentences.  At the low resource WAT 2019 machine translation competition, noisy channel modeling was also a key factor for the winning entry~.  Noisy channel modeling turns text generation on the head: instead of modeling an output sequence given an input, Bayes' rule is applied to model the input given the output, via a backward sequence to sequence model which is combined with the prior probability of the output, typically a language model.  This enables the effective use of strong language models trained on large amounts of unlabeled data.  The role of the backward model, or the channel model, is to validate outputs preferred by the language model with respect to the input.  A straightforward way to use language models is to pair them with standard sequence to sequence models~. However, this does not address explaining away effects under which modern neural sequence models still suffer~. As a consequence, models are susceptible to producing fluent outputs that are unrelated to the input~. The noisy channel approach explicitly addresses this via the channel model.   However, a major obstacle to efficient noisy channel modeling is that generating outputs is much slower than decoding from a standard sequence to sequence model. We address this issue by introducing several simple yet highly effective approximations which increase the speed of noisy channel modeling by an order of magnitude to make it practical. This includes smaller channel models as well as scoring only a subset of the channel model vocabulary.  Experiments on WMT English-Romanian translation show that noisy channel modeling can outperform recent pre-training results. Moreover, we show that noisy channel modeling benefits much more from larger beam sizes than strong pre-training methods.    
"," Pre-training models on vast quantities of unlabeled data has emerged as an effective approach to improving accuracy on many NLP tasks. On the other hand, traditional machine translation has a long history of leveraging unlabeled data through noisy channel modeling.  The same idea has recently been shown to achieve strong improvements for neural machine translation. Unfortunately, na\""{i}ve noisy channel modeling with modern sequence to sequence models is up to an order of magnitude slower than alternatives.  We address this issue by introducing efficient approximations to make inference with the noisy channel approach as fast as strong ensembles while increasing accuracy. We also show that the noisy channel approach can outperform strong pre-training results by achieving a new state of the art on WMT Romanian-English translation.",252
"  % Sentiment analysis is a text classification technique that analyses a given text and returns the nature of the underlying opinion. Therefore, sentiment analysis is widely used for tasks such as brand monitoring, political research analysis, product analysis, workforce analysis and many more. Sentiment analysis techniques could be fundamentally sub divided into two categories as lexicon-based approach and machine learning based approach. Recently introduced deep learning based sentiment analysis techniques have outperformed the lexicon based approaches and traditional machine learning approaches.  With the development of deep learning techniques such as Convolutional Neural Networks , Recurrent Neural Networks  and language independent features, the domain of sentiment analysis has reported impressive results. Over the years, many of these variants and combinations of deep learning techniques and feature representations have been used for high resourced languages such as English. There also exist certain advancements in sentiment analysis for languages such as Chinese, Arabic, Spanish and some Indic languages.   Sinhala, which is a morphologically rich Indo-Aryan language, has not experienced these advancements due to its insular and under-resourced nature. One of the main challenges is not having large enough annotated corpora. The data set from~\citet{liyanage2018sentiment} is the only publicly  available annotated data set for sentiment analysis. However it includes only 5010 comments extracted from one news source, and contains only POSITIVE and NEGATIVE samples.  %Work of~\citet{medagoda2017framework} is an example of simple solutions for Sinhala sentiment analysis. Under these approaches, rule-based techniques, lexicon based techniques, supervised and semi-supervised machine learning techniques were employed with traditional language dependent features.   The 閾夸购st experiment on using deep learning techniques for Sinhala sentiment analysis was conducted by~\citet{liyanage2018sentiment}. Under this research, basic deep learning techniques such as Long Short-Term Memory  network and CNN were used to categorize news comments as POSITIVE and NEGATIVE. %The LSTM trained with fastText embeddings outperformed traditional machine learning techniques such as Decision Tree, SVM, and Na\""ive Bayes. ~\citet{DemotteSLSTM2020Sinhala} conducted an experiment with the same data set using Sentence-State LSTM , which is a rather advanced technique where the analysis was further improved considering the n-gram features of text with word embeddings.  In this paper, we present a more comprehensive empirical study on the use of deep learning techniques for document-level sentiment analysis for Sinhala with respect to four sentiment categories as POSITIVE, NEGATIVE, NEUTRAL and CONFLICT. The experiments were conducted with the commonly used sequence models such as RNN, LSTM, Bi-LSTM, various improvements on these vanilla models such as stacking and regularization,  as well as more recent ones such as hierarchical attention hybrid neural networks and capsule networks. % for multi-class sentiment analysis using word embeddings as language independent features. These langauge independent features were able to outperform the usage of traditional language dependent features such as part of speech tagging and lexical resources.  ~Furthermore, we present a data set of 15059 comments, annotated with these four classes to be used for sentiment analysis, based on Sinhala news comments extracted from online newspapers namely GossipLanka and Lankadeepa. This is the only publicly available multi-class, multi-source dataset for Sinhala sentiment analysis.  Our code implementation, word embedding models, and annotated data set are publicly available.       % 
"," Due to the high impact of the fast-evolving fields of machine learning and deep learning, Natural Language Processing  tasks have further obtained comprehensive performances for highly resourced languages such as English and Chinese. However Sinhala, which is an under-resourced language with a rich morphology, has not experienced these advancements. For sentiment analysis, there exists only two previous research with deep learning approaches, which focused only on document-level sentiment analysis for the binary case. They experimented with only three types of deep learning models. In contrast, this paper presents a much comprehensive study on the use of standard sequence models such as RNN, LSTM, Bi-LSTM, as well as more recent state-of-the-art models such as  hierarchical attention hybrid neural networks, and capsule networks. Classification is done at document-level but with more granularity by considering POSITIVE, NEGATIVE, NEUTRAL, and CONFLICT classes. A data set of 15059 Sinhala news comments, annotated with these four classes and a corpus consists of 9.48 million tokens are publicly released. This is the largest sentiment annotated data set for Sinhala so far.   % In addition to that,  was extracted from both comments and articles of online newspapers. %Furthermore, the language-independent features such as Word2Vec and fastText were experimented for novel deep learning techniques which clearly indicate the importance of word embedding techniques for NLP tasks including sentiment analysis for Sinhala as a low resource language. % Due to the high impact of the fast-evolving field of machine learning and deep learning, the Natural Language Processing  tasks have further obtained comprehensive and prominent performances over the past few decades. Different variations and combinations of deep learning techniques have been employed for NLP tasks in general. These experiments illustrated highly improved performances with respect to the traditional rule-based and statistical machine learning techniques. These advancements were mainly impacted towards the development of popular languages such as English and Chinese. However, Sinhala which is an under-resourced language with rich morphology, have not experienced these advancements due to fewer resources for NLP tasks. For sentiment analysis, there exist only two previous research with deep learning approaches, which also conducted with less granularity while giving sub optimality with respect to recent advancements in deep learning techniques. In this paper, we present the use of state-of-the-art deep learning approaches such as RNN, LSTM, Bi-LSTM, Hierarchical Attention Hybrid Neural Networks, and capsule networks for multi-class sentiment analysis for Sinhala news comments while considering more granularity. Under this research, we present the multi-class annotated data set which consists of Sinhala news comments extracted from online newspapers. Furthermore, the language-independent features such as word2Vec and fastText were experimented for novel deep learning techniques which clearly indicates the importance of word embedding techniques for NLP tasks including sentiment analysis.",253
" % The very first letter is a 2 line initial drop letter followed % by the rest of the first word in caps. % % form to use if the first word consists of a single letter: % \IAENGPARstart{A}{demo} file is .... % % form to use if you need the single drop letter followed by % normal text : % \IAENGPARstart{A}{}demo file is .... % % Some journals put the first two words in caps: % \IAENGPARstart{T}{his demo} file is .... % % Here we have the typical use of a ""T"" for an initial drop letter % and ""HIS"" in caps to complete the first word.  \IAENGPARstart{T}{he} Neural Machine Translation   has been used to model state-of-the-art translation systems for many high-resource languages . For many language pairs though, the amount and/or quality of parallel data is not enough to train an NMT model whose accuracy can reach an acceptable standard . This category of language pairs is known as low resource. Many works have explored how to use of the easier-to-get monolingual data to improve the quality of translation models in this category of languages -- and even high resource languages -- .  The back-translation has so far been one of the most successful methods , involving the use of the translations of the target language monolingual data to increase the amount of the training data . The additional parallel data consists of authentic sentences in the target language and their translations -- synthetic sentences in the source language -- generated using a reverse  model that is trained on the available parallel data -- see the procedure in Algorithm 1. The approach has proven to be successful at improving the quality of translations in high, middle and low resourced languages . Many studies have shown that the quality of the backward system influences the performance of the ultimate NMT model . In low resource conditions, the available parallel data may not be able to train a standard backward model and the quality of the additional data generated using this model may hurt the quality of the final model. Despite this, the aim of standard back-translation has always been to improve the performance of the target NMT model by providing sufficient training data.  Some previous works have proposed various methods to improve the performance of the backward model during training. These methods include iterative back-translation , transfer learning , self-training  and the training of a bi-directional translation model for both backward and forward translations . Others have tried to mask the deficiencies of the backward model either during inference through generating multiple translations of the same target sentence using sampling to average-out the errors in individual translations  and noising the output of beam search ; or reducing the effects of the errors in the synthetic data when training the forward model through methods such as tagged back-translation  and pre-training and fine-tuning .  We present a hybrid approach that utilizes the monolingual target data to improve both the forward and backward models in back-translation. In this approach, we used the synthetic data to enhance the backward model through self-learning and the standard back-translation for improving the forward model. The approach was preliminary investigated in  and it was shown to achieve positive results. Earlier use of stand-alone self-training in machine translation proposed extra methods of either using quality estimation  or freezing of the decoder weights  when training on the synthetic side of the training data. It was suggested that the mistakes in the synthetic data will hurt the performance of the self-trained model . Instead,  showed that self-training is capable of improving the quality of the backward model even without using either of the specialized approaches. It was shown that using all of the synthetic data generated by the backward model to help in re-training the backward model improved its performance. The work, though, did not show the benefits or otherwise of using any of the specialized approach in cleaning the data, especially in low resource languages. It also did not investigate if the model can continue to learn from its output through iterating the self-learning process.   \end{table}  This work, therefore, investigates the effects of synthetic data cleaning using automatic quality estimation when training the backward model. We observed that while the approach may improve the backward model, selecting only a subset of the synthetic data may result in a superior but less generic model. We then investigated the use of iterative self-training with quality estimation as proposed in , enabling the backward model to be trained on all the monolingual data. For low resource languages, readily available quality estimation systems or the data to train such systems may not be available. This may limit the implementation of the approach.  We, therefore, proposed a novel iterative approach that relies only on all the available monolingual target data to improve the backward model before finally generating a much improved synthetic data for the forward model's training. Experimental results show that our approach is superior over the standard back-translation and the approach proposed in ; and that our iterative approach is superior to the iterative back-translation while also requiring less number of models to be trained.  We thus make the following contributions in this paper: \renewcommand{\labelitemi}{\textbullet}   The remainder of this paper is organized as follows: In Section , we reviewed the related works. We presented the proposed methods in Section . We reported the experiments and results in Section . We discussed the results and findings of the research work in Sections  and  respectively and, finally, the paper was concluded and directions for future work were proposed in Section .  
"," %\boldmath Many language pairs are low resource, meaning the amount and/or quality of available parallel data is not sufficient to train a neural machine translation  model which can reach an acceptable standard of accuracy. Many works have explored using the readily available monolingual data in either or both of the languages to improve the standard of translation models in low, and even high, resource languages. One of the most successful of such works is the back-translation that utilizes the translations of the target language monolingual data to increase the amount of the training data. The quality of the backward model which is trained on the available parallel data has been shown to determine the performance of the back-translation approach. Despite this, only the forward model is improved on the monolingual target data in standard back-translation. A previous study proposed an iterative back-translation approach for improving both models over several iterations. But unlike in the traditional back-translation, it relied on both the target and source monolingual data. This work, therefore, proposes a novel approach that enables both the backward and forward models to benefit from the monolingual target data through a hybrid of self-learning and back-translation respectively. Experimental results have shown the superiority of the proposed approach over the traditional back-translation method on English-German low resource neural machine translation. We also proposed an iterative self-learning approach that outperforms the iterative back-translation while also relying only on the monolingual target data and require the training of less models.",254
"   End-to-end techniques for automatic speech recognition , most notably sequence-to-sequence models with attention  and Recurrent Neural Network Transducer  , are becoming increasingly popular. Compared to the traditional hybrid system based on Hidden Markov Model and Deep Neural Network  with individually-trained components, all parts of an end-to-end model are optimized jointly, which often leads to better performance on recognition tasks with sufficient training data and low training-testing mismatch. End-to-end systems are simpler to train; they typically do not require pronunciation lexicons, decision trees, initial bootstrapping, nor forced alignment. End-to-end models are also more suitable for on-device use cases due to the lack of external language models  or decoding graphs, whose sizes can be prohibitively large in hybrid setups because of large vocabulary support, complex LMs, and context-dependent decision trees.  End-to-end systems do have limitations, however. Their end-to-end nature leads to a lack of composability, such as that between acoustic, language, and pronunciation models in hybrid setups. This lack of composability in turn leads to challenges in personalization, which traditionally involves on-the-fly modification of external LMs  to add, boost, and penalize certain words or phrases. Previous work in end-to-end ASR addressed this issue by incorporating external LMs during beam search , with special modifications to handle the model's spiky output . A fundamental limitation of shallow fusion is that it relies on late combination, hence the model needs to have the potential to produce the correct output in the first place without access to biasing information. Another class of method  adds an attention-based  or simple  biasing module over contextual phrases to provide additional signal to the decoder component of end-to-end models. While promising, these methods were shown to have problems scaling to large and highly confusable biasing lists.  A closely related challenge of ASR personalization is entity recognition, since in many cases biasing items are entity names. Rare name recognition presents significant challenges to end-to-end models because of two main reasons. First, the output units of end-to-end models are typically graphemes or WordPieces , both of which do not work well when the spelling of a word does not correspond to how it is pronounced . Second, rare names often decompose into target sequences that are not seen enough in training, making them difficult to recognize correctly. By contrast, both problems are alleviated in hybrid systems due to the use of phonetic lexicons and/or clustered context-dependent acoustic targets. Popular solutions to this problem include upsampling entity-heavy data or generating synthetic training data with names using text-to-speech  . While this method alleviates the data sparsity issue, it does not address the underlying problems of under-trained targets and unconventional spelling of rare names.   In this work, we propose several novel techniques to address both challenges and further improve RNN-T personalization. To alleviate the problem of under-trained targets and recognition of unconventional names, we adopt on-the-fly sub-word regularization  to increase WordPiece coverage during training, perform pre-training  and multi-task learning   to strengthen the encoder, and leverage grapheme-to-grapheme   to generate alternative graphemic pronunciations for names. To address the limitation of shallow fusion relying on late combination, we introduce deep personalized LM  fusion to influence the model's predictions earlier. We show that the combination of these techniques results in 15.4\%--34.5\% relative Word Error Rate  improvement on top of a strong RNN-T baseline which leverages shallow fusion and TTS augmentation. Our final model is also competitive with a hybrid system that has significantly larger disk and memory footprint.  
"," End-to-end models in general, and Recurrent Neural Network Transducer  in particular, have gained significant traction in the automatic speech recognition community in the last few years due to their simplicity, compactness, and excellent performance on generic transcription tasks. However, these models are more challenging to personalize compared to traditional hybrid systems due to the lack of external language models and difficulties in recognizing rare long-tail words, specifically entity names. In this work, we present novel techniques to improve RNN-T's ability to model rare WordPieces, infuse extra information into the encoder, enable the use of alternative graphemic pronunciations, and perform deep fusion with personalized language models for more robust biasing. We show that these combined techniques result in 15.4\%--34.5\% relative Word Error Rate improvement compared to a strong RNN-T baseline which uses shallow fusion and text-to-speech augmentation. Our work helps push the boundary of RNN-T personalization and close the gap with hybrid systems on use cases where biasing and entity recognition are crucial.",255
"  Our goal is to improve information extraction from business documents and contribute to the field of automated document processing. This work leads to a higher success metric and enables less manual work regarding data entry and/or annotation in the industry.  To put the work in context and define the terms closely let's briefly recall the definition of the task, the motivation and add more details.  \paragraph{Information extraction task}  The general problem of information extraction is not a new problem . A survey on information extraction methods  defines the task as: ``Information Extraction starts with a collection of texts, then transforms them into information that is more readily digested and analyzed. It isolates relevant text fragments, extracts relevant information from the fragments, and then pieces together the targeted information in a coherent framework''.  The relevant collection of texts for this study are the texts in business documents such as invoices, pro forma invoices and debit notes. The targeted information is a classification of the texts that helps in automating various business processes 閳 such as automated payment for invoices.  \paragraph{Motivation}  The typical user of our method would be any company medium-sized and bigger because, at some point, companies start to spend significant time on document processing. Details are harder to find in referenced and peer-reviewed works since the companies keep their spending information secret. Approximations from unofficial  sources as  and  lead to an estimate of how a success metric translates to company savings. A typical medium-sized company can have approximately  invoices per month and even just  improvement roughly translates to more than  dollars saving monthly and scales with the company size. Note that this is just a heuristics and thus we do not define the metric exactly.  \paragraph{Details and overview}  As stated, we will focus on business documents. The explicit category of the documents varies. Existing works on information extraction  define these as ``visually rich documents'', ``structured'', or ``semi-structured''.   We will use the name ``structured documents'' throughout this work since the structure of the documents is clear and understandable to a human working in relevant fields, even though the specific structure varies. Moreover, the documents are machine-readable up to the detail of individual words and pictures  on a page, but for a machine, they are not ``understandable'' with respect to the goal of important information extraction.  It is important to classify all of the information that is needed in the financial/accounting industry, for the ``users'' of the documents. For example, the payment details, amount to be paid, issuer information etc. The input is a document's page and the goal is to identify and output all of the words and entities in the document that are considered important, along with their respective classifications.  One example of an input invoice and output extraction can be seen in \prettyref{fig:Example}. As you can see, the documents are not easily understandable inputs. An example of trivial inputs would be an XML document that has the desired target classes incorporated in a machine-readable way.  With this study, we aim to expand previous work , in which we have already shown that neural networks can succeed in the task of extracting important information and even identifying whole, highly specific tables.  As argued before, every improvement matters and so in this work, the focus is on improving the metrics by selecting relevant techniques from the deep learning field. A classical heuristic way to generally improve a target metric is to provide more relevant information to the network. Previously we have exhausted all the information present in a single invoice and so we will focus now on techniques related to ``similarity''. Existing works on similarity are presented in \prettyref{subsec:Inspiration} and our use and notion of similarity is defined here in \prettyref{subsec:The-learning-framework}. In short, we will present a similar annotated document as another input. More details on differences from the previous work are described in \prettyref{subsec:The-differences-to-prev}.  Since the idea of providing more information is fundamental even for simpler templating techniques , we need to stress that, due to the nature of our dataset , our problem cannot be solved by using templates. To prove this statement, a reasonable template-based baseline will be presented  and evaluated .  The research question will focus on a ``similarity'' based mechanism with various model implementations, and whether they can improve an existing solution . The hypothesis is that we are able to create at least one model that can significantly improve the results. Moreover, since the presented mechanism is theoretically applicable beyond the scope of document processing, this work can contribute to a broader audience.  Ultimately we will present a model and its source code  that outperforms the previous state-of-art results. An anonymized version of the dataset is also included as an open-source resource and should be a notable contribution since its size is greater than any other similar dataset known to date.      \subsection{Related works}  This subsection focuses on research on previous works and approaches in the relevant field of information extraction. The text in this subsection is heavily based on the text from .  The plethora of methods that have been used historically for general information extraction is hard to fully summarize or compare. Moreover, it would not be fair to compare methods developed for and evaluated on fundamentally different datasets.  However, we assessed that none of these methods is well-suited for working with structured documents , since they generally do not have any fixed layout, language, caption set, delimiters, fonts... For example, invoices vary in countries, companies and departments, and change in time. In order to retrieve any information from a structured document, you must understand it. Our criterion for considering a method to compare against is that no human-controlled preprocessing such as template specification or layout fixing is required because we aim for a fully automated and general solution. Therefore we will not be including any historical method as a baseline to compare against.  In recent works, a significant number does successfully use a graph representation of a document  and use graph neural networks. Also, the key idea close to the one-shot principle in information extraction is used and examined for example in  and . Both works use notions of finding similar documents and reusing their gold-standards . The latter  applies the principle in the form of template matching without the need for any learnable parameters.  Our approach can also be called ``word classification'' approach as written in , a work where an end-to-end architecture with a concept of memory is explored.  At this point, it is important to clarify the differences between other works and our stream of research .  The most important difference comes from the dataset that is at our disposal. The dataset explored here is far greater than the datasets used elsewhere, and allows for exploring deeper models as opposed to only using graph neural networks. Indeed in our previous paper, we have proven that graph neural networks work in synergy with additional convolution-over-sequence layers and even global self-attention. For clarity, the roles of said layers are described in \prettyref{subsec:Common-architecture}. Moreover, the dataset quality allowed us to discover  that information extraction and line-item table detection targets do boost each other.  As the research is focused on deeper models, we will not be using any of the other works as baselines and the commonly used graph neural networks will be incorporated only as one layer amidst many, with no special focus.  In the following pages, we will explore models that would be able to benefit from access to a known similar document's page. We hope that the model can exploit similarities between documents, even if they do not have similar templates.  \subsection{Broader inspiration}  A broader section on references is provided here since we are using a great variety of layers in the exploration of deep network architectures.  \paragraph{One-shot learning and similarity}  Presented in  is a model design concept that aims to improve models on new data without retraining of the network.   Typically, a classification model is trained to recognize a specific set of classes. In one-shot learning, we are usually able to correctly identify classes by comparing them with already known data. Unlike traditional multi-class classification, one-shot learning allows us to attain better scores even with surprisingly low numbers of samples . Sometimes it can work even for classes that are not present in the training set .  This concept can help in areas ranging from computer vision variants 閳 omniglot challenge   to object detection , finding similar images , face detection , autonomous vision , speech  and also the NLP area .  Among the methods that make one-shot learning able to work, the most fundamental one utilizes the concept of similarity. For similarity to work, we have two types of data 閳 ``unknown'' and ``known''. For the known data, its target values are known to the method and/or to the model. To classify any unknown input, the usual practice is to assign the same class to it as is the class of the most similar known input.  Technically speaking, the architecture  contains a 閳ユ笩iamese閳 part. In particular, both inputs  are passed to the same network architecture with tied weights. We will draw inspiration from this basic principle, and will leave other more advanced methods of one-shot learning  for further research.  Usually due to performance reasons the model is not asked to compare new inputs to every other known input 閳 only to a subset. Therefore, a prior pruning technique needs to be incorporated 閳 for example in the form of the nearest neighbor search in embedding space, as is done for example in the work . Another option would be to incorporate a memory concept  .  The loss used for similarity learning is called triplet loss because it is applied on a triplet of classes  for each data-point:   Where  is a margin between positive and negative classes and  is the model function mapping inputs to embedding space .  Generally speaking, one-shot learning can be classified as a meta-learning technique. For more on meta-learning, we suggest a recent study, like  . Taking the concept one step further yields a concept called ``zero-shot learning'' .   \paragraph{Other sources of inspiration}  It is now beneficial to mention other sources of inspiration that are also meaningfully close to one-shot learning. Since we ask ``what labels are similar in the new data'', a ``query answer'' approach should be considered. Recently, the attention principle  successfully helped to pave the way in language models . It is not uncommon to use attention in one-shot approaches   and also query answer problems in various problems domains .   The mentioned task of similarity can also be approached as pairwise classification, or even dissimilarity .    
"," The automation of document processing is gaining recent attention due to the great potential to reduce manual work through improved methods and hardware. Any improvement of information extraction systems or further reduction in their error rates has a significant impact in the real world for any company working with business documents as lowering the reliability on cost-heavy and error-prone human work significantly improves the revenue. In this area, neural networks have been applied before 闁 even though they have been trained only on relatively small datasets with hundreds of documents so far.  To successfully explore deep learning techniques and improve the information extraction results, a dataset with more than twenty-five thousand documents has been compiled, anonymized and is published as a part of this work. We will expand our previous work where we proved that convolutions, graph convolutions and self-attention can work together and exploit all the information present in a structured document. Taking the fully trainable method one step further, we will now design and examine various approaches to using siamese networks, concepts of similarity, one-shot learning and context/memory awareness. The aim is to improve micro $F_{1}$ of per-word classification on the huge real-world document dataset.  The results verify the hypothesis that trainable access to a similar  page together with its already known target information improves the information extraction. Furthermore, the experiments confirm that all proposed architecture parts  are all required to beat the previous results.  The best model improves the previous state-of-the-art results by an $8.25\,\%$ gain in $F_{1}$ score. Qualitative analysis is provided to verify that the new model performs better for all target classes. Additionally, multiple structural observations about the causes of the underperformance of some architectures are revealed.  All the source codes, parameters and implementation details are published together with the dataset in the hope to push the research boundaries since all the techniques used in this work are not problem-specific and can be generalized for other tasks and contexts.   \keywords{one-shot learning \and information extraction \and siamese networks \and  similarity \and attention}",256
"%Thanh     COLIEE is an annual competition to find automated solutions in  the field of law. This competition is challenging because legal documents are often complex and require a high level of comprehension. The problems in law are even tricky for law experts. COLIEE tasks cover two of the most popular legal systems in the world, Case law and Civil law. COLIEE provides real data from the Canadian judicial system and the Japanese legal system.     COLIEE organizes 4 tasks divided into 2 categories: retrieval and entailment. For retrieval tasks, the systems need to automatically find out the supporting cases of a given query case  or the relevant articles of a given bar question .     For the entailment tasks, the systems need to find the paragraphs in a relevant case that entail a given decision  or to conclude whether the statement of a given question is correct or incorrect .     These tasks can be solved with various text processing methods. On one hand, over the years, systems using only lexical similarity of texts yield inferior performance. On the other hand, deep learning approaches start gaining superior performance recently.       % 
"," We propose deep learning based methods for automatic systems of legal retrieval and legal question-answering in COLIEE 2020. These systems are all characterized by being pre-trained on large amounts of data before being finetuned for the specified tasks. This approach helps to overcome the data scarcity and achieve good performance, thus can be useful for tackling related problems in information retrieval, and decision support in the legal domain. Besides, the approach can be explored to deal with other domain specific problems.    \keywords{Deep Learning \and Legal Text Processing \and Pretrained Legal Text Encoders }",257
"   The dominant paradigm in supervised NLP today is learning from examples, where machine learning algorithms are trained using a large set of task-specific input-output pairs. In contrast, humans learn to perform the same task by reading a description, after which they are able to perform the task in a zero-shot manner---indeed, this is how crowd-sourced NLP datasets are constructed. In this paper, we argue that learning from task descriptions in this way is a necessary attribute of a general purpose NLP system, and we propose it as a new paradigm to train and test NLP systems.    Recent work in NLP has shown significant progress in learning tasks from examples. Large pretrained language models have dramatically improved performance on standard benchmarks and have shown promising results in zero shot prediction by leveraging their language understanding capabilities.   Despite this progress, there are many serious issues that come with learning from examples.  There is an almost infinite number of tasks that a person might wish to solve with a general-purpose NLP system.  Learning to solve these tasks by reading a description instead of observing a collection of examples would solve the problem of having to create training sets for each language task.  Such a system would also be more accessible to practitioners and domain experts in other fields, who could describe their tasks and solve them, opening up new avenues of research where it is expensive or infeasible to gather training data.    Additionally, we find that current supervised learning techniques partly achieve their success due to memorizing uninteresting aspects of the training distribution.  Teaching a system to learn a task from the description alone would alleviate these biases, as new training data would not be needed to learn a novel task.  In this paper, we synthesize prior approaches to zero-shot learning in NLP and provide a formal framework for thinking about the zero-shot prediction problem.  We show that previous zero-shot approaches are limited in both scope of application and rigour of evaluation.  For example, while prior work has used zero-shot prediction for text classification, entity typing, and relation extraction, we push this to the more complex task of slot filling.  We instantiate our formalism in an English language dataset, \dataset , that is formatted similarly to reading comprehension datasets, in that we formulate task descriptions as questions and pair them with paragraphs of text.  We choose this format as it provides a natural way to crowdsource data.  This zero-shot dataset differs from typical reading comprehension datasets, however, in that each task description is paired with twenty different passages, and we evaluate a model's ability to solve the task, not just give the correct answer for a single  pair.  That is, given a question, a model produces some decision function , and it is this function which we comprehensively evaluate on many different inputs.  We also carefully select axes on which to evaluate the generalization of a model to different kinds of task descriptions, changing task descriptions in specific ways to systematically push the field towards more interesting and complex task descriptions.  We evaluate models based on recent state-of-the-art sequence to sequence architectures, which seem most suited to the task of zero shot prediction in this setting.  We find that our best model based on T5  achieves a score of only \finalscore\% on this data, leaving a significant gap to our human performance estimate of \humanestimate\%.  Zero shot learning from complex task descriptions remains a significant challenge for current NLP systems.    
"," Typically, machine learning systems solve new tasks by training on thousands of examples. In contrast, humans can solve new tasks by reading some instructions, with perhaps an example or two. To take a step toward closing this gap, we introduce a framework for developing NLP systems that solve new tasks after reading their descriptions, synthesizing prior work in this area. We instantiate this framework with a new English language dataset, \dataset, structured for task-oriented evaluation on unseen tasks. Formulating task descriptions as questions, we ensure each is general enough to apply to many possible inputs, thus comprehensively evaluating a model's ability to solve each task. Moreover, the dataset's structure tests specific types of systematic generalization.\blfootnote{\textasteriskcentered Work done while at the Allen Institute for AI.} We find that the state-of-the-art T5 model achieves a score of \finalscore\% on \dataset, leaving a significant challenge for NLP researchers.\footnote{Data, evaluation code, baseline models, and leaderboard at \url{https://allenai.org/data/zest}}",258
"  Because of the fact that obtaining   supervised training labels is costly and time-intensive,   and that   unlabeled data is relatively easy to obtain,   semi-supervised learning  , which  utilizes  in-domain  unlabeled data  to improve models trained on the labeled dataset , is of growing interest.  Under the context of large-scale of language model pretraining ,  where a language model is pretrained on an extremely large, open-domain dataset ,   how we can make the best use of the in-domain unlabeled dataset     is poorly understood.   There are basically two ways to take advantages of  the unlabeled, in-domain dataset :   {\bf in-domain pretraining}\footnote{To note,    the pretraining on the in-domain dataset  is distinguished from the pretraining on the large-scale, open-domain dataset largeU.  The model for in-domain pretraining can be randomly initialized or   taking a pretrained model based on the open-domain dataset largeU .}, where a language model is pretrained on   the in-domain dataset  , and then   fine-tuned on ;  {\bf pseudo-label} based approach , where unlabeled data points are assigned with labels predicted by the model trained  on  , forming a new dataset .  A new model  is trained for final predictions by considering .   Many important questions regarding the behavior of semi-supervised learning models under the context of large-scale LM pretraining   remain unanswered:    Is semi-supervised training  still beneficial with the presence of large scale pretraining on largeU?    Should  be used for in-domain LM pretraining or pseudo-label generation?  How should  pseudo-label based semi-supervised models    be  implemented? How different semi-supervised strategies  affect performances regarding  of different sizes, and  of different sizes, etc.    In this paper, we conduct comprehensive studies on the behavior of semi-supervised learning in NLP  with the presence of large-scale language model pretraining.   We use the task of text classification as an example,  the method of which can be easily adapted to different NLP tasks. Our work sheds important lights on the behavior of semi-supervised learning models:  we find that   with the presence of  in-domain pretraining LM on , open-domain LM pretraining   is unnecessary, and we are able to achieve better performance with pretraining on  the in-domain dataset ;    both the in-domain pretraining strategy and the pseudo-label based strategy  lead to significant performance boosts, with the former performing better with larger , the latter performing better with smaller , and the  combination   of both performing the best;  for pseudo-label based strategies,  self-training  yields better performances when  is small, while joint training on the combination of   and  yields better performances when  is large.   Using semi-supervised learning models, we are able to achieve a performance of around  accuracy with only 50 training data points on the IMDB dataset, and   a competitive performance of 96.6 with the full dataset.  More importantly, our work marks an initial step toward understanding the behavior of semi-supervised learning models in the context of large-scale pretraining.    The rest of this paper is organized as follows: related work is detailed in Section 2.  Different strategies for training semi-supervised models are shown in Section 3.  Experimental results and findings are shown in Section 4, followed by a brief conclusion in Section 5.   
"," The goal of semi-supervised learning is to utilize the unlabeled, in-domain dataset $U$ to improve models trained on the labeled dataset $D$.     Under the context of   large-scale language-model  pretraining,   how we  can make the best use of   $U$   is poorly understood:   Is semi-supervised learning still beneficial   with the presence of  large-scale pretraining?  Should $U$ be used for in-domain LM pretraining or pseudo-label generation? How should the pseudo-label based semi-supervised model    be actually implemented? How different semi-supervised strategies  affect performances regarding $D$ of different sizes, $U$ of different sizes, etc.   In this paper, we conduct comprehensive studies  on  semi-supervised learning in the  task of text classification   under the context of  large-scale LM pretraining. Our studies shed important  lights on the  behavior of semi-supervised learning methods.   We find that:    with the presence of  in-domain LM pretraining  on $U$, open-domain LM pretraining \cite{devlin2018bert}  is unnecessary, and we are able to achieve better performance with pretraining on  the in-domain dataset $U$;  both the in-domain pretraining strategy and the pseudo-label based strategy introduce  significant performance boosts,  with the former performing better with larger $U$,  the latter performing better with smaller $U$, and the combination leading to the largest performance gain;   vanilla self-training  yields better performances when $D$ is small, while joint training on the combination of  $D'$ and $D$ yields better performances when $D$ is large.   %We use the task of text classification as an example,  the method of which can be easily adapted to different NLP tasks.  Using semi-supervised learning strategies, we are able to achieve a performance of around $93.8\%$ accuracy with only 50 training data points on the IMDB dataset, and   a competitive performance of 96.6$\%$ with the full  IMDB dataset.  Our work marks an initial step toward understanding the behavior of semi-supervised learning models under the context of large-scale pretraining.\footnote{Code, models and datasets  can be found at https://github.com/ShannonAI/Neural-Semi-Supervised-Learning-for-Text-Classification}",259
"  \todo{Completely rewrite - emphasize that many methods have been proposed for learning embeddings  learn representations of the entities in a knowledge base  typically based on the text of each entity's Wikipedia article or the surrounding local context for mentions of each entity . %  \clm{I would have ""context surrounding mentions of each entity -- otherwise it looks like you're being just redundant and not making it clear this is what you will be calling it henceforth, tho this is just stylistic} context surrounding mentions of each entity Recent advances in neural EL have involved methods for pretraining entity embeddings using the link graph of Wikipedia to learn related entities and words . Similar to word embeddings, past work has shown that these embeddings reside in a high-dimensional pseudo-semantic space, with entities that are close in the space being semantically similar . % \glarionov{""with entities close in the space being...} However, little work has been done to understand what information different entity embeddings capture about the underlying entities and how that information affects downstream performance.  Our goal in this work is to identify semantic information in entity representations and determine how that information is linked to performance on downstream EL tasks. For this, we develop a series of probing tasks, which have previously been used to examine lexical and syntactic properties of neural model layers such as sentence encoders and decoders for neural machine translation systems .  % \glarionov{I would group these two citations at the end for readability} % \ees{for lexical and syntactic properties [this is too split, move this info to before citations]}. We extract structured data about entities using DBpedia and context words from Wikipedia anchor links to create probing tasks designed to evaluate the knowledge-based and distributional semantic contents of different entity embedding models.  We compare five entity embedding methods, first by them on two downstream EL tasks. We then probe the learned embeddings to evaluate what semantic information is important for the downstream tasks and how it is represented by the different models. % \ees{We show a strong relationship between probing task performance and performance on the downstream EL tasks. [too long, break up]} We find that pretrained entity embedding methods are generally more effective at representing distributional and knowledge-based semantic information than models that generate embeddings as a byproduct of training on an EL task. These improved representations lead to better performance on the EL tasks, with the best model showing high performance on both distributional and knowledge-based semantic tasks. We further find that entity embeddings trained to predict related words and entities in a skipgram-like model are able to learn fine-grained entity type information and specific relationship types between entities without explicitly providing this information.  Our primary contributions with this work are to:   % 1) describe methods for evaluating the semantic information learned by these methods and 2) to\clm{either move the first ""to"" after ""1)"", or delete this one} empirically demonstrate the importance of this information in creating models of entities for use in downstream tasks.\clm{I agree  with Liz you should bullet point this, because you want to highlight your contributions -- easier for reviewers} % \ees{maybe bullet point these two or put 1) .. 2) to make it mad easy to scan and get} Our hope is that this information can provide guidance in developing architectures that better combine explicit structured information with text to improve methods for representing entities that can be used in a variety of downstream tasks, similar to existing word embeddings. Our methods can additionally be used to potentially detect deficiencies in new representation methods and biases of learned attributes through other probing tasks. % and biases of current methods by probing .\clm{You might want to briefly address the means by which it detects bias, otherwise that is a question that could feel unanswered in the reader's head}  
","  \todo{Complete rewrite} Pretrained entity embedding methods have shown strong results in entity linking  systems compared to methods that generate entity representations from text descriptions. Prior work has shown that these embeddings inhabit a pseudo-semantic space, but the semantic information they contain has not been thoroughly explored nor have  they been compared with other representations for differences in information.  We introduce methods for probing learned entity representations for information about their entity types, relationships, and context words using Wikipedia anchors and DBPedia structured data and use them to compare five entity embedding models. We show that improved representation of all types of semantic information is linked to improved performance on two downstream EL tasks. Our results provide potential directions for further research to better incorporate explicit semantic information into neural entity linking models.",260
" 	In this section, we mention different tokenization techniques for SLT and explain our perspective on the problem. We mentioned about the basics of SLT and NMT. From our research perspective, NMT methods can provide successful results if we have good tokens from SL vides. Therefore, tokenization is seen as the most crucial part of this research. Firstly, the visual properties are involved in the tokenization part. Secondly, there is no a generic approach to obtain strong tokens for all the SLs. In addition to that, it is not clear that discrete tokens should be obtained for better translation quality. For this reason, we extend the meaning of tokenization for NSLT and it covers the overall process to prepare the frames for the NMT module. 	\par For spoken to spoken languages, we generally use words as tokens to feed the NMT module. The current state-of-the-art method converts those tokens to continuous embeddings to reach a semantic representation. While learning translation, the word embedding is also trained to learn the relationship between words. Eventually, a meaningful embedding is obtained before the NMT module as seen in Figure . Based on this, it may be a good idea to learn a good representation of signs to replace with word embeddings to achieve the same advancements in NSLT as NMT has done. This representation is cross-lingual; but learning it is an open problem. Our research is mainly focused on this problem. Before introducing our approach, we discuss the existing three tokenization approaches in the following subsection.   	 \subsection{Input Tokenization in NSLT}  \par  The first approach is using glosses as tokens. Glosses are intermediate word-like representations between signs and words in sentences. Therefore, they can be directly applicable to the NMT framework without any further effort. However, there are certain shortcomings in this method. Firstly, glosses rarely exist in real life. Gloss annotation requires a laborious process and special expertise. Secondly, glosses are unique to SLs. Therefore, each SL requires special effort to obtain glosses whereas sentences are commonly available. The last drawback is that a mistake in the gloss level can produce dramatic meaning differences in translation, since glosses are high level annotations, similar to words.  \par The second approach is the same as the first one in terms of tokens. On top of that, this approach learns to extract glosses from frames. In other words, this method uses glosses as explicit intermediate representations as seen in Figure . It eliminates the further search for tokenization, but it needs a special network for frame to gloss conversion. There are two main concerns. The first one is that a network for frame to gloss conversion is still dependent on gloss annotations. The second is that it is not clear that glosses are the upper bound for SLT as there is not sufficient evidence. The problem is immature and the result in  provides clues about whether glosses may restrict translation quality.    The third approach is called frame-level tokenization. This approach does not establish any explicit intermediate representation as seen in Figure . It aims to learn good sign embeddings to replace with word-embeddings. However, there is no golden way to represent signs with embeddings to feed into the NMT module. Furthermore, it is not clear what the length of the embedding should be. Embeddings can be obtained from each frame or extracted from inner short clips in the video. In addition to that, the representation can be learned with sentence-video pairs or trained outside the NSLT system. There are several ways for frame-level tokenization. However, the main difference from the gloss level tokenization is that discrete representation can be eliminated. If we find a proper one, there would be several advantages. The first one is that the resulting framework can be applied to any SL translation task without requiring annotation. The second advantage is the opportunity to inject additional supervision. The representations would be trained on different tasks and different datasets whereas gloss level tokenization cannot cover different SLs. The third one is that the token length can be adjusted. To boost translation speed, the number of tokens can be reduced to a pre-determined number.     
"," In this thesis, we propose a multitask learning based method to improve Neural Sign Language Translation  consisting of two parts, a tokenization layer and Neural Machine Translation . The tokenization part focuses on how Sign Language  videos should be represented to be fed into the other part. It has not been studied elaborately whereas NMT research has attracted several researchers contributing enormous advancements. Up to now, there are two main input tokenization levels, namely frame-level and gloss-level tokenization. Glosses are world-like intermediate presentation and unique to SLs. Therefore, we aim to develop a generic sign-level tokenization layer so that it is applicable to other domains without further effort. \par We begin with investigating current tokenization approaches and explain their weaknesses with several experiments. To provide a solution, we adapt Transfer Learning, Multitask Learning and Unsupervised Domain Adaptation into this research to leverage additional supervision. We succeed in enabling knowledge transfer between SLs and improve translation quality by 5 points in BLEU-4 and 8 points in ROUGE scores. Secondly, we show the effects of body parts by extensive experiments in all the tokenization approaches. Apart from these, we adopt  3D-CNNs to improve efficiency in terms of time and space. Lastly, we discuss the advantages of sign-level tokenization over gloss-level tokenization. To sum up, our proposed method eliminates the need for gloss level annotation to obtain higher scores by providing additional supervision by utilizing weak supervision sources.",261
" Sentiment polarity detection regarded as one of the significant research problems for opinion extraction in natural language processing . In recent years, the plenteous growth of the internet and the random access of e-devices facilitate the generation of voluminous reviews or opinions in textual form on social media or online platforms. Most of these reviews express the consumers feedback toward the products and services that they received. Several business companies, as well as online marketers, take advantage of these feedbacks to provide praiseworthy services to the consumers. In addition to that customer makes a perfect decision based on the previous reviews before receiving products or services.  Sentiment detection is a computational technique that attempts to uncover the viewpoint of a user towards a specific entity. It aims to identify the contextual polarity of the text contents  as the positive, neutral and negative . Sentiment analysis or detection has shown a remarkable impact in the business community, whereby taking into account the user opinions the communities can ensure the sustainability of their product or services. The restaurant is one such business, where customers opinions can be utilized to improve their quality of foods, environments, and services. Pompous lifestyle and assorted food habits led to a significant increase in the number of people in restaurants. To collect the excellence of services, customers instinct to look through the restaurant reviews before visit it. Therefore, reviewing a restaurant via the internet has become an ecumenical trend. Besides, an abundant amount of positive reviews can make a restaurant as a symbol of faith towards the customers. Also, it can assist a restaurant to reach the pinnacle of success. In contrast, without a sufficient amount of positive reviews, it becomes difficult to gain the attention of new customers by a restaurant. Sometimes, a restaurant with negative reviews loses the trustworthiness of the customers, which turned into reducing the profit.  Straightforwardly, users opinions on specific criteria such as food quality, ambience and service standards of a restaurant can have enough influence on the customers liking. However, it would not be wrong to say that customers inclination or reluctance towards a restaurant depends on the amount of positive and negative reviews. Therefore, the restaurants should appreciate the consents as well as the opinions of the customers. Nevertheless, scrutinizing every reviews one by one is a very time consuming as well as cumbersome task. Further, to govern such surveys, it requires plenty amount of investment in both money and human resources. Considering the fact of the explosive growth of the visitors as well as user preferences, it requires an automatic system that can comprehend the contextual polarity of reviewer opinions posted in different online platforms including Facebook, Twitter, company website, and blogs. Nevertheless, sentiment classification is a challenging research issue in a resource-poor language like Bengali. The inadequacy of benchmark dataset and the limited amount of e-textual contents or reviews in the Bengali language resulted in the sentiment classification task complicated. Deep learning algorithms are very effective to tackle such complications and classify the sentiments correctly . One main advantage of these algorithms are their ability to capture the semantic information in long texts. This paper proposed a deep learning-based sentiment classification technique to classify sentiment form reviews. By taking into consideration the current constraints of sentiment analysis in low resource languages, this paper contributions illustrate in the following:   
"," The amount of textual data generation has increased enor-mously due to the effortless access of the Internet and the evolution of various web 2.0 applications. These textual data productions resulted because of the people express their opinion, emotion or sentiment about any product or service in the form of tweets, Facebook post or status, blog write up, and reviews. Sentiment analysis deals with the process of computationally identifying and categorizing opinions expressed in a piece of text, especially in order to determine whether the writer闁炽儲鐛 attitude toward a particular topic is positive, negative, or neutral. The impact of customer review is significant to perceive the customer attitude towards a restaurant. Thus, the automatic detection of sentiment from reviews is advantageous for the restaurant owners, or service providers and customers to make their decisions or services more satisfactory. This paper proposes, a deep learning-based technique  to classify the reviews provided by the clients of the restaurant into positive and negative polarities. A corpus consists of 8435 reviews is constructed to evaluate the proposed technique. In addition, a comparative analysis of the proposed technique with other machine learning algorithms presented. The results of the evaluation on test dataset show that BiLSTM technique produced in the highest accuracy of 91.35\%.  \keywords{Natural language processing \and Opinion mining \and Embedding features \and Deep learning \and Sentiment classification \and Sentiment corpus.}",262
"  %   Storytelling is a central part of human socialization and entertainment. Many of the popular forms of storytelling throughout history \---such as novels, plays, television, and movies\--- have passive audience experiences. However, gaming is an interesting medium because interactivity is a large part of the entertainment experience, and interactivity and storytelling can often be in conflict: too much player freedom means a storyline may never be explored, while on the other hand, too many restrictions on player freedom risks reducing gaming to a passive medium. Thus, interactivity in storytelling has been an important challenge for gaming, with much design effort put into striking a balance between entertaining gameplay and compelling storytelling.  As gaming technology advances, new opportunities for interactive storytelling present themselves. Better storage technology made telling longer, more intricate stories possible, and better graphical capabilities helped foster more immersive gaming experiences. Advances in artificial intelligence have lead to more challenging opponents, more realistic NPC behavior, and other benefits. Better procedural content generation algorithms help ensure unique gameplay experiences that stay fresh for longer. Finally, recent breakthroughs in language modeling present a new opportunity: language, and thus stories, can potentially be generated on demand.   In this paper, we introduce a novel game of collaborative storytelling, where a human player and an artificial intelligence agent construct a story together. The game starts with the AI agent reciting one of a curated set of story starters \---opening sentences meant to kick-start participants' storytelling creativity\--- and the human player responds by adding a line, which we refer to from here on out as a story continuation, to the story. The AI agent and human player then take turns adding continuations to the story until the human player concludes the story. The game is designed to have a few restrictions as possible and contrasts with traditional storytelling settings where the narrative is fixed in advance.    Collaborative storytelling builds on a rich tradition of collaboration in storytelling that includes Dungeons and Dragons, improvisational comedy, and theater. It could be a useful tool for encouraging creativity and overcoming writer's block, as well as being an entertaining game in its own right.  Our end goal is to make it possible for intelligent agents, such as robot companions and avatars , to play the collaborative storytelling game, as shown in Figure.  %Our supplementary material includes a simulation of such a scenario, including real stories that were constructed by humans collaborating with an Web-only version of our current system\footnote{Stories were edited for brevity.}.  Our primary contributions are as follows:    
","   Storytelling plays a central role in human socializing and entertainment. However, much of the research on automatic storytelling generation assumes that stories will be generated by an agent without any human interaction. In this paper, we introduce the task of collaborative storytelling, where an artificial intelligence agent and a person collaborate to create a unique story by taking turns adding to it. We present a collaborative storytelling system which works with a human storyteller to create a story by generating new utterances based on the story so far. We constructed the storytelling system by tuning a publicly-available large scale language model on a dataset of writing prompts and their accompanying fictional works. We identify generating sufficiently human-like utterances to be an important technical issue and propose a sample-and-rank approach to improve utterance quality. Quantitative evaluation shows that our approach outperforms a baseline, and we present  qualitative evaluation of our system's capabilities.",263
"   The vast amounts of scientific literature can provide a significant source of information for biomedical research. Using this literature to identify relations between entities is an important task in various applications .  Existing approaches to biomedical relation extraction usually fall into one of two categories. Mention-level extraction aims to classify the relation between a pair of entities within a short span of text . In contrast, pair-level extraction aims to classify the relation between a pair of entities across an entire paragraph, document or corpus.  For both mention-level and pair-level relation extraction, recent work has been focused on representation learning. This is considered to be one of the major steps towards making progress in artificial intelligence . Representations of relations which understand their context are particularly important in biomedical research, where identifying fruitful targets is crucial due to the high costs of experimentation. Learning such representations is likely to require large amounts of unsupervised data due to the scarcity of labelled data in this domain.  Recent mention-level methods have been based on using large unsupervised models with Transformer networks  to learn representations of sentences containing pairs of entities. These representations are then used as the inputs to much smaller models, which perform supervised relation classification .  Recent pair-level methods have been based on encoding each mention of a pair of entities, and designing a mechanism to pool these encodings  into a single representation. This representation is then used to classify the relation between the entity pair .  However, representation learning methods for both mention-level and pair-level extraction typically use a point estimate for each representation. As a result, they may struggle to capture the nature of the true, potentially complex relations between each pair of entities. For example, Figure  shows sentences for two entity pairs which demonstrate that relation statements can be very different, typically depending on biological circumstances . Such nuanced relations can be difficult to capture with a single point estimate.  We hypothesise that there is a true underlying relation for each entity pair, and that this relation can be multimodal . The sentences containing each pair are textual observations of these underlying relations.  We therefore propose a probabilistic model which uses a continuous latent variable to represent the true relation between each entity pair. The distribution of a sentence containing that pair is then conditioned on this latent variable. In order to be able to model the complex relations between each entity pair, we use an infinite mixture distribution for the latent representation.  Our model provides a unified architecture for learning representations of relations between entity pairs both at mention and pair level. We show that  the posterior distribution of the latent variable can be used for mention-level relation classification. We also demonstrate that the prior distribution from the same model can be used for pair-level classification. On both tasks, we achieve results competitive with strong baselines with a model which has fewer parameters and is significantly faster to train.  The code is released at \url{ https://github.com/BenevolentAI/RELVM} %.    
","     Extracting biomedical relations from large corpora of scientific documents is a challenging natural language processing task. Existing approaches usually focus on identifying a relation either in a single sentence  or across an entire corpus . In both cases, recent methods have achieved strong results by learning a point estimate to represent the relation; this is then used as the input to a relation classifier. However, the relation expressed in text between a pair of biomedical entities is often more complex than can be captured by a point estimate. To address this issue, we propose a latent variable model with an arbitrarily flexible distribution to represent the relation between an entity pair. Additionally, our model provides a unified architecture for both mention-level and pair-level relation extraction. We demonstrate that our model achieves results competitive with strong baselines for both tasks while having fewer parameters and being significantly faster to train. We make our code publicly available.",264
"   Human communication is inherently multi-modal in nature. Our expressions and tone of voice augment verbal communication.\ This can include vocal features like speaking rate, intonation and visual features like facial expressions . Non-verbal communication is important for tasks that involve higher level cognitive expressions like emotions , persuasiveness  and mental health analysis . We focus on a multi-modal approach to emotion recognition because humans fundamentally express emotions verbally using spoken words , as well as with acoustic signals  and visual expressions .  Getting large-scale labeled datasets for emotion recognition can be challenging.\ Our primary motivation for this paper is to study effective utilization of large unlabeled datasets to improve performance of multi-modal emotion recognition systems.\ The signals we consider are speech, visual information and spoken text.\ Our motivation stems from the popular use of pre-trained models in natural language, speech and visual understanding tasks to circumvent data limitations.\ BERT is a popular model for natural language understanding  that was trained using self-supervision.\ Devlin et al. use the masked language modeling  task on the Wikipedia corpus for pre-training.\ The model was successfully fine-tuned to improve performance on several tasks like question answering and the general language understanding evaluation benchmarks . Self-supervised learning has also been successfully applied to speech based applications.\ Schneider et al.\ in  use unsupervised pre-training on speech data by distinguishing an audio sample in the future from noise samples.\ Fine-tuning this model shows state of the art results on automatic speech recognition . Liu et al.\ show in  that a BERT-like pre-training approach can be applied to speech.\ By predicting masked frames instead of masked words, the performance on tasks like speaker recognition,\ sentiment recognition and phoneme classification can be improved. For emotion recognition, Tseng et al.\ show in  that text-based self-supervised training can outperform state of the art models. The authors use a language modeling task, that involves predicting a word given its context, to pre-train the model.\ Another area of work that has leveraged unlabeled data is detection and localization of visual objects and spoken words in multi-modal input.\ Harwath et al.\ in  train an audio-visual model on an image-audio retrieval task.\ The models are trained to learn a joint audio-visual representation in a shared embedding space.\ This model can learn to recognize word categories by sounds without explicit labels.\ Motivated by the success of these approaches, we study if similar methods can be applied to multi-modal emotion recognition.\ To the best of our knowledge, a joint self-supervised training approach using text, audio and visual inputs has not been well explored for emotion recognition.   Multi-modal emotion recognition models have been well studied in literature and typically outperform uni-modal systems .\ These models need to combine inputs with varying sequence lengths.\ In video, the sequence lengths for audio and visual frames differ from the length of text tokens by orders of magnitude.\ There has been considerable prior work in fusing multi-modal features. Liang et al.\ in  studied multiple fusion techniques for multi-modal emotion recognition and sentiment analysis.\ Their methods included early and late fusion of modalities, and a dynamic fusion graph based network.\ They showed that the graph fusion model outperforms other methods.\ Early fusion and graph fusion techniques both require alignment between various modalities.\ Late fusion can be performed without alignment, but does not allow interaction of features from different modalities at the frame level.\ To overcome this limitation,\ Tsai et al.\ introduce the cross-modal transformer in .\ It scales the features using cross-modal attention.\ In the process, the modalities are projected into sequences of equal lengths, eliminating the need for any alignment.\ This architecture has been successfully applied to problems like emotion recognition, sentiment analysis  and speech recognition .\ Recently, another transformer-based method to combine multi-modal inputs was introduced by Rahman et al. in , which uses a multi-modal adaptation gate.  In this paper, we propose using the same pre-training scheme as BERT, but extend it to a model that uses audio, visual and text inputs. We discuss the relevance of this approach in Section .\ The multi-modal representations learned in pre-training are fine-tuned for emotion recognition.\ We evaluate the efficacy of the pre-training approach.\ We also perform experiments to understand the importance of each modality on the CMU-MOSEI dataset and provide case-studies to interpret the results.   This paper is organized as follows.\ In Section  we describe our model architecture and the self-supervised approach for pre-training, along with further motivation for the self-supervised learning we choose.\ In Section , we discuss the training setup and data.\ We present our results and analysis in Section  and conclude in Section .  
","  Emotion recognition is a challenging task due to limited availability of in-the-wild labeled datasets.\ Self-supervised learning has shown improvements on tasks with limited labeled datasets in domains like speech and natural language.\ Models such as BERT learn to incorporate context in word embeddings, which translates to improved performance in downstream tasks like question answering.\ In this work, we extend self-supervised training to multi-modal applications.\ We learn multi-modal representations using a transformer trained on the masked language modeling task with audio, visual and text features.\ This model is fine-tuned on the downstream task of emotion recognition.\ Our results on the CMU-MOSEI dataset show that this pre-training technique can improve the emotion recognition performance by up to 3\% compared to the baseline.",265
" %  A long desired goal for AI systems is to play an important and collaborative role in our everyday lives.  Currently, the predominant approach to visual question answering  relies on encoding the image and question with a black-box transformer encoder.  These works carry out complex computation behind the scenes but only yield a single token as prediction output . Consequently, they struggle to provide an intuitive and human readable form of justification consistent with their predictions.  In addition, recent study has further demonstrated some unsettling behaviours of those models: they tend to ignore important question terms, look at wrong image regions, or undesirably adhere to superficial or even potentially misleading statistical associations.     To address this insufficiency, we reformulate VQA as a full answer generation task rather than a classification one, i.e. a single token answer. The reformulated VQA task requires the model to generate a full answer with natural language justification. We find that the state-of-the-art model answers a significant portion of the questions correctly for the wrong reasons.  To learn the correct problem solving process,  We propose \modelabbrevname{} , a transparent neural-symbolic reasoning framework that solves the problem step-by-step mimicking humans.     A human would first  \underline{l}ook at the image,  \underline{r}ead the question,  \underline{t}hink with multi-hop visual reasoning,      and finally  \underline{a}nswer the question.      %      Following this intuition, \modelabbrevname{} deploys four neural modules, each mimicking one problem solving step that humans would take:     %      A scene graph generation module first converts an image into a scene graph; A semantic parsing module parses each question into multiple reasoning instructions; A neural execution module  interprets reason instructions one at a time by traversing the scene graph in a recurrent manner and; A natural language generation module generates a full answer containing natural language explanations. The four modules are connected      through hidden states rather than explicit outputs.      Therefore, the whole framework can be trained end-to-end, from pixels to answers.     In addition, since \modelabbrevname{} also produces human-readable      output from individual modules during testing, we can easily     locate the error by checking the modular output.      %      %      Our experiments on GQA dataset show that      \modelabbrevname{} outperforms the state-of-the-art model by a large margin       on the full answer generation task.      Our perturbation analyses by removing relation linguistic cues from questions      confirm that      \modelabbrevname{} makes a step towards truly understanding the question rather than having a smart guess with superficial data correlations.      %      We discuss related work in Appendix A. To summarize, the main contributions of our paper are three-fold:                      %  
","   The predominant approach to visual question answering  relies on encoding the image and question with a ``black-box'' neural encoder and decoding a single token as the answer like ``yes'' or ``no''. Despite this approach's strong quantitative results, it struggles to come up with intuitive, human-readable forms of justification for the prediction process. To address this insufficiency, we reformulate VQA as a full answer generation task, which requires the model to justify its predictions in natural language. We propose LRTA [Look, Read, Think, Answer], a transparent neural-symbolic reasoning framework for visual question answering that solves the problem step-by-step like humans and provides human-readable form of justification at each step. Specifically, LRTA learns to first convert an image into a scene graph and parse a question into multiple reasoning instructions. It then executes the reasoning instructions one at a time by traversing the scene graph using a recurrent neural-symbolic execution module. Finally, it generates a full answer to the given question with natural language justifications. Our experiments on GQA dataset show that LRTA outperforms the state-of-the-art model by a large margin  on the full answer generation task. We also create a perturbed GQA test set by removing linguistic cues  in the questions for analyzing whether a model is having a smart guess with superficial data correlations. We show that LRTA makes a step towards truly understanding the question while the state-of-the-art model tends to learn superficial correlations from the training data.",266
"   Duplicate question detection  is an important application in information retrieval and NLP . It allows systems to recognize when two questions share an answer. This is significant for community forums, such as StackExchange\footnote{https://stackexchange.com/}   to increase their effectiveness in avoiding redundant questions and displaying relevant answers to search questions. It is also important for FAQ retrieval question answering systems .  To learn DQD models for \stackexchange{}, question pairs are usually annotated with duplication information that is extracted from community-provided meta-data. Such annotations are sparse for most domains, e.g., a new \stackexchange{} forum providing support for a new product.  Therefore, leveraging other training signals either from unsupervised data or supervised data from other domains is important .  Pre-trained language models  like BERT  and RoBERTA  are  great unsupervised textual representations. Several recent efforts adapt PLMs for the domains of interest  by  self-supervised fine-tuning on unsupervised domain data, which has shown  to be promising in several scenarios  .  We follow that and tune BERT on \stackexchange{} domains to obtain richer representations for the task of DQD.   Recently,  -nearest neighbors  is applied on the PLM representations for language modeling  and dialogue . We extend this line of study and apply \cdknn{} for  cross-domain generalization in DQD, where the models are trained on data from a source domain, and applied on data from a target domain.  To do so, we represent pairs from source and target in a common representation space and then score target pairs using nearest neighbors in the source pairs. \figref{knnprocess} shows an illustration of this procedure.   % The specific properties of \stackexchange{} DQD % is important to make this approach effective.  Our study on AskUbuntu as target and source datasets of , which include several domains of \stackexchange{} and also Quora and Sprint, reveals that \cdknn{} is more effective compared to cross-entropy classification if  the pair representation space from PLMs is rich for the target domain, i.e., adapted on the unsupervised data  from target or similar domains; or   source and target domains have large distributional shifts.   We make the following contributions:   We present the first study of combining strengths of \cdknn{} and      neural representations for cross-domain generalization in a sentence matching task, i.e., DQD.   Our experimental results  on cross-domain DQD demonstrate that \cdknn{} on      rich question-pair representations advances the results of      cross-entropy classification, especially when shifts in source to target domains is substantial.   
","  Duplicate question detection  is important to increase efficiency of  community and automatic question answering systems.  Unfortunately, gathering supervised data in a domain is time-consuming and expensive, and our ability to leverage annotations across domains is minimal.  In this work, we leverage neural representations and study nearest neighbors for  cross-domain generalization in DQD.   We first encode question pairs of the source and target domain in a rich representation space and then using a k-nearest neighbour retrieval-based method, we aggregate the neighbors' labels and distances to rank pairs. We observe robust performance of this method in different cross-domain scenarios of StackExchange, Spring and Quora datasets, outperforming cross-entropy classification in multiple cases. We will release our codes as part of the publication. % ervised adaptation to StackExchange domains by self-supervised finetuning of contextualized embedding models like BERt. %We show the effectiveness of this adaptation in scenarios when source domain comes from different types of distributions. %Our analysis also reveals that unsupervised domain adaptation on even small amounts of data boosts the performance significantly. %Further, we show how an approach based on nearest neighbors is effective  for this problem and outperforms training the full model using cross entropy.",267
"   Learning vocabulary is a major component of foreign language learning. In the school context, initially vocabulary learning is typically organized around the words introduced by the text book. In addition to the incrementally growing vocabulary lists, some textbooks also provide thematically organized word banks. When other texts are read, the publisher or the teacher often provides annotations for new vocabulary items that appear in the text.  A wide range of digital tools have been developed to support such vocabulary learning, from digital versions of file cards to digital text editions offering annotations.  While such applications serve the needs of the formal learning setting in the initial foreign language learning phase, where the texts that are read are primarily chosen to systematically introduce the language, later the selection of texts to be read can in principle follow the individual interests of the student or adult, which boosts the motivation to engage with the book. Linking language learning to a functional goal that someone actually wants to achieve using language is in line with the idea of Task-Based Language Teaching  as a prominent strand of foreign language education .  Naturally, not all authentic texts are accessible to every learner, but linguistically-aware search engines, such as FLAIR , make it possible to identify authentic texts that are at the right reading level and are rich in the language constructions next on the curriculum. Where the unknown vocabulary that the reader encounters in such a setting goes beyond the around 2\% of unknown words in a text that can be present without substantial loss of comprehension , many digital reading environments provide the option to look up a word in a dictionary. Yet, frequently looking up words in such a context is cumbersome and distracts the reader from the world of the book they are trying to engage with. Relatedly, one of the key criteria of TBLT is that learners should rely on their own resources to complete a task . But this naturally can require pre-task activities preparing the learner to be able to successfully tackle the task . But how can a learner systematically prepare for reading a text or book they are interested in reading?  In this paper, we explore how computational linguistic methods such as distributional semantics, morphological clustering, and exercise generation can be combined with graph-based learner models to answer this question both conceptually and in practice. On the practical side, we developed an application that supports vocabulary learning as a pre-task activity for reading a self-selected book. The conceptual goal is to automatically organize the lexical semantic space of any given English book in the form of a graph that makes it possible to sequence the vocabulary learning in a way efficiently exploring the space and to visualize this graph for the users as an open learner model  showing their growing mastery of the book's lexical space.  Lexical learning is fostered and monitored through automatically generated multi-gap activities  that support learning and revision of words in the contexts in which they occur in the book.  In section we discuss how a book or other text chosen by the learner is turned in to a graph encoding the lexical space that the learner needs to engage with to read the book, and how words that are morphologically related as word families  are automatically identified and compactly represented in the graph . In section we then turn to the use of the graph representation of the lexical semantic space of the book to determine the reader's learning path and represent their growing lexical knowledge as spreading activation in the graph. In section, the conceptual ideas are realized in an application. We discuss how the new learner cold-start problem is avoided using a very quick word recognition task we implemented, before discussing the content selection and activity generation for practice and testing activities. Section then provides a conceptual evaluation of the approach and compares it with related with, before wrapping up with a conclusion in section.  % learning of rare words of English  but what is the purpose? And % the relevance of learning entire frequency bands of words is unclear  % How about combining the goal of reading a book with systematic % learning of what is needed to do so? Problem: Individuals are % interested in different books, and individual differ in language % competence and vocabulary knowledge. So how about the vocabulary of % books organizing themselves individually adaptive organization  % Goal:  %   % Solution: %   
","   How can a learner systematically prepare for reading a book they are   interested in? In this paper, we explore how computational   linguistic methods such as distributional semantics, morphological   clustering, and exercise generation can be combined with graph-based   learner models to answer this question both conceptually and in   practice. Based on the highly structured learner model and concepts   from network analysis, the learner is guided to efficiently explore   the targeted lexical space. They practice using multi-gap learning   activities generated from the book focused on words that are central   to the targeted lexical space. As such the approach offers a unique   combination of computational linguistic methods with concepts from   network analysis and the tutoring system domain to support learners   in achieving their individual, reading task-based learning goals.",268
" Speaking and listening are the most common ways in which humans convey and understand each other in daily conversations. Nowadays, the speech interface has also been widely integrated into many applications/devices like Siri, Google Assistant, and Alexa . These applications use speech recognition-based approaches  to understand the spoken user queries. Like speech, the text is also a widely used medium in which people converse. Recent advances in language modeling and representation learning using deep learning approaches  have proven to be very promising in understanding the actual meanings of the textual data, by capturing semantical, syntactical, and contextual relationships between the textual words in their corresponding learned fixed-size vector representations.   Such computational language modeling is difficult in the case of speech for spoken language understanding because unlike textual words,  spoken words can have different meanings of the same word when spoken in different tones/expressions ,  it is difficult to identify sub-word units in speech because of the variable-length spacing and overlapping between the spoke-words , and  use of stress/emphasis on few syllables of a multi-syllabic word can increase the variability of speech production . Although the textual word representations capture the semantical, syntactical, and contextual properties, they fail to capture the tone/expression. Using only speech/audio data for training spoken-word representations results in semantically and syntactically poor representations.   So in this paper, we propose a novel spoken-word representation learning approach called STEPs-RL that uses speech and text entanglement for learning phonetically sound spoken-word representations, which not only captures the acoustic and contextual features but also are semantically, syntactically, and phonetically sound. STEPs-RL is trained in a supervised manner such that the learned representations can capture the phonetic structure of the spoken-words along with their inter-word semantic, syntactic, and contextual relationships. We validated the proposed model by  evaluating semantical and syntactical relationships between the learned spoken-word representations on four widely used word similarity benchmark datasets, and comparing its performance with the textual word representations learned by Word2Vec \& FastTexT , and  investigating the phonetical soundness of the generated vector space.   
","   In this paper, we present a novel multi-modal deep neural network architecture that uses speech and text entanglement for learning phonetically sound spoken-word representations. STEPs-RL is trained in a supervised manner to predict the phonetic sequence of a target spoken-word using its contextual spoken word's speech and text, such that the model encodes its meaningful latent representations. Unlike existing work, we have used text along with speech for auditory representation learning to capture semantical and syntactical information along with the acoustic and temporal information. The latent representations produced by our model were not only able to predict the target phonetic sequences with an accuracy of 89.47\% but were also able to achieve competitive results to textual word representation models, Word2Vec \& FastText , when evaluated on four widely used word similarity benchmark datasets. In addition, investigation of the generated vector space also demonstrated the capability of the proposed model to capture the phonetic structure of the spoken-words. To the best of our knowledge, none of the existing works use speech and text entanglement for learning spoken-word representation, which makes this work first of its kind.",269
"  Recent decades have brought about an increase in the use of computer-based tools in practically every  field of human endeavor. The field of education is no exception. Such tools can be used to augment or  even completely replace traditional face-to-face teaching methods. The emergence of online learning platforms has necessitated the development of means to enable learning activities, such as  group discussions, to be performed through the use of technology. One such example of a learning  platform is the IMapBook software suite aimed at increasing the literacy and reading  comprehension skills of elementary school-aged children through the use of web-based eBooks,  embedded games related to their contents, as well as moderated group discussions. Keeping these discussions constructive and relevant can be difficult and usually requires a  discussion moderator to be present at all times. This can limit the opportunities for such discussions to take place. Leveraging the methods and insights  from the fields of artificial intelligence and machine learning, we can attempt to develop systems to automatically classify messages into  different categories and detect when the discussion has veered off course and necessitates intervention. Our research tackles this problem using a  compilation of discussions obtained during pilot studies testing the effectiveness of using the IMapBook software suite in 4th-grade classrooms.  The studies were performed in 8 different Slovene primary schools and, in total, included 342 students.  The discussions consist of 3541 messages along with annotations specifying their relevance to the  book discussion, type, category, and broad category. The ID of the book being discussed and the time  of posting are also included, as are the poster's school, cohort, user ID, and username.  Each message was also manually translated into English to aid non-Slovene-speaking researchers.  The use of the Slovene language presents unique challenges in applying standard language  processing methods, many of which are not as readily available as for other, more widely spoken languages.  Given a sequence of one or more newly observed messages, we want to estimate the relevance of  each message to the actual topic of discussion. Namely, we want to assign messages into two categories 閳 relevant to the book being discussed or not.  Additionally, we want to predict whether the message is a question, an answer, or a statement which we call the type of the message. Finally, we want to  assign a category label to each message where the possible labels can be either 'chatting', 'switching', 'discussion', 'moderating', or 'identity'.  Building a predictive model capable of performing such predictions with acceptable performance would allow us to experiment with including this new  level of automation in the IMapBook software suite as well as in any related products. The research insights are also applicable to areas such as  online user comments and content moderation.  
"," The increasing adoption of technology to augment or even replace traditional face-to-face learning has led to the development of a myriad of tools and platforms aimed at engaging the students and facilitating the teacher's ability to present new information. The IMapBook project aims at improving the literacy and  reading comprehension skills of elementary school-aged children by presenting them with interactive  e-books and letting them take part in moderated book discussions. This study aims to develop and  illustrate a machine learning-based approach to message classification that could be used to  automatically notify the discussion moderator of a possible need for an intervention and also to collect other useful information about the ongoing discussion. We aim to predict whether a message posted in the discussion is relevant to the discussed book, whether the message is a statement, a question, or an answer, and in which broad category it can be classified. We incrementally enrich our used feature subsets and compare them using standard classification algorithms as well as the novel Feature stacking method.  We use standard classification performance metrics as well as the Bayesian correlated t-test to show  that the use of described methods in discussion moderation is feasible. Moving forward, we seek to  attain better performance by focusing on extracting more of the significant information found in the  strong temporal interdependence of the messages.",270
" The Winograd Schema Challenge\/  was proposed by  as a means to test whether a  machine has human-like intelligence. It is an alternative to the well known Turing Test\/  and has been designed with the motivation of reducing certain problematic aspects that affect the TT. Specifically, while the TT is subjective in nature, the WSC provides a purely objective evaluation; and whereas passing the TT requires a machine to behave in a deceptive way, the WSC takes the form of a positive demonstration of intelligent capability.  The core problem of the WSC is to resolve the reference of pronouns occurring in natural language sentences.  To reduce the possibility that the task can be accomplished by procedures based on superficial or statistical characteristics, rather than `understanding' of the sentence, they specify that the test sentences used in the WSC, should be constructed in pairs, which have similar structure and differ only in some key word or phrase, and such that the correct referent of the pronoun is different in the two cases. This sentence pair, together with an indication of which pronoun is to be resolved and a pair of two possible candidates, is called a Winograd Schema.   The following is an example of the Winograd schemas from the original WSC273 data set :    \item The trophy doesn't fit in the brown suitcase because {\bf it} is too small\/.  \end{enumerate}   design Winograd schemas to require background knowledge to resolve a pronoun, which can be an evidence of thinking\/. Therefore, they exclude the sentences that can be resolved by a statistical association within a sentence.   In this paper, we introduce a keyword method to define domains in Winograd schemas. To our best knowledge, this is the first work to use keywords for defining domains in WSC and explore high-level patterns in them. To use the domain-specific high-level patterns, we also develop an advanced high-level knowledge-based reasoning method by modifying the method of . Furthermore, we suggest a simple ensemble method that combines knowledge-based reasoning and machine learning. By the experiments on the domain-specific data set, the ensemble method gives a better performance than each single method. Lastly, we also propose a `robust' accuracy  measure that is more objective by improving the switching method of .   
"," The Winograd Schema Challenge\/  is a common sense reasoning task that requires background knowledge. In this paper, we contribute to tackling WSC in four ways. Firstly, we suggest a keyword method to define a restricted domain where distinctive high-level semantic patterns can be found. A thanking domain was defined by keywords, and the data set in this domain is used in our experiments. Secondly, we develop a high-level knowledge-based reasoning method using semantic roles which is based on the method of \cite{sharma:2019}. Thirdly, we propose an ensemble method to combine knowledge-based reasoning and machine learning which shows the best performance in our experiments. As a machine learning method, we used Bidirectional Encoder Representations from Transformers  \citep{kocijan:2019}. Lastly, in terms of evaluation, we suggest a `robust' accuracy measurement by modifying that of \cite{trichelair:2018}. As with their switching method, we evaluate a model by considering its performance on trivial variants of each sentence in the test set.",271
"  % overview + widespread applications Text classification, as an extensively applied fundamental cornerstone for natural language processing  applications, such as sentiment analysis, spam detection and spoken dialogue systems, has been widely studied for decades. In general, almost all NLP tasks can be cast into classification problems on either document, sentence, or word level. Here we are focusing on the means of it in a narrow sense, i.e., given a sequence of tokens with arbitrary length, predicting the most likely categorization it belongs to.  % conventional approaches, CNN/LSTM pros, + cons: lack the efficacy to capture the latent representations. Considerable compelling neural approaches to the text classification task have empirically demonstrated their remarkable behaviors in recent years, to whom how to orchestrate and compose the semantic and syntactic representations from texts are central. Much of the work concentrated on learning the composition of distributional word representations for categorization, wherein plenty of deep learning methods have been adopted, such as TextCNNs, RCNNs, recurrent neural networks , FastText, BERT, etc. Most of them learn the word representations by firstly projecting the one-hot encoding of each token through a pretrained or randomly initialized word embedding matrices to acquire the dense real-valued vectors, and then feed them into neural models for classification.    These methods, however, have only exploited the low-dimensional semantic representations for each sample text in a supervised way. Some argued that unsupervised latent representations such as topic or cluster modeling mined by latent variable models may be of benefit.  maintained that word clustering could deliver the useful semantic information by grouping all words in the corpus and can thus promote the classification accuracy. Moreover,  incorporated the neural topic models with Variational Autoencoder  into the classification tasks so as to discover the latent topics in the document level and encode the co-occurrence of words with bag-of-words statistics.   Learning such corpus-level representation can administer to the enrichment of more globally informative features and is thus favorable to the task performance. There are plenty of works adopting VAE for learning these latent variables to boost the text classification performance. Nevertheless, there remain problems that we cannot directly treat the sampled latent space of VAE for clustering centroids since there is no mechanism to modulate the representation of different samples towards different mean and variance for a better discrimination purpose under the Gaussian distribution assumption.  and  alleviate these issues by minimizing the distance between the learnable latent representation from latent variable models and the clustering centers generated from statistical clustering approaches.   % trained with, in which projecting the word indices into the dense word representations. Grounding on this, we design an ad hoc Clustering-Enchanced neural model  that jointly learns the distributional clustering and the alignment between the domain-aware clustering centroids and word representations in the Euclidean hidden semantic space for text classification, with the vector space assumption that words with similar meanings are close to each other. Instead of directly treating the latent variables as the clustering centroids, we employ a co-adaptation strategy to minimize the difference between the hidden variables and trainable clustering centroids initialized by traditional clustering algorithms with soft alignments.  In the present work, we propose the cluster-token alignment mechanism by assigning relevance probability distribution of clusters to each token, indicating how likely it is that tokens are correlated with each cluster center. In which clustering centroids are co-regulated with learned latent variables and can be regarded as the domain- or task-specific feature indicators.   Our work illustrates that jointly adapting the clustering centroids and learning the cluster-token alignment holds the promise of advancing the text classification performance by incorporating the clustering-aware representations. Our key contributions are:  {} % graph GCN -> time cost for building graphs   % cluster explanation, importance, usage, application %  inspiration % learn the latent variables with unsupervised approaches to aid in the interaction between multi-hop clusters and word representations  % our contribution:  % 1. unsupervised approaches to learn to maneuver the cluster representation % 2. proposed a cluster-token alignment mechanism to assign each word to implied clusters % 3. our methods outperform previous approaches on eight of the different datasets of both short texts and long texts.    %   %         %             % % \begin{figure*}[thb] %   %      
"," Distributional text clustering delivers semantically informative representations and captures the relevance between each word and semantic clustering centroids. We extend the neural text clustering approach to text classification tasks by inducing cluster centers via a latent variable model and interacting with distributional word embeddings, to enrich the representation of tokens and measure the relatedness between tokens and each learnable cluster centroid. The proposed method jointly learns word clustering centroids and clustering-token alignments, achieving the state of the art results on multiple benchmark datasets and proving that the proposed cluster-token alignment mechanism is indeed favorable to text classification. Notably, our qualitative analysis has conspicuously illustrated that text representations learned by the proposed model are in accord well with our intuition.",272
"      The use of deep learning for processing natural language is becoming a standard, with excellent results in a diverse range of tasks. Two state-of-the-art  architectures for text-related modeling are long short-term memory  networks~ and transformers~. LSTMs are recurrent neural networks that process the text sequentially, meaning that they process text one token at a time, building up its internal representation in hidden states of the network. Due to the recurrent nature of LSTM, which degrades the efficiency of parallel processing, as well as demonstrated improvements in performance, models based on the transformer architecture are gradually replacing LSTMs across many tasks. Transformers can process the text in parallel, using self-attention and positional embeddings to model the sequential nature of the text.  A common trend in using transformers is to first pre-train them on large monolingual corpora with abstract, general-purpose objective, and then fine-tune them for a specific task, such as text classification.  For example, the BERT  architecture  uses transformers and is pretrained with masked language modelling and order of sentences prediction tasks to build a general language understanding model. During the fine-tuning for a specific downstream task, additional layers are added to the BERT model, and the model is trained on specific data to capture the specific knowledge required to perform the task.   Most of the research in the natural language processing  area focuses on English, ignoring the fact that English is specific in terms of the low amount of information expressed through morphology . In our work, we focus on adapting modern deep neural networks, namely LSTMs and BERT, for several morphologically rich languages, by explicitly including the morphological information. The languages we analyze contain rich information about grammatical relations in the morphology of words instead of in particles or relative positions of words . For comparison,  we also evaluate our models on English. Although previous research has shown that the state of the art methods such as BERT already captures some information contained in the morphology~, our experiments involve several languages with rich morphology where neural networks could benefit from explicit morphological features.  Specifically, we present methods which combine BERT with separately encoded morphological properties: universal part of speech tags  and universal features . We evaluate them on three downstream tasks: named-entity recognition , dependency parsing , and comment filtering . We perform similar experiments on LSTM networks and compare the results for both architectures. Besides English, we analyze eight more languages: Croatian, Estonian, Finnish, Latvian, Lithuanian, Russian, Slovene and Swedish. The choice of these languages reflects a mix of different language groups , for which we were able to obtain sufficient resources , due to their coverage in the EU EMBEDDIA project.   Our experiments show that the addition of morphological features has mixed effects depending on the task. Across the tasks where the added morphological features improve the performance, we show that  they benefit the LSTM-based models even if the features are noisy and  they benefit the BERT-based models only when the features are of high quality , suggesting that BERT models already capture the morphology of the language; however, there is a room for improvement either in designing pre-training objectives that can capture these properties or when high-quality features are available.   The remainder of this paper is structured as follows. In Section, we present different attempts to use morphological information in machine learning, in particular neural networks, as well as an overview of recent work in the three evaluation tasks. In Section, we describe the used datasets and their properties. In Section, we present the baseline models and models with additional morphological information, whose performance we discuss in Section. Finally, we summarize our work and present directions for further research in Section.  
"," Currently, deep learning approaches are superior in natural language processing due to their ability to extract informative features and patterns from languages. Two most successful neural architectures are LSTM and transformers, the latter mostly used in the form of large pretrained language models such as BERT.  While cross-lingual approaches are on the rise, a vast majority of current natural language processing techniques is designed and applied to English, and less-resourced languages are lagging behind. In morphologically rich languages, plenty of information is conveyed through changes in morphology, e.g., through different prefixes and suffixes modifying stems of words. The existing neural approaches do not explicitly use the information on word morphology. We analyze the effect of adding morphological features to LSTM and BERT models. As a testbed, we use three tasks available in many less-resourced languages: named entity recognition , dependency parsing , and comment filtering . We construct sensible baselines involving LSTM and BERT models, which we adjust by adding additional input in the form of part of speech  tags and universal features. We compare the obtained models across subsets of eight languages. Our results suggest that adding morphological features has mixed effects depending on the quality of features and the task. The features improve the performance of LSTM-based models on the NER and DP tasks, while they do not benefit the performance on the CF task. For BERT-based models, the added morphological features only improve the performance on DP when they are of high quality , while they do not show any practical improvement when they are predicted. As in NER and CF datasets manually checked features are not available, we only experiment with the predicted morphological features and find that they do not cause any practical improvement in performance.",273
" Past work has found that variability in speech signals is often poorly modeled, despite recent advances in speech representation learning using deep neural networks . An important source of acoustic variability comes from accent information embedded in the speech signals . Non-native accents are frequently observed when a second language is spoken, and are mainly caused by the first language background of non-native speakers. The accent strength of a non-native speaker is dependent on the amount of transfer from the native language, and is generally influenced by a variety of variables from which the age of second-language learning is one of the most valuable predictors . However, accent variability is often overlooked in modeling language, and consequently high-resource languages such as English are often treated as homogeneous . That this assumption is problematic is, for example, shown by comparing the number of native and non-native speakers of English, with the latter group being almost twice as large as the former group . It is therefore important to accurately model pronunciation variation using representations of speech that allow this variability to be incorporated.  Traditionally, pronunciations are often represented and evaluated by phonetically transcribing speech . However, transcribing speech using a phonetic alphabet is time consuming, labor intensive, and interference from transcriber variation might lead to inconsistencies . Additionally, fine-grained pronunciation differences that are relevant for studying accented speech may not be captured by using a set of discrete symbols .  \citet{acoustic-measure} therefore introduced an acoustic-only measure for comparing pronunciations.  In their method, they represented accented speech as 39-dimensional Mel-frequency cepstral coefficients , which were used to compute acoustic-based non-native-likeness ratings between non-native and native speakers of English.  They found a strong correlation of  between their automatically determined acoustic-based non-native-likeness ratings and native-likeness ratings provided by human raters .  This result was close to, but still not equal to the performance of a phonetic transcription-based approach . \citet{acoustic-measure} also conducted several small-scale experiments to investigate whether more fine-grained characteristics of human speech were captured compared to the phonetic transcription-based pronunciation difference measure.  Their results showed that the acoustic-only measure captured segmental differences, intonational differences, and durational differences, but that the method was not invariant to characteristics of the recording device.  The quality of MFCC representations is known to be dependent on the presence of additive noise .  Recent work has shown that self-supervised representation learning models are less affected by noise, while being well-equipped to model complex non-linear relationships .  For example, these models can learn meaningful representations on the basis of read English speech without direct supervision. Fine-tuning these models using transcribed speech resulted in representations which resembled phonetic structure, and offered significant improvements in downstream speech recognition tasks .  Consequently, in this paper, we employ these self-supervised neural models to create an automatically determined acoustic-only pronunciation difference measure, and investigate whether this results in improved performance compared to the MFCC-based approach of \citet{acoustic-measure} and the phonetic transcription-based approach of \citet{wieling2014a}.  In the following, we compare and evaluate several neural models, namely , \citep[subsequently denoted by ]{schneider2019wav2vec},  ,  \citep[subsequently denoted by ]{baevski2019vq}, and  \citep[subsequently denoted by ]{baevski2020wav2vec}. We evaluate the performance of these algorithms using two different datasets. The first is identical to the dataset used by \citet{acoustic-measure} and \citet{wieling2014a}. The second is a new dataset which only focuses on accented speech from a single group of  non-native speakers for which human native-likeness judgements are also available. For reproducibility, we provide our code via \url{https://github.com/Bartelds/neural-acoustic-distance}. The performance of our model is assessed by comparing the obtained neural acoustic-only pronunciation differences to phonetic transcription-based pronunciation distances, MFCC-based acoustic-only pronunciation distances, and human perception.  To understand which aspects of pronunciation variation the neural models can capture, we conduct several additional small-scale experiments, in line with those of \citet{acoustic-measure}.  
"," Variation in speech is often represented and investigated using phonetic transcriptions, but transcribing speech is time-consuming and error prone. To create reliable representations of speech independent from phonetic transcriptions, we investigate the extraction of acoustic embeddings from several self-supervised neural models.  We use these representations to compute word-based pronunciation differences between non-native and native speakers of English, and evaluate these differences by comparing them with human native-likeness judgments.  We show that Transformer-based speech representations lead to significant performance gains over the use of phonetic transcriptions, and find that feature-based use of Transformer models is most effective with one or more middle layers instead of the final layer.  We also demonstrate that these neural speech representations not only capture segmental differences, but also intonational and durational differences that cannot be represented by a set of discrete symbols used in phonetic transcriptions.",274
"  In this paper we focus on the problem of integrating syntactic features in a neural architecture for the Frame-Semantic parsing  process. Frame-semantic parsing is the task of extracting full semantic frame structures from text, as  defined by   Frame Semantics theory .  %Semantic frames are conceptual structures describing general situations, evoked in language by target words referred to as lexical units. Each frame is enriched by a set of semantic roles called frame elements, defining specific participants in the described situation.  %An example of a sentence annotated with Frame Semantics is shown in Figure .  From a theoretical perspective, Frame-Semantic parsing can be decomposed into three sub-tasks: 1) Target Identification  -- identifying target words acting as lexical units; 2)  Frame Identification  -- disambiguating each target into a possible frame; and 3) Semantic Role Labeling  -- extracting all the possible frame elements for a given frame.  Early neural approaches have focused in this regard on the integration of features extracted from dependency trees, both for the FI and SRL tasks , with positive results. Amongst all, SRL is the task that has received more attention when investigating methods for injecting syntax into neural models, mostly due to the strict correlation between syntax and argument structures .  Several solutions have been proposed, setting new baselines over general Frame-semantic parsing and specific SRL corpora. These include the use of dependency path embeddings , the application of Graph Convolutional Networks  to learn representations of the dependency graphs , or restricting the set of candidate arguments using pruning algorithms . Multi-task learning has been also applied, either directly supervising attention to learn dependency parsing  for both TI and SRL, or to implicitly bias learned encoded representations when jointly training a simplified syntactic dependency parser , or a semantic dependency parser for both FI and SRL . %   Although effective, these approaches have focused on exploiting syntactic dependencies rather than  constituency information, partly because dependencies are more suited to be encoded as features or learned through attention mechanisms. %, as they express relationships between words.  Semantic roles are technically provided over syntactic constituents, which directly cast argument boundaries over word sequences. This is demonstrated also by earlier work on SRL, which relied on constituency derived features . It follows that using constituency information should be beneficial, especially because reconstructing argument boundaries through dependencies would require an unbounded number of hops among words, making the problem hard to model in neural architectures . Following this idea, two recent approaches have attempted to rely on such constituency information to improve SRL performance. \citet{wang-etal-2019-best} use linearised representations of constituency trees in different learning settings, either by extracting salient features, by multi-task learning, or by combining both approaches in an auto-encoding fashion. \citet{marcheggiani19:axiv}, instead, train a  GCN with the SRL objective to learn constituent representations, which are then infused into words through the same GCN via the message-passing operation .  In this paper, we foster the same idea of relying on constituency information for every sub-task of Frame-semantic parsing, namely TI, FI, and SRL. We train a GCN to learn specific constituency representations, which are used in turn to compute syntactic paths between constituency nodes.  Our approach is similar to that of \citet{marcheggiani19:axiv}, although it significantly differs in: i) the initialisation and topology of the underlying graph; ii) the lower number of required parameters; and iii) the way that syntactic information is infused in every word representation, i.e. computing node-to-node syntactic paths. We show that our approach improves the state-of-the-art over the main Frame-semantic parsing benchmark, i.e.\ the FrameNet corpus , on the single TI and SRL tasks, and on FI in a joint-learning setting. Moreover, we demonstrate the generality of the approach by testing the same network on the CoNLL 2005 dataset .  
"," We study the problem of integrating syntactic information from constituency trees into a neural model in Frame-semantic parsing sub-tasks, namely Target Identification , Frame Identification , and Semantic Role Labeling . We use a Graph Convolutional Network to learn specific representations of constituents, such that each constituent is profiled as the production grammar rule it corresponds to. We leverage these representations to build syntactic features for each word in a sentence, computed as the sum of all the constituents on the path between a word and a task-specific node in the tree, e.g. the target predicate for SRL. Our approach improves state-of-the-art results on the TI and SRL of \texttildelow$1\%$ and \texttildelow3.5\% points, respectively , when tested on FrameNet 1.5, while yielding comparable results on the CoNLL05 dataset to other syntax-aware systems. %When testing the approach on the FrameNet 1.5 corpus, our system gives us strong insight on the role of syntax on TI, while improving the state-of-the-art on SRL of 3.5 points.  %While it yields comparable results on the CoNLL05 dataset.",275
" KR\&R systems work well for certain knowledge-rich domains that typically involve a  set of axioms or rules, use structured queries and datasets, and have a need for precise logical inference with explanations. Formal logic-based reasoning engines such as Cyc  and Ergo  have been successfully deployed in domains such as legal, healthcare and finance.  One of the main advantages of using such systems is transparency 閳 the underlying reasoning of the system is well-understood and can be justified to end-users.   However, there are several known drawbacks of logic-based approaches. For one, the inference procedures are highly brittle in that they require precise matching/unification of logical terms and formulae in order to construct a complete explanation. Secondly, traditional reasoners don閳ユ獩 deal with uncertainty well , whereas rules in real-world applications are often probabilistic and contextual. Thirdly, all such systems suffer from the knowledge acquisition problem . Often, the rules are hand-coded, an approach which doesn閳ユ獩 scale in general.  Our problem domain is Natural Language Understanding , an area where all the issues mentioned above come into play 閳 the need to acquire and use implicit background knowledge to understand text, the application of rules differently based on the context, and the use of imperfect/fuzzy alignment of concepts and relations when doing reasoning.   To address these issues, we devise a novel FOL-based reasoner, called Braid. Braid includes a backward and forward chainer, assumption based reasoner and a constraint solver. This paper only refers to the backward chaining component, which we refer to as Braid-BC.    Braid-BC supports rules with confidences, and uses the notion of custom unification functions and dynamic rule generation to overcome the brittle matching and knowledge-gap problem prevalent in traditional reasoning engines.  The custom-unifiers can be based on any statistical techniques, as long as they can propose and score mappings between the terms of two logical propositions . For example, we use neural matching functions as unifiers. Their purpose is to help the reasoner find proofs even when  goals, rule conditions and/or facts do not align perfectly.     The dynamic rule-generator  is given a target proposition  and a knowledge base  as input, and outputs a scored list of hypothesized rules that could be used to prove that proposition. The purpose of rule-generation is to connect the dots when the knowledge required for an inference is missing from the static KB. We describe two DRG implementations - one using a neural  rule generation model that was fine-tuned on a dataset of crowd-sourced causal rules, known as GLUCOSE , and the second that uses a rule-template based technique.     We describe the reasoning algorithms used in Braid-BC, and their implementation in a distributed task-based framework that builds proof/explanation graphs for an input query in a highly scalable manner. Our approach shares some similarities with the RETE framework  for matching production rules  but makes several novel extensions: we primarily do backward chaining via a heuristic best-first search , leverage a Master-Worker architecture where the Master builds the main proof graph while Workers make local inferential updates, and define general functions for Unifiers and Provers that lets us plug in various reasoning strategies combining standard reasoning  with statistical approaches .  
","  Traditional symbolic reasoning engines, while attractive for their precision and explicability, have a few major drawbacks: the use of brittle inference procedures that rely on exact matching  of logical terms, an inability to deal with uncertainty, and the need for a precompiled rule-base of knowledge . These issues are particularly severe for the Natural Language Understanding  task, where we often use implicit background knowledge to understand and reason about text, resort to fuzzy alignment of concepts and relations during reasoning, and constantly deal with ambiguity in representations.   To address these issues, we devise a novel FOL-based reasoner, called Braid, that supports probabilistic rules, and uses the notion of custom unification functions and dynamic rule generation to overcome the brittle matching and knowledge-gap problem prevalent in traditional reasoners. In this paper, we describe the reasoning algorithms used in Braid-BC , and their implementation in a distributed task-based framework that builds proof/explanation graphs for an input query in a scalable manner. We use a simple QA example from a children闁炽儲鐛 story to motivate Braid-BC闁炽儲鐛 design and explain how the various components work together to produce a coherent logical explanation.",276
" Humans compose documents to record and preserve information. As information carrying vehicles, documents are written using different layouts to represent diverse sets of information for a variety of different consumers. In this work, we look at the problem of document understanding for documents written in English. Here, we take the term document understanding to mean the automated process of reading, interpreting, and extracting information from the written text and illustrated figures contained within a document's pages. From the perspective as practitioners of machine learning, this survey covers the methods by which we build models to automatically understand documents that were originally composed for human consumption. Document understanding models take in documents and segment pages of documents into useful parts , often using optical character recognition ~ with some level of document layout analysis. These models use this information to understand the contents of the document at large, e.g. that this region or bounding box corresponds to an address. In this survey, we focus on these aspects of document understanding at a more granular level and discuss popular methods for these tasks. Our goal is to summarize the approaches present in modern document understanding and highlight current trends and limitations.  In Section, we discuss some general themes in modern NLP and document understanding and provide a framework for building end-to-end automated document understanding systems. Next, in Section, we look at the best methods for OCR encompassing both text detection  and text transcription . We take a broader view of the document understanding problem in section, presenting multiple approaches to document layout analysis: the problem of locating relevant information on each page. Following this, we discuss popular approaches for information extraction .  
"," Documents are a core part of many businesses in many fields such as law, finance, and technology among others. Automatic understanding of documents such as invoices, contracts, and resumes is lucrative, opening up many new avenues of business. The fields of natural language processing and computer vision have seen tremendous progress through the development of deep learning such that these methods have started to become infused in contemporary document understanding systems. In this survey paper, we review different techniques for document understanding for documents written in English and consolidate methodologies present in literature to act as a jumping-off point for researchers exploring this area.",277
" Sequence labeling is one of the commonly used techniques for solving natural language understanding  tasks such as named-entity recognition  and slot filling. Furthermore, for these tasks, the state-of-the-art results are typically based on deep neural networks . However, the performance of these models is highly dependent on the availability of large amounts of annotated data. Moreover, compared with classification tasks, which require only one label for a sample, the sequence learning tasks require a series of token-level labels for an entire sequence, which makes them time-consuming and a costly annotation process.  \\ This problem can be mitigated using active learning , which achieves improved performance with fewer annotations by strategically selecting the examples to annotate .  There are two major strategies for active learning, namely, diversity-based sampling and uncertainty-based sampling . % 闉愵剠璧 闉氭尗鍎婇爟 Traditionally, uncertainty-based sampling is the most common pool-based AL approach. However, previous work pointed out that focusing only on the uncertainty leads to a sampling bias . It creates a pathological scenario where selected samples are highly similar to each other, which clearly indicates inefficiency. This may cause problems, especially in the case of noisy and redundant real-world datasets. Another approach is diversity-based sampling, wherein the model selects a diverse set such that it represent the input space without adding considerable redundancy . This approach can select samples while ensuring a maximum batch diversity. However, this approach might select points that provide little new information, thereby reducing the uncertainty of the model. Certain recent studies for classification tasks implemented an algorithm named Batch Active learning by Diverse Gradient Embeddings . This algorithm first computes embedding for each unlabeled sample based on induced gradients, and then geometrically picks the instances from the space to ensure their diversity .  Although it proves to be a robust improvement when performing an image classification task, its performance in sequence labeling tasks is yet unproven. \\ In this study, we investigated some practical active learning algorithms that consider uncertainty and diversity in sequence labeling tasks over different datasets and models. Moreover, we suggested a method to expand BADGE with weighted sampling based on the sequence length to ensure cost-effective labeling. This simple modification in it has a positive implication that it tends to select cost-effective samples.  The proposed model trades off between uncertainty and diversity by selecting diverse samples in the gradient space depending on the parameters in the final layer, for which, the currently available models focus only on uncertainty. To the best of our knowledge, our study is the first to apply diverse gradient embedding to a sequence labeling task. We experimented with the CoNLL 2003 English, ATIS, and Facebook Multilingual Task Oriented Dataset . Accordingly, it was empirically demonstrated that the proposed method consistently outperformed the baseline method including Bayesian AL by disagreement , which shows state-of-the-art performance in NER task, across the datasets, tasks and model architectures.   
","  Recently, several studies have investigated active learning  for natural language processing tasks to alleviate data dependency. However, for query selection, most of these studies mainly rely on uncertainty-based sampling, which generally does not exploit the structural information of the unlabeled data. This leads to a sampling bias in the batch active learning setting, which selects several samples at once. In this work, we demonstrate that the amount of labeled training data can be reduced using active learning when it incorporates both uncertainty and diversity in the sequence labeling task. We examined the effects of our sequence-based approach by selecting weighted diverse in the gradient embedding approach across multiple tasks, datasets, models, and consistently outperform classic uncertainty-based sampling and diversity-based sampling.",278
"}  {I}{n} the past decade, we have seen the emergence of various Knowledge Graphs , such as YAGO and DBPedia. They have achieved great success in both academic and industrial applications, ranging from recommendation to Question Answering. However, these KGs are far from complete, which limits the benefits of transferred knowledge. Relation Extraction  is a vital step to complete KGs by extracting the relations between entities from texts. It is nontrivial since the same relation type may have various textual expressions, and meanwhile, different types of relations can also be described with the same words. Such ambiguity between relations and texts challenges the supervision of RE models.  Due to the expensive human annotation cost, distant supervision is proposed to automatically annotate the mappings between sentences and relations. It assumes that if two entities participate in a relation, a.k.a., a triple  but express another relation . As shown in Figure, given the triple , we collect two sentences that include the entity pair . Clearly, the first sentence expresses a similar meaning with the given relation type, but the second one implies another type of relation city of, which brings in noise to the training corpora\footnote{As the term relation can refer to either relation type or relation instance , in the paper, we simplify the use of term relation for relation type unless otherwise stated.}. To highlight informative sentences, many existing works introduce the attention mechanism to assign sentences with different learning weights.  In terms of quantity, on the other hand, most of the training data collected by distant supervision concentrate mainly on a few relations, leading to the issue of the lack of sufficient annotations for the remaining relations. Take the widely used dataset, New York Times , as an example, we present the number of training instances of each relation in Figure. Unsurprisingly, the annotations are long-tail concerning different relations, and the tail relations suffer from insufficient training corpora. More specifically, each relation  refers to multiple entity pairs  is smaller than that between  should be more similar with respect to RE prediction distributions because of the more common textual contexts. Therefore, how to capture relation proximity in a more precise and general way remains challenging.  Another major challenge is to distinguish between different relations, in case the knowledge transfer introduces a bias towards the same prediction for proximate relations. For example, as mentioned above, both /location/us\_state/capital and /location/fr\_region/capital indicate the capital relation, and the only difference is that between two United States entities or French entities. DPEN incorporates entity type information to learn relation-specific classifier dynamically. However, entity type information is sparse in KGs , challenging the scalability.  To address the first issue, we propose to learn relation prototypes that capture the proximity relationship among relations from involved entity pairs. Inspired by Prototypical Networks, we represent each relation prototype with the centroid of its training data, and each data point is defined as the difference between the pair of entity embeddings, namely implicit mutual relation . Given any entity pair, we compute the implicit mutual relation and its distance to each relation prototype. These proximities suggest possible relations to the classifier, which further makes correct predictions by extracting discriminative signals from supportive sentences. Relation prototypes can also be enhanced by prior information , and be applied to arbitrary sentence encoder.  To address the second issue, we enhance entity embeddings with textual information for implicit mutual relation learning. In specific, we construct an entity co-occurrence graph from unlabeled texts and modeling both the first-order and second-order structural proximity. The massive textual contexts are helpful to infer entity types for distinguishment. Besides, long-tail entity pairs can also benefit from additional textual information. We summarize our main contributions as follows:    A preliminary version of this work has been published in the conference of ICDE 2020. We summarize the main changes as follows:    %The rest of the paper is organized as follows. In Section, we formulate the problem and overview the framework, and Section introduces our proposed method in detail. We report the promising experiment results on real-world datasets in Section. Section covers the related works. Finally, we conclude the paper in Section.    
","   Relation Extraction  is a vital step to complete Knowledge Graph  by extracting entity relations from texts. However, it usually suffers from the long-tail issue. The training data mainly concentrates on a few types of relations, leading to the lack of sufficient annotations for the remaining types of relations. In this paper, we propose a general approach to learn relation prototypes from unlabeled texts, to facilitate the long-tail relation extraction by transferring knowledge from the relation types with sufficient training data. We learn relation prototypes as an implicit factor between entities, which reflects the meanings of relations as well as their proximities for transfer learning. Specifically, we construct a co-occurrence graph from texts, and capture both first-order and second-order entity proximities for embedding learning. Based on this, we further optimize the distance from entity pairs to corresponding prototypes, which can be easily adapted to almost arbitrary RE frameworks. Thus, the learning of infrequent or even unseen relation types will benefit from semantically proximate relations through pairs of entities and large-scale textual information.      We have conducted extensive experiments on two publicly available datasets: New York Times and Google Distant Supervision. Compared with eight state-of-the-art baselines, our proposed model achieves significant improvements . Further results on long-tail relations demonstrate the effectiveness of the learned relation prototypes. We further conduct an ablation study to investigate the impacts of varying components, and apply it to four basic relation extraction models to verify the generalization ability. Finally, we analyze several example cases to give intuitive impressions as qualitative analysis. Our codes will be released later.    %Relation Extraction  is a paramount step to complete Knowledge Graph by extracting entity relations from texts. However, it usually suffers from the long-tail issue, as the training data mainly concentrates on a few types of relations, leading to the lack of sufficient annotations for the remaining types of relations. In this paper, we propose a general approach to learn relation prototypes from unlabeled texts, to facilitate the long-tail RE by transferring knowledge from those with sufficient data. We learn prototypes as an implicit factor between entities, to reflect the meanings of relations and their proximities. Specifically, we construct an entity co-occurrence graph from texts, and capture structural proximities for embedding learning. Furthermore, we optimize the distance from entity pairs to corresponding prototypes, which can be easily adapted to many RE framework. We have conducted extensive experiments on two publicly available datasets. Compared with eight state-of-the-art baselines, our model achieves significant improvements . Further results on long-tail relations demonstrate the effectiveness of the learned relation prototypes. We further conduct an ablation study to investigate the impacts of varying components and the generalization ability. Finally, we analyze several example cases to give intuitive impressions as qualitative analysis.",279
"  % Understanding how BERT works is important. % the presence of blackbox nlp  is an indication that the research community values the ability to understand the internals of deep neural networks. Pre-trained transformer models such as BERT  are currently ubiquitous within natural language processing  research and have demonstrated improvements in topics from sentiment analysis to semantic parsing . The widespread development and use of such models has led to an increased effort to interpret such models' decisions . % * understanding models is important in society % * BERT is used all over, so important to understand BERT As defined in \citet{doshivelez2017rigorous}, model interpretability is ``the ability [of a model] to explain or present in understandable terms to a human''.  Intuitively, a more interpretable model is easier to understand, debug and improve.  % It's hard to understand BERT because  % * it's a neural model with many, many parameters % * pre-training + fine-tuning is newer than just training from scratch  -> read literature introductions/motivations Interpreting modern pre-trained transformer models is difficult. First, modern deep learning models have hundreds of millions of parameters, and scale only continues to increase . Understanding the impact of a single parameter is nearly impossible because these models are densely connected. Combined with the sheer number of parameters, manual analysis is infeasible. Secondly, while both pre-training and fine-tuning are required for state-of-the-art performance, effort has focused on alternative pre-training methods . % Understanding the impacts of fine-tuning is still not well understood. \todo{do I need a citation here?}   % Previous work attempted to use attention Previous work uses BERT's self-attention mechanism to interpret the model's predictions . However, a body of work  shows that models' attention mechanisms cannot be interpreted on single-sequence classification tasks.  % We apply bert to a sequence classification task We apply BERT and two BERT-based models  to an existing sentence classification task proposed in \citet{aesw}. We compare BERT-based models' performances with previous baselines and then use methods presented in \citet{vashishth2019attention} and \citet{deyoung-etal-2020-eraser} to evaluate BERT's interpretability in single-sequence classification tasks. We find that fine-tuning can teach BERT to recognize previously unknown patterns in natural language and that BERT is more interpretable than the attention-based models analyzed in \citet{jain-wallace-2019-attention} and \citet{vashishth2019attention}. To summarize, the key contributions of this paper are:   % this is nice because  % * BERT hasn't been applied to it % * professional data set, we have a baseline, all human-annotated % * it has marked spans of edits before and after % To the best of our knowledge, BERT has not been applied to the Automatic Evaluation of Scientific Writing  task.  
"," Pre-trained transformer language models such as BERT are ubiquitous in NLP research, leading to work on understanding how and why these models work. Attention mechanisms have been proposed as a means of interpretability with varying conclusions. We propose applying BERT-based models to a sequence classification task and using the data set's labeling schema to measure each model's interpretability. We find that classification performance scores do not always correlate with interpretability. Despite this, BERT's attention weights are interpretable for over 70\% of examples.",280
"  .          % % final paper: en-us version           %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. }    Recently, neural machine translation  has demonstrated impressive performance improvements and became the de-facto standard .    However, like other neural methods, NMT is data-hungry.   This makes it challenging when we train such a model in low-resource scenarios .   Researchers have developed promising approaches to low-resource NMT.   Among these are data augmentation , transfer learning , and pre-trained models .   But these approaches rely on external data other than bi-text.   To date, it is rare to see work on the effective use of bilingual data for low-resource NMT.    In general, the way of feeding samples plays an important role in training neural models.   A good instance is that it is popular to shuffle the input data for robust training in state-of-the-art systems.   More systematic studies on this issue can be found in recent papers .   For example,  have pointed out that deep neural networks tend to prioritize learning ``easy'' samples first.   This agrees with the idea of curriculum learning  in that an easy-to-hard learning strategy can yield better convergence for training.    In NMT, curriculum learning is not new.   Several research groups have applied it to large-scale translation tasks although few of them discuss the issue in a low-resource setup .   The first question here is how to define the ``difficulty'' of a training sample.   Previous work resorts to functions that produce a difficulty score for each training sample.   This score is then used to reorder samples before training.   But the methods of this type enforce a static scoring strategy and somehow disagrees with the fact that the sample difficulty might be changing when the model is updated during training.   Another assumption behind curriculum learning is that the difficulty of a sample should fit the competence of the model we are training.   Researchers have implicitly modeled this issue by hand-crafted curriculum schedules  or simple functions , whereas there has no in-depth discussion on it yet.    In this paper, we continue the line of research on curriculum learning in low-resource NMT.   We propose a dynamic curriculum learning  method to address the problems discussed above.   The novelty of DCL is two-fold.   First, we define the difficulty of a sample to be the decline of loss .   In this way, we can measure how hard a sentence can be translated via the real objective used in training.   Apart from this, the DCL method explicitly estimates the model competence once the model is updated, so that one can select samples that the newly-updated model has enough competence to learn.      DCL is general and applicable to any NMT system.   In this work, we test it in a Transformer-based system on three low-resource MT benchmarks and different sized data selected from the WMT'16 En-De task.   Experimental results show that our system outperforms the strong baselines and several curriculum learning-based counterparts.    
","      Large amounts of data has made neural machine translation  a big success in recent years.    But it is still a challenge if we train these models on small-scale corpora.   In this case, the way of using data appears to be more important.    Here, we investigate the effective use of training data for low-resource NMT.   In particular, we propose a dynamic curriculum learning  method to reorder training samples in training.   Unlike previous work, we do not use a static scoring function for reordering.   Instead, the order of training samples is dynamically determined in two ways - loss decline and model competence.   This eases training by highlighting easy samples that the current model has enough competence to learn.    We test our DCL method in a Transformer-based system.   Experimental results show that DCL outperforms several strong baselines on three low-resource machine translation benchmarks and different sized data of WMT'16 En-De.",281
"    One of the fundamental problems in Natural Language Processing  is learning a distributed encoding of sentences, as this is the stepping stone for many NLP tasks, such as sentence classification, sentiment analysis and natural language inference. The multitude of approaches addressing this problem can be categorised according to how a sentence is represented.  %In bag-of-words models, sentences are represented as words multisets and their encodings are generated by averaging word representations  The simpler sentence representation is bag-of-words, which depicts sentences as words multisets ignoring the word order. Despite the simple representation, it has been used to obtain meaningful sentence encodings .   Sequence representation overcomes this limitation considering the sentence as an ordered sequence of words. It allows building models which progressively constructs a sentence encoding, processing one word at the time. Recurrent Neural Network  and Long-Short Term Memory   are probably the most famous models which use this representation.% to produce sentence embeddings.  %This representation reflects how we read text, word after word. %One of the major drawbacks of bag-of-words models is that they are insensitive to word order. Sequence-based models overcome this limitation considering a sentence as a sequence of words .  A key aspect of sentences, which is missing in sequential processing, is compositionality. For example, the sentence ""The sky is blue and the grass is green"" is obtained by composing the two sub-phrases  ""The sky is blue"" and ""the grass is green"" with the conjunction ""and"". The intrinsic compositionality of sentences makes them suitable for a tree representation, where the whole sentence  is built in terms of sub-phrases  which in turn are defined in terms of smaller constituents; the base cases are words  since they are the atomic piece of information. This representation takes the name of constituency tree. In Fig.\  we show the constituency tree of the sentence ""Effective but too-tepid biopic"": the leaves are the words while internal nodes represent syntactic categories which are the constituents of the whole sentence.  There are many models which compute a sentence encoding starting from its constituency tree. For our purposes, we restrict the discussion on bottom-up Recursive Neural Networks  . The parsing direction is constrained by the structure of constituency trees, having information  on leaf nodes. In this domain, we refer to the term composition function to indicate the state-transition function which computes the representation  of a tree node combining the representation of its constituents . Then, the hidden state of the root  is taken as sentence encoding.  The Matrix-Vector Recurrent Neural Network   and the Recursive Neural Tensor Network   apply the RecNN architecture to binary constituency trees using complex composition functions.  % apply the RecNN architecture to binary constituency trees. Moreover, they propose two new architectures which leverage more complex composition functions: the MV-RNN and the RNTN. In the MV-RNN , every word and sub-phrase is encoded as both a vector and a matrix. When two constituents are combined the matrix of one is multiplied with the vector of the other and vice versa, obtaining a composition function which is parameterised by constituents that participate in it. MV-RNN requires a huge number of parameters, since a composition matrix is attached to each word. RNTN  solves this limitation defining a tensor composition function. The tensor allows to obtain composition function parameters directly from the constituent that participate in it.   extends the well known Long-Short Term Memory  architecture to tree-structured data. They propose two different Tree-LSTMs : the -ary Tree-LSTM defines a composition function which considers constituent order while the child-sum Tree-LSTM ignores such an order. However, only the former model is applied to binary constituency trees. The latter is applied to dependency trees, which are another kind of tree representation for sentences, out of our scope.  In recent years, Tree-LSTM has been used as a building block to develop more sophisticated models. For example, , , ,   build new Tree-LSTM models which define dynamic composition functions depending on syntactic categories . Instead,  introduces a Bidirectional Tree-LSTM which takes advantage of both parsing directions: bottom-up and top-down. As we stated before, constituency trees are intrinsically bottom-up; to this end, the author introduces a first bottom-up pass, called head lexicalization, to propagate information from leaves to the root. All these models are applied only to binary constituency trees.    Thus far, we have shown that most of the models compute sentences encodings starting from binary constituency trees. This simplification solves one crucial problem of tree-structured data: the variable number of child nodes. However, the price to pay is the loss of structural information. For example, in Fig.\  and Fig.\  we report the constituency and the binary constituency tree of the sentence ""Effective but too-tepid biopic"". Comparing the two representation, we can observe that binary tree has one more node that breaks the ternary relation in the non-binary tree; in general, to break a node with  child nodes, we need to add  new nodes. All these new nodes create a chain which moves away the child nodes of the n-ary relation from their parent. The composition of them is obtained by considering one child at a time, as it happens in sequence representation. Hence, the binarisation removes the equality among child nodes, with the risk of weakening contribution of child nodes that are moved far away from their parent and strengthening the contribution of the ones that remain close.  As far as we know, the only work which builds a model suitable for non-binary constituency trees is the TreeNet . The idea is to consider all child nodes in a chain: the hidden state of a node depends on the hidden state of its left sibling and its rightmost child. Even if the model itself works with non-binary trees, the composition function expressed is binary since it always composes two elements. We discuss this observation in details in Sec. .  The definition of models for non-binary constituency trees requires to go beyond the standard definition of composition function. Standard RecNNs define learnable composition functions which are based on the summation of the contribution of each constituent.  proposed a generalisation of such sum-based composition functions leveraging more expressive multi-affine maps represented as tensors. The exponential number of parameters with respect to the tree out-degree  required by the full-tensorial approach can be controlled by applying tensor decomposition. The tensorial models outperform sum-based models, especially when the tree out-degree increases .  Within the scope of this paper, we unveil that non-binary constituency trees can be effectively exploited to improve predictive performance in NLP task, showing that more powerful composition functions are necessary to take advantages of such a rich representation. To this end, we introduce two new Tree-LSTM models which leverage canonical tensor decomposition: the former is suitable for binarised constituency trees, while the latter can process general non-binary constituency trees imposing weight sharing on the tensor decomposition factors. Finally, we test the quality of sentence encodings produced by our models on different NLP tasks, showing that the combination of a rich representation and a powerful composition function is able to outperform baseline models using the same number of parameters.  
"," Processing sentence constituency trees in binarised form is a common and popular approach in literature. However, constituency trees are non-binary by nature. The binarisation procedure changes deeply the structure, furthering constituents that instead are close. In this work, we introduce a new approach to deal with non-binary constituency trees which leverages tensor-based models. In particular, we show how a powerful composition function based on the canonical tensor decomposition can exploit such a rich structure. A key point of our approach is the weight sharing constraint imposed on the factor matrices, which allows limiting the number of model parameters. Finally, we introduce a Tree-LSTM model which takes advantage of this composition function and we experimentally assess its performance on different NLP tasks.",282
" Searching for code fragments is a very common activity in software development. The advent of large code repositories like GitHub\footnote{https://github.com/} and StackOverflow\footnote{https://stackoverflow.com/} has only increased the number of developers to rely on these repositories to search and reuse existing code . Traditional Information Retrieval techniques  do not work well for code search and retrieval tasks due to limited shared vocabulary between the source code and the natural language search text . Often, developers who are new to a programming language, search for code snippets in a context-free natural language. The choice of words used to search may not overlap with the code snippets leading to failure of traditional information retrieval systems. Therefore, there is a need to gain a deeper understanding of code and text in order to find semantically relevant code snippet.  Consider an example where a developer has a functional requirement to validate if age is always lesser than  and alert otherwise. The developer is tasked to enforce this check in Java. A naive Java developer who is not familiar with the language might make a query based on the requirement as: java check condition correctness. The top 10 results\footnote{As of December 9, 2019} in StackOverflow do not discuss the assert keyword. A more programming friendly query such as java boolean check or the assert keyword itself results in code snippets demonstrating the steps as the top result in StackOverflow.  Use of deep neural network models have shown tremendous improvements in many tasks across domains including language tasks . This success can be largely attributed, in part, to their ability to learn meaningful relationships among words in documents efficiently and represent them in a way such that semantically equivalent words tend to have similar representations . One such family of models that are popular for determining text similarity are Siamese networks. First introduced by , a typical Siamese network consists of two identical sub networks that share weights. They work in tandem on different inputs and the output of both the networks are evaluated by a distance measure that also acts as a scoring function. This has been successfully applied in many similarity tasks in image domain  and recently in text domain as well . Another useful property of these models is their capability to learn from fewer data examples . Since code can be treated as a special kind of text data, one possible way to approach the problem of Semantic Code Search  is to treat it as a similarity task where the objective is to bring semantically equivalent code snippets and their natural language descriptions closer. Therefore, we study the application of Siamese networks to code and corresponding text descriptions for semantic code search.  We apply multiple variations of the base Siamese network model on two different datasets for semantic code search and study its efficacy.  We further take the state of the art baselines -  and  on these datasets and observe that Siamese networks can improve over the baseline results invariably . Finally, we present our analysis on the  performance of different Siamese network architectures explored and identify the conditions for improved performance.  The rest of the paper is organized as follows. We introduce some relevant prior art in section . Next, in section , we provide some background on Siamese networks and semantic code search and introduce terminology. In section , we describe our approach and the different architectures investigated. In section , we describe our experiments and present the results. Finally in section , we perform a detailed analysis of our observations, followed by conclusions in section .  % \tikz \draw[]  rectangle  node[pos=.2]{Answer Here:};  
"," % Availability of large code repositories and discussion forums, has enabled code search as a common activity among developers. They tend to express their intent as a query in natural language to find examples of related code. However performance of such systems are restricted due to 1) limited shared vocabulary across code and user query and 2) lack of semantic understanding of the user query.   % In this work, we evaluate Siamese network for the task of code retrieval. Building on two sub network, our siamese model can jointly learn between code and its description and represent them based on their semantic distance. We evaluate the performance of applying siamese networks 1) as a stand-alone model directly feeding code and its description 2) as a model stacked on existing state of the art models. We experiment on 2 datasets and 3 baseline models, and conclude that applying siamese networking on top of base models yield better embedding and improves the performance of the code sesearch taks significantly.  With the increase in the number of open repositories and discussion forums, the use of natural language for semantic code search has become increasingly common. The accuracy of the results returned by such systems, however, can be low due to 1) limited shared vocabulary between code and user query and 2) inadequate semantic understanding of user query and its relation to code syntax. Siamese networks are well suited to learning such joint relations between data, but have not been explored in the context of code search. In this work, we evaluate Siamese networks for this task by exploring multiple extraction network architectures. These networks independently process code and text descriptions before passing them to a Siamese network to learn embeddings in a common space. We experiment on two different datasets and discover that Siamese networks can act as strong regularizers on networks that extract rich information from code and text, which in turn helps achieve impressive performance on code search beating previous baselines on $2$ programming languages. We also analyze the embedding space of these networks and provide directions to fully leverage the power of Siamese networks for semantic code search.",283
"  We are motivated by the problem of labelling a dataset for word sense disambiguation, where we want to use a limited budget to collect annotations for a reasonable number of examples of each sense for each word.  This task can be thought of as an active learning problem , but with two nonstandard challenges. First, for any given word we can get a set of candidate labels from a knowledge base such as WordNet . However, this label set is not necessarily representative of what occurs in the data: there may exist labels in the knowledge base that do not occur in the corpus because the sense is rare in modern English;  conversely, there may also exist true labels that do not exist in our knowledge base. For example, consider the word ``bass.'' It is frequently used as a noun or modifier, e.g., ``the bass and alto are good singers'', or ``I play the bass guitar''. It is also commonly used to refer to a type of fish, but because music is so widely discussed online, the fish sense of the word is orders of magnitude less common than the low-frequency sound sense in internet text. The Oxford dictionary  also notes that bass  once referred to a fibrous material used in matting or chords, but that sense is not common in modern English. We want a method that collects balanced labels for the common senses, ``bass frequencies'' and ``bass fish'', and ignores sufficiently rare senses, such as ``fibrous material''. Second, the empirical distribution of the true labels may exhibit extreme skew: word sense usage is often power-law distributed  with frequent senses occurring orders of magnitudes more often than rare senses.    When considered individually, neither of these constraints is incompatible with existing active learning approaches:  incomplete label sets do not pose a problem for any method that relies on classifier uncertainty for exploration ; and extreme skew in label distributions has been studied under the guided learning framework wherein annotators are asked to explicitly search for examples of rare classes rather than simply label examples presented by the system .  But taken together, these constraints make standard approaches impractical. Search-based ideas from guided learning are far more sample efficient with a skewed label distribution, but they require both a mechanism through which annotators can search for examples and a correct label set because it is undesirable to ask annotators to find examples that do not actually occur in a corpus.    Our approach is as follows. We introduce a frequency threshold, , below which a sense will be deemed to be ``sufficiently rare'' % to be ignored  = p_y < \thresholdp_y\hat{p}_y$ by using importance-weighted samples. Once we have found examples of common classes, we switch to more standard active learning methods to find additional examples to reduce classifier uncertainty.  Overall, this paper makes two key contributions. First, we present an Exemplar Guided Active Learning  algorithm that offers strong empirical performance under extremely skewed label distributions by leveraging exemplar embeddings. Second, we identify a stopping rule that makes EGAL robust to misspecified label sets and prove that this robustness only imposes a logarithmic cost over a hypothetical approach that knows the correct label set.  Beyond these key contributions, we also present a new Reddit word sense disambiguation dataset, which is designed to evaluate active learning methods for highly skewed label distributions.  
"," We consider the problem of wisely using a limited budget to label a small subset of a large unlabeled dataset. We are motivated by the NLP problem of word sense disambiguation. For any word, we have a set of candidate labels from a knowledge base, but the label set is not necessarily representative of what occurs in the data: there may exist labels in the knowledge base that very rarely occur in the corpus because the sense is rare in modern English; and conversely there may exist true labels that do not exist in our knowledge base. Our aim is to obtain a classifier that performs as well as possible on examples of each 闁炽儲绔穙mmon class闁 that occurs with frequency above a given threshold in the unlabeled set while annotating as few examples as possible from 闁炽儲绗re classes闁 whose labels occur with less than this frequency. The challenge is that we are not informed which labels are common and which are rare, and the true label distribution may exhibit extreme skew. We describe an active learning approach that  explicitly searches for rare classes by leveraging the contextual embedding spaces provided by modern language models, and  incorporates a stopping rule that ignores classes once we prove that they occur below our target threshold with high probability. We prove that our algorithm only costs logarithmically more than a hypothetical approach that knows all true label frequencies and show experimentally that incorporating automated search can significantly reduce the number of samples needed to reach target accuracy levels.",284
" Argumentation is a paramount process in society, and debating on socially relevant topics requires high-quality and relevant arguments. In this work, we deal with the problem of argument search, which is also known as argument retrieval. The goal is to develop an \acrfull{arg_ret_sys} which organizes arguments, previously extracted from various sources  , in an accessible form. Users then formulate a query to access relevant arguments retrieved by the \acrshort{arg_ret_sys}.  The query can be defined as a topic, e.g. Energy in which case the \acrshort{arg_ret_sys} retrieves all possible arguments without further specification. Our work deals with a more advanced case, where a query is formulated in the form of a claim, and the user expects premises attacking or supporting this query claim.  An example of a claim related to the topic Energy could be ``We should abandon Nuclear Energy"" and a supporting premise, e.g., ``Accidents caused by Nuclear Energy have longstanding negative impacts"". % A popular search methodology to find relevant premises is a similarity search, where the representations of the retrieved premises are similar to the representation of the  query claim. However, as noted by, the relevance of a premise does not necessarily coincide with pure text similarity.  Therefore, the authors of  advocate to utilize the similarity between the query claim and other claims in an \acrshort{arg_ret_sys} database and retrieve the premises assigned to the most similar claims. However, such \acrshort{arg_ret_sys} requires ground truth information about the premise to claim assignments and therefore has limited applicability: Either the information sources are restricted to those sources where such information is already available or can automatically be inferred, or expensive human annotations are required. To mitigate this problem and keep the original system's advantages, we propose to use a machine learning model to learn the relevance between premises and claims. Using this model, we can omit the  claim-claim matching step and evaluate the importance of  candidate premises directly for the query claim. Since the relevance is defined on the semantic level, we have to design an appropriate training task to enable the model to learn semantic differences between relevant and non-relevant premises. Furthermore, an essential subtask for an \acrshort{arg_ret_sys} is to ensure that the retrieved premises do not repeat the same ideas.  Previous approaches employ clustering to eliminate duplicates.  However, clustering approaches often group data instances by other criteria than expected by the users, as also observed in \gls{argument-mining} applications.  For our method, we propose an alternative to clustering based on the idea of core-sets, where the goal is to cover the space of relevant premises as well as possible. % This is samplepaper.tex, a sample chapter demonstrating the % LLNCS macro package for Springer Computer Science proceedings; % Version 2.20 of 2017/10/04 % \documentclass[runningheads]{llncs} % \usepackage{graphicx} \usepackage{xcolor} \usepackage{amsmath} \usepackage{amssymb} %\usepackage{ulem} \usepackage{multirow} \usepackage{booktabs} \usepackage{footnote} \makesavenoteenv{tabular} \makesavenoteenv{table} \usepackage{cite} \usepackage[ruled,vlined]{algorithm2e} \usepackage{float} \interfootnotelinepenalty=10000  % Used for displaying a sample figure. If possible, figure files should % be included in EPS format. % % If you use the hyperref package, please uncomment the following line % to display URLs in blue roman font according to Springer's eBook style: \usepackage{hyperref} \renewcommand\UrlFont{\color{blue}\rmfamily}  % for equal contribution \makeatletter \newcommand{\printfnsymbol}[1]{%   \textsuperscript{\@fnsymbol{#1}}% } \makeatother  % \title{Diversity Aware Relevance Learning for Argument Search} % %\titlerunning{Abbreviated paper title} % If the paper title is too long for the running head, you can set % an abbreviated paper title here %  \author{ Michael Fromm\thanks{equal contribution}\inst{1} %\orcidID{0000-0002-7244-4191} \and Max Berrendorf\printfnsymbol{1} \inst{1} %\orcidID{0000-0001-9724-4009} \and Sandra Obermeier \inst{1}  \and Thomas Seidl \inst{1} %\orcidID{0000-0002-4861-1412} \and Evgeniy Faerman \inst{1} }   \authorrunning{Fromm et al.} % First names are abbreviated in the running head.  % If there are more than two authors, 'et al.' is used.   \institute{Database Systems and Data Mining, LMU Munich, Germany  \\  \email{fromm@dbs.ifi.lmu.de}}   \newcommand{\todo}[1]{\textcolor{red}{#1}}  % Acronyms \usepackage[acronym]{glossaries} %\makeglossaries  % Example % \newacronym{acrid}{ACR}{Acronym for Clustering Representations} % \acrshort{arcrid} -> ACR % \acrlong{arcid} -> Acronym for Clustering Representations % \acrfull{arcid} -> Acronym for Clustering Representations  \newacronym{argument-mining}{AM}{Argument Mining} \newacronym{arg_ret_sys}{ARS}{Argument Retrieval System} \newacronym{bert-based-premise-representation}{BERT}{BERT} \newacronym{claim-based-premise-representation}{CLAIM-SIM}{CLAIM-SIM} \newacronym{relevance-model}{relevance-model}{relevance model}  % methods \newacronym{dumani-first512}{first512}{Dumani first512} \newacronym{dumani-sentences}{sentences}{Dumani sentences} \newacronym{dumani-sliding-window}{sliding}{Dumani sliding} \newacronym{bert-zero-shot-knn}{BERT Zero-Shot}{BERT Zero-Shot} \newacronym{learned-similarity-knn}{Learned Similarity}{Learned Similarity} \newacronym{biased-coreset}{Biased Coreset}{Biased Coreset}  \newacronym{bert-zero-shot-clustered}{BERT Zero-Shot + Cluster}{}   \DeclareMathOperator*{\argmax}{argmax}  %\newcommand{\relevanceModel}{relevance model }  % \newcommand{\dumaniFirst}{Dumani first512 } % \newcommand{\dumaniSentences}{Dumani sentences }  % \newcommand{\dumaniSliding}{Dumani sliding }  % \newcommand{\topSimilar}{Premise Similarity } % \newcommand{\topSimilarClusterRepresentatives}{Clustered Premise Similarity } % \newcommand{\mostImportant}{Premise Importance }  % \newcommand{\BertNegatives}{Bert-Negatives } % \newcommand{\SimpleNegatives}{Simple-Negatives } % \newcommand{\SameTopicNegatives}{Same-Topic-Negatives }  % disable hyperref for glossaries  \glsdisablehyper      \keywords{Argument Similarity  \and Argument Clustering \and Argument Retrieval}      
"," In this work, we focus on retrieving relevant arguments for a query claim covering diverse aspects. State-of-the-art methods rely on explicit mappings between claims and premises and thus cannot utilize extensive available collections of premises without laborious and costly manual annotation. Their diversity approach relies on removing duplicates via clustering, which does not directly ensure that the selected premises cover all aspects. This work introduces a new multi-step approach for the argument retrieval problem. Rather than relying on ground-truth assignments, our approach employs a machine learning model to capture semantic relationships between arguments. Beyond that, it aims to cover diverse facets of the query instead of explicitly identifying duplicates.  Our empirical evaluation demonstrates that our approach leads to a significant improvement in the argument retrieval task, even though it requires fewer data than prior methods. Our code is available at \url{https://github.com/fromm-m/ecir2021-am-search}.",285
"  Speaker diarization is the process of partitioning an audio stream into homogeneous segments according to speaker identities. Thus, diarization determines ``who spoke when'' in a multi-speaker environment, with a variety of applications to conversations involving multiple speakers, such as meetings, television shows, medical consultations, or call center conversations. In particular, the speaker boundaries produced by a diarization system can be used to map transcripts generated by a multi-speaker automatic speech recognition  system into speaker-attributed transcripts . Moreover, speaker embeddings inferred by diarization can help the ASR system adapt to, or focus on the speech of a targeted speaker .   Conventional speaker diarization systems are based on clustering of speaker embeddings. In this approach, several components are integrated into a single system: speech segments are determined by voice activity detection ; these speech segments are further divided into smaller chunks of fixed size; speaker embeddings are then extracted by speaker embedding extractors for each chunk; finally, those speaker embeddings are clustered to map each segment to a speaker identity . For embeddings, i-vectors , x-vectors , or d-vectors  are commonly used. Clustering methods typically used for speaker diarization are agglomerative hierarchical clustering  , k-means clustering , and spectral clustering . Recently, neural network-based clustering has been explored . Clustering-based speaker diarization achieves good performance but has several shortcomings. First, it relies on multiple modules  that are trained separately. Therefore, clustering-based systems require careful joint calibration in the building process. Second, systems are not jointly optimized to minimize diarization errors; clustering in particular is an unsupervised process. Finally, clustering does not accommodate overlapping speech naturally, even though recent work has proposed ways to handle regions with simultaneously active speakers in clustering .  End-to-end neural diarization  with self-attention  is one of the approaches that aim to model the joint speech activity of multiple speakers. It integrates voice activity and overlap detection with speaker tracking in end-to-end fashion.  Moreover, it directly minimizes diarization errors and has demonstrated excellent diarization accuracy on two-speaker telephone conversations. However, EEND as originally formulated is limited to a fixed number of speakers because the output dimension of the neural network needs to be prespecified. Several methods have been proposed recently to overcome the limitations of EEND. One approach uses a speaker-wise chain rule to decode a speaker-specific speech activity iteratively conditioned on previously estimated speech activities . Another approach proposes an encoder/decoder-based attractor calculation . The embeddings of multiple speakers are accumulated over the time course of the audio input, and then disentangled one-by-one, for speaker identity assignment by speech frame.  However, all these state-of-the-art EEND methods only work in an offline manner, which means that the complete recording must be available before diarization output is generated. This makes their application impractical for settings where potentially long multi-speaker recordings need to be processed incrementally .   In this study, we propose a novel method to perform EEND in a blockwise online fashion so that speaker identities are tracked with low latency soon after new audio arrives, without much degradation in accuracy compared to the offline system. We utilize the incremental Transformer encoder, where we attend to only its left contexts and ignore its right contexts, thus enabling blockwise online processing. Furthermore, the incremental Transformer encoder uses block-level recurrence in the hidden states to carry over information block by block, reducing computation time while attending to previous blocks. To our knowledge, ours is the first method that uses the incremental Transformer encoder with block-level recurrence to enable online speaker diarization.   
"," We present a novel online end-to-end neural diarization system, BW-EDA-EEND, that processes data incrementally for a variable number of speakers. The system is based on the Encoder-Decoder-Attractor  architecture of Horiguchi et al., but utilizes the incremental Transformer encoder, attending only to its left contexts and using block-level recurrence in the hidden states to carry information from block to block, making the algorithm complexity linear in time. We propose two variants: For unlimited-latency BW-EDA-EEND, which processes inputs in linear time, we show only moderate degradation for up to two speakers using a context size of 10 seconds compared to offline EDA-EEND. With more than two speakers, the accuracy gap between online and offline grows, but the algorithm still outperforms a baseline offline clustering diarization system for one to four speakers with unlimited context size, and shows comparable accuracy with context size of 10 seconds. For limited-latency BW-EDA-EEND, which produces diarization outputs block-by-block as audio arrives, we show accuracy comparable to the offline clustering-based system.",286
" % The very first letter is a 2 line initial drop letter followed % by the rest of the first word in caps. %  % form to use if the first word consists of a single letter: % {A}{demo} file is .... %  % form to use if you need the single drop letter followed by % normal text : % {A}{}demo file is .... %  % Some journals put the first two words in caps: % {T}{his demo} file is .... %  % Here we have the typical use of a ""T"" for an initial drop letter % and ""HIS"" in caps to complete the first word.  {T}{here} are many methods for automatic speech recognition  systems, such as GMM-HMM and deep neural network  based acoustic models . Recently, end-to-end speech recognition methods  have made significantly breakthroughs. Although these ASR methods have made a lot of progresses on clean speech signals, the performance could be dramatically degraded in the noisy and reverberation environments. In realistic environments, recorded speech signals are always interfered by various background noises and reverberations. Therefore, improving the robustness of ASR is very important. This paper focuses on boosting the noise robustness of end-to-end speech recognition.  %In realistic environments, recorded speech signals are always interfered by various background noises and reverberations. However, these interferences can dramatically degrade the performance of automatic speech recognition  systems.  In order to boost the noise robustness of ASR, there are three mainstream methods. The first mainstream method is adding the speech enhancement component at the front-end of ASR. Speech enhancement methods include spectral subtraction , Wiener filtering  and deep neural network  based speech enhancement methods . However, speech enhancement optimizes their models to estimate the target speech, which is different from the speech recognition part. Therefore, speech enhancement methods fail to optimize towards the final objective, which leads to a suboptimal solution . In addition, the enhanced speech by these speech enhancement methods usually generates over-smoothed speech, which is the reason of speech distortion after speech enhancement. The speech distortion can degrade the performance of ASR . Therefore, the performance of this approach is highly dependent on the performance of the enhancement front-end . %In addition, speech enhancement methods usually tend to generate over-smoothed speech because they use a particular form of loss functions such as mean squared error , which leads to speech distortion. However, the performance of ASR can be degraded by the speech distortion . Therefore, the performance of this approach is highly dependent on the performance of the enhancement front-end .  The second mainstream method uses the multi-condition training  to boost the noise robustness of ASR. MCT uses different kinds of data  to train the speech recognition model. However, the complexity and computing costs of MCT are increased. In addition, it gives unimpressive performance on the unmatched conditions  and the performance is also affected by the speech distortion . In order to alleviate speech distortion problem, the enhancement front-end enhances both training and test set first, and ASR model is trained with the enhanced data. It can improve the ASR performance in some degree, but it still highly depends on the performance of the enhancement front-end. Different from the MCT method, the SpecAugment  directly applies the  data augmentation to the input features of neural networks . The SpecAugment is used only during the training, which consists of three spectrogram deformations:  time warping,  time and frequency masking. Although the SpecAugment can improve the performance of end-to-end ASR, it needs to be improved on the noisy condition.  %%MCT not only uses the clean data for training but also the noisy data. Therefore, MCT can learn different distributions from the clean and noisy data so that the speech recognition model boosts the noise robustness. However, the complexity and computing costs of MCT are increased. In addition, it gives unimpressive performance in unmatched conditions  and the performance is also affected by the speech distortion . In order to alleviate speech distortion problem, the enhancement front-end enhances both training and test set first, and ASR model is trained on the enhanced training set. It can improve the ASR performance in some degree, but it still highly dependent on the performance of the enhancement front-end. Different from the MCT method, the SpecAugment  directly applies the  data augmentation to the input features of neural networks . The SpecAugment is used only during the training, which consists of three spectrogram deformations:  time warping,  time and frequency masking. Although the SpecAugment can improve the performance of end-to-end ASR, it is still affected by the speech distortion problem.  %Then it applies the enhanced data for test. Although this method can boost the robustness of ASR in some degree, the complexity and computing costs are increased. In addition, the performance of MCT is also affected by the speech distortion .  %In order to boost the noise robustness of ASR, the mainstream method is adding the speech enhancement component at the front-end of ASR. However, speech enhancement aims to estimate the target speech , which is different from the speech recognition. Therefore, speech enhancement methods fail to optimize towards the final objective, which leads to a suboptimal solution . In addition, speech enhancement usually leads to speech distortion. However, the performance of ASR can be degraded by the speech distortion . Therefore, the performance of this approach is highly dependent on the performance of the enhancement front-end . In order to alleviate this issue, the multi-condition training   method is proposed. MCT not only uses the clean data for training but also the noisy and enhanced data. Although this method can boost the robustness of ASR in some degree, the complexity and computing costs are increased. In addition, the performance of MCT is also affected by the speech distortion.  %speech enhancement methods usually tend to generate over-smoothed speech because they use a particular form of loss functions such as mean squared error , which leads to speech distortion. However, the performance of ASR can be degraded by the speech distortion . Therefore, the performance of this approach is highly dependent on the performance of the enhancement front-end .  %In order to boost the noise robustness of ASR, the mainstream method is adding the speech enhancement component at the front-end of ASR. Speech enhancement methods include spectral subtraction , Wiener filtering  and deep neural network  based speech enhancement methods . However, speech enhancement part optimizes their models to estimate the target speech, which is different from the speech recognition part. Therefore, speech enhancement methods fail to optimize towards the final objective, which leads to a suboptimal solution . In addition, speech enhancement methods usually tend to generate over-smoothed speech because they use a particular form of loss functions such as mean squared error , which leads to speech distortion. However, the performance of ASR can be degraded by the speech distortion . Therefore, the performance of this approach is highly dependent on the performance of the enhancement front-end . %speech enhancement methods usually lead to speech distortion. %However, applying these speech enhancement methods has two shortcomings. Firstly,  %Firstly, these speech enhancement methods will increase the computation and has more complex pipelines, which limit the applications of speech recognition. Secondly,   The third mainstream method is the joint training methods . These methods apply the joint training framework to optimize the speech enhancement and recognition, simultaneously. The reason is that speech enhancement and speech recognition are not two independent tasks and they can clearly benefit from each other. In order to boost noise robustness of end-to-end ASR, in , authors propose a joint adversarial enhancement training method. They utilize the joint training framework to optimize the mask based enhancement network and attention based encoder-decoder speech recognition network. However, this method only uses the enhanced feature as the input of speech recognition, which is still affected by the speech distortion problem. In addition, in the noisy AISHELL-1  dataset, the character error rate  of this method is still more than 50\%, which needs to be improved. As for the end-to-end speech recognition, speech transformer  models have shown impressive performance and acquired state-of-the-art results. Self-attention network  is one of the key components of speech transformer and it is more powerful to model long-term dependencies than recurrent neural networks  based sequence to sequence models. Therefore, applying the joint training of enhancement and speech transformer can further improve the performance of robust end-to-end ASR. %One of the key components of speech transformer is self-attention network , which is more powerful to model long-term dependencies than Recurrent neural networks -based sequence to sequence models. Therefore, the performance of robust end-to-end ASR can be further improved by using the joint training of enhancement and speech transformer. %To address the speech distortion problem and acquire an optimal performance, the joint training method of speech enhancement and speech recognition is proposed for robust ASR . This is because that speech enhancement and speech recognition are not two independent tasks and they can clearly benefit from each other. In , a joint adversarial enhancement training method is proposed to boost noise robustness of end-to-end ASR systems. It applies the joint training of mask based-enhancement network and attention-based encoder-decoder speech recognition network. However, this method only uses the enhanced features as the input of speech recognition, which is still affected by the speech distortion problem. And the character error rate of this method is still more than 50\% in the noisy AISHELL-1  dataset. Speech transformer models have shown impressive performance in end-to-end speech recognition  and acquire state-of-the-art performances. One of the key components of speech transformer is self-attention network , which is more powerful to model long-term dependencies than Recurrent neural networks -based sequence to sequence models. Therefore, the performance of robust end-to-end ASR can be further improved by using the joint training of enhancement and speech transformer. %Liu et al propose a joint adversarial enhancement training to boost noise robustness of end-to-end ASR systems . They use the joint training of mask based-enhancement network and attention-based encoder-decoder speech recognition network   %In this paper, we propose a joint training method of enhancement and speech transformer for robust end-to-end ASR, which uses the deep attention fusion representations of noisy and enhanced features. To be our best knowledge, it is the first time to apply the speech transformer and enhancement joint training for robust end-to-end ASR. Specifically, the proposed joint training method includes two parts:   In , a one-pass robust speech recognition method is proposed. It combines the noisy and enhanced features by a gating mechanism. Although it can improve the robust of ASR, the enhancement and speech recognition are trained separately instead of the joint training algorithm. In addition, the simple gate mechanism can not make full use of the sequence information so that it can not fuse the noisy and enhanced features very well. %The speech enhancement and speech recognition are   Fig. illustrates the spectrogram example of a test speech sample. From Fig. we can find that the spectrogram of the enhanced speech by the enhancement network has significant leaks  by block boxes), which leads to the speech distortion. There are significant leaks in these black boxes. This is because that the noise is dominant in these T-F bins, which drowns the target speech. Therefore, the enhancement network deals with these T-F bins as the noise signals and removes most of the information. These leaks lose so much very important speech information, for example: formants. Although the enhancement network can remove noise signals in some degree, these leaks are unknown for the speech recognition system and lose so much speech information. These are the reasons why speech distortion damages the performance of speech recognition.   In this paper, we propose a gated recurrent fusion  with joint training framework for robust end-to-end ASR. In order to address the speech distortion problem, motivated by , the GRF is utilized to dynamically combine the noisy and enhanced features. Therefore, the GRF can offset these leaks from the noisy features. In addition, GRF can reduce the noise from the enhanced features. So the GRF aims to learn to adaptively select and fuse the relevant information from noisy and enhanced features by making full use of the gate and memory modules. The GRF can extract more appropriate and robust speech features. In addition, we apply the joint training algorithm to optimize the enhancement and speech recognition. The state-of-the-art end-to-end ASR method speech transformer with self-attention method is used as the speech recognition component. Specifically, the proposed joint training method includes three parts: speech enhancement, gated recurrent fusion and speech recognition. With the joint optimization of enhancement and recognition, the proposed model is expected to learn more robust representations suitable for the recognition task automatically.  %In this paper, we propose a joint training method of enhancement and speech transformer for robust end-to-end ASR, which uses the deep attention fusion  representations of noisy and enhanced features. We apply the state-of-the-art end-to-end ASR method speech transformer with self-attention as the speech recognition component. In addition, to further alleviate speech distortion problem, the deep attention fusion component is utilized to combine the noisy and enhanced features, which can dynamically fuse these features in a deep way so that can extract more appropriate and robust speech features. Therefore, these GRF representations can learn the raw fine structures from the noisy features to make up the speech distortion. Meanwhile, they can also remove the noise signals form the enhanced features. Specifically, the proposed joint training method includes three parts: speech enhancement, deep attention fusion and speech recognition. With the joint optimization of enhancement and recognition, the proposed model is expected to learn more robust representations suitable for the recognition task automatically.  %dynamically select appropriate speech features. As for the enhancement component, we apply the mask-based enhancement network to estimate the clean speech. As for the speech recognition component, the speech transformer with self-attention is used for ASR.   To summarize, the main contribution of this paper is two-fold. Firstly, to address the speech distortion problem, the gated recurrent fusion algorithm is utilized to dynamically fuse the noisy and enhanced features. Secondly, to the best of our knowledge, it is the first time to apply the speech transformer and single channel speech enhancement for the joint training framework. Our experiments are conducted on AISHELL-1 Mandarin dataset. Experimental results show that the proposed method achieves the relative CER reduction of 10.02\% over the conventional joint enhancement and transformer method using the enhanced features only. Especially for the low signal-to-noise ratios, our proposed method can achieve better performance.  %The rest of this paper is organized as follows. Section 2 presents the conventional joint training method for robust ASR. Our proposed method is stated in section 3. Section 4 shows detailed experiments and results. Section 5 draws conclusions. The rest of this paper is organized as follows. Section \ presents the conventional joint training method for robust ASR. Section \ introduces our proposed joint training method with gated recurrent fusion algorithm. The experimental setup is stated in section \. Section \ shows experimental results. Section \ shows the discussions. Section \ draws conclusions.     %The rest of this paper is organized as follows. Section \ presents discriminative learning for monaural speech separation using deep embedding features. Section \ introduces the proposed end-to-end post-filter speech separation method. The experimental setup is stated in section \. Section \ shows experimental results. Section \ shows the discussions. Section \ draws conclusions.    
"," %The joint training of speech enhancement and speech recognition methods have acquired good performances for robust end-to-end automatic speech recognition . However, they only use the enhanced features as the input of speech recognition component, which is still affected by the speech distortion problem. In this paper, we propose a deep attention fusion  of noisy and enhanced features with joint enhancement and speech transformer training method for robust end-to-end ASR. We apply the state-of-the-art end-to-end ASR method speech transformer as our speech recognition component. To address the speech distortion problem and extract more robust features for ASR, we propose the deep attention fusion algorithm to combine the noisy and enhanced features deeply. Therefore, these GRF representations can learn the raw fine structures from the noisy features to alleviate the speech distortion. Meanwhile, they can also remove the noise signals form the enhanced features. Systematic experiments on AISHELL-1 show that the proposed method achieves the relative character error rate  reduction of 8.32\% over the conventional joint enhancement and transformer method using the enhanced features only. Especially for the low signal-to-noise ratios, our proposed method can achieves better performances. %The joint training framework for speech enhancement and recognition methods have obtained quite good performances for robust end-to-end automatic speech recognition . However, these methods only utilize the enhanced feature as the input of speech recognition component, which are affected by the speech distortion problem. In order to address this problem, in this paper, we propose a gated recurrent fusion  method with joint training framework for robust end-to-end ASR. The proposed method consists of speech enhancement, GRF and speech recognition. Firstly, the mask based speech enhancement network is applied to enhance the input speech. Secondly, to address the speech distortion problem and extract more robust features for end-to-end ASR, the GRF algorithm is used to dynamically combine the noisy and enhanced features. Therefore, the GRF can not only remove the noise signals from the enhanced features, but also learn the raw fine structures from the noisy features so that it can alleviate the speech distortion. Thirdly, to improve the performance of ASR, the state-of-the-art end-to-end speech recognition method speech transformer with self-attention algorithm is used as the speech recognition component. Finally, the joint training framework is utilized to optimize these three components, simultaneously. Our experiments are conducted on an open-source Mandarin speech corpus called AISHELL-1. Experimental results show that the proposed method achieves the relative character error rate  reduction of 10.04\% over the conventional joint enhancement and transformer method only using the enhanced features. Especially for the low signal-to-noise ratio , our proposed method can achieves better performances with 12.67\% reduction, which suggests the potential of our proposed method. The joint training framework for speech enhancement and recognition methods have obtained quite good performances for robust end-to-end automatic speech recognition . However, these methods only utilize the enhanced feature as the input of the speech recognition component, which are affected by the speech distortion problem. In order to address this problem, this paper proposes a gated recurrent fusion  method with joint training framework for robust end-to-end ASR. The GRF algorithm is used to dynamically combine the noisy and enhanced features. Therefore, the GRF can not only remove the noise signals from the enhanced features, but also learn the raw fine structures from the noisy features so that it can alleviate the speech distortion. The proposed method consists of speech enhancement, GRF and speech recognition. Firstly, the mask based speech enhancement network is applied to enhance the input speech. Secondly, the GRF is applied to address the speech distortion problem. Thirdly, to improve the performance of ASR, the state-of-the-art speech transformer algorithm is used as the speech recognition component. Finally, the joint training framework is utilized to optimize these three components, simultaneously. Our experiments are conducted on an open-source Mandarin speech corpus called AISHELL-1. Experimental results show that the proposed method achieves the relative character error rate  reduction of 10.04\% over the conventional joint enhancement and transformer method only using the enhanced features. Especially for the low signal-to-noise ratio , our proposed method can achieves better performances with 12.67\% CER reduction, which suggests the potential of our proposed method. %The joint training of speech enhancement and speech recognition methods have acquired good performances for robust end-to-end automatic speech recognition . However, they only use the enhanced features as the input of speech recognition component, which is still affected by the speech distortion problem. In this paper, we propose a gated recurrent fusion  of noisy and enhanced features with joint enhancement and speech transformer training method for robust end-to-end ASR. We apply the state-of-the-art end-to-end ASR method speech transformer with self-attention algorithm as our speech recognition component. To address the speech distortion problem and extract more robust features for end-to-end ASR, we apply the GRF algorithm to dynamically combine the noisy and enhanced features. Therefore, these GRF representations can learn the raw fine structures from the noisy features so that they can make up the speech distortion. Meanwhile, they can also remove the noise signals form the enhanced features to improve the robustness of end-to-end speech recognition. Our experiments are conducted on an open-source Mandarin speech corpus called AISHELL-1. Experimental results show that the proposed method achieves the relative character error rate  reduction of 10.04\% over the conventional joint enhancement and transformer method only using the enhanced features. Especially for the low signal-to-noise ratio , our proposed method can achieves better performances with 12.67\% reduction, which suggests the potential of our proposed method.",287
"  {M}{usic} composition is a human creative process that requires a wide range of strong musical knowledge and expertise to create soothing music which continues to remain in our heart forever. Given the vast majority of music lovers and the limited availability of professional music composers, there is a strong need for machines to assist human creativity. Recent advancement in the software based music creation technology helped the professional and amateur music creators to produce music with great joy and ease of production in masses to be consumed by the music consumers with personal computers and hand-held devices. %The software applications such as Ableton Live, FL Studio, Logic Pro X, Garageband are the few examples which changed the way the music is produced in the past.  Though there exists a plenty of machine assistance to create high quality music with relative ease of production, the process of songwriting that is automatically generating lyrics, composing melody corresponding to the generated lyrics and synthesizing singing voice corresponding to the generated melody and lyrics remained as mutually exclusive tasks. Till date, the construction of novel/original songs is limited to the individuals who possess the following skills: the ability to create lyrics, compose melody and combine lyrics and melody to create a rational, relevant and soothing final complete songs. %Though by remixing technology, we can create new music to some extent which satisfies some music lovers, there is a need for creating truly novel songs under multiple constraints on remaking existing works.                       % -------------------------------------------------------------------------------------------------------------             In literature, we can find considerable amount of research work published on automatic music generation . Early machine assisted music generation is mostly based on music theory and expert domain knowledge to create novel works. With the advent of data driven approaches and exploded public music collections in the internet, data driven methods such as Hidden Markov models, graphic models and deep learning models showed a potential for music creation. Though there exists substantial amount of research on unconditional music generation, there exists considerably less amount of work done so far on generating melody from lyrics given in the form of text, which we call conditional melody/song generation from lyrics. The primary reasons for substantially less research on conditional melody generation can be attributed to i) the non-availability of the direct source for lyrics-melody pair dataset to train the data driven models, ii) a lyrics composition can have multiple melodic representations, which makes it hard to learn the correlation between the lyrics and melodies, and iii) it is hard to evaluate the generated melodies by objective measures.  This paper focuses on the most challenging aspect of algorithmic songwriting process which enables the human community to discover original lyrics, and  melodies suitable for the generated lyrics. To the best our knowledge, the proposed AutoNLMC is the first attempt to make the whole process of songwriting automatic using artificial neural networks. We also present the lyrics to vector model which is trained on a large dataset of popular English songs to obtain the dense representation of lyrics at syllables, words and sentence levels. The proposed AutoNLMC is an attention based encoder-decoder sequential recurrent neural network model consists of a lyric generator, lyric encoder and melody decoders trained end-to-end. We train several encoder-decoder models on various dense representations of the lyric tokens to learn the correlation between lyrics and corresponding melodies. Further, we prove the importance of dense representation of lyrics by various qualitative and quantitative measures. AutoNLMC is designed in such a way that it can generate both lyrics and corresponding melodies automatically for an amateur or a person without music knowledge by accepting a small piece of initial seed lyrics as input. It can also take lyrics from professional lyrics writer to generate the matching meaningful melodies.  
"," In this paper, we propose a technique to address the most challenging aspect of algorithmic songwriting process, which enables the human community to discover original lyrics, and melodies suitable for the generated lyrics. The proposed songwriting system, Automatic Neural Lyrics and Melody Composition  is an attempt to make the whole process of songwriting automatic using artificial neural networks. Our lyric to vector  model trained on a large set of lyric-melody pairs dataset parsed at syllable, word and sentence levels are large scale embedding models enable us to train data driven model such as recurrent neural networks for popular English songs. AutoNLMC is a encoder-decoder sequential recurrent neural network model consisting of a lyric generator, a lyric encoder and melody decoder trained end-to-end. AutoNLMC is designed to generate both lyrics and corresponding melody automatically for an amateur or a person without music knowledge. It can also take lyrics from professional lyric writer to generate matching melodies. The qualitative and quantitative evaluation measures revealed that the proposed method is indeed capable of generating original lyrics and corresponding melody for composing new songs.",288
"  Machine learning systems struggle to learn predictors that are robust to distribution shift. When tested on i.i.d data drawn from the training distribution these systems can achieve nearly perfect accuracy, even when regularized to prevent over-fitting. However, performance can degrade to below-chance accuracy when the testing and training distributions are even slightly different . The field of Domain Generalization   addresses this challenge by proposing robust methods that ensure good test performance on distributions that are different from but systematically related to the training distribution . Invariant Risk Minimization   is a one of several recently successful approaches to Multi-Source Domain Generalization  which encourages models to learn predictors with invariant performance across different ``domains'', or ``environments'' . Given  different training environments, these models extract a set of predictors from the feature space such that the conditional distribution of the outcomes given the predictors is invariant  across all training environments. These predictors can consequently generalize well to all test out-of-distribution  environments which share this same invariance. Building on work in philosophy which characterizes causation as invariance , existing invariance-based DG methods have been interpreted as a weak form of causal discovery whose returned predictors are the causal factors underlying the phenomena we wish to predict.   Fairness can be often characterized by robustness to changes in ``sensitive attributes'' , especially in the context of toxicity classification.      Consider an automated moderation system used by an online news platform to determine which comments on a news article are toxic to online discourse and should be censored. The performance of a fair system should not be affected by characteristics such as whether the comment is about issues related to race, gender or other politically sensitive topics. Alternative definitions of distributive fairness differ in how the system's predictions should be invariant to changes in the sensitive attribute. For instance, statistical definitions such as Demographic Parity require that some conditional distribution of predictions given the sensitive attribute are invariant to the sensitive attribute , and causal definitions such as counterfactual fairness  require that every individual's prediction is invariant to counterfactual changes in that individual's sensitive attribute. There are a number of ethical and legal criticisms to be levied against systems that predict based on sensitive group membership . Moreover, over-reliance on sensitive information could decrease robustness when the predictive performance of this information spuriously depends on the environmental context in which it is employed. Discussion about non-caucasian racial identities, for example, may be highly predictive of comment toxicity on white supremacist internet forums whose members routinely make discriminatory remarks about ethnic minorities. However, on other internet forums that are more welcoming of diversity the association between racial identity mention and toxicity would likely be far weaker. This brittleness of sensitive information has been identified as a key challenge that Perspective API, a Google-backed internet comment toxicity classifier, faced during implementing in real-world contexts , and has also been observed to cause bias in sentiment analysis  and facial detection  tasks. Fair models, then, can perhaps be constructed by learning predictors whose performance remains invariant across a variety of different environments.   In this work, we empirically demonstrate that Domain Generalization can used to build fair machine learning systems by constructing models that are invariant to spurious correlations involving the sensitive attribute. Specifically, we assess  the performance of IRM on a fair internet comment toxicity classification task derived from the Civil Comments Dataset. In this task, the model must generalize from biased training environments exhibiting a strong but spurious correlation between mention of a particular demographic identity and toxicity to a test environment in which this correlation is reversed.    Our contributions are as follows:     
","     Robustness is of central importance in machine learning and has given rise to the fields of domain generalization and invariant learning, which are concerned with improving performance on a test distribution distinct from but related to the training distribution. In light of recent work suggesting an intimate connection between fairness and robustness, we investigate whether algorithms from robust ML can be used to improve the fairness of classifiers that are trained on biased data and tested on unbiased data. We apply Invariant Risk Minimization , a domain generalization algorithm that employs a causal discovery inspired method to find robust predictors, to the task of fairly predicting the toxicity of internet comments. We show that IRM achieves better out-of-distribution accuracy and fairness than Empirical Risk Minimization  methods, and analyze both the difficulties that arise when applying IRM in practice and the conditions under which IRM will likely be effective in this scenario. We hope that this work will inspire further studies of how robust machine learning methods relate to algorithmic fairness.",289
"   Deep Neural Networks  are the current state-of-the-art models in many speech related tasks. From a computational neuroscience perspective, DNNs can be seen as rate coding based models, in the sense that if a neuron is responsive to a given stimulus, then if we augment the stimulus intensity, the neuron output intensity will also increase. Temporal coding based models try to also take into account information carried by the temporal structure of the stimulus. In the case of Spiking Neural Networks , spike timing and delays between spikes is important in order to retrieve patterns in the spike sequences given as input to a model. %https://en.wikipedia.org/wiki/Neural_coding  There is a growing interest for SNNs applied to speech recognition tasks, from isolated word and phone recognition,to large-vocabulary automatic speech recognition  very recently. Reasons are that the audio speech signal is particularly suited to event-driven models such as SNNs, SNNs are also more biologically realistic than DNNs, hardware friendly and energy efficient models, if implemented on dedicated energy-efficient neuromorphic chips. Furthermore, it has been shown recently that SNNs can be trained efficiently, in a supervised manner, using backpropagation with a surrogate gradient trick. This new approach allows to train SNNs as one would do for DNNs.  In this work, we propose to use supervised SNNs for speech command  recognition. We explore the Leaky Integrate-and-Fire  neuron model for this task, and show that convolutional SNNs can reach an accuracy very close to the one obtained with state-of-the-art DNNs, for this task. Our main contributions are the following: i) we propose to use dilated convolution spiking layers, ii) we define a new regularization term to penalize the averaged number of spikes to keep the spiking neuron activity as sparse as possible, iii) we show that the leaky variant of the neuron model outperforms the non-leaky one , used in.  In order to facilitate reproducibility, our code using PyTorch is available online\footnote{https://github.com/romainzimmer/s2net}.  
"," Deep Neural Networks  are the current state-of-the-art models in many speech related tasks. There is a growing interest, though, for more biologically realistic, hardware friendly and energy efficient models, named Spiking Neural Networks . Recently, it has been shown that SNNs can be trained efficiently, in a supervised manner, using backpropagation with a surrogate gradient trick. In this work, we report speech command  recognition experiments using supervised SNNs. We explored the Leaky-Integrate-Fire  neuron model for this task, and show that a model comprised of stacked dilated convolution spiking layers can reach an error rate very close to standard DNNs on the Google SC v1 dataset: \ER{94.5}\%, while keeping a very sparse spiking activity, below 5\%, thank to a new regularization term. We also show that modeling the leakage of the neuron membrane potential is useful, since the LIF model outperformed its non-leaky model counterpart significantly.",290
" Books have been the one of the most important mediums for recording information and imparting knowledge in human history. Books can be classified into different categories based on their physical formats, contents, languages, and so on. In this paper, we focus on the task of book classification by its genre using the information provided just by the cover. Book covers are usually the very first impression to its readers and they often convey important information about the content of the book. Figure  presents some sample book covers. The information provided by a cover includes visual and textual information . For instance, in Figure 1, the background picture contains different food items and cookware which give the readers a visual impression about the book, while the texts shown on the cover states that it is a book about the ``authentic recipes from Malaysia"". Both the visual and textual information are shown in the cover and they together indicate that its genre is ``Cookbooks, Food \& Wine"". It is worth to mention that having only the visual information often makes the task extremely hard without textual information. For instance, in Figure 1 , without reading the texts on the cover, someone may classify the book as ``Cookbooks, Food \& Wine"" as well solely based on the visual information we get from the cover that includes food items on a table in a dining room setting. Therefore, it is sometimes essential to consider both visual information and textual information extracted from the cover when we conduct book genre classification. The automatic classification of books based on only covers without human intervention would be utterly beneficial to many modern retrieval systems, considering that the complete digitization of books is an extremely expensive task.     The challenges of this task are the following. First, there exists a wide variety of book genres, many of which are not concretely defined. Second, book covers, as graphic designs, varies in many different ways such as colors, styles, textual information, etc, even for books of the same genre. Third, book cover designs may vary due to many external factors such as country, culture, target reader populations, etc . To overcome these difficulties, we present a deep learning framework involving two moralities: one for visual information and the other for textual information extracted from the covers.   Recently, deep learning approaches have reached high performances across a wide variety of problems . In particular, some deep convolutional neural networks can achieve a satisfactory level of performance on many visual recognition and categorization tasks, exceeding human performances. One of the most attractive qualities of these techniques is that they can perform well without any external hand-designed resources or task-specific feature engineering.  The theoretical foundations of deep learning are well rooted in the classical neural network  literature. It involves many hidden neurons and layers as an architectural advantage in addition to the input and output layers . A deep convolutional neural network is universal, meaning that it can be used to approximate any continuous function to an arbitrary accuracy when the depth of the neural network is large enough .  The main contributions of this paper are fourfold:   The rest of the paper is structured as follows. Section 2 presents related works about book cover classification. Section 3 elaborates on the details of the proposed multi-modal architectures. In section 4, we discuss the experimental results. The last section concludes the paper and discusses future work.  
"," Book covers are usually the very first impression to its readers and they often convey important information about the content of the book. Book genre classification based on its cover would be utterly beneficial to many modern retrieval systems, considering that the complete digitization of books is an extremely expensive task. At the same time, it is also an extremely challenging task due to the following reasons: First, there exists a wide variety of book genres, many of which are not concretely defined. Second, book covers, as graphic designs, vary in many different ways such as colors, styles, textual information, etc, even for books of the same genre. Third, book cover designs may vary due to many external factors such as country, culture, target reader populations, etc. With the growing competitiveness in the book industry, the book cover designers and typographers push the cover designs to its limit in the hope of attracting sales. The cover-based book classification systems become a particularly exciting research topic in recent years. In this paper, we propose a multi-modal deep learning framework to solve this problem. The contribution of this paper is four-fold. First, our method adds an extra modality by extracting texts automatically from the book covers. Second, image-based and text-based, state-of-the-art models are evaluated thoroughly for the task of book cover classification. Third, we develop an efficient  and salable multi-modal framework based on the images and texts shown on the covers only.  Fourth, a thorough analysis of the experimental results is given and future works to improve the performance is suggested. The results show that the multi-modal framework significantly outperforms the current state-of-the-art image-based models. However, more efforts and resources are needed for this classification task in order to reach a satisfactory level.",291
"  \vskip 0.15in  Despite recent developments of activation functions for Machine Learning -based classifiers, such as the m-arcsinh~ for shallow Multi-Layer Perceptron ~, usable, repeatable and reproducible functions for both shallow and deep neural networks, e.g., the Convolutional Neural Network ~, have remained very limited and confined to three activation functions regarded as 'gold standard'. These include the Rectified Linear Unit , the sigmoid function and its modified version, hyperbolic tangent sigmoid or 'tanh'~, which extends its range from [0, +1] to [-1, +1]. The sigmoid and tanh have well-known vanishing gradient issues; thus, the ReLU function was devised to be more scalable for deep neural networks, despite its 'dying ReLU' problem, which has recently been solved by~. These have been made freely accessible in the open source Python library named 'Keras'~ for Deep Learning. The availability of these functions in the public domain has enabled not-for-profit and for-profit organisations to leverage them for several intelligence-based applications, from academic to industrial applications~~. \\  Nevertheless, considering the above-mentioned challenges in the Computer Science and ML communities, such activation functions lack robustness with classification tasks of varying degrees of complexity, e.g., slow or lack of convergence~ ~, caused by trapping at local minima~. Moreover, amongst the three above-mentioned activation functions, only the ReLU is applicable from shallow to deep neural networks, with its novel quantum variations  found more scalable than its traditional version only recently~. \\  On the other side, in sciences dealing with the study of human behaviour, in the last 20 years, considerable progress has been made towards the prevention of mental health disorders~~. Specifically, professionals working in the field of counselling psychology have slightly enhanced their ability of grasping relational issues in their subjects via novel ML-based tele-monitoring technologies~. Nevertheless, these technologies have not yet changed the traditional counselling psychology practice, which is still based on a structured methodology that is adopted to help individuals to become more self-aware, more conscious of their own needs and moods~. The main goal counsellors pursue is guiding individuals to get to know themselves at a deeper level and to help them discover and resurface their own resources to better manage their emotions in their daily life. This process first requires a tailored dialogue between the counsellor and the individual and, subsequently, leveraging practical tools to aid the individual in their experience to understand their inner self more deeply~. Moreover, there are still limitations within the counselling setting. For instance, individuals, out of fear, may not reveal fundamental aspects of their persona that would help counsellors guide them better in getting to know themselves. Furthermore, in many cases, subjects may express a verbal language opposite to their non-verbal one. Counsellors often hardly understand the dynamic patterns observed in the behaviours of their subjects, thus being unable to provide the required help and support to them. \\  In counselling, neural network algorithms, both shallow and deep depending on the amount of good-quality data and hardware available, have the potential to support counsellors in image and text classification tasks to understand and guide their subjects by helping them infer subtle dynamic changes in their behaviours. Via a careful and effective observation of images, micro- and macro- body movements, and facial expressions~~, it is possible to better interpret and understand the subjects' non-verbal language. Even the emotions underlying the written content from subjects may reveal inner aspects of their persona that are fundamental for counsellors to help resurface to increase the subjects' self-awareness and related capability of 'self-healing'~. \\  Therefore, from both theoretical and practical standpoints, there is an increasing need for accurate and reliable open source activation functions, which reach convergence faster, avoiding trapping at local minima, are more stable and can also be used and scale across both shallow and deep neural network algorithms for image and text classification. Entirely written in Python and made freely available in TensorFlow~ and Keras~, the proposed hyperbolic function is demonstrated as a competitive function with respect to gold standard functions, which suits both shallow and deep neural networks, thus being accurate and reliable for pattern recognition to aid image and text classification tasks.  Thanks to its liberal license, it has been widely distributed as a part of the free software Python libraries TensorFlow~ and Keras~, and it is available for use for both academic research and commercial purposes.\\  %%%%%%%%%%%%%%% Methods section %%%%%%%%%%%%%%%%%%%%%  \vskip 0.3in  
","%   <- trailing '%' for backward compatibility of .sty file This paper presents the 'hyper-sinh', a variation of the m-arcsinh activation function suitable for Deep Learning -based algorithms for supervised learning, such as Convolutional Neural Networks . hyper-sinh, developed in the open source Python libraries TensorFlow and Keras, is thus described and validated as an accurate and reliable activation function for both shallow and deep neural networks.  Improvements in accuracy and reliability in image and text classification tasks on five  benchmark data sets available from Keras are discussed.  Experimental results demonstrate the overall competitive classification performance of both shallow and deep neural networks, obtained via this novel function.  This function is evaluated with respect to gold standard activation functions, demonstrating its overall competitive accuracy and reliability for both image and text classification.",292
"  In grounded language theory, the semantics of language are given by how symbols connect to the underlying real world---the so-called ``symbol grounding problem''. For example, we want a robotic system that sees an eggplant  to ground the recognition object to a canonical symbol for `eggplant.' When a user asks ""Please grab me the eggplant,"" the robot should ground the natural language word ""eggplant"" to the same symbol that denotes the relevant visual percepts. Once both language and vision successfully ground to the same symbol, it becomes feasible for the robot to complete the task. We learn this connection by using physical sensors in conjunction with language learning: paired language and perceptual data are used to train a joint model of how linguistic constructs apply to the perceivable world.   Machine learning of grounded language often demands large-scale natural language annotations of things in the world, which can be expensive and impractical to obtain. It is not feasible to build a dataset that encompasses every object and possible linguistic description. Novel environments will require symbol grounding to occur in real time, based on inputs from a human interactor. Learning the meanings of language from unstructured communication with people is an attractive approach, but requires fast, accurate learning of new concepts, as people are unlikely to spend hours manually annotating even a few hundred samples, let alone the thousands or millions commonly required for machine learning.  % Active learning, in which a system queries for specific training data, has the potential to improve learning efficiency and reduce the number of labels required to learn a grounded language model.  In this work we study active learning, in which a system deliberately seeks information that will lead to improved understanding with less data, to minimize the number of samples/human interactions required. The field of active learning typically assumes that a pool of unlabeled samples is available, and the model can request specific example that it would like to obtain a label for. By having the model select the most informative data points for labeling, the number of samples that need to be labeled is reduced. This maps to the goal of human-robot learning with minimum training data provided by the human. Furthermore, active learning can be part of a pipeline with other few-shot learning methods.   However, active learning is not a magic bullet. When not carefully applied, it does not outperform sequential or random sampling baselines. Thoughtful selection of suitable approaches for problems is required. While active learning has been used for language grounding %, , to the best of our knowledge, we present the first broad exploration of the best methods for active learning for grounding vision-language pairs. %  In this paper, our focus is on developing guidelines by which active learning methods might be appropriately selected and applied to vision-language grounding problems. We test different active learning approaches on grounded language problems of varying linguistic and sensory complexity, and use our results to drive a discussion of how to select active learning methods for different grounded language data acquisition problems in an informed way.  We consider the grounded language task of learning novel language about previously unseen object types and characteristics. Our emphasis is on determining what methods can reduce the amount of training data needed to achieve performance consistent with human evaluation. Primarily, we address five relevant questions concerning characteristic-based grounded language learning:  % We make conclusions with respect to these questions in \cref{sec:results}. % In addition to addressing the above research questions, we verify how generalizable these learning techniques are beyond characteristic-based grounding.    We find that a right ordering of training data makes it possible to learn successfully from significantly fewer descriptions in most cases, but also that the active learning methodology chosen is specific to the nature of the learning problem. Our main contribution is a principled analysis of using active learning methods as unsupervised data sampling techniques in language grounding with a discussion of what aspects of those problems are relevant to approach selection. While our contributions are primarily analytic rather than algorithmic, we argue they address a critical need within grounded language understanding, an active research area in which questions of efficiency and data collection are widespread, and have the potential to support additional algorithmic developments.  
"," % In grounded language acquisition, a physical agent uses language combined with high-frequency sensor data to learn a model of how language refers to the physical world. This approach, while powerful, often requires extensive data annotation, which can be difficult to obtain. This work  % Ordering the selection of training data using active learning can lead to improvements in learning efficiently from smaller corpora. We present an exploration of active learning approaches applied to three grounded language problems of varying complexity in order to analyze what methods are suitable for improving data efficiency in learning. We present a method for analyzing the complexity of data in this joint problem space, and report on how characteristics of the underlying task, along with design decisions such as feature selection and classification model, drive the results. We observe that representativeness, along with diversity, is crucial in selecting data samples.",293
" Deep neural networks are powerful and have been widely applied in natural language processing. However, recent studies demonstrate that these models are vulnerable to adversarial examples, which are malicious inputs intentionally crafted to fool the models. % The introduction of the adversarial example ushered in a new era to understand and improve neural the network-based models.  % Adversarial attacks and defenses against these attacks have drawn significant attention in recent years . Although generating adversarial examples for texts has proven to be a more challenging task than for images due to their discrete nature, a number of methods have been proposed to generate adversarial text examples and reveal the vulnerability of deep neural networks in natural language processing  tasks including reading comprehension , text classification , machine translation , dialogue systems , and dependency parsing . These methods attack text examples by replacing, scrambling, and erasing characters or words or other language units.  To settle the susceptible attack direction, they require a large number of queries to the target model for the predictions of given inputs. Thus the adversarial examples are typically generated for a specific model.  This motivates the main questions we aim to answer in this paper: Are there universal adversarial examples that can fool almost every neural network-based model? And are there universal attack rules for constructing such universal adversarial examples? %are there universal adversarial examples that can transfer to any neural network-based models?  It is well known that adversarial examples exhibit black-box transferability, meaning that adversarial examples generated for one model can fool another model .  Transfer attackers launch white-box attacks on local models to find candidate adversarial examples that may transfer to the target model. % In the white-box setting, an adversary can access the model's architecture, parameters and input feature representations while not in the black-box one. % However, adversarial examples are typically overfitted to the particular architecture and feature representation of a source model, resulting in sub-optimal black-box transfer attacks to other target models.  However, which factors most affect the transferability of adversarial examples is still unclear, especially for NLP models. In this study, we quantitatively investigate how adversarial transferability is impacted by several critical factors, including the network architecture, input form, word embedding type, and model capacity.  Based on the understanding of transferability among various neural models, we study whether it is possible to craft universal, model-agnostic text adversarial examples for almost all existing models.  Universal adversarial examples have at least two advantages. First, the adversaries do not need white-box access to the target models. They launch the attacks by their own models trained on similar data, which can transfer across models .  Second, universal adversarial examples are a useful analysis tool because, unlike typical attacks, they are model-agnostic.  Thus, they highlight general input-output patterns learned by a model. We can leverage this to study the influence of dataset biases and to identify those biases that are learned by models.   \end{center}  \end{table*}   In this study, we first systematically investigated a few critical factors of neural models, including network architectures , input forms , embedding types , and model capacities  and how they impact the transferability of text adversarial examples through extensive experiments on two datasets of text classification.  We vary one factor at a time while fixing all others to see which factor is more significant, and found that the input form has the greatest influence on the adversarial transferability, following by network architecture, embedding type, and model capacity. Then, we propose a genetic algorithm to find an optimal ensemble with minimum number of members on the basis of our understanding of the adversarial transferability among neural models.  The adversarial examples generated by attacking the ensemble found by our algorithm strongly transfer to other models, and for some models, they exhibit better transferability than those generated by attacking models with different random initialization. Finally, we generalize the adversarial examples constructed by the ensemble method into universal semantics-preserving word replacement rules that can induce adversaries on any text input strongly transferring to any neural network-based NLP model . Since those rules are model-agnostic, they provide an analysis of global model behavior, and help us to identify dataset biases and to diagnose heuristics learned by the models.   
"," Deep neural network models are vulnerable to adversarial attacks. In many cases, malicious inputs intentionally crafted for one model can fool another model in the black-box attack setting. However, there is a lack of systematic studies on the transferability of adversarial examples and how to generate universal adversarial examples.  In this paper, we systematically study the transferability of adversarial attacks for text classification models.  In particular, we conduct extensive experiments to investigate how various factors, such as network architecture, input format, word embedding, and model capacity, affect the transferability of adversarial attacks.  Based on these studies, we then propose universal black-box attack algorithms that can induce adversarial examples to attack almost all existing models. These universal adversarial examples reflect the defects of the learning process and the bias in the training dataset.  Finally, we generalize these adversarial examples into universal word replacement rules that can be used for model diagnostics.     \if0 It has been known that adversarial examples exhibit black box transfer, i.e. malicious inputs intentionally crafted for one model can also cause another model to make mistakes. However, which factors affect the most and how they impact the transferability of adversarial examples are still unclear, especially for NLP models.  Through extensive experiments, we systematically investigate how adversarial transferability is impacted with a few critical, model-specific factors, including the network architecture, input form, pre-trained word embedding, and model capacity. Based on the understanding of the adversarial transferability among neural models, we propose a population-based algorithm to find an optimal ensemble with minimum number of models, which can be used to generate adversarial examples that strongly transfer across other neural models.  We also generalize the adversarial examples generated by the ensemble method into universal word replacement rules that can induce adversaries on any text input to fool almost all the existing models with a much higher success rate. Those rules also help us to identify dataset biases and diagnose heuristics improperly learned by the models. \fi",294
" Recent works have shown that NN models that are trained solely to maximize prediction performance are often vulnerable to adversarial attacks . Even though several works have been proposed to defend NN models against such attacks, only a few of them focus on the NLP domain . Since many recent NLP models are shown to be vulnerable to adversarial attacks--e.g., fake news detection  and dialog system , the investigation of robust defense methods for textual NN models has become necessary. To defend against adversarial texts, one can use either adversarial detection or model enhancement . Adversarial texts are often generated by replacing or inserting critical words  or characters  in a sentence, that are usually exhibiting grammatical errors. Hence, many detection methods have focused on recognizing and correcting such misspellings from texts--e.g., ScRNN  and DISP . While misspelling-based methods are model-independent and require neither re-training nor modifying the models, they only work well on character-based attacks. In contrast, model enhancement approaches perform well under both character and word-based attacks.  %Such generalization to a variety of attacks is critical since one might not know what type of adversarial techniques will be employed by adversaries.  Particularly, most of the model enhancement methods enrich NN models by training them with adversarial data augmented via known attack strategies such as in adversarial training , or with external information such as knowledge graphs . Nevertheless, these augmentations usually induce overhead costs in training. Therefore, we are in search of defense algorithms that directly enhance the models' structures  while achieving higher extendability without acquiring additional data.  %While developing these solutions is more challenging and still under exploration .   Fortunately, recent literature in computer vision shows that ensemble NNs achieve high adversarial robustness . In theory, by directly extending a single NN model to an ensemble of multiple diverse sub-models, we challenge adversaries to attack not only one but a set of very different models . This makes any attacks significantly more difficult. However, applying such an idea from computer vision to the NLP domain faces one main challenge. Current ensemble methods require simultaneous training of several NN sub-models . This introduces impractical computational overhead during both training and inference, especially when one wants to maximize prediction accuracy by utilizing complex state-of-the-art  sub-models such as BERT  and ROBERTA , both of which have more than 100M parameters. Furthermore, applying current ensemble or other defensive approaches that directly enhance a model's architecture to a large-scale NN model would usually require re-training everything from scratch, which may not be practical in many settings.  % Second, current ensemble approaches aim to promote the diversity of sub-models at either feature-level  or at class-level .  % Second, current ensemble approaches promote the diversity of sub-models by maximizing their differences among either prediction output vectors  or gradient vectors w.r.t an input image . However, most of NLP tasks, classification particularly, have much less labels than those of computer vision, which results in a much smaller degree-of-freedom when directly regularizing the differences of prediciton probability vectors.   % On the other hand, forcing the sub-models to focus on different tokens of an input text by directly regularizing their gradient vectors is not straightforward because text are discrete in nature. This can be easily resolved by regularizing the gradients w.r.t the continuous word-embedding vectors of the sentence instead. However, since every sub-model contributes equally to an input, there will be many overlaps among the key features, i.e., words or phrases, of each of the sub-models when the input text is short.   To address these challenges, we are borrowing ideas from Software Engineering, by first introducing the notion of {\bf Neural Patching} to improve the adversarial robustness of NN models by ``patching"" only parts of models . Next, we develop a novel neural patching algorithm, {\mymethod}, that patches only the last layer of an already deployed textual NN model of diverse architectures  and transforms it into an ensemble of multi-experts with enhanced adversarial robustness.  By patching only the last layer of a model, {\mymethod} introduces a lightweight computational overhead and requires no additional training data. %requires low construction overheads without compromising much computational complexity or additional training data.  Distinguished from current ensemble methods, each sub-model trained by {\mymethod} is specialized not only in a specific subset of features of the same input, i.e., an expert at feature-level , or a sub-set of labels, i.e., an expert at class-level , but also in texts of a distinguished topic, i.e., an expert at instance-level, during prediction. Such diversity at all of the feature-, class-, and instance-level expertise makes it challenging for adversaries to exploit multiple sub-models at the same time. In summary, our contributions in this paper are as follows:   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
"," Neural network  models that are solely trained to maximize the likelihood of an observed dataset are often vulnerable to adversarial attacks. Even though several methods have been proposed to enhance NN models' adversarial robustness, they often require re-training from scratch. This leads to redundant computation, especially in the NLP domain where current state-of-the-art models, such as BERT and ROBERTA, require great time and space resources. By borrowing ideas from Software Engineering, we, therefore, first introduce the Neural Patching mechanism to improve adversarial robustness by ``patching"" only parts of a NN model. Then, we propose a novel neural patching algorithm, {\mymethod}, that transforms a textual NN model into a stochastic ensemble of multi-expert predictors by upgrading and re-training its last layer only. {\mymethod} forces adversaries to attack not only one but multiple models that are specialized in diverse sub-sets of features, labels, and instances so that the ensemble model becomes more robust to adversarial attacks. By conducting comprehensive experiments, we demonstrate that all of CNN, RNN, BERT, and ROBERTA-based textual models, once patched by {\mymethod}, witness an absolute increase of as much as 20\% in accuracy on average under 5 different white and black-box attacks, outperforming 6 defensive baselines across 4 public NLP datasets. All codes and datasets are to be released.",295
" Emotional analysis has been an active research area for a few decades, especially in recognition domains of text and speech emotions. Even if text and speech emotions are closely relevant, both kinds of emotions have different challenges. One of the challenges in text emotion recognition is ambiguous words, resulting from omitted words . On the other hand, one of the challenges in speech emotion recognition is creating an efficient model. However, this paper focuses on only the recognition of speech emotions. In this area, two types of information, linguistic and paralinguistic, were mainly considered in speech emotion recognition. The linguistic information refers to the meaning or context of speech. The paralinguistic information implies the implicit message meaning, like the emotion in speech . Speech characteristics can interpret the meaning of speech; therefore, behavioral expression was investigated in most of the speech emotion recognition works  .   In recent works, local feature learning block  , one of the efficient methods, has been used in integrating local and global speech emotion features, which provide better results in recognition. Inside LFLB, convolution neural network  was used for extracting local features, and then long short-term memory  was applied for extracting contextual dependencies from those local features to learn in a time-related relationship. However, vanishing gradient problems may occur with CNN . Therefore, residual deep learning was applied to the CNN by using skip-connection to reduce unnecessary learning and add feature details that may be lost in between layers.  Furthermore, the accuracy of speech recognition does not only rely on the efficiency of a model, but also of a speech feature selection . In terms of speech characteristics, there are many distinctive acoustic features that usually used in recognizing the speech emotion, such as continuous features, qualitative features, and spectral features . Many of them have been investigated to recognize speech emotions. Some researchers compared the pros and cons of each feature, but no one can identify which feature was the best one until now .  As previously mentioned, we proposed a method to improve the efficiency of LFLB  for deeper learning. The proposed method, deep residual local feature learning block , was inspired by the concept of human brain learning; that is, 閳ユΜepeated reading makes learning more effective,閳 as the same way that Sari  and Shanahan  were used. Responding to our inspired concept, we implemented a learning method for speech emotion recognition with three parts: Part 1 is for general learning, like human reading for the first time, Part 2 is for further learning, like additional readings, and the last part is for associating parts learned to decide types of emotions. Besides, the feature selection is compared with two types of distinctive features to find the most effective feature in our work: the normal and specific distinctive features are log-mel spectrogram , which is fully filtered sound elements, and %log-mel spectrogram,  MFCC deltas, delta-deltas, and chromagram  are more clearly identify speech characteristics extracted based on %according to  the human mood.  Our main contributions of this paper are as follows:   Deep residual local feature learning block  was proposed. DeepResLFLB was arranged its internal network as LFLB, batch normalization , activation function, normalization-activation-CNN , and deep layers.   Learning sequences of DeepResLFLB were imitated from human re-reads.   Speech emotion features, %according to  based on human mood determination factors such as LMS and LMSDDC, were applied and compared their performances.   
"," Speech Emotion Recognition  is becoming a key role in global business today to improve service efficiency, like call center services. Recent SERs were based on a deep learning approach. However, the efficiency of deep learning depends on the number of layers, i.e., the deeper layers, the higher efficiency. On the other hand, the deeper layers are causes of a vanishing gradient problem, a low learning rate, and high time-consuming. Therefore, this paper proposed a redesign of existing local feature learning block . The new design is called a deep residual local feature learning block . DeepResLFLB consists of three cascade blocks: LFLB, residual local feature learning block , and multilayer perceptron . LFLB is built for learning local correlations along with extracting hierarchical correlations; DeepResLFLB can take advantage of repeatedly learning to explain more detail in deeper layers using residual learning for solving vanishing gradient and reducing overfitting; and MLP is adopted to find the relationship of learning and discover probability for predicted speech emotions and gender types. Based on two available published datasets: EMODB鐠虹棏nd RAVDESS, the proposed DeepResLFLB can significantly improve performance when evaluated by standard metrics: accuracy, precision, recall, and F1-score.  \keywords{Speech Emotion Recognition \and Residual Feature Learning \and CNN Network \and Log-Mel Spectrogram \and Chromagram}",296
" Classification is an important task of knowledge discovery in databases and data mining. It is a task of learning a discriminative function from the given data that classifies previously unseen data to the correct classes. Current research trends in natural language processing focus on developing deep neural network models such as BERT  that have been pre-trained with a large text corpus and thus show immense improvement in different text classification tasks. Despite the success of large pre-trained models, DNNs still suffer from generalizing to a balanced testing criterion in cases of data imbalance . In realistic settings, it is rarely the case where the discrete distribution of the data acquired is perfectly balanced across all classes. Realistic settings are prone to be skewed to specific classes while such classes are often the class of interest. Some situations may be binary, as in detecting spams in forums . The majority of the contents posted from users are not spams and is in accordance with the intended goal. As a result, the number of spam samples is sparse in comparison to non-spam samples. Imbalanced data may also occur in a multi-classification setting such as classifying articles into different categories .     Text classification can be used for numerous application purposes. In this paper, we address the problem of detecting sexual harassment and toxicity in comments from news articles. In the name of anonymity, online discussion platforms have become a place where people undermine, harass, humiliate, threaten, and bully others  based on their superficial characteristics such as gender, sexual orientation, and age . Each toxic comment can further be classified into classes based on their degree of toxicity . Figure  shows the overall procedure of detecting sexual harassment and performing sentimental analysis on comment data in the wild. When collecting and annotating comments, data skewness occurs naturally since users do not consider data imbalance levels when writing toxic or non-toxic comments. Classifiers trained in imbalanced settings tend to become biased toward the class with more samples in the training data. This is because standard deep learning architectures  do not take the data imbalance level into consideration. In order to develop intelligent classifiers, methods to temper the classifier from biasing towards certain classes are of great importance.   Previous methods addressing data imbalance in the text can be divided into data-level and algorithm-level methods. Data-level methods  apply manipulation on the data by undersampling majority classes or oversampling minority classes. However, most of the methods require an effective numerical representation algorithm since methods are applied directly to the representation instead of on the actual text. Algorithm-level methods modify the underlying learner or its output to reduce bias towards the majority group. However, these methods are task-sensitive and somewhat heuristic since it requires the researchers to modify the classifier considering the innate properties of the task. This property leads to the inefficiency in training the learner since heuristic approaches are often time-consuming and arbitrary. Since only traditional oversampling and undersampling methods, which simply duplicate or sample data instances, are independent of these two limitations, methods addressing data imbalance in the text without the utilization of feature spaces or task-dependent is needed.  We propose a novel training architecture, Sequential Targeting , that handles the data imbalance problem by forcing an incremental learning setting. ST divides the entire training data set into mutually exclusive partitions, target-adaptively balancing the data distribution. Target distribution is a predetermined distributional setting that enables the learner to exert maximum performance when trained with. In an imbalanced distributional setting, the target distribution is idealistic to follow a uniform distribution where all the classes hold equal importance. Optimal class distribution may differ by innate property of the data but research shows that a balanced class distribution has an overall better performance compared to other distributions~. The remaining partitions are then sorted in the magnitude of similarity with the target distribution which is measured by KL-divergence. The first partition of the split data is imbalanced while the last partition is arbitrarily modeled to be uniform across classes and all the partitions are utilized to train the learner sequentially.   We handle the issue of catastrophic forgetting~, which is an inevitable phenomenon when transfer learning, by utilizing Elastic Weight Consolidation~ to stabilize the knowledge attained from the previous tasks. This allows the discriminative model to learn from the incoming data while not forgetting the previously inferred parameters from previous tasks.   Our proposed method is both independent of the numerical representation method and the task at hand. We validate our method on simulated datasets with varying imbalance levels and apply our method to a real-world application. We annotated and construct three datasets consisting of comments made by users from different social platforms of NAVER\footnote{NAVER is the Korean No.1 web search portal where around 16 million users visit every day. www.naver.com}: two for detecting sexual harassment and one for multiple sentimental analysis. Annotations on the datasets were improved iteratively by in-lab annotations and crowdsourcing. Experimental results show that ST outperforms traditional approaches, with a notable gap, especially in extremely imbalanced cases. Lastly, ST proves to be compatible with previous approaches.  Our contribution in this paper is three-folds:   The rest of the paper is organized as follows. Section  summarizes related works. Section  provides the details of the proposed method. Section    presents dataset descriptions, experiment setups, and qualitative experimental results on various datasets. Finally, Section  concludes the paper.  
"," %% Text of abstract Classification tasks require a balanced distribution of data to ensure the learner to be trained to generalize over all classes. In real-world datasets, however, the number of instances vary substantially among classes. This typically leads to a learner that promotes bias towards the majority group due to its dominating property. Therefore, methods to handle imbalanced datasets are crucial for alleviating distributional skews and fully utilizing the under-represented data, especially in text classification. While addressing the imbalance in text data, most methods utilize sampling methods on the numerical representation of the data, which limits its efficiency on how effective the representation is. We propose a novel training method, Sequential Targeting, independent of the effectiveness of the representation method, which enforces an incremental learning setting by splitting the data into mutually exclusive subsets and training the learner adaptively. To address problems that arise within incremental learning, we apply elastic weight consolidation. We demonstrate the effectiveness of our method through experiments on simulated benchmark datasets  and data collected from NAVER.",297
"  As the growth of robots interacting with humans, different levels of environment understanding is required by the robot. A robot acting in an environment has to deal with many open questions, thus needs different levels of reasoning to do a task. Usually, robots rely on their initial knowledge, perception and their cognitive abilities to be able to understand and do reasoning in their situated environment. A recently hooked topic to a better Knowledge-Based cognition is dialogic interaction between a human and a robot, where the robot captures fresh information about the environment from a user through Natural Language. Information comes from Natural Language together with visually perceived information, and a Knowledge Base  lets a cognitive agent reach different levels of understanding in the environment.   The first level of understanding can be seen as classification and detection on sensory inputs, e.g. detection of objects in visual perception, or role tagging of lexical in a sentence. The second level of understanding concerns finding relations between different sensory inputs, e.g. finding common attributes in language and vision. Some famous problems such as symbol grounding  and anchoring  concern finding correspondences in different sensory input modalities. A higher and abstract level of understanding can be thought to find relations between the entities in an environment. e.g. in a scene with a desk and a book on top, some of the relationships between these are their relative physical position and their semantics that shows how entities  are similar.   Understanding relationships between physical entities can also be extended to the attributes of entities. Indeed the same definition of the relationship between entities can be found for the attributes. For example, when a user declares freshness attribute of 'apple-1' is 'spoiled', as well as 'apple-2' and 'apple-3', but 'orange-1' and 'banana-1' are 'fresh', a relation between the values of freshness attribute exists which connects semantic of entities; In this example, is that all apples are 'spoiled', and the rest of fruits are 'fresh', with closed world assumption.   Relation and rules for attributes of entities can help a robot that is interacting with a human in many applications. For example when a user utters ""bring me a fruit"", using the rules obtained for freshness attribute, the robot notices which fruits are spoiled and which are fresh to eat. Such logical rules between attributes let the robot realize that apples are spoiled, apples should be thrown out, and added to the shopping list. Moreover, the obtained rule for attributes can be used in a robot's low-level sensory input processing. Consider an utterance where the user of our example is declaring that a physical entity is spoiled, but the robot's visual perception has doubt whether the perceived object is apple or pear. As the robot already found that all apples are spoiled and other fruits are fresh, so the perceptual detection refines the recognized object as the apple.  %Different attributes can represent characteristics of an entity, where some are computed from visual perception and some from Natural Language through interaction with a user. In this work, we deal with nine different attributes, as a category, color, label, functionality, owner, size, weight, restriction, and location of entities in a scene; where the first two are computed from visual perception and the rest are obtained from Natural Language.  %%It is worth emphasis on the importance of attributes that come from Natural Language. Such information is almost impossible to obtain from visual perception, e.g. the information that a user can give about owner of an entity, cannot be obtained from the camera. Also, an initial knowledge base only gives information about the category of an entity, and not about a particular entity , and some of the assignments might be temporary. On the other hand, information about size, weight, and location, may be used for refinement of knowledge base and camera, or just a shortcut to obtaining such information from the user.   In this work, we propose a framework for learning logical rules that represent relations between attributes in a semantic model of the robot's environment. Such logical rules help the robot to find which attributes  entail a specific attribute. A distinctive novelty of our work is to generalize rules from a semantic model built via Human-Robot Interaction , through the integration of visual and linguistic cues. Our framework goes all the way from sensory input data to abstract First-Order Logic formulas that describe abstract relationship between attributes of entities in a scene. %Our approach differs from other works as our system is able to capture more attributes from Natural Language in addition to attributes from computer vision.  %Our proposed framework compute First-Order Logic formulas, which is useful for general reasoning upon entities that have common attributes.  We focus on latent rules, which the robot can capture implicitly when a human describes objects to the robot. In other words, we do not require the user to give rules explicitly to the robot, but rather we let the robot find rules and do further reasoning based on self-computed rules for improving its interaction with the user.    This paper continues with the review of related work, and then in Section  the proposed framework is described, followed by an implementation to demonstrate the viability of the proposed framework in Section . In Section  results of a test scenario are given, followed by the discussion about the applicability of the framework. In the end, conclusions of this work are drawn.      
"," Humans have a rich representation of the entities in their environment. Entities are described by their attributes, and entities that share attributes are often semantically related.  For example, if two books have ``Natural Language Processing'' as value of their `title' attribute, we can expect that their `topic' attribute will also be equal, namely, ``NLP''.  Humans tend to generalize such observations, and infer sufficient conditions under which the `topic' attribute of any entity is ``NLP''.  If robots need to interact successfully with humans, they need to represent entities, attributes, and generalizations in a similar way. This ends in a contextualized cognitive agent that can adapt its understanding, where context provides sufficient conditions for a correct understanding. In this work, we address the problem of how to obtain these representations through human-robot interaction.  We integrate visual perception and natural language input to incrementally build a semantic model of the world, and then use inductive reasoning to infer logical rules that capture generic semantic relations, true in this model.  These relations can be used to enrich the human-robot interaction, to populate a knowledge base with inferred facts, or to remove uncertainty in the robot's sensory inputs.",298
" In recent years, science, engineering and mathematics education has emphasized supporting students閳 disciplinary ""practices"" of inquiry. These practices閳ユ敃uch as of formulating questions, designing investigations, or arguing from evidence閳ユ攣re more difficult to identify and assess than the traditional objectives of particular correct content knowledge. In order to study students閳 practices, researchers rely mainly on qualitative analyses of naturalistic data. These studies have advanced the field閳ユ獨 understanding of practices.\\  These studies have been limited in scope, however, because they are extremely labor-intensive: Analysis of naturalistic data requires significant and extensive effort by trained researchers, from transcribing to coding to the construction of meaning. It has been time and cost-prohibitive to conduct qualitative studies with large samples of data. Our purpose in this project is to develop computational tools that can support qualitative research at large scales on students' inquiry practices in science. In this paper we report on initial progress towards applying natural language processing  techniques to research on students' written arguments in college biology laboratory reports. \\  In this work, we report on our success in designing NLP that approached the reliability of trained, human coders. Specifically, we show that contrastive learning in the Wasserstein space is able to achieve a high level of agreement  on average. \\  The rest of the paper is organized as follows. In section  first overview current state-of-art in automating assessment of writing in science using machine learning for natural language processing. Following that we outline the writing assessment setting particular to our case in section .  In section  we briefly survey the relevant literature in machine learning for NLP and introduce our novel approach for automatic scoring. In section  we evaluate the performance of the proposed approach and discuss the results in detail.\\  
"," Qualitative analysis of verbal data is of central importance in the learning sciences. It is labor-intensive and time-consuming, however, which limits the amount of data researchers can include in studies. This work is a step towards building a statistical machine learning  method for achieving an automated support for qualitative analyses of students闁 writing, here specifically in score laboratory reports in introductory biology for sophistication of argumentation and reasoning. We start with a set of lab reports from an undergraduate biology course, scored by a four-level scheme that considers the complexity of argument structure, the scope of evidence, and the care and nuance of conclusions. Using this set of labeled data, we show that a popular natural language modeling processing pipeline, namely vector representation of words, a.k.a word embeddings, followed by Long Short Term Memory  model for capturing language generation as a state-space model, is able to quantitatively capture the scoring, with a high Quadratic Weighted Kappa  prediction score, when trained in via a novel contrastive learning set-up. We show that the ML algorithm approached the inter-rater reliability of human analysis. Ultimately, we conclude, that machine learning  for natural language processing  holds promise for assisting learning sciences researchers in conducting qualitative studies at much larger scales than is currently possible.",299
"   Mental illnesses are a common problem of our modern world. More than one in ten people was living with mental health disorders in 2017 , with women being the most affected. These disorders affect people's way of thinking, mood, emotions, behaviour and their relationships with others. Most mental illnesses remain undiagnosed because of the social stigma around them.  Depression is one of the main causes of disability globally , it affects people of all ages. Prevention is used to reduce depression and to save the lives of people at risk of suicide, but prevention is only limited to raising awareness and programs to cultivate positive thinking in case of depression and monitoring people who attempted suicide or self-harm.  With the rise in social media use, more computational efforts are made to detect mental illnesses such as depression  and PTSD , but also to detect misogyny , irony and sarcasm  from users' texts.  People tend to talk more about their emotions and mental health problems online and to seek support. The sources of mental health cues used for detection are Twitter, Facebook, Reddit and forums . Reddit is a social media site very similar to forums. It is organized in subreddits with specific topics, some dedicated to mental health problems. The use of throwaway accounts to maintain anonymity promotes disclosure, and users are more likely to share problems they have not discussed with anyone before. The use of these accounts makes it difficult for users to receive more social support because the majority of them are used only for one post .  In this work, we choose to tackle the problem of detecting early onset of depression from users' posts on social media, specifically from Reddit. As such, we explore the eRisk 2018 dataset through topic analysis by means of Latent Semantic Indexing  and learned out-of-distribution confidence scores . Due to the nature of the dataset, we repurpose the learned confidence score to make a decision on whether to label the user as depressed or non-depressed or to wait for more data, as test chunks were progressively released every week.   
","   English.  Computational research on mental health disorders from written texts covers an interdisciplinary area between natural language processing and psychology. A crucial aspect of this problem is prevention and early diagnosis, as suicide resulted  from  depression being the second leading cause of death for young adults. In this work, we focus on methods for detecting the early onset of depression from social media texts, in particular from Reddit. To that end, we explore the eRisk 2018 dataset and achieve good results with regard to the state of the art by leveraging topic analysis and learned confidence scores to guide the decision process. \footnote{Copyright \copyright2020 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International .} %   Our analysis paves way to more in depth exploration of detection of mental illnesses from social media interactions.",300
" 	 		Neural text-to-speech  techniques have significantly improved the naturalness of speech produced by TTS systems. We refer to NTTS systems as a subset of TTS systems that use neural networks to predict mel-spectrograms from phonemes, followed by the use of neural vocoder to generate audio from mel-spectrograms. 			 		In order to improve the prosody\footnote{We use the subtractive definition of prosody from .} of speech obtained from NTTS systems, there has been considerable work in learning prosodic latent representations from ground truth speech. These methods use the target mel-spectrograms as input to an encoder which learns latent prosodic representations. These representations are used by the decoder in addition to the input phonemes, to generate mel-spectrograms. The latent representations obtained by encoding a target mel-spectrogram at the sentence level will have information that is not directly available from the phonemes, and by the subtractive definition of prosody, we may claim that these representations capture prosodic information. Several variational and non-variational methods have been proposed for learning prosodic latent representations. While these methods improve the prosody of synthesised speech, they need an input mel-spectrogram which is not available while running inference on unseen text. This gives rise to the problem of sampling from the learnt prosodic space. Sampling at random from the prior may result in the synthesised speech not having contextually appropriate prosody, as it has no relationship with the text being synthesised. In order to improve the contextual appropriateness of prosody in synthesised speech, there has been work on using textual features like contextual word embeddings and other grammatical information to directly condition NTTS systems. These methods require the NTTS model to learn an implicit correlation between the given textual features and the prosody of the sentence. One work also poses this sampling problem as a selection problem and uses both syntactic distance and BERT embeddings to select a latent prosodic representation from the ones seen at training time.  		Bringing both the aforementioned ideas of using ground truth speech to learn prosodic latent representations and using textual information, we build Kathaka, a model trained using a two-stage training process to generate speech with contextually-appropriate prosody. In Stage~\Romannum{1}, we learn the distribution of sentence-level prosodic representations from ground truth speech using a VAE. In Stage~\Romannum{2}, we learn to sample from the learnt distribution using text. In this work, we introduce the BERT+Graph sampler, a novel sampling mechanism which uses both contextual word-piece embeddings from BERT and the syntactic structure of constituency parse trees through graph attention networks. We then compare Kathaka against a strong baseline and show that it obtains a relative improvement of  in naturalness. 		 	 	
"," 		In this paper, we introduce Kathaka, a model trained with a novel two-stage training process for neural speech synthesis with contextually appropriate prosody. In Stage, we learn a prosodic distribution at the sentence level from mel-spectrograms available during training. In Stage, we propose a novel method to sample from this learnt prosodic distribution using the contextual information available in text. To do this, we use BERT on text, and graph-attention networks on parse trees extracted from text. We show a statistically significant relative improvement of $13.2\%$ in naturalness over a strong baseline when compared to recordings. We also conduct an ablation study on variations of our sampling technique, and show a statistically significant improvement over the baseline in each case.",301
" Due to the growing presence of AI-powered systems in our lives, affective computing has become an important part of human-computer interaction. Emotion plays a role in our thoughts and actions and is an integral part of the way we communicate . The ability to leverage context to understand emotions communicated both verbally and non-verbally is trivial for humans but remains difficult for machines . Emotional responses depend on both our psyche and physiology and are governed by our perception of situations, people and objects. They also depend on our mental state   . The way we exhibit and perceive emotion may also differ based on our age, gender, race, culture and accent . In addition to all of this, unlike targets in other classification tasks, the emotions we experience are rarely distinct: they often coexist without clear temporal boundaries, adding considerable complexity to the task .  Despite these difficulties, automated emotion recognition has social and commercial applications that make it worth pursuing. In the medical domain, it has exciting potential: to identify and diagnose depression and stress in individuals , to monitor and help people with bipolar disorder  and to assist the general public in maintaining mental health. Commercial applications include call center customer management, advertising through neuro-marketing and social media engagement . As intelligent chatbots and virtual assistants have become more widely used, emotion detection has become a vital component in the design, development and deployment of these conversational agents .  Early research in emotion detection focused on binary classification in a single modality, whether in text, speech , or images . Text-based classifiers used the n-gram vocabulary of sentences to predict their polarity and speech models modeled the vocal dynamics that characterize these emotions. These approaches are inherently limited: a binary granularity and cues from a single modality are far removed from the actual human process they're meant to model. As a result, joint approaches which leverage all available modalities  are promising.  While existing multi-modal emotion corpora like IEMOCAP  and Crem-D  have been critical for the progress in affective computing to date, they suffer from three issues that are the focus of our work. First, these corpora tend to be small due to the high costs of annotating for emotion. This precludes the use of deep neural models with high model complexity as they require many training samples to generalize well. This also compounds the second difficulty inherent to many emotion datasets: while there are usually many neutral, happy and sad training examples, there are often very few examples of rarer emotions like disgust making them difficult to classify. This issue is not easily solved by combining different corpora due to the third issue, their lack of mutual compatibility -- they differ in the emotions identified, the types of dialogue and number of speakers represented and the naturalness of the recordings . This severely restricts the generalizability of models trained on a single corpus.  Contemporary literature has dealt with these problems by dropping labels . Hard and scarce emotions like disgust are dropped from the corpus and the models are trained and evaluated on the trimmed corpus. This allows evaluating models on different corpora by using utterances exhibiting only the most common emotions. While this is a reasonable, the resulting performance is not a complete reflection of how these models perform once deployed to production. When emotion models are used in real-world applications, we can expect them to encounter utterances corresponding to dropped labels. For such cases, these models are likely to exhibit degraded performance by predicting one of the known, but incorrect labels.  In this work, we address the problem of data sparsity by transfer learning via the pretrain-then-finetune paradigm. Deep complex models can be trained on large datasets for an auxiliary but related task to learn network parameters that reflect abstract notions related to the target task. As the expression of emotions is highly dependent on the individual, we train a multilayer TDNN  on the task of speaker identification using the VoxCeleb corpus  and then fine-tune its final few layers on the task of emotion identification using the Crema-D corpus . Using this network, we extract speech embeddings for Crema-D from each of its layers, generate and concatenate text embeddings for the accompanying transcripts using a fine-tuned BERT model  and then train an LDA - pLDA  model on the resulting dense representations.  pLDA allows our model to more easily adapt to previously unseen classes and domains, a requirement for both evaluating against a different emotion corpus with an incompatible label set and performing well in the wild.  To understand the merits of each component, we exhaustively evaluate the predictive power of every permutation: the TDNN alone, speech embeddings from each of its layers alone, text embeddings alone and every combination thereof. Our best variant, trained on only VoxCeleb and Crema-D and evaluated on IEMOCAP, achieves an Equal Error Rate  of \%. Including a portion of IEMOCAP during training produces a 5-fold averaged EER of \%.   
"," Automated emotion detection in speech is a challenging task due to the complex interdependence between words and the manner in which they are spoken. It is made more difficult by the available datasets; their small size and incompatible labeling idiosyncrasies make it hard to build generalizable emotion detection systems. To address these two challenges, we present a multi-modal approach that first transfers learning from related tasks in speech and text to produce robust neural embeddings and then uses these embeddings to train a pLDA classifier that is able to adapt to previously unseen emotions and domains. We begin by training a multilayer TDNN on the task of speaker identification with the VoxCeleb corpora and then fine-tune it on the task of emotion identification with the Crema-D corpus.  Using this network, we extract speech embeddings for Crema-D from each of its layers, generate and concatenate text embeddings for the accompanying transcripts using a fine-tuned BERT model and then train an LDA - pLDA classifier on the resulting dense representations. We exhaustively evaluate the predictive power of every component: the TDNN alone, speech embeddings from each of its layers alone, text embeddings alone and every combination thereof.  Our best variant, trained on only VoxCeleb and Crema-D and evaluated on IEMOCAP, achieves an EER of $38.05$\%. Including a portion of IEMOCAP during training produces a 5-fold averaged EER of $25.72$\% .",302
"   Vocoders were originally used for speech compression in the field of communication. Recently, vocoders have been utilized in various fields such as text-to-speech and voice conversion or speech-to-speech translation. Neural vocoders generate human-like voices using neural networks, instead of using traditional methods that contain audible artifacts .  Recently, it has been demonstrated that vocoders exhibit superior performances in generation speed and audio fidelity when trained with single speaker utterances. However, some models face difficulty when generating natural sounds in multiple domains such as speakers, language, or expressive utterances. The ability of these models can be evaluated by the sound quality when the model is trained on data of multiple speakers and the sound quality of the unseen domain . A vocoder that can generate high-fidelity audio in various domains,  regardless of whether the input has been encountered during training or has come from an out-of-domain source, is usually called a universal vocoder.  MelGAN is a vocoder based on generative adversarial networks . It is a lightweight and robust model for unseen speakers but yields lower fidelity than popularly employed models . MelGAN alleviates the metallic sound that occurs mainly in unvoiced and breathy speech segments through multi-scale discriminators that receive different scale waveforms as inputs. However, it has not been implemented efficiently for learning with multiple speakers for a universal vocoder.  In this study, we propose Universal MelGAN. The generated waveform of the original MelGAN with audible artifacts appears as an over-smoothing problem with a non-sharp spectrogram. We added multi-resolution spectrogram discriminators to the model to address this problem in the frequency domain. Our multi-scale discriminators enable fine-grained spectrogram prediction by discriminating waveforms and spectrograms. In particular, they alleviate the over-smoothing problem in the high frequency band of the large footprint model, enabling the generation of realistic multi-speaker waveforms.  To evaluate the performance of the proposed model, we compare with full-band MelGAN  as a baseline and two other vocoders: WaveGlow and WaveRNN. We designed experiments in both Korean and English for language independency. For evaluation, we prepared multiple speaker utterances that included unseen domain scenarios, such as new speakers, emotions, and languages.  The evaluation results indicate that the proposed model achieved the best mean opinion score  in most scenarios and efficiently preserved the fidelity in unseen speakers. In addition, the evaluations show that the model efficiently preserves the original speech, even in challenging domains such as expressive utterances and unseen languages. In multi-speaker text-to-speech scenarios, our model can generate high-fidelity waveforms with high MOS, and the model outperforms compared vocoders. This results without any external domain information suggest the possibility of the proposed model as a universal vocoder.  
"," We propose Universal MelGAN, a vocoder that synthesizes high-fidelity speech in multiple domains. To preserve sound quality when the MelGAN-based structure is trained with a dataset of hundreds of speakers, we added multi-resolution spectrogram discriminators to sharpen the spectral resolution of the generated waveforms. This enables the model to generate realistic waveforms of multi-speakers, by alleviating the over-smoothing problem in the high frequency band of the large footprint model. Our structure generates signals close to ground-truth data without reducing the inference speed, by discriminating the waveform and spectrogram during training. The model achieved the best mean opinion score  in most scenarios using ground-truth mel-spectrogram as an input. Especially, it showed superior performance in unseen domains with regard of speaker, emotion, and language. Moreover, in a multi-speaker text-to-speech scenario using mel-spectrogram generated by a transformer model, it synthesized high-fidelity speech of 4.22 MOS. These results, achieved without external domain information, highlight the potential of the proposed model as a universal vocoder.",303
" %What is spoken term detection  Unsupervised speech modeling is the task of discovering and modeling speech units at various levels from audio recording without using any prior linguistic information. It is an interesting, challenging and impactful research problem as phonetic, lexical and even semantic information could be acquired without the process of transcribing and understanding the given speech data. The relevant technology is particularly important to facilitate data preparation especially in the scenarios where: 1) a large  amount of audio data are readily available online but they are untranscribed; 2) a large amount of audio recording is available for an unpopular language about which no structured linguistic knowledge or documentation can be found.  Spoken term discovery is a representative task of unsupervised speech modeling. It aims to discover repetitively occurred words and/or phrases from untranscribed audio.  The problem is commonly tackled with a two-stage approach. In the first stage, a set of subword units are automatically discovered from untranscribed speech data and these units in turn can be used to represent the speech data as a symbol sequence. In the second stage, variable-length sequence matching and clustering are performed on the subword sequence representations. One major drawback of this is that the subword decoding errors in the first stage would propagate to deteriorate the outcome of spoken term discovery in the second stage. The present study investigates the use of Siamese and Triplet networks in spoken term discovery. Siamese network has been commonly applied to pattern classification or matching problems when only weak labels are available. We propose to train a Siamese/Triplet network with a small dataset of matched and mismatched sequence pairs obtained and use the trained network to generate feature representations for unseen subword sequences. The training dataset is constructed based on hypothesized spoken term clusters from an baseline spoken term discovery system developed in our previous study. With the new feature representations learned by the Siamese/Triplet network, re-clustering of subword sequences is carried out to generate an improved set of discovered spoken terms.   
"," Spoken term discovery from untranscribed speech audio could be achieved via a two-stage process. In the first stage, the unlabelled speech is decoded into a sequence of subword units that are learned and modelled in an unsupervised manner. In the second stage, partial sequence matching and clustering are performed on the decoded subword sequences, resulting in a set of discovered words or phrases. A limitation of this approach is that the results of subword decoding could be erroneous, and the errors would impact the subsequent steps. While Siamese/Triplet network is one approach to learn segment representations that can improve the discovery process, the challenge in spoken term discovery under a complete unsupervised scenario is that training examples are unavailable. In this paper, we propose to generate training examples from initial hypothesized sequence clusters. The Siamese/Triplet network is trained on the hypothesized examples to measure the similarity between two speech segments and hereby perform re-clustering of all hypothesized subword sequences to achieve spoken term discovery.  Experimental results show that the proposed approach is effective in obtaining training examples for Siamese and Triplet networks, improving the efficacy of spoken term discovery as compared with the original two-stage method.",304
" Evidence-based medicine  is a medical practice that aims to find all the evidence to support medical decisions. This evidence nowadays is obtained from biomedical journals, usually accessible through online databases like PubMed and EMBASE, which provide free access to articles' abstracts and in some cases, to full articles. In the context of the COVID-19 pandemic, EBM is critical to making decisions at the individual level and public health since research articles address topics like treatments, adverse cases, and effects of public policies in medicine. The EBM foundation Epistemonikos has made essential contributions by curating and publishing updated guides of what treatments are working and not against COVID-19~\footnote{http://epistemonikos.org/}. Epistemonikos addresses EBM by a combination of software tools for data collection, storage, filtering , and retrieval, as well as by the vital labor of volunteer physicians who curate and label research articles based on quality , type  and PICO labels . However, this workflow has been challenged during 2020 by increasing growth and rapidly evolving evidence of COVID-19 articles published in the latest months. Moreover, to ensure the rapid collection of the latest evidence published, pre-print repositories such as medRXiv and bioRXiv have been added to the traditional online databases. % In order to support Epistemonikos' effort to filter and curate the flood of articles related to COVID-19, we present the results of an applied AI project where we implement and evaluate a text classification system to filter and categorize research articles related to COVID-19. The current model, based on Random Forests, has an acceptable performance classifying systematic reviews  but fails on classifying other document categories. In this article, we show how using BioBERT yields marginal improvements, while XLNET results in significant progress with the best performance. These results save a considerable amount of time from volunteer physicians by pre-filtering the articles worth of manual curation and labeling for EBM. In average, a physician takes two minutes in reviewing one article, while the system we present in this article can review up to  within one hour.   %With the help of volunteer physicians, they classify emergent literature for the COVID-19 virus in systematic reviews, broad syntheses, or primary studies, which is the first step for finding relevant clinical evidence. Until now, they produced a Random Forest model for classifying documents into different categories. However, in this paper, we show how the use of Transformers-based Language Models  helped this foundation save significant effort to their collaborators.   %
","  The COVID-19 has brought about a significant challenge to the whole of humanity, but with a special burden upon the medical community. Clinicians must keep updated continuously about symptoms, diagnoses, and effectiveness of emergent treatments under a never-ending flood of scientific literature. In this context, the role of evidence-based medicine  for curating the most substantial evidence to support public health and clinical practice turns essential but is being challenged as never before due to the high volume of research articles published and pre-prints posted daily. Artificial Intelligence can have a crucial role in this situation. In this article, we report the results of an applied research project to classify scientific articles to support Epistemonikos, one of the most active foundations worldwide conducting EBM. We test several methods, and the best one, based on the XLNet neural language model, improves the current approach by 93\% on average F1-score, saving valuable time from physicians who volunteer to curate COVID-19 research articles manually.",305
"  The natural language processing community has made tremendous progress  in using pre-trained language models to improve predictive accuracy . Models have now surpassed human performance on language understanding benchmarks such as SuperGLUE . However, studies have shown that these results are partially driven by these models detecting superficial cues that correlate well with labels but which may not be useful for the intended underlying task . This brittleness leads to overestimating model performance on the artificially constructed tasks and poor performance in out-of-distribution or adversarial examples.  A well-studied example of this phenomenon is the natural language inference dataset MNLI . The generation of this dataset led to spurious surface patterns that correlate noticeably with the labels.  highlight that negation words  are often associated with the contradiction label.  show that a model trained solely on the hypothesis, completely ignoring the intended signal, reaches strong performance. We refer to these surface patterns as dataset biases since the conditional distribution of the labels given such biased features is likely to change in examples outside the training data distribution .  A major challenge in representation learning for NLP is to produce models that are robust to these dataset biases. Previous work  has targeted removing dataset biases by explicitly factoring them out of models. These works explicitly construct a biased model, for instance, a hypothesis-only model for NLI experiments, and use it to improve the robustness of the main model. The core idea is to encourage the main model to find a different explanation where the biased model is wrong. During training, products-of-experts ensembling  is used to factor out the biased model.   While these works show promising results, the assumption of knowledge of the underlying dataset bias is quite restrictive. Finding dataset biases in established datasets is a costly and time-consuming process, and may require access to private details about the annotation procedure, while actively reducing surface correlations in the collection process of new datasets is challenging given the number of potential biases .  In this work, we explore methods for learning from biased datasets which do not require such an explicit formulation of the dataset biases. We first show how a model with limited capacity, which we call a weak learner, trained with a standard cross-entropy loss learns to exploit biases in the dataset. We then investigate the biases on which this weak learner relies and show that they match several previously manually identified biases. Based on this observation, we leverage such limited capacity models in a product of experts ensemble to train a more robust model and evaluate our approach in various settings ranging from toy datasets up to large crowd-sourced benchmarks: controlled synthetic bias setup , natural language inference  and extractive question answering .  Our contributions are the following:  we show that weak learners are prone to relying on shallow heuristics and highlight how they rediscover previously human-identified dataset biases;  we demonstrate that we do not need to explicitly know or model dataset biases to train more robust models that generalize better to out-of-distribution examples;  we discuss the design choices for weak learners and show trade-offs between higher out-of-distribution performance at the expense of the in-distribution performance.   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
"," State-of-the-art natural language processing  models often learn to model dataset biases and surface form correlations instead of features that target the intended underlying task. Previous work has demonstrated effective methods to circumvent these issues when knowledge of the bias is available. We consider cases where the bias issues may not be explicitly identified, and show a method for training models that learn to ignore these problematic correlations.  Our approach relies on the observation that models with limited capacity primarily learn to exploit biases in the dataset. We can leverage the errors of such limited capacity models to train a more robust model in a product of experts, thus bypassing the need to hand-craft a biased model. We show the effectiveness of this method to retain improvements in out-of-distribution settings even if no particular bias is targeted by the biased model.",306
" Topic models  have been popularly used to extract abstract topics which occur commonly across documents in a corpus in the field of Natural Language Processing. Each topic is a group of semantically coherent words that represent a common concept. In addition to gaining insights from unstructured texts, topic models have been used in several tasks of practical importance such as learning text representations for document classification , keyphrase extraction , review understanding for recommendations in e-commerce domain , semantic similarity detection between texts  etc.  % in order to make topic sampling distribution converge to the desired posterior distribution  Early popular works on topic discovery include statistical methods such as Latent Dirichlet Allocation   which approximates each topic as a probability distribution over word vocabulary and performs approximate inference over document-topic and topic-word distributions through Variational Bayes . This was followed by a modified inference algorithm - Collapsed Gibbs sampling  that follows Markov Chain Monte Carlo  . However, these methods require an expensive iterative inference step which has to be performed for each document. This was circumvented through introduction of deep neural networks  and emergence of Variational Autoencoders   in particular, where variational inference can be performed in single forward pass.  % while estimating the posterior distribution. % Laplace approximation of  % The re-parameterisation trick of VAEs allows to perform variational inference in a differentiable manner while training the neural network.  Such neural variational inference based topic models  outperformed the traditional probabilistic sampling methods. Broadly, they model a document as Bag-of-Words  determined on the basis of frequency count of each vocabulary token in the given document. The BoW input is processed through an MLP followed by variational inference  which samples a latent document-topic vector. A decoder network then reconstructs original BoW using latent document-topic vector which allows it to capture relationship between document-topic and topic-word distributions. VAE family of neural topic models can be categorised on the basis of prior enforced on latent document-topic distribution. Methods such as NVDM , NTM-R , NVDM-GSM  use Gaussian prior. NVLDA and ProdLDA  use Dirichlet prior approximation which enables model to capture that a document stems from sparse set of topics.  % and perform better by providing more coherent topics compared to Gaussian prior.  % in order to capture latent document-topic distribution,  % The context vector obtained as a result of attention is used to perform variational inference %  and capture semantics effectively  % which can further help in inferring latent document-topic vector % as carried  in usual VAE based topic models  % using the final LSTM state and the outputs corresponding  While the main focus of previous neural topic models has been to enforce suitable priors, little effort has been spent on explicitly improving the document encoding framework in order to capture document semantics better. In this work, we build upon VAE based topic model using laplace approximation to Dirichlet prior and propose a novel framework where we model the input document as a sequence of tokens. The sequence is processed through an LSTM  that allows it to encode the sequential order which does not remain preserved in BoW. To allow the model to focus on specific parts in the document, we use an attention mechanism  to attend at different document tokens. We hypothesise that topic-word distribution being learned by the model can be factored in the attention mechanism to enable the model to attend on tokens which convey topic related information and cues. We validate our hypothesis and propose TAN-NTM: Topic Attention Networks for Neural Topic Modeling which performs attention efficiently in a topic guided manner. We perform separate attention for each topic using its corresponding word probability distribution and obtain topic-wise context vectors. The context vectors are then composed using topic weights which represent proportion of each topic present in a given document. These topic weights are obtained using the learned token embedding and topic-word distribution. The final composed context vector is then used to perform variational inference followed by BoW decoding. We perform extensive ablations to compare different ways of composing topic-wise context vectors.  % and averages the coherence score over the topics % generated by the model   In order to evaluate our approach, we estimate commonly used NPMI coherence  which measures the extent to which most probable words in a topic are semantically related to each other. Using this metric, we compare our TAN-NTM model with several previous state-of-the-art topic models  outperforming them significantly over 4 benchmark datasets of varying scale and complexity - 20NewsGroup  , Yelp Review Polarity, DBpedia  and AGNews . We demonstrate the efficacy of our model in learning better document feature representations and latent document-topic vectors by achieving higher document classification accuracy over baseline topics models. Furthermore, topic models have previously been used to improve supervised keyphrase generation . We show that our proposed framework can be adapted to modify the topic model and further improve keyphrase generation achieving SOTA performance on StackExchange and Weibo datasets. Our contributions can be summarised as:       % 
"," Topic models have been widely used to learn representations from text and gain insight into document corpora. To perform topic discovery, existing neural models use document bag-of-words  representation as input followed by variational inference and learn topic-word distribution through reconstructing BoW. Such methods have mainly focused on analysing the effect of enforcing suitable priors on document distribution. However, little importance has been given to encoding improved document features for capturing document semantics better. In this work, we propose a novel framework: TAN-NTM which models document as a sequence of tokens instead of BoW at the input layer and processes it through an LSTM whose output is used to perform variational inference followed by BoW decoding. We apply attention on LSTM outputs to empower the model to attend on relevant words which convey topic related cues. We hypothesise that attention can be performed effectively if done in a topic guided manner and establish this empirically through ablations. We factor in topic-word distribution to perform topic aware attention achieving state-of-the-art results with $\sim$ $9$ - $15$ percentage improvement over score of existing SOTA topic models in NPMI coherence metric on four benchmark datasets - 20NewsGroup, Yelp, AGNews, DBpedia. TAN-NTM also obtains better document classification accuracy owing to learning improved document-topic features. We qualitatively discuss that attention mechanism enables unsupervised discovery of keywords. Motivated by this, we further show that our proposed framework achieves state-of-the-art performance on topic aware supervised generation of keyphrases on StackExchange and Weibo datasets.",307
" Popular static word representations such as word2vec~ lie in Euclidean space and are evaluated against symmetric judgments. Such a measure does not expose the geometry of word relations, e.g., asymmetry. For example, ``ellipses are like circles'' is much more natural than ``circles are like ellipses''. An acceptable representation may exhibit such a property.  ~\citet{tversky1977features} proposed a similarity measure that encodes asymmetry. %Similarity sim  of two words A, B can be evaluated by  where  and  are two factors weighting contribution of each non-overlapping part differently.  It assumes each word is a feature set, and asymmetry manifests when the common features of two words take different proportions in their respective feature sets, i.e., a difference of the likelihoods  and  for a word pair . In this regard, the degree of correlation between asymmetry obtained from humans and a word embedding may indicate the quality of the embedding of encoding features.  Word evocation experiment devised by neurologist Sigmund Freud around the 1910s was to obtain such word directional relationship, where a word called cue is shown to a participant who is asked to ``evoke'' another word called target freely. ~ The experiment is usually conducted on many participants for many cue words. The data produced from the group of people exhibit a collective nature of word relatedness. The  and  can be obtained from such data to obtain an asymmetry ratio~ that resonates with the theory of~\citet{tversky1977features} and ~\citet{Resnik:1995:UIC:1625855.1625914}.  Large scale evocation datasets had been created to study the psychological aspects of language. We are interested in three of them; the Edinburgh Association Thesaurus %  ~, Florida Association Norms % ~ and Small World of Words %    %.  Those three datasets have thousands of cue words each and all publicly available. We use them to derive the human asymmetry judgments and see how well embedding-derived asymmetry measure aligns with this data.  Evocation data was rarely explored in the Computational Linguistics community, except that~\citet{griffiths2007topics} derived from the Florida Association Norms an asymmetry ratio for a pair of words to measure the directionality of word relations in topic models, and~\citet{nematzadeh2017evaluating} used it for word embedding. In this paper, we conduct a larger scale study using three datasets, on both static embedding ~, GloVe ~, fasttext~) and contextual embedding such as BERT~. We hope the study could help us better understand the geometry of word representations and inspire us to improve text representation learning.  To obtain  for static embedding, we leverage vector space geometry with projection and soft-max similar to ~; For contextual embedding such as BERT we can not use this method because the embedding varies by context. Thus, we use a Bayesian method to estimate word conditional distribution from thousands of contexts using BERT as a language model. In so doing, we can probe the word relatedness in the dynamic embedding space in a principled way.  Comparing an asymmetry measure to the popular cosine measure, we observe that similarity judgment fails to correctly measure BERT's lexical semantic space, while asymmetry judgment shows an intuitive correlation with human data. In the final part of this paper, we briefly discuss the result and what it means to representation learning. This paper makes the following contributions:    
"," Human judgments of word similarity have been a popular method of evaluating the quality of word embedding. But it fails to measure the geometry properties such as asymmetry. For example, it is more natural to say ``Ellipses are like Circles'' than ``Circles are like Ellipses''. Such asymmetry has been observed from a psychoanalysis test called word evocation experiment, where one word is used to recall another. Although useful, such experimental data have been significantly understudied for measuring embedding quality. In this paper, we use three well-known evocation datasets to gain insights into asymmetry encoding of embedding. We study both static embedding as well as contextual embedding, such as BERT. Evaluating asymmetry for BERT is generally hard due to the dynamic nature of embedding. Thus, we probe BERT's conditional probabilities  using a large number of Wikipedia contexts to derive a theoretically justifiable Bayesian asymmetry score. The result shows that contextual embedding shows randomness than static embedding on similarity judgments while performing well on asymmetry judgment, which aligns with its strong performance on ``extrinsic evaluations'' such as text classification. The asymmetry judgment and the Bayesian approach provides a new perspective to evaluate contextual embedding on intrinsic evaluation, and its comparison to similarity evaluation concludes our work with a discussion on the current state and the future of representation learning.",308
" Humor plays an important role in social communications. Unlike many objective classification tasks, the task of humor recognition is constrained by its subjectivity. The perception of the same joke can differ among people due to individual differences in their cognitive processes responsible for humor processing , which is as illustrated in Figure . This makes it challenging for humor recognition models to generalize to a wide range of users, as the training data may reflect the subjectivity of the annotators  or experiment participants . To achieve personalized humor recognition, it is necessary to consider the diversity of user preferences.    Previous research on automated humor recognition casts the task as a binary classification problem . These methods mainly focus on how to design humor-related linguistic features as input to a classifier to obtain high classification performance. With well-established computational humor theories , they can curate many heuristics to extract informative features. The key of heuristic rules is to design effective approaches to capture linguistic patterns  or n-gram statistics  that can distinguish humorous text from plain text. These methods are able to characterize intra-sentence and inter-sentence dependencies that are unique to humor, and thus do not rely a lot on the complexity of classifiers. Nevertheless, the feature generation process requires significant efforts and many have difficulties to cope with newly encountered terms .    Deep learning shifts the focus of AI research from feature engineering to automatic feature extraction. Convolutional Neural Networks  and Transformer-based language models  have been used for end-to-end humor recognition.  Most of previous studies are conducted on curated and explicitly balanced datasets  with the underlying assumption that people more or less agree on the distinction between humorous and non-humorous text. This assumption could limit the model's ability to generalize in practice.  Federated Learning, a technique that trains a deep neural network based on iterative averaging of decentralized local updates, has been proved to be good at handling unbalanced and non-IID data distributions . Inspired by recent progress of federated learning in diversity  and personalization , we propose to improve the ability of humor recognition models to generalize to diverse user preferences with the help of federated learning. We name the model . Specifically, we adopt the Federated Averaging  algorithm  in the fine-tuning of a pretrained Transformer-based language model on our task, and employ a diversification strategy  to handle disparate user preferences.   The main idea of our solution is to force the humor recognition model to learn from a diverse range of user preferences, thereby enhancing the adaptability to new users. For this purpose, there are two important issues to consider. First, as users are increasingly aware of privacy issues and reluctant to provide personal information , it is imperative that we preserve users' privacy and avoid direct harvesting of explicit user preference from their personal devices. To address this, we propose an approximation strategy to generate implicit user feedback  on given humorous text and we diversify the label distributions to represent diverse user preferences. Second, marginal distributions of user preferences  often lead to salient class imbalance issue which requires us to select a more suitable evaluation metric rather than widely adopted accuracy. As such, we use F1 score to evaluate and select best models.   To the best of our knowledge, FedHumor is the first federated learning-based humor recognition model. Extensive results show that our approach is able to increase the generalization bounds of the humor recognition model compared to 9 other state-of-the-art approaches. It is a promising approach to help future AI applications recommend suitable humorous texts to users under tightened data privacy protection regulations , thereby enabling innovative and emerging forms of human-AI interaction.  
"," Understanding humor is critical to creative language modeling with many applications in human-AI interaction. However, due to differences in the cognitive systems of the audience, the perception of humor can be highly subjective. Thus, a given passage can be regarded as funny to different degrees by different readers. This makes training humorous text recognition models that can adapt to diverse humor preferences highly challenging. In this paper, we propose the FedHumor approach to recognize humorous text contents in a personalized manner through federated learning . It is a federated BERT model capable of jointly considering the overall distribution of humor scores with humor labels by individuals for given texts. Extensive experiments demonstrate significant advantages of FedHumor in recognizing humor contents accurately for people with diverse humor preferences compared to 9 state-of-the-art humor recognition approaches.",309
"  .     %      % % final paper: en-us version      %       % space normally used by the marker     This work is licensed under a Creative Commons      Attribution 4.0 International License.     License details:     \url{http://creativecommons.org/licenses/by/4.0/}. } Rhetorical Structure Theory   is one of the most influential theories of discourse analysis, under which a document is represented by a hierarchical discourse tree. As shown in Figure a, the leaf nodes of an RST tree are text spans named Elementary Discourse Units , and the EDUs are connected by rhetorical relations  to form larger text spans until the entire document is included.  The rhetorical relations are further categorized to Nucleus  and Satellite  based on their relative importance. Thus, document-level discourse parsing consists of three sub-tasks: tree construction, nuclearity determination and relation classification. Moreover, downstream natural language processing tasks can benefit from RST-based structure-aware document analysis, such as summarization  and machine comprehension .  By utilizing various linguistic characteristics , statistical approaches have obtained substantial improvement on the English RST-DT benchmark . Recently, neural networks have been making inroads into discourse analysis frameworks, such as attention-based hierarchical encoding  and integrating neural-based syntactic features into a transition-based parser . Lin et al. \shortcite{lin2019unified} and their follow-up work  successfully explored encoder-decoder neural architectures on sentence-level discourse analysis, with a top-down parsing procedure.  Although discourse parsing has received much research attention and progress, the models are mainly optimized and evaluated in English. The main challenge is the shortage of annotated data, since manual annotation under the RST framework is labor-intensive and requires specialized linguistic knowledge. For instance, the most popular benchmark English RST-DT corpus  only contains 385 samples, which is much smaller than those of other natural language processing tasks. The treebank size of other languages such as German , Dutch  and Basque  are even more limited. Such limitations make it difficult to achieve acceptable performance on these languages required to fully support downstream tasks, and also lead to poor generalization ability of the computational approaches.  Since the treebanks of different languages share the same underlying linguistic theory, data-driven approaches can benefit from joint learning on multilingual RST resources . Therefore, in this paper, we investigate two methods to build a cross-lingual neural discourse parser:  From the embedding perspective: with the cross-lingual contextualized language models, we can train a parser on the shared semantic space from multilingual sources without employing a language indicator;  From the text perspective: since each EDU is a semantically-cohesive unit, we can unify the target language space by EDU-level translation, while preserving the original EDU segmentation and the discourse tree structures . To this end, we adapted and enhanced an end-to-end neural discourse parser, and investigated the two proposed approaches on 6 different languages. While the RST data for training is still in a small scale, we achieved the state-of-the-art performance on all fronts, significantly surpassing previous models, and even approaching the upper bound of human performance. Moreover, we conducted a topic modeling analysis on the collected multilingual treebanks to evaluate the model generality across various domains.     
"," Text discourse parsing plays an important role in understanding information flow and argumentative structure in natural language. Previous research under the Rhetorical Structure Theory  has mostly focused on inducing and evaluating models from the English treebank. However, the parsing tasks for other languages such as German, Dutch, and Portuguese are still challenging due to the shortage of annotated data. In this work, we investigate two approaches to establish a neural, cross-lingual discourse parser via:  utilizing multilingual vector representations; and  adopting segment-level translation of the source content. Experiment results show that both methods are effective even with limited training data, and achieve state-of-the-art performance on cross-lingual, document-level discourse parsing on all sub-tasks. \newline",310
" In recent years, smart devices with built-in personal assistants like Google Assistant and Siri are becoming omnipresent. Behind these intelligent systems, a key question is how to identify the underlying intent of a user utterance, which has triggered a large amount of work on intent detection . Most existing intent detection systems are built on deep learning models trained on large-scale annotated data. However, as user demands and the functions of smart devices continue to grow, collecting supervised data for every new intent becomes time-consuming and labor-intensive.  To address this issue, some studies tackle intent detection in the zero-shot learning  manner, attempting to utilize the learned knowledge of seen classes to help detect unseen classes. The recent methods of zero-shot intent detection  can be roughly divided into two categories: The first category , referred to as the transformation-based methods, utilizes word embeddings of label names to establish a similarity matrix, which is then used to transfer the prediction space of seen intents to unseen intents. Another line of work is based on the compatibility-based methods , which aims to encode the label names and utterances into representations in the same semantic space and then calculate their similarity. In both kinds of methods, a critical problem is learning intent representations. However, most existing ZSID methods are class-inductive, which relies entirely on labeled data from seen intents in the training stage. Consequently, the representations of unseen intents cannot be learned, resulting in two limitations.  First, the ZSID methods are not good at modeling the relationship between seen and unseen intents. For the transformation-based methods, when the label names are given in the form of raw phrases or sentences, word embeddings of label names are inadequate to associate the connections between seen and unseen intents. For example, 閳ユ窂ookRestaurant閳 is similar to 閳ユ珐ateBook閳 when measured by word embeddings, as they share the word 閳ユ窂ook閳 . However, the meaning of these two intents are not that relevant. % As a result, the computed similarity matrix is inadequate in associating the connections between seen and unseen intents .  For the compatibility-based methods, they minimize the similarity between seen intent samples and seen label names in a shared semantic space, and directly transfer it to detect unseen intents. Since the unseen intent representations are not learned, they might be entangled with the representations of seen intents. This can severely hurt the accuracy of the predicted label-utterance similarity, especially when the expressions of utterances are diverse. %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Second, the vanilla ZSL methods are not applicable to generalized intent detection . Compared with the ZSL setting , which assumes that the models are only presented with utterances from unseen classes at test time, GZSID requires the model to detect both seen and unseen intents. In GZSID, existing ZSL models usually suffer from the dubbed domain shift  problem, in which utterances from unseen intents are almost always mistakenly classified into seen intents.   Unlike the class-inductive methods, class-transductive ZSL uses semantic information about the unseen classes for model training . In the context of intent detection, the label name provides a proper sketch of the intent meaning. Motivated by this, we propose to utilize label names of the unseen intents to learn disentangled intent representations . Specifically, we include the unseen intents into the prediction space during training, with the label names serving as the pseudo utterances. This allows the model to learn the boundary of each seen and unseen class in the semantic space. Under this framework, we introduce an assistant task that forces the model to find the distinction between seen and unseen intents, thereby alleviating the domain-shift problem. On this basis, we refine the word embedding based similarity matrix by averaging the representations of all corresponding  utterances and  label names. As a result, we can better capture the intent meanings and the similarity matrix reflects more accurate intent connections.   In summary, our contribution is three-fold:  We believe that the potential of class-transductive ZSL  in intent detection is still not fully exploited, to encourage more related studies in the future, we will release our codes and data.    
"," Zero-shot intent detection  aims to deal with the continuously emerging intents without annotated training data. However, existing ZSID systems suffer from two limitations: 1) They are not good at modeling the relationship between seen and unseen intents, when the label names are given in the form of raw phrases or sentences. 2) They cannot effectively recognize unseen intents under the generalized intent detection  setting. A critical factor behind these limitations is the representations of unseen intents, which cannot be learned in the training stage. To address this problem, we propose a class-transductive framework that utilizes unseen class labels to learn Disentangled Intent Representations . Specifically, we allow the model to predict unseen intents in the training stage, with the corresponding label names serving as input utterances. Under this framework, we introduce a multi-task learning objective, which encourages the model to learn the distinctions among intents, and a similarity scorer, which estimates the connections among intents more accurately based on the learned intent representations. % Moreover, we present a novel approach to calculate the inter-intent similarities, on the basis of the learned intent representations, which estimates the connections among intents more accurately.  Since the purpose of DIR is to provide better intent representations, it can be easily integrated with existing ZSID and GZSID methods. Experiments on two real-world datasets show that the proposed framework brings consistent improvement to the baseline systems, regardless of the model architectures or zero-shot learning strategies.",311
" Multi-turn open-domain dialogue modeling is an active research topic in the field of natural language processing.  However, generating a coherent and informative response for a given dialogue context remains a challenge. % However, it is still challenging for dialogue models to generate a coherent and informative response for a given dialogue context. %Research in this domain mainly addresses the following two questions: 1) How can we learn to represent the context? 2) In the presence of context representation, how can we infer the distribution of the response?  A critical challenge is the learning of rich and robust context representations of dialogue utterances~, namely the challenge of encoding a dialogue context into a vector that adequately captures the semantics . % A major challenge in this domain is to learn rich and robust context representations of dialogue utterances~, namely the challenge of encoding a dialogue context into a vector that adequately captures the semantics .  Large-scale pre-training language models using Transformer-based architectures have recently achieved remarkable successes in a variety of NLP tasks~. % Recently, large-scale pre-training language models using Transformer-based architectures have achieved remarkable successes in a variety of NLP tasks~.  As such, there are increasingly work that aims to use pre-training language models for conversation modeling~. For example, DialoGPT~ extends the GPT-2~ to generate conversation responses on large-scale dialogue corpus. Meena~ trains a sequence-to-sequence model~ with the Evolved Transformer~ on large-scale multi-turn conversations.  Blender, developed by Facebook, provides recipes for building open-domain chatbots that perform well in human evaluations~.  However, existing pre-training conversation models usually view the dialogue context as a linear sequence of tokens and learns to generate the next word through token-level self-attention.  One issue of this approach is that the high-level relationships between utterances are harder to capture using word-level semantics. % One issue of this approach is that the relationships between utterances are scattered into individual words, hindering the capturing of discourse-level coherence.  For example, the discourse-level relationship between the utterances ``coffee please'' and ``here you are''  is apparent, but word-level comparisons, such as coffee, you and please, are, obscures the high-level relationship. % For example, the utterance ``coffee please'' and ``here you are'' in Figure have a strong certain relationship, by contrast, pairs of individual words in these two utterances such as coffee, you and please, are have obscure correlations. Furthermore, this full pairwise attention is inefficient since it requires each word in the context and the decoder to interact with all other words regardless of their distances and semantic units. % Furthermore, the full pairwise attention is inefficient since it requires each word in the context and the decoder to interact with all other words regardless of their distances and semantic units.  To alleviate the issues above, we present DialogBERT, a novel conversational response generation model.  % To alleviate the aforementioned issues, we present DialogBERT, a novel conversational response generation model.  DialogBERT employs a hierarchical Transformer architecture to represent the dialogue context.  It first encodes dialogue utterances through a Transformer encoder and then encodes the resulting utterance vectors using a discourse-level Transformer to obtain a representation of the entire dialogue context.  To efficiently capture discourse-level coherence among utterances, we propose two training objectives in analogy to the original BERT training: 1) masked context regression, which masks a randomly-selected utterance and predicts the encoding vector for the masked utterance directly; and 2) distributed utterance order ranking, which %reconstructs the order of utterances that belong to the same dialog context   organizes randomly shuffled utterances of a conversation into a coherent dialogue context  through a Learning-to-Rank~ neural network.  We evaluate DialogBERT on popular multi-turn conversation datasets, namely Weibo, MultiWOZ and DailyDialog.  Results show that DialogBERT outperforms baselines in terms of perplexity, BLEU, and NIST. Human evaluation supports the superiority of our approach in capturing discourse-level semantics and generating more plausible dialogue responses.  %Our contributions can be summarized as follows: %   
"," Recent advances in pre-trained language models have significantly improved neural response generation.  However, existing methods usually view the dialogue context as a linear sequence of tokens and learn to generate the next word through token-level self-attention.  Such token-level encoding hinders the exploration of discourse-level coherence among utterances.  This paper presents DialogBERT, a novel conversational response generation model that enhances previous PLM-based dialogue models. DialogBERT employs a hierarchical Transformer architecture.  To efficiently capture the discourse-level coherence among utterances, we propose two training objectives, including masked utterance regression and distributed utterance order ranking in analogy to the original BERT training.  Experiments on three multi-turn conversation datasets show that our approach remarkably outperforms the baselines, such as BART and DialoGPT, in terms of quantitative evaluation.  The human evaluation suggests that DialogBERT generates more coherent, informative, and human-like responses than the baselines with significant margins. % Pre-trained language models  have been successfully adapted to neural response generation.  % However, existing methods usually view the dialogue context as a linear sequence of tokens and learn to generate the next word through token-level self-attention. % Such token-level encoding hinders the exploration of discourse-level coherence among utterances. % In this paper, we present DialogBERT, a novel conversational response generation model that enhances previous PLM-based dialogue models. % %In order to model the utterance-level interactions, % Instead of a flat encoding of linear tokens, DialogBERT employs a hierarchical Transformer architecture.  % %DialogBERT consists of an utterance encoder for encoding utterances and a context encoder for learning to contextualize given utterances' representations. % To efficiently capture the discourse-level coherence among utterances, we propose two new training objectives including masked utterance regression and distributed utterance order ranking in analogy to the original BERT training.  % Experiments on three multi-turn conversation datasets show that  % our approach remarkably outperforms three baselines such as BART and DialoGPT in terms of quantitative evaluation.  % Human evaluation  % suggests  % %\jw{supports}  % that DialogBERT generates more coherent, informative and human-like responses than the baselines with significant margins.",312
" Event Detection ,  the task of which involves identifying the boundaries of event triggers and classifying them into the corresponding event types, aims to seek recognize events of specific types from given texts. As a fundamental task of information extraction, many high-level NLP tasks, such as information retrieval and question answering, need an event detector as one of their essential components.    % 娑旂喕顩﹂崝鐘茬穿閻 Recent studies show that English ED models have achieved great performance by treating the problem as a word-by-word sequence labeling task.  Different from English ED, many East Asian languages, including Chinese, are written without explicit word boundary, resulting a much tricky ED task. An intuitive solution is to apply Chinese Word Segmentation  tools first to get word boundaries, and then use a word-level sequence labeling model similar to the English ED models.  However, word boundary is ambiguous in Chinese thus word-trigger mismatch problem exists in Chinese ED, where an event trigger may not exactly match with a word, but is likely to be part of a word or cross multiple words as Figure demonstrates. Meanwhile, character-level sequence tagging is able to alleviate this problem, but Chinese character embedding can only carry limited information due to the lack of word and word-sequence information, resulting to ambiguous semantics.  % Therefore, how to better integrate segmentation-related information and character-level semantics is a key feature in Chinese ED models.  Several recent works have demonstrated that considering the lexicon word information could provide more exact information to discriminate semantics of characters. \citeauthor{lin-etal-2018-nugget}~\shortcite{lin-etal-2018-nugget} designed NPN, a CNN-like network to model character compositional structure of trigger words and introduced a gate mechanism to fuse information from characters and words. ~\citeauthor{ding-etal-2019-event}~\shortcite{ding-etal-2019-event} proposed TLNN, a trigger-aware Lattice LSTM architecture, exploiting semantics from matched lexicon words to improve Chinese ED.    Although these methods have achieved great success, they continue to have difficulty in fully exploiting the interaction between characters and lexicon words. Specifically, for each character, NPN exploits a gate mechanism to fuse its information with one corresponding word. This means that each character could only be incorporated with one matched word, but actually one character is likely to match with several words, leading to information loss. For TLNN, it constructs cut paths to link the start and end character for each matched word, but semantic information of the matched lexicon word fails to flow into all the characters it covers except the last one, due to the inherently unidirectional sequential nature of Lattice LSTM. %For characters without matched words, no extra information is provided enhance its representation.  Besides, previous ED works usually ignore semantic information maintained by the event types. We observe that event types are usually semantically related to the corresponding event triggers.   Such an observation shows that considering the semantic information of event labels may provide fine-grained semantic signals to guide the detection of event triggers, and accordingly benefit ED performance.  In this paper, we propose a novel neural architecture, named Label Enhanced Heterogeneous Graph Attention Networks , for Chinese ED. To promote better information interaction between words and characters, we transform each sentence into a graph.  We first connect lexicon words with all the characters it covers. And then neighboring characters are also linked with each other to provide local context information to enhance character representations, especially for those without matched lexicon word. To capture different granularity of semantic information from words and characters, we formulate words and characters as two types of nodes, thus a heterogeneous graph attention networks is utilized to enable rich information propagation over the graph. Additionally, we design a matcher module to leverage the semantic information of event labels. Specifically, we transform event labels into an event-trigger-prototype based embedding matrix by summarizing the trigger representations belonging to each event label. Based on the generated event label representation, a margin loss is further exploited to enhance the ability to discriminate confusing event labels. Comparing with previous works, our contributions are as follows:   
"," Event Detection  aims to recognize instances of specified types of event triggers in text. Different from English ED, Chinese ED suffers from the problem of word-trigger mismatch due to the uncertain word boundaries.  Existing approaches injecting word information into character-level models have achieved promising progress  to alleviate this problem, but they are limited by two issues. First, the interaction between characters and lexicon words is not fully exploited. Second, they ignore the semantic information provided by event labels.  We thus propose a novel architecture named Label enhanced Heterogeneous Graph Attention Networks .  Specifically, we transform each sentence into a graph, where character nodes and word nodes are connected with different types of edges, so that the interaction between words and characters is fully reserved. A heterogeneous graph attention networks is then introduced to propagate relational message and enrich information interaction. Furthermore, we convert each label into a trigger-prototype-based embedding, and design a margin loss to guide the model distinguish confusing event labels. Experiments on two benchmark datasets show that our model achieves significant improvement over a range of competitive baseline methods.",313
" % \rev{@Ileana: this is an example on how to indicate changes in the text, based on the revision.} % \todo[inline]{we need to add color bars on figures, as promised to the reviewers}    Given enough computational power, the scalability of the attention mechanism~ will allow for building ever larger Natural Language Processing  models with billions of parameters . While impressive, these advances also pose a responsibility to the NLP community to interpret the behavior of the hundreds of attention heads in a single model, and potentially to reduce the number of computations. Responding to this challenge, previous work has taken pioneering steps to discover and to explain the sparseness in the attention patters. Here, we argue that as the number of heads grows in the range of thousands, automatic measures would be needed to discover and to impose sparseness to such models.  We introduce a simple task-agnostic data-informed pruning method for attention mechanisms: Attention Pruning. We train Transformer-based models and we analyze global observed attention patterns, averaged over all input sequences in the train set, in order to identify and to remove weak connections between the input tokens. Following \citet{lottery}, we then retrain these models, enforcing sparseness through masking, and we demonstrate that attention mechanisms incorporate extraneous connections between the input tokens: we obtain comparable  % \question{or even marginally better performance} performance while using sparse attention patterns for NLP tasks such as language and sequence-to-sequence  modelling, as well as %Natural Language  Inference . \rev{prediction on GLUE tasks. Figure summarizes the impact of using our pruning method on standard NLP tasks.}     These global sparseness patterns could help improve both interpretability and inference-time computational efficiency for widely-used attention models. Our contributions are as follows:    % The rest of the paper is organized as follows: In Section, we present related work. In Section, we introduce the details behind our attention pruning method. In Section, we apply AP to experiments with language modelling. In Section, we apply AP for seq2seq modelling on machine translation tasks. In Section, we extend our machine translation experiments to demonstrate that AP is compatible with -entmax regularization~, which is another promising sparseness technique. In Section, we study the effect of AP with BERT on the GLUE benchmark. % % Section. In Section we discuss theoretically how our pruned Transformers could yield speedups in terms of MACs.  % In Section, we discuss the hardware efficiency of AP and its promise for speeding up modelling for really long sequences. In Section, we conclude and we point to promising directions for future work.  
"," The attention mechanism is a key component of the neural revolution in Natural Language Processing . As the size of attention-based models has been scaling with the available computational resources, a number of pruning techniques have been developed to detect and to exploit sparseness in such models in order to make them more efficient. The majority of such efforts have focused on looking for attention patterns and then hard-coding them to achieve sparseness, or pruning the weights of the attention mechanisms based on statistical information from the training data. In this paper, we marry these two lines of research by proposing Attention Pruning : a novel pruning framework that collects observations about the attention patterns in a fixed dataset and then induces a global sparseness mask for the model. Through attention pruning, we find that about 90\% of the attention computation can be reduced for language modelling and about 50\% for machine translation and %natural language inference \rev{prediction with BERT on GLUE tasks}, while maintaining the quality of  the results. Additionally, using our method, we discovered important distinctions between self- and cross-attention patterns, which could guide future NLP research in attention-based modelling. Our approach could help develop better models for existing or for new NLP applications, and generally for any model that relies on attention mechanisms. Our implementation and instructions to reproduce the experiments are available at \url{https://github.com/irugina/AP}.",314
" DEEP learning  is a modern machine learning technique based on artificial neural networks. The field of natural language processing  has significantly benefited from the use of deep learning techniques in recent years . There are three prevalent deep learning architectures concerned with  NLP tasks: long-short term memory   %networks , transformer networks  and convolutional neural networks  . LSTMs exhibit relatively slow inference speeds and are less performant than transformers and CNNs with regards to text classification accuracy . Transformers are a recent innovation and have shown significant successes in many NLP tasks . Their massive complexity with trainable parameters in the order of hundreds of millions presents critical experiment reproducibility challenges to researchers. State-of-the-art transformers are difficult to reproduce in lab conditions as they have a high training cost in monetary terms. There are only a limited number of pre-trained transformer models available for different languages. \par CNNs have demonstrated excellent success in text classification tasks . There are two paradigms available when using CNNs for text classification tasks, namely: world-level   and character-level CNNs . \par Word-level approaches are dependant on a word-model to represent the text. The reliance on a pre-trained word-model poses the potential problem of not having one available for a particular language. Training new word models is computationally time-consuming and costly. There is also the technical challenges of dealing with misspellings and words that may not exist in the word-model. The other paradigm is char-CNNs. No pre-trained language or word models are required. They also do not require a costly pre-processing step of the text data. In general, char-CNNs are not as accurate as word-level CNNs or transformers. Adding depth has not given the benefit of improved classification accuracy, as seen in image classification tasks. There is an open question in the research literature of what is the optimal architecture for char-CNNs. Little research has been performed to address these limitations. Deep learning is an iterative process requiring the tuning of many hyper-parameters and repeated experiments to test the efficacy of any potential architecture. It is a time consuming, costly and a tedious process that requires expert skills and domain knowledge. The task of finding optimal char-CNNs is an NP-hard problem. \par Evolutionary computation   is a collection of search algorithms inspired by the principals of biological evolution, in particular the concept of survival of the fittest. EC methods use a population of individuals  to conduct a simultaneous search during a limited time frame to improve the optimisation of a specified objective function via the exchange of information between individuals in the population. The exchange of information is one of the key motivating factors of selecting EC methods for evolving char-CNNs in this work. There is the potential that this information exchange may reveal the essential characteristics of what makes a non-performant char-CNN into a performant one. EC methods are concerned with locating near-optimal solutions to NP-hard problems. \par Evolutionary deep learning  is the technique of using EC methods to search for candidate CNN architectures combined with the backpropagation algorithm to train any potential candidate network architecture. EDL has demonstrated success when searching for performant CNN architectures on image classification tasks . EDL has not been used to search for performant char-CNN architectures. \par Motivated by the success of applying EDL techniques in the image classification domain, we propose a novel surrogate-based EDL algorithm appropriate for searching the landscape of char-CNN architectures for the text classification domain. The proposed algorithm is based on genetic programming  and an indirect encoding that is capable of representing novel char-CNN  architectures. The algorithm employs the use of surrogate models to significantly reduce the training time of the candidate char-CNNs during the evolutionary process.  In summary, the contributions of the proposed algorithm and work are:  %------------------------------------------------------------------------------ 
"," Character-level convolutional neural networks  require no knowledge of the semantic or syntactic structure of the language they classify. This property simplifies its implementation but reduces its classification accuracy. Increasing the depth of char-CNN architectures does not result in breakthrough accuracy improvements. Research has not established which char-CNN architectures are optimal for text classification tasks. Manually designing and training char-CNNs is an iterative and time-consuming process that requires expert domain knowledge. Evolutionary deep learning  techniques, including surrogate-based versions, have demonstrated success in automatically searching for performant CNN architectures for image analysis tasks. Researchers have not applied EDL techniques to search the architecture space of char-CNNs for text classification tasks. This article demonstrates the first work in evolving char-CNN architectures using a novel EDL algorithm based on genetic programming, an indirect encoding and surrogate models, to search for performant char-CNN architectures automatically. The algorithm is evaluated on eight text classification datasets and benchmarked against five manually designed CNN architectures and one long short-term memory  architecture. Experiment results indicate that the algorithm can evolve architectures that outperform the LSTM in terms of classification accuracy and five of the manually designed CNN architectures in terms of classification accuracy and parameter count.",315
" .     %     % % final paper: en-us version     %     %   % space normally used by the marker     % This work is licensed under a Creative Commons     % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. } Pre-trained language models have received great interest in the natural language processing  community in the last recent years . These models are trained in a semi-supervised fashion to learn a general language model, for example, by predicting the next word of a sentence . Then, transfer learning  can be used to leverage the learned knowledge for a down-stream task, such as text-classification .  \citet{devlin_bert:_2019} introduced the  ``Bidirectional Encoder Representations from Transformers'' , a pre-trained language model based on the Transformer architecture . BERT is a deeply bidirectional model that was pre-trained using a huge amount of text with a masked language model objective where the goal is to predict randomly masked words from their context . The fact is, BERT has achieved state of the art results on the ``General Language Understanding Evaluation''  benchmark  by only training a single, task-specific layer at the output and fine-tuning the base model for each task. Furthermore, BERT demonstrated its applicability to many other natural language tasks since then including but not limited to sentiment analysis , relation extraction  and word sense disambiguation , as well as its adaptability to languages other than English . However, the fine-tuning data set often contains thousands of labeled data points. This plethora of training data is often not available in real world scenarios .  In this paper, we focus on the low-resource setting with less than 1,000 training data points. Our research attempts to answer the question if pool-based active learning can be used to increase the performance of a text classifier based on a Transformer architecture such as BERT. That leads to the next question: How can layer freezing techniques , i.e. reducing the parameter space, impact model training convergence with fewer data points?  To answer these questions, we explore the use of recently introduced Bayesian approximations of model uncertainty  for data selection that potentially leads to faster convergence during fine-tuning by only introducing new data points that maximize the knowledge gain of the model. To the best of our knowledge, the work presented in this paper is the first demonstration of combining modern transfer learning using pre-trained Transformer-based language model such as the BERT model with active learning to improve performance in low-resource scenarios. Furthermore, we explore the effect of trainable parameters reduction on model performance and training stability by analyzing the layer-wise change of model parameters to reason about the selection of layers excluded from training. %Furthermore, we explore whether a more sophisticated decoder architecture, i.e. convolutional neural networks  can improve the overall performance or if the added complexity hinders a fast model adaption with such little training data.  The main findings of our work are summarized as follows: a) we found that the model's classification uncertainty on unseen data can be approximated by using Bayesian approximations and therefore, used to efficiently select data for manual labeling in an active learning setting; b) by analyzing layer-wise change of model parameters, we found that the active learning strategy specifically selects data points that train the first and thus more general natural language understanding layers of the BERT model rather than the later and thus more task-specific layers.  
","     Recently, leveraging pre-trained Transformer based language models in down stream, task specific models has advanced state of the art results in natural language understanding tasks. However, only a little research has explored the suitability of this approach in low resource settings with less than 1,000 training data points. In this work, we explore fine-tuning methods of BERT - a pre-trained Transformer based language model - by utilizing pool-based active learning to speed up training while keeping the cost of labeling new data constant. Our experimental results on the GLUE data set show an advantage in model performance by maximizing the approximate knowledge gain of the model when querying from the pool of unlabeled data. Finally, we demonstrate and analyze the benefits of freezing layers of the language model during fine-tuning to reduce the number of trainable parameters, making it more suitable for low-resource settings.",316
"   % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  .     %      % % final paper: en-us version      %       % space normally used by the marker     This work is licensed under a Creative Commons      Attribution 4.0 International License.     License details:     \url{http://creativecommons.org/licenses/by/4.0/}. }  Multilingual relation extraction is an important problem in NLP, facilitating a diverse set of downstream tasks from the autopopulation of knowledge graphs  to question answering . While early efforts in relation extraction used supervised methods that rely on a fixed set of predetermined relations, research has since shifted to the identification of arbitrary unseen relations in any language. In this paper, we present a method for extracting high quality relation training examples from date-marked news articles. This technique leverages the predictable distributional structure of such articles to build a corpus that is denoised . We use this corpus to learn general purpose relation representations and evaluate their quality on few-shot and standard relation extraction benchmarks in English and Spanish with little to no task-specific fine-tuning, achieving comparable results to a significantly more data-intensive approach that is the current state-of-the-art.  The current state-of-the-art model, ``Matching the Blanks"" or MTB, is a distant supervision technique that provides large gains on many relation extraction benchmarks and builds on Harris' distributional hypothesis and its extensions. ~ assume that the informational redundancy of very large text corpora  results in sentences that contain the same pair of entities generally expressing the same relation. Thus, an encoder trained to collocate such sentences can be used to identify the relation between entities in any sentence  by finding the labeled relation example whose embedding is closest to . While~ achieve state-of-the-art on FewRel and SemEval 2010 Task 8, their approach relies on a huge amount of data, making it difficult to retrain in English or any other language with standard computational resources: they fine-tune BERT large, which has mil parameters, on mil+ relation pair statements with a batch size of  for mil steps. In contrast our method, with only  relations statements and a language-model one-third the size, achieves comparable performance when fine-tuned on little to no task-specific data.   Our main contribution is a distant supervision approach in which we assume that sections of news corpora exhibit even more informational redundancy than Wikipedia. Specifically, news in the days following an event  frequently re-summarizes the event before adding new details. As a result, news exhibits a strong form of local consistency over short rolling time windows where otherwise fluid relations between entities remain fixed. For example, the relation between Italy and France as expressed in a random piece of text is dynamic and context-dependent, spanning a wide range of possibilities that include ``enemies"", ``neighbors"" and ``allies"".  But, in the news coverage following the 2006 World Cup, it is static -- they are sporting competitors. Therefore, by considering only sentences around specific events, we extract groups of statements that express the same relation and are relatively free of noise .    Training multilingual BERT  on our denoised corpus yields relation representations that adapt well to resource-constrained downstream tasks: we evaluate their quality on FewRel and SemEval 2010 Task 8, producing near state-of-the-art results when finetuned on little to no task-specific data. In addition to the strong performance of our approach in English, it is easily generalizable to other languages, requiring only news corpora and event descriptions from Wikipedia to build a high-quality training corpus. We evaluate this in Spanish and find our method outperforms mBERT on the TAC KBP 2016 relation corpus. We share our code to allow other researchers to apply our approach to news corpora of their own.    
"," General purpose relation extraction has recently seen considerable gains in part due to a massively data-intensive distant supervision technique from that produces state-of-the-art results across many benchmarks. In this work, we present a methodology for collecting high quality training data for relation extraction from unlabeled text that achieves a near-recreation of their zero-shot and few-shot results at a fraction of the training cost. Our approach exploits the predictable distributional structure of date-marked news articles to build a denoised corpus -- the extraction process filters out low quality examples. We show that a smaller multilingual encoder trained on this corpus performs comparably to the current state-of-the-art  on few-shot and standard relation benchmarks in English and Spanish despite using many fewer examples .",317
" Domain shift is common in language applications. One is more likely to find ""internet"" or ""PC"" in reviews on electronics than those on books, while he or she is more likely to find ""writing"" or ""B.C."" in reviews on books than those on electronics. This proposes a fundamental challenge to NLP in that many computational models fail to maintain comparable level of performance across domains. Formally, a distribution shift happens when a model is trained on data from one distribution , but the goal is to make good predictions on some other distribution  that shares the label space with the source.   We study unsupervised domain adaptation in this work, where we have fully-labeled data on source domain but no labeled data on target domain. The most prevailing methods in this field aim to learn domain-invariant feature by aligning the source and target domains in the feature space. The pioneering works in this field try to bridge domain gap with discrepancy-based approach.  first introduce MMD to measure domain discrepancy in feature space and use its variant MK-MMD as an objective to minimize domain shift. Another line of work introduces a domain classifier and adversarial training to induce domain invariant feature, followed by works using generative models to enhance adversarial training. However, note that both MMD-based approach and adversarial training formulates with a minimax optimization procedure that is widely known as hard to converge to a satisfactory local optimum. Moreover, some recent works have discovered that both of them don't guarantee good adaptation and will introduce inevitable error on target domain under label distribution shift because they may render incorrect distribution matching. For example, thinking of a binary classification task, the source domain has 50\% of positive samples and 50\% of negative samples while the target domain has 30\% postive and 70\% negative. Successfully aligning these distributions in representation space requires the classifier to predict the same fraction of positive and negative on source and target. If one achieves 100\% accuracy on the source, then target accuracy will be at most 80\%, that is 20\% error at best.   % Self-supervised learning is prominent in feature representation learning. Recent works have approached unsupervised domain adaptation for computer vision with SSL[][]. [] adopted rotation prediction, flip prediction and patch location prediction to induce domain-invarint feature and find that some auxiliary tasks involving fine-grained semantics like pixel reconstruction may force the model to focus on domain-specific feature, further widening the domain gap.   Self-supervised representation learning could be a good workaround for this problem because it enforces predictive behaviour matching instead of distribution matching. The main idea is to learn discriminative representation that is able to genenralize across domains.  use sentiment-indicating pivot prediction as their auxiliary task for cross-domain sentiment analysis. The method proposed in this paper adopts contrastive learning to extract generalizable discriminative feature. Contrastive learning is a subclass of self-supervised learning that is gaining popularity thanks to recent progress. It utilizes positive and negative samples to form contrast against the queried sample on pretext tasks in order to learn meaningful representations. However, the pretext tasks must be carefully chosen. shows with experiments on computer vision tasks that the transfer performance will suffer under improper pretext tasks like pixel reconstruction.  % Recent developments in contrastive learning obtained promising results both on representation learning benchmarks for CV[][][] and for NLP[][][]. % Like with self-supervised learning[], joint learning of pretext tasks in contrastive learning is able to align domain in the feature space, as illustrated in figure.  There are a group of works adopting it for domain adaptation for CV[][][]. However, these method cannot be easily adopted to NLP due to the inherent signal difference between the to domains. .   Therefore, in this paper we explore two classic data augmentation methods in natural language processing閳ユ敃ynonym substitution and back translation to define our pretext task. Experiments on two cross-domain sentiment classification benchmarks show the efficacy of the proposed method. We also examine whether in-domain contrastive learning and entropy minimization helps cross-domain sentiment classification under varied label distribution settings. Our main contributions in this work  are summarized as follows:    
","   Contrastive learning  has been successful as a powerful representation learning method. In this paper, we propose a contrastive learning framework for cross-domain sentiment classification. We aim to induce domain invariant optimal classifiers rather than distribution matching. To this end, we introduce in-domain contrastive learning and entropy minimization. Also, we find through ablation studies that these two techniques behaviour differently in case of large label distribution shift and conclude that the best practice is to choose one of them adaptively according to label distribution shift. The new state-of-the-art results our model achieves on standard benchmarks show the efficacy of the proposed method.",318
"  Data augmentation is a widely-used technique in classification tasks. In the field of computer vision , data is augmented by flipping, cropping, tilting, and altering RGB channels of the original images~; however, similar intuitive and simple strategies do not obtain equal success in NLP tasks. Existing methods tend to produce augmentation with low readability or unsatisfying semantic consistency~.                 & So Cute! is The baby very!                                                           \\ \midrule  &  \\ \midrule  & \underline{Cute}! The baby is very \underline{cute}!                                                         \\ \midrule Data Boost                                                             &  \\ \bottomrule \end{tabular}% }    \end{table}  Table shows some output samples of popular text augmentation methods. Naive methods imitate pixel manipulation in CV, augmenting sentences by adding spelling errors~, or randomly deleting and swapping tokens~. The output of such augmentation methods are often illegible since the word order is disrupted ; even worse, crucial feature words  could be mistakenly removed through random deletion. A more advanced method is synonym insertion or replacement~, which uses Word2Vec~ to replace words with their synonyms. Such a method respects the original sentence structure but fails to consider the context. It sometimes replaces words with synonyms that are awkward in the full context of the sentence. For example, replacing lovely with fabulous to get the sentence  ``The baby is fabulous!"". Recent work leans towards translation-based methods for augmentation~. In particular, \citet{yu2018qanet} proposed a back-translation method that first translates the text to French and then translates back to English, using the noisy output as the augmentation data. Although back-translation is intuitive and valid, its generation skews towards high frequency words , which not only causes repetition but also leads to lexical shrinkage in the augmented data. In a nutshell, existing techniques  are still far from perfect, partially due to the strong interdependency of syntactic and semantic features in text data.   In recent years, we have witnessed extensive progress in language models . Large-scale LMs such as BERT~, XLNet~, and GPT-2~, are commonly trained on large amounts of text data . One of the most interesting usages of these models is utilizing them as text generators~. In this paper, we explore whether we can leverage the generation ability of the state-of-the-art LMs, to generate augmented samples for a given target class.   Augmentation samples should exhibit features of the target class. Off-the-shelf LMs cannot be directly used to augment data; since they are not trained for specific contexts, their generation is undirected and random. Conditional LMs can generate text directed by certain condition , but they require training a LM from scratch with data covering all the conditions. \citet{keskar2019ctrl}, for instance, trained a 1.6 billion-parameter LM conditioned to a variety of control codes. The training is rather costly; however, collecting sufficient data for the training is also tedious, especially in low-resource tasks~.  We thus present Data Boost: a  reinforcement learning guided text data augmentation framework built on off-the-shelf LM . Data Boost requires neither collecting extra data nor training a task-specific LM from scratch. We convert GPT-2 into a conditional generator, and for a given task, we guide the generator towards specific class labels during its decoding stage through reinforcement learning. The generated samples can then serve as augmentation data which are similar to the original data in terms of semantics and readability.    The advantages of Data Boost are three-fold: First, Data Boost is powerful. We achieve significant advances in three tasks on five different classifiers compared with six related works. Second, Data Boost generates sentence-level augmentation. Unlike prior methods that do word-level or phrase-level replacement~, our augmented data is of much greater variety in terms of vocabulary and sentence structure. Human evaluations also verify the high readability and label consistency of our augmentation. Third, Data Boost is easy to deploy. It does not require external datasets or training separate systems . Instead, we take the off-the-shelf GPT-2 language model and modify its decoding stage without changing its architecture.    
"," Data augmentation is proven to be effective in many NLU tasks, especially for those suffering from data scarcity. In this paper, we present a powerful and easy to deploy text augmentation framework, Data Boost, which augments data through reinforcement learning guided conditional generation. We evaluate Data Boost on three diverse text classification tasks under five different classifier architectures. The result shows that Data Boost can boost the performance of classifiers especially in low-resource data scenarios. For instance, Data Boost improves F1 for the three tasks by 8.7\% on average when given only 10\% of the whole data for training. We also compare Data Boost with six prior text augmentation methods. Through human evaluations , we confirm that Data Boost augmentation has comparable quality as the original data with respect to readability and class consistency.",319
"   %Recently, Neural machine translation~ has achieved great success and reached satisfactory translation performances for several language pairs~. %These NMT models are sequence-to-sequence models trained on large parallel data.  % Ensemble learning, which aggregates multiple diverse models during inference, has attracted huge interest in both academia and industry communities thanks to its effectiveness in a variety of computational intelligence problems such as classification, prediction and function approximation. So far, many aggregating approaches have been developed such as bagging and boosting to improve the practical performance.  % % Ensemble learning is primarily used to improve the classification task or reduce the likelihood of a poorly learned model.  % Recently, ensemble of different neural networks  has greatly improved the accuracy of neural machine translation , making it a vital widely used technique in state-of-the-art Neural NMT systems. In the scenario of NMT, a common implementation is to average the probability of each token computed by different individual models and then decode with the averaged probabilities. Previous studies show that the performance of ensemble method heavily depends on both the accuracy and diversity of base models, which are typically obtained through independent training on different sets of attributes.  % % Ensemble learning, which aggregates multiple models during inference, is an   % Despite its success in various tasks and applications, in practice there are a few common challenges of ensemble methods, which prevent its wide usage: 1) High computational cost. For ensemble learning, all individual models have to conduct encoding and decoding, which is prohibitively time and memory consuming. It gets even worse in the context of NMT due to the large size of state-of-the-art networks like transformer. 2) Absence of monolingual data. Ensemble exploit the independence cannot make full use of the large scale monolingual data from source side.  Recently, self-training method has shown remarkable success in image recognition.  % Taking advantage of unlabeled data, Trained on noisy augmented data, an EfficientNet model finetuned with self-training can achieve 87.4\% top-1 accuracy on ImageNet, which is 1.0\% better than the state-of-the-art model that requires 3.5B weakly labeled images. Typically, in self-training we first train a base model on the labeled data, and then utilize the learned model to label unannotated data. Finally, both labeled and pseudo data are combined as the training set to yield the next level model. In the context of natural language processing, many works have successfully applied self-training technique including word sense disambiguation and parsing. %  Nevertheless, the performance gains achieved through self-training are still limited for structured prediction tasks such as Neural Machine Translation~ where the target space is vast. Originally designed for classification problems, previous work suggests that self-training can be effective only when the predictions on unlabeled samples are good enough, and otherwise it will suffer from the notorious reinforced mistakes. However, this problem is common in NMT scenario, where the hypotheses generated from a single model are often far away from the ground-truth target due to the compositionality of the target space. \citet{zhang2016exploiting} found that training on this biased pseudo data may accumulate the mistakes at each time step and enlarge the error, and thus they propose to freeze the decoder parameters when training on the pseudo parallel data which may negatively impact the decoder model of NMT.  % We argue that the performance drop of self-training for NMT mainly comes from the reinforced mistakes.  To overcome this issue, in this paper we borrow the reciprocal teaching concept from the educational field and revisit the core idea of classic ensemble approaches. Ensemble is built upon the assumption that different models have different inductive biases and better predictions can be made by majority voting. We propose to replace the self-supervision with Reciprocal-Supervision in NMT, leading to a novel co-EM  scheme named \method. In \method, we use multiple separately learned models to provide diverse proper pseudo data, allowing us to enjoy the independence between different models and dramatically reduce the error through strategic aggregation. %Most of these NMT works use only one type of neural network model such as ConvS2S~ and Transformer~. %Usually, different neural models have different performances and they may also catch minor different patterns in the sequences. More specifically, we first learn multiple different models on the parallel data. Then in the E-step all individual models are used to translate the monolingual data. And in the M-step the generated pseudo data produced by different models are combined to tune all student models. %To combine these advantages and diversities, the intuitive method is ensemble, in which several models are trained and every model will be used during inference, then the output of these models are combined for a better prediction.  \method is inspired by the success of ensemble method. However, ensemble is resource-demanding during inference, which prevents its wide usage. Besides, it cannot make use of the large scale monolingual data from source side. %The teacher-student framework can be used to make one model learn from others. Some works have been done to explore the assistance from right-to-left decoding model to usual left-to-right model~. These works shown that a regular NMT model can learn from a right-to-left decoding model and obtain better performance. However, to our best knowledge, there are no such work exploring the assistance from several different models. So in this work, we try to utilize multiple different models as teachers, and train a student model to learn from them. Through this procedure, the student model can have better performance. %Following this procedure, we have another advantage that monolingual data of source side language can be easily utilized to extend the training method to our self-training framework with diverse teachers. %Similar to teacher-student framework for zero-shot NMT~, the student model can also learn from teachers by monolingual data. \method is also related to the data augmentation approaches for NMT.  While most of previous works concentrate on monolingual data of target side such as back-translation~, we pay more attention to the source side. Knowledge distillation  is another relevant research topic. However, KD is preliminary designed to improve a weak student model with a much stronger teacher model. By contrast, \method boosts the performance of base models through reciprocal-supervision from other just comparable or even weaker learners. % Unsupervised machine translation~ can also be seen as utilizing target side monolingual data. To the best of our knowledge, we are the first self-training framework with reciprocal-supervision, which can correct the bias of each model and fully utilize the monolingual data of source side language. More precisely, the advantages of \method % our cooperative-supervised framework with diverse parameterized networks  can be summarized as follows:  Through extensive experiments, \method achieves significant gains on several standard translation tasks including En\{Ro, De\}. Surprisingly, we also have found that \method with other much weaker learners could even outperform a strong BERT enhanced NMT model with big margins.
","  % Neural machine translation~ has achieved great success with the help of large amount of parallel data. % However, different model architectures have different advantages and translation abilities, but it is hard to integrate them all together to one model. % The ensemble method is too time-consuming for inference. % Besides, monolingual data are also not fully utilized. % Some works such as back-translation and unsupervised machine translation have tried to utilize monolingual data of target side, whereas the utilization of source side monolingual data still need be further explored. % In this work, we propose a self-training framework with diverse teachers to make one model be able to learn advantages and diversities from other models, and monolingual data of source side language can also be utilized to further improve the translation performances. % This method is very simple but much effective. % Empirical results show that our method can obtain further improvements on the standard En$\to$De and En$\to$Fr translation tasks.  Despite the recent success on image classification, self-training has only achieved limited gains on structured prediction tasks such as neural machine translation . This is mainly due to the compositionality of the target space, where the far-away prediction hypotheses lead to the notorious reinforced mistake problem. In this paper, we revisit the utilization of multiple diverse models and present a simple yet effective approach named Reciprocal-Supervised Learning . \method first exploits individual models to generate pseudo parallel data, and then cooperatively trains each model on the combined synthetic corpus. \method leverages the fact that different parameterized models have different inductive biases, and better predictions can be made by jointly exploiting the agreement among each other. Unlike the previous knowledge distillation methods built upon a much stronger teacher, \method is capable of boosting the accuracy of one model by introducing other comparable or even weaker models. \method can also be viewed as a more efficient alternative to ensemble. Extensive experiments demonstrate the superior performance of \method on several benchmarks with significant margins.\footnote{Code is available at \url{https://github.com/MinkaiXu/RSL-NMT}.} % \method takes advantage of different parameterized networks to generate diverse proper pseudo parallel data, and then dramatically reduce the bias through strategic combination of the pseudo data. %More specifically, we first train several NMT teachers with heterogeneous networks, then use the heterogeneous teacher models to label unlabeled data respectively and finally use the labeled data and unlabeled data to jointly train a student NMT model. % \method is very simple but much effective. % Empirical results demonstrate the effectiveness of \method on several benchmarks, where we even outperforms a strong BERT-enhanced baseline.   % Ensemble learning, which strategically aggregates multiple models for inference, has been shown effective to improve the accuracy of Neural Machine Translation . However, in practice it cannot be widely adopted due to the high computation and memory cost for involving all individual models.  % % Recently, transductive method has been proposed to overcome this obstacle, which, however, suffers the premise that the test data has to be available in advance.  % In this paper, we present a simple yet effective approach named Cooperative Training NMT , where we firstly use individual models to translate the source corpus into pseudo parallel data, and then cooperatively train all models on the translated synthetic corpus. \method leverages the fact that different parameterized models have different inductive biases, and better predictions can be made by jointly exploiting the independence between each other. Furthermore, given source monolingual data, \method enables us to avoid the reinforced mistakes problem of self-training and make the most of the monolingual set. Extensive experiments demonstrate our proposed approach can always achieve superior or comparable performance on several benchmarks with less computational cost.",320
"    One of the first steps in language acquisition is to learn word--meaning mappings, e.g., the word ``dog'' in the sentence ``see the dog'' refers to the tail-wagging animal under the kitchen table. This seemingly simple problem of word learning is a complex puzzle; in the initial phases of language development, children do not have any knowledge about word meanings  and face a great deal of uncertainty.  % Without prior information, for a given word  , there is a high level of referential uncertainty -- there are a great number of potential meanings in a child's environment  that the word could refer to. % Similarly, there is high level of linguistic uncertainty in mapping a referent  to words in the utterances .  % Moreover, an additional difficulty arises because not all the mappings between words and referents are one-to-one; sometimes words are mapped to more than one referent  or referents are mapped to more than one word .   Strong empirical evidence suggests that statistical cross-situational learning helps both children and adults navigate these challenges, by gradually keeping track of statistical regularities across different situations , and using them to help resolve these ambiguous mappings \citep[\eg,][]{yu.smith.2007,smith.yu.2008}.  However, cross-situational learning does not provide a detailed account of what mechanisms are responsible for resolving each type of uncertainty at different stages of word learning.  Moreover, a large body of developmental research has studied inductive biases that might facilitate word learning in the presence of different types of uncertainty \citep[\eg,][]{markman.1987}. A common theme among these biases is that competition can remove a number of possible hypotheses for a word meaning . For example, the mutual exclusivity bias asserts that each referent is only mapped to one word .  % This competition among referents means that given a new word and a number of possible referents, a learner reduces the uncertainty by not considering referents that are already associated to other words.   It is also suggested that such competitive processes play a role both locally and globally: there is competition when associating words and meanings from one observation  as well as among all observed words and referents \citep[\eg,][]{yurovsky2013competitive}.\footnote{Work in computational modeling of cross-situational learning typically does not distinguish between the referent indicated by a word and its meaning.  We will use the terms referent and meaning interchangeably throughout this paper, while recognizing that there are important notions about the relations between those two that are being abstracted away by such an approach.}   Previous computational modeling work has shed light on the mechanisms and biases that might be involved in cross-situational learning \citep[\eg,][]{frank.etal.2007, fazly.etal.2010.cogsci,trueswell.etal.2013,nematzadeh.etal.2017.cogsci.bias}. % However, to our knowledge, no previous work has done an exhaustive analysis of the role of competition in the in-the-moment learning mechanisms and how these mechanisms interact with different representations of word meanings,  which may also be influenced by competition. % In this work, our contributions are threefold:   We provide a general probabilistic formulation of cross-situational word-learning and show that the influential model of \citet{fazly.etal.2010.csj} is an instance of this formulation.   Using this formulation, we show how  % inductive biases can be modeled as competitive processes during in-the-moment and overall word learning, as well as comprehension of a word meaning.   Moreover, we examine how each modeling choice  affects learning in the presence of different sources of uncertainty, such as increased referential and linguistic uncertainty, fewer exposures to words, or acquiring homonyms and synonyms.    We find that the best model across all the tasks is the one that implements two types of competition, both among words and referents. Moreover, this competition happens during in-the-moment learning  and comprehension . This result is different than previous modeling assumptions where a competition among referents was introduced during the overall learning of word meaning representations . It also suggests that the observed behavior in people \citep[\eg,][]{yurovsky2013competitive} might be explained by the competition during comprehension and not a global competitive process during learning. We also observe that the best model performs better than the model of \citet{fazly.etal.2010.csj} in the presence of linguistic and referential uncertainty, and can learn homonyms as opposed to their model.  
"," Children learn word meanings by tapping into the commonalities across different situations in which words are used and overcome the high level of uncertainty involved in early word learning experiences. In a set of computational studies, we show that to successfully learn word meanings in the face of uncertainty, a learner needs to use two types of competition: words competing for association to a referent when learning from an observation and referents competing for a word when the word is used.",321
"  Hypernym, sometimes also known as hyperonym, is the term in linguistics referring to a word or a phrase whose semantic field covers that of its hyponym. The most common relationship between a hypernym and a hyponym is an ``is-a'' relationship. For example, ``red is a color'' provides the relationship between ``red'' and ``color'', where ``color'' is the hypernym of ``red''.   The hypernym-hyponym relation is an essential element in the semantic network and corresponding tasks related to semantic network analysis . The hypernym graph built on a collection of hyponym-hypernym relations can enhance the accuracy of taxonomy induction . The linkage between the hyponym and the hypernym can be used to improve the performance of link prediction and network completion in the knowledge graph or semantic network . In natural language processing , the hyponym-hypernym relation can help the named entity recognition , and the question-answering tasks for ``what is'' or ``is a'' . The data mining, information search and retrieval can also benefit from the hyponym-hypernym relation .   Given the role and application of the hypernym-hyponym relation, it is essential to explore an automatic method to extract such the relation between two entities, which presents an important task in knowledge-driven NLP . Following the landmark work focusing on lexico-syntactic patterns , several pattern-based methods are developed for hypernym extraction . Then the feature-based classification methods are introduced , which applies machine learning tools to enhance the recall rate. Recently, distributional methods and hybrid distributional models are successfully applied to learn the embedding of words, based on which the hypernym-hyponym relation can be inferred . The deep learning approach is also effective in many sequence labeling tasks including hypernym extraction .    While the extraction of hyponym-hypernym relation can be done in many different environments, in this work we focus on the hypernym extraction from definitions. More specifically, the definition refers to a short statement or description of a word. Take the word ``red'' as an example, whose definition on Wikipedia  is ``Red is the color at the end of the visible spectrum of light, next to orange and opposite violet.'' The aim is to identify the word ``color'' as the hypernym of ``red'' from all the nouns in the definition. Intuitively, this task can be solved by general resources such as WordNet dictionary  or Wikipedia. But given a word's different meanings in different contexts, these resources can not sufficiently complete this task. As an example, the term ``LDA'' in Wikipedia denotes ``Linear Discriminant Analysis'' in machine learning, ``Low dose allergens'' in medicine, and ``Landing distance available'' in aviation. The combination of general resources and context identification would also fail in some domain-specific applications where the general resources do not cover the special or technical terms in that area. Moreover, existing technical approaches also demonstrate certain limitations in the task of hypernym extraction from definitions, which we summarize as follows:   To briefly illustrate the difficulty, let us consider a definition from the Stack-Overflow with an irregular format: ``fetch-api: the fetch API is an improved replacement for XHR''. The term ``fetch-api'' is not included in any common dictionary. While the definition has the ``is an'' pattern, it does not connect to the hypernym. The definition is very short and every distinct word in this definition appears just once, which makes it difficult to accurately learn the word representation. Overall, it is challenging to find a method that would accurately identify ``API'' as the correct hypernym.   The definition of a word represents a certain type of knowledge extracted and collected from disordered data. Indeed, there are tools capable of extracting definitions from the corpora with good accuracy . Nevertheless, tools to extract hypernym from definitions remain limited.  % To cope with this issue, we propose a recurrent network method using syntactic features. Because the definition directly points to a noun, the hyponym is already given. Therefore, the hypernym extraction is to identify the correct hypernym from all words in the definition sentence. This task can be considered as a binary classification, in which the classifier judges if a candidate noun is a hypernym or not. In order to better learn the syntactic feature, we transfer the definition sentence into the part of speech  sequence after labeling the PoS of each word by a standard tool . The syntactic structure surrounding the candidate is learned by a bidirectional gated recurrent units  based model. To further fine tune the results, we use a set of features including the centrality of the word in the hypernym co-occurrence network. We use two corpora to evaluate our method. One is Wikipedia, featuring definitions with canonical syntax structure and intensively used by previous studies. The other is from Stack-Overflow, whose definition is domain-specific and usually with the irregular format. Our method is compared with several existing ones. Overall, it outperforms all others in both corpora, which demonstrates the advantage of combing both the tool of RNN and the PoS information in the task of hypernym extraction.    This paper is organized as follows. We review related works in Section  and introduce details of the method in Section . Experiments and evaluations of the proposed model are presented in Section . After that, we draw a conclusion about this research in Section .   
"," % The abstract should briefly summarize the contents of the paper in % 150--250 words. The hyponym-hypernym relation is an essential element in the semantic network. Identifying the hypernym from a definition is an important task in natural language processing and semantic analysis. While a public dictionary such as WordNet works for common words, its application in domain-specific scenarios is limited. Existing tools for hypernym extraction either rely on specific semantic patterns or focus on the word representation, which all demonstrate certain limitations. Here we propose a method by combining both the syntactic structure in definitions given by the word闁炽儲鐛 part of speech, and the bidirectional gated recurrent unit network as the learning kernel. The output can be further tuned by including other features such as a word闁炽儲鐛 centrality in the hypernym co-occurrence network. The method is tested in the corpus from Wikipedia featuring definition with high regularity, and the corpus from Stack-Overflow whose definition is usually irregular. It shows enhanced performance compared with other tools in both corpora. Taken together, our work not only provides a useful tool for hypernym extraction but also gives an example of utilizing syntactic structures to learn semantic relationships \footnote{Source code and data available at \url{https://github.com/Res-Tan/Hypernym-Extraction}}.  \keywords{Hypernym Extraction \and Syntactic Structure \and Word Representation \and Part of Speech \and Gated Recurrent Units.}",322
"  Although neural machine translation  has achieved great success on sentence-level translation tasks, many studies pointed out that  translation mistakes become more noticeable at the document-level. They proved that these mistakes can be alleviated by feeding the inter-sentential contexts into context-agnostic NMT models.  Previous works have explored various methods to integrate context information into NMT models. They usually take a limited number of previous sentences as contexts and learn context-aware representations using hierarchical networks  or extra context encoders . Different from representation-based approaches, ~\citeauthor{tu2018learning}~\shortcite{tu2018learning} and ~\citeauthor{kuang-etal-2018-modeling}~\shortcite{kuang-etal-2018-modeling} propose using a cache to memorize context information, which can be either history hidden states or lexicons. To keep tracking of most recent contexts, the cache is usually updated when new translations are generated. Therefore, long-distance contexts would likely to be erased.  How to use long-distance contexts is drawing attention in recent years. Approaches, like treating the whole document as a long sentence  and using memory and hierarchical structures , are proposed to take global contexts into consideration. However, \citeauthor{kim2019and}~\shortcite{kim2019and} point out that not all the words in a document are beneficial to context integration, suggesting that it is essential for each word to focus on its own relevant context.    \footnotetext{Dependency and coreference relations are from Stanford CoreNLP .}  To address this problem, we suppose to build a document graph for a document, where each word is connected to those words which have a  direct influence on its translation. Figure  shows an example of a document graph. Explicitly, a document graph %for a document  is defined as a directed graph where:  each node represents a word in the document;  each edge represents one of the following relations between words:  adjacency;  syntactic dependency;  lexical consistency; or  coreference.   We apply a Graph Convolutional Network  on the document graph to obtain a document-level contextual representation for each word,  fed to the conventional Transformer model  by additional attention and gating mechanisms. We evaluate our model on four translation benchmarks, IWSLT English--French  and Chinese--English , Opensubtitle English--Russian , and WMT English--German . Experimental results demonstrate that our approach is consistently superior to previous works  on all the language pairs.   The contributions of this work are summarized as:    
","     Previous works have shown that contextual information can improve the performance of neural machine translation . However, most existing document-level NMT methods failed to leverage contexts beyond a few set of previous sentences. How to make use of the whole document as global contexts is still a challenge. To address this issue, we hypothesize that a document can be represented as a graph     that connects relevant contexts regardless of their distances. We employ several types of relations, including adjacency, syntactic dependency, lexical consistency, and coreference, to construct the document graph. Then, we incorporate both source and target graphs into the conventional Transformer architecture with graph convolutional networks. Experiments on various NMT benchmarks, including IWSLT English--French, Chinese-English, WMT English--German and Opensubtitle English--Russian, demonstrate that using document graphs can significantly improve the translation quality.",323
"  Automatic summarization is a fundamental task in natural language generation and computational linguistics. It is crucial to help the user quickly read and understand daily events, and has been continuously studied for decades. . In this paper, we focus on meeting summarization, which is an extensively studied task in the field of automatic summarization. Given multiple speakers and corresponding utterances in text, the task calls for generating a shorter transcript, covering salient information of the entire meeting. An example is shown in Figure , which includes 3 speakers and their utterances , and , as well as a human-written summary.  Meeting summarization is typically regarded as a kind of abstractive summarization problem in the literature. The majority of existing studies build summarization systems based on the sequence-to-sequence model, which adopts a sequence modeling strategy for encoding utterances . Despite the effectiveness of these approaches, they typically only use sequential text information while ignoring the important influences of dialogue structure. We claim that dialogue-specific structural information is important for meeting summarization. For example, dialogue discourse is an effective structural feature. As shown in Figure , ``Contrast閳, ``Question-Answer閳 and ``Continuation閳 are three dialogue discourse relations, which can provide more precise semantic relationships between each utterance. Specifically, we can see that the existing sequence modeling method is unable to generate correct summary results ), which can be attributed to the system not knowing the  and  are opposed to the 閳ユ獨 proposal. Differently, the dialogue discourse can provide this key information via labeling the 閳ユ窅ontrast閳 relationship, as shown in Figure . Accordingly, how to effectively integrate the discourse relationship into the existing summarization model become a crucial step in meeting summarization.  In this paper, we propose Dialogue Discourse-Aware Graph Convolutional Networks  to address this problem. In detail, we first convert the entire meeting with dialogue discourse labeling into a discourse graph, which represents both utterances and discourse relationships as vertices. Afterwards, we additionally design six types of directed edges and one global vertex in the discourse graph to facilitate information flow. Finally, we employ a graph convolutional network  to encode the graph and pass the semantic representation to the RNN decoder. Besides, we further use the question-answer discourse relationship to construct a pseudo-summarization corpus for pre-training DDA-GCN. In a conversation, a question often sparks a discussion, so naturally, the question can be used as a pseudo-summary for subsequent discussions.  We conduct experiments on the widely used AMI benchmark . Our approach outperforms various baselines. Moreover, we analyze the effectiveness of dialogue discourse and pseudo-summarization corpus. In the end, we give a brief summary of our contributions:  To the best of our knowledge, we are the first to apply dialogue discourse to model the structure of a meeting for meeting summarization;  We design a discourse-aware graph model to encode the entire meeting;  Our model achieves a new SOTA on the AMI dataset.    
"," Sequence-to-sequence methods have achieved promising results for textual abstractive meeting summarization. Different from documents like news and scientific papers, a meeting is naturally full of dialogue-specific structural information. However, previous works model a meeting in a sequential manner, while ignoring the rich structural information. In this paper, we develop a Dialogue Discourse-Aware Graph Convolutional Networks  for meeting summarization by utilizing dialogue discourse, which is a dialogue-specific structure that can provide pre-defined semantic relationships between each utterance. We first transform the entire meeting text with dialogue discourse relations into a discourse graph and then use DDA-GCN to encode the semantic representation of the graph. Finally, we employ a Recurrent Neural Network to generate the summary. In addition, we utilize the question-answer discourse relation to construct a pseudo-summarization corpus, which can be used to pre-train our model. Experimental results on the AMI dataset show that our model outperforms various baselines and can achieve state-of-the-art performance.",324
"  Pre-trained language models such as BERT or RoBERTa learn contextualized word representations on large-scale text corpus through self-supervised learning, and obtain new state-of-the-art results on many downstream NLP tasks . Recently, researchers have observed that pre-trained language models can internalize real-word knowledge into their model parameters. For example, pre-trained language models are able to answer the questions such as ``the sky is }'' or ``Beethoven was born in }'' with moderate accuracy. To further explore their potential, researchers have proposed various approaches to guide the pre-training of the language models by injecting different forms of knowledge into them, such as structured knowledge graph or linguistic knowledge  .      	 \end{table*}  Table lists some of the previous knowledge-guided pre-trained language models with their training methods. We group them into two categories: generative tasks and discriminative tasks. Generative tasks are often formulated as predicting the masked tokens given the context. By particularly masking out the words that contain certain types of knowledge  in generative pre-training, the model can be more adept in memorizing and completing such knowledge. While discriminative tasks are often formulated as a classification problem with respect to the sentence or the tokens. By training on the positive and negative examples constructed according to the external knowledge, the discriminator can be more capable of verifying the true or false knowledge in natural language. Existing research has demonstrated that generative and discriminative training have their advantages: the former has a large negative sample space so that the model can learn fine-grained knowledge, while the latter avoids the ``'' tokens in pre-training, and is therefore more consistent with fine-tuning. On the other hand, generative and discriminative capture the different aspects of data distribution and could be complementary to each other in knowledge consolidation. However, to the best of our knowledge, there is not previous work in combining the two approaches in a systematic way. Inspired by the recent success on the generative-discriminative pre-trained model named ELECTRA, we propose to learn the generator and discriminator jointly in the knowledge-guided pre-training, which we call the KgPLM model.  In this paper, we design masked span prediction as the generative knowledge completion task, and span replacement checking as the discriminative knowledge verification task. Hybrid knowledge, including link structure of Wikipedia and structured knowledge graph in Wikidata, is used to guide the both tasks. The spans covering the factual knowledge are more likely to be selected for masking or replacement, and the choices of their replacements are also related to the proximity to the original span in the knowledge space. Figure shows an example of the span masking and replacement tasks. To further explore effective ways to the joint training of the two tasks, we design two learning schemes, which we called two-tower scheme and pipeline scheme. Basically, the generator and discriminator are trained in parallel with the shared parameters in the two-tower scheme. While in the pipeline scheme, the output of generator is input to the successive discriminative training. The generator and discriminator in our KgPLM model are both pre-trained based on RoBERTa. They have some additional benefits: 1) the model can be readily extended to much larger pre-training corpus, which keeps some potential room for further improvement; 2) the model retains the same amount of parameters as RoBERTa, and does not require any modifications in fine-tuning for the downstream tasks.  We evaluate the model performance on LAMA~, which consists of several zero-shot knowledge completion tasks, and MRQA shared tasks~, which include several benchmark question answering datasets. The experiments show the proposed KgPLM, especially that trained with the pipeline scheme, achieves the state-of-the-art performance, and significantly outperform several strong baselines  on some of the tasks. The results indicate that the knowledge-guided generative and discriminative pre-training provides an effective way to incorporate external knowledge and achieve competitive performance on the knowledge intensive NLP tasks.  
"," Recent studies on pre-trained language models have demonstrated their ability to capture factual knowledge and applications in knowledge-aware downstream tasks. In this work, we present a language model pre-training framework guided by factual knowledge completion and verification, and use the generative and discriminative approaches cooperatively to learn the model. Particularly, we investigate two learning schemes, named two-tower scheme and pipeline scheme, in training the generator and discriminator with shared parameter. Experimental results on LAMA, a set of zero-shot cloze-style question answering tasks, show that our model contains richer factual knowledge than the conventional pre-trained language models. Furthermore, when fine-tuned and evaluated on the MRQA shared tasks which consists of several machine reading comprehension datasets, our model achieves the state-of-the-art performance, and gains large improvements on NewsQA  and TriviaQA  over RoBERTa.",325
" 	 	Knowledge graphs , such as WordNet , Freebase  and Wikidata , aggregate a large amount of human knowledge and express in a structured way. 	% are representative of existing KGs, in which knowledge is formalized as triples. 	%, such as  where  is the head entity,  is the tail entity, and  is the relation between these two entities. 	The large number of triples in these KGs have constructed a complex knowledge network, but it is far from complete. 	In recent years, knowledge graph completion  tasks have attracted great attention. 	 	 	 	Despite new state-of-the-art  models  emerge constently, most methods ignore the topological structure information of the KGs.  	Relation paths are the most common topological structure in KGs, and Figure shows some relation path instances.  	 is a relation triple, while  is a two-step relation path. 	Similar to word context in language models , relation paths can be considered as one kind of contextual information in KGs. 	We call it ``graph contextual information''. 	And Harris's famous distributional hypothesis   can also be extend to knowledge graphs: you shall know an entity by the relationships it involves. 	Although these two kinds of contextual information are similar, the latter has its own specialities. 	In knowledge graphs, not all relation paths are meaningful. 	For example,  is a valid relation path, but this does not indicate that there must be a relationship between  and . 	Unreliable relation paths are common in knowledge graphs, and  \citet{lin2015modeling} found that it is necessary to select reliable relation paths for knowledge representation learning. 	%In this work, a path-constraint resource allocation algorithm is proposed to measure the weights of inference patterns. 	They learn inference patterns between relations and paths to utilize knowledge contained in relation paths. 	%Despite its success, the modeling objects are more limited to the inference patterns between relations and paths. 	%Recently, \citeauthor{wang2019coke} \shortcite{wang2019coke} propose a method to model the contextual nature of triples and relation paths, and they explore the benefits of graph contextual information for link prediction tasks on two specific datasets. 	%However, simply adding graph contextual information  into the training pool is not always effective, and this operation may reduce the performance of the original model. 	Instead of relying on inference patterns, we propose PPKE, a path-based pre-training approach that integrates  graph contextual information contained in relation paths into the model parameters. 	We think this is a more general way to develop the unexploited graph contextual information. 	During the path-based pre-training procedure,  two-step relation paths are extracted from the knowledge graph and fed into the pre-training module with original triples. 	Then, the pre-trained model can be finetuned for downstream KGC tasks, such as link prediction and relation prediction. 	Our contributions are as follows: 	  
"," 		Entities may have complex interactions in a knowledge graph , such as multi-step relationships, which can be viewed as graph contextual information of the entities. 		Traditional knowledge representation learning  methods usually treat a single triple as a training unit, and neglect most of the graph contextual information exists in the topological structure of KGs. 		In this study, we propose a Path-based Pre-training model to learn Knowledge Embeddings, called PPKE, which aims to integrate more graph contextual information between entities into the KRL model. 		Experiments demonstrate that our model achieves state-of-the-art results on several benchmark datasets for link prediction and relation prediction tasks, indicating that our model provides a feasible way to take advantage of graph contextual information in KGs.",326
"  Machine reading comprehension  is a challenging natural language understanding task which lets the machine predict appropriate answer to the question according to a given passage or document .  According to answer styles, MRC tasks can be roughly divided into generative , extractive  and multi-choice  tasks . The multi-choice task is the focus of this work.  Recently, various datasets and tasks have been proposed, promoting a rapid improvement of MRC techniques . Early MRC datasets usually provide passages whose contents are extracted from articles . Recently, conversational reading comprehension has aroused great interests whose passages are derived from multi-turn dialogue segments , making the task be more challenging.    The popular practice to solve MRC problems is adopting pre-trained language models  as encoder module . Instead of better exploiting pre-trained LMs, this paper is motivated by human reading strategies to decouples MRC into sketchy reading by extracting the critical spans from the passage, and extensive reading by seeking external knowledge.  As a result, we propose a knowledge enhancement model based on extracted critical information called RekNet . In detail, the proposed RekNet refines the fine-grained critical information by a span extraction model and defines it as Reference Span, then quotes relevant external knowledge in the form of quadruples by the co-occurrence information of Reference Span and answer options. An example process of RekNet is shown in Figure .   In summary, our main contributions are follows:\\ 1) We propose a novel reference-based knowledge enhancement model RekNet, which makes the first attempt to obtain fine-grained evidence for inference and knowledge retrieving on MRC tasks.\\ 2) RekNet uses novel knowledge quadruples to quote relevant and credible knowledge.\\ 3) RekNet is applied to two multi-choice MRC benchmarks, RACE  and DREAM  and improves the performance of baseline models by 1.0\% and 1.1\% respectively, which both pass the significance test of MRC tasks.  
"," Multi-choice Machine Reading Comprehension  is a major and challenging form of MRC tasks that requires model to select the most appropriate answer from a set of candidates given passage and question. Most of the existing researches focus on the modeling of the task datasets without explicitly referring to external fine-grained commonsense sources, which is a well-known challenge in multi-choice tasks. Thus we propose a novel reference-based knowledge enhancement model based on span extraction called 	extbf{Reference Knowledgeable Network }, which simulates human reading strategy to refine critical information from the passage and quote external knowledge in necessity. In detail, RekNet refines fine-grained critical information and defines it as Reference Span, then quotes external knowledge quadruples by the co-occurrence information of Reference Span and answer options. Our proposed method is evaluated on two multi-choice MRC benchmarks: RACE and DREAM, which shows remarkable performance improvement with observable statistical significance level over strong baselines.",327
"   Data collection is an essential part of the field of spoken dialogue systems and conversational AI. %, and requires developers to make difficult decisions and budget accordingly.   In particular, designing a dialogue system for a completely new domain is still a very challenging task.  Data collection options include running lab-based experiments, crowd-sourced tasks  or gathering data from social media platforms, such as Reddit or Twitter. Ambitious large scale data collections across multiple domains have resulted in widely used datasets, such as MultiWOZ . % and collected from various platforms .% to create representations of dialogues in the vector space.   However, starting off in a new domain from scratch still has its challenges. Difficult and costly decisions have to be made as to how and where to collect the data.    A large majority of recent dialogue corpora has been collected using crowd-sourcing either by pairing workers and letting them chat, often about a given topic , or by asking them to add the next utterance to the dialogue given a set of conditions . Other studies have recruited subjects to play the role of the system, i.e., to act as a wizard or user . Each of these approaches has its own advantages and disadvantages, depending on if the dialogue is task-oriented or not. By letting users type in an unrestricted way, the richness of the dialogue increases, which is a positive feature for chit-chat. On the other hand, too much variability could be a problem for a high stakes, task-oriented dialogues, such as in the medical domain. Letting multiple users contribute with one utterance per dialogue , speeds up the data collection, however, dialogues may lack coherence and severely diverge from real dialogues. On the other hand, hiring and training subjects to chat or perform the wizard role results in a more controlled data collection but dramatically increases the cost of the data collection and makes it less scalable.     The quality of such datasets has been often assessed according to the degree of variability  observed  or the lexical complexity of the utterances collected . %, however to the best of our knowledge, there is no work assessing the impact of the different methods directly on training dialogue models.   %This paper aims at addressing this issue by investigating the impact of two different data collection methods on the performance of the model. Furthermore, most of the above-mentioned datasets focus on increasing the size of the dataset available for dialogue research, rather than investigating the impact of the data collection strategies on the performance of the models trained. The work presented in this paper aims at highlighting the pros and cons, using a methodology to quickly leverage a robust dialogue system, minimising the cost and effort involved in the data collection process. Analyses comparing different strategies for the data collection process across various platforms have been done in the past , but we are not aware of a similar study for dialogue data.  The data used in this study was collected in the scope of an emergency response system to be used on an off-shore energy platform as part of the EPSRC ORCA Hub programme . One of the collections was done using crowd-sourcing  and the second one was done in a lab using a Wizard-of-Oz setting, where participants were interacting either with a social robot or a smart speaker. Both datasets were used to train a dialogue model using an implementation of a Hybrid Code Network  and here we compare the results achieved by models trained on data collected by either method. To validate the use of crowd-sourced data to bootstrap a dialogue system for situated interaction, we ran experiments where we train the model on the crowd-sourced data and test it on the lab data, in order to verify if it %This will result in an estimate of the number of dialogues needed to  %varied the amount of crowd-sourced dialogues during training to estimate the necessary amount of crowd-sourced data needed to  achieves comparable performances with the models trained only with the lab data.   The contributions of this paper are as follows: 1) a comparison of models trained with two datasets collected in different ways but on the same task, 2) evidence that suggests that specialised dialogue tasks, such as our emergency response task, are not well covered by current pre-trained dialogue models, and 3) a set of recommendations regarding the data collection for dialogue research.\footnote{Please find code and data in: \href{https://github.com/zedavid/TheLabVsTheCrowd}{}.}  The paper is organised as follows. Section  will cover previous work related to this problem. Our experimental set-up will be introduced in Section , followed by the results in Section . The paper concludes with the discussion in Section  and future work and conclusions in Section .       
","  Challenges around collecting and processing quality data have hampered progress in data-driven dialogue models. %, particularly data-hungry neural and hybrid models.   Previous approaches are moving away from costly, resource-intensive lab settings, where collection is slow but where the data is deemed of high quality. The advent of crowd-sourcing platforms, such as Amazon Mechanical Turk, has provided researchers with an alternative cost-effective and rapid way to collect data.   %However, these platforms are sometimes notorious for data anomalies due to the rapid nature of which data is collected.   However, the collection of fluid, natural spoken or textual interaction can be challenging, particularly between two crowd-sourced workers. In this study, we compare the performance of dialogue models for the same interaction task but collected in two different settings: in the lab vs. crowd-sourced. We find that fewer lab dialogues are needed to reach similar accuracy, less than half the amount of lab data as crowd-sourced data.. We discuss the advantages and disadvantages of each data collection method. %, which is of interest to the community in terms of platform choice and how much data will be needed to be collected.",328
" .     %     % % final paper: en-us version     %    % space normally used by the marker  This work is licensed under a Creative Commons  Attribution 4.0 International License. \\  License details:  \url{http://creativecommons.org/licenses/by/4.0/}. } The recent surge in popularity of voice assistants, such as Google Home, Apple閳ユ獨 Siri, or Amazon閳ユ獨 Alexa resulted in interest in scaling these products to more regions and languages. This means that all the components supporting Spoken Language Understanding  in these devices, such as Automatic Speech Recognition , Natural Language Understanding , and Entity Resolution  are facing the challenges of scaling the development and maintenance processes for multiple languages and dialects.  When a voice assistant is launched in a new locale, its underlying speech processing components are often developed specifically for the targeted country, marketplace, and the main language variant of that country. Many people assume that if a device ``understands'' and ``speaks'' in a specific language, for example English, it should be able to work equally well for any English-speaking country, but this is a misunderstanding. For instance, if a speaker of UK English asks a device trained on data collected in the United States ``tell me a famous football player'', it is highly unlikely that this device will provide the user's desired answer, since football means different things in the US and UK cultures. As a result, developers need to take into account not only the language or dialectal differences, but also local culture, to provide the right information in the right language setup. An increase in the number of target marketplaces often means a linear increase in effort needed to develop and maintain such locale-specific models.  NLU models, which classify the user閳ユ獨 intent and extract any significant entities from the user閳ユ獨 utterance, face the same challenge of maintaining high accuracy while being able to accommodate multiple dialects or language content. The major tasks in NLU are intent classification and slot filling. Intent classification is a task to predict what action the user intends the voice assistant to take. Slot filling is a task to identify the specific semantic arguments for the intention. For example, if the user閳ユ獨 request is to ``play Poker Face by Lady Gaga'', the user閳ユ獨 intention will be ``play music'', while in order to fulfill this command with specified details, the system needs to capture the slots for \{song name = Poker Face\}, and \{artist name = Lady Gaga\}. These tasks are called intent classification  and named entity recognition , respectively.  One common approach is to use a max-entropy  classification model for the IC task and a conditional random fields  model for the NER task. Following the advent of deep learning techniques in related fields, such as computer vision and natural language processing, deep learning is becoming more popular in NLU as well. Some of the recent multilingual approaches to NLU include, for example, the Convolutional Neural Network  model for sentence classification , or the Long Short-Term Memory  model for NER prediction . In the deep neural network architecture, the aforementioned NLU tasks can be combined into a single multi-task classification model. An increasing number of experiments also focus on multilingual setups, especially in the field of machine translation, where the task is to translate input from one language to another .  One recent thread of multilingual research centers around learning multilingual word representation. Multilingual word embeddings in the shared cross-lingual vector space have one main property: words from different languages but with similar meaning must be geometrically close. This property allows for transfer learning from one language to another in various multilingual tasks, such as dependency parsing  or classification and NER . A number of model architectures have been proposed to pre-train multilingual word representations, such as leveraging large-scaled LSTM networks trained on monolingual corpora and adversarial setup for space alignment , or transformers trained on multilingual corpora as a single language model .  Although some of these models can be used to solve IC and NER tasks by appending corresponding decoders to generate final predictions, it is not straightforward to use them in production environments due to latency and memory constrains. A different way of benefitting from larger models could be to use them for transfer learning to smaller-size models to improve their performance by initializing some parts of the model with close-to-optimal rather than random weights. In this paper, we extend the multi-task approach studied in  to a general multilingual model for IC and NER tasks, based on deep learning techniques, such as a bidirectional Long Short-Term Memory  CRF sequence labeling model for NER along with a multilayer perceptron  for IC.  We also explore multilingual transfer learning and its benefits to our setup. Transfer learning is widely adapted for zero-shot or few-shot setups, and was explored in some multilingual NLP studies , and also has been used in multi-task IC-NER models ,  yet to the best of our knowledge, there is no study applying transfer learning for data-rich target languages in a multilingual setup. In our experiment, we apply few-shot transfer learning from data-rich languages to a language with a smaller amout of training data. In additon, we also apply  transfer learning to mimic the situation of expanding the model ability to same-level-resource language with known context from another high-resource language, such that the new multilingual model will ``inherit'' context information from its ancestors. We investigate these approaches to transfer learning and their effects on model performance. We show that transfer learning can improve NLU model performance even in data-rich conditions.  
"," 	With the recent explosion in popularity of voice assistant devices, there is a growing interest in making them available to user populations in additional countries and languages. However, to provide the highest accuracy and best performance for specific user populations, most existing voice assistant models are developed individually for each region or language, which requires linear investment of effort. In this paper, we propose a general multilingual model framework for Natural Language Understanding  models, which can help bootstrap new language models faster and reduce the amount of effort required to develop each language separately. We explore how different deep learning architectures affect multilingual NLU model performance. Our experimental results show that these multilingual models can reach same or better performance compared to monolingual models across language-specific test data while require less effort in creating features and model maintenance.",329
"  . 	%  	% % final paper: en-us version  	% 	%	   % space normally used by the marker 	This work is licensed under a Creative Commons  	Attribution 4.0 International License. 	License details: 	\url{http://creativecommons.org/licenses/by/4.0/}. }  The widespread dissemination of fake news has lead to a significant influence on personal fame, public trust, and security. For example, spreading misinformation, such as ``Asians are more vulnerable to novel coronavirus''~\footnote{https://www.thestar.com.my/news/regional/2020/03/11/myth-busters-10-common-rumours-about-covid-19} about COVID-19 has very serious repercussions, making people ignore the harmfulness of the virus and directly affecting public health. Research has shown that misinformation spreads faster, farther, deeper, and more widely than true information. Therefore, fake news detection on social media has attracted tremendous attention recently in both research and industrial fields.   Early research on fake news detection mainly focused on the design of effective features from various sources, including textual content, user profiling data, and news diffusion patterns. Linguistic features, such as writing styles and sensational headlines, lexical and syntactic analysis, have been explored to separate fake news from true news. Apart from linguistic features, some studies also proposed a series of user-based features, and temporal features about the news diffusion. However, these feature-based methods are very time-consuming, biased, and require a lot of labor to design. Besides, these features are easily manipulated by users.   To solve the above problems, many recent studies apply various neural networks to automatically learn high-level representations for fake news detection. For example, recurrent neural network , convolutional neural network , matrix factorization and graph neural network are applied to learn the representation of content and diffusion graph of news. These methods only apply more types of information for fake news detection, but paying little attention to early detection. Moreover, these models can only detect fake news in consideration of all or a fixed proportion of repost information, while in practice they cannot detect fake news in the early stage of news propagation. Some studies explore to detect fake news early by relying on a minimum number of posts. The main limitation of these methods is that they ignore the importance of publishers' and users' credibility for the early detection of fake news.   When we humans see a piece of breaking news, we firstly may use common sense to judge whether there are factual errors in it. At the same time, we will also consider the reputation of the publishers and reposted users. People tend to believe the news from a trusted and authoritative source or the news shared by lots of users with a good reputation. If the publisher is reliable, we tend to believe this news. On the other hand, if the news is reposted by many low-reputation users in a short period, it may be that some spammers tried to heat up on the news, resulting in lower credibility of the news.   Inspired by the above observation, we explicitly take the credibility of publishers and users as supervised information, and model fake news detection as a multi-task classification task. We can annotate a small part of publishers and users by their historical publishing and reposting behaviors. Although the credibility of publishers and users does not always provide correct information, they are necessary complementary supervised information for fake news detection. To make the credibility information generalized to other unannotated users, we construct a heterogeneous graph to build the connections of publishers, news, and users. Through a graph-based encoding algorithm, every node in the graph will be influenced by the credibility of publishers and users.    In this paper, we address the following challenges:  How to fully encode the heterogeneous graph structure and news content; and  How to explicitly utilize the credibility of publishers and users for facilitating early detection of fake news. To tackle the above challenges, we propose a novel structure-aware multi-head attention network for the early detection of fake news. Firstly, we design a structure-aware multi-head attention module to learn the structure of the publishing graph and produce the publisher representations for the credibility prediction of publishers. Then, we apply the structure-aware multi-head attention module to encode the diffusion graph of the news among users and generate user representations for the credibility prediction of users. Finally, we apply a convolutional neural network to map the news text from word embedding to semantic space and utilize the fusion attention module to combine the news, publisher, and user representations for early fake news detection.   The contributions of this paper can be summarized as follows:      
"," The\let\thefootnote\relax\footnotetext{* Corresponding author.} dissemination of fake news significantly affects personal reputation and public trust. Recently, fake news detection has attracted tremendous attention, and previous studies mainly focused on finding clues from news content or diffusion path. However, the required features of previous models are often unavailable or insufficient in early detection scenarios, resulting in poor performance. Thus, early fake news detection remains a tough challenge. Intuitively, the news from trusted and authoritative sources or shared by many users with a good reputation is more reliable than other news. Using the credibility of publishers and users as prior weakly supervised information, we can quickly locate fake news in massive news and detect them in the early stages of dissemination.  In this paper, we propose a novel Structure-aware Multi-head Attention Network , which combines the news content, publishing, and reposting relations of publishers and users, to jointly optimize the fake news detection and credibility prediction tasks. In this way, we can explicitly exploit the credibility of publishers and users for early fake news detection. We conducted experiments on three real-world datasets, and the results show that SMAN can detect fake news in 4 hours with an accuracy of over 91\%, which is much faster than the state-of-the-art models. The source code and dataset can be available at https://github.com/chunyuanY/FakeNewsDetection.",330
"  Real-world events such as sports games or elections involve competing teams, each with capabilities and tactics, aiming to win . The performance of such teams is typically not only dependent on the teams' abilities but also on the environment within which they operate. For example, a political party may have the best orators and policies but their opponents may be better at getting votes in key areas. Similarly, a top football team may be playing the worst team in a league but the fact that the latter may be facing relegation  may provide them with extra motivation to win the game. Given this, in many cases, the performance of such teams may not be easily predictable.    % In particular, in sporting events many human factors impact how a team performs in given games. There are often situations that would be very hard to represent in numbers and statistics alone. For example, sporting rivalries often affect human emotions and team performance and teams fighting to avoid relegation from a league often obtain unexpected results.   Traditional AI and machine learning techniques to predict the outcome of real-world events tend to focus on the use of statistical machine learning using historical data about the individual teams .  However, as per the examples above, historical performance may not be useful when team performance may be dependent on dynamic factors such as human performance  or environmental variables . In turn, humans can be better judges than algorithms when faced with previously unseen situations. Journalists, online communities, and experienced analysts may be better at evaluating human and environmental elements to forecast an outcome. For example, one approach of looking at more than just statistics in sports have been through sentiment analysis on social media platforms. Schumaker, Jarmoszko and  Labedz  use this approach to predict English Premier League  results and achieve an accuracy of 50\% and  show use of similar analysis being performed for American Football results in the National Football League  predicting the winner 63.8\% of the time. However, these approaches focus on opinion aggregation rather than trying to extract the potential indicators of performance for individual human teams from human experts.  Against this background, we set new baselines for results when predicting real-world sporting events involving humans based on the combination of Natural Language Processing  and statistical machine learning techniques. In more detail, we focus specifically on football games in the EPL using match previews from the media alongside statistical machine learning  techniques. The prediction of football match outcomes is a challenging computational problem due to the range of parameters that can influence match results. To date, probabilistic methods devised since the seminal work of Maher  have generated fairly limited results and appear to have reached a glass ceiling in terms of accuracy. By using media previews we can improve on the accuracy of current approaches for match outcome prediction. By so doing, we show that by incorporating human factors into our model, rather than just basic performance statistics, we can improve accuracy . Thus, the contributions of this paper are as follows:    In the next section we discuss the match outcome prediction problem for football and the new feature set we explore.  %The rest of this paper is organised as follows. Section  discusses the problem that we are aiming to solve, Section  outlines how we model human opinion and use this to predicting real-world football games. Section  provides the detail of how we test our models and set the baseline for the prediction accuracy. Finally, Section  concludes.  %
"," In this paper, we present a new application-focused benchmark dataset and results from a set of baseline Natural Language Processing and Machine Learning models for prediction of match outcomes for games of football . By doing so we give a baseline for the prediction accuracy that can be achieved exploiting both statistical match data and contextual articles from human sports journalists. Our dataset is focuses on a representative time-period over 6 seasons of the English Premier League, and includes newspaper match previews from The Guardian. The models presented in this paper achieve an accuracy of 63.18\% showing a 6.9\% boost on the traditional statistical methods.",331
"  Deep neural networks are successful at various morphological tasks as exemplified in the yearly SIGMORPHON Shared Task. However these neural networks operate with continuous representations and weights which is in stark contrast with traditional, and hugely successful, rule-based morphology. There have been attempts to add rule-based and discrete elements to these models through various inductive biases.   In this paper we tackle two morphological tasks and the copy task as a control with an interpretable model, \sopa.  Soft Patterns or \sopa is a finite-state machine parameterized with a neural network, that learns linear patterns of predefined size. The patterns may contain epsilon transitions and self-loops but otherwise are linear. Soft refers to the fact that the patterns are intended to learn abstract representations that may have multiple surface representations, which \sopa can learn in an end-to-end fashion. We call these surface representations subwords, while the abstract patterns, patterns throughout the paper.  An important upside of \sopa is that interpretable patterns can be extracted from each sample.  shows that \sopa is able to retrieve meaningful word-level patterns for sentiment analysis. Each pattern is matched against every possible subword and the highest scoring subword is recovered via a differentiable dynamic program, a variant of the forward algorithm.  We apply this model as the encoder of a sequence-to-sequence or seq2seq\footnote{also called encoder-decoder model} model, and add an LSTM decoder.  We initialize the decoder's hidden state with the final scores of each \sopa pattern and we also apply Luong's attention on the intermediate outputs generated by \sopa. We call this model \sopaseq.  We compare each setup to a sequence-to-sequence with a bidirectional LSTM encoder, unidirectional LSTM decoder and Luong's attention.  We show that \sopaseq is often competitive with the LSTM baseline while also interpretable by design. \sopaseq is especially good at \morphana, often surpassing the LSTM baseline, which confirm our linguistic intuition namely that subword patterns are useful for extracting morphological information.  We also compare these models using a generalized form of Jaccard-similarity and we find that some trends coincide with linguistic intuition.  
","  We examine the role of character patterns in three tasks: morphological analysis, lemmatization and copy. We use a modified version of the standard sequence-to-sequence model, where the encoder is a pattern matching network. Each pattern scores all possible N character long subwords  on the source side, and the highest scoring subword's score is used to initialize the decoder as well as the input to the attention mechanism.  This method allows learning which subwords of the input are important for generating the output. By training the models on the same source but different target, we can compare what subwords are important for different tasks and how they relate to each other. We define a similarity metric, a generalized form of the Jaccard similarity, and assign a similarity score to each pair of the three tasks that work on the same source but may differ in target. We examine how these three tasks are related to each other in \goodlangno languages. Our code is publicly available.\footnote{https://github.com/juditacs/deep-morphology}",332
"  Infusing emotions into conversation systems can substantially improve its usability and promote customers' satisfaction. Moreover, perceiving emotions sufficiently is the core premise of expressing emotions. In real-life scenarios, humans can instinctively perceive complex or subtle emotions from multiple aspects, including the emotion flow of dialogue history, facial expressions and personalities of speakers, and then express suitable emotions for feedback. Figure shows the organization of multi-source information in a dialogue graph and the relationship between them.     
"," The success of emotional conversation systems depends on sufficient perception and appropriate expression of emotions. In a real-world conversation, we firstly instinctively perceive emotions from multi-source information, including the emotion flow of dialogue history, facial expressions, and personalities of speakers, and then express suitable emotions according to our personalities, but these multiple types of information are insufficiently exploited in emotional conversation fields. To address this issue, we propose a heterogeneous graph-based model for emotional conversation generation. Specifically, we design a Heterogeneous Graph-Based Encoder to represent the conversation content  with a heterogeneous graph neural network, and then predict suitable emotions for feedback. After that, we employ an Emotion-Personality-Aware Decoder to generate a response not only relevant to the conversation context but also with appropriate emotions, by taking the encoded graph representations, the predicted emotions from the encoder and the personality of the current speaker as inputs. Experimental results show that our model can effectively perceive emotions from multi-source knowledge and generate a satisfactory response, which significantly outperforms previous state-of-the-art models.",333
" Text classification is one of the fundamental tasks in natural language processing  with wide applications such as sentiment analysis, news filtering, spam detection and intent recognition. Plenty of algorithms, especially deep learning-based methods, have been applied successfully in text classification, including recurrent neural networks ,  convolutional networks   . More recently, large pre-training language models such as ELMO , BERT , Xlnet  and so on have also shown their outstanding performance in all kinds of NLP tasks, including text classification.   Although numerous deep learning models have shown their success in text classification problems, they all share the same learning paradigm: a deep model for text representation, a simple classifier to predict the label distribution and a cross-entropy loss between the predicted probability distribution and the one-hot label vector. However, this learning paradigm have at least two problems:  In general text classification tasks, one-hot label representation is based on the assumption that all categories are independent with each other. But in real scenarios, labels are often not completely independent and instances may relate to multiple labels, especially for the confused datasets that have similar labels. As a result, simply representing the true label by a one-hot vector fails to take the relations between instances and labels into account, which further limits the learning ability of current deep learning models.  The success of deep learning models heavily relies on large annotated data, noisy data with labeling errors will severely diminish the classification performance, but it is inevitable in human-annotated datasets. Training with one-hot label representation is particularly vulnerable to mislabeled samples as full probability is assigned to a wrong category. In brief, the limitation of current learning paradigm will lead to  confusion in prediction that the model is hard to distinguish some labels, which we refer as label confusion problem . A label smoothing  method is proposed to remedy the inefficiency of one-hot vector labeling , however, it still fails to capture the realistic relation among labels, therefore not enough the solve the problem.      In this work, we propose a novel Label Confusion Model  as an enhancement component to current deep learning text classification models and make the model stronger to cope with label confusion problem. In particular, LCM learns the representations of labels and calculates their semantic similarity with input text representations to estimate their dependency, which is then transferred to a label confusion distribution . After that, the original one-hot label vector is added to the LCD  with a controlling parameter and normalized by a softmax function to generate a simulated label distribution . We use the obtained SLD to replace the one-hot label vector and supervise the training of model training. With the help of LCM, a deep model not only capture s the relations between instances and labels, but also learns the overlaps among different labels, thus, performs better in text classification tasks. We conclude our contributions as follows:   
"," Representing a true label as a one-hot vector is a common practice in training text classification models. However, the one-hot representation may not adequately reflect the relation between the instances and labels, as labels are often not completely independent and instances may relate to multiple labels in practice. The inadequate one-hot representations tend to train the model to be over-confident, which may result in arbitrary prediction and model overfitting, especially for confused datasets  or noisy datasets . While training models with label smoothing  can ease this problem in some degree, it still fails to capture the realistic relation among labels. In this paper, we propose a novel Label Confusion Model  as an enhancement component to current popular text classification models. LCM can learn label confusion to capture semantic overlap among labels by calculating the similarity between instances and labels during training and generate a better label distribution to replace the original one-hot label vector, thus improving the final classification performance. Extensive experiments on five text classification benchmark datasets reveal the effectiveness of LCM for several widely used deep learning classification models. Further experiments also verify that LCM is especially helpful for confused or noisy datasets and superior to the label smoothing method.",334
" Over recent years, various task-oriented conversational agents, such as Amazon Alexa, Apple閳ユ獨 Siri, Google Assistant, and Microsoft閳ユ獨 Cortana, have become more popular in people閳ユ獨 everyday life and are expected to be highly intelligent. For the NLU component, this means that we expect models to perform recognition of the actions and entities within a user閳ユ獨 request with high accuracy. When first training an NLU model on a new language , there is a strong requirement for high quality annotated data that would support the most common user requests across a range of domains. As the modeling space expands to support new features and additional languages, NLU models are regularly re-trained on updated data sets to ensure support for these new functions. The major bottleneck in both of these processes is the labor and cost associated with collecting and annotating new training utterances for every new feature or language.   Recent advances in machine learning methods, including the use of techniques such as transfer learning~ and active learning, can lead to more efficient data usage by NLU models and therefore decrease the need for annotated training data. Additionally, data augmentation models are being widely explored. The advantage of data augmentation is that once synthetic data is generated, it can be ingested into subsequent models without additional effort, allowing for faster experimentation.   NLU models in dialog systems can perform a variety of tasks. In this study, we will focus on three of them: Domain classification  -- identify the domain that the user request belongs to , Intent classification  -- extract actions requested by users , and Named Entity Recognition  -- identify and extract entities  from user requests.  For each utterance we expect our NLU model to output a domain, intent, and set of extracted entities with corresponding tags. For example, if a user requests ``play Bohemian Rhapsody by Queen'', we expect the NLU model to return \{domain: music, intent: play\_song, named\_entities: [, ]\}. We call this output annotation, and the utterance along with annotation is called an annotated utterance. Named entities with corresponding labels are called slots.  For our NLU model to perform well on real-time user requests, we need to train it on a large dataset of diverse annotated utterances. However, there could be some areas of functionality where large datasets for training are not available. To boost model performance in situations where training data is limited, we use synthetic data generated from a small set of unique utterances that cover the basic functionality of the user experience, called Golden utterances. We leverage a Sequence Generative Adversarial Networks  introduced by~\citet{Yu2016SeqGANSG} to generate new utterances from this ``seed'' set, and use these generated utterances to augment training data and evaluate the performance of the classification and recognition tasks. We also investigate how the metrics that we use to evaluate the quality of the generated synthetic data links to the performance boost in the underlying tasks.   
"," Data sparsity is one of the key challenges associated with model development in Natural Language Understanding  for conversational agents. The challenge is made more complex by the demand for high quality annotated utterances commonly required for supervised learning, usually resulting in weeks of manual labor and high cost. In this paper, we present our results on boosting NLU model performance through training data augmentation using a sequential generative adversarial network . We explore data generation in the context of two tasks, the bootstrapping of a new language and the handling of low resource features. For both tasks we explore three sequential GAN architectures, one with a token-level reward function, another with our own implementation of a token-level Monte Carlo rollout reward, and a third with sentence-level reward. We evaluate the performance of these feedback models across several sampling methodologies and compare our results to upsampling the original data to the same scale. We further improve the GAN model performance through the transfer learning of the pre-trained embeddings. Our experiments reveal synthetic data generated using the sequential generative adversarial network provides significant performance boosts across multiple metrics and can be a major benefit to the NLU tasks.",335
"  	Encoder-decoder architecture~ has been extensively used in neural machine translation ~. Given a source sentence, an encoder firstly converts it into hidden representations, which are then conditioned by a decoder to generate the target sentence. Attention mechanism~ is very effective in learning the alignment between a source sentence and a target sentence. Hence, attention mechanism is usually used in the architecture to improve its capability, such as capturing long-distance dependencies.  	Similar to traditional machine learning efforts~, some recent approaches in deep learning attempt to improve encoder-decoder architecture with multiple passes of decoding~. NMT refers this to polish mechanism~. Under this scheme, more than one translations are generated for a source sentence and, except for the first translation, each of them is based on the translation from the previous decoding pass. While these methods have achieved promising results, they lack a proper termination policy to the multi-turn process. \citet{xia2017deliberation,zhang2018asynchronous} adopt a fixed number of decoding passes that can be inflexible in deciding the optimal number of decoding passes. \citet{geng2018adaptive} use reinforcement learning ~ to automatically decide the optimal number of decoding passes. However, RL is unstable due to its high variance of gradient estimation and objective instability~. Since these methods may have premature termination or over translation, their potential can be limited.  	 	 To address this problem, we propose a novel framework, Rewriter-Evaluator, in this paper. It consists of a rewriter and an evaluator. The translation process involves multiple passes. Given a source sentence, at every pass, the rewriter generates a new target sequence aiming at improving the translation from prior passes, and the evaluator measures the translation quality to determine whether to terminate the rewriting process. We also propose a prioritized gradient descent  method that facilitates training the rewriter and the evaluator jointly. The essential idea is using a priority queue to improve sampling efficiency by collecting the translation cases that yield low scores from the evaluator for next-pass rewriting. The size of the queue is a few times larger than the batch size. Although Rewriter-Evaluator involves multiple decoding passes, training time using PGD method is comparable to that of training an encoder-decoder~ that doesn't have multiple decoding passes.  	  	 We apply Rewriter-Evaluator to improve the widely used NMT models,  RNNSearch~ and Transformer~. Extensive experiments have been conducted on two translation tasks, Chinese-English and English-German, to verify the proposed method. The results demonstrate that the proposed framework notably improves the performance of NMT models and significantly outperforms prior methods.  
"," 	 	Encoder-decoder architecture has been widely used in neural machine translation . A few methods have been proposed to improve it with multiple passes of decoding. However, their full potential is limited by a lack of appropriate termination policy. To address this issue, we present a novel framework, Rewriter-Evaluator. It consists of a rewriter and an evaluator. Translating a source sentence involves multiple passes. At every pass, the rewriter produces a new translation to improve the past translation and the evaluator estimates the translation quality to decide whether to terminate the rewriting process. We also propose a prioritized gradient descent  method that facilitates training the rewriter and the evaluator jointly. Though incurring multiple passes of decoding, Rewriter-Evaluator with the proposed PGD method can be trained with similar time to that of training encoder-decoder models. We apply the proposed framework to improve the general NMT models . We conduct extensive experiments on two translation tasks, Chinese-English and English-German, and show that the proposed framework notably improves the performances of NMT models and significantly outperforms previous baselines.",336
"    In this article we are proposing to add coinduction\footnote{Throughout this article we use the term `coinduction' in its most generic meaning, encompassing also coalgebra, corecursion, and bisimulation. This terminology will be explained.} to the computational apparatus of natural language semantics. This, we argue, will provide a basis for a more realistic, computationally sound, and scalable model of natural language understanding. Given that the bottom up, inductively\footnote{We use the terms `induction' and `inductive' in their logical and mathematical sense, e.g. as in 	`definition by induction' or `proof by induction,' and not in the philosophical sense of deriving general knowledge from specific cases, as in `inductive reasoning.'}  constructed, semantic structures are brittle, and seemingly incapable of correctly representing the meanings of longer sentences or realistic dialogues, semantics is in the need of a new foundation. Coinduction, which uses top down constraints, has been successfully used in the design of operating systems and programming languages. Moreover,  implicitly it has been present in text mining, machine translation, and in some attempts to model intensionality and modalities. So, there is scattered evidence it works. Since coinduction and induction can coexist, they can provide a common language and a conceptual model for research in natural language  understanding.   We elaborate on this proposal in several ways. We motivate it by discussing the accuracy and conceptual gaps between inductive and coinductive views of NL semantics. We introduce the coinduction, coalgebras and related concepts focusing on intuitions and referring the reader to other works for in depth treatments. We show the natural match between coinduction and several natural language processing  tasks such as modeling dialogue and text mining.  And we show examples of how induction and coinduction can jointly improve the process of assigning representations to text.    We argue for the joint use of deep learning and deep semantics in natural language understanding. Just as the tensor product allows us to jointly explore and use two different but related algebras or vector spaces, we imagine induction and coinduction as jointly providing a better foundation for NL understanding. Although in the remainder of this article we try to convey some intuitions about their joint use, the mathematical and computational requirements for their optimal joint use are not at this point clear to us.    \subsection{Motivation}  Our motivation to pursue this topic comes from two sources, which we elaborate below. The first one is the difference in concepts used in deep learning vs. traditional semantics. The second one has to do with the limitations of processing of long sentences using the traditional semantic representation vs. the relatively successful assignment of much shallower structures using deep learning. Our proposal to think coinductively about the latter allows us to incorporate both methodologies within a single conceptual framework.    \subsubsection{Motivation \#1: The conceptual gap between deep neural networks and deep semantic analysis}  Intuitively there is a gap between using deep neural networks for natural language processing  and using deep semantic analysis for natural language understanding . If we dig deeper into this gap we might observe that their  conceptual apparatus is different.   Reading the textbooks. This can be perhaps most clearly seen in the new version of a leading NLP textbook. Looking at Chapter 16 ``Logical Representations of Sentence Meaning""\footnote{We are using here the manuscript from  \url{https://web.stanford.edu/~jurafsky/slp3/}, version from  October 16, 2019} we notice it not sharing the vocabulary of the encoder-decoder and embedding models introduced earlier in the book. This is not a criticism of the book: first, this is work on progress; second, a currently missing section might create a bridge. Our point is that a bridge is needed.  To reverse the perspective, logical representations do not appear in deep learning focused NLP books such as  and , and NLP doesn't appear as topic in .  A similar gap can be seen in , where lambda calculus and discourse representation is avoided in the sections mentioning the applications of logistic regression and Naive Bayes to NLP, and vice versa.   Even much earlier the problem of bridging the two views of language, one governed by rules and the other by observations was discussed at length ,  but arguably with little impact on the field. Somewhat similar sentiment has been expressed more recently in , commenting on capabilities of deep learning:  ``really dramatic gains may only have been possible on true signal processing tasks.""    This article takes the position that such bridge should be formed by creating an abstraction of both approaches, and not by an ad hoc combination. The value of this abstraction could lie in informing the theory, i.e. models of meaning, but it could also be in guiding the process of creation of better tools for human-computer interaction and natural language understanding.   The historical analogy we might keep in mind is the creation of modern computer architectures and operating systems , which introduced new layers of abstraction  and new disciplines . \\   What about recent research? There was no research article on Google Scholar, as of early June 2020, mentioning ``mathematics of deep learning"" and ``logical inference,"" although aspects of both are covered in experimental research -- ``deep learning"" + ``logical inference"" produces about 800 hits. Thus,   combining logical and neural model is an active area of research. For example,   presents a data set for question answering using both scene graphs modeling of elements present in images and challenging questions about them. In context of a different problem,   discuss ensuring factual correctness of summaries, using two models, one logical  and one neural . In our third example,  show that neural attention based models such as BERT  can be retrained to master aspects of natural language inference. On the other hand, the examples we present in Section  show that deep neural networks still seem incapable of deeper reasoning without special purpose architectures, and even modeling elementary arithmetical operations is a challenge.     \subsubsection{Motivation \#2: Accuracy gap for long sentences between deep learning and deep semantic models}   There is a gap in the accuracy between deep neural networks and deep semantic analysis, irrespective of the fact that they try to address different aspects of natural language understanding.  %illustrated in Table  and Figure   %   \end{table}  %\FloatBarrier   Table , viewed through the lenses of the systems' ability to successfully attend to long sentences, shows in the left column intuitively `successful' NLP applications; and in the right the areas where in our view we have seen limited progress in the last 30 years. Obviously, metrics used by the applications mentioned in the two columns are different. For example, one can argue that computational pragmatics did not exist 30 years ago, and only recently we have started to see computational, probabilistic models of pragmatics    ,\footnote{\url{https://michael-franke.github.io/probLang/} \url{http://www.mit.edu/~tessler/short-courses/2017-computational-pragmatics/} last retrieved on May 13 2020}, which suggests a big jump.  Nevertheless, the areas on the right do not scale with sentence length. And later, in Sections   and  , we will argue, from a more abstract perspective, that the differences between the columns can be attributed to the differences in their respective computational models.  \FloatBarrier  Let us discuss long sentences. Prior research in this area shows how parsing accuracy decreases with the length of the sentence. For example,   observe fast drop  in precision and recall of dependency parsing with the increase of the dependency length, the distance to the root, and length of sentences. Similar results appear in Fig.4 of .  Actually the situation might be worse than these sources suggests. In an analysis of parsing of sentences up to the length of 156, 	   	   entertain a possibility that 閳 parsing of long sentences would be intractable."" Clearly, deep neural networks improved the accuracy of parsing. However, even with the attention-based models,  reports a 20\%  drop in the labeled attachment scores when the dependency length increases from 1 to 5. 	  	    This is clearly a problem, even for linguistically oriented data sets. A recent statistics  given in  shows that depending on the language and the corpus the average sentence length is 19-38 words. However, thousands of  sentences in each corpus are longer than 100 words. The average sentence length in the Penn Treebank is 20.54 words ; in Genesis, 34 words; but, per classic , in ``Biographia Literaria""   10\% of sentences are long and have the average length of about 70 words.      	    The situation is even more problematic when we switch from general corpora to specific ones. In our previous work , we discussed the problem of parsing long sentences in the context of legal text corpora, namely patents.  Fig.   shows the distribution of the lengths of the main patent claim . These claims are expressed as single long sentences. The sentences of Claim 1 average 150-180, and can be up to 1400 words long . Although the extreme length is due to legal rules, nevertheless we note that 93\% of the claims in this series are longer than 50 words. 	    	   This means that any analysis of an average Claim 1 is likely to be wrong. . 	       What about semantic parsing? Semantic representations are difficult to build even for short sentences, as shown in Fig. , from  . All systems submitted to the competition on the shared task on semantic parsing show  a drop in accuracy as the length of the sentence increases.           \FloatBarrier    \bss  \color{black}  \subsection{Hypothesis}    Adding coinduction to semantics can provide a foundation  for a more realistic, computationally sound and scalable model of natural language understanding. \\   We are proposing adding coinduction to the computational apparatus of semantics. This we argue, will provide a basis for a more realistic, computationally sound and scalable model of natural language understanding. Given that the bottom up, inductively constructed semantic structures are brittle, and seemingly incapable of representing longer sentences or realistic dialogues, semantics is in the need of a new foundation. Coinduction, which uses top down constraints, has been successful in the design of operating systems and programming languages. Moreover, one can argue that implicitly it has been present in text mining, machine translation and in some attempts to model intensionality .  In , which is a good introductory textbook, it is used to describe self reference, paradoxes and modal logics.    So, there is scattered evidence coinduction works. Since coinduction and induction can coexist, they can provide a common language and conceptual model for research in NL understanding.   We should mention that one of the first theoretical proposals to look at agent interaction as coinduction appeared in , and it included explicit mention of NL dialogue and question answering. Within the following twenty years, as argued in the present article, the focus of NLP shifted towards coinductive  methods\footnote{ This shift occurred without ever mentioning the concept itself. There are literally 12 entries mentioning ""deep learning"" and ""coinduction"", mostly accidentally, although     is an exception. .},  namely deep learning, with the theoretical justification coming from the universal approximation properties of neural networks.  This article summarizes some of these developments and argues for an explicit introduction of the term `coinduction' to the vocabulary of NLP. \\   To make the argument, we will focus on the following questions:       \color{black} 
","  This article contains a proposal to add coinduction to the computational apparatus of natural language understanding. This, we argue, will provide a basis for more realistic, computationally sound, and scalable models of natural language dialogue, syntax and semantics. Given that the bottom up, inductively constructed, semantic and syntactic structures are brittle, and seemingly incapable of adequately representing the meaning of longer sentences or realistic dialogues, natural language understanding is in need of a new foundation. Coinduction, which uses top down constraints, has been successfully used in the design of operating systems and programming languages. Moreover,  implicitly it has been present in text mining, machine translation, and in some attempts to model intensionality and modalities, which provides evidence that it works.  This article shows high level formalizations of some of such uses.   Since coinduction and induction can coexist, they can provide a common language and a conceptual model for research in natural language understanding. In particular, such an opportunity seems to be emerging in research on compositionality. This article shows several examples of the joint appearance of induction and coinduction in natural language processing. We argue that the known individual limitations of induction and coinduction can be overcome in empirical settings by a combination of the the two methods. We see an open problem in providing a theory of their joint use.",337
" The problem of predicting citation counts of papers has been a long-standing research problem. Predicting citation counts allows us to better understand the relationship between a paper %and its citation count and gives us insight into what affects a paper's impact. and its impact. However, prior research has viewed this as a static prediction problem, i.e. only predicting a single citation count at a static point in time. %With the natural development of new papers being published,  %However,  %Viewing this as a static problem  This ignores the natural development of the data as new papers are being published. Here, we propose to view the problem as a sequence prediction task, with models then having the ability to capture the evolving nature of citations.  %By extending the problem to a sequence prediction problem,  This, in turn, requires a dataset to contain the papers' citation counts over a period of time, which adds a temporal element to the data, which can then be encoded by sequential machine learning models, such as Long short-term memory models . Additionally, scholarly documents exhibit a natural graph-like structure in their citation networks. Given recent developments in modeling such data and prior research showing that modeling input as graphs can be beneficial, we hypothesize that modeling a paper's citation network is useful for predicting citation counts over time.   In this paper, we consider citation networks, a dynamic graph which evolves over time as new citations and papers are added to the network. Leveraging the structured data in the graph allows us to discover complex relationships between papers. We want to tap into that knowledge and treat the citation data as a network, such that we can further exploit topological information and not just temporal information. By doing so, we investigate the hypothesis of paper citation counts being correlated with features such as authors, venue, and topics.  We use the well-established Semantic Scholar dataset to construct our citation network. Its meta-data allows us to construct a dynamic citation network which covers a  year time-line, with an updated graph for each year. The Semantic Scholar dataset's meta-data also contains information about each paper's authors, venue, and topics, allowing us to study the correlation between these features and the citation count of a paper when considering the evolving nature of the citation network. The correlation between these features and citation counts is well known and studied by prior work. Prior studies show that citations are correlated and there is a strong correlation between features such as authors, but are limited by only predicting a single citation, and not predicting the natural evolution of a papers growth.   We propose to use the constructed dynamic citation network  to predict the trajectory of the number of citations papers will receive over time, a new sequence prediction task introduced in this work. Furthermore, we propose an encoder-decoder model to solve the proposed task, which uses graph convolutional layers to exploit the graphs' topological features and an LSTM to model the temporal component of the graphs. We compare our model against a standard GCN and standard LSTM, which individually incorporate either the topological information or the temporal information, but not both.  Our contributions are as follows: 1) A dynamic citation network based on the Semantic Scholar dataset. The dynamic citation network contains  time-steps, with an updated graph at each time-step, based on yearly information. 2) We introduce the task of sequence citation count prediction. 3) A novel encoder-decoder model based on a GCN and LSTM to extract the dynamic graph's topological and temporal components. 4) A thorough study of the correlation between citation counts and temporal components.  \iffalse Our contributions are as follows:   \fi  
"," %Citation count prediction is the task of predicting the number of citations, which a paper has gained after a given period. Prior work view this as a static prediction task, but due to papers and their citations develops over time, predicting the sequence of citations will also capture the papers' development. We further employ the recent development in graph structured data and view the papers as a citation network, linked by papers citations. Viewing the papers as a citation network allows us to exploit the topological information of the citation network. While no prior citation network allow for predicting the development of a paper's citations over time. Therefore, we use the well known Semantic Scholar dataset to construct a dynamic citation network, i.e., a graph which evolves over time. This dynamic citation network spans over $42$. Using the constructed dynamic citation network, we introduce the task of sequence citation count prediction. To solve the introduced task, we propose a model which exploits topological and temporal information. We compare the proposed model against baseline models and analyze the performance. Furthermore, we study the importance of topological and temporal features for predicting a paper's citation count. \andreas{findings TBD} % revised Citation count prediction is the task of predicting the number of citations a paper has gained after a period of time. Prior work viewed this as a static prediction task. As papers and their citations evolve over time, considering the dynamics of the number of citations a paper will receive would seem logical. Here, we introduce the task of sequence citation prediction, where the goal is to accurately predict the trajectory of the number of citations a scholarly work receives over time. We propose to view papers as a structured network of citations, allowing us to use topological information as a learning signal. Additionally, we learn how this dynamic citation network changes over time and the impact of paper meta-data such as authors, venues and abstracts. To approach the introduced task, we derive a dynamic citation network from Semantic Scholar which spans over $42$ years. We present a model which exploits topological and temporal information using graph convolution networks paired with sequence prediction, and compare it against multiple baselines, %where we will use GCN and LSTM as standalone, to  testing the importance of topological and temporal information and analyzing model performance.  Our experiments show that leveraging both the temporal and topological information greatly increases the performance of predicting citation counts over time.",338
"   Recent advances in open domain question answering  have mostly revolved around machine reading comprehension   where the task is to read and comprehend a given text and then answer questions based on it. However, most recent work in MRC has only been in English \eg\ SQuAD , HotpotQA  and Natural Questions . Significant performance gains and the state-of-the-art  on these datasets are credited to large pre-trained language models .    Multilingual BERT , which is trained on Wikipedia articles from 104 languages and equipped with a 120k shared wordpiece vocabulary, has encouraged a lot of progress on cross-lingual tasks \eg{} XNLI , NER  and QA  by performing zero-shot training: train on one language and test on unseen target languages.  In this work, we focus on multilingual QA and, in particular, on two recent large-scale datasets: MLQA and TyDiQA\footnote{All uses of TyDiQA in our paper refer to the Gold Passage task.}. Both datasets contain English QA pairs but also examples from 13 other diverse languages.  Some examples are shown in Figure . MLQA evaluates two challenging scenarios: 1) Cross-Lingual Transfer   when the question and the context are in the same language, and 2) Generalized Cross-lingual Transfer  when the question is in one language  and the context is in another language .  %In both cases, MLQA is zero-shot because it does not provide training data in any language. \avi{Maybe remove this previous sentence?} % TyDiQA consists of QA examples in English and 8 other languages.  TyDiQA is designed for XLT only. Both datasets are challenging for multilingual QA due to the large number of languages and the variety of linguistic phenomena they encompass .  Ideally, we want to build QA systems for all existing languages but it is impractical to collect manually labeled training data for all of them.  In the absence of labeled data,  suggested several research directions for pushing the boundaries in multilingual QA, including zero-shot QA, exploring data augmentation with machine translation, as well as effective transfer learning. These are avenues we explore in our work in addition to asking the following research questions:\\             1. Is a large pre-trained LM sufficient for zero-shot multi-lingual QA? \\      Prior work proposes zero-shot transfer learning from English SQuAD data  to other languages using only a pre-trained LM and competitive results are achieved  on MLQA  and TyDiQA . We venture beyond zero-shot training by first exploring data augmentation  on top of their underlying model. We achieve this by using translation methodologies  to augment the English training data.       We use machine translation to obtain additional silver labeled data allowing us to improve cross-lingual transfer at a low cost.  Our approach introduces several multilingual extensions to the SQuAD training data: translating just the questions but keeping the context in English, translating just the context but keeping the question in English, and translating the question and the context to other languages. This enables us to augment the original English human-labeled training examples with 14 times more multilingual silver-labeled QA pairs.\\           2. Can we bring language-specific embeddings in multi-lingual LMs closer for effective cross-lingual transfer?\\         %To do better MLQA, we believe it is important that the model      Our hypothesis is that we can make the cross-lingual QA transfer more effective if we can bring the embeddings in a multilingual pre-trained LM closer to each other in the same semantic space. To answer a question in French it should suffice to train the system on Hindi and not be necessary to train a system on the target language:  hence, French and Hindi should look as if they are the same language.     We propose two approaches to explore cross-lingual transfer:              In our first approach, we propose a novel strategy based on adversarial training  . We investigate how the addition of a language-adversarial task during QA finetuning for a pretrained LM can significantly improve the cross-lingual transfer performance while causing the embeddings in the LM to become less language-dependent.           In our second approach, we develop a novel Language Arbitration Framework  to consolidate the embedding representation across languages using properties of the translation.         We train additional auxiliary tasks \eg{} making sure an English question and its translation in Arabic produces the same answer when they see the same input context in Spanish. The intuition behind language arbitration is that while we are training the model on English and translated examples, the proposed multi-lingual objectives bring the language-specific embeddings closer to the English embeddings.\\               Overall, our main contributions in this paper are as follows:               
"," Prior work on multilingual question answering has mostly focused on using large multilingual pre-trained language models  to perform zero-shot language-wise learning: train a QA model on English and test on other languages. In this work, we explore strategies that improve cross-lingual transfer by bringing the multilingual embeddings closer in the semantic space.  Our first strategy augments the original English training data with machine translation-generated data. This results in a corpus of multilingual silver-labeled QA pairs that is 14 times larger than the original training set. In addition, we propose two novel strategies, language adversarial training and language arbitration framework, which significantly improve the  cross-lingual transfer performance and result in LM embeddings that are less language-variant. Empirically, we show that the proposed models outperform the previous zero-shot baseline on the recently introduced multilingual MLQA and TyDiQA  datasets.",339
"   % \subsection{Problem Statement and Motivation}  Researchers' ability to automate natural language processing has grown exponentially over the past few years, particularly with the advent of the Transformer architecture . Despite the fact that recent machine learning methods achieve impressive and almost human-level performance on tasks such as dialogue modeling  and natural language generation , many intelligent voice assistants still rely on rule-based architectures and cached responses in open domain dialogue . This is primarily due to the lack of controls in deep learning architectures for producing specific phrases, tones, or topics, which makes these models inherently unpredictable and therefore too risky for most entities - corporate or otherwise - who wish to deploy public-facing intelligent agents. For example, it is often desirable for a conversational agent to maintain a specific identity  throughout an exchange of dialogue and it is currently impossible to condition deep learning algorithms to maintain a coherent identity across dialogue without training them on highly specialized  data sets. Fine-tuning on these specialized data sets comes with an additional, significant cost: it can lead to catastrophic forgetting of the language model . Despite this aspect of fine-tuning, current state-of-the-art methods  require fine-tuning  of the entire network when their original data set proves unsuitable for a given task , even if the language being modeled is the same across tasks. Furthermore, models produced by current methods are almost entirely uninterpretable and therefore generally difficult to test for egregious failure cases.  % \subsection{Solution Overview}  In this paper, we address both the issue of content control as well as that of catastrophic forgetting induced by fine-tuning. We define `content control' as being able to command a network to either incorporate or eschew an exact word, phrase, topic, style, or sentiment in its output, and therefore attempt a more granular level of control than the purely topic/style-level control that has been published in recent literature . We also introduce an alternative to fine-tuning neural language models and demonstrate through experimentation that the high-cost of overwriting model weights through fine-tuning  often fails to induce the desired behavior in generalized settings.  %is inspired by the ``No Free Lunch"" theorems introduced by Wolpert \& Macready  in that we seek to avoid training a neural network to simultaneously model language and act on explicit commands.  Instead,  we recast the problem of control in natural language generation as one of combining separate models - one of the natural language itself and one of high-level command responses - to produce desired linguistic output. In doing so, we develop a framework for interpreting and subsequently controlling the hidden activations of a pretrained neural network without any adjustments being made to the pretrained model. This framework is biologically consistent with the findings of Knutson et al., who discovered that neural pathways in humans are inhibited by other neuron clusters , and has applications to other neural network architectures and questions outside the domain of controllable text generation.  
"," %   Current solutions to the problem of controlling generative neural language models are usually formulated under a training paradigm in which the language model is trained to simultaneously model natural language and respond to high-level commands. We recast the problem of control in natural language generation as that of learning to interface with a pretrained language model to generate desired output, just as Application Programming Interfaces  control the behavior of programs by altering hyperparameters. In this new paradigm, a specialized neural network  learns to interface with a pretrained language model by manipulating the hidden activations of the pretrained model in real time to produce desired outputs, such that no permanent changes are made to the weights of the original language model.     It is notoriously difficult to control the behavior of artificial neural networks such as generative neural language models. We recast the problem of controlling natural language generation as that of learning to interface with a pretrained language model, just as Application Programming Interfaces  control the behavior of programs by altering hyperparameters. In this new paradigm, a specialized neural network  learns to interface with a pretrained language model by manipulating the hidden activations of the pretrained model to produce desired outputs. Importantly, no permanent changes are made to the weights of the original model, allowing us to re-purpose pretrained models for new tasks without overwriting any aspect of the language model. We also contribute a new data set construction algorithm and GAN-inspired loss function that allows us to train NPI models to control outputs of autoregressive transformers. In experiments against other state-of-the-art approaches, we demonstrate the efficacy of our methods using OpenAI闁炽儲鐛 GPT-2 model, successfully controlling noun selection, topic aversion, offensive speech filtering, and other aspects of language while largely maintaining the controlled model's fluency under deterministic settings. %Finally, we describe the ethical implications of this work. %   Applications for this approach include re-purposing a pretrained model  for a new task without a specialized data set in the problem domain. We present experimental results from training several NPI models to control the outputs of OpenAI's GPT-2 language model \cite{openaiGPT2}, as well as a novel data curation approach in which hidden activations of an uninterpretable pretrained model are associated with specific outputs. Finally, we describe potential methods whereby NPIs might be leveraged to interpret the inner workings of pretrained networks, as well as the related ethical implications of this work.",340
"  Emotion analysis of user-generated content  available on the web provides insights toward making meaningful decisions. Micro-blog platforms such as Twitter has gained profuse popularity for textual content holding people's opinions. The past decade has seen the active growth in emotion analysis models in many domains. Recently there has been an increasing interest in analysis of emotions of informal short texts such as tweets. In this paper, we introduce and analyze a system to accurately identify the emotions of the individual tweets with the associated intensities~\footnote{Intensity refers to the degree or amount of an emotion}.  % explain why it is important to analyze emotions  Analyzing emotions in social media such as twitter benefits society in a number of ways. Policymakers can use emotional information in social media to accurately identify concerns of people when making decisions. Monitoring social media for health issues benefits not only public health but also government decision makers. Furthermore, organizations can monitor opinion of the public on their products and services to provide better service to the society. Once emotions are recognized, emotion intensity can be used to prioritize the major concerns.  Studies in emotion analysis have often focused on emotion classification. However, emotions may exhibit varying levels of intensities. Here, emotion intensity can be defined as the degree or the intensity of particular emotion felt by the speaker. Additionally, we may observe multiple emotions simultaneously in the same tweet with varying intensities.   One purpose of this study is to develop a model to accurately identify the emotions and associated emotion intensities for a given tweet. In this paper, we propose a transfer learning approach backed by a neural network classifier and a regressor. Although the proposed neural network alone is inadequate to beat the benchmark, we show that features learned when training the above neural networks can be used to improve the overall performance when combined with other features.  Another purpose of this study is to explain how the input word level features affect the features extracted by the neural network.  % [complete the actual findings here] The findings should make an important contribution in understanding how features are used in a neural network and to effectively select features to improve the effectiveness of extracted features.   Our main contributions of this study:   \pagebreak  Major challenge in using deep learning to train emotion intensity prediction models is the lack of large labeled datasets. More recently, emoji and hashtags were used in studies to create large naturally labeled datasets. However, it is not possible to use a similar technique to obtain the intensity of emotions. Furthermore, creating a large dataset manually is time consuming and expensive.  are some existing datasets for emotional intensity prediction. Due to the limited amount of task-specific training data the previous researches have opted for transfer learning approaches~\citet{baziotis2018ntua, duppada2018seernet} and traditional machine learning. However, in this paper we argue that even with reasonable size dataset we can train a neural network to obtain good performance provided that there is proper regularization. Additionally, we show that features learned when training the neural network can be combined with other features to improve the overall performance of emotion intensity prediction.   % [explain methodology in brief]  % } \end{table}  In \S, we outline related works on sentiment and emotion mining. Next, in \S we will discuss the datasets used in this study. After, we introduce the background and our methodology in \S and \S accordingly. Then, in \S we will discuss the evaluation results. Finally, we will conclude this paper in \S.  
"," In this paper, we present an experiment on using deep learning and transfer learning techniques for emotion analysis in tweets and suggest a method to interpret our deep learning models. The proposed approach for emotion analysis combines a Long Short Term Memory  network with a Convolutional Neural Network . Then we extend this approach for emotion intensity prediction using transfer learning technique. Furthermore, we propose a technique to visualize the importance of each word in a tweet to get a better understanding of the model. Experimentally, we show in our analysis that the proposed models outperform the state-of-the-art in emotion classification while maintaining competitive results in predicting emotion intensity.",341
"  Online reviewing for businesses becomes more and more important nowadays, where customers can publish their reviews for businesses, and other potential customers or shop owners can view them. Positive feedback from customers may prosper the store businesses, while negative one could have opposite consequences. Yelp, one of the largest company founded in 2004 for publishing crowd-sourced reviews about businesses, provides one open dataset, Yelp Open Dataset , which has tremendously many data about businesses, reviews, and users. Such dataset has been proven to be a good material for personal, educational, and academic purposes.  Among multiple tasks on the Yelp Open Dataset, predicting ratings for restaurants based their reviews is one of fundamental and important tasks. This task can help Yelp classify reviews into proper groups for its recommendation system, detect anomaly reviews to protect businesses from malicious competitions, and assign rating to texts automatically.  Yelp review rating prediction can be done in multiple ways, such as sentiment analysis and 5-star rating classification. In this paper, we will focus on rating prediction for restaurants based only on their review texts. This task can be viewed as a multiclass classification problem, where the input is the textual data , and output is the predicted class . We will apply both machine learning and deep learning models. After analyzing data distribution, splitting datasets, and extracting features, we will use four machine learning methods, including Naive Bayes, Logistic Regression, Random Forest, and Linear Support Vector Machine  . Then we will focus on four transformer-based models, including BERT , DistilBERT , RoBERTa , and XLNet , where several different architectures will be tried with hyperparameter tuning. This project is done on \href{https://colab.research.google.com/}{Google Colab}, where multi-processors and GPUs are available. The code is publicly available at GitHub .   
","    We predict restaurant ratings from Yelp reviews based on Yelp Open Dataset. Data distribution is presented, and one balanced training dataset is built. Two vectorizers are experimented for feature engineering. Four machine learning models including Naive Bayes, Logistic Regression, Random Forest, and Linear Support Vector Machine are implemented. Four transformer-based models containing BERT, DistilBERT, RoBERTa, and XLNet are also applied. Accuracy, weighted $ F_1 $ score, and confusion matrix are used for model evaluation. XLNet achieves 70\% accuracy for 5-star classification compared with Logistic Regression with 64\% accuracy.",342
"  Language processing requires tracking information over multiple timescales. To be able to predict the final word ``timescales"" in the previous sentence, one must consider both the short-range context  and the long-range context . How do humans and neural language models encode such multi-scale context information? Neuroscientists have developed methods to study how the human brain encodes information over multiple timescales during sequence processing. By parametrically varying the timescale of intact context, and measuring the resultant changes in the neural response, a series of studies  showed that higher-order regions are more sensitive to long-range context change than lower-order sensory regions. These studies indicate the existence of a ``hierarchy of processing timescales"" in the human brain. More recently, \citet{chien2020constructing} used a time-resolved method to investigate how the brain builds a shared representation, when two groups of people processed the same narrative segment preceded by different contexts. By directly mapping the time required for individual brain regions to converge on a shared representation in response to shared input, we confirmed that higher-order regions take longer to build a shared representation. Altogether, these and other lines of investigation suggest that sequence processing in the brain is supported by a distributed and hierarchical structure: sensory regions have short processing timescales and are primarily influenced by the current input and its short-range context, while higher-order cortical regions have longer timescales and track longer-range dependencies .  How are processing timescales organized within recurrent neural networks  trained to perform natural language processing? Long short-term memory networks   have been widely investigated in terms of their ability to successfully solve sequential prediction tasks. However, long-range dependencies have usually been studied with respect to a particular linguistic function , and there has been less attention on the broader question of how sensitivity to prior context -- broadly construed --  is functionally organized within these RNNs. Therefore, drawing on prior work in the neuroscience literature, here we demonstrate a model-free approach to mapping processing timescale in RNNs. We focused on existing language models that were trained to predict upcoming tokens at the word level  and at the character level . The timescale organization of these two models both revealed that the higher layers of LSTM language models contained a small subset of units which exhibit long-range sequence dependencies; this subset includes previously reported units  as well as previously unreported units.  After mapping the timescales of individual units, we asked: does the processing timescales of each unit in the network relate to its functional role, as measured by its connectivity? The question is motivated by neuroscience studies which have shown that in the human brain, higher-degree nodes tend to exhibit slower dynamics and longer context dependence than lower-degree nodes . More generally, the primate brain exhibits a core periphery structure in which a relatively small number of ``higher order閳 and high-degree regions  maintain a large number of connections with one another, and exert a powerful influence over large-scale cortical dynamics . Inspired by the relationships between timescales and network structure in the brain, we set out to test corresponding hypotheses in RNNs:  Do units with longer-timescales tend to have higher degree in neural language models? and  Do neural language models also exhibit a ``core network"" composed of functionally influential high-degree units? Using an exploratory network-theoretic approach, we found that units with longer timescales tend to have more projections to other units. Furthermore, we identified a set of medium-to-long timescale ``controller"" units which exhibit distinct and strong projections to control the state of other units, and a set of long-timescale ``integrator units"" which showed influence on predicting words where the long context is relevant. In summary, these findings advance our understanding of the timescale distribution and functional organization of LSTM language models, and provide a method for identifying important units representing long-range contextual information in RNNs.  
"," In the human brain, sequences of language input are processed within a distributed and hierarchical architecture, in which higher stages of processing encode contextual information over longer timescales. In contrast, in recurrent neural networks which perform natural language processing, we know little about how the multiple timescales of contextual information are functionally organized. Therefore, we applied tools developed in neuroscience to map the ``processing timescales闁 of individual units within a word-level LSTM language model. This timescale-mapping method assigned long timescales to units previously found to track long-range syntactic dependencies. Additionally, the mapping revealed a small subset of the network  with long timescales and whose function had not previously been explored. We next probed the functional organization of the network by examining the relationship between the processing timescale of units and their network connectivity. We identified two classes of long-timescale units: ``controller闁 units composed a densely interconnected subnetwork and strongly projected to the rest of the network, while ``integrator闁 units showed the longest timescales in the network, and expressed projection profiles closer to the mean projection profile. Ablating integrator and controller units affected model performance at different positions within a sentence, suggesting distinctive functions of these two sets of units. Finally, we tested the generalization of these results to a character-level LSTM model and models with different architectures. In summary, we demonstrated a model-free technique for mapping the timescale organization in recurrent neural networks, and we applied this method to reveal the timescale and functional organization of neural language models.\footnote{The code and dataset to reproduce the experiment can be found at \url{https://github.com/sherrychien/LSTM_timescales}}",343
"    We summarize our contribution as follows:    
"," Keyphrase Generation  is the task of generating central topics from a given document or literary work, which captures the crucial information necessary to understand the content. Documents such as scientific literature contain rich meta-sentence information, which represents the logical-semantic structure of the documents.  However, previous approaches ignore the constraints of document logical structure, and hence they mistakenly generate keyphrases from unimportant sentences. To address this problem, we propose a new method called Sentence Selective Network  to incorporate the meta-sentence inductive bias into KG. In SenSeNet, we use a straight-through estimator for end-to-end training and incorporate weak supervision in the training of the sentence selection module. Experimental results show that SenSeNet can consistently improve the performance of major KG models based on seq2seq framework, which demonstrate the effectiveness of capturing structural information and distinguishing the significance of sentences in KG task.",344
"   % With the recent development of end-to-end text-to-speech  system, the synthesised speech has achieved high intelligibility and quality in various languages . Recently neural network based text-to-speech  systems have achieved certain success in prosody and naturalness of synthesized speech over conventional methods .  % Because Chinese is non-alphabet and its character set is very large, grapheme-to-phoneme  is essential when hiring end-to-end model in Chinese . By applying encoder-decoder framework with attention , these systems can directly predict speech parameters from graphemes or phonemes by learning acoustic and prosodic patterns via a flexible mapping from linguistic to acoustic space.  % But they still can only model part of prosody structural information from raw text  because of their limited model capacity, resulting poor expressiveness even prosody errors. % V_1021But they still can only model part of prosody structural information from raw text  resulting in poor expressiveness even prosody errors. However, the learnt prosodic patterns only contain part of prosodic structural information , resulting in poor prosody and naturalness performance even improper prosody.  % So additional prosody structure information is important to improve the naturalness of synthesized speech for text-to-speech system. % V5: So adding prosody information, such as prosody structure annotations, in encoder-decoder based models is important to improve the expressiveness of synthesized speech in TTS systems. % The G2P module converts the text input into a sequence of phonemes with tones, after which the intelligibility and naturally of synthesised Chinese speech can perform better than the conventional TTS . % However, the limited coverage of phoneme permutation in training data causes the decline of ability to predict prosody, resulting in unnatural prosody and unexpected pause.   %   % V4: There are many attempts to improve the prosody prediction ability of TTS system by introducing prosody structure information explicitly. % V5: Prosody structure annotations have been successfully applied in TTS systems to improve expressiveness. % V1020: To improve expressiveness of synthesized speech, directly adding prosodic structure annotations, such as Tones and break indices  labels  and The MATE meta-scheme % v_1021:To improve expressiveness of synthesized speech, adding prosodic structure annotations such as tones and break indices  labels  or other prosodic structure annotation  to input sequence of encoder-decoder based models has been proposed. To further improve prosody and naturalness of synthesized speech, adding prosodic structure annotations such as tones and break indices  labels  or other prosodic structure labels  to the input sequence of neural network based TTS models has been proposed. Prosodic structure annotations need to be subjectively labeled from speech, which is time-consuming.  Although these annotations can be automatically annotated by training another prosodic structure prediction model , the accuracy of predicted prosodic structure labels is still limited by using subjectively labeled annotations as the ground-truths. The high correlation between syntactic structure and prosodic information has been proved by successful syntactic-to-prosodic mapping . % V1020: The syntactic parsing models trained with a large text database with rich grammatical structure  provide text in TTS dataset with usefully syntactic structure information. A set of rule-based syntactic features such as part-of-speech  and positions of the current word in parent phrases are proposed and used in hidden Markov model  based acoustic model . % So subjective labeled prosodic structure annotations can be replaced with syntactic structure information, which obtained from text without referring to speech. % In hidden markov model  based acoustic model, a set of rules to create syntactic features including part of speech  and positions of the current word in parent phrases are hired as syntactic structure information to improve prosody and naturalness exceeds prosodic structure annotations in comprehensiveness and granularity . % This provides us with another method to implicitly improve prosody using syntactic structure information, which exceeds using prosody structure information explicitly in comprehensiveness and granularity.  % Early in the hidden markov based TTS model, rich syntactic context instead of prosody structure information is used to improved prosody of synthesized . % The word relation based features  proposed by  are prior features, which require expert knowledge to be designed. % to explore syntactic information from parse tree, to improve the generalization of synthesised speech. To utilize more syntactic structure information, phrase structure based feature  and word relation based feature  are proposed in neural network based TTS . PSF and WRF expand the set of syntactic features used in HMM model. More features such as highest-level phrase beginning with current word  and lowest common ancestor  are further introduced to model syntactic structure .  However, the expanded features are still manually designed features rather than automatically learned high-level representations. PSF only contains features from limited layers of the whole syntactic tree structure. WRF only exposes the information of partial nodes and edges from the whole syntactic parse tree.  % PSF and WRF can only model the syntactic relation among limited subtrees rather than the whole syntactic parse tree structure. % contain feature from syntactic tree structure by design. % needs  and expert knowledge to select  % V1020: which makes it harder to extract useful information and leads to instability. % and the way to select the specific layers from parse tree, which makes it harder to extract useful information and leads to instability. % V1020: And WRF focuses on the relation between two adjacent words in parsing tree structure, which can only model limited information from the whole syntactic parse tree. For example, one of WRF features is highest-level phrase beginning with current word . % WRF only models . %  which expand partial higher structure . % limited by manual selection strategy, WRF only considers the influence of former word on next word and specific layer of parent nodes, so cannot model the whole structure parse tree.  % This makes the prosody performance largely determined by the selected strategy, and at the same time very unstable. %In Fig., we show a example of how synthesised speech from phoneme sequence input  is different from reference speech  because of failing to respect syntax structure.  % Without parsing tree's limit, the third word ""cu4 jin4"" is pronounced separately .  % Besides, without parsing tree information, synthesised speech does not pause between the fifth word ""ti2 xiao4"" and the sixth word ""shi4"", which have a obvious gap in parsing tree reflected in reference speech . % simply plugging these parsing tree information during TTS does not perform well. Limited by the manual design rule, these features have some disadvantages to model syntax tree structure information. Firstly, using phrase structure feature needs to fix the number of tree layers and the way to select specific layer, while using word relation feature has to make the model select only part of parse tree structure, which cannot be proved to be the most useful part for prosody modeling. This makes the prosody performance largely determined by the selected strategy, and at the same time very unstable. Secondly, word relation feature only consider the former words' influence on next word and ignore the impact of the backward structure importance. Last but not least, manual design features require very high accuracy of syntax tree annotation, which can not be easily achieved. Otherwise, Otherwise under the influence of manual selection strategy, the destructive influence of mislabeling on prosody prediction will be magnified.  % A syntactic parse tree traversal based method is proposed to learn syntactic representation and employed in neural machine translation . To maker better use of the syntactic information, motivated by the syntactic parse tree traversal approach in neural machine translation , we propose a syntactic representation learning method to further improve the prosody and naturalness of synthesized speech in neural network based TTS.  % To make a better use of the syntactic information, in this paper, we propose a syntactic representation learning method to further improve the prosody in neural network based TTS. % which also known as phrase structure parsing, for TTS system to control prosody more effective.  Syntactic parse tree is linearized into two constituent label sequences through left-first and right-first traversal. % Word level bidirectional  Then syntactic representations are extracted from the constituent label sequences using different uni-directional GRU network for each sequence. After which, the syntactic representations are up-sampled from word level to phoneme level and concatenated with phoneme embeddings.  Tacotron 2 is employed to generate spectrogram from the concatenated syntactic representations and phoneme embeddings, with Griffin-Lim  to reconstruct the waveform. % directly Nuclear-norm maximization loss  is introduced to the constituent label embedding layer to enhance discriminability and diversity.  Compared to only hiring left-first traversal , right-first traversal is proposed to alleviate the ambiguity.  Experimental results show that our proposed model outperforms the baseline in terms of prosody and naturalness. Mean opinion score  increases from  to  compared with the baseline approach . % compared to baseline approach, with  is  from a one-way ANOVA test. ABX preference rate exceeds the baseline approach by . % One-way ANOVA test reveals a significant improvement . % We go further to explore how the enhanced controllability of prosody can benefit eliminate ambiguity. For sentences with multiple different syntactic parse trees, prosodic differences can be clearly perceived from corresponding synthesized speeches.  %We linearize a phrase parse tree into a structural label sequence and propose a rnn-based model to learn useful syntactic information by itself, and experimental shows significantly better than the method of manually extracting features. %To our best known, we first exploite syntactic information to chinese TTS system and first to apply syntactic information to lower input level than word. %We have also introduce rank loss of syntactic label embedding to enhance the ability of the syntax structure to control prosody, which expanded the specific application of parsing tree information, including different sentences in the same parsing tree structure to bring the same prosodic structure, and different trees in the same sentence to produce different prosodic readings. The latter brings solutions to the ambiguity caused by grammatical structure  
"," Syntactic structure of a sentence text is correlated with the prosodic structure of the speech that is crucial for improving the prosody and naturalness of a text-to-speech  system.  Nowadays TTS systems usually try to incorporate syntactic structure information with manually designed features based on expert knowledge.  In this paper, we propose a syntactic representation learning method based on syntactic parse tree traversal to automatically utilize the syntactic structure information.  Two constituent label sequences are linearized through left-first and right-first traversals from constituent parse tree. Syntactic representations are then extracted at word level from each constituent label sequence by a corresponding uni-directional gated recurrent unit  network.  Meanwhile, nuclear-norm maximization loss is introduced to enhance the discriminability and diversity of the embeddings of constituent labels.  Upsampled syntactic representations and phoneme embeddings are concatenated to serve as the encoder input of Tacotron2.  Experimental results demonstrate the effectiveness of our proposed approach, with mean opinion score  increasing from $3.70$ to $3.82$ and ABX preference exceeding by $17\%$ compared with the baseline. In addition, for sentences with multiple syntactic parse trees, prosodic differences can be clearly perceived from the synthesized speeches.",345
" Semantic parsing is the task of mapping natural language utterances to machine interpretable meaning representations. Many semantic parsing methods are based on the principle of semantic compositionality ~, of which the main idea is to put together the meanings of utterances by combining the meanings of the parts~. However, these methods suffer from heavy dependence on handcrafted grammars, lexicons, and features.  To overcome this problem, many neural semantic parsers have been proposed and achieved promising results~. %\textcolor{red}{However, compared to compositional semantic parsers, neural semantic parsers are not aware of the compositional structure of utterances, which often limits their generalization between various compound-complex utterances: However, due to the lack of capturing compositional structures in utterances, neural semantic parsers usually have poor generalization ability to handle unseen compositions of semantics~. For example, a parser trained on ``How many rivers run through oklahoma?'' and ``Show me states bordering colorado?'' may not perform well on ``How many rivers run through the states bordering colorado?''.     \end{table}     \end{table*}  In this paper, we propose a novel framework to boost neural semantic parsers with the principle of compositionality~. It iterates between segmenting a span from the utterance and parsing it into a partial meaning representation. Table  shows an example. Given an utterance ``How many rivers run through the states bordering colorado?'', we parse it through three iterations:  we segment a span ``the states bordering colorado'' from the utterance, and parse it into ;  as the utterance is reduced to ``How many rivers run through \?'', we segment a span ``rivers run through \'' from it, and parse it into ;  the utterance is further reduced to ``How many \?'', and we parse it into . We compose these partial meaning representations into the final result.  Our framework consists of two neural modules: an utterance segmentation model  and a base parser . The former is in charge of segmenting a span from an utterance, and the latter is in charge of parsing the span into its meaning representation. These two modules work together to parse complex input utterances in a divide-and-conquer fashion.  One key advantage of this framework is that it does not require any handcraft templates or additional labeled data for utterance segmentation: we achieve this through proposing a novel training method, in which the base parser provides pseudo supervision to the utterance segmentation model. Specifically: we train a preliminary base parser on the original train data; then, for each train sample , we use this preliminary base parser to check whether spans in  can be parsed to \textcolor{red}{be} a part of  \textcolor{red}{or not}. If true, we leverage these spans as pseudo supervision signals for training the utterance segmentation model, and thereby do not require any handcraft templates or additional labeled data.} %The key to implement this framework is to address the challenge of lacking labeled data for utterance segmentation. %We achieve this through cooperative training of the segmentation model and the base parser: %leverage pre-trained base parser to derive synthetic supervision signals for training the segmentation model, then leverage the segmentation model to derive synthetic supervision signals for updating the base parser.  % \textcolor{green}{Moreover, considering that there are usually no labeled data for utterance segmentation, we propose to search for reasonable segmentation points from utterances via the base parser, and use them as a distant supervision. This improves the domain adaptability of our framework.}  % While lacking the direct supervision for segmentation model, we seek to address this challenge in a distantly supervised way. % shaped like  %\textcolor{red}{ %Firstly, we train the base parser, and use it to search for and evaluate all viable ways to segment training utterances. %Then, these segmentations are leveraged as distant supervision for training the utterance segmentation model and fine-tuning the base neural semantic parser.}  In summary, our proposed framework has four advantages:  the base parser learns to parse simpler spans instead of whole complex utterances, thus alleviating the training difficulties and improving the compositional generalization ability;  our framework is flexible to incorporate various popular encoder-decoder models as the base parser;  our framework does not require any handcraft templates or additional labeled data for utterance segmentation; % our framework addresses the challenge of lacking labeled data for utterance segmentation through cooperative training.  our framework improves the interpretability of neural semantic parsing by providing explicit alignment between spans and partial meaning representations.  We conduct experiments on three datasets: Geo~, ComplexWebQuestions~, and Formulas . They use different forms of meaning representations: FunQL, SPARQL, and Spreadsheet Formula. Experimental results show that our framework consistently improves the performances of neural semantic parsers in different domains. On data splits that require compositional generalization, our framework brings significant accuracy gain: Geo , Formulas , ComplexWebQuestions .    
"," Neural semantic parsers usually fail to parse long and complex utterances into correct meaning representations, due to the lack of exploiting the principle of compositionality. To address this issue, we present a novel framework for boosting neural semantic parsers via iterative utterance segmentation. Given an input utterance, our framework iterates between two neural modules: a segmenter for segmenting a span from the utterance, and a parser for mapping the span into a partial meaning representation. Then, these intermediate parsing results are composed into the final meaning representation. One key advantage is that this framework does not require any handcraft templates or additional labeled data for utterance segmentation: we achieve this through proposing a novel training method, in which the parser provides pseudo supervision for the segmenter. Experiments on Geo, ComplexWebQuestions and Formulas show that our framework can consistently improve performances of neural semantic parsers in different domains. On data splits that require compositional generalization, our framework brings significant accuracy gains: Geo $63.1\to 81.2$, Formulas $59.7\to 72.7$, ComplexWebQuestions $27.1\to 56.3$.",346
"     Word alignment is a task of finding the corresponding words in a sentence pair  and used to be a key component of statistical machine translation . Although word alignment is no longer explicitly modeled in neural machine translation , it is often leveraged to interpret and analyze NMT models . Word alignment is also used in many other scenarios, such as imposing lexical constraints on the decoding process , improving automatic post-editing  and providing guidance for translators in computer-aided translation .  Recently, unsupervised neural alignment methods have been studied and outperformed GIZA++  on many alignment datasets . However, these methods are trained with a translation objective, which computes the probability of each target token conditioned on source tokens and previous target tokens. This will bring noisy alignments when the prediction is ambiguous . To alleviate this problem, previous studies modify Transformer  by adding alignment modules to re-predict the target token , or computing an additional alignment loss on the full target sequence . Moreover, \citet{chen2020accurate} propose an extraction method that induces alignment when the to-be-aligned target token is the decoder input.   Although these methods have demonstrated their effectiveness, they have two drawbacks. First, they retain the translation objective which is not tailored for word alignment. Consider the example in Figure . When predicting target token ``Tokyo'', the translation model may wrongly generate ``1968'' as it only considers the previous context, which will result in an incorrect alignment link . A better modeling is needed for obtaining more accurate alignments. Second, they need an additional guided alignment loss  to outperform GIZA++, which requires inducing alignments for entire training corpus.  In this paper, we propose a self-supervised model specifically designed for the word alignment task, namely Mask-Align. Our model masks each target token and recovers it with the source and the rest of the target tokens. For example, as shown in Figure , the target token ``Tokyo'' is masked and re-predicted. During this process, our model can identify that only the source token ``Tokio'' has not been translated yet, so the to-be-predicted target token ``Tokyo'' is aligned to ``Tokio''. Comparing with the translation model, this masked modeling method is highly related to word alignment, and based on that our model generates more accurate predictions and alignments.  % We model the target token conditioned on all other tokens in both source and target, which will disambiguate the prediction and thus lead to an accurate alignment ). As the vanilla transformer architecture requires sequential time to model this probability, we modify the attention in the decoder by separating the queries from keys and values and  % updating only the former in each layer. This allows our model to predict all target tokens in a single forward pass without information leakage. Besides, we also propose a variant of attention called leaky attention that allieviates the unexpected high attention weights on some specific tokens such as periods, which is helpful for the alignment extraction from attention matrix. Finally, we leverage the attention weights from the models in two directions by incorporating an agreement loss in the training process.  % Experiments on four public datasets show that our model significantly outperforms all existing statistical and neural methods without using guided alignment loss.  To summarize, the main contributions of our work are listed as follows:    
"," Neural word alignment methods have received increasing attention recently. These methods usually extract word alignment from a machine translation model. However, there is a gap between translation and alignment tasks, since the target future context is available in the latter. In this paper, we propose Mask-Align, a self-supervised model specifically designed for the word alignment task. Our model parallelly masks and predicts each target token, and extracts high quality alignments without any supervised loss. In addition, we introduce leaky attention to alleviate the problem of unexpected high attention weights on special tokens. Experiments on four language pairs show that our model significantly outperforms all existing unsupervised neural baselines and obtains new state-of-the-art results.  % However, the original translation objective ignores the future context in the target, which is available in the alignment task.",347
" The sequence-to-sequence  models~, which learn to map an arbitrary-length input sequence to another arbitrary-length output sequence, have successfully tackled a wide range of language generation tasks. % including machine translation, text summarization, question generation, to name a few.  Early seq2seq models have used recurrent neural networks to encode and decode sequences, leveraging attention mechanism  that allows the decoder to attend to a specific token in the input sequence to capture long-term dependencies between the source and target sequences. Recently, the Transformer~, which is an all-attention model that effectively captures long-term relationships between tokens in the input sequence as well as across input and output sequences, has become the de facto standard for most of the text generation tasks due to its impressive performance. Moreover, Transformer-based language models trained on large text corpora  have shown to significantly improve the model performance on text generation tasks. %Seq2seq tasks are becoming increasingly more important, as  show that most of text-based language problems can be cast into sequence-to-sequence problems.  However, a crucial limitation of seq2seq models is that they are mostly trained only with teacher forcing, where ground truth is provided at each time step and thus  never exposed to incorrectly generated tokens during training ), which hurts its generalization. This problem is known as the ``exposure bias"" problem  and often results in the generation of low-quality texts on unseen inputs. Several prior works tackle the problem, such as using reinforcement learning  to maximize non-differentiable reward . %  --- BLEU or Rouge.   Another approach is to use RL or gumbel softmax  to match the distribution of generated sentences to that of the ground truth, in which case the reward is the discriminator output from a Generative Adversarial Network  . Although the aforementioned approaches improve the performance of the seq2seq models on text generation tasks, they either require a vast amount of effort in tuning hyperparameters or stabilize training. %Moreover,  show that RL methods for machine translation often do not optimize the expected reward and the performance gain is attributed to the side effects, such as increasing the peakiness of the output distribution.      In this work, we propose to mitigate the exposure bias problem with a simple yet effective approach, in which we contrast a positive pair of input and output sequence to negative pairs, to expose the model to various valid or incorrect sentences. Na鑼倂ely, we can construct negative pairs by simply using random non-target sequences from the batch~. However, such a na鑼倂e construction yields meaningless negative examples that are already well-discriminated in the embedding space ), which we highlight as the reason why existing methods~ require large batch size. This is clearly shown in Fig., where a large portion of positive-negative pairs can be easily discriminated without any training, which gets worse as the batch size decreases as it will reduce the chance to have meaningfully difficult examples in the batch. Moreover, discriminating positive and na鑼倂e negative pairs becomes even more easier for models pretrained on large text corpora.   To resolve this issue, we propose principled approaches to automatically generate negative and positive pairs for constrastive learning, which we refer to as Contrastive Learning with Adversarial Perturbation for Seq2seq learning . Specifically, we generate a negative example by adding a small perturbation to the hidden representation of the target sequence, such that its conditional likelihood is minimized ). Conversely, we construct an additional positive example ) by adding a large amount of perturbation to the hidden representation of target sequence such that the perturbed sample is far away from the source sequence in the embedding space, while enforcing it to have high conditional likelihood by minimizing Kullback-Leibler  divergence between the original conditional distribution and perturbed conditional distribution. This will yield a negative example that is very close to the original representation of target sequence in the embedding space but is largely dissimilar in the semantics, while the generated positive example is far away from the original input sequence but has the same semantic as the target sequence. This will generate difficult examples that the model fails to correctly discriminate , Fig.2), helping it learn with more meaningful pairs.  To verify the efficacy of our method, we empirically show that it significantly improves the performance of seq2seq model on three conditional text generation tasks, namely machine translation, text summarization and question generation. Our contribution in this work is threefold:     
"," Recently, sequence-to-sequence  models with the Transformer architecture have achieved remarkable performance on various conditional text generation tasks, such as machine translation. However, most of them are trained with teacher forcing with the ground truth label given at each time step, without being exposed to incorrectly generated tokens during training, which hurts its generalization to unseen inputs, that is known as the ``exposure bias"" problem. In this work, we propose to mitigate the conditional text generation problem by contrasting positive pairs with negative pairs, such that the model is exposed to various valid or incorrect perturbations of the inputs, for improved generalization. However, training the model with na閼煎俥 contrastive learning framework using random non-target sequences as negative examples is suboptimal, since they are easily distinguishable from the correct output, especially so with models pretrained with large text corpora. Also, generating positive examples requires domain-specific augmentation heuristics which may not generalize over diverse domains. To tackle this problem, we propose a principled method to generate positive and negative samples for contrastive learning of seq2seq models. Specifically, we generate negative examples by adding small perturbations to the input sequence to minimize its conditional likelihood, and positive examples by adding  large perturbations while enforcing it to have a high conditional likelihood. Such ``hard'' positive and negative pairs generated using our method guides the model to better distinguish correct outputs from incorrect ones. We empirically show that our proposed method significantly improves the generalization of the seq2seq on three text generation tasks --- machine translation, text summarization, and question generation.",348
"   %缁楊兛绔村▓纰夌窗headline瀵板牓鍣哥憰 With the rapid growth of information spreading throughout the Internet, readers get drown in the sea of documents, and will only pay attention to those articles with attractive headlines that can catch their eyes at first sight. On one hand, generating headlines that can trigger high click-rate is especially important for different avenues and forms of media to compete for user's limited attention. On the other hand, only with the help of a good headline, can the outstanding article be discovered by readers.                     %缁楊兛绗佸▓纰夌窗閹存垳婊戦惃鍕侀崹瀣簼绠為幀搴濈疄閸 To generate better headlines, we first analyze what makes the headlines attractive. By surveying hundreds of headlines of popular websites, we found that one important feature that influences the attractiveness of a headline is its content. For example, when reporting the same event, the headline ``Happy but not knowing danger: Children in India play on the poisonous foam beach'' wins over 1000 page views, while the headline ``Chennai beach was covered with white foam for four days in India'' only has 387 readers. The popular headline highlights the fact that ``the beach is poisonous and affects children'',  which will concern more people than ``white foam''. On the other hand, the style of the headline also has a huge impact on attractiveness. For example, the headline ``Only two people scored thousand in the history of NBA Finals'' attracts fewer people than the headline ``How hard is it to get 1000 points in the NBA finals? Only two people in history!'', due to its conversational style that makes readers feel the need to see the answer to this question.      %缁楊兛绨╁▓纰夌窗challenge:婵″倷缍嶉惌銉╀壕attractive Most of the recent researches regard the headline generation task merely as a typical summarization task . This is not sufficient because a good headline should not only capture the most relevant content of an article but also be attractive to the reader. However, attractive headline generation tasks were paid less attention by researchers. \citet{xu2019clickbait} tackle this task by adversarial training, using an attractiveness score module to guide the summarization process. \citet{jin2020hooks} introduce a parameter sharing scheme to disentangle the attractive style from the attractive text. However, previous works neglect the fact that attractiveness is not just about style, but also about content. %     the negative samples generated by the pre-trained model can be non-fluent, inconsistent, and incoherent, which makes it difficult for the scorer to learn the attractiveness standard given such huge noise.           Based on the above analysis, we propose a model named Disentanglement-based Attractive Headline Generation , which learns to write attractive headlines from both style and content perspectives.  These two attractiveness attributes are learned from an attractive prototype headline, \ie the headline of the document in the training dataset that is most similar to the input document. First, DAHG separates the attractive style and content of the prototype headline into latent spaces, with two auxiliary constraints to ensure the two spaces are indeed disentangled. Second, the learned attractive content space is utilized to iteratively polish the input document, emphasizing the parts in the document that are attractive. Finally, the decoder generates an attractive headline from the polished input document representation under the guidance of the separated attractive style space. Extensive experiments on the public Kuaibao dataset show that DAHG outperforms the summarization and headline generation baselines in terms of ROUGE metrics, BLEU metrics, and human evaluations by a large margin. Specifically, DAHG triggers 22\% more clicks than the strongest baseline.           %缁楊剙娲撳▓纰夌窗閹崵绮╟ontribution The major contributions of this paper are as follows:   We devise a disentanglement mechanism to divide the attractive content and style space from the attractive prototype headline.  We propose to generate an attractive headline with the help of disentangled content space under the style guidance.  Experimental results demonstrate that our model outperforms other baselines in terms of both automatic and human evaluations.      
"," Eye-catching headlines function as the first device to trigger more clicks, bringing reciprocal effect between producers and viewers. Producers can obtain more traffic and profits, and readers can have access to outstanding articles. When generating attractive headlines, it is important to not only capture the attractive content but also follow an eye-catching written style.  In this paper, we propose a Disentanglement-based Attractive Headline Generator  that generates headline which captures the attractive content following the attractive style. Concretely, we first devise a disentanglement module to divide the style and content of an attractive prototype headline into latent spaces, with two auxiliary constraints to ensure the two spaces are indeed disentangled. The latent content information is then used to further polish the document representation and help capture the salient part. %The latent attractive content space further helps to distill salient and attractive knowledge from the input document. Finally, the generator takes the polished document as input to generate headline under the guidance of the attractive style.  Extensive experiments on the public Kuaibao dataset show that DAHG achieves state-of-the-art performance.  Human evaluation also demonstrates that DAHG triggers 22\% more clicks than existing models.",349
"  Task-specific finetuning of pretrained deep networks has become the dominant paradigm in contemporary NLP, achieving state-of-the-art results across a suite of natural language understanding tasks . While straightforward and empirically effective, this approach is difficult to scale to multi-task, memory-constrained settings , as it requires shipping and storing a full set of model parameters for each task. Inasmuch as these models are learning generalizable, task-agnostic language representations through self-supervised pretraining, finetuning the entire model for each task seems especially profligate.   A popular approach to parameter-efficiency with pretrained models is to learn sparse models for each task where a subset of the final model parameters  are exactly zero~. Such approaches often face a steep sparsity/performance tradeoff, and a substantial portion of nonzero parameters  are still typically required to match the performance of the dense counterparts. An alternative is to use multi-task learning or feature-based transfer for more parameter-efficient transfer learning with pretrained models~. These methods learn only a small number of additional parameters  on top of a shared model. However, multi-task learning generally requires access to all tasks during training to prevent catastrophic forgetting~, while feature-based transfer learning  is typically outperformed by full finetuning~.    Adapters~ have recently emerged as a promising approach to parameter-efficient transfer learning within the pretrain-finetune paradigm~.  Adapter layers are smaller, task-specific modules that are inserted between layers of a pretrained model, which remains fixed and is shared across tasks.  These approaches do not require access to all tasks during training, making them attractive in settings where one hopes to obtain and share performant models as new tasks arrive in stream.  \citet{houlsby2019adapters} find that adapter layers trained on BERT can match the performance of fully finetuned BERT on the GLUE benchmark  while only requiring 3.6\% additional parameters  per task.   In this work, we consider a similar setting as adapters but propose a new diff pruning approach with the goal of even more parameter-efficient transfer learning.  Diff pruning views finetuning as learning a task-specific \underline{diff}erence  vector%\footnote{Similar to the  command in Unix operating systems.}    \ that is applied on top of the pretrained parameter vector, which remains fixed and is shared across different tasks.   In order to learn this vector, we reparameterize the task-specific model parameters as , where the pretrained parameter vector  is fixed  and  the task-specific diff vector  is finetuned. The diff vector is regularized with a differentiable approximation to the -norm penalty~ to encourage sparsity. This approach can become  parameter-efficient as the number of tasks increases as it only requires storing the nonzero positions and weights of the diff vector for each task. The cost of storing the shared pretrained model remains constant and is amortized across multiple tasks.  On the GLUE benchmark~, diff pruning can match the performance of the fully finetuned BERT baselines  while finetuning only  of the pretrained parameters per task, making it a potential alternative to adapters for parameter-efficient transfer learning.   
"," While task-specific finetuning of pretrained networks has led to significant empirical advances in NLP, the large size of networks makes finetuning difficult to deploy in multi-task, memory-constrained settings. We propose diff pruning as a simple approach to enable parameter-efficient transfer learning within the pretrain-finetune framework. This approach views finetuning as learning a task-specific ``diff"" vector that is applied on top of the pretrained parameter vector, which remains fixed and is shared across different tasks. The diff vector is adaptively pruned during training with a differentiable approximation to the $L_0$-norm penalty to encourage sparsity. Diff pruning becomes parameter-efficient as the number of tasks increases, as it requires storing only the nonzero positions and weights of the diff vector for each task, while the cost of storing the shared pretrained model remains constant. It further does not require access to all tasks during training, which makes it attractive in settings where tasks arrive in stream or the set of tasks is unknown. We find that models finetuned with diff pruning can match the performance of fully finetuned baselines on the GLUE benchmark while only modifying 0.5$\%$ of the pretrained model's parameters per task.\blfootnote{ \hspace{-6mm} Our code is available at \url{https://github.com/dguo98/DiffPruning}}",350
"   Goal-oriented dialogue systems is a hot topic in machine learning research. The systems have widespread applications in the industry and are the foundation of many successful products, including Alexa, Siri, Google Assistant, and Cortana. One core component of a dialog system is spoken language understanding , which consists of two main problems, intent classification  and slot labeling  . In IC, we attempt to classify the goal of a user query, usually input in text or transcribed by automatic speech recognition  system from audio. SL, similar to the named-entity recognition  problem, aims to label each token in a query an entity type. The only difference is that entity types in SL are domain-specific and based upon dialog ontology. Recent advances in neural models have enabled greatly improved SLU .  However, two significant challenges hinder the broad application and expansion of the SLU models in industrial settings. First of all, neural methods require a large amount of labeled data for training . SLU is often coupled with the ontology of the underlying dialog system and thus domain-dependent. Collecting a large number of in-domain labeled data for neural models is prohibitively expensive and time-consuming. Secondly, the performance of SLU models in practice often suffers from fluctuations due to various types of noises. One common noise is adaptation data perturbation. In many industrial applications such as cloud services\footnote{Alexa ASK: https://developer.amazon.com/en-US/alexa/alexa-skills-kit; Google DialogFlow: https://dialogflow.com/}, the SLU model is built by fine-tuning  a pre-trained, shared network to the target domain with data provided by developers. The developers often have a limited background in SLU and machine learning. Thus the data provided varies in quality and is subject to different types of perturbations, such as missing or replaced data samples  and typos. Another common noise comes from the mismatch of input modalities between adaptation and inference stages. For instance, the model is adapted with human transcription yet deployed to understand ASR decoded text, or the input at adaptation and inference stages relies on the recognition of different versions of ASR models. Given that most neural methods comprise a large number of parameters and are heavily optimized for the training  data provided, the resulting model is usually sensitive to these noises. The requirement of noise-free adaptation and inference conditions also prohibits the use of neural SLU techniques because it is often infeasible to achieve such conditions.  Transfer learning and meta-learning are two conventional techniques that have been applied to address the challenge of data scarcity. Transfer learning usually refers to pre-training initial models using mismatched domains with rich human annotations and then adapting the models with limited labels in targeted domains. Previous works  have shown promising results in applying transfer learning to SLU. Note that pre-training discussed here covers methods including using a pre-trained language model like BERT  directly and further training downstream tasks on data in mismatched domains with the pre-trained model. In the following, we focus on the latter due to utilizing data from other domains better and yielding higher accuracy. In recent years, meta-learning has gained growing interest among the machine learning fields for tackling few-shot learning  scenarios. Model-Agnostic Meta-Learning   focuses on learning parameter initialization from multiple subtasks, such that the initialization can be fine-tuned with few labels and yield good performance in targeted tasks. Metric-based meta-learning, including prototypical networks   and matching networks , aim to learn embedding or metric space which can be generalized to domains unseen in the training set after adaptation with a small number of examples from the unseen domains. Recent work unveils excellent potential in applying meta-learning techniques to SLU in the few-shot learning context .  As compared to data scarcity, another challenge for SLU, the robustness against noises, is also gaining attention. Simulated ASR errors are used to augment training data for SLU models . Researchers also leverage information from confusion networks or lattices , and adversarial training techniques  for models to learn query embeddings that are robust against ASR errors. For text input, methods have also been explored on model robustness against noises from misspelling and acronym . In contrast to these noise types that have gained attention, to our best knowledge, there is no prior work investigating the impact of missing or replaced examples in adaptation data. Moreover, the intersection of data scarcity and noise robustness is unexplored. Since the scarcity of labeled data and data noisiness usually co-occur in SLU applications , the lack of studies in the intersectional areas hinders the use of neural SLU models and its expansion to broader use cases.  Given the deficiency, we establish a novel few-shot noisy SLU task by introducing two common types of natural noise, adaptation example missing/replacing and modality mismatch, to the previously defined few-shot IC/SL splits . The task is built upon three public datasets, ATIS , SNIPS , and TOP . We further propose a noise-robust few-shot SLU model based on ProtoNets for the established task. In summary, our primary contributions are 3-fold: 1) formulating the first few-shot noisy SLU task and evaluation framework, 2) proposing the first working solution for the few-shot noisy SLU with the existing ProtoNet algorithm, and 3) in the context of noisy and scarce learning examples, comparing the performance of the proposed method with conventional techniques, including MAML and fine-tuning based adaptation.   
","    Recently deep learning has dominated many machine learning areas, including spoken language understanding . However, deep learning models are notorious for being data-hungry, and the heavily optimized models are usually sensitive to the quality of the training examples provided and the consistency between training and inference conditions. To improve the performance of SLU models on tasks with noisy and low training resources, we propose a new SLU benchmarking task: few-shot robust SLU, where SLU comprises two core problems, intent classification  and slot labeling . We establish the task by defining few-shot splits on three public IC/SL datasets, ATIS, SNIPS, and TOP, and adding two types of natural noises  to the splits. We further propose a novel noise-robust few-shot SLU model based on prototypical networks. We show the model consistently outperforms the conventional fine-tuning baseline and another popular meta-learning method, Model-Agnostic Meta-Learning , in terms of achieving better IC accuracy and SL F1, and yielding smaller performance variation when noises are present.",351
"  In the modern world, social media is playing its part in several ways, for instance in news dissemination and information sharing, social media outlets, such as Twitter, Facebook, and Instagram, have been proved very effective . However, it also comes with several challenges, such as collecting information from several sources, detecting and filtering misinformation . Similar to other events and pandemics, being one of the deadly pandemics in the history, COVID-19 has been the subject of discussion over social media since its emergence. Without any surprise, a lot of misinformation about the pandemic are circulated over social networks. In order to identify misinformation spreaders and filter fake news about COVID-19 and 5G conspiracy, a task namely ""FakeNews: Corona Virus and 5G Conspiracy Multimedia Twitter-Data-Based Analysis"" has been proposed in the benchmark MediaEval 2020 competition .   This paper provides a detailed description of the methods proposed by team DCSE\_UETP for the fake news detection task. The task consists of two parts, namely  text-based misinformation detection , and  structure-based misinformation detection . The first task  is based on textual analysis of COVID-19 related information shared on Twitter during January 2020 and 15th of July 2020, and aims to detect different types of conspiracy theories about COVID-19 and its vaccines, such as that ""the 5G weakens the immune system and thus caused the current corona-virus pandemic etc., . In the SMD task, the participants are provided with a set of graphs, each representing a sub-graph of Twitter, and corresponds to a single tweet where the vertices of the graphs represent accounts. Similar to TMD, in this task, the participants need to detect and differentiate between 5G and other COVID-19 conspiracy theories.   
"," The paper presents our solutions for the MediaEval 2020 task namely FakeNews: Corona Virus and 5G Conspiracy Multimedia Twitter-Data-Based Analysis. The task aims to analyze tweets related to COVID-19 and 5G conspiracy theories to detect misinformation spreaders. The task is composed of two sub-tasks namely  text-based, and  structure-based fake news detection. For the first task, we propose six different solutions relying on Bag of Words  and BERT embedding. Three of the methods aim at binary classification task by differentiating in 5G conspiracy and the rest of the COVID-19 related tweets while the rest of them treat the task as ternary classification problem. In the ternary classification task, our BoW and BERT based methods obtained an F1-score of .606\% and .566\% on the development set, respectively. On the binary classification, the BoW and BERT based solutions obtained an average F1-score of .666\% and .693\%, respectively. On the other hand, for structure-based fake news detection, we rely on Graph Neural Networks  achieving an average ROC of .95\% on the development set.",352
"   Recurrent neural networks are the basis of the state-of-the-art models in natural language processing, including language modeling , machine translation  and named entity recognition . It is needless to say that complex learning tasks require relatively large networks with millions of parameters to be accomplished. However, large neural networks need more data and/or strong regularization techniques to be trained successfully and avoid overfitting. Without the means to collect more data, which is the case in the majority of real-world problems, data augmentation and regularization methods are standard alternative practices to overcome this barrier.  Data augmentation in natural language processing is limited, and often task-specific . On the other hand, adopting the same regularization methods that are originally proposed for feed-forward  networks needs to be done with extra care to avoid hurting the network's information flow between consecutive time-steps. To overcome such limitations, we present Sequence Mixup: a set of training methods, regularization techniques, and data augmentation procedures for RNNs. Sequence Mixup can be considered as the RNN-generalization of input mixup  and manifold mixup , which are already introduced for feed-forward neural networks. Generally speaking, the core idea behind mixup strategies is to mix training samples in the network's input or hidden layers, where by mix, we simply mean to consider random convex combinations of pairs of samples as alternatives for the actual training data points. Mixup in non-recurrent networks has led to smoother decision boundaries, more robustness to adversarial examples, and better generalization compared to many rival regularization methods . Here, we extend input mixup to RNNs and also propose two variants of manifold mixup, namely Pre-Output Mixup  and Through-Time Mixup , where mixing occurs in the hidden space of the RNN. POM and TTM differ from each other in the way information flow is passed from one time-step to the next.  In order to elucidate the effect of sequence mixup during the learning stage, consider the classification of half-moons data plotted in figure  with a simple two-timestep RNN. We have also added some levels of noise to the original data points to make the classification task more challenging. Figures  and  show the learned decision boundaries from noisy data via regular training and Pre-Output Mixup, respectively. As can be seen, mixup expands the margin between the classes and increases the decision boundary levels, which in turn renders a smoother decision boundary with less certainty about nearby cross-class samples. Intuitively speaking, this type of training creates artificial samples whose labels and hidden states are obtained from intermixing those of the original samples, in a respective manner. Based on our experiments, applying sequence mixup has improved both the test F-1 score and loss of BiLSTM-CRF model  on CoNLL-03 data  .     We have also provided a theoretical analysis on the impact of our regularization techniques in the asymptotic regime where network widths become increasingly large, and learning rates become infinitesimally small. In a nutshell, our analysis reveals that as long as the number of hidden state neurons, which we denote by  in this work, is less than the number of distinct classes in a classification problem, both POM and TTM cannot achieve a zero training error regardless of how large the training dataset is or how deep the neural networks become. Moreover, we show that as long as  is less than twice the number of classes, the hidden-state generating section of the RNN acts as a memoryless unit and produces hidden states that are almost independent of previous time-steps. On the other hand, given that  is chosen sufficiently large, both POM and TTM are able to divide the hidden representation space of the RNN into a set of orthogonal affine subspaces, where each subspace is an indicator of a unique class. We refer to this property as spectral compression of sequence mixup, which is a similar behaviour to that of manifold mixup for feed-forward networks.  The rest of the paper is organized as follows: Section  reviews a number of related works to this problem. In Section , we propose Sequence Mixup, describe its challenges and specifications in detail, and also present our theoretical analysis.  Section  is devoted to our experiments on real-world data. Finally, Section  concludes the paper. %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
"," In this paper, we extend a class of celebrated regularization techniques originally proposed for feed-forward neural networks, namely Input Mixup \citep{zhang2017mixup} and Manifold Mixup \citep{verma2018manifold}, to the realm of Recurrent Neural Networks . Our proposed methods are easy to implement and have a low computational complexity, while leverage the performance of simple neural architectures in a variety of tasks. We have validated our claims through several experiments on real-world datasets, and also provide an asymptotic theoretical analysis to further investigate the properties and potential impacts of our proposed techniques. Applying sequence mixup to BiLSTM-CRF model \citep{huang2015bidirectional} to Named Entity Recognition task on CoNLL-2003 data \citep{sang2003introduction} has improved the F-1 score on the test stage and reduced the loss, considerably. \blfootnote{Emails: \{karamzade,najafy\}@ce.sharif.edu,~motahari@sharif.edu} \blfootnote{An implementation of our method is avaiable at \href{https://github.com/ArminKaramzade/SequenceMixup}{https://github.com/ArminKaramzade/SequenceMixup.}}",353
" Sentiment classification is the task of analyzing a piece of text to predict the orientation of the attitude towards an event or opinion. The sentiment of a text can be either positive or negative. Sometimes, a neutral perspective is also considered for classification. SA has many different applications, such as reducing the early age suicide rate by identifying cyberbullying , discouraging unwarranted activities towards a particular community through hate-speech detection , and monitoring public response towards a proposed government bill  among many others.    The task of SA has achieved superior improvement in other languages, i.e. English - about 97.1\% accuracy for 2-class  and 91.4\% accuracy for 3-class SA . But only a few research works have been published for the SA in Bengali. This is because we lack quality datasets in Bengali for training a computation model for the sentiment classification. However, in the last few years, we have seen the rise of Internet users in the Bengali domain mostly due to the development of wireless network infrastructure throughout South East Asia. This resulted in a massive increase in the total number of online social network users as well as newspaper readers. So it became comparatively easier to collect the public comments posted online on the Bengali news websites.    %  \end{table}  Thus we created two SA datasets for 2-class and 3-class SA in Bengali and trained a multi-lingual BERT model via transfer learning approach for sentiment classification in Bengali, referred as  in this paper.  achieves an accuracy of 71\% for the 2-class and 60\% for the 3-class manually tagged dataset. We further use this model to analyze the sentiment of 1,002 public comments collected from the online daily newspaper. Table  shows that in general, sentiment in public comments is positive for religious news articles, while that is negative for political or sports news articles. In this paper, we present the following contributions:    % \makeatletter % \patchcmd{\@makecaption} %   {\scshape} %   {} %   {} %   {} % \makeatletter % \patchcmd{\@makecaption} %   {\\} %   {.\ } %   {} %   {} % \makeatother % \def\tablename{Table}   
"," Sentiment analysis  in Bengali is challenging due to this Indo-Aryan language's highly inflected properties with more than 160 different inflected forms for verbs and 36 different forms for noun and 24 different forms for pronouns. The lack of standard labeled datasets in the Bengali domain makes the task of SA even harder. In this paper, we present manually tagged 2-class and 3-class SA datasets in Bengali. We also demonstrate that the multi-lingual BERT model with relevant extensions can be trained via the approach of transfer learning over those novel datasets to improve the state-of-the-art performance in sentiment classification tasks. This deep learning model achieves an accuracy of 71\% for 2-class sentiment classification compared to the current state-of-the-art accuracy of 68\%. We also present the very first Bengali SA classifier for the 3-class manually tagged dataset, and our proposed model achieves an accuracy of 60\%. We further use this model to analyze the sentiment of public comments in the online daily newspaper. Our analysis shows that people post negative comments for political or sports news more often, while the religious article comments represent positive sentiment. The dataset and code is publicly available \footnote{ https://github.com/KhondokerIslam/Bengali\_Sentiment}.",354
"  Methods for automatically learning phone- or word-like units from unlabelled speech audio could enable speech technology in severely low-resourced settings and could lead to new cognitive models of human language acquisition. The goal in unsupervised representation learning of phone units is to learn features which capture phonetic contrasts while being invariant to properties like the speaker or channel. Early approaches focussed on learning continuous features. In an attempt to better match the categorical nature of true phonetic units, more recent work has considered discrete representations. One approach is to use a self-supervised neural network with an intermediate layer that quantizes features using a learned codebook. While the discrete codes from such vector quantized  networks have given improvements in intrinsic phone discrimination tasks, they still encode speech at a much higher bitrate than true phone sequences.  As an example, the top of Figure shows the code indices from a vector-quantized variational autoencoder  overlaid on the input spectrogram. While there is some correspondence between the code assignments and the true phones , and although there is some repetition of codes in adjacent frames , the input speech are often assigned to codes that are distinct from those of surrounding frames. This is not surprising since the VQ model is not explicitly encouraged to do so. The result is an encoding at a much higher bitrate  than that of true phone sequences .    In this paper we consider ways to constrain VQ models so that contiguous feature vectors are assigned to the same code, resulting in a low-bitrate segmentation of the speech into discrete units. We specifically compare two VQ segmentation methods. Both of these are based on a recent method for segmenting written character sequences. The first method is a greedy approach, where the closest adjacent codes are merged until a set number of segments are reached. The second method allows for an arbitrary number of segments. A squared error between blocks of feature vectors and VQ codes are used together with a penalty term encouraging longer-duration segments. The optimal segmentation is found using dynamic programming.  We apply these two segmentation approaches using the encoders and codebooks of the two VQ models from . The first is a type of VQ-VAE. The second is a vector-quantized contrastive predictive coding  model. The combination of these two models with the two segmentation approaches gives a total of four VQ segmentation models to consider. %Applying both these models with both segmentation approaches gives a total of four model combinations. We evaluate these on four different tasks: unsupervised phone segmentation, ABX phone discrimination, same-different word discrimination, and as inputs to a symbolic word segmentation algorithm. The last-mentioned is particularly important since the segmentation and clustering of % word-like units %from unlabelled speech remains a major but important challenge.  On most metrics in the four tasks the combination of the VQ-VAE with the penalized dynamic programming approach is the best VQ segmentation method. Example output is shown in the middle of Figure. Compared to other existing methods, it does not achieve state-of-the-art performance in all four evaluation tasks. However, it achieves reasonable performance at a much lower bitrate than most existing methods. This is noteworthy since, while most of the other methods have been tailored to the respective tasks, a single VQ segmentation approach can be used without any alteration directly to a range of problems. 
"," We investigate segmenting and clustering speech into low-bitrate phone-like sequences without supervision. We specifically constrain pretrained self-supervised vector-quantized~ neural networks so that blocks of contiguous feature vectors are assigned to the same code, thereby giving a variable-rate segmentation of the speech into discrete units. Two segmentation methods are considered. In the first, features are greedily merged until a prespecified number of segments are reached. The second uses dynamic programming to optimize a squared error with a penalty term to encourage fewer but longer segments. We show that these VQ segmentation methods can be used without alteration across a wide range of tasks: unsupervised phone segmentation, ABX phone discrimination, same-different word discrimination, and as inputs to a symbolic word segmentation algorithm. The penalized method generally performs best. While results are only comparable to the state-of-the-art in some cases, in all tasks a reasonable competing approach is outperformed at a substantially lower bitrate.",355
"   Natural language has provided a key cohesive ingredient for pushing the boundaries of technological advances beyond individuals to the 4th industrial revolution. In textual form, it provides a long term, stable, knowledge base, which can be used to preserve knowledge across generations. Digital evolution in the last century has greatly accelerated this preservation process and provided a means to extract hidden meaning and information from texts, largely considered illegible and indecipherable for human beings. Natural Language Processing  is a divergent field, with state-of-the-art research initiative looking towards resolving the various challenges of automatic information extraction. Foremost, amongst these challenges is the ability to identify the various concepts and their relationship, which form the epitome of the target corpus . For humans and machines, cause-effect represents an essential relation, which provides ample support for the reasoning and decision making process . Automatic causality detection has benefited greatly from numerous dedicated research efforts . However, challenges such as the dynamicity of syntax and semantics, and in particular the evolution of vocabulary have hindered the development and usage of any generic and cross-domain solution . On the other hand, applications such as information retrieval , question answering , and event reasoning and predictions  have gained valuable improvements through the identification of cause-effect relationships. \\ The commonly used approaches for causality detection, fall into two categories: pattern-based traditional rule bases, and machine learning based automatic classification and entity extraction . Pattern based approaches are based on partial or complete expert intervention for crafting and verifying the conditions based on the syntactic and semantic analysis of the corpus. This approach, requires intensive human effort and lacks cross-domain generalization. Even after utilizing a substantial amount of human time, the extracted rules cannot cover all possible linguistic patterns and are usually not usable beyond the original domain/corpus. Such an approach also suffers from the diversity in linguistic typology, leading to rules formed for a language based on the Subject-Verb-Object sentence structure  not being compatible with those based on other structures such as Subject-Object-Verb and others .\\ Automatic machine learning based approaches utilize labeled datasets for extracting causality relationships from unseen data and thereby requires less expert intervention, relatively. With this approach, most human time is spent on labeling the data and verifying the results, while providing a reusable model for cross-domain applications. However, any evolution of labels and change in text can render the model unusable. Additionally, machine learning models, are typically independent of the linguistic topology features and can be customized to work on any sentence structure albeit with some effort towards creating and optimizing language vectors, and incorporating natural heuristics derived from syntactically labelled  or a well distributed large corpus  .\\ A solution to managing change in the machine learning models and reducing the expert intervention is available as Transfer Learning, where the machine can learn a new tasks by reusing a foundational model, originally employed for a different but related task in another domain . Such a cross-domain application may not replicate the original performance benchmarks, and thereby requires some model tuning and tweaking before becoming useful. Model tuning is achieved with the help of a human expert who provides feedback to the machine learning model for improving its learning tasks, a technique more commonly known as active learning . To gain benefits of these two approaches active transfer learning is applied to various tasks in diverse domains , transferring similar models and improving its performance in a single workflow. This performance is mainly improved by enhancing the pre-trained model with few annotated dataset and expert involvement from the new domain.\\ Causality mining as an application of causality detection is typically based on two tasks, which includes identification of causal triggers, and causal pairs participating in each relationship . Also known as causal connectives; causal triggers are transitive verbs which form a bridge between causality concepts and identify the cause and its effect. Leveraging the sentence structuring in English language, typical causality relation identification methodologies, found in research literature, follow the Noun Phrase  - Verb  - NP pattern which corresponds to either Cause - Trigger - Effect or Effect - Trigger - Cause forms   . Based on this heuristic, Kaplan and Berry-Bogge  provided an early model for creating and using handcrafted linguistic template for causality detection. Kalpana Raja et al. , built upon the same idea in addition to identifying and organizing a dictionary based on causal trigger keywords, which was then used to define patterns for causality detection. R. Girju et al.  refined the process of identifying the causal verbs by utilizing the WordNet dictionary. Cole et al.  utilized a syntactic parser to convert the SVO structures into SVO triples, which were then passed through various rule based filters for causality detection. S. Zhao et al. , pointed towards the existence of diversity in the manner each causal trigger expresses causality. However, the syntactic structure of causal sentences and the way the trigger invokes the causality, can provide satisfactory categorization of the causal triggers, enabling smart application of the causality identification filters. Son Doan et al.  presented an application of causal mining by marking several verbs and nouns as causal triggers for extracting causal relations from twitter messages. Girju and Moldovan  proposed a semi-supervised approach towards causality relation identification by using the underlying linguistic patterns of the corpus.\\ Many other automatic causal pattern identification methodologies have relied on the evolution of machine learning models. In particular,  has presented a causal relation extraction model using unsupervised learning to detect the noun phrases corresponding to the subject and object of the sentence. By analysing an unannotated raw corpus and using Expected Maximization along with a Naive Bayes classifier, the authors were able to precisely identify 81.29\% of causal relations. \\ On the other hand, E. Blanco et al.  utilized a supervised learning approach by first annotating ternary instances as being a causal relation or not, and then applied Bagging with C4.5 decision trees to achieve a precision of over 95\% in causal relations ad above 86\% in non causal ones. These and many other machine learning approaches have been comprehensively classified by , which indicates a general trend towards the utilizing of the same, as the models become more mature and stable. Of particular interest are the word embedding methods, which due to their requirement of unsupervised data, scalability, and accuracy have piqued the interest of the NLP research community. \\ Several initiatives have already led to the state-of-the-art results in completing NLP tasks such as sentiment analysis, text classification, topic modeling, and relation extraction . Zeng et al.  classified relations in the SemEval Task 8 dataset using deep convolution neural networks . Nguyen et al.  introduced positional embedding to the input sentence vector in CNNs for improved relation extraction. Silva et al.  proposed a deep learning  based causality extraction methodology that can detect causality along with its direction. The author addressed the causality detection problem as a three class classification problem, where class 1 indicates the annotated pairs has causal relation with direction entity1  entity2, class 2 implies the causal relation has the direction entity2  entity1, and class 3 entities are non-causal.\\ Ning An et al.  has utilized a word embedding with cosine similarity based approach, which uses an initial causal seed list to identify the causal relationships as a multi-class  classification problem. With one-hot encoding the authors, convert the causal verbs in the seed list and the verbs identified in Noun Phrase-Verb Phrase-Noun Phrase ternary into encoding vectors. These vectors are then converted into Embedding vectors using Continuous Skip-Gram based on a Wikipedia dataset of 3.7 million articles. Finally the encoded vectors are then compared using cosine similarity and the pair with maximum similarity above a pre-defined threshold value of 0.5 are used to classify the causal relationship and evolve the seed list. This method achieved an average F-score of 78.67\%. While this methodology presents a significant improvement on previous research initiatives towards causal relationship identification, it suffers from low accuracy, due to its focus on causal verb identification based on a small initial seed list and its limited extension, and classification based, solely on these verbs meanwhile losing context of the causal phrase. \\ In this paper we present a novel causal relationship identification framework, which outperform, in the domain of causality mining in clinical text. This framework uses a multi-dimensional approach, which resolves syntactic and semantic matching problems in clinical textual data, providing causal knowledge which is useful to summarize clinical text for quick review, create patient personas for reapplication of medical procedures and predictive analysis, discovering medical knowledge from volumnous data sources, and provide evidence supporting clinical decision making.\\ This novel framework identifies causal phrases using automatic seed list generation from training data set, seed expansion using transfer learning, causal phrase generation, and BERT based phrase embedding and semantic matching. It then applies semantic enrichment on the causal phrases using Unified Medical Language System , to extend  healthcare terms with their semantic and uniquely identifiable corresponding codes. Finally, the trained model is evolved based on expert feedback, by employing active learning.\\  In the presented approach, we extracted the initial causal seed list from SemEval Task 8 dataset and expanded it by utilizing synonyms from WordNet dictionary, pre-trained Google News model , ConceptNet Numberbatch Model , and Facebook Fasttext Model . We then generated causal quads using dependency based linguistic patterns for identifying the subject, object, causal verb, and a confidence measure. Causal triples, under a threshold, were then filtered from the quads and converted into embedding vectors using BERT to create an initial model. This trained model was used to identify candidate causal triples in unseen textual data, which were then semantically enriched from UMLS and converted into a causal quad by augmenting a confidence score. The semantically enriched causal quads were then verified by an expert by increasing or decreasing the confidence value and used to evolve the trained model, iteratively.\\ This detailed methodology is presented in section , with details workflows in section  and its results following in section . Finally, section  will conclude the paper.        
"," Objective: Causality mining is an active research area, which requires the application of state-of-the-art natural language processing techniques. In the healthcare domain, medical experts create clinical text to overcome the limitation of well-defined and schema driven information systems. The objective of this research work is to create a framework, which can convert clinical text into causal knowledge. \\ Methods: A practical approach based on term expansion, phrase generation, BERT based phrase embedding and semantic matching, semantic enrichment, expert verification, and model evolution has been used to construct a comprehensive causality mining framework. This active transfer learning based framework along with its supplementary services, is able to extract and enrich, causal relationships and their corresponding entities from  clinical text.\\ Results: The multi-model transfer learning technique when applied over multiple iterations, gains performance improvements in terms of its accuracy and recall while keeping the precision constant. We also present a comparative analysis of the presented techniques with their common alternatives, which demonstrate the correctness of our approach and its ability to capture most causal relationships.\\ Conclusion: The presented framework has provided cutting-edge results in the healthcare domain. However, the framework can be tweaked to provide causality detection in other domains, as well. \\ Significance: The presented framework is generic enough to be utilized in any domain, healthcare services can gain massive benefits due to the voluminous and various nature of its data. This causal knowledge extraction framework can be used to summarize clinical text, create personas, discover medical knowledge, and provide evidence to clinical decision making.",356
" Content based websites such as Quora, Reddit, StackOverflow are primarily used for seeking genuine answers to questions. People from different domains put up their questions and educators or people knowledgeable in a certain field answer them. One major impediment to a plain sailing execution of information exchange is the proliferation of toxic comments. The key challenge is to weed out such toxic comments termed as Insincere Questions. An Insincere Question is designated as a comment intended to make a statement than to look for genuine answers.  An Insincere Question is characterised by:   This major class of problem pertains to Text classification which has been a benchmark problem of evaluating various research advancements in natural language processing. While traditional machine learning algorithms such as naive bayes, logistic regression and decision trees can be rightfully applied to this problem, they suffer with major impediments in their constructs. Vanilla RNNs, Gated Recurrent Unit and Long Short Term Memory Networks replaced their usage as the new state of the art. Even though LSTMs and GRUs performed well, they failed to capture the dependencies in long range sentences. Now with the advent of Transfer Learning, Language model pre-training has proven to be useful in learning universal language representations. Researchers in the field are developing new and better language models at an unprecedented speed. Applying these new state of the art models could improve current methods and replace manual labeling tasks for text classification, but also find widespread application in similar other fields, such as machine translation and question answering. In this paper, we test this by applying new transformer models from the BERT-family to improve the current method of binary text classification in the context of Insincere Questions Classification. We make use of the Quora Insincere Questions Classification dataset  for this purpose We find that all of our models achieve remarkable results in classifying the given  data , with BERT achieving the best results compared to RoBERTa, DistilBERT, and ALBERT. This indicates that the models are well equipped to take over tasks that researchers have previously solved in less optimal ways.    
","  The internet today has become an unrivalled source of information where people converse on content based websites such as Quora, Reddit, StackOverflow and Twitter asking doubts and sharing knowledge with the world. A major arising problem with such websites is the proliferation of toxic comments or instances of insincerity wherein the users instead of maintaining a sincere motive indulge in spreading toxic and divisive content. The straightforward course of action in confronting this situation is detecting such content beforehand and preventing it from subsisting online. In recent times Transfer Learning in Natural Language Processing has seen an unprecedented growth. Today with the existence of transformers and various state of the art innovations, a tremendous growth has been made in various NLP domains. The introduction of BERT has caused quite a stir in the NLP community. As mentioned, when published, BERT dominated performance benchmarks and thereby inspired many other authors to experiment with it and publish similar models. This led to the development of a whole BERT-family, each member being specialized on a different task. In this paper we solve the Insincere Questions Classification problem by fine tuning four cutting age models viz BERT, RoBERTa, DistilBERT and ALBERT",357
"     The term ``Readability'' measures how much energy the reader will have to expend in order to understand a writing at optimal speed and find interesting. Readability measuring formulas, such as Automated Readability Index  , Flesch Reading Ease , and Dale閳ユ弲hall formula  calculate a score that estimates the grade level or years of education of a reader based on the U.S. education system, which is illustrated in Figure . These formulas are still used in many widely known commercial readability measuring tools such as Grammarly and Readable. This measurement plays a significant role in many places, such as education, health care, and government . Government organizations use it to ensure that the official texts meet a minimum readability requirement. For instance, the Department of Insurance at Texas has a requirement that all insurance policy documents have a Flesch Reading Ease  score of 40 or higher, which translates to the reading level of a first-year undergraduate student based on the U.S. education system. A legal document which is hard to read can lead someone to sign a contract without understanding what they are agreeing to. Another common usage area is the healthcare sector to ensure the proper readability of the care and treatment documents . Better readability will attract visitors or readers of different websites or blogs, whereas poor readability may decrease the number of readers . Readability measures are also often used to assess the financial documents such as annual reports of a company閳ユ獨 economic performance so that the information is more transparent to the reader . Dyslexia is a disorder that causes difficulties with skills associated with learning, namely reading and writing, which affects up to 20\% of the general population. Readability formulas have been applied to measure the difficulty of reading texts for people with dyslexia .   The scores from readability formulas have been generally found to correlate highly with the actual readability of a text written in the English language. The adaptation of readability formulas to no-English texts is not straightforward. Measuring readability is also essential for every non-English language, but not all of the readability formulas mentioned above are language-independent. These formulas require some resources like a 3000-word list, which is easily understandable by fourth-grade American students, syllable counting dictionary, stemmer, lemmatizer etc. Resource availability for Natural Language Processing  research is an obstacle for some low-resource-languages . In this paper, we aim to develop a readability analysis tool for the Bengali Language. Bengali is the native language of Bangladesh, also used in India  and has approximately 230 million native speakers. Despite being the  most spoken language in the world, Bengali suffers from a lack of fundamental resources for NLP. For a low resource language like Bengali, the research in this area so far can be considered to be narrow and sometimes incorrect. \citet{islam2012text, sinha2012new} tried to adapt the formula-based approaches used for the English language. Unfortunately, it isn't straightforward as these formulas are developed for U.S. based education system and which predicts U.S. grade level of the reader. Since the Bangladeshi education system and grade levels are different from U.S., therefore, the mapping is faulty and led to incorrect results. There is a strong relationship between reading skills and human cognition, which varies depending on different age groups . Therefore, to eliminate this incompatibility, in this paper, we map grade level to different age groups to present age-to-age comparison. Moreover,  used traditional machine learning models to address this task on a very small scale dataset, which isn't publicly available. There are readability analysis tools available for English , Arabic , Italian , and Japanese  language. Unfortunately, no such tool is available for Bengali language that can validate the readability of a text. On the other hand, there is no large-scale human annotated readability analysis dataset available to train supervised neural models for this extremely low-resource language. Our main contributions are summarized as follows:    
","  Determining the readability of a text is the first step to its simplification. In this paper, we present a readability analysis tool capable of analyzing text written in the Bengali language to provide in-depth information on its readability and complexity. Despite being the $7^{th}$ most spoken language in the world with 230 million native speakers, Bengali suffers from a lack of fundamental resources for natural language processing. Readability related research of the Bengali language so far can be considered to be narrow and sometimes faulty due to the lack of resources.  Therefore, we correctly adopt document-level readability formulas traditionally used for U.S. based education system to the Bengali language with a proper age-to-age comparison. Due to the unavailability of large-scale human-annotated corpora, we further divide the document-level task into sentence-level and experiment with neural architectures, which will serve as a baseline for the future works of Bengali readability prediction. During the process, we present several human-annotated corpora and dictionaries such as a document-level dataset comprising 618 documents with 12 different grade levels,  a large-scale sentence-level dataset comprising more than 96K sentences with simple and complex labels, a consonant conjunct count algorithm and a corpus of 341 words to validate the effectiveness of the algorithm, a list of 3,396 easy words, and an updated pronunciation dictionary with more than 67K words. These resources can be useful for several other tasks of this low-resource language. \footnote{We make our Code \& Dataset publicly available at \url{https://github.com/tafseer-nayeem/BengaliReadability} for reproduciblity.}",358
" A typical text retrieval system uses a multi-stage retrieval pipeline, where documents flow through a series of ``funnels``  that discard unpromising candidates using increasingly more complex and accurate ranking components. These systems have been traditionally relying on simple term-matching techniques to generate an initial list of candidates . In that, retrieval performance is adversely affected by a mismatch between query and document terms, which is known as a vocabulary gap problem.  The vocabulary gap can be mitigated by learning  dense or sparse representations for effective first-stage retrieval. Despite recent success in achieving this objective , existing studies have have at least one of the following flaws:    This motivated us to develop a carefully-tuned traditional, i.e., non-neural, system, which we evaluated in \href{https://microsoft.github.io/MSMARCO-Document-Ranking-Submissions/leaderboard/}{the MS MARCO document ranking task} .  Our objectives are:   Our submission  achieved MRR=0.298 on the hidden validation set  and outperformed all other traditional systems. It was the first system  that outstripped several neural baselines.  According to our own evaluation  on TREC NIST data , our system achieves NDCG@10 equal to 0.584 and 0.558 on  2019 and 2020 queries, respectively. It, thus, outperforms a tuned BM25 system by 6-7\%: NDCG@10 is equal to 0.544 and 0.524 on 2019 and 2020 queries, respectively. \href{https://github.com/oaqa/FlexNeuART/blob/repr2020-12-06/scripts/data_convert/msmarco/README.md}{We posted two notebooks to reproduce results}:   
"," This short document describes a traditional IR  system that achieved MRR@100 equal to 0.298 on the MS MARCO Document Ranking leaderboard . Although inferior to most BERT-based models,  it outperformed several neural runs ,  including two submissions that used a large pretrained Transformer model for re-ranking. We provide software and data to reproduce our results.",359
" Figurative language, or a figure of speech , is phrasing that goes beyond the literal meaning of words to get a message or point across. Writers and poets use figurative language to build imagery and elicit aesthetic experiences. %A handful of figurative types help make foreign concepts familiar and graspable, including but not limited to simile , metaphor , irony, etc. %.  In computational linguistics, figurative language processing  has long been an interesting research topic, including both detection  and generation tasks . } \\ \hline \multicolumn{1}{|c|}{After} & \multicolumn{2}{l|}{} \\ \hline \rowcolor[HTML]{ECF4FF}  \multicolumn{3}{|c|}{\cellcolor[HTML]{ECF4FF}Other Figurative Language Generation} \\ \hline \rowcolor[HTML]{ECF4FF}  Task & Status & \multicolumn{1}{c|}{\cellcolor[HTML]{ECF4FF}Text} \\ \hline  & Before & A metaphorical pair of  \\ \cline{2-3}  \multirow{-2}{*}{Metaphor} & After & She devoured his novels. \\ \hline  &  &  \\ \cline{2-3}  \multirow{-2}{*}{} & Ironic &  \\ \hline \end{tabular} }  \end{table}  There exist a handful of figurative types that help make concepts become vivid and graspable, including but not limited to simile , metaphor , irony, etc. %.  Among them, similes play a vital role for human writings to be attractive. Different from metaphors' using implicit comparisons, a simile is a description that uses ``like'' or ``as'' to make a clear comparison between two separate concepts. As shown in Table , human writers add coherent similes into proper locations of the original text to vivify plain writings. Such an interpolation-based text polishing process is especially unique for similes, since most polishing objectives clearly requires text rephrasing, e.g., grammar error correction for fluency polishment, text editing for irony style transfer, etc. Distinctly, interpolating similes is like putting proper ingredients to an unflavored dish, instead of totally re-cooking a new one based on a different recipe. Despite the importance of simile, only a few work has explored simile recognition. To the best of our knowledge\footnote{We encourage readers to also refer to a contemporary work by , which shares a different point of view of simile generation.}, none of existing work has ever investigated simile generation given a plain text, which is indispensable for amplifying writing with similes. % very few works have explored simile generation in the field of FLP polishing text with simile interpolation. % interpolation for text polishment.  Although sequence-to-sequence models work well for story generation , irony generation , or metaphor and personification generation , it is non-trivial for these models to generate proper and creative simile for a given text. In particular, writing polishment with similes is a unique task because it requires to together address the challenges listed below:%that together make writing polishment with similes a unique task.   %Apparently, one of the biggest challenge for most text polishing studies is data insufficiency. Either it's the lack of labelled data for continuous figurative language generation, such as metaphor  and personification , or the lack of parallel data for style transfer on text attributes such as sentiment, formality , offensivity , political slant  and irony  etc. Apart from expensive human annotation, previous works either adopted semi-supervised methods to construct new datasets, or applied complex unsupervised approaches to deal with this issue. In contrast, obtaining simile data is relatively cheap, since it can be identified with clear patterns such as the occurrence of connecting words as ``like''. Even better, there are a rich dozen of simile patterns in Chinese , which further facilitates the automatic construction of simile data.   %In the field of figurative language processing however, while the detection tasks have been thoroughly explored , very few studies actually focused on the generation task , almost all existing works are limited by the lack of annotated or parallel data to a great deal.    %Despite its simple form, simile plays a vital role for written narratives to be attractive. A creative and coherent simile that occurs at proper position of a narration will greatly improve the reading experience . However, existing work on metaphor generation are mostly non-contextual and only focus on continuous generation.  developed a web-driven approach for simile generation within a single sentence.  only focused on generating unconditional verb-oriented metaphors, which requires a pair of fit word  and target word as input. Although  studied the contextual metaphorical generation of poetry, the generation is still in continuous manner, which always generates next lines given previous lines. Hence, none of these works shed lights on polishing plain narrations with both the simile generation and positioning.     %Beyond all that, current researches on text editing or style transfer mostly focused on single sentence rephrasing towards various text attributes such as sentiment, formality , offensivity , political slant  and irony  etc. . Most existing approaches could not be directly applied in the case of narration simile polishment, since our objective is not to rephrase given sentences on the whole. Rather, the proposed task is to generate similes only at proper locations without changing anything from the original writing.   %Meanwhile, there has been great progress in neural generation approaches in recent years due to rapid growth of model architectures as well as available corpus , resulting in various creative applications, from chatbots , to livebot commenting , to streamlined video captioning , etc. Unfortunately, in the field of figurative language processing, despite well-studied detection tasks , the lack of labelled or parallel data limits the research on generation tasks to a great deal .  To this end, we propose a new task of Writing Polishment with Simile 閳ユ敄o firstly decide where to put a simile within plain input text, then figure out what content to generate as a coherent simile. To facilitate our research, we propose a new Chinese Simile  dataset, which contains roughly 5.5 million similes in fictional contexts. %from Chinese online fictions with 92\% simile extraction precision. %For model design, We also set up a benchmark model Locate\&Gen to validate the feasibility and potentials of WPS task. Locate\&Gen model is a two-stage biased generation model upon the framework of transformer encoder-decoder . At the first step, it locates a pointer position for simile interpolation, and then generates a location-specific simile using a novel insertion bias. The two-stage design allows both automatic and semi-automatic inference modes to assist writing polishment flexibly. To summarize, our contributions are three-folded:   %Hence, in contrast with the romance of open machine story/metaphor generation discussed above, SP aims at practically improving narrative writings in the hands of human writers. It also poses several new challenges for AI as follows: 1) There are no existing large-scale figurative language corpus, and its annotation is fairly difficult since annotators need to be familiar with writing techniques. 2) Besides generating coherent simile that must be faithful to the original context, the model should also learn to put the generated ingredients at proper position. In this work, we address the SP problem and propose two-staged Locate\&Gen model. In order to obtain large-scale simile dataset, we adopt Chinese simile patterns\footnote[1]{Different from English, there are dozens of patterns for simile expressions in Chinese like ""婵傝棄鍎"", ""閹鎶"", ""娴犲じ缍"", ""鐎规稑顩"", ""娣囥劎鍔"", ""婵″倽瀚"", ""閻樼懓顩"", etc., all standing for the meaning of ""as if"". Also note that similes and metaphors can be exchanged easily by exchanging simile patterns with verbs such as ""Be"" or ""Become"".} to automatically extract sentences containing similes. 
"," A simile is a figure of speech that directly makes a comparison, showing similarities between two different things, e.g. ``Reading papers can be dull sometimes, like watching grass grow"". Human writers often interpolate appropriate similes into proper locations of the plain text to vivify their writings. However, none of existing work has explored neural simile interpolation, including both locating and generation. In this paper, we propose a new task of Writing Polishment with Simile  to investigate whether machines are able to polish texts with similes as we human do. Accordingly, we design a two-staged Locate\&Gen model based on transformer architecture. Our model firstly locates where the simile interpolation should happen, and then generates a location-specific simile. We also release a large-scale Chinese Simile  dataset containing 5 million similes with context. The experimental results demonstrate the feasibility of WPS task and shed light on the future research directions towards better automatic text polishment.%with model achieving 76.9\% simile positioning accuracy and decent performance on generation metrics as well as human evaluations.  %Current studies on figurative language generation are either non-contextual or focus only on continuous left-to-right generation manner, which is impractical for polishing written narratives with simile embellishments which may take place at any position of the original content. In this paper, we propose Simile Positioning \& Generation  task闁炽儲鏁刼 first decide a proper insertion position of simile then generate coherent simile content in plain narrations, to investigate whether computational methods are able to refine written narratives with similes as human novelists do. We introduce a large-scale Chinese Simile  dataset, which contains millions of similes with contexts extracted automatically from Chinese online fictions of various types. We establish baseline Insert\&Gen model performances based on SOTA transformer architecture. The experimental results demonstrate the feasibility of SPG task with model achieving 76.9\% accuracy on simile positioning and decent performance on generation metrics and human evaluations.  %Human author is capable of applying figurative language to bring their stories to life, so that readers ""devour"" his vivid narratives. Current researches mainly focused on the continuous story generation  as well as global text editing or style transfer, topics around machines learning to apply figurative techniques for writing is seldom discussed. In this paper, we propose a new task of Simile Positioning\&Generation , which aims to decorate plain narrative sentences with similes at appropriate positions, while being faithful to the original writings. We introduce a large-scale Chinese Simile  dataset, containing millions of contextual similes extracted automatically from Chinese online fictions of various types. We establish baseline Locate\&Gen model performances based on SOTA transformer architecture. The experimental results demonstrate the feasibility of SPG task with model achieving around 80\% accuracy on simile positioning and decent performance on generation metrics and human evaluations.",360
"  A contract is a legally binding agreement that recognizes and governs the rights and duties of the parties to the agreement. Correctly composing contracts is crucial to ensure its legal validity. In many real-world scenarios, a standard contract is prepared by filling   blanks in a precompiled form. Due to carelessness, two blanks that should be filled with the same  content may be incorrectly filled with different  content. This will result in contract inconsistencies, which may severely impair the legal validity of the contract.  Contract review is widely used by companies to check contract inconsistencies. However, contract review is labor-intensive and costly. Big companies have to hire tens of thousands of lawyers to conduct contract review, and it is estimated that Fortune Global  and Fortune  companies spend about 35281299,62194.05\%90.90\%$.  Our contributions are summarized as follows:   We formulate the Contract Inconsistency Checking  problem. As far as we know, this problem has not yet been studied in the AI community.   We propose a novel Pair-wise Blank Resolution  framework to address the CIC problem. In PBR, we propose a  that extends the Transformer encoder architecture to efficiently model meaningless blanks.   We collected and labeled  a large-scale Chinese contract corpus for CIC. The experimental results show the promising performance of our PBR method.     
"," Contract consistency is important in ensuring the legal validity of the contract. In many scenarios, a contract is written by filling the blanks in a precompiled form. Due to carelessness, two blanks that should be filled with the same  content may be incorrectly filled with different  content. This will result in the issue of contract inconsistencies, which may severely impair the legal validity of the contract. Traditional methods to address this issue mainly rely on manual contract review, which is labor-intensive and costly. In this work, we formulate a novel Contract Inconsistency Checking  problem, and design an end-to-end framework, called Pair-wise Blank   Resolution , to solve the CIC problem with high accuracy. Our PBR model contains a novel \texttt{BlankCoder} to address the challenge of modeling meaningless blanks. \texttt{BlankCoder} adopts a two-stage attention mechanism that adequately associates a meaningless blank with its relevant descriptions while avoiding the incorporation of irrelevant context words. Experiments conducted on real-world datasets show the promising performance of our method with a balanced accuracy of $94.05\%$ and an F1 score of $90.90\%$ in the CIC problem.",361
"  Building a human-like open-domain conversational agent  has been one of the milestones in artificial intelligence . Early conversational agents are primarily based on rules , e.g., Eliza , the first CA developed in 60's, simulates a Rogerian psychotherapist based on hand-crafted pattern matching rules. In recent years, with the advancement of data-driven neural networks, neural open-domain conversational models are becoming dominant .  Recent efforts in open-domain neural conversational models are primarily aiming to improve the response diversity  and endowing responses with knowledge , personality , emotion  and empathy .  All the efforts mentioned above are focusing on models that passively respond to user messages. However, in many real-world scenarios, e.g., conversational recommendation, psychotherapy and education, conversational agents are required to actively lead the conversation by smoothly changing the conversation topic to a designated one. For example, during a casual conversation, the agent may actively lead the user to a specific product or service that the agent wants to introduce and recommend.  In this paper, we follow the line of research in  and study the problem of imposing conversational goals/keywords on open-domain conversational agents, where the agent is required to lead the conversation to a target keyword smoothly and fast. As illustrated in Figure , given a target keyword ``juice"" and a random starting keyword ``comics"", the agent is required to converse with the user in multiple exchanges and lead the conversation to ``juice"". The challenge of this problem lies in how to balance the tradeoff between maximizing keyword transition smoothness and minimizing the number of turns taken to reach the target. On the one hand, passively responding to the user solely based on the conversation context would achieve high smoothness but may take many turns to reach the target, but on the other hand, directly jumping to the target word by ignoring the conversation context would minimize the number of turns but produce non-smooth keyword transitions.  \citet{tang2019target} proposed to break down the problem into two sub-problems: next-turn keyword selection and keyword-augmented response retrieval. \citet{tang2019target} proposed a next-turn keyword predictor and a rule-based keyword selection strategy to solve the first sub-problem, allowing the agent to know what is the next keyword to talk about given the conversation history and the target keyword. In addition, \citet{tang2019target} proposed a keyword-augmented response retrieval model to solve the second sub-problem, allowing the agent to produce a response that is relevant to the selected keyword.    However, there are two major limitations in existing studies . First, the training and evaluation datasets for next-turn keyword prediction are directly extracted from conversations without human annotations, thus, the majority of the ground-truth keyword transitions are noisy and have low correlations with human judgements. As illustrated in Figure , only a few keyword transitions in a conversation are considered relevant. In fact, in our human annotation studies of over 600 keyword transitions, we found that around 70\% of keyword transitions in the next-turn keyword prediction datasets are rated as not relevant, which renders the trained next-turn keyword predictor in existing studies less reliable.  Second, the rule-based keyword selection strategy primarily leverages the cosine similarity between word embeddings to select keywords that are closer to the target keyword. Word embeddings are trained based on the distributional hypothesis that words that have similar contexts have similar meanings, which may not reflect how humans relate words in conversational turn-taking.  In this paper, we assume that human conversations are grounded on commonsense and propose a keyword-guided neural conversational model that can leverage external commonsense knowledge graphs  for both next-turn keyword selection and keyword-augmented response retrieval. Humans rely on commonsense to reason, and commonsense reasoning plays an important role in the cognitive process of conversational turn-taking . Relying on a CKG for keyword transition would allow the agent to select a more target-related keyword for the next-turn.  Moreover, we leverage commonsense triplets from the CKG using Graph Neural Networks  for both next-turn keyword prediction and keyword-augmented response retrieval to achieve more accurate predictions.   In summary, our contributions are as follows:  
"," We study the problem of imposing conversational goals/keywords on open-domain conversational agents, where the agent is required to lead the conversation to a target keyword smoothly and fast. Solving this problem enables the application of conversational agents in many real-world scenarios, e.g., recommendation and psychotherapy. The dominant paradigm for tackling this problem is to 1) train a next-turn keyword classifier, and 2) train a keyword-augmented response retrieval model. However, existing approaches in this paradigm have two limitations: 1) the training and evaluation datasets for next-turn keyword classification are directly extracted from conversations without human annotations, thus, they are noisy and have low correlation with human judgements, and 2) during keyword transition, the agents solely rely on the similarities between word embeddings to move closer to the target keyword, which may not reflect how humans converse. In this paper, we assume that human conversations are grounded on commonsense and propose a keyword-guided neural conversational model that can leverage external commonsense knowledge graphs  for both keyword transition and response retrieval. Automatic evaluations suggest that commonsense improves the performance of both next-turn keyword prediction and keyword-augmented response retrieval. In addition, both self-play and human evaluations show that our model produces responses with smoother keyword transition and reaches the target keyword faster than competitive baselines.",362
"   Despite of remarkable progress made in NMT recently , most NMT systems are still prone to translation errors caused by noisy input sequences. One common type of input noise is homophone noise, where words or characters are mis-recognized as others with same or similar pronunciation in ASR or input systems for non-phonetic languages , as illustrated by the example in Table.   Previous works suggest that incorporating phonetic embeddings into NMT and augmenting training data with adversarial examples with injected homophone noise would alleviate this issue. Intuitively, humans usually have no trouble in disambiguating sentences corrupted with moderate homophone noise via context and syllable information. We propose a human-inspired robust NMT framework tailored to homophone noise for Chinese-English translation, which is composed of a homophone noise detector  and a syllable-aware NMT  model.     \\ Output of NMT~&~build a primary school \\ \specialrule{0.05em}{3pt}{3pt} Noisy Input~&~ \\  Output of NMT~&~suggest a primary school \\  \specialrule{0.05em}{3pt}{3pt} Mixed Transcript~&~ \\  Output of Ours~&~build a primary school\\ \bottomrule[1pt] \end{tabular}   Due to the lack of data annotated with homophone noise, we propose to train our detector on monolingual data in a self-supervised manner, where Chinese characters sequences as input and their corresponding syllables sequence as label to predict the possibility that a character is homophone noise. The identified homophone errors from a source sentence are then converted into corresponding syllables to produce a new source sequence mixed with characters and syllables. Augmenting bilingual training data with instances where original source sentences are substituted with their corresponding character-syllable-mixed sequences, we train the SANMT model to translate such unconventional inputs. To examine the effectiveness of our proposed model, we conduct extensive experiments on both artificial noisy test sets and a real-world noise test set with homophone noise in speech translation  scenario. The test set will be released soon. Our experimental results on ChineseEnglish translation clearly show that the proposed method is not only significantly superior to previous approaches in alleviating the impact of homophone noise on NMT, but also achieves a substantial improvement on the clean text. %Due to the lack of data annotated with homophone noise, we propose to train our detector on monolingual data in a self-supervised manner, where Chinese characters are automatically transformed into syllables to predict homophone noise. The identified homophone errors from a source sentence are then converted into corresponding syllables to produce a new source sequence mixed with characters and syllables. Augmenting training data with instances where original source sentences are substituted with their corresponding character-syllable-mixed sequences, we train the SANMT model to translate such unconventional inputs. To examine the effectiveness of our proposed model, we conduct extensive experiments on both artificial noisy test sets and a real-world noise test set with homophone noise in speech translation  scenario. The test set will be released soon. Our experimental results on ChineseEnglish translation clearly show that the proposed method is not only significantly superior to previous approaches in alleviating the impact of homophone noise on NMT, but also achieves a substantial improvement on the clean text.    
"," In this paper, we propose a robust neural machine translation  framework. The framework consists of a homophone noise detector and a syllable-aware NMT model to homophone errors. The detector identifies potential homophone errors in a textual sentence and converts them into syllables to form a mixed sequence that is then fed into the syllable-aware NMT. Extensive experiments on Chinese$\rightarrow$English translation demonstrate that our proposed method not only significantly outperforms baselines on noisy test sets with homophone noise, but also achieves a substantial improvement on clean text.",363
" In recent years, there has been a dramatic surge in the adoption of voice assistants such as Amazon Alexa, Apple Siri, and Google Assistant. Customers use them for a variety of tasks such as playing music and online shopping.  These voice assistants are built on complex Spoken Language Understanding  systems that are typically too large to store on an edge device such as a mobile phone or a smart speaker. Hence, user traffic is routed through a cloud server to process requests. This has led to privacy concerns and fueled the push for tiny AI and edge processing, where the user requests are processed on the device itself.   Traditional SLU systems consist of a two-stage pipeline, an Automatic Speech Recognition  component that processes customer speech and generates a text transcription , followed by a Natural Language Understanding  component that maps the transcription to an actionable hypothesis consisting of intents and slots . An end-to-end  system that goes directly from speech to the hypothesis would help make the SLU system smaller and faster, allowing it to be stored on an edge device. It could potentially also be better optimized than a pipeline since it eliminates cascading errors.  However, E2E systems are not used in practice because they have some key issues. These systems are hard to build since they consist of large neural components such as transformers and require massive amounts of E2E training data. They also don't make use of the vastly available training data for the ASR and NLU components that could be used to enhance their performance, because the examples in these datasets may not be aligned to create an E2E training sample. Another issue is feature expansion, a scenario where a new domain, with new intents and slots, is added to the voice assistant's capabilities. Here, developers typically only have access to some synthetically generated text-hypothesis examples. Speech data isn't readily available and it is very expensive to collect. E2E models thus fail as they require lots of new audio and hypothesis data to learn this new domain.  In this work, we build an E2E model that mitigates these issues using transfer learning. We call it the Audio-Text All-Task  Model. AT-AT is an E2E transformer-based model that is jointly trained on multiple audio-to-text and text-to-text tasks. Examples of these tasks include speech recognition , hypothesis prediction from speech , masked LM prediction , and hypothesis prediction from text . Our model achieves this by converting data from all these tasks into a single audio-to-text or text-to-text format. Figure shows this joint training phase in detail. Our findings indicate that there is significant knowledge transfer taking place from multiple tasks, which in turn helps in downstream model performance. We see that the AT-AT pretrained model shows improved performance on SLU hypothesis prediction on internal data collected from Alexa traffic. We also report state-of-the-art results on two public datasets: FluentSpeech , and SNIPS Audio .   Furthermore, since our model contains a text encoder, it can consume both audio and text inputs to generate a target sequence. By jointly training on both audio-to-text and text-to-text tasks, we hypothesize that this model learns a shared representation for audio and text inputs. This allows us to simply train on new text-to-text data and get audio-to-text performance for free, giving us a way to do E2E hypothesis prediction in a zero-shot fashion during feature expansion. We test this approach on an internal dataset from Alexa traffic, and an external dataset, Facebook TOP . Since TOP consists of only text data, we collected speech data for the test split using an internal tool at Amazon. We will soon release this dataset.  In summary, our contributions are as follows.   
"," Voice Assistants such as Alexa, Siri, and Google Assistant typically use a two-stage Spoken Language Understanding pipeline; first, an Automatic Speech Recognition  component to process customer speech and generate text transcriptions, followed by a Natural Language Understanding  component to map transcriptions to an actionable hypothesis. An end-to-end  system that goes directly from speech to a hypothesis is a more attractive option. These systems were shown to be smaller, faster, and better optimized. However, they require massive amounts of end-to-end training data and in addition, don't take advantage of the already available ASR and NLU training data.  In this work, we propose an E2E system that is designed to jointly train on multiple speech-to-text tasks, such as ASR  and SLU , and text-to-text tasks, such as NLU . We call this the Audio-Text All-Task  Model and we show that it beats the performance of E2E models trained on individual tasks, especially ones trained on limited data. We show this result on an internal music dataset and two public datasets, FluentSpeech and SNIPS Audio, where we achieve state-of-the-art results. Since our model can process both speech and text input sequences and learn to predict a target sequence, it also allows us to do zero-shot E2E SLU by training on only text-hypothesis data  from a new domain. We evaluate this ability of our model on the Facebook TOP dataset and set a new benchmark for zeroshot E2E performance. We will soon release the audio data collected for the TOP dataset for future research.",364
"  Neural Machine Translation   has achieved state of the art in various MT systems, including rich and low resource language pairs . However, the quality of low-resource MT is quite unpretentious due to the lack of parallel data while it has achieved better results on systems of the available resource. Therefore, low-resource MT is one of the essential tasks investigated by many previous works .    Recently, some works present MT systems that have achieved remarkable results for low-resource language . Inspired by these works, we collect data from the TED Talks domain, then attempt to build multilingual MT systems from French, English-Vietnamese. Experiments demonstrate that both language pairs: French-Vietnamese and English-Vietnamese have achieved significant performance when joining the training. %  Although multilingual MT can reduce the sparse data in the shared space by using word segmentation, however, rare words still exist, evenly they are increased more if languages have a significant disparity in term vocabulary. Previous works suggested some strategies to reduce rare words such as using translation units at sub-word and character levels or generating a universal representation at the word and sentence levels . These help to downgrade the dissimilarity of tokens shared from various languages. However, these works require learning additional parameters in training, thus increasing the size of models.   Our paper presents two methods to augment the translation of rare words in the source space without modifying the architecture and model size of MT systems:  exploiting word similarity. This technique has been mentioned by previous works . They employ monolingual data or require supervised resources like a bilingual dictionary or WordNet, while we leverage relation from the multilingual space of MT systems.  Adding a scalar value to the rare word embedding in order to facilitate its translation in the training process.  %  Due to the fact that NMT tends to have bias in translating frequent words, so rare words  often have less opportunity to be considered. Our ideal is inspired by the works of .  and  proposed various solutions to urge for translation of rare words, including modification embedding in training. They only experimented with recurrent neural networks  while our work uses the state-of-the-art transformer architecture.  transforms the word embedding of a token into the universal space, and they learn plus parameters while our method does not.  We apply our strategies in our fine-tuning processes, and we show substantial improvements of the systems after some epochs only.    Monolingual data are widely used in NMT to augment data for low-resource NMT systems . Back-translation  is known as the most popular technique in exploiting target-side monolingual data to enhance the translation systems while the self-learning method  focuses on utilizing source-side monolingual data. Otherwise, the dual-learning strategy  also suggests using both source- and target-side monolingual data to tackle this problem. Our work investigates the self-learning method  on the low-resource multilingual NMT systems specifically related to Vietnamese. Besides, monolingual data are also leveraged in unsupervised or zero-shot translation.  % learn the lexical relative between one token on a source language and the other once from another source language without modifying the system architecture as well as the model size. We also do not use any additional resources in our systems.   The main contributions of our work are:    In section 2, we review the transformer architecture used for our experiments. The brief of multilingual translation is shown in section 3. Section 4 presents our methods to deal with rare words in multilingual translation scenarios. The exploitation of monolingual data for low-resource multilingual MT is discussed in section 5. Our results are described in section 6, and related work is shown in section 7. Finally, the paper ends with conclusions and future work. % 
"," % Prior works have demonstrated that a low-resource language pair can be benefited from a multilingual machine translation  system which relies on the jointly training many language pairs. In this paper, we propose two simple strategies to address the rare word issue in multilingual MT systems for two low-resource language pairs: French-Vietnamese,  English-Vietnamese. The first strategy learns  dynamically word similarity of tokens in the shared space among source languages whilst the other one augments the translation ability of rare words through updating their embeddings during the training. In addition, we attempt to leverage monolingual data which is generated from multilingual MT to reinforce synthetic parallel in the data sparsity situation. We show that significant improvements of up to +1.62 and +2.54 BLEU points over the bilingual baseline systems for both language pairs and release datasets for the research community.  Prior works have demonstrated that a low-resource language pair can benefit from multilingual machine translation  systems, which rely on many language pairs' joint training. This paper proposes two simple strategies to address the rare word issue in multilingual MT systems for two low-resource language pairs: French-Vietnamese and English-Vietnamese. The first strategy is about dynamical learning word similarity of tokens in the shared space among source languages while another one attempts to augment the translation ability of rare words through updating their embeddings during the training. Besides, we leverage monolingual data for multilingual MT systems to increase the amount of synthetic parallel corpora while dealing with the data sparsity problem. We have shown significant improvements of up to +1.62 and +2.54 BLEU points over the bilingual baseline systems for both language pairs and released our datasets for the research community.",365
" % Fabian: Describing what it is Entity linking  is the task of mapping entity mentions in text documents to standard entities in a given knowledge base. For example, the word ``Paris'' is ambiguous: It can refer either to the capital of France or to a hero of Greek mythology. Now given the text ``Paris is the son of King Priam'', the goal is to determine that, in this sentence, the word refers to the Greek hero, and to link the word to the corresponding entity in a knowledge base such as YAGO  or DBpedia .  %Intriguingly, the Greek hero also goes by the name of ``Alexander''. Thus, the words ``Paris'' and ``Alexander'' are synonymous, and if they both refer to the Greek hero in some input text, they both have to be linked to the same entity in the knowledge base.  % Fabian: Describing why it's important In the biomedical domain, entity linking maps mentions of diseases, drugs, and measures to normalized entities in standard vocabularies. It is an important ingredient for automation in medical practice, research, and public health. Different names of the same entities in Hospital Information Systems seriously hinder the integration and use of medical data. If a medication appears with different names, researchers cannot study its impact, and patients may erroneously be prescribed the same medication twice.   % Fabian: Describing why it's difficult The particular challenge of biomedical entity linking is not the ambiguity: a word usually refers to only a single entity. Rather, the challenge is that the surface forms vary markedly, due to abbreviations, morphological variations, synonymous words, and different word orderings.  For example, ``Diabetes Mellitus, Type 2'' is also written as ``DM2'' and ``lung cancer'' is also known as ``lung neoplasm malignant''. In fact, the surface forms vary so much that all the possible expressions of an entity cannot be known upfront. This means that standard disambiguation systems cannot be applied in our scenario, because they assume that all forms of an entity are known. %, and thus they cannot be applied in our scenario.  One may think that variation in surface forms is not such a big problem, as long as all variations  of an entity are sufficiently close to its canonical form. Yet, this is not the case. For example, the phrase ""decreases in hemoglobin"" could refer to at least 4 different entities in MedDRA, which all look alike:  ""changes in hemoglobin"", ""increase in hematocrit"", ""haemoglobin decreased"", and ""decreases in platelets"". In addition, biomedical entity linking cannot rely on external resources such as  alias tables, entity descriptions, or entity co-occurrence, which are often used in classical entity linking settings.   % Fabian: what has been done For this reason, entity linking approaches have been developed particularly for biomedical entity linking. Many methods use deep learning: the work of \citet{li2017cnn} casts biomedical entity linking as a ranking problem,  leveraging convolutional neural networks .  More recently, the introduction of BERT has advanced the performance of many NLP tasks, including in the biomedical domain .  BERT creates rich pre-trained representations on unlabeled data and achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures. However, considering the number of parameters of pre-trained BERT models,  the improvements brought by fine-tuning them come with a heavy computational cost and memory footprint.  This is a problem for energy efficiency, for smaller organizations, or in poorer countries.  In this paper, we introduce a very lightweight model that achieves a performance statistically indistinguishable from the state-of-the-art BERT-based models. The central idea is to use an alignment layer with an attention mechanism,  which can capture the similarity and difference of corresponding parts between candidate and mention names. Our model is 23x smaller and 6.4x faster than BERT-based models on average; and more than twice smaller and faster than the lightweight BERT models. Yet, as we show, our model achieves comparable performance on all standard benchmarks. Further, we can show that adding more complexity to our model is not necessary: the entity-mention priors, the context around the mention, or the coherence of extracted entities \cite[as used, e.g., in][]{hoffart2011robust} do not improve the results any further. \footnote{All data and code are available at  \url{https://github.com/tigerchen52/Biomedical-Entity-Linking}.}   
"," Biomedical entity linking aims to map biomedical mentions, such as diseases and drugs, to standard entities in a given knowledge base.  The specific challenge in this context is that the same biomedical entity can have a wide range of names,  including synonyms, morphological variations, and names with different word orderings.  Recently, BERT-based methods have advanced the state-of-the-art by allowing for rich representations of word sequences. However, they often have hundreds of millions of parameters and require heavy computing resources, which limits their applications in resource-limited scenarios. Here, we propose a lightweight neural method for biomedical entity linking, which needs just a fraction of the parameters of a BERT model and much less computing resources.  Our method uses a simple alignment layer with attention mechanisms to capture the variations between mention and entity names. Yet, we show that our model is competitive with previous work on standard evaluation benchmarks.",366
" Although deep neural networks have recently been contributing to state-of-the-art advances in various areas , %in NLP problems ,  such black-box models may not be deemed reliable in situations where safety needs to be guaranteed, such as legal judgment prediction and medical diagnosis. Interpretable deep neural networks are a promising way to increase the reliability of neural models. To this end, extractive rationales, i.e., subsets of features of instances on which models rely for their predictions on the instances, can be used as evidence for humans to decide whether or not to trust a predicted result and, more generally, to trust a~model.   Previous works mainly use selector-predictor types of neural models to provide extractive rationales, i.e., models composed of two modules:  a selector that selects a subset of important features, and  a  predictor that makes a prediction based solely on the selected features. For example,  and  use a selector network to calculate a selection probability for each token in a sequence, then sample a set of tokens that is exclusively passed to the predictor. %The supervision is solely on the answer given by the prediction. %One then calculates a loss between the result given by the predictor and the ground-truth answer.  An additional typical desideratum in natural language pro\-cessing  tasks is that the selected tokens form a semantically fluent rationale. To achieve this,  added a non-differential regularizer that encourages any two adjacent tokens to be simultaneously selected or unselected. %The selector and predictor are jointly trained in a REINFORCE-style manner [cite Williams 92] because the sampling process and the regularizer are not differentiable.  further improved the quality of the rationales by using a Hard Kuma regularizer that also encourages any two adjacent tokens to be selected or unselected together. %, which is differentiable.    One drawback of previous works is that the learning signal for both the selector and the predictor comes mainly from comparing the prediction of the selector-predictor model with the ground-truth answer. %, while the predictor tells the selector to what extent the selected features contribute to the prediction, it does not directly tell the selector what kind of features are still missing or over-selected for a correct prediction.  Therefore, the exploration space to get to the correct rationale is large, decreasing the chances of converging to the optimal rationales and predictions.  Moreover, in NLP applications, the regularizers commonly used for achieving fluency of rationales treat all adjacent token pairs in the same way. This often leads to the selection of unnecessary tokens due to their adjacency to informative~ones. %Intuitively, if two tokens frequently occur adjacently, then they are more likely to be simultaneously selected or unselected. These important adjacent token pairs should receive more priority by the regularizer.   In this work, we first propose an alternative method to rationalize the predictions of a neural model. Our method aims to squeeze more information from the predictor in order to guide the selector in selecting the rationales. Our method trains two models: a ``guider"" model that solves the task at hand in an accurate but black-box manner, and a selector-predictor model that solves the task while also providing rationales. We use an adversarial-based method to encourage the final information vectors generated by the two models to encode the same information. We use an information bottleneck technique in two places: ~to encourage the features selected by the selector to be the least-but-enough features, and ~to encourage the final information vector of the guider model to also contain the least-but-enough information for the prediction.    Secondly, we propose using language models as regularizers for rationales in natural language understanding tasks. A language model  regularizer encourages rationales to be fluent subphrases, which means that the rationales are formed by consecutive tokens while avoiding unnecessary tokens to be selected simply due to their adjacency to informative tokens. %novel regularizer for the consecutiveness and semantic fluency of the rationale in NLP applications. This regularizer is based on a language model , which gives priority to important adjacent tokens so that they are simultaneously being selected.  The effectiveness of our LM-based regularizer is proved by both mathematical derivation and experiments. All the further details are given in the Appendix of the extended  paper.  Our contributions are briefly summarized as follows:        
"," \begin{quote} Explaining the predictions of AI models is paramount in safety-critical applications, such as in legal or medical domains. One form of explanation for a prediction is an extractive rationale, i.e., a subset of features of an instance that lead the model to give its prediction on the instance.  Previous works on generating extractive rationales usually employ a two-phase model: a selector that selects the most important features  followed by a predictor that makes the prediction based exclusively on the selected features. One disadvantage of these works is that the main signal for learning to select features comes from the comparison of the answers given by the predictor and the ground-truth answers. In this work, we propose to squeeze more information from the predictor via an information calibration method. More precisely, we train two models jointly: one is a typical neural model that solves the task at hand in an accurate but black-box manner, and the other is a selector-predictor model that additionally produces a rationale for its prediction. The first model is used as a guide to the second model. We use an adversarial-based technique to calibrate the information extracted by the two models such that the difference between them is an indicator of the missed or over-selected features. In addition, for natural language tasks, we propose to use a language-model-based regularizer to encourage the extraction of fluent rationales. Experimental results on a sentiment analysis task as well as on three tasks from the legal domain show the effectiveness of our approach to rationale extraction.  \end{quote}",367
" % background Sentence semantic matching is a fundamental Natural Language Processing~ task that tries to infer the most suitable label for a given sentence pair. For example, Natural Language Inference~ targets at classifying the input sentence pair into one of the three relations~. Paraphrase Identification~ aims at identifying whether the input sentence pair expresses the same meaning. Figure gives some examples with different semantic relations from different datasets.    % Current state As a fundamental technology, sentence semantic matching has been applied successfully into many NLP fields, e.g., information retrieval, question answering, and dialog system.  Currently, most work leverages the advancement of representation learning techniques to tackle this task.  They focus on input sentences and design different architectures to explore sentence semantics comprehensively and precisely.  Among all these methods, BERT plays an important role.  It adopts multi-layer transformers to make full use of large corpus~ for the powerful pre-trained model.  Meanwhile, two self-supervised learning tasks~ are designed to better analyze sentence semantics and capture as much information as possible.  % more citation Based on BERT, plenty of work has made a big step in sentence semantic modeling.    In fact, since relations are the predicting targets of sentence semantic matching task, most methods do not pay enough attention to the relation learning.  They just leverage annotated labels to represent relations, which are formulated as one-hot vectors.  However, these independent and meaningless one-hot vectors cannot reveal the rich semantic information and guidance of relations, which will cause an information loss.  \citeauthor{gururangan2018annotation}~ has observed that different relations among sentence pairs imply specific semantic expressions.  Taking Figure as an example, most sentence pairs with ``contradiction'' relation contain negation words~.  ``entailment'' relation often leads to exact numbers being replaced with approximates~.  ``Neutral'' relation will import some correct but irrelevant information~.  Moreover, the expressions between sentence pairs with different relations are very different.  Therefore, the comparison and contrastive learning among different relations~ can help models to learn more about the semantic information implied in the relations, which in turn helps to strengthen the sentence analysis ability of models. They should be treated as more than just meaningless one-hot vectors.   One of the solutions for better relation utilization is the embedding method inspired by Word2Vec.  Some researchers try to jointly encode the input sentences and labels in the same embedding space for better relation utilization during sentence semantic modeling.  Despite the progress they have achieved, label embedding method requires more data and parameters to achieve better utilization of relation information.  It still cannot fully explore the potential of relations due to the small number of relation categories or the lack of explicit label embedding initialization.   To this end, in this paper, we propose a novel \fullname~approach to make full use of relation information in a simple but effective way.  In concrete details, we first utilize pre-trained BERT to model semantic meanings of the input words and sentences from a global perspective.  Then, we develop a CNN-based encoder to obtain partial information~ of sentences from a local perspective.  Next, inspired by self-supervised learning methods in BERT training processing, we propose a Relation of Relation~ classification task to enhance the learning ability of \shortname~for the implicit common features corresponding to different relations.    Moreover, a triplet loss is used to constrain the model, so that the intra-class and inter-class relations are analyzed better.   Along this line, input sentence pairs with the same relations will be represented much closer and vice versa further apart.  Relation information is properly integrated into sentence pair modeling processing, which is in favor of tackling the above challenges and improving the model performance.  Extensive evaluations of two sentence semantic matching tasks  demonstrate the effectiveness of our proposed \shortname~and its advantages over state-of-the-art sentence semantic matching baselines.    
"," 	% background 	Sentence semantic matching is one of the fundamental tasks in natural language processing, which requires an agent to determine the semantic relation among input sentences.  	% current state 	Recently, deep neural networks have achieved impressive performance in this area, especially BERT.  	% problem 	Despite their effectiveness, most of these models treat output labels as meaningless one-hot vectors, underestimating the semantic information and guidance of relations that these labels reveal, especially for tasks with a small number of labels.  	% solution 	To address this problem, we propose a \fullname~for sentence semantic matching. 	 	Specifically, we first employ BERT to encode the input sentences from a global perspective. 	Then a CNN-based encoder is designed to capture keywords and phrase information from a local perspective.  	To fully leverage labels for better relation information extraction, we introduce a self-supervised relation of relation classification task for guiding \shortname~to consider more about relations.  	Meanwhile, a triplet loss is employed to distinguish the intra-class and inter-class relations in a finer granularity. 	% result 	Empirical experiments on two sentence semantic matching tasks demonstrate the superiority of our proposed model.  	As a byproduct, we have released the codes to facilitate other researches.",368
" 	Discovering novel user intents is important to improve the service quality in dialogue systems. By analyzing the discovered new intents, we may find underlying user interests, which could provide business opportunities and guide the improvement direction.  	 	 	Intent discovery has attracted much attention in recent years. Many researchers regard it as an unsupervised clustering problem, and they manage to incorporate some weak supervised signals to guide the clustering process. For example,~\citet{hakkani-tr2013a} propose a hierarchical semantic clustering model and collect web page clicked information as implicit supervision for intent discovery.~\citet{hakkani2015clustering} utilize a semantic parsing graph as extra knowledge to mine novel intents during clustering.~\citet{Padmasundari2018} benefit from the consensus predictions of multiple clustering techniques to discover similar semantic intent-wise clusters.~\citet{haponchyk2018supervised} cluster questions into user intent categories under the supervision of structured outputs.~\citet{shi2018auto} extract intent features with an autoencoder and automatically label the intents with a hierarchical clustering method. 	  	However, all of the above methods fail to leverage the prior knowledge of known intents. These methods assume that the unlabeled samples are only composed of undiscovered new intents. A more common case is that some labeled data of known intents are accessible and the unlabeled data are mixed with both known and new intents. As illustrated in Figure, we may have a few labeled samples  of known intents in advance. The remaining known and new intent samples are all unlabeled. Our goal is to find known intents and discover new intents with the prior knowledge of limited labeled data. Our previous work CDAC+ directly tackles this problem. Nevertheless, it uses pairwise similarities as weak supervised signals, which are ambiguous to distinguish a mixture of unlabeled known and new intents. Thus, the performance drops with more new intents. 	 	To summarize, there are two main difficulties in our task. On the one hand, it is challenging to effectively transfer the prior knowledge from known intents to new intents with limited labeled data. On the other hand, it is hard to construct high-quality supervised signals to learn friendly representations for clustering both unlabeled known and new intents. 	 	To solve these problems, we propose an effective method to leverage the limited prior knowledge of known intents and provide high-quality supervised signals for feature learning.  As illustrated in Figure, we firstly use the pre-trained BERT model to extract deep intent features. Then, we pre-train the model with the limited labeled data under the supervision of the softmax loss. We retain the pre-trained parameters and use the learning information to obtain well-initialized intent representations. Next, we perform clustering on the extracted intent features and estimate the cluster number   by eliminating the low-confidence clusters. 	 	As most of the training samples are unlabeled, we propose an original alignment strategy to construct high-quality pseudo-labels as supervised signals for learning discriminative intent features. For each training epoch, we firstly perform k-means on the extracted intent features, and then use the produced cluster assignments as pseudo-labels for training the neural network. However, the inconsistent assigned labels cannot be directly used as supervised signals, so we use the cluster centroids as the targets to obtain the alignment mapping between pseudo-labels in consequent epochs. Finally, we perform k-means again for inference. Benefit from the relatively consistent aligned targets, our method can inherit the history learning information and boost the clustering performance. 	 	We summarize our contributions as follows. Firstly, we propose a simple and effective method that successfully generalizes to mass of new intents and estimate the number of novel classes with limited prior knowledge of known intents. Secondly, we propose an effective alignment strategy to obtain high-quality self-supervised signals by learning discriminative features to distinguish both known and new intents. Finally, extensive experiments on two benchmark datasets show our approach yields better and more robust results than the state-of-the-art methods.  	 	
"," 		Discovering new intents is a crucial task in dialogue systems. Most existing methods are limited in transferring the prior knowledge from known intents to new intents. They also have difficulties in providing high-quality supervised signals to learn clustering-friendly features for grouping unlabeled intents. In this work, we propose an effective method, Deep Aligned Clustering, to discover new intents with the aid of the limited known intent data. Firstly, we leverage a few labeled known intent samples as prior knowledge to pre-train the model. Then, we perform k-means to produce cluster assignments as pseudo-labels. Moreover, we propose an alignment strategy to tackle the label inconsistency problem during clustering assignments. Finally, we learn the intent representations under the supervision of the aligned pseudo-labels. With an unknown number of new intents, we predict the number of intent categories by eliminating low-confidence intent-wise clusters. Extensive experiments on two benchmark datasets show that our method is more robust and achieves substantial improvements over the state-of-the-art methods. The codes are released at \url{https://github.com/thuiar/DeepAligned-Clustering}.",369
" The U.S.~NIH's precision medicine  initiative calls for designing treatment and preventative interventions considering genetic, clinical, social, behavioral, and environmental exposure variability among patients. The initiative rests on the widely understood finding that considering individual variability is critical in tailoring healthcare interventions to achieve substantial progress in reducing disease burden worldwide. Cancer was chosen as its near term focus with the eventual aim of expanding to other conditions. As the biomedical research enterprise strives to fulfill the initiative's goals, computing needs are also on the rise in drug discovery, predictive modeling for disease onset and progression, and in building NLP tools to curate information from the evidence base being generated.  \subsection{TREC Precision Medicine Series}     \end{table}   In a dovetailing move, the U.S.~NIST's  TREC  has been running a PM track since 2017 with a focus on cancer. The goal of the TREC-PM task is to identify the most relevant biomedical articles and clinical trials for an input patient case. Each case is composed of    a disease name,   a gene name and genetic variation type, and  demographic information . Table shows two example cases from the 2019 track. So the search is ad hoc in the sense that we have a free text input in each facet but  the    facets themselves highlight the PM related attributes that ought to characterize the retrieved documents. We believe this style of faceted retrieval is going to be more common across medical IR tasks for many conditions as the PM initiative continues its mission.   \subsection{Vocabulary Mismatch and Neural IR}  The vocabulary mismatch problem is a prominent issue in medical IR given the large variation in the expression of medical concepts and events. For example, in the query ``What is a potential side effect for Tymlos?'' the drug is referred by its brand name. Relevant scientific literature may contain the generic name Abaloparatide more frequently. Traditional document search engines have clear limitations on resolving   mismatch issues. The IR community has extensively explored methods to address the vocabulary mismatch problem, including query expansion based on relevance feedback, query term re-weighting, or query reconstruction by optimizing the query syntax.  Several recent studies highlight exploiting neural network models for query refinement in document retrieval  settings. \citet{nogueira2017task}  address  this issue by generating a transformed query from the initial query using a neural model.  They use reinforcement  learning  to train it where an agent  learns to reformulate the initial query to maximize the expected return  through actions . In a different approach, \citet{narayan2018ranking}  use RL for sentence ranking for extractive summarization.  \subsection{Our Contributions}  In this paper, building on the BERT architecture, we focus on a different hybrid document scoring and reranking setup involving three components: .~a document relevance classification model, which predicts  whether a document is relevant to the given query ; .~a keyword extraction model which spots tokens in a document that are likely to be seen in PM related queries; and .~an abstractive document summarization model that generates a pseudo-query given the document context and a facet type  via the BERT encoder-decoder setup. The keywords ) and the pseudo-query ) are together compared with the original query to generate a score. The scores from all the components are combined to rerank top   documents returned with a basic Okapi BM25 retriever from a Solr index of the corpora. %This is critical because neural document-query matching and summarization are expensive operations that cannot practically scale to the full corpus.  Our main innovation is in pivoting from the focus on queries by previous methods to emphasis on transforming candidate documents into pseudo-queries via summarization. Additionally, while generating the pseudo-query, we also let the   decoder output concept codes from biomedical terminologies that capture disease and gene names. We do this by embedding both words and concepts in a common semantic space before letting the decoder generate summaries that include concepts. Our overall architecture was evaluated using the TREC-PM datasets  with the 2019 dataset used as the test set. The results show an absolute  improvement in P@10 compared to prior best approaches while obtaining a small  gain in R-Prec. Qualitative analyses also highlight how the summarization is able to focus on document segments that are highly relevant to patient cases.  
"," Information retrieval  for precision medicine  often involves looking for multiple pieces of evidence that characterize a patient case. This typically includes at least the name of a condition and a genetic variation that applies to the patient. Other factors such as demographic attributes, comorbidities, and social determinants may also be pertinent. As such, the retrieval problem is often formulated as ad hoc search but with multiple facets  that may need to be incorporated. In this paper, we present a document reranking approach that combines neural query-document matching and text summarization toward such retrieval scenarios. Our architecture builds on the basic BERT model with three specific components for reranking: . document-query matching . keyword extraction and . facet-conditioned abstractive summarization. The outcomes of  and  are used to essentially transform a candidate document into a concise summary that can be compared with the query at hand to compute a relevance score. Component  directly generates a matching score of a candidate document for a query. The full architecture benefits from the complementary potential of document-query matching and the novel document transformation approach based on summarization along PM facets. Evaluations using NIST's TREC-PM track datasets  show that our model achieves state-of-the-art performance. To foster reproducibility, our code is made available here: \url{https://github.com/bionlproc/text-summ-for-doc-retrieval}.",370
"  . }   In real-world dialogue systems, a substantial portion of all user queries are ambiguous ones for which the system is unable to precisely identify the underlying intent.  %For example, nearly 30\% of user queries in a real-world QA system are ambiguous questions. % Can't give statistics in an academic paper without mentioning details We observed that many such queries in our question answering  system exhibited one of the following two characteristics.  %The ambiguous questions in our QA system can be summarized into 2 types:\\ %  \\ %       Given such limited information, it is difficult for a system to accurately respond to a user's ambiguous queries, often resulting in that the user's needs cannot be addressed. For example, the specific intent underlying an utterance such as ``How to apply?"" remains obscure, because there are too many products related to the action of ``applying"". In practice, one often needs to fall back to human agents to assist with such requests, increasing the workload and cost. The main purpose of deployed automated systems is to reduce the human workload in scenarios such as customer service hotlines. The lack of an ability to deal with ambiguous questions may directly lead to these sessions being transferred to human agents. In our real-world customer service system, this affects up to 30\% of sessions. Hence, it is valuable to find an effective solution to clarify such ambiguous questions automatically, greatly reducing the number of cases requiring human assistance.   Automated question clarification involves confirming a user's intent through interaction.  %. % is essential for a Question Answering  system.  Previous work has explored asking questions . Unfortunately, clarification by asking questions requires substantial customization for the specific dialogue setting. It is challenging to define appropriate questions to guide users towards providing more accurate information. Coarse questions may leave users confused, while overly specific ones may fail to account for the specific information a user wishes to convey.   In our work, we thus instead investigate interactive clarification by providing the user with specific choices as options, such as intent options . Unlike previous work, we propose an end-to-end model that suggests labels to clarify ambiguous questions.  %  In our experiments, we will show our method significantly over performs rule based method in recall of potential FAQs.  %  This paper focused on closed-domain question clarification in dialogue, solving all kinds of ambiguous questions in one method. %濡紕纭﹂梻顕顣藉☉鍫燁劆娑撴槒顩﹂柅姘崇箖娴溿倓绨扮涵顔款吇閻€劍鍩涢幇蹇撴禈閵嗗倹婀侀崙鐘殿潚娴溿倓绨伴弬鐟扮础閿涘苯寮介梻顔藉灗閹绘劒绶甸悽銊﹀煕闁銆嶉妴鍌氬冀闂傤喚娈戦弬瑙勭《闁俺绻冮悽鐔稿灇濞戝牊顒犻梻顔煎綖閿涘苯顩ч弸婊冨冀闂傤噣妫舵０妯跨箖娴滃骸绱戦弨鎾呯礉鐎硅妲楀鏇炲弳娑撳秴褰叉０鍕埂閻ㄥ嫬娲栨径宥冨倷姘︽禍鎺旀畱閺傜懓绱￠柅姘崇箖缂佹瑥鍤惄绋垮彠闁銆嶆笟娑氭暏閹撮攱绉峰褝绱濆В鏂款洤閻╃ǹ鍙AQ閹存牜娴夐崗铏Х濮澭囧銆嶉妴 %Previous methods either solve lack of semantic elements questions or solve entity ambiguity questions. % %娑撳窋revious work娑撴槒顩﹂崠鍝勫焼: %1.閸欏秹妫堕惃鍕煙濞夋洟妫堕惃鍕６妫版ê銇婂閺鎾呯礉閻€劍鍩涢惃鍕礀缁涙柨褰查懗鐣岄兇缂佺喐甯存稉宥勭瑐閿涘本鍨ㄩ懓鍛晸閹存劒绔存稉顏勭发婵傚洦顏嗘畱闂傤噣顣,閻喎鐤勭化鑽ょ埠闁倻鏁ら懟锕傛 %2.query 缁墽鍋ч弰顖滄暏閸︺劍鎮崇槐銏犵穿閹垮海娈戦敍灞芥躬鐎电鐦藉鍕娑撳﹤銇婇柌宥忕礉娑撳秴褰查懗钘夋躬鐎电鐦介柌宀绮伴崙杞扮閸棙鎮崇槐銏㈢波閺 %閻╃ǹ鎮撹ぐ銏犵础閻ㄥ嫪姘︽禍鎺戠础闂傤噣顣藉鍕娴ｈ法鏁ゆ稉娑擃亜鐔娴滃侗MI閸滃瓥DF閻ㄥ嫯顫夐崚娆愭煙濞夋洩绱濈圭偤鐛欑拠浣规閹存垳婊戦惃鍕煙濞夋洘妯夐拋妞剧喘娴滃氦顫夐崚娆愭煙濞夋洏 %閹存垳婊戦幓鎰毉娑撶粔宥囩暆濞蹭胶娈戝☉鍫燁劆閺傝顢嶉敍宀娲块幒銉ュ灙閸戠儤顒犳稊澶屽仯閿涘矂鍌滄暏娴滃骸顕拠婵嗘簚閺 %Question clarification by asking question generated by model may receive unexpected reply from user like ``I'm not sure"" or generate a weird question in real application. Query refinement method  which helps to improve search results is not applicable for clarification in dialogue. We aimed to interact with user by concise phrases to clarify user's question. % In a closed-domain QA system, we believe that an ambiguous question has a series of potential clear questions. For example, in Figure~, there are at least three FAQ questions corresponding to ambiguous question ``How to apply"". We argue that the essence of clarifying ambiguous questions lies in finding the key points of differentiation between potential questions. It's possible to clarify the user's true intents by confirming key points with users as shown in Figure~. %  %  % An example of this sort of approach is given in Figure. Here, we consider a closed-domain QA system, where a typical method is to build an intent inventory to address high-frequency requests. In this setting, the set of unambiguous candidate labels for an ambiguous user utterance corresponds to a set of frequently asked questions covered by the intent inventory.  %By constraining the problem to close-domain, the potential clear questions of an ambiguous question is a finite set.  In a closed domain, we consider the candidate set to be finite. For example, in Figure, there are three specific intents corresponding to the ambiguous question ``How to apply"".   Our approach induces phrase tags as labels for each intent. Thus, we have a catalog of intents with corresponding labels that can be presented to the user. The challenge lies in selecting a suitable list of labels that can effectively clarify the ambiguous question. In our approach, the problem of finding the label sequence is formulated as a collection partitioning problem, where the objective is to cover as many elements as possible while distinguishing elements as clearly as possible.  % According to Aristotle, the definition of a species consists of genus proximum and differ. The differential is the attribute by which one species is distinguished from all others of the same genus.  The task of question clarification thus amounts to obtaining a suitable set of labels. %is to get a differential intents set of potential FAQs. % \todo{update with section 3.2}  % We will illustrate the method of finding such intents set in detail in the methodology section. % %  introduced methods to ask clarification questions for information that is missing from a given linguistic context.  use generative model to generate clarification questions for solving entity ambiguities. But it has some obstacles to use these methods in real application. One reason is users in real world sometimes doesn't respond as clarification question expected like just reply ``I'm not sure"". Compared withing ask a clarification question, we directly list potential ambiguities as options.  proposed a query refinement method based on reinforcement learning, which helps to improve search results in search engine. Limited to the form of a dialogue system, it's not practical to show long list of potential results in dialogue. We aimed to interact with user by concise phrases to clarify user's question. % % A similar idea from \citet{DBLP:conf/chiir/RadlinskiC17} also suggests that, a conversational interface may be easier for users to clarify their needs given precise choices rather than expecting them to come up with particular terms.  % The complete question clarification process in our work is illustrated in Figure . Through real-world application experiments, our method has a lower rate on transferring to human agents and significant higher CTR  than other baselines. Our method also performs better than other baselines on the recall of potential FAQs on our annotated corpora.  %This paper focuses on closed-domain question clarification in dialogue, solving all kinds of ambiguous questions in one method.   The main contributions of our work are:    %% This part is comparison between related works.  % We investigated related works to clarify ambiguous questions in QA. The classic solution is to rank the most semantic similar questions [ranker ref] to the ambiguous questions. However, considering the limitation to display information in a dialogue based QA system, generally only the three results can be displayed, resulting in that this method cannot cover enough potential clear questions. In our experiments, we use the relevance ranker as the baseline for comparison. The results show that the human transferring rate of our method is much lower than the ranking method. The second method is to ask clarification questions . . However, the method of generative clarification question has some limitations in the real-word QA system. The biggest obstacle is that the user's answer space maybe to too open to answer, which complicates the dialogue. In addition, there is a lot of works to disambiguate questions through question refinement, but most refinement methods usually supplements information by a single key point, which not able to achieve all the key point recall we mentioned earlier.  % Question clarification is essential for a question answering system. In a real-world QA system, nearly 30\% of the user queries are ambiguous questions. Without clarification, dialogue participants risk missing information and ambiguous failing to achieve mutual understanding. The ability to ask clarification questions is one of the key desired components of conversational systems .  introduced methods to ask clarification questions for information that is missing from a given linguistic context.  use generative model to generate clarification questions for solving entity ambiguities.  % However, it is difficult to achieve a high success rate. For example, ``how to apply?"" is ambiguous, because there are too many products related to the ``apply"". By asking only one option question, such as ``Do you want to apply for a credit card?"" or two options question, such as ``Do you want to apply for a credit card or a loan ?"", which are both less efficient. Phenomena mentioned above exist in our real world customer service robot  system. CSRobot based on FAQ question answering is widely used in the real world, especially in the financial industry. When user enter a question in CSRobot system , information is retrieved by computing semantic similarity between user question and pre-manually prepared FAQ. Due to factors such as user's age, gender, geography, familiarity with our system, and urgency of user's problem, user may enter many ambiguous questions. In our CSRobot environment, the ratio is nearly 30\%. The ambiguous questions in our system can be summarized into 5 types:  Missing subject or object, e.g. ``how to apply"", ``how to change it back"",  Missing predicate, e.g. ``credit card"", ``my QR code"",  Missing of all subject predicates and objects,  e.g. ``How benefit"", ``its not right"",  Entity ambiguous,  e.g. ``My health insurance"", because health insurance contains many sub-categories,  Misspelling ambiguous. ``how to exist"" , ``exist"" may be misspelling of ``exit"". In this work, we focus on asking clarification questions using intents recommendation in FAQ-based question answering system. Previous methods either solve missing information questions or solve entity ambiguity questions, while our proposed method can handle both missing information and entity ambiguous mentioned above.   % The complete question clarification process in our work can be seen in Figure . The user enters an incomplete or ambiguous question, and agent recommends a list of candidate intents, each of which clarifies the user's question and can be clicked. Then user clicks on an intent associated with himself, and the agent finds a list of related FAQ in the FAQ knowledge base with the clarified question. Our work focuses on recommend a list of candidate intents for question clarification. A similar idea from \citet{DBLP:conf/chiir/RadlinskiC17} also suggests that, a conversational interface may be easier for users to clarify their needs given precise choices rather than expecting them to come up with particular terms.   % introduce question clarification as collection partition thought in detail   %   % One of the challenges in designing this method is how to design a cold start scenario. We use the end-to-end sequential intents recommendation method based on reinforcement learning for user question clarification. We did not use supervised method mainly because it is difficult for human annotators directly labeling intents related to user's ambiguous question . The reward is designed to recommend the closest clear question list and maximize the information gain after clicking one intent for better question clarification. We conducted offline and online experiments in a real-world CSRobot environment and collected the data of more than 100 million online real-users' interactions with our system in one month. To the best of our knowledge, we are the first to use intents recommendation for question clarification on real-world CSRobot environment, and interactions with more than 100 million of real users. The experiments proved the effectiveness and scalability of our proposed method. Contributions are summarized as follows:  %   %% FORMATTING  \newcommand{\NTCIR}{NTCIR-13} \newcommand{\metric}[1]{{\mbox{#1}}} \newcommand{\metricfont}[1]{{\small\sf{#1}}} \newcommand{\ydata}{{\metricfont{Y!S1}}} \newcommand{\govdata}{{\metricfont{GOV2}}} \newcommand{\RBP}{\metric{RBP}} \newcommand{\Pat}{\metric{P}} %\newcommand{\AP}{\metric{AP}} \newcommand{\AP}{\metric{MAP}} \newcommand{\NDCG}{\metric{NDCG}} \newcommand{\ERR}{\metric{ERR}} \newcommand{\BPref}{\metric{BPref}} \newcommand{\Qmeasure}{\metric{Qmeasure}}  \newcommand{\Patk}[1]{\mbox{\Pat@}} \newcommand{\RBPatp}[1]{\mbox{\RBP@}} \newcommand{\RBPatptok}[2]{\mbox{\RBP@}} \newcommand{\NDCGatk}[1]{\mbox{\NDCG@}} \newcommand{\ERRatk}[1]{\mbox{\ERR@}} \newcommand{\APtok}[1]{\mbox{\AP}} \newcommand{\APatk}[1]{\mbox{\AP}} \newcommand{\NDCGtok}[1]{\mbox{\NDCG}} \newcommand{\ERRtok}[1]{\mbox{\ERR}}  \newcommand{\ssvar}[1]{\mbox{\tiny#1}} \newcommand{\trisk}{} \newcommand{\urisk}{} \newcommand{\combsum}{\method{CombSUM}\xspace} \newcommand{\rrf}{\method{RRF}\xspace} %-- Baselines \newcommand{\gbrt}{\method{GBRT}} \newcommand{\lstm}{\method{LSTM}} \newcommand{\dqn}{\method{DQN}} \newcommand{\dodqn}{\method{DoDQN}} \newcommand{\doddqn}{\method{DoDDQN}} \newcommand{\ddqn}{\method{DDQN}} \newcommand{\pdodqn}{\method{PER-DoDQN}} \newcommand{\pdoddqn}{\method{PER-DoDDQN}} \newcommand{\per}{\method{PER}} \newcommand{\mlp}{\method{MLP}}   %\newcommand{\gbdtbl}{\method{GBDT-BL}} %\newcommand{\gbrtbl}{\method{GBRT-BL}} %\newcommand{\lambdamartbl}{\method{LambdaMART-BL}} %\newcommand{\gbdtbbl}{\method{GBDT-Budget-BL}} %\newcommand{\qlbl}{\method{QL-BL}} %\newcommand{\bmbl}{\method{BM25-BL}} %\newcommand{\sdmbl}{\method{SDM-BL}} %\newcommand{\adarankbl}{\method{AdaRank-BL}} %\newcommand{\wlmbl}{\method{WLM-BL}} % %%-- Experimental methods %\newcommand{\lmccost}{\method{LM-C3-Cost}} %\newcommand{\lmcce}{\method{LM-C3-CE}} %\newcommand{\lmcrnd}{\method{LM-C3-Rnd}} %\newcommand{\gbdtccost}{\method{GBDT-C3-Cost}} %\newcommand{\gbdtcce}{\method{GBDT-C3-CE}} %\newcommand{\gbdtcrnd}{\method{GBDT-C3-Rnd}} %\newcommand{\gbrtccost}{\method{GBRT-C3-Cost}} %\newcommand{\gbrtcce}{\method{GBRT-C3-CE}} %\newcommand{\gbrtcrnd}{\method{GBRT-C3-Rnd}} %\newcommand{\lambdamartccost}{\method{LambdaMART-C3-Cost}} %\newcommand{\lambdamartcce}{\method{LambdaMART-C3-CE}} %\newcommand{\lambdamartcrnd}{\method{LambdaMART-C3-Rnd}} % %\newcommand{\lmc}{\method{LM-C3-C}} %\newcommand{\lme}{\method{LM-C3-E}} %\newcommand{\lmf}{\method{LM-C3-F}} %\newcommand{\gbdtc}{\method{GBDT-C3-C}} %\newcommand{\gbdte}{\method{GBDT-C3-E}} %\newcommand{\gbdtf}{\method{GBDT-C3-F}} %\newcommand{\gbrtc}{\method{GBRT-C3-C}} %\newcommand{\gbrte}{\method{GBRT-C3-E}} %\newcommand{\gbrtf}{\method{GBRT-C3-F}} %\newcommand{\lambdamartc}{\method{LambdaMART-C3-C}} %\newcommand{\lambdamarte}{\method{LambdaMART-C3-E}} %\newcommand{\lambdamartf}{\method{LambdaMART-C3-F}}  %-- Tools \newcommand{\xgboost}{} \newcommand{\scikit}{} \newcommand{\tensorflow}{}  %-- misc formatting \def\D{\hphantom{1}} \def\C{\hphantom{1,}} %-- Misc control commands \newcommand\method[1]{{\sf\small{#1}}} \newcommand\smethod[1]{{\sf\scriptsize{#1}}} \newcommand\mytt[1]{{\bf{\tt{\small{#1}}}}} \newcommand{\alginp}[1]{\makebox[15mm][l]{\sc Input:}\\[0.5ex]} \newcommand{\algout}[1]{\makebox[15mm][l]{\sc Output:}\\} %--- Ranking Stuff \newcommand{\smin}{\var{s\_min}} \newcommand{\smax}{\var{s\_max}} \newcommand{\Answers}{\var{Ans}} \newcommand{\docweight}{\var{docweight}_{d}} \newcommand{\score}{\var{score}_{d,t}} \newcommand{\pivot}{\var{pivot}} \newcommand{\cpivot}{c_{\mbox{\scriptsizepivot}}} \newcommand{\tpivot}{t_{\mbox{\scriptsizepivot}}} \newcommand{\posting}{\ensuremath{}}  \newcommand{\dfdt}{\rangle d,f_{d,t} \langle} \newcommand{\tf}{\mbox{	extsc{TF}}\xspace} \newcommand{\tfidf}{\mbox{tfidf}\xspace} \newcommand{\tftd}{\ensuremath{\tf_{t,d}}} \newcommand{\tfqd}{\ensuremath{\tf_{q,d}}} \newcommand{\idf}{\mbox{	extsc{idf}}\xspace} \newcommand{\idft}{\ensuremath{idf_t}} \newcommand{\idfq}{\ensuremath{idf_q}} \newcommand{\idld}{\ensuremath{idl_d}} \newcommand{\idl}{\mbox{	extsc{idl}}\xspace} \newcommand{\wtd}{\ensuremath{wt_d}} %--- Ops %% \newcommand{\opstyle}[1]{\mbox{	extsc{#1}}} \newcommand{\opstyle}[1]{\mbox{{#1}}} \newcommand{\bmax}{\opstyle{BLOCK-MAX}\xspace} \newcommand{\wand}{\opstyle{WAND}\xspace} \newcommand{\bmwand}{\opstyle{BM-WAND}\xspace} \newcommand{\maxscore}{\opstyle{MAXSCORE}\xspace} \newcommand{\hsv}{\opstyle{HSV}\xspace} \newcommand{\pst}{\opstyle{PST}\xspace} \newcommand{\gtaat}{\opstyle{Greedy-TAAT}\xspace} \newcommand{\taat}{\opstyle{TAAT}\xspace} \newcommand{\daat}{\opstyle{DAAT}\xspace} %--- Misc  \newcommand{\bwt}{{\sc bwt}\xspace} \newcommand{\fmindex}{{\sc FM-index}\xspace} \def\xbwt{\mtxt^{\mbox{\scriptsize {\sc bwt}}}} \newcommand{\sa}{\mbox{\mbox{\sc sa}}} \newcommand{\lf}{\mbox{\mbox{\sc lf}}} \newcommand{\cpu}{{\sc cpu}\xspace} \newcommand{\ram}{{\sc ram}\xspace} \newcommand{\ascii}{{\sc ascii}\xspace} \newcommand{\sgml}{{\sc sgml}\xspace} \newcommand{\trec}{{\sc trec}\xspace} \newcommand{\collection}[1]{\mbox{\small\sc{#1}}} \newcommand{\newswire}{\collection{NewsWire}} \newcommand{\wt}{\collection{WT10G}} \newcommand{\gov}{\collection{GOV2}} \newcommand{\raw}{\mbox{\sc raw}\xspace} %--- Macros \newcommand{\bm}{\opstyle{BM25}} \newcommand{\lm}{\opstyle{LMDS}} \newcommand{\pl}{\mbox{\bf\scriptsize PL2}\xspace} \newcommand{\tfbm}{\ensuremath{\mbox{TF}_{\mbox{\scriptsize{BM25}}}\xspace}} \newcommand{\utf}{\mbox{\scriptsize UTF-8}\xspace} %%\newcommand{\newt}{\mbox{\method{NeWT}\xspace}} \newcommand{\newt}{\mbox{\method{NewSys}\xspace}} \newcommand{\indri}{\method{Indri\xspace}} \newcommand{\lynx}{\method{Lynx\xspace}} \newcommand{\boilerpipe}{\method{Boilerpipe\xspace}} \newcommand{\terrier}{\method{Terrier\xspace}} \newcommand{\Space}{space\xspace} \newcommand{\prefix}{prefix\xspace} \newcommand{\suffix}{suffix\xspace} \newcommand{\plain}{plain\xspace} \newcommand{\intent}{{\sc intent}\xspace}  \newcommand{\slarge}{S-Lrg} \newcommand{\ssmall}{S-Sml} \newcommand{\realdat}{R-Data}      \newcommand{\SA}{\mbox{SA}} \newcommand{\Tmin}{T_{\mbox{\scriptsize{min}}}} \newcommand{\Tmax}{T_{\mbox{\scriptsize{max}}}}   \newcommand{\argmin}{\operatornamewithlimits{argmin}} \newcommand{\argmax}{\operatornamewithlimits{argmax}} \newcommand{\lmax}{\operatornamewithlimits{max}} \newcommand{\llim}{\operatornamewithlimits{lim}}   %-- Sizes \newcommand\kb[1]{\,kB} \newcommand\mb[1]{\,MB} \newcommand\gb[1]{\,GB} \newcommand\tb[1]{\,TB} %-- maths \newcommand{\ith}{\ensuremath{i^{\mbox{\scriptsize th}}}} \newcommand{\nmax}{n_{\mbox{\tiny max}}} \newcommand{\newlne}{{}n} \newcommand{\var}[1]{\mbox{#1}} \newcommand{\svar}[1]{\mbox{\scriptsize#1}} %-- misc formatting \def\D{\hphantom{1}} \def\C{\hphantom{1,}} \newcommand{\myurl}[1]{{\url{#1}}} \newcommand{\mycaption}[1]{}} \newcommand{\myquery}[1]{{``{\tt{#1}}''}} %%AM \newcommand{\myparagraph}[1]{\paragraph*{\normalsize\it{#1}}} %% \newcommand{\myparagraph}[1]{\mysubsection{#1}} %\newcommand{\myparagraph}[1]{~\\{#1}.~} \newcommand{\myparagraph}[1]{{#1}.~} \newcommand{\mysubsection}[1]{\subsubsection*{{#1}}} \newcommand{\noi}{} \newcommand{\mycomment}[1]{} \newcommand{\mylabel}[1]{} \newcommand{\mytab}{\makebox[6mm]{~}} \newcommand{\fixed}[1]{\makebox[18mm]{#1}} %-- big iron \newcommand{\haatheshort}{ Intel Xeon E5640 fgcessors with a {\mb{12}} cache and {\gb{144}} of SDRAM} %\newcommand{\haathee}{ Intel Xeon E5640 Processors with %a {\mb{12}} smart cache, {\gb{144}} of DDR3 DRAM, eight {\tb{2}} SATA-II disks, %and running Ubuntu Linux 11.10} \newcommand{\haathee}{ Intel Xeon E5640 Processors with a {\mb{12}} smart cache, {\gb{144}} of DDR3 DRAM, and running Ubuntu Linux 11.10}   %-- table formatting \newlength{\onedigit} \settowidth{\onedigit}{} \newcommand{\w}{\makebox[\onedigit]{~}}   \newcounter{todocount} \setcounter{todocount}{1} \newcommand{\todo}[1]{{\color{blue}*** 	[\thetodocount] #1 ***\addtocounter{todocount}{1}}} % % File acl2020.tex % %% Based on the style files for ACL 2020, which were %% Based on the style files for ACL 2018, NAACL 2018/19, which were %% Based on the style files for ACL-2015, with some improvements %%  taken from the NAACL-2016 style %% Based on the style files for ACL-2014, which were, in turn, %% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009, %% EACL-2009, IJCNLP-2008... %% Based on the style files for EACL 2006 by  %%e.agirre@ehu.es or Sergi.Balari@uab.es %% and that of ACL 08 by Joakim Nivre and Noah Smith   \documentclass[11pt]{article} \usepackage{coling2020} \usepackage{times} \usepackage{url} \usepackage{latexsym}  \renewcommand{\UrlFont}{\ttfamily\small}  \usepackage{booktabs} % For formal tables \usepackage[normalem]{ulem} \usepackage{xcolor} %%xl: I need xcolour.... \usepackage{algorithm} \usepackage{algpseudocode} \usepackage{amsmath} \usepackage{mathrsfs}  \usepackage{amssymb} \usepackage{subfigure} \usepackage{makecell} \usepackage{mathtools} \usepackage[font=rm]{caption} % \usepackage{subcaption} \DeclareCaptionType{copyrightbox} \usepackage{shortvrb} \usepackage{tabularx} \usepackage{verbatim} \usepackage{xspace} \usepackage{listings} \lstset{basicstyle=\small\ttfamily,mathescape,columns=fullflexible,keepspaces=true} \usepackage{fontawesome} \usepackage[multiple]{footmisc} \usepackage[all]{nowidow} \usepackage{balance} % This is not strictly necessary, and may be commented out, % but it will improve the layout of the manuscript, % and will typically save some space. \usepackage{microtype} \usepackage{wrapfig} %\aclfinalcopy % Uncomment this line for the final submission %\def\aclpaperid{***} %  Enter the acl Paper ID here  %\setlength\titlebox{5cm} % You can expand the titlebox if you need extra space % to show all the authors. Please do not make the titlebox % smaller than 5cm ; we will check this % in the camera-ready version and ask you to change it back.  \usepackage[medium,compact]{titlesec} \usepackage{enumitem} \setlist{itemsep=0pt,parsep=0pt}  \colingfinalcopy    \title{Interactive Question Clarification in Dialogue via Reinforcement Learning}  \author{       Xiang Hu\footnotemark[2]    \\ %       Ant Financial Services Group\footnotemark[2]\\    Hasso Plattner Institute, University of Potsdam\footnotemark[3]\\     \\\And    Zujie Wen\footnotemark[2] \\ %   Rutgers University\\    %       \\\And    Yafang Wang \footnotemark[2] \thanks{\ \  corresponding author, email: yafang.wyf@antfin.com}   \\ %   Ant Financial Services Group\\  %    %   \thanks{Corresponding author, Email: yafang.wyf@antfin.com}   \\\And    Xiaolong Li\footnotemark[2] \\ %   Rutgers University\\    %       \\\And   Gerard de Melo\footnotemark[3]  \\ %   Ant Financial Services Group\\  %    \\    tu   }  \date{}         
"," %闂侇偅淇虹换鍐矗瀹ュ锛栭柣銊ュ閺岀喎顕ｈ箛鏃傜獮婵炴挸鎳庤ぐ鏌ユ嚄閽樺鏁ㄩ柡澶堝劜濡倕鈻旈弴銏╂殨闁哄牏鍠撳▓鎴︽偨閵婏箑鐓曢柛娆忕Ч椤╊參鏁嶇仦鑺ヨ含闁活亞鍠庨悿鍕寲閼姐倗鍩犲☉鎿冨幖缁扁晠宕楅妷銈囩憹缁绢収鍠栭悾楣冨箑瑜嬫穱uestion reformulation闁哄倽顫夌涵璺侯嚗鐎典即寮悩宕囥婂ù鍏煎椤撴悂骞嶉柡鍫濐槹缂嶆棃宕烽妸銉ヨ闁煎疇濮よ閸屾碍绀堟慨婵勫栭崹婊勭椤戝灝鈻忛柣鈧妺缁斿绮斿鍕攭濞存粍甯掔槐锟犳儍閸曨垱锛栧Λ鐗埳戠欢鐐层掗崨顔界厵婵炲娲 % \todo{explain defect of previous works} Coping with ambiguous questions has been a perennial problem in real-world dialogue systems. Although clarification by asking questions is a common form of human interaction, it is hard to define appropriate questions to elicit more specific intents from a user. In this work, we propose a reinforcement model to clarify ambiguous questions by suggesting refinements of the original query. We first formulate a collection partitioning problem to select a set of labels enabling us to distinguish potential unambiguous intents. We list the chosen labels as intent phrases to the user for further confirmation. The selected label along with the original user query then serves as a refined query, for which a suitable response can more easily be identified. The model is trained using reinforcement learning with a deep policy network.  We evaluate our model based on real-world user clicks and demonstrate significant improvements across several different experiments. % The ability to ask clarification questions to solve ambiguity and missing information phenomena is essential for question answering systems. The current research mainly uses questions generation or questions ranking to ask a clarification question, which lead to low success rate and redundant information. Insufficient use of the graphic user interface  results in more interactions with users. There is usually no guarantee for replying the user after the clarification. To solve these problems, we propose a question clarification method based on intents recommendation. intents are extracted from the historical Frequently Asked Questions of our system. The recommended intents can provide more concise candidates for user to click. Once an intent is clicked, the system guaranteed to provide a clear question list relative to the real question. We use the reinforcement learning method to recommend intents, and the most challenging problem is cold start. The reward is designed to recommend the most relevant clear question list and maximize the information gain after clicking one intent for better question clarification. The method we proposed for question clarification can solve both ambiguity and missing information phenomena. Experiments on interactions with more than 100 million real-world online users shows the effectiveness of this method.",371
" %Discourse Parsing is a key NLP %an important task, aiming to establish a better understanding of multi-sentential natural language. %, which is inherently ambiguous and intent-driven.  %Most research in the area thereby focuses on one of the two main discourse theories RST  or PDTB , both proposed over a decade ago. Discourse Parsing is a key Natural Language Processing  task for processing multi-sentential text. Most research in the area focuses on one of the two main discourse theories -- RST  or PDTB . The latter thereby postulates shallow discourse structures, combining adjacent sentences and mainly focuses on explicit and implicit discourse connectives. The RST discourse theory, on the other hand, proposes discourse trees over complete documents in a constituency-style manner, with tree leaves as so called Elementary Discourse Units , representing span-like sentence fragments. Internal tree-nodes encode discourse relations between sub-trees as a tuple of \{Nuclearity, Relation\}, where the nuclearity defines the sub-tree salience in the local context, and the relation further specifies the type of relationship between the binary child nodes   with automatically inferred discourse structures and nuclearity attributes from large-scale sentiment datasets already reached state-of-the-art  performance on the inter-domain discourse parsing task. Similarly, \citet{liu2018learning} infer latent discourse trees from the text classification task, and \citet{liu2019single} employ the downstream task of summarization using a transformer model to generate discourse trees. Outside the area of discourse parsing, syntactic trees have previously been inferred according to several strategies, e.g. \citet{socher2011semi, yogatama2016learning, choi2018learning, maillard2019jointly}. %including: Discrete decisions frameworks using a Gumbel-softmax component , applying a reinforcement approach to syntactic parsing , using the reconstruction error of adjacent spans as an indicator for syntactic coherence within a sentence  or by employing a CKY approach to select syntactic trees from a soft model .  In general, the approaches mentioned above  %to automatically annotate text with discourse structures or syntactic trees  have shown to capture valuable structural information. Some models outperform baselines trained on human-annotated datasets , others have proven to enhance diverse downstream tasks . However, despite these initial successes, one critical limitation that all aforementioned models share is the task-specificity, possibly only capturing downstream-task related information. %of discourse,  This potentially compromises the generality of the resulting trees, as for instance shown for the model using text classification data  in \citet{ferracane2019evaluating}.  %For instance, the approach by \citet{huber2019predicting} uses document-level sentiment information to inform the discourse tree generation, with others %have been  %using summarization data  or sentence-level sentiment cues  to achieve the results.  In order to alleviate this limitation of task-specificity, we propose a new strategy to generate tree structures in a task-agnostic, unsupervised fashion by extending the latent tree induction framework proposed by \citet{choi2018learning} with an auto-encoding objective. %.  Our system thereby extracts important knowledge from natural text by optimizing both the underlying tree structures and the distributed representations. We believe that the resulting discourse structures effectively aggregate related and commonly appearing patterns in the data by merging coherent text spans into intermediate sub-tree encodings, similar to the intuition presented in \citet{drozdov2019unsupervised}. However, in contrast to the approach by \citet{drozdov2019unsupervised}, our model makes discrete structural decisions, rather than joining possible subtrees using a soft attention mechanism. We believe that our discrete tree structures allow the model to more efficiently achieve the autoencoder objective in reconstructing the inputs, directly learning how written language can be aggregated in the wild . In general, the proposed approach can be applied to any tree-structured objective, such as syntactic parsing, discourse parsing and further problems outside of NLP, like tree-planning  and decision-tree generation . Yet, due to the especially difficult annotation process to generate discourse trees, we initially develop a method to %complement task-specific models in  generate much larger and more diverse discourse treebanks.    
"," Discourse information, as postulated by popular discourse theories, such as RST and PDTB, has been shown to improve an increasing number of downstream NLP tasks, showing positive effects and synergies of discourse with important real-world applications. While methods for incorporating discourse become more and more sophisticated, the growing need for robust and general discourse structures has not been sufficiently met by current discourse parsers, usually trained on small scale datasets in a strictly limited number of domains. This makes the prediction for arbitrary tasks noisy and unreliable. The overall resulting lack of high-quality, high-quantity discourse trees poses a severe limitation to further progress.  In order the alleviate this shortcoming, we propose a new strategy to generate tree structures in a task-agnostic, unsupervised fashion by extending a latent tree induction framework with an auto-encoding objective. The proposed approach can be applied to any tree-structured objective, such as syntactic parsing, discourse parsing and others. However, due to the especially difficult annotation process to generate discourse trees, we initially develop a method to generate larger and more diverse discourse treebanks. In this paper we are inferring general tree structures of natural text in multiple domains, showing promising results on a diverse set of tasks.  %With this paper, we intend to initiate a new line of research on inferring discourse structures in an unbiased manner. %With a growing need for robust and general discourse structures in many downstream tasks and real-world applications, the current lack of high-quality, high-quantity discourse trees poses a severe shortcoming. %In order the alleviate this limitation, we propose a new strategy to generate tree structures in a task-agnostic, unsupervised fashion by extending a latent tree induction framework with an auto-encoding objective. The proposed approach can be applied to any tree-structured objective, such as syntactic parsing, discourse parsing and others. However, due to the especially difficult annotation process to generate discourse trees, we initially develop such method to complement task-specific models in generating much larger and more diverse discourse treebanks.",372
"  Retrieval technique or response selection is a very popular and elegant approach  to framing a chatbot i.e. open-domain dialog system. Given the conversation context, a retrieval-based chatbot aims to select the most appropriate utterance as a response from a pre-constructed database. %that saves a large number of human written utterances. In order to balance the effectiveness and efficiency, mosts of the retrieval-based chatbots  employ coarse-grained selection module to recall a set of candidate  that are semantic coherent with the conversation context to speed up processing.  % 鐠囧瓨妲戦敍姘妧娑斿孩鏅ラ悳鍥ф嫲閺佸牊鐏夐獮鏈电瑝閼宠棄鍙忛柈銊ュ絿瀵 % 閸滃elated work闁插矂娼伴柌宥咁槻娴滃棴绱濋惄瀛樺复閸掔姴骞撻敍 To the best of our knowledge, there are two kinds of approaches  to build a coarse-grained selection module in retrieval-based chatbots:  sparse representation:  TF-IDF or BM25  is a widely used method. It matches keywords with an inverted index and can be seen as representing utterances in highdimensional sparse vectors ; %This method runs very quickly, but lacks rich semantic information.  dense representation:  Large scale pre-trained langauge models , e.g. BERT  are commonly used to obtain the semantic representation of utterances, which could be used to recall semantic coherent candidates by using cosine similarity . %Due to the high computational burden of similarity calculating, %this method runs slowly, but could consider rich semantic information %.  % 鐠囧瓨妲戦惄顔煎楠炶埖妫ょ化鑽ょ埠娑擃亜顕В鏃撶礉鐠囧瓨妲戠紒鍡氬Ν閸欘垯浜掗崷銊ョ杽妤犲奔鑵戦幍鎯у煂閿涘矂妲撻弰搴＄杽妤犲瞼绮ㄩ弸婊冩嫲dense vectors閻ㄥ墜eakness閿涘瞼鍔ч崥搴＄穿閸戠儤鍨滄禒顒傛畱閸欙缚绔存稉鐚祌oposed method % Luan2020SparseDA鏉╂瑤閲滅拋鐑樻瀮娑旂喕顕╅弰搴濈啊BM25閺堝妞傞崐娆愭櫏閺嬫粍娲挎總 % Dense 閺鐟版倳 BERT So far, there is no systematic comparison between these two kinds of approaches in retrieval-based chatbots, and which kind of method is most appropriate in real scenarios is  still an open question that confuses researchers in dialog system community. Thus, in this paper, we first conduct extensive experiment to compare these two approaches from four important aspects:   effectiveness;  search time cost;  index storage occupation;  human evaluation. Extensive experiment results on four popular response selection datasets  demonstrate that the dense representation  significantly outperforms the sparse representation at the expense of  the lower speed and bigger storage than sparse representation, which is unsufferable in real scenarios. Then, in order to overcome the fatal weaknesses of dense representation methods, we propose an ultra-fast, low-storage and highly effective  Deep Semantic Hashing Coarse-grained selection module  %based on a given dense representation method, which effectively balances the effectiveness and efficiency. Specifically,  we first stack a novel hashing optimizing module that consists of two autoencoders on a given  dense representation method. Then, three well designed loss functions are used to optimize  these two autoencoders in hashing optimizing module:  preserved loss;  hash loss;  quantization loss. After training, the autoencoders could effectively preserve rich semantic and similarity information of the dense vectors into the hash codes, which are very computational and storage efficient . \iffalse first of all, we train a dense representation method  by using dual-architecture , which contains a context BERT encoder and a candidate BERT encoder. Then, we separately stack an deep autoencoder model on each encoder. The auto-encoder model could encode the semantic information in dense vectors into hashing codes. Finally, a novel deep semantic hashing approach is used to learn the binary compressed representation of the dense vectors. % 鐠囧瓨妲戞禍宀冪箻閸掕泛鎼辩敮宀绱惍浣烘畱娴兼ê濞嶉敍灞肩瑝閸氬奔绨瑂parse閸滃畳ense缂傛牜鐖滈惃鍕偨婢跺嫨 It should be noted that,  different from the dense vectors,  binary hashing code is storage-efficient and ultra-fast to calculate ,  and it also keeps the rich semantic information in dense vectors. \fi Extensive experiment results on four popular response selection datasets demonstrate that our proposed DSHC model can achieve much faster search speed  and lower storage occupation than sparse representation method, and very limited performance loss compared with the given dense representation method.  In this paper, our contributions are three-fold:   The rest of this paper is organized as follows: we introduce the important concepts and background covered in our paper in Section 2. The experiment settings is presented in Section 3. In Section 4, we systematically compare the current two kinds of methods in coarse-grained selection module:   sparse representation;  dense representation. In Section 5, we introduce our proposed DSHC model, and detailed experiment results are elaborated. In Section 6, we conduct the case study. Finally, we conclude our work in Section 7. Due to the page limitation, more details and extra analysis can be found in Appendix.  
","   We study the coarse-grained selection module in retrieval-based chatbot.   Coarse-grained selection is a basic module in a retrieval-based chatbot,   which constructs a rough candidate set from the whole database to speed up the interaction with customers.   So far, there are two kinds of approaches for coarse-grained selection module:     sparse representation;  dense representation.   To the best of our knowledge, there is no systematic comparison between these two approaches in retrieval-based chatbots,   and which kind of method is better in real scenarios is still an open question.   In this paper, we first systematically compare these two methods from four aspects:     effectiveness;  index stoarge;  search time cost;  human evaluation.   Extensive experiment results demonstrate that dense representation method    significantly outperforms the sparse representation,    but costs more time and storage occupation.   In order to overcome these fatal weaknesses of dense representation method,    we propose an ultra-fast, low-storage, and highly effective    Deep Semantic Hashing Coarse-grained selection method, called DSHC model.   Specifically, in our proposed DSHC model,   a hashing optimizing module that consists of two autoencoder models is    stacked on a trained dense representation model,   and three loss functions are designed to optimize it.   The hash codes provided by hashing optimizing module effectively    preserve the rich semantic and similarity information in dense vectors.   Extensive experiment results prove that,   our proposed DSHC model can achieve much faster speed and lower storage than sparse representation,   with limited performance loss compared with dense representation.   Besides, our source codes have been publicly released for future research\footnote{\url{https://github.com/gmftbyGMFTBY/HashRetrieval}}.",373
"  With huge quantities of natural language documents, search engines have been essential for the time saved on information retrieval tasks. Usually, deployed search engines achieve the task of ranking documents by relevance according to a query. \\ Recently, research has focused on the task of extracting the span of text that exactly matches the user's query through Machine  Reading Comprehension and Question Answering. \\ Question Answering deals with the extraction of the span of text in a short paragraph that exactly answers a natural language question. Recent deep learning models based on heavy pretrained language models like BERT achieved better than human performances on this tasks .  \\ One could try to apply QA models for the Open-Domain Question Answering paradigm which aims to answer questions taking a big amount of documents as knowledge source. Two main issues emerge from this : first, applying 100M parameters language models to potentially millions of documents requires unreasonable GPU-resources. Then, QA models allow to compare spans of text coming exclusively from a single paragraph while in the open-domain QA paradigm, one needs to compare spans of text coming from a wide range of documents. \\ Our system, as done in previous work, deals with the resources issue thanks to a Retriever module, based on the BM25 algorithm, that allows to reduce the search space from millions of articles to a hundred of paragraphs. The second issue is tackled by adding a deep learning based Scorer module that re-ranks with more precision the paragraphs returned by the Retriever. Eventually, the Extractor module uses a QA deep learning model to extract the best span of text in the first paragraph returned by the Scorer. To avoid a heavy and hardly scalable pipeline consisting of two huge deep learning models, we parallelize the re-ranking and span extraction tasks thanks to multitask learning : while maintaining high performances, it allows to significantly reduce both memory requirements and inference time. Our system achieve state-of-the-art results on the open-squad benchmark.  
","   In this paper, we introduce MIX : a   multi-task deep learning approach to solve Open-Domain Question  Answering. First, we design our system as a multi-stage pipeline made of 3 building blocks : a BM25-based Retriever, to reduce the search space; RoBERTa based Scorer and Extractor, to rank retrieved paragraphs and extract relevant spans of text respectively. Eventually, we further improve computational efficiency of our system to deal with the scalability challenge : thanks to multi-task learning,   we parallelize the close tasks solved by the Scorer and the Extractor. Our system is on par with state-of-the-art performances on the squad-open benchmark while being simpler conceptually.",374
"  Named Entity Recognition  is the task of identifying the span and the class of a Named Entity  in unstructured text. NEs typically include but are not limited to persons, companies, dates, and geographical locations .   Legal NER is a central task in language processing of legal documents, especially for extracting key information such as the name of the parties in a case, the court name or the case number, or references to laws or judgements, to name a few. The extracted NEs could be integrated in legal research workflows for functionalities such as search, document anonymization or case summarization  thereby enabling and expediting insights for legal professionals .  NER is commonly formalized as a sequence labeling task: each token of the document is assigned a single label that indicates whether the token belongs to an entity from a predefined set of categories . To create a training dataset in such a format the annotator is required to manually label each token in a sentence with the respective category. In this format, both the NE and the location of the NE in the source text are known. This format of training data is what we refer to hereafter as 閳ユ笀old standard閳 data. Obtaining the required voluminous gold standard data to train such models is, therefore, a laborious and costly task.    In this paper, we perform NER in filed lawsuits in US courts. Specifically, we aim to identify the party names in each case, i.e. the names of the plaintiffs and the defendants, in a large collection of publicly available cases from more than 200 courts in different US jurisdictions. The party names have been identified by legal annotators but their exact location in the text is unknown. In this respect, we do not have access to 閳ユ笀old standard閳 training data even though the target NEs are available. This feature of our dataset introduces a key difference of our task to most NER tasks.  One solution to this problem is to generate the 閳ユ笀old standard閳 training data by searching for the locations of the known NEs in the source text . By performing this additional transformation to our data, we would be able to train sequence labeling NER models. For the following reasons, this solution is nontrivial. First, as our source text is also extracted from scanned PDF files , it contains Optical Character Recognition  mistakes and/or typos which may not be present in the target NEs. Second, besides the potential OCR errors at the character level, the closely spaced, two-column page layouts that can be often found as headers in the filed cases, represent an additional challenge for the OCR, which tends to concatenate the text across columns . In such cases, the tokens that make up the NEs in the source text may be intertwined with other words and/or sentences. Third, variations of the names may be also present in the source text and in our human-generated labels, such as presence of first and/or middle names whole or as initials and, to a lesser extent, typos.        To address some of the challenges imposed by the format of our training data and inspired by the work in the field of abstractive summarization, we propose to reformulate the NER task, not as a sequence labeling problem, but as a text-to-text sequence generation problem with the use of a pointer generator network . With this reformulation, in contrast to sequence labeling, we do not require knowledge of the NE閳ユ獨 locations in the text as training labels. A recent study by \citet{Li2020} proposed a different formulation of the NER task as a question answering task and achieved state-of-the-art performance in a number of published NER datasets . In this study, we adopt a hybrid extractive-abstractive architecture, based on recurrent neural networks coupled with global  attention and copying  attention  mechanisms . The proposed architecture can be successfully used for abstractive summarization since it can copy words from the source text via pointing and can deal effectively with out-of-vocabulary  words 閳 words that have not been seen during training. Our approach is conceptually simple but empirically powerful and we show that the pointer generator outperforms the typical NER architectures in the case of noisy and lengthy inputs where the NE's location in the text is not known.   In addition, we examine how our approach can be used for the related NER task of case number extraction. The case number is a unique combination of letters, numbers and special characters as a single token and are, therefore, particularly challenging for NER models as they are often dealt with as OOV words by the model. As in the party names task discussed above, in the case number task we do not have 閳ユ笀old standard閳 labels of the case number閳ユ獨 location in the text. We show that a character level sequence generation network can dramatically increase our ability to extract case numbers from the source text, compared to a word level sequence generation network.  The rest of the paper is organized as follows. In Section 2, we discuss related work in the field of NER in the legal domain. In Section 3, we describe our proposal of NER as a text-to-text sequence generation task in the absence of gold standard data and formulate the task in two ways:  as a combination of automatically labeling the NE's location and then using the conventional sequence labeling method for NER, and  as a text-to-text sequence generation task where the NEs are directly generated as text. Section 4 presents our experimental design, results and analysis. Section 5 presents the case number case study. Finally, we conclude and discuss directions for future work.  
"," Named Entity Recognition  is the task of identifying and classifying named entities in unstructured text. In the legal domain, named entities of interest may include the case parties, judges, names of courts, case numbers, references to laws etc. We study the problem of legal NER with noisy text extracted from PDF files of filed court cases from US courts. The 闁炽儲绗old standard闁 training data for NER systems provide annotation for each token of the text with the corresponding entity or non-entity label. We work with only partially complete training data, which differ from the gold standard NER data in that the exact location of the entities in the text is unknown and the entities may contain typos and/or OCR mistakes. To overcome the challenges of our noisy training data, e.g. text extraction errors and/or typos and unknown label indices, we formulate the NER task as a text-to-text sequence generation task and train a pointer generator network to generate the entities in the document rather than label them. We show that the pointer generator can be effective for NER in the absence of gold standard data and outperforms the common NER neural network architectures in long legal documents.",375
" Speech translation~, which translates audio signals of speech in one language into text in a foreign language, is a hot research subject nowadays and has widespread applications, like cross-language videoconferencing or customer support chats.   Traditionally, researchers build a speech translation system via a cascading manner, including an automatic speech recognition~ and a machine translation~ subsystem. Cascade systems, however, suffer from error propagation problems, where an inaccurate ASR output would theoretically cause translation errors.  Owing to recent progress of sequence-to-sequence modeling for both neural machine translation~ and end-to-end speech recognition, it becomes feasible and efficient to train a fully end-to-end ST model. This end-to-end fashion attracts much attention due to its appealing properties: a) modeling without intermediate ASR transcriptions obviously alleviates the propagation of errors; b) a single and unified ST model is beneficial to deployment with lower latency in contrast to cascade systems.  % However, the end-to-end paradigm is far from reaching industry requirements because it requires large-scale end-to-end corpora of audios paired with textual translations, which is hard to acquire.  Recent studies show that end-to-end ST models achieve promising performance and are comparable with cascaded models. The end-to-end solution has great potential to be the dominant technology for speech translation, however challenges remain.  The first is about benchmarks. Many ST studies conduct experiments on different datasets. ~\citet{liu2019end} evaluate the method on TED English-Chinese; and ~\citet{dong2020ted} use Augmented Librispeech English-French and  IWSLT2018 English-German dataset; and ~\citet{wu2020self} show the results on CoVoST dataset and the FR/RO portions of MuST-C dataset. Different datasets make it difficult to compare the performance of their approaches. Further,  even for the same dataset, the baseline results are not necessarily kept in the consist. Take the Augmented Librispeech English-French dataset as an example.  ~\citet{dong2020ted} report the pre-trained baseline as 15.3 and the result of ~\citet{liu2019end} is 14.3 in terms of tokenized BLEU, while~\citet{inaguma2020} report 15.5 . The mismatching baseline makes the comparison of their final results meaningless.  One of the primary reasons is that the preprocessing of audio data is complex, and the ST model training involves many tricks, such as pre-training and data augmentation.    Therefore a reproducible and reliable benchmark is required. In this work, we present \method, a toolkit for easily building and training end-to-end ST models, as well as end-to-end ASR and NMT for cascade systems.  We implement start-of-the-art Transformer-based models and provide step-by-step recipes for feature extraction, data preprocessing, model training, and inference for researchers to reproduce the benchmarks. Though there exist several counterparts, such as Lingvo, fairseq-ST and Kaldi~ style~ESPnet-ST,  \method is specially designed for speech translation tasks, which encapsulates the details of speech processing and frees the developers from data engineering. It is easy to use and extend. The contributions of this work are as follows:      % \method provides straightforward preprocessing of several publicly available audio datasets, which encourages researchers to concentrate more on innovating in ST technology but to be less aware of speech processing.  % \method aims at ST tasks using end-to-end framework. Moreover, to our knowledge,  is the pioneer of the community, which follows the Kaldi~ style data processing and recipes. But for \method, we stand in the perspective of natural language processing~. 
"," \method is an open-source toolkit for neural speech translation developed by Bytedance AI Lab.  The toolkit mainly focuses on end-to-end speech translation, which is easy to use, modify, and extend to advanced speech translation research and products.   \method aims at facilitating the speech translation research for NLP researchers and provides a complete setup for speech translation benchmarks, including feature extraction, data preprocessing, distributed training, and evaluation.  Moreover, The toolkit implements several major architectures for end-to-end speech translation. It shows experimental results for different benchmark datasets, which can be regarded as reliable baselines for future research. The toolkit is publicly available at \url{https://github.com/bytedance/neurst}.",376
" Query reformulation and paraphrase generation techniques are employed for a variety of purposes in natural language processing , such as dialogue generation , machine translation , and especially in question answering  systems . Generating coherent and clean texts can reduce potential errors in downstream systems. In the cases when users are at the receiving end of NLP pipelines, it is essential to show them fluent and human-like languages before they lose faith and recede into requiring human agents for the sake of better understanding and communication. In search or question answering systems, query reformulation aims to paraphrase or restructure original question sequences, transforming them into ones that are more interpretable with natural well-formedness in both grammar and semantics. Typically, users may not have the patience to input an entirely grammatical or coherent question, which can cause issues for the downstream components to understand and give accurate predictions or answers. When human representatives are present, an originally noisy query or question can be reiterated and rephrased to double-check with users what they are asking for. This is a costly operation if every convoluted question needs to be restated. By having an NLP model to reformulate input queries, reformulations are fed back to users to confirm their original intentions in an automated way. As a result, unnecessary errors are eliminated and noises are prevented from propagating in an NLP pipeline, which can contain a series of models such as intent classification, information retrieval and question answering.  Traditionally, rule-based and statistical methods have been studied for paraphrase and reformulation generation . The advent of sequence-to-sequence learning   made it feasible to train deep neural networks as a new paradigm. We investigate how to paraphrase and denoise queries and generate well-formed reformulations using Seq2Seq learning models such as LSTMs  and transformers . Following the framework from AQA , a Seq2Seq model is pre-trained on supervised tasks and further tuned using reinforcement learning  on a machine comprehension QA dataset SearchQA , learning from a pre-trained BiDAF  QA system that generates rewards. SearchQA is a suitable and challenging dataset as queries contain noisy phrases and the associated contexts are concatenated web text snippets from Google's search engine. Our goal is to obtain a model that can generate better-formed reformulations based on the original query sequences and achieve good QA performance with these reformulations. We use transfer learning  from pre-trained transformers with text-to-text task formulations . In our approach, pre-trained T5 models are first fine-tuned on paraphrase generation  and denoising  datasets to gain general paraphrasing capabilities. Then, reinforcement learning of downstream QA rewards is performed to further encouraged the model to produce task-specific reformulations. To our knowledge, this is a first attempt to fine-tune text-to-text transformers with RL, nudging the model to generate reward-acquiring query trajectories to get better answers. We show that fine-tuned text-to-text transformers are better starting points for RL as they are more sample efficient in achieving the same level of QA performance, acquiring rewards faster than the previous AQA approach that uses translation-based LSTMs. T5 models also generate reformulations with better readability and can generalize to out-of-sample data. We provide a new way to evaluate fluency on a sequence level using an trained metric on the well-formedness   dataset, which is based on real evaluations from humans, a more reliable source than widely-used algorithmic metrics based on overlapping n-grams.   
"," Query reformulation aims to alter potentially noisy or ambiguous text sequences into coherent ones closer to natural language questions. In this process, it is also crucial to maintain and even enhance performance in a downstream environments like question answering when rephrased queries are given as input. We explore methods to generate these query reformulations by training reformulators using text-to-text transformers and apply policy-based reinforcement learning algorithms to further encourage reward learning. Query fluency is numerically evaluated by the same class of model fine-tuned on a human-evaluated well-formedness dataset. The reformulator leverages linguistic knowledge obtained from transfer learning and generates more well-formed reformulations than a translation-based model in qualitative and quantitative analysis. During reinforcement learning, it better retains fluency while optimizing the RL objective to acquire question answering rewards and can generalize to out-of-sample textual data in qualitative evaluations. Our RL framework is demonstrated to be flexible, allowing reward signals to be sourced from different downstream environments such as intent classification.",377
" % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  %.     %      % % final paper: en-us version      % %       % space normally used by the marker %     This work is licensed under a Creative Commons  %     Attribution 4.0 International License. %     License details: %     \url{http://creativecommons.org/licenses/by/4.0/}. %}  Relation extraction aims to extract relations between entities in text, where distant supervision proposed by automatically establishes training datasets by assigning relation labels to instances that mention entities within knowledge bases. However, the wrong labeling problem can occur and various multi-instance learning methods have been proposed to address it. Despite the wrong labeling problem, each instance in distant supervision is crawled from web pages, which is informal with many noisy words and can express multiple similar relations. This problem is not well-handled by previous approaches and severely hampers the performance of conventional neural relation extractors. To handle this problem, we have to address two challenges:  Identifying and gathering spotted relation information from low-quality instances;  Distinguishing multiple overlapped relation features from each instance.    First, a few significant relation words are distributed dispersedly in the sentence, as shown in Figure, where words marked in red brackets represent entities, and italic words are key to expressing the relations. For instance, the clause ``evan\_bayh son of birch\_bayh"" in S1 is sufficient to express the relation /people/person/children of evan\_bayh and birch\_bayh. Salient relation words are few in number and dispersedly in S1, while others excluded from the clause can be regarded as noise. Traditional neural models have difficulty gathering spotted relation features at different positions along the sequence because they use Convolutional Neural Network  or Recurrent Neural Network  as basic relation encoders, which model each sequence word by word and lose rich non-local information for modeling the dependencies of semantic salience. Thus, a well-behaved relation extractor is needed to extract scattered relation features from informal instances.  Second, each instance can express multiple similar relations of two entities. As shown in Figure, Changsha and Hunan possess the relations /location/location/contains and /location/province/capital in S2, which have similar semantics, introducing great challenges for neural extractors in discriminating them clearly. Conventional neural methods are not effective at extracting overlapped relation features, because they mix different relation semantics into a single vector by max-pooling or self-attention. Although  first propose an attentive capsule network for multi-labeled relation extraction, it treats the CNN/RNN as low-level capsules without the diversity encouragement, which poses the difficulty of distinguishing different and overlapped relation features from a single type of semantic capsule. Therefore, a well-behaved relation extractor is needed to discriminate diverse overlapped relation features from different semantic spaces.  To address the above problem, we propose a novel Regularized Attentive Capsule Network  to identify highly overlapped relations in the low-quality distant supervision corpus. First, we propose to embed multi-head attention into the capsule network, where attention vectors from each head are encapsulated as a low-level capsule, discovering relation features in an unique semantic space. Then, to improve multi-head attention in extracting spotted relation features, we devise relation query multi-head attention, which selects salient relation words regardless of their positions. This mechanism assigns proper attention scores to salient relation words by calculating the logit similarity of each relation representation and word representation. Furthermore, we apply disagreement regularization to multi-head attention and low-level capsules, which encourages each head or capsule to discriminate different relation features from different semantic spaces. Finally, the dynamic routing algorithm and sliding-margin loss are employed to gather diverse relation features and predict multiple specific relations. We evaluate RA-CapNet using two benchmarks. The experimental results show that our model achieves satisfactory performance over the baselines. Our contributions are summarized as follows:    
","   Distantly supervised relation extraction has been widely applied in knowledge base construction due to its less requirement of human efforts. However, the automatically established training datasets in distant supervision contain low-quality instances with noisy words and overlapped relations, introducing great challenges to the accurate extraction of relations. To address this problem, we propose a novel Regularized Attentive Capsule Network  to better identify highly overlapped relations in each informal sentence. To discover multiple relation features in an instance, we embed multi-head attention into the capsule network as the low-level capsules, where the subtraction of two entities acts as a new form of relation query to select salient features regardless of their positions. To further discriminate overlapped relation features, we devise disagreement regularization to explicitly encourage the diversity among both multiple attention heads and low-level capsules. Extensive experiments conducted on widely used datasets show that our model achieves significant improvements in relation extraction.",378
" 	Identifying the user's open intent plays a significant role in dialogue systems. As shown in Figure, we have two known intents for specific purposes, such as book flight and restaurant reservation. However, there are also utterances with irrelevant or unsupported intents that our system cannot handle. It is necessary to distinguish these utterances from the known intents as much as possible. On the one hand, effectively identifying the open intent can improve customer satisfaction by reducing false-positive error. On the other hand, we can use the open intent to discover potential user needs. 	 	We regard open intent classification as an -class classification task as suggested in, and group open classes into the  class . Our goal is to classify the n-class known intents into their corresponding classes correctly while identifying the  class open intent. To solve this problem,~\citet{scheirer2013toward} propose the concept of open space risk as the measure of open classification.~\citet{fei-liu-2016-breaking} reduce the open space risk by learning the closed boundary of each positive class in the similarity space. However, they fail to capture high-level semantic concepts with SVM.  	~\citet{bendale2016towards} manage to reduce the open space risk through deep neural networks , but need to sample open classes for selecting the core hyperparameters.~\citet{hendrycks17baseline} use the softmax probability as the confidence score, but also need to select the confidence threshold with negative samples.~\citet{Shu2017DOCDO} replace softmax with the sigmoid activation function, and calculate the confidence thresholds of each class based on statistics. However, the statistics-based thresholds can not learn the essential differences between known classes and the open class.~\citet{lin-xu-2019-deep} propose to learn the deep intent features with the margin loss and detect unknown intents with local outlier factor. However, it has no specific decision boundaries for distinguishing the open intent, and needs model architecture modification.  	 	Most of the existing methods need to design specific classifiers for identifying the open class and perform poorly with the common classifier. Moreover, the performance of open classification largely depends on the  decision conditions. Most of these methods need negative samples for determining the suitable decision conditions. It is also a complicated and time-consuming process to manually select the optimal decision condition, which is not applicable in real scenarios.  	 	To solve these problems, we use known intents as prior knowledge, and propose a novel post-processing method to learn the adaptive decision boundary  for open intent classification. As illustrated in Figure, we first extract intent representations from the BERT model. Then, we pre-train the model under the supervision of the softmax loss. We define centroids for each known class and suppose known intent features are constrained in the closed ball areas. Next, we aim to learn the radius of each ball area to obtain the decision boundaries. Specifically, we initialize the boundary parameters with standard normal distribution and use a learnable activation function as a projection to get the radius of each decision boundary.  	 	The suitable decision boundaries should satisfy two conditions. On the one hand, they should be broad enough to surround in-domain samples as much as possible. On the other hand, they need to be tight enough to prevent out-of-domain samples from being identified as in-domain samples. To address these issues, we propose a new loss function, which optimizes the boundary parameters by balancing both the open space risk and the empirical risk. The decision boundaries can automatically learn to adapt to the intent feature space until balance with the boundary loss. We find that our post-processing method can still learn discriminative decision boundaries to detect the open intent even without modifying the original model architecture. 	 	We summarize our contribution as follows. Firstly, we propose a novel post-processing method for open classification, with no need for prior knowledge of the open class. Secondly,  we propose a new loss function to automatically learn tight decision boundaries adaptive to the feature space. To the best of our knowledge, this is the first attempt to adopt deep neural networks to learn the adaptive decision boundary for open classification. Thirdly, extensive experiments conducted on three challenging datasets show that our approach obtains consistently better and more robust results compared with the state-of-the-art methods.  	 		 	\end{table*} 	
"," 		Open intent classification is a challenging task in dialogue systems. On the one hand, we should ensure the classification quality of known intents. On the other hand, we need to identify the open  intent during testing. Current models are limited in finding the appropriate decision boundary to balance the performances of both known and open intents. In this paper, we propose a post-processing method to learn the adaptive decision boundary  for open intent classification. We first utilize the labeled known intent samples to pre-train the model. Then, we use the well-trained features to automatically learn the adaptive spherical decision boundaries for each known intent. Specifically, we propose a new loss function to balance both the empirical risk and the open space risk. Our method does not need open samples and is free from modifying the model architecture. We find our approach is surprisingly insensitive with less labeled data and fewer known intents. Extensive experiments on three benchmark datasets show that our method yields significant improvements compared with the state-of-the-art methods.\footnote{Code: https://github.com/thuiar/Adaptive-Decision-Boundary}",379
"  Recently, deep contextual language models have shown their effective modeling ability for text, achieving state-of-the-art results in series of NLP tasks. These models capture the syntactic and semantic information of the input text, generating fine-grained contextual embeddings, which can be easily applied to downstream models. Despite the success of large scale pre-trained language models on various tasks,  it is less clear how to extend them to semantic parsing tasks such as text-to-SQL, which requires joint reasoning of the natural language utterance and structured database schema information. Recent work shows that with more powerful pre-trained language models, the highly domain-specific semantic parsers can be further improved, even though these language models are trained for pure text encoding.  %      %    \end{table}%  However, based on error analysis on the output of neural language model-based text-to-SQL systems, we observe that these models can be further enhanced if we could mitigate the following three pain points, which are also illustrated in Table.  The model is ineffective to match and detect column names in utterances. The model should learn to detect column names mentioned in utterances by matching utterance tokens with the schema, and use the matched columns in the generated SQL.  The error analysis indicates that, in some cases, models miss some columns when synthesizing the target SQL, while the column is mentioned explicitly in the utterance.   The model fails to infer the columns implicitly from cell values. This problem is trickier than the first one, because the model is expected to infer the column name based on some cell values mentioned in the utterance, instead of just matching the utterance tokens with the schema. This requires the model to have more domain knowledge. For example, as presented in the second section of Table, the model should know  is a .  The model should learn to compose complex queries. Besides the column selection, to generate a correct SQL, the model should learn to attach the selected columns to the correct clauses. This is a non-trivial task, especially when the target SQL is complex, e.g., when the query is nested. As shown in the last section of Table, the model should learn to use corresponding column  in the nested SQL, instead of using column .  Recent work has demonstrated that jointly pre-training on utterances and table contents  can benefit downstream tasks such as table parsing and semantic parsing . These models are pre-trained using the Masked Language Modeling  task by either masking tokens from the utterance input or tokens from the schema input. However, this learning objective can only model the alignment between the utterance and schema implicitly. We hypothesize that, in order to cope with the three pain points previously listed, it is necessary to use pre-training objectives that enforce the learning of contextual representations that better capture the alignment between utterances and schema/table contents.  In this work, we present a language model pre-training framework, Generation-Augmented Pre-training~, that exploits multiple learning objectives  and synthetic data generation to jointly learn contextual representations of natural language utterances and table schema. We propose the following three new learning objectives that not only enforce joint learning but also improve the ability of the model to grasp more domain knowledge, which is helpful in cross-domain scenarios:  column prediction task, which is a pre-training task that consists in giving a label for each column in the input schema to decide whether it is used in the input utterance or not. This task is intent to improve the column detection ability of the model.  column recovery task,  which consists in randomly replacing some of the column names with one of their cell values and asking the model to recover the original column name either based on the cell value itself or based on the contextual information of the utterance when the column is explicitly mentioned in the utterance. This learning objective is meant to enhance the column inferring ability of the model.  SQL generation, which consists in generating SQL queries given utterances and schema. This task can boost the ability of the model to compose complex queries by leveraging large scale SQL datasets from the Web.%, such as Github.  A key challenge to use the proposed pre-training tasks is training data. Although it is easy to obtain large scale datasets of crawled tables and SQL queries,  it is difficult to obtain high-quality utterances interrelated with the tables or logically consistent with crawled SQL queries. Recent work used the surrounding text of tables as a proxy of natural language utterances. However, this option is far from optimal because those texts are dissimilar to user utterances in terms of text length, composition and content. The surrounding text of a table is usually a paragraph, while natural language utterances in the downstream task are short sentences. Furthermore, the content of surrounding text of tables can be quite noisy because the text may be irrelevant to the table. In \modelname, we overcome the pre-training data challenge through the use of synthetic data. We propose two sequence-to-sequence  generative models, SQL-to-text and table-to-text, that can produce large scale datasets with enough quality for pre-training. We train our generative models by finetuning BART, a state-of-the-art pre-trained language model. Concurrently,~\citet{yu2020grappa} and~\citet{deng2020structure} utilized synthetic data generated from synchronized context-free grammar and existing data-to-text datasets for pre-training, respectively, which requires extra crowd and expert annotation efforts.  The outcome of \modelname is a pre-trained model that can be plugged into neural semantic parsers to compute contextual representations of utterances and schema. We apply \modelname to text-to-SQL semantic parsing datasets, and experimental results show that systems augmented with \modelname~outperform state-of-the-art semantic parsers on Spider and Criteria-to-SQL datasets. In summary, our work presents the following main contributions:   
","  Most recently, there has been significant interest in learning contextual representations for various NLP tasks, by leveraging large scale text corpora to train large neural language models with self-supervised learning objectives, such as Masked Language Model~. However, based on a pilot study, we observe three issues of existing general-purpose language models when they are applied to text-to-SQL semantic parsers: fail to detect column mentions in the utterances, fail to infer column mentions from cell values, and fail to compose complex SQL queries. To mitigate these issues, we present a model pre-training framework, Generation-Augmented Pre-training~, that jointly learns representations of natural language utterances and table schemas by leveraging generation models to generate pre-train data. \modelnamelm\footnote{This refers to the language models that are pre-trained with GAP framework.} is trained on 2M utterance-schema pairs and 30K utterance-schema-SQL triples, whose utterances are produced by generative models. Based on experimental results, neural semantic parsers that leverage \modelnamelm~as a representation encoder obtain new state-of-the-art results on both Spider and Criteria-to-SQL benchmarks.",380
"    Neural Machine Translation   yields  state-of-the-art translation performance when a large number of parallel sentences are available. However, only a few parallel corpora are available for the majority of language pairs and domains. It has been known that NMT does not perform well in the specific domains where the domain-specific corpora are limited, such as medical domain. As such, high-quality domain-specific machine translation  systems are in high demand whereas general purpose MT has limited applications.  There are many studies of domain adaptation for NMT, which can be mainly divided into two categories: data-centric and model fine-tuning.  Data-centric methods focus on  selecting or generating target domain data from general domain corpora, which is effective and well explored.  In this paper, we focus on the second approach. Fine-tuning is  very common in domain adaptation, which first trains a base model on the general domain data and then fine-tunes it on each target domain . However, unconstrained or full fine-tuning  requires very careful hyper-parameter tuning,  and is prone to over-fitting on the target domain as well as forgetting on the general domain. To tackle these problems, researchers have proposed several constructive approaches, with the view to limiting the size or plasticity of parameters in the fine-tuning stage, which can be roughly divided into two categories: regularization and partial-tuning strategy. Regularization methods often integrate extra training objectives to prevent parameters from large deviations, such as model output regularization , elastic weight consolidation  . Regularization methods, which impose arbitrary global constraints on parameter updates, may further restrict the adaptive process of the network, especially when domain-specific corpora are scarce. Partial-tuning methods either freeze several sub-layers of the network and fine-tune the others, or integrate domain-specific adapters into the network. By only fine-tuning the domain-specific part of the model, they can alleviate the over-fitting and forgetting problem in fine-tuning. However, the structure designed to adapting is usually hand-crafted, which relies on experienced experts and the adapter brings additional parameters. Therefore,  a more adaptive, scalable, and parameter-efficient approach for domain adaptation is very valuable and worth well studying.    In this paper, we propose \method, a novel domain adaptation method via adaptive structure pruning. Our motivation is inspired from Continual Learning  and the lottery hypothesis that a randomly-initialized, dense neural network contains a sub-network which  can match the test accuracy of the original network after training for at most the same number of iterations.   We therefore suppose that multiple  machine translation models for different domains can share different sparse subnetworks within a single neural network.   Specifically, we first apply a standard pruning technique to automatically uncover the subnetwork from a well-trained NMT model in the general domain.  The  subnetwork is capable of  reducing the parameter without compromising accuracy. Therefore, it has the potential to keep as much general information as possible.   Then we freeze this informative sparse network and leave the unnecessary  parameters unfixed for the target  domain, which enables our approach to be parameter efficient, and eases the scalability of the approach to more domains.  The capacity of these non-fixed parameters can be tuned to match the requirements of the target domain, while keeping the parameters of the general domain. Our method successfully circumvents catastrophic forgetting problem and retains the quality on the general domain.  As the benefits of the flexible design, \method can be easily extended to other transfer learning problems, such as multilingual machine translation.      We summarize our main contribution as follows:      % --------------------Background-------------------- 
"," Fine-tuning is a major approach for domain adaptation in Neural Machine Translation .  However, unconstrained fine-tuning requires very careful hyper-parameter tuning otherwise it is easy to fall into over-fitting on the target domain and degradation on the general domain.  To mitigate it, we propose \method, a novel domain adaptation method via gradual pruning.  It learns tiny domain-specific subnetworks for tuning. During adaptation to a new domain, we only tune its corresponding subnetwork.  \method alleviates the over-fitting and the degradation problem without model modification. Additionally, with no overlapping between domain-specific subnetworks, \method is also capable of sequential multi-domain learning.    Empirical experiment results show that \method outperforms several strong competitors in the target domain test set without the quality degradation of the general domain in both single and multiple domain settings. \footnote{The source code and data are available at \url{https://github.com/ohlionel/Prune-Tune}}",381
" As an important task in a dialogue system, response selection aims to find the best matched response from a set of candidates given the context of a conversation. The retrieved responses usually have natural, fluent and diverse expressions with rich information owing to the abundant resources. Therefore, response selection has been widely used in industry and has attracted great attention in academia.  Most existing studies on this task pay more attention to the matching problem between utterances and responses, but with insufficient concern for the reasoning issue in multi-turn response selection. Just recently, MuTual, the first human-labeled reasoning-based dataset for multi-turn dialogue, has been released to promote this line of research. Reasoning is quite different from matching in the conversations. Specifically, matching focuses on capturing the relevance features between utterances and responses, while reasoning not only needs to identify key features , but also needs to conduct inference based on these clue words. The challenges of this new task include:  how to identify the clue words in utterances, which is fundamental for inference;  how to conduct inference according to the clue words in utterances. Figure illustrates a motivating example. To infer the current time, we must first identify the clue words `10:45' in  and `15 minutes' in . Then we must conduct a logical inference based on these clue words in  and .    To tackle these challenges, first, we need better contextual representation for identifying the clue words in conversations. This is because clue word identification inevitably relies on the context of a conversation. Although previous literature publications have achieved promising results in context modeling, there are still several limitations of these approaches. More concretely, the existing studies either concatenate the utterances to form context or process each utterance independently, leading to the loss of dependency relationships among utterances or important contextual information. It has been validated that the chronological dependency between utterances, as well as the semantical dependency between utterances, are crucial for multi-turn response selection. Thus, how to model the dependencies in utterances remains a challenging problem for context representation.  Second, we need to devise a new strategy to collect the clue words scattered in multiple utterances and need to reason according to these clue words. In recent years, we have witnessed great success in KBQA  and MRC  tasks. However, new obstacles emerge for transferring current reasoning approaches in KBQA and MRC to conversational reasoning.  A clear reasoning path based on entities in a well-structured knowledge base exists in KBQA, but there is no similar reasoning path in utterances.  Current approaches on MRC conduct inference based on graph while taking shared entities as nodes, while it is difficult to construct such graphs based on entities in short utterances, which usually suffer from greater coreference resolution, poor content and serious semantic omission problems in comparison with document text.  In this paper, we propose a new model named GRN  which can tackle both challenges in an end-to-end way. We first introduce two pre-training tasks called NUP  and UOP  which are specially designed for response selection. NUP endows GRN with context-aware ability for semantical dependency, and UOP facilitates GRN with the ability to capture the chronological dependency. These customized pre-training methods are beneficial for modeling dependencies contained in utterances to achieve better context representation. We perform task-adaptive pre-training with the combined NUP and UOP tasks based on the ALBERT model. To conduct reasoning based on clue words, we devise a graph neural network called UDG , which not only models the dependencies between utterances with each utterance as a node but also collects the clue words from different utterances. Reasoning is achieved by propagating the messages of clue words between nodes along various utterance paths on UDG, and this graph reasoning structure realizes the inference based on an utterance-level context vector with local perspective. On the other hand, we also implement a reasoning network by the output of the trained model and self-attention mechanism. This sequence reasoning structure realizes the inference based on the highly summarized context vector with global perspective. To summarize, we make the following contributions:    
"," We investigate response selection for multi-turn conversation in retrieval-based chatbots. Existing studies pay more attention to the matching between utterances and responses by calculating the matching score based on learned features, leading to insufficient model reasoning ability. In this paper, we propose a graph reasoning network  to address the problem. GRN first conducts pre-training based on ALBERT using next utterance prediction and utterance order prediction tasks specifically devised for response selection. These two customized pre-training tasks can endow our model with the ability of capturing semantical and chronological dependency between utterances. We then fine-tune the model on an integrated network with sequence reasoning and graph reasoning structures. The sequence reasoning module conducts inference based on the highly summarized context vector of utterance-response pairs from the global perspective. The graph reasoning module conducts the reasoning on the utterance-level graph neural network from the local perspective. Experiments on two conversational reasoning datasets show that our model can dramatically outperform the strong baseline methods and can achieve performance which is close to human-level.",382
"  A disease is an abnormal medical condition that poses a negative impact on the organisms and enabling access to disease information is the goal of various information extraction as well as text mining tasks. The task of disease normalization consists of assigning a unique concept identifier to the disease names occurring in the clinical text. However, this task is challenging as the diseases mentioned in the text may display morphological or orthographical variations, may utilize different word orderings or equivalent words. Consider the following examples: %}   \end{center} In Example 1, the disease mention short trunk and extremities should be mapped to a candidate Knowledge Base entry containing synonyms like Growth Disorder. In Example 2, Renal amyloidosis should be assigned to Knowledge Base ID  which has synonyms such as, Amyloidosis 8.  Based on our studies and analysis of the medical literature, it has been observed that the same disease name may occur in multiple variant forms such as. synonyms replacement , spelling variation , a short description modifier precedes the disease name , different word orderings .   In this paper, we have formulated the task of learning mention-candidate pair similarity using Triplet Networks . Furthermore, we have explored in-domain word\footnote{http://evexdb.org/pmresources/vec-space-models/} and subword embeddings  as input representations. We find that sub-word information boosts up the performance due to gained information for out-of-vocabulary terms and word compositionality of the disease mentions.  The primary contributions of this paper are three-fold:  1) By identifying positive and negative candidates concerning a disease mention, we optimize the Triplet Network with a loss function that influences the relative distance constraint  2) We have explored the capability of in-domain sub-word level information\footnote{https://github.com/ncbi-nlp/BioSentVec.git} in solving the task of disease normalization. 3) Unlike existing systems , , we present a robust and portable candidate generation approach without making use of external resources or hand-engineered sieves to deal with morphological variations. Our system achieves state-of-the-art performance on NCBI disease dataset    
","   Entity linking  is an essential task in text mining that maps the entity mentions in the medical text to standard entities in a given Knowledge Base . This task is of great importance in the medical domain. It can also be used for merging different medical and clinical ontologies. In this paper, we center around the problem of disease linking or normalization. This task is executed in two phases: candidate generation and candidate scoring. In this paper, we present an approach to rank the candidate Knowledge Base entries based on their similarity with disease mention. We make use of the Triplet Network for candidate ranking. While the existing methods have used carefully generated sieves and external resources for candidate generation, we introduce a robust and portable candidate generation scheme that does not make use of the hand-crafted rules. Experimental results on the standard benchmark NCBI disease dataset demonstrate that our system outperforms the prior methods by a significant margin.",383
" As a fundamental task in natural language processing , coherence analysis can benefit various downstream tasks, such as sentiment analysis  and document summarization . Rhetorical Structure Theory   is one of the most influential theories of text coherence, under which a document is represented by a hierarchical discourse tree, which consists of a set of semantic units organized in the form of a dependency structure, labeled with their rhetorical relations. As shown in Figure , the leaf nodes of an RST discourse tree are basic text spans called Elementary Discourse Units , and the EDUs are iteratively connected by rhetorical relations  to form larger text spans until the entire document is included.  The rhetorical relations are further categorized to Nucleus and Satellite based on their relative importance, in which Nucleus corresponds to the core part while Satellite corresponds to the subordinate part. While manual coherence analysis under the RST theory is labor-intensive and requires specialized linguistic knowledge, a discourse parser serves to automatically transform a document into a discourse tree. Document-level discourse parsing consists of three sub-tasks: hierarchical span splitting, rhetorical nuclearity determination, and rhetorical relation classification.    Models for RST-style discourse parsing have made much progress in the past decade. While statistical methods utilize hand-crafted lexical and syntactic features , data-driven neural approaches reduce feature-engineering labor by effective representation learning, and are capable of characterizing implicit semantic information. Neural networks are first used as feature extractors along with traditional shift-reduce approaches  or dynamic programming approaches . Then, \citet{yu2018transition} bridges the gap between neural and traditional methods by an end-to-end transition-based neural parser via an encoder-decoder architecture. Recently, pointer networks are introduced to achieve linear-time complexity, and models with top-down parsing procedures achieve favorable results on sentence-level discourse analysis tasks .  However, there is still much space for improvement in document-level discourse parsing. First, compared to sentence-level parsing, document-level parsing is more challenging due to the deeper tree structures and longer dependencies among EDUs: in the benchmark dataset RST Discourse Tree Bank  , the average EDU number at the document level is 56, which is 20 times larger than that of sentence-level parsing. Thus modeling context information across a long span is essential, especially if considering a top-down parsing procedure where poor accuracy at the top of the tree will propagate toward the leaf nodes. Second, the three sub-tasks of discourse parsing strongly rely on nuanced semantic judgments, which require comprehensive contextual representation with various types of linguistic information. Take discourse relation classification for example, explicit relations are overtly signaled by a connective word such as ``although'' and ``because'', which can be determined by lexical and syntactic features. However, this approach can not be readily adapted to implicit discourse relations determination, as it requires high-order features with semantic information. Moreover, to compensate for the lack of large-scale corpora, prior work in neural modeling has leveraged inductive biases through syntactic features such as part-of-speech tagging to improve performance. However, such models still suffer from insufficient linguistics information from the lack of data, thus they are incapable of acquiring deeper and richer contextual representations useful for discourse processing.  In this paper, to tackle the aforementioned challenges, we propose a document-level neural discourse parser with robust representation modeling at both the EDU and document level, based on a top-down parsing procedure. To take advantage of widely-adopted vector representations that encode rich semantic information, we first exploit a large-scale pre-trained language model as a contextual representation backbone.  Then we incorporate boundary information with implicit semantic and syntactic features to the EDU representations, and introduce a hierarchical encoding architecture to more comprehensively characterize global information for long dependency modeling. To improve inference accuracy and alleviate the aforesaid error propagation problem, we present breadth-first span splitting to propose a layer-wise beam search algorithm.  We train and evaluate our proposed model on the benchmark corpus RST-DT\footnote{https://catalog.ldc.upenn.edu/LDC2002T07} , and achieve the state-of-the-art performance on all fronts, significantly surpassing previous models while approaching the upper bound of human performance. We also conduct extensive experiments to analyze the effectiveness of our proposed method.  
"," Document-level discourse parsing, in accordance with the Rhetorical Structure Theory , remains notoriously challenging. Challenges include the deep structure of document-level discourse trees, the requirement of subtle semantic judgments, and the lack of large-scale training corpora. To address such challenges, we propose to exploit robust representations derived from multiple levels of granularity across syntax and semantics, and in turn incorporate such representations in an end-to-end encoder-decoder neural architecture for more resourceful discourse processing. In particular, we first use a pre-trained contextual language model that embodies high-order and long-range dependency to enable finer-grain semantic, syntactic, and organizational representations. We further encode such representations with boundary and hierarchical information to obtain more refined modeling for document-level discourse processing. Experimental results show that our parser achieves the state-of-the-art performance, approaching human-level performance on the benchmarked RST dataset.",384
" Due to the substantial growth and effortless access to the Internet in recent years, an enormous amount of unstructured textual contents have generated. It is a crucial task to organize or structure such a voluminous unstructured text in manually. Thus, automatic classification can be useful to manipulate a huge amount of texts, and extract meaningful insights which save a lot of time and money. Text categorization is a classical NLP problem which aims to categorize texts into organized groups. It has a wide range of applications like machine translation, question answering, summarization, and sentiment analysis. There are several approaches available to classify texts according to their labels. However, deep learning method outperforms the rule-based and machine learning-based models because of their ability to capture sequential and semantic information from texts . We propose a classifier using CNN , and BiLSTM  to classify technical texts in the computer science domain. Furthermore, by sequentially adding these networks, remarkable accuracy in several shared classification tasks can be obtained. The rest of the paper is organized as follows: related work given in section 2. Section 3 describes the dataset. The framework described in section 4. The findings presented in section 5.   %%%%%%%%%%%% Related Work %%%%%%%%% 
"," This paper illustrates the details description of technical text classification system and its results that developed as a part of participation in the shared task TechDofication 2020. The shared task consists of two sub-tasks:  first task identify the coarse-grained technical domain of given text in a specified language and  the second task classify a text of computer science domain into fine-grained sub-domains. A classification system  is developed to perform the classification task using three techniques: convolution neural network , bidirectional long short term memory  network, and combined CNN with BiLSTM. Results show that CNN with BiLSTM model outperforms the other techniques concerning task-1 of sub-tasks  and task-2a. This combined model obtained $f_1$ scores of 82.63 , 81.95 , 82.39 , 84.37 , and 67.44  on the development dataset. Moreover, in the case of test set, the combined CNN with BiLSTM approach achieved that higher accuracy for the subtasks 1a , 1b , 1c , 1g  and 2a .",385
" The traditional task-oriented dialogue systems, which focuses on providing information and performing actions by the given databases or APIs, often meet the limitation that the DB/API can not cover enough necessary cases. A good enhance can be achieved with lots of relevant domain knowledge in the form of descriptions, FAQs and customer reviews, which we call unstructured knowledge. Track 1 of the 9th Dialogue System Technology Challenges , Beyond Domain APIs: Task-oriented Conversational Modeling with Unstructured Knowledge Access, aims at generating a response based on dialogue history and unstructured knowledge access. The whole task can be divided into three subtasks, knowledge-seeking turn detection, knowledge selection and knowledge-grounded response. Test set of this track includes seen and unseen parts. The unseen test set are collected on different domains, entities, and locales, aiming to evaluate models' generalization ability.   Knowledge-seeking turn detection, as the first subtask, needs to determine whether the related knowledge is contained in the unstructured knowledge base. In other words, this subtask can be modeled as a binary classification problem. If the model predicts that there exists related knowledge, then subtask 2  will search for the most relevant knowledge snippets and then pass them to the generation process . If the model predicts that there is no related knowledge for the specific question, the remaining two subtasks will not be performed. In this paper, we first conduct an entity matching for each question and then add the domain label from matching results to the end of dialogue history as model input.  Knowledge selection is to retrieve the most relevant knowledge snippets from the database according to the dialogue history and provide information for the subsequent response generation. The dialogue history is a conversation between the human speaker and the machine. Close to the end of the conversation, the human speaker brings up a question about a certain place  or service . The given knowledge database consists of question-answer pairs involving diverse facts and is organized by different domains and entities. % Note that the knowledge-seeking turn detection model determines whether our dialog system needs to access the knowledge database before generating the response.  % We perform knowledge selection for the samples  that requires relevant knowledge in the database. The retrieved knowledge snippets provide information for the subsequent response generation.  % Information retrieval  techniques are widely applied to search for related candidates in retrieval-based knowledge-grounded system. Some researchers  compute the traditional tf-idf score to search the most relevant document to the user's query, while others leverage the power of neural networks to learn the ranking score directly through an end-to-end learning process. Recently, due to the significant improvements on numerous natural language processing tasks, large scale pre-trained language models have also been applied to better model the semantic relevance in knowledge selection.  In this paper, we first apply retrieval techniques to narrow down the searching space and then use a neural network initialized by a pre-trained model to formulate the ranking function. % We propose two base models for the knowledge selection, and the final ensemble model combines the predictions of different base models to improve the selection performance.  % The Retrieve \& Rank model first gathers the knowledge snippets of potentially relevant entities from the knowledge base, then a ranking model is trained to select the most plausible knowledge snippets from the retrieved candidates. % Different from the Retrieve \& Rank model, Three-step model divides the ranking model into three cascade parts to rank domain, entity and documents respectively in order to force the model to take the knowledge hierarchy into account. % We also ensemble these two models together and experiments show the ensemble model has a better performance than two base model separately.   % briefly introduce the three-step pipeline model.   Knowledge-grounded response generation requests to give a response automatically from the model using dialogue history and unstructured knowledge as input. There are two different types of dialogue systems, retrieval-based system, and generation-based system. Retrieval-based dialogue system, giving responses from a list of candidate sentences, only has fixed answer forms in candidate sets. To deal with our problem, which needs more flexible and natural responses, the generation-based model is a better choice. Dialogue generation requires an encoder to represent the input and a decoder to generate the response. The network often needs to minimize the cross-entropy loss between the output and the ground truth. In this paper, we use a latent variable to encode dialog history and selected knowledge better and generate responses combined with copy mechanism.  % Pre-trained language models make a great progress on dialogue generation. Note that bi-directional model is not designed for dialogue generation task, and thus PLATO and  PLATO-2 use uni- and bi-directional processing for pre-training. Moreover, large-scale Reddit and Twitter conversations are utilized to further pre-train the generation model to reduce data distribution gaps. Furthermore, a latent variable  is used to capture one-to-many relations of post-response pairs.  As shown in released evaluation results, our proposed system ranks second under objective metrics and ranks fourth under human metrics. In the following sections, we will explain the details of our proposed model. Experiment results will be shown next with some analysis and conclusions.  
"," Task-oriented conversational modeling with unstructured knowledge access, as track 1 of the 9th Dialogue System Technology Challenges , requests to build a system to generate response given dialogue history and knowledge access. This challenge can be separated into three subtasks,  knowledge-seeking turn detection,  knowledge selection, and  knowledge-grounded response generation. We use pre-trained language models, ELECTRA and RoBERTa, as our base encoder for different subtasks. For subtask 1 and 2, the coarse-grained information like domain and entity are used to enhance knowledge usage. For subtask 3, we use a latent variable to encode dialog history and selected knowledge better and generate responses combined with copy mechanism. Meanwhile, some useful post-processing strategies are performed on the model's final output to make further knowledge usage in the generation task.  As shown in released evaluation results, our proposed system ranks second under objective metrics and ranks fourth under human metrics.",386
"  %    Recent years have witnessed the rapid advancement of online recruitment platforms. With the increasing amount of online recruitment data, more and more interview related studies have emerged such as person-job  fit and automatic analysis of asynchronous video interviews , which aim to enable automated job recommendation and candidate assessment. Among these studies, person-job fit is to casting the task as a supervised text match problem. Given a set of labeled data , it aims to predict the matching label between the candidate resumes and job description. More recently, deep learning has enhanced person-job fit methods by training more effective text match or text representations models. AVI is to determine whether the candidate is hirable by evaluating the answers of interview questions. In AVIs, an interview is usually considered as a sequence of questions and answers containing salient socials signals. To evaluate the candidates more comprehensively, AVI models will extract the features of video , text, and voice in the process of answering questions. In this work, we focus on the scoring of multiple QA pairs,  we only extract the features of text modality and define this task as the scoring competency of candidates rather than the score of whether or not to be employed. Based on the anatomy of the human interviewers' evaluation process, the solutions consist of two stages:  analyzing and evaluating individual QA pair one by one, then acquiring the evaluation status, and  grading the competency of the candidate based on the evaluation status of multiple QA pairs.      For the first stage, existing methods tend to employ text matching or attentional text matching algorithms to evaluate QA pairs, which feeds the concatenated representation of the question and the answer to the subsequent classifier. As we all know, questions in an asynchronous video interview are not limited to specific domains. That is to say, candidates can answer questions according to their work or study experience. In this way, the candidates' answers will be varied and it is difficult to evaluate the answer accurately only by text matching. Intuitively, it is more reasonable to evaluate QA pairs through the semantic interaction between questions and answers. A critical challenge along this line is how to reveal the latent relationships between each question and answer.  %Intuitively, experienced interviewers could discover the semantic-level correlation between interview questions and candidates' answers, then obtain a preliminary judgement on the answer to the current question, and finally give an assessment based on the judgements of several problems. Therefore,  %In this work, we propose a sentence-level reasoning GNN to assess the single QA pair at the semantic interaction level. Graph neural networks  can learn effective representation of nodes by encoding local graph structures and node attributes. Due to the compactness of model and the capability of inductive learning, GNNs are widely used in modeling relational data and logical reasoning. Recently, ~\citet{zhang2020efficient} proposed a GNN variant, Named ExpressGNN, to strike a nice balance between the representation power and the simplicity of the model in probabilistic logic reasoning.~\citet{ghosal2019dialoguegcn} constructed the DialogeGCN to address context propagation issues present in the RNN-based methods. Specifically, they leverage self and inter-speaker dependency of the interlocutors to model conversational context for emotion recognition. Inspired by, we present a sentence-level relational GCN to represent the internal temporal and QA interaction dependency in the process of answering questions. %Recently, graph neural network or graph emebedding has attracted wide attention. Graph neural networks have been effective at tasks thought to have rich relational structure and can preserve global structure information of a graph in graph emebedding.   %In this work, we aim to address the task of automatically scoring the textual answer of candidates at the semantic interaction level.  %The automatic short answer scoring  is a task of estimating a score of a short text answer written as response to a given prompt on the basis of whether the answer satisfies the rubrics prepared by a human in advance. ASAS systems have mainly been constructed to markedly reduce the scoring cost of human rater.   %鐟欏嫬鍨鍫ユ閸掕泛鐣鹃敍灞芥礈濮濄倕顕梻顕顣介崪灞芥礀缁涙棃妫跨拠顓濈疅娴溿倓绨伴惃鍕閹烘ɑ妯夊妤佹纯閸旂娀鍣哥憰 %閸ョ偓膩閸ㄥ婀梻顕顣介幒銊ф倞娴犺濮熸稉濂僥ep learning has proven to be effective in long text NLP tasks. Due to the lack of information in the short sentence of the ASAS corpus, it seems not good enough in the ASAS task.  For the second stage of grading the candidate, based on the representation of QA pairs, exists methods prefer to encoder question-answer pairs as a sequence directly. However, this kind of approaches lead to insufficient interaction between the semantic information of question and answer pairs. Therefore, it is difficult to ensure the rationality and explainability of the evaluation. To mitigate this issue, in the first stage, we present a semantic-level graph attention network  to model the interaction states of each QA session.    %Automatic scoring of answer transcriptions in job interview aims to evaluate multiple question-answer pairs.  %To alleviate this limitation of previous approaches, To this end, we propose a Hierarchical Reasoning Graph Neural Network  for the automatic scoring of answer transcriptions  in job interviews. Specifically, the proposed sentence-level relational graph convolutional neural network  is used to capture the contextual dependency, and the semantic-level Reasoning graph attention network  is applied to acquire the latent interaction states. And the contribution of our work can be summarized as follows:   
"," %Automatic scoring of answer transcripts in job interview aims to evaluate multiple question-answer pairs. The key challenge is how to conduct deep interaction on the semantic level for each question-answer pair, and give the evaluation results combined with multiple interaction states. Recent studies either use text matching approaches to evaluate each question-answer pair roughly, or employ the sequential model to deal with disordered question-answer pairs which fail to take advantages of the semantic association between questions and answers, and the logical connection between question-answer pairs. In this work, we propose a hierarchical reasoning Graph Neural Network  for the automatic assessment of multi-question answering. Specifically, we construct a sentence-level reasoning GNN to assess the single question-answer pair. Based on these graphs, we propose a document-level reasoning GNN to model the interaction states of question-answer pairs. The first module utilizes each sentence in the question and answer to establish the connection between them. The second module adopts a graph convolutional network to encoder interaction states of each pair and aggregates evidence with graph attention mechanism for predicting the final score. Empirical results on Chinese and English interview datasets show that our proposed model outperforms both sequence-based and pre-training based  benchmark models.  %We address the task of automatically scoring the answer competency of candidates based on textual features from the automatic speech recognition transcriptions. The key challenge is how to conduct deep interaction on the semantic level for each question-answer  pair, and give the evaluation results combined with multiple interaction states. Recent studies either use text matching approaches to evaluate each QA pair roughly, or employ the sequential model to deal with disordered QA pairs which fail to take advantages of the semantic association between questions and answers, and the logical connection between QA pairs. In this work, we propose a hierarchical reasoning Graph Neural Network  for the automatic assessment of question-answer pairs. Specifically, we construct a sentence-level reasoning GNN to assess the single QA pair. Based on these graphs, we propose a document-level reasoning GNN to model the interaction states of QA pairs. The first module utilizes each sentence in the question and answer to establish the connection between them. The second module adopts a graph convolutional network to encoder interaction states of each pair and aggregates evidence with graph attention mechanism for predicting the final score. Empirical results conducted on CHNAT and ENGIAT  clearly validate that our proposed model outperforms both text matching based benchmark models.  %We address the task of automatically scoring the competency of candidates based on textual features, from the automatic speech recognition  transcriptions in the video job interview. The key challenge is how to conduct deep interaction on the semantic level for each question-answer  pair, and then give the evaluation results combined with multiple interaction states. Recent studies tend to use text matching approaches to evaluate each QA pair roughly, which fails to take advantage of the semantic association between questions and answers. In this work, we propose a Hierarchical Reasoning Graph Neural Network  for the automatic assessment of question-answer pairs. Specifically, we construct a sentence-level relational graph neural network to capture the latent semantic interaction of sentences in the question or the answer. Based on these graphs, we employ a semantic-level reasoning graph attention network to model the interaction states of the current QA session. Finally, we propose a gated recurrent unit with a global fusion mechanism to aggregates evidence of temporal QA pairs for the final score. Empirical results conducted on CHNAT  clearly validate that our proposed model significantly outperforms text-matching based benchmark models. Ablation studies and experimental results with 10 random seeds also show the effectiveness and stability of our models.    We address the task of automatically scoring the competency of candidates based on textual features, from the automatic speech recognition  transcriptions in the asynchronous video job interview . The key challenge is how to construct the dependency relation between questions and answers, and conduct the semantic level interaction for each question-answer  pair. However, most of the recent studies in AVI focus on how to represent questions and answers better, but ignore the dependency information and interaction between them, which is critical for QA evaluation. In this work, we propose a Hierarchical Reasoning Graph Neural Network  for the automatic assessment of question-answer pairs. Specifically, we construct a sentence-level relational graph neural network to capture the dependency information of sentences in or between the question and the answer. Based on these graphs, we employ a semantic-level reasoning graph attention network to model the interaction states of the current QA session. Finally, we propose a gated recurrent unit encoder to represent the temporal question-answer pairs for the final prediction. Empirical results conducted on CHNAT  validate that our proposed model significantly outperforms text-matching based benchmark models. Ablation studies and experimental results with 10 random seeds also show the effectiveness and stability of our models.",387
"  Social media is a unique source of information. On the one hand, their low cost, easy access and distribution speed make it possible to quickly share the news. On the other hand, the quality and reliability of social media news is difficult to verify . This is the source of a lot of false information that has a negative impact on society.   Over the past year, the world has been watching the situation developing around the novel coronavirus pandemic. The COVID-19 pandemic has become a significant newsworthy event of 2020. Therefore, news related to COVID-19 are actively discussed on social media and this topic generates a lot of misinformation. Fake news related to the pandemic have large-scale negative social consequences, they provoke huge public rumor spreading and misunderstanding about the COVID-19 and aggravate effects of the pandemic. Moreover, recent studies  show an increase in symptoms such as anxiety and depression in connection with the pandemic. This is closely related to the spread of misinformation, because fake news can be more successful when the population is experiencing a stressful psychological situation . The popularity of fake news on social media can rapidly increase, because the rebuttal is always published too late. In this regard, there is evidence that the development of tools for automatic COVID-19 fake news detection plays a crucial role in the regulation of information flows.  In this paper, we present our approach for the Constraint@AAAI2021 Shared Task: COVID-19 Fake News Detection in English  that attracted 433 participants on CodaLab. This approach achieved the weighted F1-score of 98.69  on the test set among 166 submitted teams in total.  The rest of the paper is organized as follows. A brief review of related work is given in Section 2. The definition of the task has been summarized in Section 3, followed by a brief description of the data used in Section 4. The proposed methods and experimental settings have been elaborated in Section 5. Section 6 contains the results and error analysis respectively. Section 7 is a conclusion.  
"," The COVID-19 pandemic has had a huge impact on various areas of human life. Hence, the coronavirus pandemic and its consequences are being actively discussed on social media. However, not all social media posts are truthful. Many of them spread fake news that cause panic among readers, misinform people and thus exacerbate the effect of the pandemic. In this paper, we present our results at the Constraint@AAAI2021 Shared Task: COVID-19 Fake News Detection in English. In particular, we propose our approach using the transformer-based ensemble of COVID-Twitter-BERT  models. We describe the models used, the ways of text preprocessing and adding extra data. As a result, our best model achieved the weighted F1-score of 98.69 on the test set  of this shared task that attracted 166 submitted teams in total.  \keywords{COVID-Twitter-BERT, social media, fake news, ensembling learning, coronavirus, infodemic, text classification}",388
"  	 	 	      	Medical dialogue system  aims to converse with patients to inquire additional symptoms beyond their self-reports and make a diagnosis automatically, which has gained increasing attention . 	It has a significant potential to simplify the diagnostic process and relieve the cost of collecting information from patients . Moreover, preliminary diagnosis reports generated by MDS may assist doctors to make a diagnosis more efficiently.   	Because of these considerable benefits, many researchers devote substantial efforts  to address critical sub-problems in  MDS, such as natural language understanding , dialogue policy learning, dialogue management, and make promising  progress to build a satisfactory MDS.   	 	 	Medical dialogue generation , which generates responses in natural language to request additional symptoms or make a diagnosis, is critical in MDS but rarely studied. 	Conventional generative dialogue models often employ neural sequence modeling and cannot be applied to the medical dialogue scenario directly in absence of medical knowledge. Recently, large-scale pre-training language models  over unsupervised corpora have achieved significant success.      However, fine-tuning such large language models in the medical domain requires sufficient task-specific data  so as to learn the correlations between diseases and symptoms. 	Unfortunately, as depicted in Fig., there are a large portion of diseases that only have a few instances in practice, which means that newly-coming diseases in the realistic diagnosis scenario are often under low-resource conditions. Therefore, it is highly desirable to transfer the diagnostic experience from high-resource diseases to others of data scarcity.  	Besides, existing knowledge-grounded approaches may fail to perform such transfer well, as they only learn one unified model for all diseases and ignore the specificity and relationships of different diseases. 	Finally, in practice, the disease-symptom relations of each disease may vary or evolve along with more cases, which is also not considered in prior works.    	Contributions.	To address the above challenges, we first propose an end-to-end  dialogue system for the low-resource medical dialogue generation. 	This model integrates three components seamlessly, a hierarchical context encoder,  a meta-knowledge graph reasoning  network and a graph-guided response generator. Among them, the context encoder encodes  the conversation into hierarchical representations. For MGR, it mainly contains a parameterized meta-knowledge graph, which is initialized by a prior commonsense graph and characterizes the correlations among diseases and symptoms.  When fed into the context information, MGR can adaptively evolve its meta-knowledge graph to reason the disease-symptom correlations and then predict related symptoms of the patient in the next response to further determine the disease. Finally, the response generator generates a response for symptoms request  under the guidance of the meta-knowledge graph.   	The second contribution is that we further develop a novel Graph-Evolving Meta-Learning  framework to  transfer the diagnostic experience in the low-resource scenario. Firstly, GEML trains the above medical dialogue model under the meta-learning framework. It regards generating responses to a handful of dialogues as a task and learns a meta-initialization for the above dialogue model that can fast adapt to each task of the new disease with limited dialogues. In this way, the learnt model initialization contains sufficient meta-knowledge\footnote{We name such knowledge as ``meta-knowledge"" since it is obtained through meta-training from different source diseases.} from all source diseases and can serve as a good model initialization to quickly transfer meta-knowledge to a new disease. More importantly, GEML also learns a good parameterized meta-knowledge  graph in the MGR module to characterize the disease-symptom relationships from source diseases. Concretely, under the meta learning framework, for each disease, GEML enriches the meta-knowledge graph via constructing a global-symptom graph from the online dialogue examples. In this way, the learnt meta-knowledge graph can bridge the gap between the commonsense medical graph and the real diagnostic dialogues and thus can be fast evolved for the new target disease. Thanks to graph evolving, the dialogue model can request patients for underlying symptoms more efficiently and thus improve the diagnostic accuracy. Besides,  GEML can also well address the real-world challenge that the disease-symptom correlations could vary along with more cases, since the meta-knowledge  graph is trainable based on collected dialogue examples.  	 	Finally, we  construct a large medical dialogue dataset, called Chunyu\footnote{Code and dataset are released at https://github.com/ha-lins/GEML-MDG.}.  	It covers 15  kinds of diseases and 12,842 dialogue examples totally, and  is much larger than the existing CMDD  medical dialogue dataset. The more challenging benchmark can better comprehensively evaluate the performance of medical dialogue systems.  Extensive experimental results on both datasets demonstrate the superiority of our method over the state-of-the-arts.  
","  	 	Human doctors with well-structured medical knowledge can diagnose a disease merely via a few conversations with patients about symptoms. In contrast, existing knowledge-grounded dialogue systems often require a large number of dialogue instances to learn as they fail to capture the correlations between different diseases and neglect the diagnostic experience shared among them. To address this issue, we propose a more natural and practical paradigm, i.e., low-resource medical dialogue generation, which can transfer the diagnostic experience from source diseases to target ones with a handful of data for adaptation. It is capitalized on a commonsense knowledge graph to characterize the prior disease-symptom relations.  	Besides, we develop a Graph-Evolving Meta-Learning  framework that learns to evolve the commonsense graph for reasoning disease-symptom correlations in a new disease, which effectively alleviates the needs of a large number of dialogues. More importantly, by dynamically evolving disease-symptom graphs, GEML also well addresses the real-world challenges that the disease-symptom correlations of each disease may vary or evolve along with more diagnostic cases. Extensive experiment results on the CMDD dataset and our newly-collected Chunyu dataset testify the superiority of our approach  over state-of-the-art approaches.  	Besides, our GEML can generate an enriched dialogue-sensitive knowledge graph in an online manner, which could benefit other tasks grounded on knowledge graph.",389
" Identifying emotion in dialogues is one of the most challenging tasks in the area of natural language processing, which is very important for building the dialogue systems . Though sentiment analysis is being studied in natural language community for a long time , understanding multiple emotions expressed in chats and conversations comes up with relatively harder challenges  for many reasons.   Various individuals may respond differently towards the same comment. Absence of voice modulations and facial expressions in informal chatting makes it difficult to capture emotion in the conversation. This type of problem is earlier addressed in EmoContext  and EmotionX-2018 . People during chatting often take help of emojis, figures and message contractions not only to reduce effort and shorten comments but also to express their emotions better. In this case, GIFs play an essential role . Often social media users reply with GIFs only, without any text response. Therefore, to understand the different emotions associated with a GIF in the reply, it is necessary to consider the comment and its relation with its associated reply. This type of problem is earlier addressed in EmotionX-2019 challenge  where the task was to predict emotions in spoken dialogues and chats.   The enhanced and better expressiveness of GIFs in comparison to other popular graphics-based media, such as emojis and emoticons have made their utilization amazingly mainstream on social media and a significant expansion to online human communication which has motivated the introduction of EmotionGIF 2020  shared task.   \subsection{Problem Description}  Given an unlabelled tweet and its reply , the challenge is to recommend the possible categories its GIF response may belong to. All tweets in the training set have their GIF responses with some tweets having their text responses as well. The task requires to return a non-empty subset of 1-6 categories from among 43 possible GIF categories for a given unlabelled tweet.     \end{table*}  In this paper, we develop a deep learning framework for predicting the categories of a GIF response for an unlabelled tweet. We build multiple deep neural-based systems, such as CNN , Bidirectional Gated Recurrent Unit  , and Bidirectional Long Short Term Memory Networks  . We support our models with an attention mechanism  that emphasizes on the important parts of a given input tweet. We combine multiple basic models to result in a couple of stacked architectures , and finally, we report the final predictions from a majority voting-based ensemble  method that combines all the developed models. Our proposed frameworks are less complex than the standard transformer models, but can provide reasonably good results and can be trained using local GPU support conveniently.  The rest of the paper is organized as follows: Section 2 gives a brief description of the dataset and the various preprocessing measures applied to them. The details of the proposed methodologies are discussed in section 3. In Section 4, we discuss the various experimental details and their results. Finally, we conclude the paper in section 5.  
"," In this paper, we describe the systems submitted by our IITP-AINLPML team in the shared task of SocialNLP 2020, EmotionGIF 2020, on predicting the category of a GIF response for a given unlabelled tweet.  For the round 1 phase of the task, we propose an attention-based Bi-directional GRU network trained on both the tweet  and their replies  and the given category for its GIF response. In the round 2 phase, we build several deep neural-based classifiers for the task and report the final predictions through a majority voting based ensemble technique. Our proposed models attains the best Mean Recall  scores of 52.92\% and 53.80\% in round 1 and round 2, respectively.",390
"   Machine translation has been shown to exhibit gender bias , and several solutions have already been proposed to mitigate it . The general gender bias in Natural Language Processing  has been mainly attributed to data . Several studies show the pervasiveness of stereotypes in book collections , or Bollywood films , among many others. As a consequence, our systems trained on this data exhibit biases. Among other strategies, several studies have proposed to work in data augmentation to balance data  or forcing gender-balanced datasets . In parallel, other initiatives focus on documenting our datasets  to prioritize transparency.  However, data is not the only reason for biases, and recent studies show that %algorithms and training strategies matter.  our models can be trained in a robust way to reduce the effects of data correlations . In , the authors explored available mitigations and by increasing dropout, which resulted in improving how the models reasoned about different stereotypes in WinoGender examples .   The purpose of the current paper is to explore if the Multilingual Neural Machine Translation  architecture can impact the amount of gender bias. To answer this question, we compare MNMT architectures trained with the same data and quantify their amount of gender bias with the standard WinoMT evaluation benchmark . Results show that the Language-Specific encoders-decoders  exhibit less bias than the Shared encoder-decoder . Then, we analyze and visualize why the MNMT architecture impacts mitigating or amplifying this bias by studying its internal workings. We study the amount of gender information that the source embeddings encode, and we see that Language-Specific surpasses Shared in these terms, allowing for a better prediction of gender.  Additionally, and taking advantage that both Shared and Language-Specific are based on the Transformer , we study the coefficient of variation in the attention , which shows that the attention span is narrower for the Shared system than for the Language-Specific one. Therefore, the context taken into account is smaller for the Shared system, which causes a higher gender bias.   %We observe that this is caused by using a Shared encoder-decoder with several languages since pairwise Bilingual systems have a wider attention span. Given the similarities of not sharing modules and parameters across languages in both Bilingual and Language-Specific, this characteristic  of the Bilingual systems prevails in the language-specific architecture.   Finally, we also do a manual analysis to investigate which biases have a linguistic explanation. %implications in gender bias has the target language from the linguistic and social point of view.    
"," Multilingual Neural Machine Translation architectures mainly differ in the amount of sharing modules and parameters among languages. In this paper, and from an algorithmic perspective, we explore if the chosen architecture, when trained with the same data, influences the gender bias accuracy. Experiments in four language pairs show that Language-Specific encoders-decoders exhibit less bias than the Shared encoder-decoder architecture. Further interpretability analysis of source embeddings and the attention shows that, in the Language-Specific case, the embeddings encode more gender information, and its attention is more diverted. Both behaviors help in mitigating gender bias.",391
" Commonsense question answering  is recently an attractive field in that it requires systems to understand the common sense information beyond words, which are normal to human beings but nontrivial for machines. There are plenty of datasets that are proposed for this purpose, for instance, CommonsenseQA , CosmosQA , WIQA . Different from traditional machine reading comprehension  tasks such as SQuAD  or NewsQA  that the key information for answering the questions is directly given by the context paragraph, solving commonsense questions requires a more comprehensive understanding of both the context and the relevant common knowledge, and further reasoning out the hidden logic between them. There are varieties of knowledge bases that meet the need, including text corpora like Wikipedia, and large-scale knowledge graphs .  Recent popular solution resorts to external supporting facts from such knowledge bases as evidence, to enhance the question with commonsense knowledge or the logic of reasoning . However, the quality of the supporting facts is not guaranteed, as some of them are weak in interpretability so that do not help the question answering. Specifically, current methods are mainly two-fold. The first group of methods  pre-train language models on those external supporting facts  so that the models could remember some of the common knowledge, which is empirically proven by Tandon et al. \shortcite{tandon2019wiqa} and Trinh and Le \shortcite{trinh2018do}. The second group of methods  incorporates the question with knowledge subgraphs or paths that carry information such as relation among concepts or show multi-hop reasoning process. The structured information is typically encoded via graph models such as GCN , and after which merged with the question features. Generally, current methods all handle evidence by brute force, without further selection or refinement according to the interpretability of the supporting facts. But as the example shown in Figure, some of the supporting facts do not interpret the question, regardless that they are semantically related. Thus, there is need for models that will further our processing of the evidence.  In this paper, we introduce a new recursive erasure memory network  that further refines the candidate supporting fact set. The REM-Net consists of three main components: a query encoder, an evidence generator, and a novel recursive erasure memory  module. Specifically, the query encoder is a pre-trained encoder that encodes the question. The evidence generator is a pre-trained generative model that produces candidate supporting facts based on the question. Compared with those retrieved supporting facts, the generated facts provides new question-specific information beyond the existing knowledge bases. The REM module refines the candidate supporting fact set by recursively matching the supporting facts and the question in feature space to estimate each fact's quality. This estimation helps both updating the question feature and the supporting fact set. The question feature is updated by a residual term, whereas the supporting fact set is updated by removing the low-quality facts. Compared with the standard attention mechanisms  that allocate weights to the supporting facts once, the multi-hop operation in REM module widens the gap of how much each supporting fact contributes to the question answering by the number of recursive steps their features are incorporated for the feature update. Therefore this procedure leads to a refined use of given supporting facts.  We conduct experiments on two commonsense QA benchmarks, WIQA  and CosmosQA . The experimental results demonstrate that REM-Net outperforms current methods, and the refined supporting facts are more qualified for the questions. Our contributions are mainly three-fold:     
"," When answering a question, people often draw upon their rich world knowledge in addition to the particular context. While recent works retrieve supporting facts/evidence from commonsense knowledge bases to supply additional information to each question, there is still ample opportunity to advance it on the quality of the evidence. It is crucial since the quality of the evidence is the key to answering commonsense questions, and even determines the upper bound on the QA systems' performance. In this paper, we propose a recursive erasure memory network  to cope with the quality improvement of evidence. To address this, REM-Net is equipped with a module to refine the evidence by recursively erasing the low-quality evidence that does not explain the question answering. Besides, instead of retrieving evidence from existing knowledge bases, REM-Net leverages a pre-trained generative model to generate candidate evidence customized for the question. We conduct experiments on two commonsense question answering datasets, WIQA and CosmosQA. The results demonstrate the performance of REM-Net and show that the refined evidence is explainable.",392
"  We typically train neural machine translation  systems on human-translated parallel texts, then ask them to decode previously-unseen source sentences.  Trained parameter values induce a distribution  over all pairs of source/target strings .  Given a new source string , the NMT decoder searches for the best target string :    This optimization is unsolvable for general recurrent neural networks , while \citet{byrne2019} present an exact optimization search algorithm for consistent NMT models.           \end{table}  In practice, we build up a target string  using a left-to-right, word-by-word greedy strategy.  All target sentences end with the pseudo-word EoS, in both train and test data.  When the greedy search selects EoS, the translation ends.    We can easily find higher-probability strings  with beam search .  However, when we use a large beam, the higher-probability strings turn out to be worse translations, as judged by both Bleu  and human evaluators.  In fact, the highest-probability string is often very short, or even empty .    We therefore typically revert back to a small beam size, hoping for a good translation despite a worse .  When this happens, we have a ``fortuitous'' search error .  As NMT system architectures have moved from LSTM recurrent neural networks  to self-attention Transformer models , the empty translation problem has lessened a bit, but is still very present .   Table shows the behavior of four transformer-based NMT models trained on German-English and Chinese-Japanese parallel data, using decoder beam size~512. The length ratio is the token ratio of the generated translations compared to reference translations. The empty ratio is the percentage of empty translations  over the source sentences. We see that around half the translations on German-English models are empty.  Our central question is: why are empty translations preferred?  Our training data does not contain any source strings translated to empty strings, so why does NMT learn to assign high probability to empty translations? Our findings are:     
","  We investigate why neural machine translation  systems assign high probability to empty translations. We find two explanations. First, label smoothing makes correct-length translations less confident, making it easier for the empty translation to outscore them. Second, NMT systems use the same, high-frequency EoS word type to end all target sentences, regardless of length. This creates an implicit smoothing that increases the relative probability zero-length translations.  Using different EoS types in target sentences of different lengths exposes this implicit smoothing.",393
"   Understanding emotion in human social conversations or chitchat has gained popularity in the natural language processing community due to its usefulness in developing human-like conversational agents. Emotions revealed in social chitchat are rather complex. It has many categories of emotions to distinguish due to subtle variations present in human emotion. For example, Sadness and Disappointment are pursued and dealt differently in human conversations. Also, the listeners' reaction to emotions is not always a straightforward mirroring effect of the speakers' emotions. Rather it can be more neutral and convey a specific intent, as is evident from the dialogue example in Table .      \end{table}    %  containing 25K dialogues grounded in 32 emotions,   Welivita and Pu \shortcite{taxonomy} have analyzed listener responses in the EmpatheticDialogues dataset  and discovered 9 listener specific empathetic response intents contained in emotional dialogues: Questioning; Agreeing; Acknowledging; Sympathizing; Encouraging; Consoling: Suggesting; Wishing; and Neutral . They have automatically annotated the EmpatheticDialogues dataset  with 32 fine-grained emotions and the 9 empathetic response intents and discovered frequent emotion-intent exchange patterns in human social conversations. They observe that this type of dataset tagged with fine-grained emotions and response intents could train neural chatbots to generate empathetically appropriate responses conditioned on a selected emotion or intent. However, for this purpose, a large-scale emotion and intent labeled dataset is even more desirable. Curating such a dataset is technically challenging because 1) annotating such a large-scale dataset require human labor that is costly, and 2) given the fine-granularity of the emotion and intent labels, the human labeling task is more difficult compared to more generic Angry-Happy-Sad. As a result, existing manually labeled emotional dialogue datasets such as IEMOCAP , MELD , and DailyDialogue  are smaller in scale and contain only a limited set of emotions , with simpler dialogue responding strategies, or both. Also, existing datasets often contain a label Neutral or Other for responses that do not convey emotion, which introduces vagueness and limits the ability of automatic agents that use such datasets in learning useful response strategies.   % , EmotionLines , and EmoContext   To fill the above gap, we curate a novel large-scale dialogue dataset, OSED , containing 1M emotional dialogues from movie subtitles, in which each dialogue turn is automatically annotated with 32 fine-grained emotions and 9 empathetic response intents. Movie subtitles well approximate human social conversations and how emotion is handled in them. It is one of the major sources to learn emotional variations and corresponding response strategies. To reduce the cost of human labeling and the complexity of labeling dialogues with fine-grained emotions and intents, we devise a semi-automated human computation task to collect fine-grained emotion and intent labels for a small set of movie dialogues . We then follow a semi-supervised approach to expand the labeled data and train a dialogue emotion classifier to automatically annotate 1M emotional dialogues.   The process of curating the dataset consists of several stages. First, we apply automatic turn and dialogue segmentation methods on movie subtitles in the OpenSubtitles  corpus  and obtain close to 9M dialogues. After data cleaning and removing duplicates, we reduce its size to 4M. Then, we apply a weak labeler, EmoBERT  trained on the EmpatheticDialogues dataset , to label utterances in OS dialogues and filter 1M emotional dialogues . Thirdly, with semi-supervised learning methods, we refine EmoBert and obtain EmoBert+, a more advanced dialogue emotion classifier trained on OS dialogues. To evaluate EmoBert+, we compare it with FastText. The former is more accurate than FastText. Finally, we use EmoBert+ to label dialogues in OSED initial to obtain the final 1M OSED dataset. We evaluate the quality of the resultant dataset by visually inspecting the emotion-intent flow patterns that occur in the dataset and checking if they conform to the patterns of human social conversations discovered in existing work . Figure  summarizes the process of creating OSED. The data curation pipeline we follow substantially reduces the cost of human labor, while ensuring quality annotations.   Our contributions in this paper are three-fold. 1) We curate a dialogue dataset, OSED, containing 1M emotional dialogues labeled with 32 fine-grained emotions and 9 empathetic response intents. Compared to existing dialogue datasets tagged with emotions, OSED is more general-purpose, significantly larger, and contains more fine-grained emotions and empathetic response strategies. 2) We outline the complex pipeline used to derive this dataset and evaluate the annotation quality using visualization methods. 3) We release our fine-grained emotion classifier used to annotate the OSED dataset, which can be used as a general-purpose classifier capable of recognizing fine-grained emotions and empathetic response intents in social chitchat.     
"," We propose a novel large-scale emotional dialogue dataset, consisting of 1M dialogues retrieved from the OpenSubtitles corpus and annotated with 32 emotions and 9 empathetic response intents using a BERT-based fine-grained dialogue emotion classifier. This work explains the complex pipeline used to preprocess movie subtitles and select good movie dialogues to annotate. We also describe the semi-supervised learning process followed to train a fine-grained emotion classifier to annotate these dialogues. Despite the large set of labels, our dialogue emotion classifier achieved an accuracy of $65\%$ and was used to annotate 1M emotional movie dialogues from OpenSubtitles. This scale of emotional dialogue classification has never been attempted before, both in terms of dataset size and fine-grained emotion and intent categories. Visualization techniques used to analyze the quality of the resultant dataset suggest that it conforms to the patterns of human social interaction.",394
"  Neural machine translation  has advanced significantly in recent years . In particular, the Transformer model has become popular for its well-designed architecture and the ability to capture the dependency among positions over the entire sequence . Early systems of this kind stack 4-8 layers on both the encoder and decoder sides , and the improvement often comes from the use of wider networks . More recently, researchers try to explore deeper models for Transformer. Encouraging results appeared in architecture improvements by creating direct pass from the low-level encoder layers to the decoder , and proper initialization strategies .  Despite promising improvements, problems still remain in deep NMT. Deep Transformer stacked by dozens of encoder layers always have a large number of parameters, which are computationally expensive and memory intensive. For example, a 48-layer Transformer is  larger than a 6-layer system and  slower for inference. It is difficult to deploy such models on resource-restricted devices, such as mobile phones. Therefore, it is crucial to compress such heavy systems into light-weight ones while keeping their performance.  Knowledge distillation is a promising method to address the issue. Although several studies  have attempted to compress the 12-layer BERT model through knowledge distillation, effectively compressing extremely deep Transformer NMT systems is still an open question in the MT community. In addition, these methods leverage sophisticated layer-wise distillation loss functions to minimize the distance between the teacher and the student models, which requires huge memory consumption and enormous training cost.  In this paper, we investigate simple and efficient compression strategies for deep Transformer. We propose a novel Transformer compression approach ) to transfer the knowledge from an extremely deep teacher model into a shallower student model. We disturb the computation order among each layer group during the teacher training phase, which is easy to implement and memory friendly. Moreover, to further enhance the performance of the teacher network, we introduce a vertical ``dropout''  into training by randomly omitting sub-layers to prevent co-adaptations of the over-parameterized teacher network. Although similar technique has been discussed in \citet{fan2019reducing}'s work, we believe that the finding here is complementary to theirs. Both Gpkd and regularization training methods can be well incorporated into the teacher training process, which is essential for obtaining a strong but light-weight student model.  \pgfdeclarepatternformonly{soft horizontal lines}{\pgfpointorigin}{\pgfqpoint{100pt}{1pt}}{\pgfqpoint{100pt}{3pt}}% {   \pgfsetstrokeopacity{0.3}   \pgfsetlinewidth{0.1pt}   \pgfpathmoveto{\pgfqpoint{0pt}{0.5pt}}   \pgfpathlineto{\pgfqpoint{100pt}{0.5pt}}   \pgfusepath{stroke} }  \pgfdeclarepatternformonly{soft crosshatch}{\pgfqpoint{-1pt}{-1pt}}{\pgfqpoint{6pt}{6pt}}{\pgfqpoint{5pt}{5pt}}% {   \pgfsetstrokeopacity{0.3}   \pgfsetlinewidth{0.4pt}   \pgfpathmoveto{\pgfqpoint{4.5pt}{0pt}}   \pgfpathlineto{\pgfqpoint{0pt}{4.5pt}}   \pgfpathmoveto{\pgfqpoint{0pt}{0pt}}   \pgfpathlineto{\pgfqpoint{4.5pt}{4.5pt}}   \pgfusepath{stroke} }  \definecolor{ugreen}{rgb}{0,0.5,0}    	%reoder 1 	\draw[line width=1pt,draw=red!30,fill=red!20] -- --  --  -- --  --  -- ; 	\draw[line width=1pt,draw=blue!35,fill=blue!20] -- --  --  -- --  --  -- ;  	%reoder 2 	\draw[line width=1pt,draw=red!30,fill=red!20] -- --  --  -- --  --  -- ; 	\draw[line width=1pt,draw=blue!35,fill=blue!20] -- --  --  -- --  --  -- ;  	%reoder 3 	\draw[line width=1pt,draw=red!30,fill=red!20] -- --  --  -- --  --  -- ; 	\draw[line width=1pt,draw=blue!35,fill=blue!20] -- --  --  -- --  --  -- ; 	\node[anchor=north,inner sep=0pt] at {}; 	\node[anchor=north,inner sep=0pt] at {};   	\node[anchor = south,font=;   	\node[anchor = south,font=;   	\node[anchor = south,font=;   	\node[anchor = south,font=\footnotesize]  at  {};   	\node[anchor = east,font=\footnotesize,rotate=-90]  at  {};   	\node[anchor = east,font=\footnotesize,rotate=-90]  at  {};   	\node[anchor = east,font=\footnotesize,rotate=-90]  at  {};    	\node[anchor = north,font=\scriptsize]  at {reorder};    	\node[anchor = west]   at  {};   	\node[anchor = west]  at  {};   	\node[anchor = west]  at  {};     %draw   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;    \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;    \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;    \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;      \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;          \draw[-latex',very thick,red!40] ..controls + and +..;   \draw[-latex',very thick,blue!40] ..controls + and +..; 	\node[font=\tiny]  at  {sampling};    \node [auto,anchor=west,font=\footnotesize,rotate=-90] at {Teacher Training} ;    \node [auto,anchor=west,font=\footnotesize,rotate=-90] at {Student Training} ;       \node[auto,anchor=south,font=\footnotesize,inner sep=0pt] at  {Generate Skd-data} ;      \node[draw=gray!70,line width=1pt,fill=gray!10,single arrow,minimum height=2.2em,minimum width=4pt,single arrow head extend=3pt]  at {};    \node[draw=gray!70,line width=1pt,fill=gray!10,single arrow,minimum height=1.6em,minimum width=4pt,single arrow head extend=3pt,rotate=-90]  at {};    \end{tikzpicture}       \end{figure*}   We ran experiments on the WMT16 English-German, NIST OpenMT12 Chinese-English and WMT19 Chinese-English translation tasks. The Gpkd method compressed a 48-layer Transformer into a 6-layer system with almost no loss in BLEU. It outperformed the baseline with the same depth by + BLEU points. Through skipping sub-layer method, the teacher network achieved a BLEU score of  BLEU on the newstest2014 English-German task, and the student obtains additional improvements of  BLEU points.  % Moreover, we present a deep-encoder and shallow-decoder architecture  which achieves a speedup of  times with almost no loss in BLEU.    
","     Recently, deep models have shown tremendous improvements in neural machine translation . However, systems of this kind are computationally expensive and memory intensive. In this paper, we take a natural step towards learning strong but light-weight NMT systems. We proposed a novel group-permutation based knowledge distillation approach to compressing the deep Transformer model into a shallow model. The experimental results on several benchmarks validate the effectiveness of our method. Our compressed model is $8\times$ shallower than the deep model, with almost no loss in BLEU. To further enhance the teacher model, we present a Skipping Sub-Layer method to randomly omit sub-layers to introduce perturbation into training, which achieves a BLEU score of 30.63 on English-German newstest2014. The code is publicly available at https://github.com/libeineu/GPKD.",395
"  {S}{emantic} role labeling , also known as shallow semantic parsing, conveys the meaning of a sentence by forming a predicate-argument structure for each predicate in a sentence, which is generally described as the answer to the question ""Who did what to whom, where and when?"". The relation between a specific predicate and its argument provides an extra layer of abstraction beyond syntactic dependencies  , such that the labels are insensitive to syntactic alternations and can also be applied to nominal predicates. Given a sentence in Figure , SRL pipeline framework consists of 4 subtasks, including predicate identification , predicate disambiguation , arguments identification  and arguments classification . SRL is a core task of natural language processing  having wide range of applications such as neural machine translation , information extraction , question answering , emotion recognition from text , document summarization  etc.   Semantic role labeling can be categorized into two categories, span and dependency. Both types of SRL are useful for formal semantic representations but dependency based SRL is better for the convenience and effectiveness of semantic machine learning. Johansson and Nugues  concluded that the best dependency based SRL system outperforms the best span based SRL system through gold syntactic structure transformation. The same conclusion was also verified by Li et al.  through a solid empirical verification. Furthermore, since 2008, dependency based SRL has been more studied as compared to span based SRL. With this motivation, we focus on dependency based SRL, which is mainly popularized by CoNLL-2008 and CoNLL-2009 shared tasks .   The traditional approaches to SRL focus on feature engineering which struggles in apprehending discriminative information  while neural networks are proficient enough to extract features automatically . Specifically, since large scale empirical verification of Punyakanok et al. , syntactic information has been proven to be extremely beneficial for SRL task. Later works  achieve satisfactory performance for SRL with syntax-agnostic models which creates conflict with the long-held belief that syntax is essential for high-performance SRL .  The study of Li et al.  shows that the empirical results from neural models on the less importance of syntax indicate a potential challenge and despite the satisfactory performance of syntax-agnostic SRL systems, the reasons behind the absence of syntax in these models are three-fold. First, the effective incorporation of syntax in neural SRL models is quite challenging as compared to traditional approaches. Second, neural SRL models may cover partial syntactic clues more or less. Third, syntax has always been a complicated formalism in linguistics and its not easy to encode syntax for later usage. %Despite the satisfactory performance of syntax-agnostic SRL models, the reasons behind the absence of syntax in these models are two-fold. First, the effective incorporation of syntax information in neural SRL models is quite challenging. Second, the unreliability of syntactic parsers on account of the risk of erroneous syntactic input may lead to error proliferation. This has been proven by Li et al.  through a strong empirical verification. They show that the effective method of syntax incorporation and the high quality of syntax can promote SRL performance.%   
"," Semantic role labeling  aims at elaborating the meaning of a sentence by forming a predicate-argument structure. Recent researches depicted that the effective use of syntax can improve SRL performance. However, syntax is a complicated linguistic clue and is hard to be effectively applied in a downstream task like SRL. This work effectively encodes syntax using adaptive convolution which endows strong flexibility to existing convolutional networks. The existing CNNs may help in encoding a complicated structure like syntax for SRL, but it still has shortcomings. Contrary to traditional convolutional networks that use same filters for different inputs, adaptive convolution uses adaptively generated filters conditioned on syntactically-informed inputs. We achieve this with the integration of a filter generation network which generates the input specific filters. This helps the model to focus on important syntactic features present inside the input, thus enlarging the gap between syntax-aware and syntax-agnostic SRL systems. We further study a hashing technique to compress the size of the filter generation network for SRL in terms of trainable parameters. Experiments on CoNLL-2009 dataset confirm that the proposed model substantially outperforms most previous SRL systems for both English and Chinese languages.",396
"     Learning dialogue policies are typically formulated as a reinforcement learning  problem . However, dialogue policy learning via RL from scratch in real-world dialogue scenarios is expensive and time-consuming, because it requires real users to interact with and adjusts its policies online . A plausible strategy is to use user simulators as an inexpensive alternative for real users, which randomly sample a user goal from the user goal set for the dialogue agent training . In task-oriented dialogue settings, the entire conversation revolves around the sampled user goal implicitly. Nevertheless, the dialogue agent's objective is to help the user to accomplish this goal even though the agent knows nothing about this sampled user goal , as shown in Figure.        The randomly sampling-based user simulator neglects the fact that human learning supervision is often accompanied by a curriculum . For instance, when a human-teacher teaches students, the order of presented examples is not random but meaningful, from which students can benefit . Therefore, this randomly sampling-based user simulators bring two issues:     Most previous studies of dialogue policy have focused on the efficiency issue, such as reward shaping , companion learning , incorporate planning , etc. However, stability is a pre-requisite for the method to work well in real-world scenarios. It is because, no matter how effective an algorithm is, an unstable online leaned policy may be ineffective when applied in the real dialogue environment. This can lead to bad user experience and thus fail to attract sufficient real users to continuously improve the policy. As far as we know, little work has been reported about the stability of dialogue policy. Therefore, it is essential to address the stability issue.    In this paper,  we propose a novel policy learning framework that combines curriculum learning and deep reinforcement learning,  namely Automatic Curriculum Learning-based Deep Q-Network . As shown in Figure, this framework replaces the traditional random sampling method in the user simulator with a teacher policy model that arranges a meaningful ordered curriculum and dynamically adjusts it to help dialogue agent  for automatic curriculum learning. As a scheduling controller for student agents, the teacher policy model arranges students to learn different user goals in different learning stages without any requirement of prior knowledge. Sampling the user goals that match the ability of student agents regarding different difficulty of each user goal, can not only increases the feedback of the environment to the student agent but also makes the learning of the student agent more stable.  There are two criteria for evaluating the sampling order of each user goal: the learning progress of the student agent and the over-repetition penalty. The learning progress of the student agent emphasizes the efficiency of each user goal, encouraging the teacher policy model to choose the user goals that match the ability of the student agent to maximize the learning efficiency of the student agent. The over-repetition penalty emphasizes the sampled diversity, preventing the teacher policy model from cheating\footnote[1]{The teacher policy model repeatedly selects user goals that the student agent has mastered to obtain positive rewards.}. The incorporation of the learning progress of the student agent and the over-repetition penalty reflects both sampled efficiency and sampled diversity to improve efficiency as well as stability of ACL-DQN.   Additionally, the proposed ACL-DQN framework can equip with different curriculum schedules. Hence, in order to verify the generalization of the proposed framework, we propose three curriculum schedule standards for the framework for experimentation: i) Curriculum schedule A: there is no standard, only a single teacher model; ii) Curriculum schedule B: user goals are sampled from easiness to hardness in proportion; iii) Curriculum schedule C: ensure that the student agents have mastered simpler goals before learning more complex goals.   Experiments have demonstrated that the ACL-DQN significantly improves the dialogue policy through automatic curriculum learning and achieves better and more stable performance than DQN. Moreover, the ACL-DQN equipped with the curriculum schedules can be further improved. Among the three curriculum schedules we provided, the ACL-DQN under curriculum schedule C with the strength of supervision and controllability, can better follow up on the learning progress of students and performs best. In summary, our contributions are as follows:      
"," Dialogue policy learning based on reinforcement learning is difficult to be applied to real users to train dialogue agents from scratch because of the high cost. User simulators, which choose random user goals for the dialogue agent to train on, have been considered as an affordable substitute for real users. However, this random sampling method ignores the law of human learning, making the learned dialogue policy inefficient and unstable. We propose a novel framework, Automatic Curriculum Learning-based Deep Q-Network , which replaces the traditional random sampling method with a teacher policy model to realize the dialogue policy for automatic curriculum learning. The teacher model arranges a meaningful ordered curriculum and automatically adjusts it by monitoring the learning progress of the dialogue agent and the over-repetition penalty without any requirement of prior knowledge. The learning progress of the dialogue agent reflects the relationship between the dialogue agent's ability and the sampled goals' difficulty for sample efficiency. The over-repetition penalty guarantees the sampled diversity. Experiments show that the ACL-DQN significantly improves the effectiveness and stability of dialogue tasks with a statistically significant margin. Furthermore, the framework can be further improved by equipping with different curriculum schedules, which demonstrates that the framework has strong generalizability.",397
"} {R}{epresentations} learned by deep neural models have attracted a lot of attention in Natural Language Processing .  % and has been widely used in various applications such as information retrieval  and question answering . In real-world situations, different levels of linguistic units % usually appear at the same time. For example, considering a traditional information retrieval task, the system is required to capture the semantic meanings for queries of different lengths. Therefore, it is critical to come up with a method that can handle multiple levels of linguistic objects in a unified way. However, previous language representation learning methods such as Word2Vec , LASER  and USE  focus on either words or sentences.  % achieving encouraging performance at a certain level of linguistic unit but less satisfactory results at other levels.  Later proposed pre-trained contextualized language representations like ELMo , GPT, BERT  and XLNet  may seemingly handle different sized input sentences, but all of them focus on sentence-level specific representation still for each word, leading to unsatisfactory performance in real-world situations. Although the latest BERT-wwm-ext , StructBERT  and SpanBERT  perform MLM on a higher linguistic level, the masked segments  either follow a pre-defined distribution or focus on a specific granularity. Besides, the random sampling strategy ignores important semantic and syntactic information of a sequence, resulting in a large number of meaningless segments.  However, universal representation among different levels of linguistic units may offer a great convenience when it is needed to handle free text in language hierarchy in a unified way. As well known that, embedding representation for a certain linguistic unit  enables linguistics-meaningful arithmetic calculation among different vectors, also known as word analogy. For example, vector  - vector  + vector  results in vector . Thus universal representation may generalize such good analogy features or meaningful arithmetic operation onto free text with all language levels involved together. For example, Eat an onion : Vegetable :: Eat a pear : Fruit. In fact, manipulating embeddings in the vector space reveals syntactic and semantic relations between the original sequences and this feature is indeed useful in true applications. For example, ``London is the capital of England.閳 can be formulized as . Then given two documents one of which contains ``England閳 and ``capital閳, the other contains ``London閳, we consider these two documents relevant. % Such features can be generalized onto higher language levels for phrase/sentence embedding.   In this paper, we explore the regularities of representations including words, phrases and sentences in the same vector space. To this end, we introduce a universal analogy task derived from Google's word analogy dataset. To solve such task, we present BURT, a pre-trained model that aims at learning universal representations for sequences of various lengths. Our model follows the architecture of BERT but differs from its original masking and training scheme. Specifically, we propose to efficiently extract and prune meaningful segments  from unlabeled corpus with little human supervision, and then use them to modify the masking and training objective of BERT. The n-gram pruning algorithm is based on point-wise mutual information  and automatically captures different levels of language information, which is critical to improving the model capability of handling multiple levels of linguistic objects in a unified way, i.e., embedding sequences of different lengths in the same vector space.   Overall, our pre-trained models improves the performance of baselines in both English and Chinese. In English, BURT-base reaches 0.7 percent gain on average over Google BERT-base. In Chinese, BURT-wwm-ext obtains 74.5\% on the WSC test set, 13.4\% point absolute improvement compared with BERT-wwm-ext and exceeds the baselines by 0.2\%  0.6\% point accuracy on five other CLUE tasks including TNEWS, IFLYTEK, CSL, ChID and CMRC 2018. Extensive experimental results on our universal analogy task demonstrate that BURT is able to map sequences of variable lengths into a shared vector space where similar sequences are close to each other. Meanwhile, addition and subtraction of embeddings reflect semantic and syntactic connections between sequences. Moreover, BURT can be easily applied to real-world applications such as Frequently Asked Questions  and Natural Language Generation  tasks, where it encodes words, sentences and paragraphs into the same embedding space and directly retrieves sequences that are semantically similar to the given query based on cosine similarity. All of the above experimental results demonstrate that our well-trained model leads to universal representation that can adapt to various tasks and applications.  % needed in second column of first page if using \IEEEpubid % \IEEEpubidadjcol 
"," \justifying Although pre-trained contextualized language models such as BERT achieve significant performance on various downstream tasks, current language representation focuses on linguistic objective at a specific granularity.  % which may not applicable when multiple levels of linguistic units are involved at the same time.  Thus this work introduces  % and explores  the universal representation learning, i.e., embeddings of different levels of linguistic unit in a uniform vector space. We present a universal representation model, BURT , to encode different levels of linguistic unit into the same vector space. Specifically, we extract  % and mask  meaningful segments based on point-wise mutual information  to incorporate different granular objectives into the pre-training stage. We conduct experiments on datasets for English and Chinese including the GLUE and CLUE benchmarks, where our model surpasses its baselines and alternatives on a wide range of downstream tasks. We present our approach of constructing analogy datasets in terms of words, phrases and sentences and experiment with multiple representation models to examine geometric properties of the learned vector space through a task-independent evaluation. Finally, we verify the effectiveness of our method  % unified pre-training strategy  in two real-world text matching scenarios. As a result, our model significantly outperforms existing information retrieval  methods and yields universal representations that can be directly applied to retrieval-based question-answering and natural language generation tasks.",398
" Exponential growths of micro-blogging sites and social media not only provide platforms for empowering freedom of expressions and individual voices, but also enables people to express anti-social behavior such as online harassment, cyberbullying, rumors, and spreading hatred statements. %In recent years, micro-blogging sites and social media sites have grown exponentially, enabling the users to express anti-social behavior, false political or religious rumor, and spreading hatred activities. Besides, abusive or threatening speech that expresses prejudice against a certain group, which religious, political, geopolitical, personal, and gender abuse are very common and on the basis of race, religion, and sexual orientation are getting pervasive. United Nations Strategy and Plan of Action on Hate Speech defines hate speech as ``any kind of communication in speech, writing or behaviour, that attacks or uses pejorative or discriminatory language with reference to a person or a group on the basis of who they are, in other words, based on their religion, ethnicity, nationality, race, colour, descent, gender or other identity factor''.  Bengali is spoken by 230 million people in Bangladesh and India, making it one of the major languages in the world. Although, a rich language with a lot of diversity, Bengali is severely low-resourced for natural language processing~, which is due to the scarcity of computational resources such as language models, labeled datasets, and efficient machine learning~ methods required for different NLP tasks. Similar to other major languages like English, the use of hate speech in Bengali is also getting rampant. This is mainly due to unrestricted access and use of social media and digitalization. Some examples of Bengali hate speech and their respective English translations are shown in \cref{cdec_wf3} that are either directed towards a specific person or entity or generalized towards a group. These examples signify how severe Bengali hateful statements could be. Nevertheless, there is a potential chance that these could lead to serious consequences such as hate crimes, regardless of languages, geographic locations, or ethnicity.    Automatic identification of hate speech and creating awareness among people is very challenging. However, manual reviewing and verification from a vast amount of online content is not only labor-intensive but also time-consuming. Nevertheless, accurate identification requires automated, robust, and efficient machine learning~ methods. Compared to traditional ML and neural network~-based approaches, state-of-the-art~ language models are becoming increasingly effective. On a serious drawback: a prediction made by many models can neither be traced back to the input, nor it is clear why the output is transformed in a certain way. This makes even the most efficient DNN models `black-box' methods. On the other hand, the General Data Protection Regulation~ by the European Parliament enforces the `right to explanation', which prohibits the use of ML for automated decisions unless a clear explanation of the logic used to make each decision is well explained. Therefore, how a prediction is made by an algorithm should be as transparent as possible in order to gain human trust.    %Recent research efforts from both the NLP and ML communities have proven to be very useful for well-resourced languages like English. %Nevertheless, accurate identification requires automated, robust, and efficient machine learning~ methods. As state-of-the-art language models becoming increasingly effective, their decisions should be made as transparent as possible in order to improve human trust. %Some of these techniques are based on the model閳ユ獨 local gradient information while other methods seek to redistribute the function閳ユ獨 value on the input variables, typically by reverse propagation in the neural network graph. Bach et al. proposed specific propagation rules for neural networks . These rules were shown to produce better explanations than e.g. gradient-based techniques not only for computer vision but also text data. To overcome the shortcomings of `black-box'-based methods and inspired by the outstanding success of transformer language models~, we propose an explainable approach for hate speech detection from under-resourced Bengali language. Our approach is based on the ensemble of several BERT variants, including monolingual Bangla BERT-base, m-BERT~, and XLM-RoBERTa. Further, we not only provide both global and local explanations of the predictions, in a post-hoc fashion but also provide the measure of explanations in terms of faithfulness.  The rest of the paper is structured as follows: \Cref{rw} reviews related work on hate speech and Bengali word embedding. \Cref{section:3} describes the data collection and annotation process. \Cref{nettwork} describes the process of Bengali neural embedding, network construction, and training. \Cref{er} illustrates experiment results, including a comparative analysis with baseline models on all datasets. \Cref{con} summarizes this research with potential limitations and points some possible outlook before concluding the paper.  
","   The exponential growths of social media and micro-blogging sites not only provide platforms for empowering freedom of expressions and individual voices, but also enables people to express anti-social behavior like online harassment, cyberbullying, and hate speech. Numerous works have been proposed to utilize the textual data for social and anti-social behavior analysis, by predicting the contexts mostly for highly-resourced languages like English. However, some languages are under-resourced, e.g., South Asian languages like Bengali, that lack  computational resources for accurate natural language processing~. In this paper, we propose an explainable approach for hate speech detection from the under-resourced Bengali language, which we called \texttt{DeepHateExplainer}. In our approach, Bengali texts are first comprehensively preprocessed, before classifying them into political, personal, geopolitical, and religious hates, by employing the neural ensemble method of different transformer-based neural architectures~. Subsequently, important~ terms are identified with sensitivity analysis and layer-wise relevance propagation~, before providing human-interpretable explanations. Finally, to measure the quality of the explanation~, we compute the comprehensiveness and sufficiency. Evaluations against machine learning~ and deep neural networks~ baselines yield F1 scores of 84\%, 90\%, 88\%, and 88\%, for political, personal, geopolitical, and religious hates, respectively, outperforming both ML and DNN baselines.%, during 3-fold cross-validation tests.",399
"  Sentence embeddings map sentences into a vector space. The vectors capture rich semantic information that can be used to measure semantic textual similarity~ between sentences or train classifiers for a broad range of downstream tasks~. State-of-the-art models are usually trained on supervised tasks such as natural language inference~, or with semi-structured data like question-answer pairs~ and translation pairs~. However, labeled and semi-structured data are difficult and expensive to obtain, making it hard to cover many domains and languages. Conversely, recent efforts to improve language models include the development of masked language model  pre-training from large scale unlabeled corpora .  While internal MLM model representations are helpful when fine-tuning on downstream tasks, they do not directly produce good sentence representations, without further supervised  or semi-structured  fine-tuning.  In this paper, we explore an unsupervised approach, called Conditional Masked Language Modeling , to effectively learn sentence representations from large scale unlabeled corpora. CMLM integrates sentence representation learning into MLM training by conditioning on sentence level representations produced by adjacent sentences. The model therefore needs to learn effective sentence representations in order to perform good MLM. Since CMLM is fully unsupervised, it can be easily extended to new languages. We explore CMLM for both English and multilingual sentence embeddings for 100+ languages.  Our English CMLM model achieves state-of-the-art performance on SentEval~, even outperforming models learned using supervised signals. Moreover, models training on the English Amazon review data using our multilingual vectors exhibit strong multilingual transfer performance on translations of the Amazon review evaluation data to French, German and Japanese, outperforming existing multilingual sentence embedding models by  for non-English languages and by  on the original English data.   We further extend the multilingual CMLM to co-train with parallel text  retrieval task, and finetune with cross-lingual natural language inference  data, inspired by the success of prior work on multitask sentence representation learning~ and NLI learning~. We achieve performance  better than the previous state-of-the-art multilingual sentence representation model . Language agnostic representations require semantically similar cross-lingual pairs to be closer in representation space than unrelated same-language pairs~. While we find our original sentence embeddings do have a bias for same language sentences, we discover that removing the first few principal components of the embeddings eliminates the self language bias.  The rest of the paper is organized as follows. \Cref{sec:cmlm} describes the architecture for CMLM unsupervised learning. In \Cref{sec:en_cmlm} we present CMLM trained on English data and evaluation results on SentEval. In \Cref{sec:en_cmlm} we apply CMLM to learn sentence multilingual sentence representations. Multitask training strategies on how to effectively combining CMLM, bitext retrieval and cross lingual NLI finetuning are explored. In \Cref{sec:analysis}, we investigate self language bias in multilingual representations and how to eliminate it.  The contributions of this paper can be summarized as follows:  A novel pre-training technique CMLM for unsupervised sentence representation learning on unlabeled corpora .  An effective multitask training framework, which combines unsupervised learning task CMLM with supervised learning Bitext Retrieval and cross-lingual NLI finetuning.  An evaluation benchmark for multilingual sentence representations.  A simple and effective algebraic method to remove same language bias in multilingual representations. The pre-trained models are released at \url{https://tfhub.dev/s?q=universal-sentence-encoder-cmlm}.   
"," This paper presents a novel training method, Conditional Masked Language Modeling , to effectively learn sentence representations on large scale unlabeled corpora. CMLM integrates sentence representation learning into MLM training by conditioning on the encoded vectors of adjacent sentences. Our English CMLM model achieves state-of-the-art performance on SentEval, even outperforming models learned using supervised signals. As a fully unsupervised learning method, CMLM can be conveniently extended to a broad range of languages and domains. We find that a multilingual CMLM model co-trained with bitext retrieval~ and natural language inference~ tasks outperforms the previous state-of-the-art multilingual models by a large margin. We explore the same language bias of the learned representations, and propose a principle component based approach to remove the language identifying information from the representation while still retaining sentence semantics.",400
" Many seemingly convincing rumors such as ``Most humans only use 10 percent of their brain'' are widely spread, but ordinary people are not able to rigorously verify them by searching for scientific literature. In fact, it is not a trivial task to verify a scientific claim by providing supporting or refuting evidence rationales, even for domain experts.  %Such The situation worsens as misinformation is proliferated  %by the  on social media or news websites, manually or programmatically, at every moment. As a result, an automatic fact-verification tool becomes more and more crucial for combating  %against  the spread of misinformation.  %There are many existing datasets and %the corresponding  %systems for fact-verification tasks %, emphasizing on  %in various domains, such as Wikipedia , social media , and politics . These tasks  %are  The existing fact-verification tasks usually consist of three sub-tasks: document retrieval, rationale sentence extraction, and fact-verification. However, due to the nature of scientific literature that requires domain knowledge, it is challenging to collect a large scale scientific fact-verification dataset, and further, to perform fact-verification under a low-resource setting with limited training data. \citet{Wadden2020FactOF} collected a scientific claim-verification dataset, SciFact, and proposed a scientific claim-verification task: given a scientific claim, find evidence sentences that support or refute  %such the claim  %from  in a corpus of scientific paper abstracts. \citet{Wadden2020FactOF} also proposed a simple, pipeline-based, sentence-level model, VeriSci, as a baseline solution based on \citet{deyoung2019eraser}.  %Despite the simplicity of VeriSci ,  VeriSci is a pipeline model that runs modules for abstract retrieval, rationale sentence selection, and stance prediction sequentially, and thus the error generated from  %the an upstream module may propagate to the downstream modules. To overcome this drawback, we hypothesize that a module jointly optimized on multiple sub-tasks may mitigate the error-propagation problem to improve the overall performance.  %On the other hand,  In addition, we observe that a complete set of rationale sentences usually contains multiple inter-related sentences from the same paragraph. Therefore, we propose a novel, paragraph-level, multi-task learning model for the SciFact task.  In this work, we employ compact paragraph encoding, a novel strategy of computing sentence representations using BERT-family models. We directly feed an entire paragraph as a single sequence to BERT, so that the encoded sentence representations are already contextualized on the neighbor sentences by taking advantage of the attention mechanisms in BERT. In addition, we jointly train the modules for rationale selection and stance prediction as multi-task learning  by leveraging the confidence score of rationale selection as the attention weight of the stance prediction module. Furthermore, we compare two methods of transfer learning that mitigate the low-resource issue: pre-training and domain adaptation . Our experiments show that: % the compact paragraph encoding method is beneficial over separately computing sentence embeddings, and  with negative sampling, the joint training of rationale selection and stance prediction is beneficial over the pipeline solution. %\todo{you may want to create a list of contribution. -Violet}    
"," Even for domain experts, it is a non-trivial task to verify a scientific claim by providing supporting or refuting evidence rationales. The situation worsens as misinformation is proliferated on social media or news websites, manually or programmatically, at every moment. As a result, an automatic fact-verification tool becomes crucial for combating the spread of misinformation. %\citet{Wadden2020FactOF} collected a scientific claim-verification dataset, SciFact, to facilitate research on scientific claim-verification.  In this work, we propose a novel, paragraph-level, multi-task learning model for the SciFact task by directly computing a sequence of contextualized sentence embeddings from a BERT model and jointly training the model on rationale selection and stance prediction.",401
" Self attention networks  have been widely studied on many natural language processing  tasks, such as machine translation , language modeling  and natural language inference . It is well accepted that SANs can leverage both the local and long-term dependencies through the attention mechanism, and are highly parallelizable thanks to their position-independent modeling method.  However, such position-independent models are incapable of explicitly capturing the boundaries between sequences of words, thus overlook the structure information that has been proven to be robust inductive biases for modeling texts . Unlike RNNs that model sequential structure information of words by using memory cells, or CNNs that focus on learning local structure dependency of words via convolution kernels, SANs learn flexible structural information in an indirect way almost from scratch. One way to integrate structural information into SAN models is via pre-training, such as BERT , which learns to represent sentences by using unsupervised learning tasks on the large-scale corpus. Recent studies  have shown the ability of pre-training models on capturing structure information of sentences.  Another method to deal with structural information is introducing structure priors into SANs by mask strategies. \citeauthor{shen2018disan} \shortcite{shen2018disan} proposed the directional self-attention mechanism, which employs two SANs with the forward and backward masks respectively to encode temporal order information. \citeauthor{guo2019gaussian} \shortcite{guo2019gaussian} introduced the Gaussian prior to the transformers for capturing local compositionality of words. Admittedly, structure priors can strengthen the model's capability of modeling sentences and meanwhile assist in capturing proper dependencies. With the help of these learned structure priors, SANs can model sentences accurately even in resource-constrained conditions.    Though these models get success on many NLP tasks, these studies commonly focus on integrating one single type of structure priors into SANs, thus fail at making full use of multi-head attentions. One straightforward advantage of using the multi-head attentions lies in the fact that different heads convey different views of texts . In other words, multi-head attentions enable the model to capture the information of texts at multiple aspects, which in return brings thorough views when modeling the texts.  Besides, it is well accepted that one type of structural prior can only reveal part of the structural information from one single perspective. A variety of types of structural priors are needed in order to gain complete structural information of texts. This can be achieved by introducing different structural priors into different parts of attention heads, where different structural priors can complement each other, guiding the SAN models to learn proper dependencies between words. Therefore, to gain a better representation of the texts, a desirable solution should make full use of the multi-head attention mechanism and utilize multiple types of structural priors.  To better alleviate the aforementioned problems, in this paper, we propose a lightweight self attention network, i.e., the Multiple Structural Priors Guided Self Attention Network . The novel idea behind our model lies in the usage of the multi-mask based multi-head attention , which helps our model to better capture different types of dependencies between texts. Thanks to the MM-MH Attention mechanism, our model can capture multiple structural priors, which in return brings benefits in modeling sentences.  Especially, the structural priors we employed come from two categories: the sequential order and the relative position of words. Since the standard SANs are incapable of distinguishing the order between words, we apply the direction mask  directly to each attention head. Motivated by the Bidirectional RNNs , we split the attention heads into two parts. For a given word, we apply the forward mask to the first half of attention heads, which allows it to attend on only the previous words when modeling the reference word. Accordingly, the backward mask is applied to the rest of the attention heads.  Since the direction masks take no consideration of the difference between long-distance words and nearby words, we employ the second category of structural prior as a complement, which could be measured by the distance between pair of words. We integrate two types of distance masks into different attention heads. The first one we utilized is the word distance mask, which describes the physical distance between each pair of words. Besides, for the purpose of capturing the latent hierarchical structure of sentences, we integrate another kind of distance information, i.e., dependency distance that is defined as the distance between each pair of words on a dependency syntax tree. The word distance mask helps our model to focus on the local words and the dependency distance mask enables our model to capture the hierarchical relationships between words. Consequently, they provide our model the ability of capturing the local and non-local dependency of words properly.  To illustrate the effectiveness of our model, we conduct experiments on two NLP tasks: natural language inference and sentiment classification. Experimental results show that MS-SAN outperforms other baselines and achieves a competitive performance comparing with the state-of-the-art models.   Our contributions are listed as follows:   
"," Self attention networks  have been widely utilized in recent NLP studies. Unlike CNNs or RNNs, standard SANs are usually position-independent, and thus are incapable of capturing the structural priors between sequences of words. Existing studies commonly apply one single mask strategy on SANs for incorporating structural priors while failing at modeling more abundant structural information of texts. In this paper, we aim at introducing multiple types of structural priors into SAN models, proposing the Multiple Structural Priors Guided Self Attention Network  that transforms different structural priors into different attention heads by using a novel multi-mask based multi-head attention mechanism. In particular, we integrate two categories of structural priors, including the sequential order and the relative position of words. For the purpose of capturing the latent hierarchical structure of the texts, we extract these information not only from the word contexts but also from the dependency syntax trees. Experimental results on two tasks show that MS-SAN achieves significant improvements against other strong baselines.",402
"  Building intelligent conversation systems is a long-standing goal of artificial intelligence and has attracted much attention in recent years . A central challenge for building such conversation systems is the response selection problem, that is, selecting the best response to a given dialogue context from a pool of candidate responses .   } \end{center}     \end{table}  To tackle the response selection problem, different matching models are developed to measure the matching degree between a conversation context and a response candidate . Despite their differences, most prior works train the matching models with training data constructed by a simple heuristic. For each dialogue context, the human-written response is considered as positive  and the responses from other dialogue contexts are considered as negative . In practice, the negative responses are often randomly sampled and the training objective is to ensure that the positive responses score higher than the negative ones.  Recently, some researchers  has raised the concern that randomly sampled negative responses are often too trivial . Models trained with such negative data lacks the ability to handle strong distractors during testing. In general, the problem stems from the ignorance of the diversity in context-response matching; all random responses are treated as equally negative regardless of their distracting strength. For example, in Table , two negative responses  are presented. For N1, one can easily dispel its legality %as there is no lexical overlap and off-topic semantic meaning.  as it does not follow the topic discussed in the dialogue context. %as its semantic meaning \cd{what meaning} is obviously off the topic \cd{what topic},  On the other hand, judging a strong distractor like N2 can be difficult as its content overlaps significantly with the context . Only with close observation, %we find that N2 does not properly reply the context \cd{why?}. %the semantic incoherence between the context and N2 can be spotted\cd{what incoherence?}.  we find that N2 does not strongly maintain the coherence of the discussion, i.e., it starts a parallel discussion about an actor in Game of Thrones rather than elaborating on the enjoyable properties of the TV series. %Similarly, we can observe the same phenomena on the positive side.  Similarly, the positive side has the same phenomena. For the positive response P1, one can easily confirm its legality as it naturally replies the context. As for P2, while it expatiates on the enjoyable properties of the TV series, it doesn't exhibit any obvious matching clues, such as lexical overlap with the context. %share any lexical overlap with the context.  Thus, to correctly identify P2, the relationship between P2 and the context has to be carefully reasoned by the model.  %Game of Thrones, the character name Jon Snow  has not appeared in the context. Thus, to correctly identify it, the relationship between Jon Snow and Game of Thrones has to be carefully reasoned by the model.  To conclude, the above observations suggest that, to accurately recognize different positive and negative responses, the model is required to possess different levels of discriminative capability.  %require different levels of model capability to accurately recognize. %\textcolor{red}{brandenwang: since the difficulty of random sampled negative responses is diverse. Besides, we observe that such diversity also applies to the positive responses: in some cases, the relationship between context and response is explicit and easy to identify, while in others, it is difficult to find the implicit relationship between the context and response. These two kinds of diversity may result in an unstable training process and poor accuracy in real-world applications.}  %Motivated by the intuition that one should first learn to deal with easy cases before handling harder ones, we propose to employ the idea of curriculum learning   to tackle the task of response selection.   Inspired by the aforementioned observations, we propose to employ the idea of curriculum learning   for a better learning of response selection models.  %to better learn matching models for response selection.  CL is reminiscent of the cognitive process of human being, the core idea is first learning easier concepts and then gradually transitioning to learning more complex concepts based on some pre-defined learning schemes. %\cd{What about the pace function design?}.  In various NLP tasks ), CL has demonstrated its benefit in improving the model performance as well as the learning convergence.%, such as , leading to improved model performance as well as faster learning convergence.%better generalization\cd{fast and robust convergence? what is the difference between generalization and performance?}.  %which has been successfully applied to many machine learning tasks . The core idea of CL is first learning easier concepts and then gradually transitioning to learning more complex concepts.  %.\cd{what is curriculum learning? what are its applications? what is the benefits?This paragraph introduce CL, talk about the general idea of CL and its success in other tasks }  The key to applying CL is to specify an appropriate learning scheme under which all training examples are gradually learned %. . In this work, we tailor-design a hierarchical curriculum learning  framework according to the characteristics of the concerned response selection task. Our HCL framework consists of two complementary curriculum strategies, namely corpus-level curriculum  and instance-level curriculum , covering the two distinct aspects of response selection. Specifically, in CC, the model gradually increases its ability in finding matching clues between the context and the positive response. As for IC, it progressively strengthens the model's ability in identifying the mismatch information between the context and negative responses. To order all positive and negative examples, we need to assess millions of possible context-response combinations in the training data. To overcome this computational challenge, we propose to use a fast neural ranking model to assign learning priorities to all training examples based on their pairwise context-response similarity score.   Notably, our proposed learning framework is independent to the choice of matching models. % and it can be conveniently implemented without any additional modelling effort \cd{really?}. Therefore, for a comprehensive evaluation, we test our approach with three representative matching models, including the latest advance brought by pre-trained language models. Results on two benchmark datasets demonstrate that the proposed learning framework leads to remarkable performance improvement across all evaluation metrics.   In summary, our contributions are:  We propose a new hierarchical curriculum learning framework to tackle the task of response selection; % We design a decomposable neural model which works coherently with the proposed learning framework; and  and  Experimental results on two benchmark datasets demonstrate that our approach can significantly improve the performance of strong matching models, including the state-of-the-art one.     
"," We study the learning of a matching model for dialogue response selection. Motivated by the recent finding that random negatives are often too trivial to train a reliable model, we propose a hierarchical curriculum learning  framework that consists of two complementary curricula: % Motivated by the idea of curriculum learning, we propose a new hierarchical curriculum learning framework which consists of two curriculum strategies:  corpus-level curriculum ; and  instance-level curriculum . In CC, the model gradually increases its ability in finding the matching clues between the dialogue context and a response. On the other hand, IC progressively strengthens the model's ability in identifying the mismatched information between the dialogue context and a response. Empirical studies on two benchmark datasets with three state-of-the-art matching models demonstrate that the proposed HCL significantly improves the model performance across various evaluation metrics\footnote{All data, code and models are made publicly available at https://github.com/yxuansu/HCL/.}.",403
" Sequence-to-Sequence  learning~ has advanced the state of the art in various natural language processing  tasks, such as machine translation~, text summarization~, and grammatical error correction~. Seq2Seq models are generally implemented with an encoder-decoder framework, in which a multi-layer encoder summarizes a source sequence into a sequence of representation and another multi-layer decoder produces the target sequence conditioned on the encoded representation.   Recent studies reveal that fusing the intermediate encoder layers  is beneficial for Seq2Seq models, such as layer attention~, layer aggregation~, and layer-wise coordination~. Despite its effectiveness, not much is known about how fusing encoder layer representations work. The intuitive explanation is that fusing encoder layers exploits surface and syntactic information embedded in the lower encoder layers~.  However, other studies show that attending to lower encoder layers  does not improve model performance~, which is conflicted with existing conclusions. It is still unclear why and when fusing encoder layers should work in Seq2Seq models.  This paper tries to shed light upon behavior of Seq2Seq models augmented with EncoderFusion method. To this end, we propose a novel fine-grained layer attention to evaluate the contribution of individual encoder layers. We conduct experiments on several representative Seq2Seq NLP tasks, including machine translation, text summarization, and grammatical error correction. Through a series of analyses, we find that the uppermost decoder layer pays more attention to the encoder embedding layer. Masking the encoder embedding layer significantly drops model performance by generating hallucinatory  predictions. The encoded representation of the standard Seq2Seq models  may not have enough capacity to model both semantic and surface features . We call the problem described above the source representation bottleneck.  Based on this observation, we simplify the EncoderFusion approaches by only connecting the encoder embedding layer to softmax layer . The SurfaceFusion approach shortens the path distance between source and target embeddings, which can help to learn better bilingual embeddings with direct interactions. Experimental results on several Seq2Seq NLP tasks show that our method consistently outperforms both the vanilla Seq2Seq model and the layer attention model.  Extensive analyses reveal that our approach produces more aligned bilingual word embeddings by shortening the path distance between them, which confirm our claim.  Our main contributions are as follows:    
"," Encoder layer fusion  is a technique to fuse all the encoder layers  for sequence-to-sequence  models, which has proven effective on various NLP tasks. However, it is still not entirely clear why and when EncoderFusion should work. In this paper, our main contribution is to take a step further in understanding EncoderFusion. Many of previous studies believe that the success of EncoderFusion comes from exploiting surface and syntactic information embedded in lower encoder layers. Unlike them, we find that the encoder embedding layer is more important than other intermediate encoder layers.  In addition, the uppermost decoder layer consistently pays more attention to the encoder embedding layer across NLP tasks. Based on this observation, we propose a simple fusion method, SurfaceFusion, by fusing only the encoder embedding layer for the softmax layer. Experimental results show that SurfaceFusion outperforms EncoderFusion on several NLP benchmarks, including machine translation, text summarization, and grammatical error correction.   It obtains the state-of-the-art performance on WMT16 Romanian-English and WMT14 English-French translation tasks. Extensive analyses reveal that SurfaceFusion learns more expressive bilingual word embeddings by building a closer relationship between relevant source and target embeddings. Source code is freely available at \url{https://github.com/SunbowLiu/SurfaceFusion}.  \iffalse To model the inter-dependence of two sequences, sequence-to-sequence  learning extracts the source surface and abstract features through its encoder output representations. However, an overloaded use of the encoder output representations might lead to an insufficient representation capacity, which we call it source representation bottleneck. Recent studies have found that widening the bottleneck by fusing the surface features from lower level representations can boost the performance of Seq2Seq, but none of them explain the intrinsic mechanism of this benefit. In this paper, we take the first step to probe into the essence of the bottleneck on three typical Seq2Seq tasks, i.e.~machine translation, text summarization, and grammatical error correction. We observe that the representation learning of higher decoder layer suffers from the bottleneck, and thus propose a simple yet effective surface fusion method to mitigate the issue. The results over a variety of benchmarks confirm the effectiveness of the proposed method. Source code will be released. \fi",404
"  Word segmentation is a fundamental and challenging task in text classification and other NLP applications. Word segmenter determines the boundaries of words in the shape of beginning and ending. It has been largely investigated in many space-delimited languages including English, Arabic, Urdu and non-space delimited languages including Chinese, Japanese, and Burmese . However, the word segmentation in low-resource Sindhi language has not been studied well, mainly due to the lack of language resources.   Sindhi word segmentation exhibits the space omission and space insertion  problems. Although, the white spaces between words are a good sign for predicting word boundaries,  the space omission and space insertion between words bring ambiguity in the segmentation process. Therefore, the SWS task is a challenging problem because of resource scarcity, lack of standard segmentation benchmark corpus, and rich morphological features in Sindhi language. Previously, little work has been proposed to address the SWS problem by employing dictionary-based and rule-based approaches. Thus, the existing approaches lack the applicability towards open-source implementation due to following reasons,  inability to deal with out-of-vocabulary words,  less robust on the large datasets, and  lower segmentation accuracy. Our proposed novel deep SGNWS model has the capability of dealing with such issues for SWS with the Subword Representation Learning  approach.   Recently, deep neural architectures have largely gained popularity in NLP community by greatly simplifying the learning and decoding in a number of NLP applications including word segmentation with neural word embedding and powerful recurrent neural architectures. More recently, self-attention has also become a  popular approach to boost the performance of neural models. Therefore, we tackle the SWS problem by taking advantage of BiLSTM, self-attention, SRL, and CRF without relying on external feature engineering.  In this paper, we propose a language-independent neural word segmentation model for Sindhi. The proposed model efficiently captures the character-level information with subword representation learning. We convert segmentation into a sequence tagging problem using B, I, E, S, X tagging scheme. Where B denotes [Beginning], I [Inside], E [Ending] of a word in the given corpus, S [Single] is used for the tagging of a single or special character in the unlabeled text, and X tag is used for [hard-space] between words. We train task-oriented Sindhi word representations with character-level subword approach. To the best of our knowledge, this is the first attempt to tackle SWS as a sequence labeling task. We provide the open-source implementation for further investigation\footnote{https://github.com/AliWazir/Neural-Sindhi-word-segmenter}. Our novel contributions are listed as follows:   % The remaining parts of the paper are organized in the following sequence;  Section  presents the related work on SWS and its morphology, the evolution and usage of Recurrent Neural Networks  and its variants of LSTM, BiLSTM and GRU in the text segmentation in various languages. Section  presents an overview of Sindhi writing system and segmentation challenges, followed by the proposed methodology in Section  where RNN variants are employed for our task. Section  presents the experiments and results analysis. Lastly, Section  concludes this paper. 
"," Deep neural networks employ multiple processing layers for learning text representations to alleviate the burden of manual feature engineering in Natural Language Processing . Such text representations are widely used to extract features from unlabeled data. The word segmentation is a fundamental and inevitable prerequisite for many languages. Sindhi is an under-resourced language, whose segmentation is challenging as it exhibits space omission, space insertion issues, and lacks the labeled corpus for segmentation.  In this paper, we investigate supervised Sindhi Word Segmentation  using unlabeled data with a Subword Guided Neural Word Segmenter  for Sindhi. In order to learn text representations, we incorporate subword representations to recurrent neural architecture to capture word information at morphemic-level, which takes advantage of Bidirectional Long-Short Term Memory , self-attention mechanism, and Conditional Random Field .  Our proposed SGNWS model achieves an F1 value of  98.51\% without relying on feature engineering. The empirical results demonstrate the benefits of the proposed model over the existing Sindhi word segmenters.   % , such as dictionaries, morphological analyzers, or rules, for the Sindhi word segmentation. The conducted extensive empirical study demonstrates the benefits of the proposed model over the existing Sindhi word segmenters and state-of-the-art deep learning approaches.",405
"   Indonesian colloquialism is everyday and everywhere, e.g. in social media posts and conversational transcripts. Yet, existing research on Indonesian NLP models including NMTs often disregards qualitative analysis when the models are given strictly colloquial inputs. This is mainly due to the fact that the data readily available for training and testing the models are in formal Indonesian. %This follow naturally due to the fact that the models are style-agnostic, that is,   Colloquial Indonesian has several different word choices from formal language due to the diversity of regional languages and dialects. We define the spoken colloquial as a clean colloquial. In addition, in written media,  colloquial Indonesian is often abbreviated, disemvoweled, or written with voice alteration, which we define as the noisy colloquial .               \end{table}   To better evaluate English-Indonesian MT systems against colloquial text, we first create 2 new test-sets of Indonesian-English colloquial pairs. The first test is a clean colloquial taken from a YouTube transcript. The second test-set is a noisy colloquial from Twitter annotated by our team of annotators. We found that NMT systems trained on formal dataset did not perform very well on these test-sets.  Next, we develop synthetic colloquial text data by performing word-level translation of several words in the formal text into a colloquial form based on a word-to-word dictionary. By combining the formal dataset and the synthesized colloquial dataset, we increase the NMT performance on the colloquial test-set by 2.5 BLEU points.   
","  Neural machine translation  is typically domain-dependent and style-dependent, and it requires lots of training data. State-of-the-art NMT models often fall short in handling colloquial variations of its source language and the lack of parallel data in this regard is a challenging hurdle in systematically improving the existing models. In this work, we develop a novel colloquial Indonesian-English test-set collected from YouTube transcript and Twitter. We perform synthetic style augmentation to the source formal Indonesian language and show that it improves the baseline Id-En models  over the new test data. %Our experimental data and code are available on github.com.",406
"  Large-scale language models have greatly advanced NLP research in various sub-areas, such as question answering, text summarization, story generation and so on . However, these generation models still suffer from at least three major problems when applied to the dialogue system building, 1) generic and repeated responses ,   2) inconsistent statements with the dialogue context , and 3) uncontrollable task-oblivious replies  .  Many previous studies have attempted to address these problems . For instance, \citet{li2019inconsisent} penalized repetitive and inconsistent behaviors with unlikelihood loss in open-domain chats. \citet{song2020generate} detected and rewrote the contradicting responses to achieve a more consistent personality.  However, these methods optimize the language model by minimizing the loss in supervised learning, which may lead to exposure bias and uninterpretable behaviors, and consequently,  makes it harder for humans to regulate the model.   To alleviate these problems, previous work has explored RL-based methods in dialogue system building . %For instance,  integrated the goal of coherent into the reward design  and made the first step towards .designed for better generation.   However, such methods not only rely on hand-crafted user simulators that are inherently hard to build , but also require meaningful rewards that are difficult to design. To address these issues, we propose to teach the model to extract a policy directly from the data and learn from its own mistakes without the use of simulators. Leveraging decoding methods such as Nucleus Sampling , the language model finetuned on a persuasion task is able to generate lexically diverse response candidates given the same context. %One example is shown in Figure.  Some candidates are appropriate, while others are repetitive or inconsistent with the context. These good and bad examples are used as positive and negative feedback to the model through meaningful rewards in RL, and help refine the language model. During testing, to fully utilize the refined language model, we use it to generate multiple candidates again,  and filter out the repetition and inconsistency afterwards. Beyond being nonrepetitive and consistent, a good response also needs to accomplish the dialogue task, in our case, to persuade people. Therefore, we ask humans to demonstrate the persuasion process, and build a response imitator to imitate these human demonstrations and select the most persuasive response.  The above issues in language models are especially salient in complex strategic dialogue tasks such as persuasion and negotiation. These dialogues involve both a specific task goal and social contents to build rapport for better task completion, and therefore, have richer and more complicated language structures . Furthermore, due to their inherent similarity to task-oriented and open-domain dialogues, improvements made on these systems would also help in both dialogue settings. Therefore, we choose a strategic donation persuasion task  to perform our study, and conduct both automatic and human evaluations to evaluate our models.     This work  makes multiple contributions. First, we propose DialGAIL, an RL-based generative algorithm to refine MLE-based language models for dialogue  generation without the use of user simulators.  Second, we design an effective and practicable framework for strategic dialogue systems that achieves state-of-the-art performance on a complex persuasion task, with only small amount of human demonstration efforts.  %Such system achieves more diverse, consistent and fluent conversations with better persuasion outcomes on a complex persuasion task compared to the MLE-based baselines.   %a framework to automatically detect repetitive and inconsistent responses, and imitate human demonstration to select persuasive responses.  %Furthermore, experiments show that our model produces more diverse, consistent and fluent conversations with better persuasion outcomes on a complex persuasion task compared to the MLE-based baselines.  Previous dialogue research has mostly focused on pure task-oriented dialogues and pure social conversations; but looking forward, it becomes more and more important to pay attention to strategic dialogues that involves both task and social components. We sincerely hope this work could inspire more research and discussions on strategic dialogues in the community.   % how to refine the dialogue generation with limited amount of data? MLE fine-tuning woldn't work with the limited data % social content + a specific end-goal --> persuasionforgood. advance research in this area % how to easily get a usable lm without computational resources? % explore the possibility to apply GAIL in dialogue generation in a simple way  % the first to explore GAIL % raise more attention in persuasion in the community % with small amount of human demo % task-independent in repetition detection strengthen    
"," Despite the recent success of large-scale language models on various downstream NLP tasks, the repetition and inconsistency problems still persist in dialogue response generation. Previous approaches have attempted to avoid repetition by penalizing the language model's undesirable behaviors in the loss function. However, these methods focus on token-level information and can lead to incoherent responses and uninterpretable behaviors. To alleviate these issues, we propose to apply reinforcement learning to refine an MLE-based language model without user simulators, and distill sentence-level information about repetition, inconsistency and task relevance through rewards. In addition, to better accomplish the dialogue task, the model learns from human demonstration to imitate intellectual activities such as persuasion, and selects the most persuasive responses. Experiments show that our model outperforms previous state-of-the-art dialogue models on both automatic metrics and human evaluation results on a donation persuasion task, and generates more diverse, consistent and persuasive conversations according to the user feedback.% We will release the code and data upon acceptance.",407
"   Large-scale pre-training has draw much attention in both the community of Compute Vision  and Natural Language Processing  due to its strong capability of generalization and efficient usage of large-scale data. Firstly in CV, a series of models were designed and pre-trained on the large-scale dataset ImageNet, such as AlexNet , VGG  and ResNet , which effectively improved the capability of image recognition for numerous tasks. Recent years have witnessed the burst of pre-training in NLP, such as BERT , RoBERTa , XLNet  and BART , which greatly improve the capability of language understanding and generation. However, the above researches towards the single-modal learning and can only be used in single-modal  scenarios. %which greatly restricts their ability to process multi-modal  information. In order to adapt to multi-modal scenarios, a series of multi-modal pre-training methods were proposed and pre-trained on the corpus of image-text pairs, such as ViLBERT , VisualBERT  and UNITER , which greatly improve the ability to process multi-modal information. However, these models can only utilize the limited corpus of image-text pairs and cannot be effectively adapted to single-modal scenarios . %Moreover, the size of the corpus of image-text pairs is very limited, and large scale of single-modal data can't be effectively utilized.     A smarter AI system should be able to process different modalities of information effectively. There are large scale of data in different modalities on the Web, mainly textual and visual information. The textual knowledge and the visual knowledge usually can enhance and complement with each other. As the example shown in Figure , it's difficult to answer the question correctly only with the visual information in the image.  However, if we connect the visual information to the textual information which describes the background of a baseball game, it's very easy to determine the correct answer. Also, the visual information can make it easier to understand the scene described by the text. The research in neuroscience by \citet{van2018neuronal} reveals that the parts of the human brain responsible for vision can learn to process other kinds of information, including touch and sound. Inspired by the research, we propose to design a unified-modal architecture UNIMO which can process multi-scene and multi-modal data input, including textual, visual and vision-and-language data, as shown in Figure .  The greatest challenge to unify different modalities is to align and unify them into the same semantic space which are generalizable to different modalities of data. Existed cross-modal pre-training methods try to learn cross-modal representations based on only limited image-text pairs by simple image-text matching and masked language modeling . They can only learn specific representations for image-text pairs, which are not generalizable for single-modal scenarios. So their performance will drop dramatically when applied to language tasks . In this work, UNIMO learns visual representations and textual representations in similar ways, and unify them into the same semantic space via cross-modal contrastive learning  based on a large-scale corpus of image collections, text corpus and image-text pairs.  %Our unified-modal architecture can utilize large scale of image collections and text corpus, and align the visual and textual information into the same semantic space via cross-modal contrastive learning on image-text pairs. %Effectively utilizing large-scale of images and text corpus can improve the capability of vision and textual understanding respectively. UNIMO effectively utilizes the large-scale of text corpus and image collections to learn general textual and visual representations.  The CMCL aligns the visual representation and textual representation, and unifies them into the same semantic space based on image-text pairs. To facilitate different levels of semantic alignment between vision and language, we propose to utilize a series of text rewriting techniques to improve the diversity of cross-modal information. As shown in Figure , we utilize back-translation to generate several positive examples for an image-text pair. Also, to enhance the detail semantic alignment between text and image, we further parse the caption to scene graph  and randomly replace either the objects, attributes or relations in the caption to generate various negative samples. Sentence-level retrieval and replacement is also utilized to enhance the sentence-level alignment. In this way, our model can effectively unify different levels of visual and textual representations into the same semantic space.  The unified-modal architecture mainly has the following advantages compared with previous methods:     
","  Existed pre-training methods either focus on single-modal tasks or multi-modal tasks, and cannot effectively adapt to each other. They can only utilize single-modal data  or limited multi-modal data . In this work, we propose a unified-modal pre-training architecture, namely UNIMO, which can effectively adapt to both single-modal and multi-modal understanding and generation tasks. Large scale of free text corpus and image collections can be utilized to improve the capability of visual and textual understanding, and cross-modal contrastive learning  is leveraged to align the textual and visual information into a unified semantic space over a corpus of image-text pairs. As the non-paired single-modal data is very rich, our model can utilize much larger scale of data to learn more generalizable representations. Moreover, the textual knowledge and visual knowledge can enhance each other in the unified semantic space. The experimental results show that UNIMO significantly improves the performance of several single-modal and multi-modal downstream tasks.",408
" Although there are over 7,000 languages spoken worldwide~, only several dozen have enough data available to support supervised speech recognition, and many languages do not even employ a writing system~. In contrast, most people learn to use spoken language long before they learn to read and write, suggesting that linguistic annotation is not a prerequisite for speech processing systems. This line of reasoning motivates research that aims to discover meaningful linguistic abstractions  directly from the speech signal, with the intention that they could reduce the reliance of spoken language systems on text transcripts.  A rich body of work has recently emerged investigating representation learning for speech using visual grounding objectives~, as well as how word-like and subword-like linguistic units can be made to emerge within these models~. So far, these efforts have predominantly focused on inference, where the goal is to learn a mapping from speech waveforms to a semantic embedding space. Generation of speech conditioned on a point in a semantic space has been less explored, and is what we focus on in this work. We hypothesize that generative approaches offer interesting advantages over relying solely on inference. For example, prior works have demonstrated the capability of recognizing visually descriptive words, but have not been shown to learn non-visual words or grammar. Our experiments show that these aspects of spoken language are learned to some degree by a visually-grounded generative model of speech.  Specifically, we introduce a model capable of directly generating fluent spoken audio captions of images without the need for natural language text, either as an intermediate representation or a form of supervision during training . Tremendous progress has been made recently in natural language image caption generation~ and naturalistic text-to-speech synthesis ~.  Combining these models provides a means for generating spoken image descriptions, but existing approaches for training these models are reliant on text during training. Instead, we leverage sub-word speech units discovered using a self-supervised learning objective as a drop-in replacement for the text. We hypothesize that by using such techniques, an even wider variety of traditionally text-based NLP models could be applied to speech data without the need for transcription or automatic speech recognition  systems. Because all human languages utilize small, discrete phonetic inventories~, we posit that our framework should be applicable for any language in the world. In our experiments, we demonstrate that not just any set of discovered speech units can function in this role. We find the greatest success with units that are discrete, exhibit a low frame-rate, and highly robust to speaker and environmental variability. The main contributions of our paper are as follows:  1. The first methodology for fluent image-to-speech synthesis that does not rely on text. A critical aspect of our approach is factorizing the model into an Image-to-Unit  module and a Unit-to-Speech  module, where the speech units are discovered in a self-supervised fashion. This approach enables disentanglement of linguistic variability and acoustic/speaker variability.  2. Extensive analysis on the properties required for learned units to replace text. While the idea may seem simple and straightforward, obtaining proper units is not a trivial task. In fact, most of the units experimented in this paper fail to serve as drop-in replacements. Moreover, we demonstrate that what are deemed good units vary significantly for inference and generation.  3. Demonstrating insufficiency of beam search-based evaluation. We show that even when an I2U model fails to generate sensible caption through beam search decoding, it can still produce reasonable captions by sampling from the posterior, hinting that posterior mode-based evaluation can only inspect limited aspects of a model.  4. Proposing a semantic diversity-aware metric. We identify issues of an existing metric~ and propose M-SPICE for sampling-based evaluation to address the problems.  5. Over 600,000 spoken audio captions for the MSCOCO dataset. We collect 742 hours of speech from 2,352 people tasked with reading each caption out loud. This dataset will be made publicly available to support work at the intersection of speech, language, and vision.   
"," In this paper we present the first model for directly synthesizing fluent, natural-sounding spoken audio captions for images that does not require natural language text as an intermediate representation or source of supervision. Instead, we connect the image captioning module and the speech synthesis module with a set of discrete, sub-word speech units that are discovered with a self-supervised visual grounding task. We conduct experiments on the Flickr8k spoken caption dataset in addition to a novel corpus of spoken audio captions collected for the popular MSCOCO dataset, demonstrating that our generated captions also capture diverse visual semantics of the images they describe. We investigate several different intermediate speech representations, and empirically find that the representation must satisfy several important properties to serve as drop-in replacements for text.",409
"   Knowledge distillation is a technique to train smaller, more efficient student models by learning from larger teacher models, usually by mimicking the teacher's output. In the scope of neural machine translation , source-side monolingual data is run through the teacher model to produce an output that will be learnt by the student. The absence of parallel data requirements allows the student model to be trained with more data choices. This research focuses on exploring the use of monolingual datasets for knowledge distillation to find out what data should be used.  This research focuses on three aspects. The first is the language origin of the monolingual data. Student models can be trained with additional data in the form of source-side monolingual data. Besides that, the model can also be trained with back-translation data constructed from the target-side monolingual data. We show that using both source-side and target-side data are important because each of them improves performance , depending on the test-set's language origin.   Secondly, we explore the source of the monolingual data. Some research suggests or uses the same data between teacher and student. On the other hand, some research that makes use of knowledge distillation for NMT uses additional dataset, on top of the dataset learnt by the teacher. We explore whether using seen data is necessary, where we find that the student trained with a new unseen monolingual data performs equally with the one trained with the same dataset as the teacher.  The amount of data, including the synthetic ones affects model performance. Therefore, the last thing we explore is the monolingual data size. We find that adding to the monolingual data is generally better. However, varied training data based on language origin is much more important.     
","  % Smaller, lightweight Neural Machine Translation  models can be trained with interpolated knowledge distillation by learning from the output of larger NMT model. To do so, the teacher translates text from source-language to target-language, which are then combined into a dataset for student.   We explore two types of monolingual data that can be included in knowledge distillation training for neural machine translation . The first is the source-side monolingual data. Second, is the target-side monolingual data that is used as back-translation data. Both datasets are translated by a teacher model from source-language to target-language, which are then combined into a dataset for smaller student models.  We find that source-side monolingual data improves model performance when evaluated by test-set originated from source-side. Likewise, target-side data has a positive effect on the test-set in the opposite direction. We also show that it is not required to train the student model with the same data used by the teacher, as long as the domains are the same. Finally, we find that combining source-side and target-side yields in better performance than relying on just one side of the monolingual data.",410
" %What is ToD Task-oriented dialogue systems  are the core technology of the current state-of-the-art smart assistant . These systems are either modularized, Natural Language Understanding , Dialogue State Tracking , Dialogue Policy  and Natural Language Generation , or end-to-end, where a single model implicitly learn how to issue APIs  and system responses .   % what is the current problem we are trying to solve These systems are often updated with new features based on the user needs, e.g., adding new slots and intents, or even completely new domains. However, existing dialogue models are trained with the assumption of having a fixed dataset at the beginning of the training, and they are not designed to add new domains and functionalities through time, without incurring the high cost of a whole system retraining. Therefore, the ability to acquire new knowledge continuously, a.k.a. Continual Learning , is crucial in the design of a dialogue system. Figure shows an high-level intuition of CL in ToDs.     In this setting the main challenge is catastrophic forgetting. This phenomena happens since there is a distributional shift between the tasks in the curriculum which leads to catastrophic forgetting the previously acquired knowledge. To overcome this challenge three kind of methods are usually deployed such as loss regularization, for avoiding to interfere with the previously learned task, rehearsal, which uses an episodic memory to recall previously learned tasks, and architectural, which adds task-specific parameters for each learned task. However, architectural methods are usually not considered as a baseline, especially in sequence-to-sequence generation tasks, because they usually require a further step during testing for selecting which parameter to use for the given task.    To the best our knowledge, Continual Learning  in task-oriented dialogue systems is mostly unexplored or it has been studied in specific settings  using only few tasks learned continuously. Given the importance of the task in the dialogue setting, we believe that a more comprehensive investigation is required, especially by comparing multiple settings and baselines. Therefore in this paper:     In Section we introduce the basic concepts and notation used throughout the paper, for both task-oriented dialogue modelling and continual learning, in Section we introduce the proposed architectural CL method, in Section we describe datasets, baselines, evaluation metrics and experimental settings, and Section we describe the main findings of the paper.    % Based on our experimental results, we discovered that two technique are particularly effective, but they both have a linear cost with respect to the number of learned tasks. To elaborate, in rehearsal-based methods the number of samples stored in the episodic-memory grows linearly with the learned task, while instead in architectural methods the number of parameters grows linearly. Hence concluding that there is not an absolute best  when comparing different methods based on the resources needed, both in term of additional parameters or sample stored in memory.    
"," Continual learning in task-oriented dialogue systems can allow us to add new domains and functionalities through time without incurring the high cost of a whole system retraining. In this paper, we propose a continual learning benchmark for task-oriented dialogue systems with 37 domains to be learned continuously in four settings, such as intent recognition, state tracking, natural language generation, and end-to-end. Moreover, we implement and compare multiple existing continual learning baselines, and we propose a simple yet effective architectural method based on residual adapters. Our experiments demonstrate that the proposed architectural method and a simple replay-based strategy perform comparably well but they both achieve inferior performance to the multi-task learning baseline, in where all the data are shown at once, showing that continual learning in task-oriented dialogue systems is a challenging task. Furthermore, we reveal several trade-off between different continual learning methods in term of parameter usage and memory size, which are important in the design of a task-oriented dialogue system. The proposed benchmark is released together with several baselines to promote more research in this direction.",411
" % Background: % What is MT, history of MT, and current state of MT % What is NMT, current state of NMT % Reason: % Sufficient and necessity condition for writing this article % Organization of this article     Machine Translation  is an important task that aims to translate natural language sentences using computers. The early approach to machine translation relies heavily on hand-crafted translation rules and linguistic knowledge. As natural languages are inherently complex, it is difficult to cover all language irregularities with manual translation rules. With the availability of large-scale parallel corpora, data-driven approaches that learn linguistic information from data have gained increasing attention. Unlike rule-based machine translation, Statistical Machine Translation  learns latent structures such as word alignments or phrases directly from parallel corpora. Incapable of modeling long-distance dependencies between words, the translation quality of SMT is far from satisfactory. With the breakthrough of deep learning, Neural Machine Translation  has emerged as a new paradigm and quickly replaced SMT as the mainstream approach to MT.  Neural machine translation is a radical departure from previous machine translation approaches. On the one hand, NMT employs continuous representations instead of discrete symbolic representations in SMT. On the other hand, NMT uses a single large neural network to model the entire translation process, freeing the need for excessive feature engineering.  The training of NMT is end-to-end as opposed to separately tuned components in SMT. Besides its simplicity, NMT has achieved state-of-the-art performance on various language pairs. In practice, NMT also becomes the key technology behind many commercial MT systems.  As neural machine translation attracts much research interest and grows into an area with many research directions, we believe it is necessary to conduct a comprehensive review of NMT. In this work, we will give an overview of the key ideas and innovations behind NMT. We also summarize the resources and tools that are useful and easily accessible. We hope that by tracing the origins and evolution of NMT, we can stand on the shoulder of past studies, and gain insights into the future of NMT.  The remainder of this article is organized as follows: Section will review the methods of NMT. We first introduce the basics of NMT, and then we selectively describe the recent progress of NMT. We focus on methods related to architectures, decoding, and data augmentation. Section will summarize the resources such as parallel or monolingual corpora that are publicly available to researchers. Section will describe tools that are useful for training and evaluating NMT models. Finally, we conclude and discuss future directions in Section.  
"," Machine translation  is an important sub-field of natural language processing that aims to translate natural languages using computers. In recent years, end-to-end neural machine translation  has achieved great success and has become the new mainstream method in practical MT systems. In this article, we first provide a broad review of the methods for NMT and focus on methods relating to architectures, decoding, and data augmentation. Then we summarize the resources and tools that are useful for researchers. Finally, we conclude with a discussion of possible future research directions. %Machine translation  is an important sub-field of natural language processing which aims to translate natural language sentences between different languages using computers. Recent years has witnessed the great success of end-to-end neural machine translation  models, which has dominated the mainstream approach in commercial machine translation systems. In this work, we first provide a broad review of the methods and challenges in NMT. We introduce three basic components in NMT methods, namely modeling, inference, and learning. The modeling part starts with the encoder-decoder framework and the celebrated attention mechanism, which is followed by Recurrent Neural Networks , Convolutional Neural Networks , and Self-Attention Networks  as potential instances in an NMT architecture. The inference part focuses on the generation of translation sentences from NMT models, which consists of autoregressive,  non-autoregressive, and bidirectional decoding methods. The learning part concentrates on the methods that enhances the expressive capacity of NMT models to learn from data. We highlight the design of training objectives and the use of monolingual data in this part. In addition to the three basic parts, we highlight some of the most significant challenges in NMT, including open vocabulary, prior knowledge integration, as well as the interpretability and robustness issues. Then we summarize useful resources and tools for MT research and maintainance. Finally, we conclude with a discussion of promising future research directions.",412
" NMT is the task of transforming a source sequence into a new form in a particular target language using deep neural networks. Such networks commonly have an encoder-decoder architecture , in which an encoder maps a given input sequence to an intermediate representation and a decoder then uses the same representation to generate candidate translations. Both encoder and decoder are neural networks that are trained jointly. Due to the sequential nature of the NMT task, early models usually relied on recurrent architectures , or benefited from the sliding feature of convolutional kernels to encode/decode variable-length sequences .   Recently, Transformers  have shown promising results for NMT and become the new standard in the field. They follow the same concept of encoding and decoding but in a relatively different fashion. A Transformer is fundamentally a feed-forward model with its unique neural components  that alter the traditional translation pipeline accordingly. Therefore, it is expected if such a model behaves differently than its recurrent or convolutional counterparts. Our goal in this research is to study this aspect in the presence of noise.     NMT engines trained on clean samples provide high-quality results when tested on similarly clean texts, but they break easily if noise appears in the input . They are not designed to handle noise by default and Transformers are no exception. Many previous works have focused on this issue and studied different architectures . In this work, we particularly focus on Transformers\footnote{We assume that the reader is already familiar with the Transformer architecture.} as they are relatively new and to some extent understudied.   A common approach to make NMT models immune to noise is fine-tuning , where a noisy version of input tokens is intentionally introduced during training and the decoder is forced to generate correct translations despite deformed inputs. FT is quite useful for almost all situations but it needs to be run with an optimal setting to be effective. In our experiments, we propose a slightly different learning-rate scheduler to improve FT. We also define a new extension that not only modifies input words but also adds complementary tokens to the target side. We refer to this extension as Target Augmented Fine-Tuning , which is the first contribution of this paper.   In our study, we realized that data augmentation techniques  might not be sufficient enough for some cases and we need a compatible training process and neural architecture to deal with noise. Therefore, we propose Controlled Denoising  whereby noise is added to source sequences during training and the encoder is supposed to fix noisy words before feeding the decoder. This approach is implemented via an auxiliary loss function and is similar to adversarial training. CD is our second contribution.   CD only takes care of noise on the encoder side, so we propose a Dual-Channel Decoding  strategy to study what happens if the decoder is also informed about the input noise. DCD supports multi-tasking through a -channel decoder that samples target tokens and corrects noisy input words simultaneously. This form of fusing translation knowledge with noise-related information has led to interesting results in our experiments. DCD is the third and last contribution of this work.   The remainder of the paper is organised as follows: First, we review previously reported solutions for the problem of noise in NMT in Section , then we present details of our methods and the intuition behind them in Section . To validate our methods, we report experimental results in Section . Finally, we conclude the paper and discuss possible future directions in Section .  
"," Transformers \cite{transformer} have brought a remarkable improvement in the performance of neural machine translation  systems, but they could be surprisingly vulnerable to noise. Accordingly, we tried to investigate how noise breaks Transformers and if there exist solutions to deal with such issues. There is a large body of work in the NMT literature on analyzing the behaviour of conventional models for the problem of noise but it seems Transformers are understudied in this context.  Therefore, we introduce a novel data-driven technique to incorporate noise during training. This idea is comparable to the well-known fine-tuning strategy. Moreover, we propose two new extensions to the original Transformer, that modify the neural architecture as well as the training process to handle noise. We evaluated our techniques to translate the English--German pair in both directions. Experimental results show that our models have a higher tolerance to noise. More specifically, they perform with no deterioration where up to $10$\% of entire test words are infected by noise.",413
"   Cross-lingual word embeddings  represent words from two or more languages in a shared space, so that semantically similar words in different languages are close to each other. Early work focused on jointly learning CLWEs in two languages, relying on a strong cross-lingual supervision in the form of parallel corpora  or bilingual dictionaries . However, these approaches were later superseded by offline mapping methods, which separately train word embeddings in different languages and align them in an unsupervised manner through self-learning   or adversarial training .         Despite the advantage of not requiring any parallel resources, mapping methods critically rely on the underlying embeddings having a similar structure, which is known as the isometry assumption. Several authors have observed that this assumption does not generally hold, severely hindering the performance of these methods . In later work, \citet{ormazabal-etal-2019-analyzing} showed that this issue arises from trying to align separately trained embeddings, as joint learning methods are not susceptible to it.     In this paper, we propose an alternative approach that does not have this limitation, but can still work without any parallel resources. The core idea of our method is to fix the target language embeddings, and learn aligned embeddings for the source language from scratch. This prevents structural mismatches that result from independently training embeddings in different languages, as the learning of the source embeddings is tailored to each particular set of target embeddings. For that purpose, we use an extension of skip-gram that leverages translated context words as anchor points. So as to translate the context words, we start with a weak initial dictionary, which is iteratively improved through self-learning, and we further incorporate a restarting procedure to make our method more robust. Thanks to this, our approach can effectively work without any human-crafted bilingual resources, relying on simple heuristics  or an existing unsupervised mapping method to build the initial dictionary. Our experiments confirm the effectiveness of our approach, outperforming previous mapping methods on bilingual dictionary induction and obtaining competitive results on zero-shot cross-lingual transfer learning on XNLI.     
","  Recent research on cross-lingual word embeddings has been dominated by unsupervised mapping approaches that align monolingual embeddings. Such methods critically rely on those embeddings having a similar structure, but it was recently shown that the separate training in different languages causes departures from this assumption. In this paper, we propose an alternative approach that does not have this limitation, while requiring a weak seed dictionary  as the only form of supervision. Rather than aligning two fixed embedding spaces, our method works by fixing the target language embeddings, and learning a new set of embeddings for the source language that are aligned with them. To that end, we use an extension of skip-gram that leverages translated context words as anchor points, and incorporates self-learning and iterative restarts to reduce the dependency on the initial dictionary. Our approach outperforms conventional mapping methods on bilingual lexicon induction, and obtains competitive results in the downstream XNLI task.",414
" Robust and accurate detection of hate speech is important for minimizing the risk of harm to online users. However, this task has proven remarkably difficult and concerns have been raised about the performance, generalizability and fairness of existing systems. A key challenge in the research community is the lack of high quality datasets that can be freely shared among researchers, have finegrained annotations, contain challenging edge-case content and are not unduly biased by the over-representation of certain demographic groups.  We address these problems in online hate classification by utilizing a system for dynamic data collection, model training and evaluation. Specifically, we use a human-and-model-in-the-loop approach, whereby an initial model is trained and annotators are then tasked with entering content that would fool it into making an incorrect classification. Models are then retrained on all collected data and the process is repeated. New rounds of data are collected, with annotators still trying to trick the improved models by entering the most difficult and unusual forms of content. In this way models `learn from the worst' as the more challenging content they are shown, the faster they will hopefully improve.  The dataset formation was organized into four 10,000 phases. Round 1 contains content created synthetically by annotators without direction. Round 2a contains content created using directed `pivots' and Round 2b contains perturbed counterfactual `contrast sets'~ on the entries from 2a. Round 3 contains content inspired by real world hate, as well as associated perturbations, using out-of-distribution annotators for testing. Each round of data collection is designed to address issues which appeared in the previous round. The model error rate decreased across rounds, from 72.1\% in the first round to 35.8\% in the last round, showing that models became increasingly harder to trick -- even though content become progressively more adversarial as annotators became more experienced.   This work makes three major contributions to online hate classification research. First, we present the first dataset for online hate classification that has been created dynamically using a human-and-model-in-the-loop process. The system we use has been closely documented and all of the models that we create will be made publicly available. Second, a new dataset of 40,623 synthetic entries is presented of which 55\% is hate, including fine-grained annotations by trained annotators for label, type, target and pivot . We also mark in the dataset whether each entry tricked the target model for that round. Third, as part of the dataset we present over 14,000 challenging contrastive examples, which were created during the dynamic data generation process.  Dynamic dataset generation through a human-and-model-in-the-loop approach offers several advantages over static datasets. First, problems can be addressed as work is conducted -- rather than creating the dataset and then discovering any inadvertent design flaws, as would be the case with static benchmarks. Second, the model-in-the-loop means that annotators' work is guided by the model; they receive real-time feedback from the model about how effectively different strategies are beating it; this lets them target their efforts to exploit key weaknesses, creating a dataset with many hard-to-classify entries. Third, the dataset can be constructed to better meet the requirements of machine learning; our dataset is balanced, comprising 54\% hate. It includes hate targeted against a large number of targets, providing variety for the model to learn from, and many entries were directed to counter established problems in hate detection model training, such as overfitting on keywords.    
"," We present a first-of-its-kind large synthetic training dataset for online hate classification, created from scratch with trained annotators over multiple rounds of dynamic data collection. We provide a 40,623 example dataset with annotations for fine-grained labels, including a large number of challenging contrastive perturbation examples. Unusually for an abusive content dataset, it comprises 54\% hateful and 46\% not hateful entries. We show that model performance and robustness can be greatly improved using the dynamic data collection paradigm. The model error rate decreased across rounds, from 72.1\% in the first round to 35.8\% in the last round, showing that models became increasingly harder to trick -- even though content become progressively more adversarial as annotators became more experienced. Hate speech detection is an important and subtle problem that is still very challenging for existing AI methods. We hope that the models, dataset and dynamic system that we present here will help improve current approaches, having a positive social impact.",415
"   Speech separation, also known as cocktail party problem, aims to separate target speech from interference background . It is often used as the front end of speech recognition for improving the accuracy of human-machine interaction. Conventional speech separation technologies include computational auditory scene analysis , non-negative matrix factorization , HMM-GMM , and minimum mean square error . Recently, deep learning based speech separation becomes a new trend , which is the focus of this paper. According to whether speakers閳 information is known as a prior, deep-learning-based speech separation techniques can be divided into three categories, which are speaker-dependent , target-dependent, and speaker-independent speech separation.    Speaker-dependent speech separation needs to known the prior information of all speakers, which limits its practical applications. Nowadays, the research on speech separation is mostly speaker-independent and target-dependent.      Speaker-independent speech separation based on deep learning faces the speaker permutation ambiguity problem. In order to solve this problem, two techniques have been proposed. The first one is deep clustering %     .     It projects each time-frequency unit to a higher-dimensional embedding vector by a deep network, and conducts clustering on the embedding vectors for speech separation.     The second technique is permutation invariant training %     . For each training mixture, it picks the permutation of the speakers that has the minimum training error among all possible permutations to train the network.    % Besides, there are some other effective algorithm based on deep learning, such as deep ensemble learning and deep attractor network.  Target-dependent speech separation based on deep learning aims to extract target speech from a mixture given some prior knowledge on the target speaker. The earliest speech separation method takes the target speaker as the training target . It has to train a model for each target speaker, which limits its practical use. To prevent training a model for each target speaker, speaker extraction further takes speaker codes extracted from a speaker recognition system as part of the network input . Some representative speaker extraction methods are as follows.  applies a context adaptive deep neural network to extract the target speaker through a speaker adaptation layer. It takes the estimated mask and ideal binary mask as the training objective.  proposed a temporal spectrum approximation loss to estimate a phase sensitive mask for the target speaker.  generalized the end-to-end speaker-independent speech separation  to the end-to-end speaker extraction.  % It is more practical when only registered speakers need to be responded, such as speaker diarization and speech recognition .  The aforementioned methods are all single-channel methods. Although they work well in clean scenarios, their performance degrades significantly in reverberant scenarios. To improve the performance of speech separation in reverberant scenarios, many multichannel methods were proposed, which has the following two major forms. The first form combines spatial features that are extracted from microphone arrays, such as interaural time difference and interaural level difference, with spectral features as the input of single-channel speech separation networks . The second form uses a deep network to predict a mask for each speaker at each channel, and then conducts beamforming for each speaker . For brevity, we call this method deep beamforming. Some methods combined the above two forms for boosting their advantages together in reverberant scenarios, e.g. .  The aforementioned multichannel methods are only studied with traditional fixed arrays, such as linear arrays or spherical arrays. However, for  far-field speech separation problems with high reverberation, they suffer significant performance degradation. How to maintain the estimated speech at the same high quality throughout an interested physical space is of broad interests.  Ad-hoc microphone array, which is a group of randomly distributed microphones collaborating with each other, is a solution to the problem. Figure  gives a comparison example where a target speaker extraction problem with a fixed array is on the left and that with an ad-hoc microphone array on the right. From the figure, we see that, compared with the fixed array that is far from the target speaker, the ad-hoc microphone array has several apparent advantages. First, an ad-hoc microphone array may put a number of microphones around the target speaker, which significantly reduced the probability of far-field speech processing. By channel selection, it might be able to form a local microphone array around the target speaker. At last, it may be able to incorporate application devices of various physical sizes.     In literature, ad-hoc microphone arrays have consistently been an important research topic . However, they face many practical problems due to the lack of important priors. Recently,  addresses the difficulties of ad-hoc microphone arrays, such as lack of priors and insufficient estimation of variables, by deep learning for the first time. The proposed method, named deep ad-hoc beamforming , was originally designed for speech enhancement only, which predicts segment-level signal-to-noise-ratio  by deep neural networks for supervised channel selection. Later on, some speech separation methods based on ad-hoc microphone arrays were proposed.  proposed a transform-average-concatenate strategy for a filter-and-sum network  to realize the channel reweighting/selection ability for ad-hoc microphone arrays. Because ad-hoc microphone arrays lack the prior of the number and spatial distribution of microphones,  proposed a network architecture by interleaving inter-channel processing layers and temporal processing layers to leverage information across time and space alternately. %{\color{brown} ASR in ad-hoc microphone array...}  % The filter-and-sum network   first conducts pre-separation on a selected reference microphone by estimating its beamforming filters, and then estimates the beamforming filters of all remaining microphones based on pair-wise cross-channel features. They further improved the channel reweighting/selection ability of FaSNet by a transform-average-concatenate strategy  for ad-hoc microphone arrays.   However, existing deep learning based speech separation with ad-hoc microphone arrays are all speaker-independent. To our knowledge, target-dependent speech separation with ad-hoc microphone arrays are far from explored yet. In many applications, extracting and tracking target speech is of more interests than separating a mixture into its components. This is particularly the case for ad-hoc microphone arrays, where several speakers may locate far apart and talk independently.   In this paper, we propose a target-dependent speech separation algorithm with ad-hoc microphone arrays, named DAB based on speaker extraction . Our algorithm consists of three components: first, we propose a supervised channel selection based on speaker extraction, which applies bi-directional long short-term memory  networks to estimate the utterance-level SNR of the target speaker. Then, we employ the heuristic channel selection algorithms in  to pick the channels with high SNRs. We further apply a single-channel speaker extraction algorithm to the selected channels for the mask estimation problem of the target speech. At last, we use the estimated masks to derive a beamformer for the target speaker, such as minimum variance distortionless response  . Experimental results on a WSJ0-adhoc corpus show that the proposed DABse performs well in reverberant environments.   The rest of the paper is organized as follows. We introduce the signal model of the speaker extraction problem with ad-hoc microphone arrays in Section . Then, we present the deep ad-hoc beamforming system based on speaker extraction in Section . In Section , we present the experimental results. Finally, we conclude this study in Section .   
"," % abstract %\parttitle{First part title} %if any %Text for this section. %\parttitle{Second part title} %if any %Text for this section. Recently, the research on ad-hoc microphone arrays with deep learning has drawn much attention, especially in speech enhancement and separation. Because an ad-hoc microphone array may cover such a large area that multiple speakers may locate far apart and talk independently, target-dependent speech separation, which aims to extract a target speaker from a mixed speech, is important for extracting and tracing a specific speaker in the ad-hoc array. However, this technique has not been explored yet. In this paper, we propose deep ad-hoc beamforming based on speaker extraction, which is to our knowledge the first work for target-dependent speech separation based on ad-hoc microphone arrays and deep learning. The algorithm contains three components. First, we propose a supervised channel selection framework based on speaker extraction, where the estimated utterance-level SNRs of the target speech are used as the basis for the channel selection. Second, we apply the selected channels to a deep learning based MVDR algorithm, where a single-channel speaker extraction algorithm is applied to each selected channel for estimating the mask of the target speech. We conducted an extensive experiment on a WSJ0-adhoc corpus. Experimental results demonstrate the effectiveness of the proposed method.",416
" Supervised and semi-supervised Machine Learning algorithms are now ubiquitous in the analysis of social media data. At the core of these algorithms is their ability to make sense of a vast amount of semi-structured real-time data streams, allowing them to automatically categorize or filter new data examples into, usually pre-defined, classes. Multi-class text classification has been successfully used in public health surveillance, election monitoring, or vaccine stance prediction~\parencite{salathe2011assessing,bermingham2011using,brownstein2009digital}. In recent years such algorithms have also been developed to mitigate the negative effects of social media, such as in the detection of cyber-bullying, hate speech, misinformation, and automated accounts ~\parencite{reynolds2011using,davidson2017automated,shu2017fake,davis2016botornot}.  The microblogging service Twitter has played a central role in these efforts, as it serves as a public medium and provides easy access to real-time data through its public APIs, making it the primary focus of this work. Twitter is well described as a classical example of a non-stationary system with frequently emerging and disappearing topical clusters~\parencite{costa2014concept}. This poses problems for the aforementioned applications, as the underlying data distribution is different between training time and the time of the algorithm's application in the real world. This phenomenon is known as concept drift~\parencite{schlimmer1986incremental} and can lead to a change in performance of the algorithm over time.  It is important to distinguish concept drift from other reasons for performance differences between training and testing, such as random noise due to sampling biases or differences in data preprocessing~\parencite{vzliobaite2010learning,webb2016characterizing}. A classic example of concept drift is the change in the meaning of classes, which requires an update of the learned class decision boundaries in the classifier. This is sometimes also referred to as real concept drift. Often, however, an observed performance change is a consequence of a change in the underlying data distribution, leading to what is known as virtual drift~\parencite{widmer1996learning,tsymbal2004problem}. Virtual drift can be overcome by supplemental learning, i.e.\ collecting training data from the new environment. A good example are periodic seasonality effects, which may not be fully represented in the initial training data and only become fully visible over time. However, in practice it is usually very difficult  to disentangle virtual from real concept drift, and as a consequence they are treated as the same effect~\parencite{vzliobaite2010learning}.  On Twitter concept drift might appear on very different time scales and at different rates. Sudden shifts in a debate might be triggered by a quickly evolving news cycle or a catastrophic event. Concept drift may also be a slow process in which the way a topic is discussed gradually changes over time. A substantial amount of work has been dedicated to detecting and overcoming concept drift~\parencite{widmer1996learning,vzliobaite2010learning,elwell2011incremental}. Three basic re-training procedures for overcoming concept drift have been proposed:  a time-window approach,  an incremental model, and  an ensemble model~\parencite{costa2014concept}. In the time-window approach, a sliding window of recent training examples is used to train an algorithm. In this approach, the algorithm ignores training data collected outside of that time window. The incremental model, in contrast, uses all previously collected training examples to re-train the model. Lastly, the ensemble model trains a model for each time window and uses the consensus of all previous models for future predictions. As found in~\parencite{costa2014concept}, in the case of hashtag prediction on Twitter data, the incremental method gave the best results.  Although sophisticated methods have been proposed to estimate concept drift in an unsupervised way~\parencite{katakis2010tracking,yang2008conceptual}, in practice, a certain amount of re-annotation for both the detection and re-training of models seems unavoidable. The decision about which of the newly collected data to annotate points to an exploration-exploitation dilemma, which is usually addressed in the context of an active learning framework~\parencite{settles2009active}. The Crowdbreaks platform~\parencite{muller2019crowdbreaks} is an example of such a framework and has been built with the goal of exploring optimal solutions to this problem in order to overcome concept drift.  A change in the underlying data distribution might not necessarily have a negative impact on classifier performance. It is conceivable, for example, that a polarisation in a debate on Twitter about a topic could even lead to an improvement in classifier performance. It is therefore important to ask how much we should be worried about concept drift: even if model performance were to decrease, the real impacts on our analysis or interpretation might be negligible.  The consequences of concept drift are task-, environment-, and model-dependent~\parencite{vzliobaite2016overview}. Here, we will address concept drift in the specific case of vaccine stance classification. Vaccine stance classification on Twitter data has been widely studied and has shown promising links to vaccination decision making and vaccine uptake rates in different countries~\parencite{salathe2011assessing,bello2017detecting}. The COVID-19 pandemic further emphasizes its importance, as evolving concerns about vaccines may significantly influence their effect~\parencite{johnson2020online,burki2020online}.  To the best of our knowledge, only one study directly addressed concept drift in vaccine stance classification. In this study~\parencite{d2019monitoring} on tweets posted between September 2016 and January 2017 in Italian language, the authors did not find a substantial improvement of their model from incremental re-training before specific events. Re-training was performed on 60 newly annotated tweets from seven manually selected events. The authors conclude that either their original algorithm was already quite robust towards concept change, or that the newly collected training data was too small to see an effect.  Here, we use FastText~\parencite{joulin2016bag} and BERT ~\parencite{devlin2018bert}, two commonly used models in social media text classification. Most work on the topic of concept drift was conducted using classical machine learning models, to which also FastText belongs. These types of models are very reliant on high-quality annotation data. More recently, models of the transformer family, such as BERT~\parencite{devlin2018bert}, have been proposed, which require significantly less annotation data. In what follows, we will examine whether these two models also share different concept drift characteristics.  The goal of this work is to emulate a typical social media analysis study, in which data is collected for a certain period of time, and a supervised machine learning model is trained on a subset of annotated data. The model is then published and used to predict newly collected data. First, we will try to answer whether or not concept drift can be observed, and if so, at what rate it occurs. Second, we will investigate the influence of the study duration and the amount of annotation data used. Lastly, we will examine to what extent concept drift influences the final analysis outcomes, in this case a sentiment index.   
","   Social media analysis has become a common approach to assess public opinion on various topics, including those about health, in near real-time.   The growing volume of social media posts has led to an increased usage of modern machine learning methods in natural language processing.   While the rapid dynamics of social media can capture underlying trends quickly, it also poses a technical problem: algorithms trained on annotated data in the past may underperform when applied to contemporary data.   This phenomenon, known as concept drift, can be particularly problematic when rapid shifts occur either in the topic of interest itself, or in the way the topic is discussed.   Here, we explore the effect of machine learning concept drift by focussing on vaccine sentiments expressed on Twitter, a topic of central importance especially during the COVID-19 pandemic.   We show that while vaccine sentiment has declined considerably during the COVID-19 pandemic in 2020, algorithms trained on pre-pandemic data would have largely missed this decline due to concept drift.   Our results suggest that social media analysis systems must address concept drift in a continuous fashion in order to avoid the risk of systematic misclassification of data, which is particularly likely during a crisis when the underlying data can change suddenly and rapidly.",417
" In this paper, we tackle the problem of screening a finite pool of documents, where the aim is to retrieve relevant documents satisfying a given set of predicates that can be verified by human or machines . In this context, if a document does not satisfy at least one predicate, it is treated to be irrelevant. A predicate represents a property, a unit of meaning, given in natural language . By this means a predicate might be interpreted in a variety of ways in text, so making keywords-based search hard to reach high recall while keeping a decent level of precision . We interpret the screening problem as high recall problem, i.e., the aim is to retrieve all relevant documents maximizing precision. %we assume predicates and candidate documents are given  % Since predicates can be interpreted in a variety of ways, it makes the problem of document screening very challenging especially when there is a little training data.  The screening finds application in many domains, such as i) in systematic literature reviews ; % -SLRs-  AND papers studying older adults }; ii) database querying - where items filtered  in/out based on predicates ; iii) hotel search - where the hotels retrieve  based upon filters of interest . Consequently,  the document screening is an instance of finite pool binary classification problems  , where we need to classify a finite set of objects minimizing cost. % As an instance of the problem, we choose the screening phase of SLRs what makes the problem rather challenging since each review is different and has a unique set of predicates . Typically, authors of an SLR retrieve a candidate pool of documents executing a keywords-based query on a database such as Scopus. To avoid missing papers, the query tends to be inclusive, which means that it returns hundreds or thousands of results  that are later manually screened by researchers based on predefined predicates. For example, researchers might look for papers that describe all of the following predicates at the same time: 1) ""include papers that study older adults 85+ years"", 2) ""include papers conducted randomized controlled trial"", 3) "" include papers about behavioral intervention"". Therefore, here we have the conjunctive query of three inclusive predicates. A bottleneck of the screening process is the predicate evaluation, i.e.,  identifying which of the given predicates are satisfied in a current document. For example, in literature reviews, authors validate predicates, however, this is time-consuming, exhaustive, and very expensive .   An effective technique to solve screening problems is crowdsourcing where the crowd can solve even complex screening tasks with high accuracy and lower cost compared to expert screening . However, achieving a good performance in crowd-based screening requires a deep understanding of how to design tasks and model their complexity , how to test and filter workers , how to aggregate results into a classification decision, or how to improve worker engagement .   Machine learning  algorithms have also made a very impressive progress in solving complex screening tasks. However, obtaining a sufficiently large set of training data is still a key bottleneck for accurate ML classifiers. Active learning   accelerates this process by minimizing the size of training data that is required to train better classifiers via selecting the most informative instances for annotation. The effectiveness of AL have been proven in many domains , but most of the work considers single-label cases while multi-label AL problems have been far less investigated. The challenge in applying AL to a multi-label classification problem is that the algorithm should measure the unified informativeness of each unlabeled item across all labels. The state of the art multi-label AL strategies follow an object-wise  labeling, where the AL algorithm first finds the relevance scores  of  pairs, and then aggregates these scores to find the informativeness of items . However, it may ignore the interaction between labels .    \paragraph{Original contribution.} We investigate how to efficiently combine crowdsourcing and ML for item screening. It is a challenging task since the budget is limited and there are countless number of ways to spend it on the problem. We propose a multi-label AL screening specific sampling technique for querying unlabelled items for annotating. Our algorithm takes a decision how to choose unlabeled data to annotate by crowd workers in order to maximize the performance of a screening task. Unlike existing multi-label AL approaches that rely on global labeling, we choose the local labeling method, where for each label  we determine the relevancy to each item.  
"," In this paper, we explore how to efficiently combine crowdsourcing and machine intelligence for the problem of document screening, where we need to screen documents with a set of machine-learning filters. Specifically, we focus on building a set of machine learning classifiers that evaluate documents, and then screen them efficiently. It is a challenging task since the budget is limited and there are countless number of ways to spend the given budget on the problem. We propose a multi-label active learning screening specific sampling technique -objective-aware sampling- for querying unlabelled documents for annotating. Our algorithm takes a decision on which machine filter need more training data and how to choose unlabeled items to annotate in order to minimize the risk of overall classification errors rather than minimizing a single filter error.  We demonstrate that objective-aware sampling significantly outperforms the state of the art active learning sampling strategies. % on multi-filter classification problems.",418
"   One of the hallmarks of human intelligence is the ability to generalize seamlessly across heterogeneous sensory inputs and different cognitive tasks. We see objects, hear sounds, feel textures, smell odors, and taste flavors to learn underlying concepts present in our world. Much of AI's existing progress in multimodal learning, however, focuses primarily on a fixed set of predefined modalities and tasks that are consistent between training and testing. As a result, it is unclear how to transfer knowledge from models trained for one modality  to another  at test time. This scenario is particularly important for low-resource target modalities where unlabeled data is scarce and labeled data is even harder to obtain . In the unimodal case, this is regarded as meta-learning or few-shot learning. In contrast, we formally define the cross-modal generalization setting as a learning paradigm to train a model that can  quickly perform new tasks in a target modality  and  doing so while being trained on a different source modality. In this paper, we study the data and algorithmic challenges for cross-modal generalization to succeed. %Such a learning paradigm is particularly useful in leveraging high-resource source modalities to help low-resource target modalities, where unlabeled data is scarce and labeled data is even harder to obtain, such as audio from low-resource languages, real-world environments, and medical images.      As a motivating example, Figure illustrates a scenario where large-scale image classification benchmarks can help audio classification, which is a less studied problem with fewer large-scale benchmarks. In this ambitious problem statement, a key research question becomes: how can we obtain generalization across modalities despite using separate encoders for different source  and target  modalities? The technical challenge involves aligning shared knowledge learned from source image tasks with target audio tasks. Our problem statement differs from conventional meta-learning and domain adaptation where one can take advantage of the same source and target modality with shared encoders which helps generalization by having the same representation space. In our case, the discrepancies in modalities requires one to learn new output concepts expressed in new input modalities. As a result, cross-modal generalization requires new ideas to synchronize  multimodal sources and targets. What is the minimal extra supervision required to perform this alignment?  In this paper, we formalize the conditions required for successful generalization and show that another level of supervision is necessary under partial observability across modalities and tasks. Supervision comes in the form of cross-modal meta-alignment  to capture a space where representations of similar concepts in different modalities are close together while ensuring quick generalization to new tasks . We introduce a novel algorithm called \names\  that leverages readily available multimodal data from the internet  for meta-alignment and cross-modal generalization. Through theoretical analysis and empirical ablations, we study our proposed algorithm with both strongly and weakly paired multimodal data, showing that cross-modal generalization is possible even with limited extra supervision.  %How can one transfer knowledge learned from an image classification task to speech event classification? The problem of cross-modal generalization brings fundamental differences regarding how data is expressed across different modalities . In comparison to meta-learning and domain adaptation, the different input spaces now consist of extremely high-dimensional, complex, and heterogeneous source and target modalities. As a result, we are unable to use a shortcut by sharing encoders as commonly seen in same-modality, different domain settings which allow for the same representation space between source and target domains. This raises a fundamental research question: how can we obtain generalization across modalities despite using separate encoders for different source and target modalities? These discrepancies in modalities requires one to learn new output concepts expressed in new input modalities. %We show that existing domain adaptation, meta-learning, and transfer learning approaches are unable to bridge the gap between such heterogeneous paradigms where both input modalities and output tasks are different.  % emphasize cant share encoders, need explicit alignment % emphasize different label space, generalize meta-learning % formulate crossmodal ml and therefore we propose meta alignment % first para ok. like to learn but different modalities. % second para. compared to ml and da, 1 critical issue when trying to do crossmodal - have hetero data between source and target. cant use shortcut such as same encoder for images of different domains. need different encoders 1 for each. how do we solve this? need another level of supervision to help - where meta alignment comes in. what we propose - a technique to address the core technical challenge of crossmodal ml which is how to learn different encoders. meta alignment is a way to do that, a contrastive learning approach.  %To account for this technical challenge, we formalize the conditions required for successful generalization and show that another level of supervision is necessary under partial observability across modalities and tasks. This form of supervision comes in the form of cross-modal alignment to capture a space where representations of similar concepts in different modalities are close together while ensuring quick generalization to new low-resource tasks . Our analysis leads to a novel algorithm based on contrastive learning called \names\  that leverages either strongly or weakly paired multimodal data abundant on the internet. Finally, we carefully study the data and algorithmic requirements for our approach to succeed through theoretical analysis and empirical ablations.  %Very hard problem of crossmodal meta-learning. What is the minimal amount of supervision  required to solve this hard task of cross-modal meta-learning? In this paper we will explore this through theory and empirics  %We highlight two crucial distinctions:  the different input spaces consist of extremely high-dimensional, complex, and heterogeneous source and target modalities, and  there exist different task distributions between source and target modalities, such as the inherent differences between the label spaces when transferring from image to audio classification tasks. These discrepancies in both input and output spaces requires one to learn new output concepts expressed in new input modalities. We show that existing domain adaptation, meta-learning, and transfer learning approaches are unable to bridge the gap between such heterogeneous paradigms where both input modalities and output tasks are different.  % how do we handle limited resource modalities and task, we explore cross-modal approach % note: define modality, concept, task % note: a better way of saying cross-modal cross-task  %, which allows us to learn a classifier for transfer from source to target tasks. %This makes it particularly suitable for generalization across modalities and tasks due to the presence of unseen concepts and annotations in the target modality. %We show that this space:  groups similar concepts expressed across different modalities,  is well-clustered across concepts, and  generalizes well to new concepts, making it particularly suitable for generalization across modalities and tasks. %While our first attempt at meta-alignment uses strong pairings across source and target modalities , we further provide an extension to use only weak pairs between modalities. Weak pairs represent coarse groupings of semantic correspondence which better capture the many-to-many relations between real-world multimodal data  and allow us to use large banks of weakly paired multimodal data available on the internet and prepared for machine learning studies such as video data  and image captioning data . %Finally, we quantify the trade-offs between labeling more data in the target modality versus obtaining better source-target alignment.  %provide theoretical justification to quantify the benefits of our approach: {\color{red} ZIYIN TODO} \zing[ziyin: should mention and focus on the difficulty of definition and formalization] %instead of a classical generalization error in the target modality that scales wrt the sample complexity of the target modality, our approach is bounded by the sample complexity in the source modality. As a result, the error is therefore reduced with ample samples in the source modality and a well-aligned space.  We present experiments on three cross-modal tasks: generalizing from  text to image,  image to audio, and  text to speech. In all cases, the goal is to classify data from a new target modality given only a few  labeled samples. %We find that \names\ accurately performs few-shot alignment of concepts from different modalities, thereby allowing generalization from concepts in the source modality to new concepts in the target modality. We perform extensive experiments to compare with related approaches including target modality meta-learning that would be expected to perform well since they have seen thousands of labeled examples from the target modality during meta-training. Surprisingly, \names\ is competitive with these baselines and significantly outperforms other cross-modal approaches. In addition, we study settings where the target modality suffers from noisy or limited data, a scenario particularly prevalent in low-resource modalities. %While this setting makes it difficult to directly train in the target modality, our approach efficiently leverages cross-modal information to perform well.   
"," The natural world is abundant with concepts expressed via visual, acoustic, tactile, and linguistic modalities. Much of the existing progress in multimodal learning, however, focuses primarily on problems where the same set of modalities are present at train and test time, which makes learning in low-resource modalities particularly difficult. In this work, we propose algorithms for cross-modal generalization: a learning paradigm to train a model that can  quickly perform new tasks in a target modality  and  doing so while being trained on a different source modality. We study a key research question: how can we ensure generalization across modalities despite using separate encoders for different source and target modalities? Our solution is based on meta-alignment, a novel method to align representation spaces using strongly and weakly paired cross-modal data while ensuring quick generalization to new tasks across different modalities. We study this problem on 3 classification tasks: text to image, image to audio, and text to speech. Our results demonstrate strong performance even when the new target modality has only a few  labeled samples and in the presence of noisy labels, a scenario particularly prevalent in low-resource modalities. %Despite vast differences in these raw modalities, humans seamlessly perceive multimodal data, learn new concepts, and show extraordinary capabilities in generalizing across input modalities. %In addition, our method works particularly well when the target modality suffers from noisy or limited labels, a scenario particularly prevalent in low-resource modalities. %, sometimes outperforming within modality few-shot baselines that have seen thousands of labeled examples from that target modality during meta-training. %\zing[Ziyin: heterogeneous -> multimodal? since we are assuming there is an underlying shared space, so maybe not heterogeneous] %\zing[Ziyin: since this is the first sentence in the intro, maybe remove this?] %Similarly, truly general artificial intelligence  systems must learn to generalize across multiple input modalities and output tasks. %In this work, we define and propose algorithms for a new notion of generalization:  %, languages, and concepts. %We believe that our proposed methods could open new doors towards better generalization in multimodal AI systems.",419
" Cloud services have become increasingly popular and are expected to gain 331.212.6\%\ billion every year for the Fortune 1,000 . Amazon is estimated to have a 1004.11\%-91.58\%82.9\%76.3\% - 91.3\%$ in high impacted incidents. Model ablation analysis showed that each of the ML models we used provided a lift in the final ensemble for different incident types. To the best of our knowledge, we are the first one to present a deployed incident triage service for cloud-scale online services.  This paper makes three key contributions:   This paper is organized as follow: Section  presents the background of an incident management system; Section  provides details of   {\TransferAssistant}; Section  shows experimental results; Section  describes the deployment of {\TransferAssistant} in Azure; Section  discusses lessons learned and implications for implementing and deploying an incident triage service at cloud scale;  Section  presents related work;  and Section  concludes this paper.  
","   As cloud services are growing and generating high revenues, the cost of downtime in these services is becoming significantly expensive. To reduce loss and service downtime, a critical primary step is to execute incident triage, the process of assigning a service incident to the correct responsible team, in a timely manner. An incorrect assignment risks additional incident reroutings and increases its time to mitigate by 10x. However, automated incident triage in large cloud services faces many challenges:  a highly imbalanced incident distribution from a large number of teams,    wide variety in formats of input data or data sources,     scaling to meet production-grade requirements, and     gaining engineers' trust in using machine learning recommendations.    To address these challenges, we introduce {\TransferAssistant}, an intelligent incident transfer service combining multiple machine learning techniques -- gradient boosted classifiers, clustering methods, and deep neural networks -- in an ensemble to recommend the responsible team to triage an incident. Experimental results on real incidents in Microsoft Azure show that our service achieves $82.9\%$ F1 score. For highly impacted incidents, {\TransferAssistant} achieves F1 score from $76.3\% - 91.3\%$. We have applied best practices and state-of-the-art frameworks to scale {\TransferAssistant} to handle incident routing for all cloud services. {\TransferAssistant} has been deployed in Azure since October 2017 and is used by thousands of teams daily.",420
"  Program source code contains rich structure information, like the syntax structure and control or data flow. Learning from these structures has been a hot topic in the area of deep learning on source code. In recent years, instead of applying basic sequential neural models, researchers have used more complex neural networks to capture the explicit structure of source code. Most researches use abstract syntax trees  as they are easy-to-acquire for most programming languages and semantically equivalent to source code.  A problem of ASTs is that they do not explicitly reflect structural information beyond syntax dependencies, like control and data flow. A viable solution is adding different types of control and data flow edges on ASTs to generate program graphs, and apply graph neural networks  on programs to learn their representations . However, these approaches do not consider that apart from control or data flow edges, the nodes and edges of the original ASTs are also differently typed. For example, in ASTs, some nodes refer to identifiers, and some nodes define upper-level structures as control flows. For parent-child links, the relation between a function definition node to its function body or one of its arguments is apparently different. We believe if we explicitly add node and edge types to programs graphs, it will help neural models to understand programs better.  Our idea of adding types to nodes and edges in AST coincides with the concept of heterogeneous graphs. Heterogeneous graphs, or heterogeneous information networks , refer to a group of graphs with multiple types of nodes and edges. A typical example of heterogeneous graphs is knowledge graphs, in which the nodes are different types of entities, and the edges represent different relations. In this paper, we propose an approach for building heterogeneous program graphs from ASTs. To obtain the type of AST nodes and edges, we use the abstract syntax description language   grammar.  After we acquire heterogeneous graphs for code snippets, we need to find a GNN model to effectively represent these graphs. Although some existing GNN-for-code works  have pointed out that there exist different types for AST nodes, they only consider node type in the initial node embedding and neglect their differences in the message passing  step. So we turn our sight to the field of heterogeneous graph embeddings. Recently, heterogeneous graph neural networks have become widely used in heterogeneous graph embedding. Unlike traditional graph neural networks, heterogeneous graph neural networks are capable of integrating node and edge type information in the message passing stage and map different types of nodes to different feature space. We use heterogeneous graph transformer   on our heterogeneous program graphs to calculate the representation of programs.  We evaluate our approach on two tasks: comment generation and method naming, with two Python datasets from different domains. These two tasks can be seen as two different forms of code summarization, so both of them require understanding the semantics of the input code snippets. The results show that our approach outperforms existing GNN models and other state-of-the-art approaches, indicating the extra benefit of bringing heterogeneous graph information to source code.  To summarize, our contributions are:  To our knowledge, we are the first to put forward the idea of representing programs as heterogeneous graphs and apply heterogeneous GNN on source code snippets.  We propose an approach of using ASDL grammars to build heterogeneous program graphs from program ASTs.  We evaluate our approach on two different tasks involving graph-level prediction on source code snippets. Our approach outperforms other GNN models on both comment generation and method naming tasks.  
"," Program source code contains complex structure information, which can be represented in structured data forms like trees or graphs. To acquire the structural information in source code, most existing researches use abstract syntax trees . A group of works add additional edges to ASTs to convert source code into graphs and use graph neural networks to learn representations for program graphs. Although these works provide additional control or data flow information to ASTs for downstream tasks, they neglect an important aspect of structure information in AST itself: the different types of nodes and edges. In ASTs, different nodes contain different kinds of information like variables or control flow, and the relation between a node and all its children can also be different.  To address the information of node and edge types, we bring the idea of heterogeneous graphs to learning on source code and present a new formula of building heterogeneous program graphs from ASTs with additional type information for nodes and edges. We use the ASDL grammar of programming language to define the node and edge types of program graphs. Then we use heterogeneous graph neural networks to learn on these graphs. We evaluate our approach on two tasks: code comment generation and method naming. Both tasks require reasoning on the semantics of complete code snippets. Experiment results show that our approach outperforms baseline models, including homogeneous graph-based models, showing that leveraging the type information of nodes and edges in program graphs can help in learning program semantics.",421
"   % Every day pharmaceutical companies receive numerous medical inquiries related to their products from patients, healthcare professionals, research institutes, or public authorities from a variety of sources .  % These medical inquiries may relate to drug-drug-interactions, availability of products, side effects of pharmaceuticals, clinical trial information, product quality issues, comparison with competitor products, storage conditions, dosing regimen, and the like.  % On the one hand, a single medical inquiry is simply a question of a given person searching for a specific information related to a medicinal product. On the other hand, a plurality of medical inquiries from different persons may provide useful insight into matters related to medicinal products and associated medical treatments. % Examples of these insights could be early detection of product quality or supply chain issues, anticipation of treatment trends and market events, improvement of educational material and standard answers/frequently asked question coverage, potential changes in treatment pattern, or even suggestions on new possible indications to investigate. % From a strategic perspective, this information could enable organizations to make better decisions, drive organization results, and more broadly create benefits for the healthcare community.   % transition paragraph - machine learning can help However, obtaining high-level general insights is a complicated task since pharmaceutical companies receive  copius amounts of medical inquiries every year. Machine learning and natural language processing represent a promising route to automatically extract insights from these large amounts of unstructured  medical text. % % % text mining in general and in the biomedical domain Natural language processing and text mining techniques have been widely used in the medical domain, with particular emphasis on electronic health records.  In particular, deep learning has been successfully applied to medical text, with the overwhelming majority of works in supervised learning, or representation learning  to learn specialized word vector representations . % %There is little work however on unsupervised learning from unstructured medical text.  Conversely, the literature on unsupervised learning for medical text is scarce despite the bulk of real-world medical text being unstructured, without any labels or annotations. % Unsupervised learning from unstructured medical text is mainly limited to the development of topic models based on latent Dirichlet allocation . Examples of applications in the medical domain are clinical event identification in brain cancer patients from clinical reports, modeling diseases and predicting clinical order patterns from electronic health records, or detecting cases of noncompliance to drug treatment from patient forums. % Only recently, word embeddings and unsupervised learning techniques have been combined to analyze unstructured medical text to study the concept of diseases, medical product reviews, or to extract informative sentences for text summarization.  % real-world corpus of medical inquiries and its challenges In this work, we combine biomedical word embeddings and unsupervised learning to discover topics from real-world medical inquiries received by Bayer\texttrademark. % A real-world corpus of medical inquiries presents numerous challenges. From an inquirer  perspective, often the goal is to convey the information requested in as few words as possible to save time. This leads to an extensive use of acronyms, sentences with atypical syntactic structure, occasionally missing verb or subject, or inquiries comprising exclusively a single noun phrase. % Moreover, since medical inquiries come from different sources, it is common to find additional  information related to the text source; examples are references to internal computer systems, form frames  alongside with the actual form content, lot numbers, email headers and signatures, city names.  % % mixture of layman and medical language The corpus contains a mixture of layman and medical language depending  on the inquirer being either a patient or a healthcare professional. Style and content of medical inquiries vary quite substantially according to which therapeutic areas  a given medicinal product belongs to.  % add sentence to refer to the text representation %as one can see from Fig., As already mentioned, medical inquiries are short. More specifically, they comprise less than fifteen words in the vast majority of cases.  % Standard techniques for topic modelling based on LDA do not apply, since the main assumption - each document/text is a distribution over topics - clearly does not hold given that the text is short.  % Approaches based on pseudo-documents or using auxiliary information are also not suitable since no meaningful pseudo-document nor auxiliary information are available for medical inquiries. % Moreoever, these models aim to learn semantics  directly from the corpus of interest. However, the recent success of pretrained embeddings shows that it is beneficial to include semantics learned on a general  corpus, thus providing semantic information difficult to obtain from smaller corpora. This is particularly important for limited data and short text settings. To this end, there has been recently some work aimed at incorporating word embeddings into probabilistic models similar to LDA  and that - contrary to LDA - satisfies the single topic assumption .  Even though these models include  semantic information in the topic model, it is not evident how to choose the required hyper-parameters, for example determining an appropriate threshold when filtering semantically related word pairs. Concurrently to our work, document-level embeddings and hierarchical clustering have been combined to obtain topic vectors from news articles and a question-answer corpus.  % summary Here, we propose an approach based on specialized biomedical word embeddings and unsupervised learning to discover topics from short, unstructured, real-world medical inquiries. This approach - schematically depicted in Fig. - is then used to discovery topics in medical inquiries received by Bayer\texttrademark\ Medical Information regarding the oncology medicinal product Stivarga\texttrademark.    
"," %141 words % the motivation Millions of unsolicited medical inquiries are received by pharmaceutical companies every year.  It has been hypothesized that these inquiries represent a treasure trove of information, potentially giving insight into matters regarding medicinal products and the associated medical treatments.  % the challenge However, due to the large volume and specialized nature of the inquiries, it is difficult to perform timely, recurrent, and comprehensive analyses. % the solution Here, we propose a machine learning approach based on natural language processing and unsupervised learning to automatically discover key topics in real-world medical inquiries from customers. This approach does not require ontologies nor annotations.  % the results The discovered topics are meaningful and medically relevant, as judged by medical information specialists, thus demonstrating that unsolicited medical inquiries are a source of valuable customer insights. % the implications and outlook Our work paves the way for the machine-learning-driven analysis of medical inquiries in the pharmaceutical industry, which ultimately aims at improving patient care.",422
" Dynamic models of text aim at characterizing temporal changes in patterns of document generation. Most successful dynamic language models are Bayesian in nature, and lag behind state-of-the-art deep language models in terms of expressibility. A natural space to study some of the temporal aspects of language is that of the large review datasets found in e-commerce sites.  The availability of millions of reviewed items, such as business or services, books or movies, whose reviews have been recorded in time scales of years, opens up the possibility to develop deep scalable models that can predict the change in taste and preference of users as time evolves. Originally, the interaction of users in these e-commerce sites were studied in the context of collaborative filtering, where the goal was to predict user ratings, based on user interaction metrics. Here we aim to look directly at the content of reviews as time evolves.  %More KDD probably, to much focus on the ratings and recommendations  %-------- %The shear size of e-commerce and review web sites naturally lend itself to the development of data mining tools which are able to provide users with a way to sort out relevant information. This is the task assigned to recommender systems.  Originally kick started by the Netflix competition, matrix factorization  methods through collaborative filtering, aim at predicting user ratings based on user interaction metrics. This rating based methods are lacking as they are unable to clarify the nature of the user preferences, in particular how those preferences change on time. In order to address this issue, methodologies that exploit costumers reviews are gaining attention.  %--------- Costumer reviews provide a rich and natural source of unstructured data which can be leverage to improve recommender system performance . Indeed, reviews are effectively a form of recommendation. % Recently, a variety of deep learning solutions for recommendation have profit from their ability to extract latent representations from review data, encoding rich information related to both users and items. % %Review  content naturally encodes  % This type of data  % Review content is of contextual nature, as the text arises from the interaction of user preferences and items at hand.  % Time represents yet another dimension of context, as user preference and item availability change with time % -- and indeed, % causal and temporal relations have been known to improve the performance of recommender systems  .  % Despite this fact, % recent natural language processing  methodologies for rating and reviews  lag behind at incorporating temporal structure in their language representations. In the present work we exploit recurrent neural network  models for point processes, and feed them neural representations of text, to characterize costumer reviews. Our goal is to capture the changes in user taste and item importance during time, and to exploit those changes to better predict when are new reviews arriving, and what do they actually say. We summarize our contributions as follows: {}  %     We present the related work in Section  and introduce our model in Section . The baseline models used for comparison in this paper are presented in Section . The experimental setup and results are presented in Section . Finally, in Section  we conclude and discuss future work.  
"," Deep neural network models represent the state-of-the-art methodologies for natural language processing.  % Here we build on top of these methodologies to incorporate temporal information and model how review data changes with time. % Specifically, we use the dynamic representations of recurrent point process models, % % which encode the nonlinear relations between content and timing of the reviews received by e.g. businesses or services,  % which encode the history of how business or service reviews are received in time,  % to generate instantaneous language models with improved prediction capabilities.  % Simultaneously, our methodologies enhance the predictive power of our point process models by incorporating summarized review content representations.  % % as that encoded in recurrent point process models, and improve the predictive power of these model by incorporating the text representations.  % % Our methodologies resemble that of a hierarchical model, whereupon the temporal information is used as a  representation for the language model.  % We provide recurrent network and temporal convolution solutions for modeling the review content. % We deploy our methodologies in the context of recommender systems,  % as to enhance the expressibility of current models, % effectively characterizing the change in preference and taste of users as time evolves. Source code is available at \cite{source_code}.",423
"  Most authentication methods commonly used today rely on users setting custom passwords to access their accounts and devices. Password-based authentications are popular due to their ease of use, ease of implementation and the established familiarity of users and developers with the method.   However studies show that users tend to set their individual passwords predictably, favoring short strings, names, birth dates and reusing passwords across sites.  Since chosen passwords exhibit certain patterns and structure, it begs the question whether it is possible to simulate these patterns and generate passwords that a human user realistically might have chosen.  Password guessing is an active field of study, until recently dominated by statistical analysis of password leaks and construction of corresponding generation algorithms . These methods rely on expert knowledge and analysis of various password leaks from multiple sources to generate rules and algorithms for efficient exploitation of learned patterns.  On the other hand, in recent years major advances in machine-driven text generation have been made, notably by novel deep-learning based architectures and efficient training strategies for large amounts of training text data. These methods are purely data driven, meaning they learn only from the structure of the input training text, without any external knowledge on the domain or structure of the data. % Deep learning models have recently shown remarkable performance concerning text classification and text generation.  Major advancements in the field have been fueled by the development in several central directions such as:    In this paper we will continue the exploration of data driven deep-learning text generation methods for the task of password-guessing. While some applications to password guessing already show promising results, most frameworks still can not reach or surpass state-of-the-art password generation algorithms. % On the other hand, considering password guessing problems, some popular frameworks  as well as a large body of state-of-art research suggest that advanced deep learning methodologies are still to be further explored.  Ideally, one would attempt to design more efficient password-guessing models aided by neural networks and cutting-edge practices.  Our findings and contributions can be summarized as follows:    
","     Password guessing approaches via deep learning have recently been investigated with significant breakthroughs in      their ability to generate novel, realistic password candidates.     In the present work we study a broad collection of deep learning and probabilistic based models in the light of password guessing:      attention-based deep neural networks, autoencoding mechanisms and generative adversarial networks.      We provide novel generative deep-learning models in terms of variational autoencoders exhibiting state-of-art sampling performance,     yielding additional latent-space features such as interpolations and targeted sampling.     Lastly, we perform a thorough empirical analysis in a unified controlled framework over well-known datasets .      Our results not only identify the most promising schemes driven by deep neural networks, but also illustrate the strengths of each approach in terms of generation variability and sample uniqueness.",424
" % 1 page  % Definition and importance of the causality knowledge.  % causality knowledge, as an important knowledge for artificial intelligence  systems, has been proven helpful in many downstream tasks, especially in the NLP domain. % % In this work, we follow ConceptNet and COPA to focus on the causal relations between daily events. % However, due to the lack of a high-quality and large-scale causality knowledge resource, the application of causality knowledge in downstream tasks is still limited.  Humans possess a basic knowledge about facts and understandings for commonsense of causality in our everyday life.  For example, if we leave five minutes late, we will be late for the bus; if the sun is out, it's not likely to rain; and if we are hungry, we need to eat. %Causality is an important commonsense reasoning that humans use all the time,  Such causality knowledge has been shown to be helpful for many NLP tasks. Thus, it is valuable to teach machines to understand causality.   Causal relations in the commonsense domain are typically contributory and contextual.  %   By contributory\footnote{The other two levels are absolute causality  and conditional causality , which commonly appear in the scientific domain rather than our daily life.}, we mean that the cause is neither necessary nor sufficient for the effect, but it strongly contributes to the effect.  By contextual, we mean that some causal relations only make sense in a certain context. The contextual property of causal relations is important for both the acquisition and application of causal knowledge. For example, if some people tell the AI assistant  ``they are hungry'' in a meeting, a basic assistant may suggest them to order food because it has the knowledge that `being hungry' causes `eat food'. A better assistant may suggest ordering food after the meeting because it knows that the causal relation between `being hungry' and `eat food' may not be plausible in the meeting context.  % \ye{I made small adaptation to this paragraph } % For example, if a person is in the middle of a meeting, he/she may tell the AI assistant  that he/she is hungry, a good AI assistant may suggest him/her to eat some food because it has the knowledge that `being hungry' cause `eat food', but an extraordinary AI assistant may suggest that ``I can help order some food for you to eat after the meeting'' because it knows that the causal relation between `being hungry' and `eat food' may not be plausible in the context of a meeting. Without understanding the contextual property of causal knowledge, achieving such a level of intelligence would be challenging.  To help machines better understand the causality commonsense, many efforts have been devoted into developing the causality knowledge bases.  For example, ConceptNet and ATOMIC leverage  human-annotation to acquire small-scale but high-quality causality knowledge. After that, people try to leverage linguistic patterns  to acquire causality knowledge from textual corpus. However, causality knowledge, especially those trivial knowledge for humans, are rarely formally expressed in documents, a pure text-based approach might struggle at covering all causality knowledge. Besides that, none of them take the aforementioned contextual property of causal knowledge into consideration, which may restrict their usage in downstream tasks.     % Causal relations in the commonsense domain are typically contributory and contextual.  % By contributory\footnote{The other two levels of causality are absolute causality  and conditional causality , which commonly appear in the scientific domain rather than our daily life.}, we mean that the cause is neither necessary nor sufficient for the effect, but it strongly contributes to the effect.  % By contextual, we mean that some causal relations only make sense in a certain context. % The contextual property of causal relations is important for both the acquisition and application of causality knowledge. % For example, if some people tell the AI assistant  ``they are hungry'' in a meeting, a basic assistant may suggest them to order food because it has the knowledge that `being hungry' causes `eat food'. A better assistant may suggest ordering food after the meeting because it knows that the causal relation between `being hungry' and `eat food' may not be plausible in the meeting context.  % Without understanding the contextual property of causality knowledge, achieving such a level of intelligence would be challenging.  %       %      %     } %      %      % \end{table}     % % limitation of existing acquisition methods % Conventional approaches  \ye{i think this should be more elaborated. maybe give an example?} However, two drawbacks of these approaches significantly limit their usage in downstream tasks: %     In this paper, we propose to ground causality knowledge into the real world and explore the possibility of acquiring causality knowledge from visual signals .  By doing so, we have three major advantages:  Videos can be easily acquired and can cover rich commonsense knowledge that may not be mentioned in the textual corpus;  Events contained in videos are naturally ordered by time. As discussed by, there exists a strong correlation between temporal and causal relations, and thus such time-consecutive images can become a dense causality knowledge resource;  Objects from the visual signals can act as the context for detected causality knowledge, which can remedy the aforementioned lack of contextual property issue of existing approaches.   To be more specific, we first define the task of mining causality knowledge from time-consecutive images and propose a high-quality dataset .  To study the contextual property of causal relations, for each pair of events, we provide two kinds of causality annotations: one is the causality given certain context and the other one is the causality without context.  Distribution analysis and case studies are conducted to analyze the contextual property of causality. An example from Vis-Causal is shown in Figure, where the causal relation between ``dog is running'' and ``blowing leaves'' only makes sense when the context is provided because the dog is running on the leaves, so its high speed and quickly-moved pow cause the leaves blow around. Without the context  ``leaves on the ground'', this causal relation is implausible. After that, we propose a Vision-Contextual Causal  model, which can effectively leverage both the pre-trained textual representation and visual context to acquire causality knowledge and can be used as a baseline method for future works. Experimental results demonstrate that even though the task is still challenging, by jointly leveraging the visual and contextual representation, the proposed model can better identify meaningful causal relations from time-consecutive images. To summarize, the contributions of this paper are three-fold:  We formally define the task of mining contextual causality from the visual signal;  We present a high-quality dataset Vis-Causal;  We propose a Vision-Contextual Causal  model to demonstrate the possibility of mining contextual causality from the vision signal. % Experimental results prove that considering context is crucial for understanding causality and representing the visual context with textual representation is helpful. % Further analysis shows that the proposed task is still challenging for current models, and we may need to consider injecting external knowledge to better understand the videos and acquire causality knowledge. % \ye{there's no real reference to the text part in the into, NLP people might think it's not suitable for ACL? maybe add that the models use some description and objects which are represented in a textual form}   %   % 
","  Causality knowledge is crucial for many artificial intelligence systems. Conventional textual-based causality knowledge acquisition methods typically require laborious and expensive human annotations. As a result, their scale is often limited. Moreover, as no context is provided during the annotation, the resulting causality knowledge records  typically do not take the context into consideration. To explore a more scalable way of acquiring causality knowledge, in this paper, we jump out of the textual domain and investigate the possibility of learning contextual causality from the visual signal. Compared with pure text-based approaches, learning causality from the visual signal has the following advantages:  Causality knowledge belongs to the commonsense knowledge, which is rarely expressed in the text but rich in videos;  Most events in the video are naturally time-ordered, which provides a rich resource for us to mine causality knowledge from;  All the objects in the video can be used as context to study the contextual property of causal relations. In detail, we first propose a high-quality dataset Vis-Causal and then conduct experiments to demonstrate that with good language and visual representation models as well as enough training signals, it is possible to automatically discover meaningful causal knowledge from the videos. Further analysis also shows that the contextual property of causal relations indeed exists, taking which into consideration might be crucial if we want to use the causality knowledge in real applications, and the visual signal could serve as a good resource for learning such contextual causality. Vis-Causal and all used codes are available at: \url{https://github.com/HKUST-KnowComp/Vis_Causal}. % In detail, we first identify events from the videos, which are represented with natural sentences, and then leverage the visual signal to predict the contextual causal relations among these events.     % In this work, we mimic how human beings learn causality and explore the possibility of acquiring causality knowledge with visual signal. % To do so, we first define the task of mining contextual causality knowledge from visual signals, which aims at evaluating models' abilities to identify causal relation given certain visual context, and then employ the crowd-sourcing to annotate a high-quality dataset Vis-Causal. % On top of that, we propose a Vision-Contextual Causal  model that can utilize the images as context to better acquire causality knowledge. % Different from existing \revisehm{causality knowledge acquisition works}, \revisehm{to the best of our knowledge, }the proposed solution \revisehm{is the first one that }has the potential to preserve contextual property  of causal relations.",425
"  % The advent of deep learning techniques has dramatically improved accuracy of speech recognition models . Deep learning techniques first saw success by replacing the Gaussian Mixture Model  of the Acoustic Model  part of the conventional speech recognition systems  with the Feed-Forward Deep Neural Networks  , further with Recurrent Neural Network  such as the  Long Short-Term Memory  networks  or Convonlutional Neural Networks . In addition to this, there have been improvements in noise robustness by using models motivated by auditory processing , data augmentation techniques , and beam-forming . Thanks to these advances, voice assistant devices such as Google Home  and Amazon Alexa have been widely used at home environments.  Nevertheless, it was not easy to run such high-performance speech recognition systems  on devices largely because of the size of the Weighted Finite State Transducer   handling the lexicon and the language model.  Fortunately, all-neural end-to-end  speech recognition systems were introduced which do not need a large WFST or an n-gram Language Model   . These complete end-to-end systems have started surpassing the performance of the conventional WFST-based decoders with a very large training  dataset  and a better choice of target unit  such as Byte Pair Encoded  subword units.  In this paper, we provide a comprehensive review of the various components and algorithms of an end-to-end speech recognition system. In Sec., we give a brief overview of the various neural building blocks of an E2E Automatic Speech Recognition  model. The most popular E2E ASR architectures are reviewed in Sec.. Additional techniques used to improve the performance of E2E ASR models are discussed in Sec.. Techniques used for compression and quantization of the all-neural E2E ASR models are covered in Sec.. Sec. gives a summary of the paper. % % %# Data augmentation and overfitting 
","   In this paper, we review various end-to-end automatic speech recognition   algorithms and their optimization techniques for on-device applications.   Conventional speech recognition systems comprise a large number of discrete   components such as an acoustic model, a language model, a pronunciation model,    a text-normalizer, an inverse-text normalizer, a decoder based on a Weighted Finite State   Transducer , and so on. To obtain sufficiently high speech recognition   accuracy  with such conventional speech recognition systems, a very large   language model  is usually needed. Hence, the corresponding   WFST size becomes enormous, which prohibits their on-device implementation. Recently, fully neural network end-to-end speech recognition algorithms have been   proposed. Examples include speech recognition systems based on  Connectionist Temporal Classification , Recurrent Neural Network Transducer , Attention-based Encoder-Decoder models , Monotonic   Chunk-wise Attention ,    transformer-based speech recognition systems, and so on. These fully neural   network-based systems require much smaller memory footprints compared to   conventional algorithms, therefore their on-device implementation has become   feasible. In this paper, we review such end-to-end speech recognition models.   We extensively discuss their structures, performance, and advantages compared   to conventional algorithms.",426
"   Intuitively, if you see a lot of examples of natural language questions about TV shows, it ought to also help understand similar syntax in questions about movies, or in questions that refer to both movies and TV shows together. Ideally, the training examples from the related domain should strictly improve performance, not hurt it. %[nkscales] FYI -- I reverted the below sentence to close to its original form to better match the tone of the first paragraph. If the sentence still doesn't sound right, let me know. If you can satisfy that property, then you have at least a chance at eventually achieving arbitrarily robust performance across a range of domains, given sufficient training data in aggregate. %You need to satisfy that property in order to at have a shot at achieving arbitrarily robust performance across a range of domains, given simply sufficient data across those domains in aggregate.  How and to what extent current machine learning  approaches can be made to robustly solve natural language understanding  at the scale of arbitrary natural language across domain -- with or without access to large quantities of training data -- remains, however, an open question.  On one hand, research into the scaling behavior of deep learning systems has found generalization loss to decrease reliably with training size and model size in a power law or related logarithmic relationship across a range of architectures and tasks, from image classification with convolutional neural networks~ to language modeling with Transformers~. Recent results in an i.i.d.\ setting show this pattern to persist across many orders of magnitude, with no established upper limit~.  At the same time, it has been shown that current ML systems continue to struggle to achieve robust performance in classes of tasks that require compositional generalization  % [nikola] IMO this part of the sentence does not contribute much. I suggest skipping it and only keeping the citation. %-- that is, tasks in which known building blocks must be composed at test time in ways unseen during training ~ -- an ability that has been argued to be crucial to robust language understanding~.  In this paper, we combine these two lines of research by investigating the effect of training size on error rates in the context of a compositional task. Specifically, we derive a suite of extended datasets based on the Compositional Freebase Questions  semantic parsing benchmark~. We then use the compositional structure of each example to construct controlled experiments that measure the error rates when increasing training size in settings requiring compositional generalization and in settings simulating scaling to a broader scope of natural language. We apply these experiments to analysis of Transformers~ in a setting of fixed computational cost -- that is, of fixed model size and fixed training steps -- and demonstrate key limits to their scalability in this setting.  Our contributions are the following:    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
"," We present \starcfq{} : a suite of large-scale datasets of varying scope based on the \cfq{} semantic parsing benchmark, designed for principled investigation of the scalability of machine learning systems in a realistic compositional task setting. Using this suite, we conduct a series of experiments investigating the ability of Transformers to benefit from increased training size under conditions of fixed computational cost. We show that compositional generalization remains a challenge at all training sizes, and we show that increasing the scope of natural language leads to consistently higher error rates, which are only partially offset by increased training data. We further show that while additional training data from a related domain improves the accuracy in data-starved situations, this improvement is limited and diminishes as the distance from the related domain to the target domain increases.",427
" %     Solving math word problems  poses unique challenges for understanding natural-language problems and performing arithmetic reasoning over quantities with commonsense knowledge. As shown in \autoref{fig:example}, a typical MWP consists of a short narrative describing a situation in the world and asking a question about an unknown quantity. To solve the MWP in \autoref{fig:example}, a machine needs to extract key quantities from the text, such as ""100 kilometers"" and ""2 hours"", and understand the relationships between them. General mathematical knowledge like ""distance = velocity  time"" is then used to calculate the solution.   %The task of automatically solving Math Word Problems  requires mapping the human-readable natural language into machine-understandable logic forms, e.g., expressions, followed by execution process that calculates the numeric answer. \autoref{fig:example} an example of a math word problem, the ground truth answer and the expression that derives the answer when executed. Recently, researchers have focused on solving MWPs using neural models. The advantage of these neural models is that they do not rely on hand-crafted features.  Researchers have recently focused on solving MWPs using neural-symbolic models. These models usually consist of a neural perception module  that maps the problem text into a solution expression or tree, and a symbolic module which executes the expression and generates the final answer. Training these models requires the full supervision of the solution expressions.  However, these fully-supervised approaches have three drawbacks. First, current MWP datasets only provide one solution for each problem, while there naturally exist multiple solutions that give different paths of solving the same problem. For instance, the problem in \autoref{fig:example} can be solved by ``'' if we first calculate the speed and then multiply it by the total time; alternatively, we can solve it using ``'' by summing the distances of the first and second parts of the journey. The models trained with full supervision on current datasets are forced to fit the given solution and cannot generate diverse solutions. Second, annotating the expressions for MWPs is time-consuming. However, a large amount of MWPs with their final answers can be mined effortlessly from the internet . How to efficiently utilize these partially-labeled data without the supervision of expressions remains an open problem. Third, current supervised learning approaches suffer from the train-test discrepancy. The fully-supervised learning methods optimize expression accuracy rather than answer accuracy. However, the model is evaluated by the answer accuracy on the test set, causing a natural performance gap.   To address these issues, we propose to solve the MWPs with weak supervision, where only the problem texts and the final answers are required. By directly optimizing the answer accuracy rather than the expression accuracy, learning with weak supervision naturally addresses the train-test discrepancy. Our model consists of a tree-structured neural model similar to \citet{Xie2019AGT} to generate the solution tree and a symbolic execution module to calculate the answer. However, the symbolic execution module for arithmetic expressions is non-differentiable with respect to the answer accuracy, making it infeasible to use back-propagation to compute gradients. A straightforward approach is to employ policy gradient methods like REINFORCE to train the neural model. The policy gradient methods explore the solution space and update the policy based on generated solutions that happen to hit the correct answer. Since the solution space is large and incorrect solutions are abandoned with zero reward, these methods usually converge slowly or fail to converge.  %However, previous end-to-end solvers are trained in a fully-supervised setting in which the ground truth expressions are given during training. This has several drawbacks. First, a math word problem can be solved by multiple expressions given different ways of thinking, but only one is provided to training in fully-supervised setting. For instance, the problem in  \autoref{fig:example} can be solved by  if we want to calculate the speed first and then multiply it by total hours. Or, we can solve it by , if we first compute the length of the second part of the journey given the ratio of time spans, and add it to the first part. However, only the first expression is given as the ground-truth expression in the dataset, and thus neural models tend to ``punish'' the second expression. In this way, fully-supervised learning fails to generate more diverse and correct expressions. The second problem is ``train-test discrepancy''. It means that MLE uses a surrogate objective of maximizing equation likelihood during training, while the evaluation metric of the task is solution accuracy, which is non-differentiable.  proposes to solve this via reinforcement learning but still use pre-trained MLE model. Last but not least, there's the problem of lack of fully annotated data online. Recruiting crowd-workers to provide the correct equations is time consuming. However, thousands of MWPs have been posted in online forums, where the final answers can be easily mined. These data can be useful if we can train our model without the supervision of expressions.   %To address these issues, we propose to solve the MWPs with weak supervision, where only the problem texts and the final answers are required for learning. % We adopt the goal-driven tree-structured  model proposed by~\citet{Xie2019AGT} as the base model. %Since the execution process of arithmetic expressions in previous deep learning models is non-differentiable, it is infeasible to use back-propagation to compute gradients. A straightforward approach is to employ policy gradient methods like REINFORCE. In weakly-supervised MWP, policy gradient methods explore the solution space and update the policy based on generated solutions that happen to hit the right answers, while incorrect solutions are totally abandoned. Since the solution space is quite large, policy gradients methods usually converge slowly or sometimes even fail to converge.  To improve the efficiency of weakly-supervised learning, we propose a novel fixing mechanism to learn from incorrect predictions, which is inspired by the human ability to learn from failures via abductive reasoning. The fixing mechanism propagates the error from the root node to the leaf nodes in the solution tree and finds the most probable fix that can generate the desired answer. The fixed solution tree is further used as a pseudo label to train the neural model. \autoref{fig:framework} shows how the fixing mechanism corrects the wrong solution tree by tracing the error in a top-down manner.  Furthermore, we design two practical techniques to traverse the solution space and discover possible solutions efficiently. First, we observe a positive correlation between the number of quantities in the text and the size of the solution tree , and propose a tree regularization technique based on this observation to limit the range of possible tree sizes and shrink the solution space. Second, we adopt a memory buffer to track and save the discovered fixes for each problem with the fixing mechanism. All memory buffer solutions are used as pseudo labels to train the model, encouraging the model to generate more diverse solutions for a single problem.   In summary, by combining the fixing mechanism and the above two techniques, the proposed learning-by-fixing  method contains an exploring stage and a learning stage in each iteration, as shown in \autoref{fig:framework}. We utilize the fixing mechanism and tree regularization to correct wrong answers in the exploring stage and generate fixed expressions as pseudo labels. In the learning stage, we train the neural model using these pseudo labels.  We conduct comprehensive experiments on the Math23K dataset. The proposed LBF method significantly outperforms the reinforcement learning baselines in weakly-supervised learning and achieves comparable performance with several fully-supervised methods. Furthermore, our proposed method achieves significantly better answer accuracies of all the top-3/5 answers than fully-supervised methods, illustrating its advantage in generating diverse solutions. The ablative experiments also demonstrate the efficacy of the designed algorithms, including the fixing mechanism, tree regularization, and memory buffer.  % This paper makes three major contribution: %    %Policy gradient methods like REINFORCE are frequently used in weakly-supervised tasks . Such methods suffer from sparse reward, cold start and inefficient exploration of solution space. This is because neural network make wrong perceptions and generate negative samples which are ``abandoned'' by REINFORCE. Human beings, like neural networks, tend to make inaccurate perceptions. However, they embody the skills to correct their misperceptions when reasoning about the wrong forms and guessing about correct patterns. What's more, they are able to approach the solutions in diverse ways. For example, in \autoref{fig:framework}, a child might provide a wrong expression  given the problem. Then he started to reason about where he did wrong and found out he could actually fix the expression by replacing the first """" with """". The fixed expression looks nothing like the ground truth expression *$) provided by the dataset.    % Therefore, policy gradients methods converge slowly or even fail to converge without MLE pre-training on fully-supervised data.  %Inspired by this, we propose a novel fixing mechanism which resembles human閳ユ獨 ability to diagnose and fix the expressions that cannot generate desired answers. The ground truth answer  is propagated through the expression tree in a top-down manner. In the meantime, we try depth-first-search for a possible fix. Similar to Memory-Augmented Policy Optimization which utilizes a memory buffer to save previous successful trajectories given by REINFORCE, we adopt a memory buffer to store all successful fixes. Different expressions in the buffer are all used to train the model, thus allowing us to generate more diverse answers.  % Our contributions are summarized as following: %    % } %      %      % \end{table}\def\year{2021}\relax %File: formatting-instructions-latex-2021.tex %release 2021.1 \documentclass[letterpaper]{article} % DO NOT CHANGE THIS \usepackage{aaai21}  % DO NOT CHANGE THIS \usepackage{times}  % DO NOT CHANGE THIS \usepackage{helvet} % DO NOT CHANGE THIS \usepackage{courier}  % DO NOT CHANGE THIS \usepackage[hyphens]{url}  % DO NOT CHANGE THIS \usepackage{graphicx} % DO NOT CHANGE THIS \urlstyle{rm} % DO NOT CHANGE THIS \def\UrlFont{\rm}  % DO NOT CHANGE THIS \usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT \usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT \frenchspacing  % DO NOT CHANGE THIS \setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS \setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS % additional packages \usepackage{latexsym} \usepackage{makecell} \usepackage{amsmath,amssymb,mathtools,bm,etoolbox} \usepackage{algorithm} \usepackage[noend]{algorithmic} \usepackage{enumitem} \usepackage{xcolor} \usepackage{pifont} \usepackage{multirow} \usepackage{diagbox} \usepackage[switch]{lineno} \usepackage[autostyle=false, style=english]{csquotes} \MakeOuterQuote{""} \usepackage[draft]{hyperref} %  Disable links - according to AAAI format guide  \DeclarePairedDelimiter\ceil{\lceil}{\rceil} \DeclarePairedDelimiter\floor{\lfloor}{\rfloor}  \newcommand{\algorithmautorefname}{Algorithm} \newcommand{\eg}{e.g.} \newcommand{\etc}{etc} \newcommand{\ie}{i.e.} \newcommand{\etal}{et al.} \renewcommand{
"," % Most previous solvers of math word problems  are learned with full supervision and fail to generate diverse solutions for each problem. In this paper, we address this issue by introducing a weakly-supervised paradigm for learning MWPs. Our method only requires the annotations of the final answers and can generate various solutions for a single problem. To boost weakly-supervised learning, we propose a novel learning-by-fixing  framework that mimics the human ability to learn from incorrect predictions. Specifically, the fixing mechanism propagates the error from the root node to the leaf nodes of a solution tree and infers the most probable fix that can be executed to the desired answer. To generate more diverse solutions, tree regularization is applied to guide the efficient shrinkage and exploration of the solution space, and a memory buffer is designed to track and save the discovered various fixes for each problem. Experimental results on the Math23K dataset show the proposed LBF framework significantly outperforms reinforcement learning baselines in weakly-supervised learning. Furthermore, it achieves comparable top-1 and much better top-3/5 answer accuracies than fully-supervised methods, demonstrating its strength in producing diverse solutions. Previous neural solvers of math word problems  are learned with full supervision and fail to generate diverse solutions. In this paper, we address this issue by introducing a weakly-supervised paradigm for learning MWPs. Our method only requires the annotations of the final answers and can generate various solutions for a single problem. To boost weakly-supervised learning, we propose a novel learning-by-fixing  framework, which corrects the misperceptions of the neural network via symbolic reasoning. Specifically, for an incorrect solution tree generated by the neural network, the fixing mechanism propagates the error from the root node to the leaf nodes and infers the most probable fix that can be executed to get the desired answer. To generate more diverse solutions, tree regularization is applied to guide the efficient shrinkage and exploration of the solution space, and a memory buffer is designed to track and save the discovered various fixes for each problem. Experimental results on the Math23K dataset show the proposed LBF framework significantly outperforms reinforcement learning baselines in weakly-supervised learning. Furthermore, it achieves comparable top-1 and much better top-3/5 answer accuracies than fully-supervised methods, demonstrating its strength in producing diverse solutions.",428
"  % Hate speech, its extensiveness, effect     % --> humans can't inspect every sample  % Memes, their use & hateful memes % HM data set % challenge       % --> not all dataset is 'truly' requires multi-modality     % --> HM dataset proposes 'benign confounders' to make sure multimodality is required    Memes have gained huge popularity over the past years, resulting in over 180m posts on different social media platforms until 2018. Although memes are oftentimes harmless and generated especially for humorous purposes, they have also been used to produce and disseminate hate speech in toxic communities. Hate Speech  is a direct attack on people based on race, ethnicity, national origin, religious affiliation, sexual orientation, sex, gender, and serious disease or disability -- a growing problem in modern society. Giant tech companies, such as Facebook, own platforms where millions of users log in daily and they are obliged to remove a tremendous amount of HS to protect their users. According to Mike Schroepfer, Facebook CTO, they took an action on 9.6 million pieces of content for violating their HS policies in the first quarter of 2020. This amount of malicious content cannot be tackled by having humans inspect every sample. Consequently, machine learning and in particular deep learning techniques are required to alleviate the extensiveness of online hate speech. Detecting hate speech in memes is challenging due to the multimodal nature of memes . Therefore, these techniques have to process the content the way humans do: holistically. When viewing a meme, a human would not think about the words and the picture independently; but understand the combined meaning. Moreover, while the visual and linguistic information of a meme is typically neutral or funny individually, their combination may result in a hateful meme.      A recent study shows that state-of-the-art methods for hate speech detection in multimodal memes perform poorly compared to humans: 64.73\% vs. 84.7\% accuracy. To catalyze sophisticated research in this area, Facebook AI launched the Hateful Memes Challenge and published a dataset containing more than 10,000 newly created multimodal memes. Multimodal tasks reflect many real-world problems, including how humans perceive and understand the world around them.       There has been a surge of interest in multimodal problems since 2015 in visual question answering, image captioning, speech recognition and beyond. But it is not always clear to what extent genuinely multimodal reasoning and understanding are needed to solve current challenges. For instance, for some datasets language can unintentionally impose strong priors, which might result in a remarkable performance, without any understanding of the visual content. The Hateful Memes challenge design and dataset are created to encourage and measure truly multimodal understanding and reasoning of the models. A key point to achieve this are the so-called ``benign confounders''  which addresses the risk of exploiting unimodal priors by models: for every hateful meme, there are alternative images or text that flip the label to not-hateful. Such image and text confounders require multimodal reasoning to classify the original meme and its confounders correctly. Thus, making the dataset challenging and appropriate for testing the true multimodality of a model.    In the following, we analyze the challenge dataset and describe our prize-winning solution that placed third among 3,173 participants in the Hateful Memes Challenge in detail. Our solution achieves 0.811 AUROC with an accuracy of 0.765 on the challenge test set, which improves all the benchmark models, including the state-of-the-art models at that time, such as ViLBERT  and VisualBERT . Nevertheless, the accuracy is still behind humans with a mentionable gap, highlighting the need for progress in multimodal research.   
","   Memes on the Internet are often harmless and sometimes amusing. However, by using certain types of images, text, or combinations of both, the seemingly harmless meme becomes a multimodal type of hate speech -- a hateful meme. The Hateful Memes Challenge\footnote{\url{https://www.drivendata.org/competitions/70/hateful-memes-phase-2/}} is a first-of-its-kind competition which focuses on detecting hate speech in multimodal memes and it proposes a new data set containing 10,000+ new examples of multimodal content. We utilize VisualBERT -- which meant to be the ``BERT of vision and language'' -- that was trained multimodally on images and captions and apply Ensemble Learning. Our approach achieves 0.811 AUROC with an accuracy of 0.765 on the challenge test set and placed third out of 3,173 participants in the Hateful Memes Challenge\footnote{HateDetectron at \url{https://www.drivendata.org/competitions/70/hateful-memes-phase-2/leaderboard/}}. The code is available at  %   \begin{center}     \url{https://github.com/rizavelioglu/hateful_memes-hate_detectron}     % \end{center}",429
"  Designing a robust spoken language identification  algorithm is very important for the wide usability of multi-lingual speech applications . With the resurgence of deep model learning, the SLID performance has been significantly improved by current supervised deep feature and classifier learning algorithms . In most algorithms, there is an implicit assumption that the training and testing data sets share a similar statistical distribution property. However, due to the complex acoustic and linguistic patterns, it is often the case that testing data set and training data set are from quite different domains . An intuitive solution is to do domain adaptation, i.e., to align the statistical distribution of testing data set to match that of training data set thus to improve the performance. Although with large collected labeled testing data set, it is not difficult to obtain a domain transfer function with supervised learning algorithms, in real applications, the label information of testing data set is often unknown. Therefore, in this study, we mainly focus on a more preferable and challenge situation, i.e., unsupervised domain adaptation.  Unsupervised domain adaptation algorithms have been proposed for speaker verification, e.g., probabilistic linear discriminant analysis  parameter adaptation  , feature-based correlation alignment  , and feature-distribution adaptor for different domain vectors . However, in these algorithms, most of them were proposed for speaker verification under the framework of the PLDA . As our experiments showed that the PLDA framework does not perform well for our SLID task due to the less of discriminative power of the modeling. Instead, in most SLID algorithms, a multiple mixture of logistic regression  model is used as a classifier model. Moreover, due to the complex shapes of the distributions in training and testing domains, it is difficult to guarantee the match between different domain distributions.  The purpose for domain adaptation is to reduce the domain discrepancy. Recently, optimal transport  has been intensively investigated for domain adaptation in machine learning field . The initial motivation for OT in machine learning is to find an optimal transport plan to convert one probability distribution shape to another shape with the least effort . By finding the optimal transport, it naturally defines a distance measure between different probability distributions. Based on this property, the OT is a promising tool for domain adaptation and shape matching in image processing, classification, and segmentation . In this paper, inspired by the OT based unsupervised adaptation , we propose an unsupervised neural adaptation framework for cross-domain SLID tasks. Our main contributions are:  We propose an unsupervised neural adaptation model for SLID to deal with domain mismatch problem. In the model, we explicitly formulate the adaptation in transformed feature space and classifier space in order to reduce the probability distribution discrepancy between source and target domains.  We coincide the OT distance metric in measuring the probability distribution discrepancy, and integrate it into the network optimization in order to learn the adaptation model parameters. Based on the adaptation model, significant improvements were obtained. %The remainder of the paper is organized as follows. Section  introduces the background and fundamental theory of . Section  describes the implementation details of . Section  presents the SLID experiments and results based on the proposed framework by analyzing the contribution of the CSA model in detail. Section  presents the discussion of the results and conclusion of the study. 
"," Due to the mismatch of statistical distributions of acoustic speech between training and testing sets, the performance of spoken language identification  could be drastically degraded. In this paper, we propose an unsupervised neural adaptation model to deal with the distribution mismatch problem for SLID. In our model, we explicitly formulate the adaptation as to reduce the distribution discrepancy on both feature and classifier for training and testing data sets. Moreover, inspired by the strong power of the optimal transport  to measure distribution discrepancy, a Wasserstein distance metric is designed in the adaptation loss. By minimizing the classification loss on the training data set with the adaptation loss on both training and testing data sets, the statistical distribution difference between training and testing domains is reduced. We carried out SLID experiments on the oriental language recognition  challenge data corpus where the training and testing data sets were collected from different conditions. Our results showed that significant improvements were achieved on the cross domain test tasks.",430
"  The internet is having a huge impact on all of our lives and our virtual presence reflects both our personalities and beliefs but also our biases and prejudices. Billions of people are interacting with various online content every day and while some of it is highly useful and enriches our knowledge and understanding of the world, an increasing portion of this content is also harmful. This includes hate speech, misinformation and other forms of online abuse. An increasing amount of effort is required to quickly detect this content, scale up the review work and make automatic decisions to take down the harmful media fast in order to minimize the inflicted harm to the readers.  Many of our interactions happen on social media platforms, which we use to share messages and pictures with our private community or general public audiences.  Facebook AI has launched a competition  to flag hateful memes consisting of both images and text. For this purpose they provide a unique labeled dataset of 10,000+ high quality new multimodal memes. The goal of the challenge is to create an algorithm that identifies multimodal hate speech in internet memes, while also being robust to their benign flip. A meme might be mean or hateful either because of the meme image itself, or the text or their combination. Benign flipping is an augmentation technique used by the competition organizers to flip a meme from hateful to non-hateful and viceversa. This requires changing either the meme text or the image to flip its label. Figure shows how this process works.  Since the problem is formulated as a binary classification task, the primary evaluation metric used to rank the results is the area under the receiver operating characteristic curve . This represents the area under the ROC curve, which in turn plots the True Positive Rate  vs. False Positive Rate  at different classification thresholds T. The goal is to maximize the AUROC.   Accuracy is the secondary tracked metric and it calculates the percentage of instances where the predicted class \^{y} matches the actual class, y in the test set.   Ideally, the model maximizes both these metrics.  In summary, our contribution is threefold:      
","   While significant progress has been made using machine learning algorithms to detect hate speech, important technical challenges still remain to be solved in order to bring their performance closer to human accuracy. We investigate several of the most recent visual-linguistic Transformer architectures and propose improvements to increase their performance for this task. The proposed model outperforms the baselines by a large margin and ranks 5\textsuperscript{th} on the leaderboard out of 3,100+ participants.    \footnote{Code is available at \url{https://github.com/vladsandulescu/hatefulmemes}.}",431
" In traditional ad-hoc retrieval, queries and documents are represented by variants of bag-of-words representations. This leads to the so called vocabulary mismatch problem: when a query contains words that do not exactly match words in a relevant document, the search engine may fail to retrieve this document. Query expansion and document expansion, the methods of adding additional terms to the original query or document, are two popular solution to alleviate the vocabulary mismatch problem.   Document expansion has been shown to be particularly effective for short text retrieval and language-model based retrieval . Most of the existing works in document expansion are unsupervised: using information from the corpus to augment document representation, e.g., retrieval based  and clustering based , or using external information to augment document representation .  Recently, \citet{nogueira2019DE} proposed a new approach to document expansion, which is based on a popular generative sequence-to-sequence model  in NLP, transformers . It leverages supervision to train the model to predict expansion terms conditional on each document. The paper has shown significant improvement on passage  datasets, when trained in-domain. In this paper, we follow this line of supervised neural document expansion approach and explore its performance on standard IR benchmarking dataset. Our main contributions are: 1. Adapting the method to unlabeled datasets by exploring transfer learning and weak-supervision approaches. 2. Adapting the method to traditional IR datasets, where a large number of long documents are present.    
","     Recently, \citet{nogueira2019DE} proposed a new approach to document expansion based on a neural Seq2Seq model, showing significant improvement on short text retrieval task. However, this approach needs a large amount of in-domain training data.     In this paper, we show that this neural document expansion approach can be effectively adapted to standard IR tasks, where labels are scarce and many long documents are present.",432
"    Cognitive studies show that human infants develop object individuation skill from diverse sources of information: spatial-temporal information, object property information, and language~. Specifically, young infants develop object-based attention that disentangles the motion and location of objects from their visual appearance features. Later on, they can leverage the knowledge acquired through word learning to solve the problem of object individuation: words provide clues about object identity and type. The general picture from cognitive science is that object perception and language co-develop in support of one another .     Our long-term goal is to endow machines with similar abilities. In this paper, we focus on how language may support object segmentation. Recent work has studied the problem of unsupervised object representation learning, though without language. As an example, factorized, object-centric scene representations have been used in various kinds of prediction~, reasoning~, and planning tasks~, but they have not considered the role of language and how it may help object representation learning.  As a concrete example, consider the input images shown in \fig{fig:teaser} and the paired questions. From language, we can learn to associate concepts, such as black, pan, and legs, with the referred object's visual appearance. Further, language provides cues about how an input scene should be segmented into individual objects: a wrong parsing of the input scene will lead to an incorrect answer to the question. We can learn from such failure that the handle belongs to the frying pan  and the chair has four legs .  Motivated by these observations, we propose a computational learning paradigm, \modelfull , associating learned object-centric representations to their visual appearance  in images, and to concepts---words for object properties such as color, shape, and material---as provided in language. Here the language input can be either descriptive sentences or question-answer pairs. \model requires no annotations on object masks, categories, or properties during the learning process.  In \model, four modules are jointly trained. The first is an image encoder, learning to encode an image into factorized, object-centric representations. The second is an image decoder, learning to reconstruct masks for individual objects from the learned representations by reconstructing the input. These two modules share the same formulation as recent unsupervised object segmentation research: learning to decompose the image into a series of slot profiles, comprised of pixel masks and latent embeddings. Each slot profile is expected to represent a single object in the image.  The third module in \model is a pre-trained semantic parser that translates the input sentence into a semantic, executable program, where each concept  is associated with a vector space embedding. Finally, the last module, a neural-symbolic program executor, takes the object-centric representation from Module 1, intermediate representations from Module 2, and concept embeddings and the semantic program from Module 3 as input, and outputs an answer if the language input is a question, or TRUE/FALSE if it's a descriptive sentence.  The correctness of the executor's output and the quality of reconstructed images  are the two supervisory signals we use to jointly train Modules 1, 2, and 4.  %  % % %  We integrate the proposed \model with state-of-the-art unsupervised segmentation methods, MONet~ and Slot Attention~. The evaluation is based on two datasets: ShopVRB~ contains images of daily objects and question-answer pairs; PartNet~ contains images of furniture with hierarchical structure, supplemented by descriptive sentences we collected ourselves. We show that \model consistently improves existing methods on unsupervised object segmentation, % much more likely to group different parts of a single object into a single mask.  We further analyze the object-centric representations learned by \model. In \model, conceptually similar objects  appear to be clustered in the embedding space. Moreover, experiments demonstrate that the learned concepts can be used in new tasks, such as visual grounding of referring expressions, without any additional fine-tuning. %  %  %  % % % % % % % % %  % 
","     We present \modelfull , a paradigm for learning disentangled, object-centric scene representations from vision and language. \model builds upon recent advances in unsupervised object segmentation, notably MONet and Slot Attention. While these algorithms learn an object-centric representation just by reconstructing the input image, \model enables them to further learn to associate the learned representations to concepts, \ie, words for object categories, properties, and spatial relationships, from language input. These object-centric concepts derived from language facilitate the learning of object-centric representations. \model can be integrated with various unsupervised segmentation algorithms that are language-agnostic. Experiments show that the integration of \model consistently improves the object segmentation performance of MONet and Slot Attention on two datasets via the help of language. We also show that concepts learned by \model, in conjunction with segmentation algorithms such as MONet, aid downstream tasks such as referring expression comprehension.",433
"  A speech signal can be considered as a variable-length temporal sequence, and many features have been used to characterize its pattern. Short-term spectral features are used extensively because of the quasi-stationary property of the speech signal. After short-term processing, the raw waveform is converted into a two-dimensional~ matrix of size , where  represents the frequential feature dimension related to the number of filter coefficients, and  denotes the temporal frame length related to the utterance duration.  For a text-independent speaker verification~ system, the main procedure is to extract the fixed-dimensional speaker representation from the variable-length spectral feature sequence. One of the widely used spectral features is the Mel-frequency cepstral coefficient ~. Typically, MFCC feature vectors from all the frames are assumed to be independent and identically distributed. They can be projected on the Gaussian components or phonetic units to accumulate statistics over the time axis and form a high-dimensional supervector. Then, a factor analysis-based dimension reduction is performed to generate a fixed-dimensional low rank i-vector representation. Recently, with the progress of deep learning, many approaches directly train a deep neural network~ to distinguish different speakers. Systems comprising of x-vector speaker embedding followed by a probabilistic linear discriminant analysis~ have shown state-of-the-art performances on multiple TISV tasks. In the x-vector system, a time-delay neural network~ followed by statistic pooling over the time axis is used for modeling the long-term temporal dependencies from the MFCC features.              \end{figure*}  For the i-vector, x-vector, and many other speech modeling methods, the feature matrix  is viewed as a multi-channel 1-D time series. Although the duration  may vary among the utterances, the feature dimension  must be a fixed value. In this paper, we consider the feature matrix as a single-channel 2-D image. From this new perspective, the spectral feature is viewed as a ``picture"" of the sound, and a 2-D CNN  is implemented in the same way as traditional image recognition paradigms. This kind of process brings a type of flexibility, i.e., the size of the input ``image,"" including the width  and the height , can be arbitrary numbers.  In other words, a 2-D CNN trained with a 64-dimensional spectrogram could potentially also process a spectrogram with 48 dimensions.   We aim to utilize the flexibility of the 2-D CNN to tackle the mixed-bandwidth~ joint modeling problem. Currently, there are many devices and equipment that capture speech data in different sampling rates, thus solving the sampling rate mismatch problem has become a research topic in the speech community. The traditional way to accomplish this goal is to train a specific model for every target bandwidth since the sampling rates are different . An alternative solution is to uniformly downsample the wideband~ speech data or extend the bandwidth of a narrowband~ data, so that they can be combined .  In this paper, we present a unified solution to solve the MB joint modeling problem. The key idea is to view the NB spectrogram as a sub-image of the WB spectrogram. The major contributions of this work are summarized as follows.    
"," This paper proposes a unified deep speaker embedding framework for modeling speech data with different sampling rates. Considering the narrowband spectrogram as a sub-image of the wideband spectrogram, we tackle the joint modeling problem of the mixed-bandwidth data in an image classification manner. From this perspective, we elaborate several mixed-bandwidth joint training strategies under different training and test data scenarios. The proposed systems are able to flexibly handle the mixed-bandwidth speech data in a single speaker embedding model without any additional downsampling, upsampling, bandwidth extension, or padding operations. We conduct extensive experimental studies on the VoxCeleb1 dataset. Furthermore, the effectiveness of the proposed approach is validated by the SITW and NIST SRE 2016 datasets.",434
"   % Automatic speech recognition  systems are typically trained on vast quantity of paired audio and text data to attain competitive performance. Obtaining these paired data requires substantial human annotation efforts and is often time-consuming, expensive and error-prone. With the emerging popularity of end-to-end ASR models, the need for large amounts of training data is more demanding than the conventional hybrid-based ASR systems. For this purpose, semi-supervised learning  is often investigated for speech recognition, where a model is trained using a finite amount of labeled data and a much larger amount of unlabeled data.   In the long history of semi-supervised learning  in speech recognition, self-training approach  and knowledge distillation , or known as teacher-student model training  are the two commonly used SSL methods. Recent success of representation learning enables a new approach towards leveraging unlabeled data. In natural language processing community,  BERT, ELMo, XLNet , GPT   and its follow-ups are classical examples of representation learning. The key philosophy of representation learning is based on using self-supervised learning, where we obtain `free' labels from unlabeled data and train them in a supervised manner via some proxy tasks. In the context of BERT, two proxy tasks are defined including masked language model task and two-sequence prediction task. These proxy tasks are designed to force the learning of a robust, meaningful representation.  After the representation has been learned, a downstream task model is then trained using labeled data with the learned representation. Optionally, the representation learning block and downstream task block can be fine-tuned together.   Learning efficient speech representation can be traced back to restricted Boltzmann machine , which allows pre-training on large amounts of unlabeled data before training the deep neural network speech models.  More recently, speech representation learning has drawn increasing attention in speech processing community and has shown promising results in semi-supervised speech recognition .  The design of proxy tasks in learning speech representation can be categorized into two types. The first type is based on contrastive loss and has been applied to speech representation such as wav2vec and its variants . The model is trained to learn representations containing information that most discriminates the future or masked frame from a set of negative samples via contrastive loss.  The second type is based on reconstructive loss. The proxy task for these representation learning methods is to reconstruct temporal slices of acoustic features based on contextual information. These reconstruction tasks can be defined as autoregressive reconstruction, or masked-based reconstruction. APC  and its follow-up  are examples to use autoregressive reconstruction loss.  In many state-of-the-art pretrained language model task, masked-based prediction is adopted in the proxy tasks such as BERT  and XLNet .  In speech, instead of prediction, we randomly mask temporal slices of acoustic features and attempt to reconstruct them .  Orthogonal to the contrastive-/reconstructive-loss based speech representation learning, vector-quantized speech representations have been proposed. One motivation to apply vector quantization  is that enforcing quantization can lead to better linguistic unit discovery  due to the discrete nature of phonetic units. In VQ-APC , the authors use VQ as a way to limit model capacity and control information needed in encoding representation. In VQ-wav2vec  and wav2vec 2.0 , the author use VQ to facilitate direct application of BERT and other NLP algorithms.  In this paper, we introduce DeCoAR 2.0, a Deep Contextualized Acoustic Representation with vector quantization. We take inspirations from many recent advances in speech representation learning, and propose multiple improvements over vanilla DeCoAR. We summarize the contributions of this paper as follows:   % The rest of the paper is organized as follows. Section gives a brief overview of our previous DeCoAR method and related work in vector quantized speech representation learning. Section describes the proposed DeCoAR 2.0 approach. Experimental results on semi-supervised speech recognition are presented in Section followed by conclusion in Section.   % Learning robust speech representation has been exploited in recent years. Among these approaches, wav2vec 2.0  uses 10 minutes of labeled data with 53k hours of unlabeled data to achieve a word error rate  of 5.2/8.6 on LibriSpeech benchmark. The model relies on a diverse codebook learnt to correlates the underlying speech units to speech representations via the contrastive loss. However, the contrastive loss formulation can result in several locally optimal codebooks, for exmaples, acoustic condition-sensitive codebooks: where the model can easily optimized by assign acoustic condition  to the the codebooks, and temporally invariant codebooks: where the model assigns specific codes to fixed temporal locations.   %Furthermore, the codes at each time step the model select right after their feature encoder hardly contained meaningful phonetic information. So their contrastive approach might not generalize well to all datasets, espically the real world data consisted a lot of nausence factor like noise, different recording environment.   % A simple workaround could be using frame reconstruction as objective, the network allows a flow of information from the input feature back to the the latent space to preserve meaningful information in the codes, helping mitigatate the codebook learning problems in contrastive loss as discussed above. And compared to simple reconstruction where we utilize all the information available  to achieved maximal prediction while those information are less relevant to ASR. By utilizing the VQ layer, the model is able to keep the representation from those unwanted information flowing.     % Automatic speech recognition  systems are typically trained on vast quantity of paired audio and text data to attain competitive performance. Obtaining these paired data requires substantial human annotation efforts and is often time-consuming, expensive and error-prone. With the emerging popularity of end-to-end ASR models, the need for large amounts of training data is more demanding than the conventional hybrid-based ASR systems. For this purpose, semi-supervised learning  is often investigated for speech recognition, where a model is trained using a finite amount of labeled data and a much larger amount of unlabeled data.   % In the long history of SSL in speech recognition, self-training approach  is the most commonly used approach. In self-training methods, a `seed' ASR model is trained using paired audio/text data. The resulting model is then applied to transcribe the unlabeled audio data. The resulting hypotheses, combined with different data selection criteria, are treated as `pseudo-labels' and added to the original labeled dataset to retrain a new model. Simple in concept, self-training works well in practice with one major caveat - the pseudo-label injects systematic bias introduced by the seed model. To alleviate this, careful confidence calibration with system combinations are often used . Another family of SSL is based on knowledge distillation , or teacher-student model training , and is mostly applied to acoustic model training in hybrid-based ASR. In these setups, a teacher model  generates frame-wise soft label instead of hard label, and a student model is trained on the soft labels via KL divergence loss instead of a standard cross-entropy loss based on forced alignment. The knowledge distillation based SSL partially mitigates the systematic bias but is rarely being investigated towards sequence-level loss  or end-to-end ASR systems.    % Recent success of efficient representation learning, in particular in natural language processing , enables a new approach towards leveraging unlabeled data. Classical examples of representation learning for NLP include BERT, ELMo, XLNet , GPT and its follow-ups , to name but a few.  The key philosophy of representation learning is based on self-supervised learning, where we obtain `free' labels from unlabeled data and train them in a supervised manner via some proxy tasks. In the context of the well-known BERT, two proxy tasks are defined including masked language model task and two-sequence prediction task. These proxy tasks are defined in a way to force the learning of a robust, meaningful representation.  A downstream task is then trained on the labeled data with the learned representation. Optionally, the representation learning block and downstream task can be fine-tuned together.     % This paper presents DeCoAR 2.0, a follow-up on DeCoAR . We take inspiration from many recent advances in speech representation learning, and propose multiple improvements over vanilla DeCoAR. We summarize the contributions of this paper as follows: %   % The rest of the paper is organized as follows. Section gives an overview on related work in speech representation learning, and a brief recap of our previous DeCoAR method. Section describes the proposed vector quantized DeCoAR approach. Experimental results on semi-supervised speech recognition are presented in Section followed by conclusion in Section.   % In this work, we propose an improved speech representation learning paradigms towards semi-supervised speech recognition based on our previous work .   % Current state-of-the-art models for speech recognition require vast amounts of transcribed audio data to attain good performance. In particular, end-to-end ASR models are more demanding in the amount of training data required when compared to traditional hybrid models. While obtaining a large amount of labeled data requires substantial effort and resources, it is much less costly to obtain abundant unlabeled data.   % For this reason, semi-supervised learning  is often used when training ASR systems. Recently, self-supervised learning閳ユ攣 paradigm that treats the input itself or modifications of the input as learning targets閳 has obtained promising results. Those self-supervised speech representation can be fall into main categories: Contrastive Predictive Coding  incorporates contrastive objective to learn representations containing information that  most discriminates the future or masked frame from a set of negative samples. Another approach is Autoregressive Predictive Coding  , which tries to directly predict or reconstruct the frame based on context.  % More recently, vector-quantized representations of audio data has drawn increasing attention in speech processing . The motivation is that enforcing the quantization leads to a better representation for acoustic unit discovery due to the discrete nature of phonetic units. VQ-APC  also try to exactly quantified of information , to control the capacity of the models. And the use of vector quantization limited capacity are forced to retain information to achieve maximal prediction.   % Despite the success of the wav2vec 2.0 model , the model relies on a diverse codebook learnt to correlates the underlying speech units to speech representations via the contrastive loss. However, the codes at each time step the model select right after feature encoder hardly contained meaningful phonetic information. More importantly, contrastive loss formulation can result in several locally optimal codebooks. A few highly probable optima observed were acoustic condition-sensitive codebooks: where the model can easily optimized by assign acoustic condition  to the the codebooks, and temporally invariant codebooks: where the model assigns specific codes to fixed temporal locations to enable a good contrastive loss. Hence, the codebook learning methodology using contrastive loss might not generalize well to all datasets, espically the real world data consisted a lot of nausence factor like noise, different recording environment.    % A simple solution could be to enforce the codes to explicitly carry information about the input features in the process. Using frame reconstruction as objective, the network allows a flow of information from the input feature back to the the latent space to preserve meaningful information, helping mitigatate the codebook learning problems in contrastive loss as discussed above. Thus, we propose an novel self-supervised model that learns vector quantized deep transformer acoustic representations based on frames reconstruction. Since simple reconstruction utilize all the information available  to achieved maximal prediction while those information are less relevant to ASR. And by utilizing the VQ layer to limit those unwanted information flow into final representation, Vector Quantized Deep Contextualized Acoustic Representations  are able to achieve much better representation that's better suited for semi-supervised ASR tasks. By using a large amount of unlabeled data, and then applies these representations to ASR tasks using a limited amount of labeled data. In our implementation, we perform acoustic representation learning using deep transformer and a training objective that minimizes the reconstruction error of a temporal slice of filterbank features given context frames. After pre-training, we fix these parameters and add output layers with connectionist temporal classification  loss for the ASR task. We only train the small ASR model instead of fine-tuning for computing-efficiency. Our approach showed that supervision with 10 hours of labeled data on DeCoAR 2.0 achieves performance on par with training on all 960 hours directly.
","  Recent success in speech representation learning enables a new way to leverage unlabeled data to train speech recognition model. In speech representation learning, a large amount of unlabeled data is used in a self-supervised manner to learn a feature representation. Then a smaller amount of labeled data is used to train a downstream ASR system using the new feature representations. Based on our previous work DeCoAR \cite{ling2020deep} and inspirations from other speech representation learning, we propose DeCoAR 2.0, a Deep Contextualized Acoustic Representation with vector quantization. We introduce several modifications over the DeCoAR: first, we use Transformers in encoding module instead of LSTMs; second, we introduce a vector quantization layer between encoder and reconstruction modules; third, we propose an objective that combines the reconstructive loss with vector quantization diversity loss to train speech representations. Our experiments show consistent improvements over other speech representations in different data-sparse scenarios. Without fine-tuning, a light-weight ASR model trained on 10 hours of LibriSpeech labeled data with DeCoAR 2.0 features outperforms the model trained on the full 960-hour dataset with filterbank features.   % \yuzong{rewrite this} % We propose a novel approach for vector quantized deep contextualized acoustic representations. Following the same schema in DeCoAR\cite{ling2020deep}, we first exploit a large amount of unlabeled audio data via representation learning, where we reconstruct a temporal slice of filterbank features from context frames. The new resulting deep contextualized acoustic vector quantized representations  are then used to train a small CTC-based ASR system using a small amount of labeled audio data. In our experiments, we show that systems trained on DeCoAR 2.0 consistently outperform ones trained on other acoustic representations, giving the state-of-art and comparable results with wav2vec 2.0 \cite{baevski2020wav2vec} on semi-supervised experiments on Librispeech. Our approach can drastically reduce the amount of labeled data required; unsupervised training on LibriSpeech then supervision with 10 hours of labeled data achieves performance on par with training on all 960 hours directly.",435
" % % {A}{utomatic}  speech recognition , one of the core components in speech technology, has achieved significant advancements during the past decade . A key driving force behind these advancements is the rapid development of deep learning techniques .  % State-of-the-art  ASR systems    are usually trained with thousands of hours of transcribed speech data and a massive amount of text data. % % State-of-the-art  ASR systems    usually requires thousands of hours of transcribed speech data and a massive amount of text data to train a hybrid deep neural network-hidden Markov model  based acoustic model     and a recurrent neural network  language model . % Moreover, a hand-crafted pronunciation lexicon and a phoneme inventory based on linguistic expertise are often needed. Recently, end-to-end  ASR architectures, in which AM and LM training is integrated as a single pipeline, have gradually become the mainstream in ASR academic research , compared to   hybrid deep neural network-hidden Markov model  architectures . E2E architectures have the advantage of removing the need of a pronunciation lexicon and a phoneme inventory during system development. However, training an E2E ASR system tends to require even more transcribed speech data than for a hybrid DNN-HMM ASR system .  There are around  spoken languages in the world .  For most of them, the amount of transcribed speech data resources is very limited, or even non-existent . Many of these low-resource languages, such as ethnic minority languages in China and languages in Africa, may have never been formally studied. In addition to the lack of enough transcribed speech data, linguistic knowledge about such languages is incomplete, or may even be entirely lacking. Conventional supervised acoustic modeling  can therefore not be applied directly. This leads to the current situation that high-performance ASR systems are only available for a small number of major languages, e.g., English, Mandarin, French. To facilitate ASR technology for low-resource languages, investigation of unsupervised acoustic modeling  methods is necessary, which aims to find and model a set of basic speech units that represents all the sounds in the language of interest, i.e., the low-resource, target language.   Recently, there has been a growing research interest in UAM .  A strict assumption of UAM is that for the target language only raw speech data is available, while the transcriptions, phoneme inventory  and pronunciation lexicon are unknown. This is known as the zero-resource assumption .   %It is a challenging task, yet with significant research impact in a broad area of speech and language science and technology, e.g., query-by-example spoken term detection , text-to-speech without text , understanding the mechanisms underlying infant language acquisition , and the documentation  of endangered languages .  There are two main research strands in UAM. The first strand formulates the problem as discovering a finite set of phoneme-like speech units . This is often referred to as acoustic unit/model discovery  . The second strand formulates the problem as learning acoustic feature representations that can distinguish subword  units of the target language, and is robust to linguistically-irrelevant factors, such as speaker  . This is often referred to as unsupervised subword modeling . In essence, the second strand is focused on learning an intermediate representation towards the ultimate goal of UAM, while the first strand aims directly at the ultimate goal. These two strands are closely connected and can benefit from each other; for instance, a good subword-discriminative feature representation  % good feature representation that is discriminative to subword units and is robust to speaker variation  has been shown beneficial to AUD , while conversely,  discovered speech units with good consistency with true phonemes are helpful to % could provide phoneme-like pseudo transcriptions to assist the  learning   subword-discriminative acoustic feature representations .   This study addresses unsupervised subword modeling in UAM. Learning subword-discriminative feature representations in the zero-resource scenario has been shown to be a non-trivial task . The major difficulty is the separation of linguistic information   from non-linguistic information .   For instance, a speech sound such as [\ae]\footnote{International Phonetic Alphabet  symbol.} produced by different speakers  might be mistakenly modeled as different speech units .    There are many interesting attempts to unsupervised subword modeling . One typical research direction is to leverage purely unsupervised learning techniques. One method is the clustering of speech sounds that have acoustically similar patterns and that potentially correspond to the same subword units  , which results in phoneme-like pseudo transcriptions that can be used to facilitate subword-discriminative feature learning . % , e.g. cluster posteriorgrams  or DNN bottleneck features  .  Unsupervised and self-supervised representation learning algorithms are applied to learn, without using external supervision, speech features that retain the linguistic content in the original data while ignoring linguistically-irrelevant information, particularly speaker variation  .    A second research direction to unsupervised subword modeling is to exploit cross-lingual knowledge . Speech and text resources from out-of-domain  resource-rich languages have been shown beneficial to modeling subword units of in-domain low-resource languages. For instance,  used an OOD AM to extract cross-lingual bottleneck features , while  used an OOD ASR to generate cross-lingual phone labels. % by past studies .   % One idea is to utilize a pre-trained DNN AM from an OOD language to generate phoneme-discriminative representations of target speech, such as bottleneck features  . % The second idea would be to leverage an OOD ASR system to decode speech utterances in the target language and obtain cross-lingual phone labels as supervision for subsequent subword modeling  .  % These two ideas realize cross-lingual knowledge transfer at the AM level and phone label level respectively.  % Cross-lingual knowledge transfer can be done at AM level, i.e., an OOD pretrained AM used to generate  for speech of the  target language.  % It can also be done at  phone label level, i.e., an OOD ASR system decoding target speech utterances to generate phone labels as cross-lingual supervision .  %  This study adopts a two-stage learning framework which combines both research directions within the area of unsupervised subword modeling.  % The  high-level overview  of  the  proposed  framework  is  shown  in Fig. .  %, and  At the first stage, the front-end, a self-supervised representation learning model named autoregressive predictive coding      is trained. APC preserves phonetic  and speaker information from the original speech signal, but makes the two information types more separable . %This makes APC a suitable method for unsupervised subword modeling.   At the second stage, the back-end, a cross-lingual, OOD DNN model with a bottleneck layer  is trained using the APC pretrained features as the input features to create the missing  frame labels. % , as seen in Fig. .  %Frame labels required for DNN-BNF model training are not directly available due to the zero-resource assumption. In our framework, the labels are obtained using an OOD ASR system.  %By doing so, cross-lingual phonetic knowledge is exploited.  This system framework was proposed in our recent study ,  and showed state-of-the-art performances on the subword discriminability task on two databases in UAM: ZeroSpeech 2017  and Libri-light .   In this work, we expand and extend the work in . Specifically, we  compare the proposed approach to a supervised topline system that is trained on transcribed data of the target language;  compare the proposed approach with another cross-lingual knowledge transfer method  ; % investigate which of the AM-level or phone label-level knowledge transfer methods is more effective;   %  investigate the effects of the recently proposed APC model architectures in front-end pretraining in detail;   investigate the potential of our approach in relation to the amount of unlabeled training material by varying the data between  hours   and  hours, and compare the models' performance to the topline model. Throughout our experiments, English is chosen as the target low-resource language. Its phoneme inventory and transcriptions are assumed unavailable during system development. Dutch and Mandarin are chosen as the two OOD languages for which phoneme inventories and transcriptions are available.  Unsupervised subword modeling is typically evaluated using overall performance measures, such as ABX  , purity , normalized mutual information  . These metrics, however, do not provide insights on the approaches閳 ability of modeling individual phonemes or phoneme categories. As the ultimate goal beyond unsupervised subword modeling is to discover basic speech units that have a good consistency with the true phonemes of the target language, we, to the best of our knowledge for the first time in the literature, additionally present detailed analyses that explore the question of the effectiveness of the proposed approach to capturing phoneme and articulatory feature  information of the target language. % To answer this question The analyses are based on the standard ABX error rate evaluation , which we adapted for this work , and consist of two parts, i.e., an analysis at the phoneme level and at the AF level. The analyses are aimed at investigating what phoneme and AF information is  captured by the learned subword-discriminative feature representation, which can be used to guide future research to improve unsupervised subword modeling as well as AUD. Moreover, we correlate the phoneme-level ABX error rates and the quality of the cross-lingual phone labels which are used to train our back-end DNN-BNF model in order to study why the proposed approach performs differently in capturing different target phonemes' information, and how the performance is affected by the quality of cross-lingual phone labels.    %The analysis at the AF level is carried out as we are interested in the  extent to which the AF information in the target language can be learned by our subword-discriminative feature representation.  % AFs describe the target of the articulators in the vocal tract when pronouncing a specific phone . The use of AFs has been shown beneficial to low-resource ASR    and acoustic unit discovery .  % {\color{cyan}do we need a introduction to AF?} % The AFs describe the movement of the tongue, lips and other organs to produce speech sounds. % {\color{cyan}state why do we do this}  % The AF is a  compact and universal representation of speech, and is   more language-independent than the phoneme inventory representation.  % We are interested in to which extent is the AF information in the target language  learned in our subword-discriminative feature representation. %In the AF-level analysis, a new evaluation metric is proposed to measure the efficacy of our approach in capturing AF information. This metric replaces the phoneme inventory in the ABX discriminability task with the AF category.  % Specifically, the task is to predict whether a test speech segment  belongs to the same AF attribute as  or  as , where  and  contain speech sounds belonging to different AF attributes.  %Several AFs are investigated in this study, including place of articulation  and manner of articulation  for consonants, tongue height and tongue backness for monophthong vowels. %The AF-level analysis could potentially provide guidance on future research to improve unsupervised subword modeling as well as AUD. To our knowledge there are very few previous studies on AF-level analysis to unsupervised subword modeling and AUD.  % For instance, two systems achieving the same overall subword modeling performance might vary greatly in linguistic implications.   % overall performance metrics, such as ABX subword discriminability  , purity , normalized mutual information  .    % , or used as the input to perform further subword-discriminative learning .      % ,  i.e., the unsupervised feature representation learning problem.  % {\color{cyan} high-level review representative approaches.  purely unsupervised learning approaches 1-1. clustering; 1-2 unsupervised feature learning.  leveraging OOD resources.} % {\color{red}Text to be colored} % to train the deep neural network -based acoustic model and massive amount of text data to train the  %   The remainder of this paper is organized as follows. Section  provides a review of related works on the unsupervised subword modeling task. In Section , we provide a detailed description of the proposed approach to unsupervised subword modeling, and introduce comparative approaches to compare against our approach. Section  describes the methodology  used for the phoneme-level and AF-level analyses. Section   introduces the experimental design of this study, while Section  reports the results. Section  describes the setup for conducting the phoneme- and AF-level analyses, and discusses the results of the analyses. Finally, Section  draws the conclusions.  
"," % This study addresses unsupervised subword modeling, i.e., learning acoustic feature representations that can distinguish between subword units of a language. We propose a two-stage learning framework that combines self-supervised learning and cross-lingual knowledge transfer. The framework consists of autoregressive predictive coding  as the front-end and a cross-lingual deep neural network  as the back-end.  This study addresses unsupervised subword modeling, i.e., learning acoustic feature representations that can distinguish between subword units of a language. We propose a two-stage learning framework that combines self-supervised learning and cross-lingual knowledge transfer. The framework consists of autoregressive predictive coding  as the front-end and a cross-lingual deep neural network  as the back-end.  % Experiments on the ABX subword discriminability task conducted with the Libri-light and ZeroSpeech 2017 databases show our approach is competitive or superior to state-of-the-art studies. APC pretraining brings improvement to the entire framework, and brings larger improvement with increased amount of training data. Our best performance achieved by using unlabeled training data without linguistic knowledge of the target language is very close to that of a supervised system trained with labeled data of that language. The back-end of our approach is found more effective than a cross-lingual AM based BNF in cross-lingual knowledge transfer. Experiments on the ABX subword discriminability task conducted with the Libri-light and ZeroSpeech 2017 databases showed that our approach is competitive or superior to state-of-the-art studies.  % A comprehensive and systematic analysis at the phoneme- and articulatory feature - level is carried out to investigate the type of information that is captured by our learned feature representation. New metrics are proposed for the phoneme-level ABX subword discriminability task and attribute-level ABX AF task. The phoneme-level analysis showed that compared to MFCC, our approach achieves larger improvement in capturing diphthong information than monophthong vowel information, and the improvement varies greatly to different consonants. Results found there is a positive correlation between the effectiveness of the back-end in capturing a phoneme's information and the quality of cross-lingual phone labels assigned to that phoneme. The AF-level analysis showed that the proposed approach is better than MFCC and APC features in capturing manner of articulation , place of articulation , vowel height and backness information. Results indicate MoA is better captured by the proposed approach than PoA, and both MoA and PoA are better captured than vowel height and backness. Results implies AF information is less language-dependent than phoneme information.   Comprehensive and systematic analyses at the phoneme- and articulatory feature -level showed that our approach was better at capturing diphthong than monophthong vowel information, while also differences in the amount of information captured for different types of consonants were observed. Moreover, a positive correlation was found between the effectiveness of the back-end in capturing a phoneme's information and the quality of the cross-lingual phone labels assigned to the phoneme. The AF-level analysis together with t-SNE visualization results showed that the proposed approach is better than MFCC and APC features in capturing manner and place of articulation information, vowel height, and backness information.  % Taking all the analyses together, the two stages in our approach are both effective in capturing phoneme information. Monophthong vowel information is much more difficult to be captured than consonant information, which suggests a future research direction to improve the effectiveness of capturing monophthong vowel information.  Taken together, the analyses showed that the two stages in our approach are both effective in capturing phoneme and AF information. Nevertheless, monophthong vowel information is less well captured than consonant information, which suggests that future research should focus on improving capturing monophthong vowel information.",436
